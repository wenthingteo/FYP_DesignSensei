{"text": "About This E-Book\nEPUB is an open, industry-standard format for e-books. However, support for EPUB and its many\nfeatures varies across reading devices and applications. Use your device or app settings to customize\nthe presentation to your liking. Settings that you can customize often include font, font size, single or\ndouble column, landscape or portrait mode, and figures that you can click or tap to enlarge. For\nadditional information about the settings and features on your reading device or app, visit the device\nmanufacturer\u2019s Web site.\nMany titles include programming code or configuration examples. To optimize the presentation of\nthese elements, view the e-book in single-column, landscape mode and adjust the font size to the\nsmallest setting. In addition to presenting code and configurations in the reflowable text format, we\nhave included images of the code that mimic the presentation found in the print book; therefore, where\nthe reflowable format may compromise the presentation of the code listing, you will see a \u201cClick here\nto view code image\u201d link. Click the link to view the print-fidelity code image. To return to the\nprevious page viewed, click the Back button on your device or app.\n", "page": 2, "type": "text", "section": "Page 2"}
{"text": "Clean Architecture\nA CRAFTSMAN\u2019S GUIDE TO SOFTW\nARE STRUCTURE AND DESIGN\nRobert C. Martin\nBoston \u2022 Columbus \u2022 Indianapolis \u2022 New York \u2022 San Francisco \u2022 Amsterdam \u2022 Cape Town Dubai \u2022\nLondon \u2022 Madrid \u2022 Milan \u2022 Munich \u2022 Paris \u2022 Montreal \u2022 Toronto \u2022 Delhi \u2022 Mexico City S\u00e3o Paulo \u2022\nSydney \u2022 Hong Kong \u2022 Seoul \u2022 Singapore \u2022 Taipei \u2022 Tokyo\n", "page": 4, "type": "text", "section": "Page 4"}
{"text": "Many of the designations used by manufacturers and sellers to distinguish their products are claimed\nas trademarks. Where those designations appear in this book, and the publisher was aware of a\ntrademark claim, the designations have been printed with initial capital letters or in all capitals.\nThe author and publisher have taken care in the preparation of this book, but make no expressed or\nimplied warranty of any kind and assume no responsibility for errors or omissions. No liability is\nassumed for incidental or consequential damages in connection with or arising out of the use of the\ninformation or programs contained herein.\nFor information about buying this title in bulk quantities, or for special sales opportunities (which\nmay include electronic versions; custom cover designs; and content particular to your business,\ntraining goals, marketing focus, or branding interests), please contact our corporate sales department\nat corpsales@pearsoned.com or (800) 382-3419.\nFor government sales inquiries, please contact governmentsales@pearsoned.com.\nFor questions about sales outside the U.S., please contact intlcs@pearson.com.\nVisit us on the Web: informit.com\nLibrary of Congress Control Number: 2017945537\nCopyright \u00a9 2018 Pearson Education, Inc.\nAll rights reserved. Printed in the United States of America. This publication is protected by\ncopyright, and permission must be obtained from the publisher prior to any prohibited reproduction,\nstorage in a retrieval system, or transmission in any form or by any means, electronic, mechanical,\nphotocopying, recording, or likewise. For information regarding permissions, request forms and the\nappropriate contacts within the Pearson Education Global Rights & Permissions Department, please\nvisit www.pearsoned.com/permissions/.\nISBN-13: 978-0-13-449416-6\nISBN-10: 0-13-449416-4\n1 17\n", "page": 5, "type": "text", "section": "Page 5"}
{"text": "This book is dedicated to my lovely wife, my four spectacular children, and their families, including\nmy quiver full of five grandchildren\u2014who are the dessert of my life.\n", "page": 6, "type": "text", "section": "Page 6"}
{"text": "CONTENTS\nForeword\nPreface\nAcknowledgments\nAbout the Author\nPART I         Introduction\nChapter 1     What Is Design and Architecture?\nThe Goal?\nCase Study\nConclusion\nChapter 2     A Tale of Two Values\nBehavior\nArchitecture\nThe Greater V\nalue\nEisenhower\u2019s Matrix\nFight for the Architecture\nPART II       Starting with the Bricks: Programming Paradigms\nChapter 3     Paradigm Overview\nStructured Programming\nObject-Oriented Programming\nFunctional Programming\nFood for Thought\nConclusion\nChapter 4     Structured Programming\nProof\nA Harmful Proclamation\nFunctional Decomposition\nNo Formal Proofs\nScience to the Rescue\n", "page": 7, "type": "text", "section": "Page 7"}
{"text": "Tests\nConclusion\nChapter 5     Object-Oriented Programming\nEncapsulation?\nInheritance?\nPolymorphism?\nConclusion\nChapter 6     Functional Programming\nSquares of Integers\nImmutability and Architecture\nSegregation of Mutability\nEvent Sourcing\nConclusion\nPART III      Design Principles\nChapter 7     SRP: The Single Responsibility Principle\nSymptom 1: Accidental Duplication\nSymptom 2: Merges\nSolutions\nConclusion\nChapter 8     OCP: The Open-Closed Principle\nA Thought Experiment\nDirectional Control\nInformation Hiding\nConclusion\nChapter 9     LSP: The Liskov Substitution Principle\nGuiding the Use of Inheritance\nThe Square/Rectangle Problem\nLSP and Architecture\nExample LSP Violation\nConclusion\nChapter 10   ISP: The Interface Segregation Principle\nISP and Language\nISP and Architecture\nConclusion\nChapter 11   DIP: The Dependency Inversion Principle\n", "page": 8, "type": "text", "section": "Page 8"}
{"text": "Stable Abstractions\nFactories\nConcrete Components\nConclusion\nPART IV      Component Principles\nChapter 12   Components\nA Brief History of Components\nRelocatability\nLinkers\nConclusion\nChapter 13   Component Cohesion\nThe Reuse/Release Equivalence Principle\nThe Common Closure Principle\nThe Common Reuse Principle\nThe Tension Diagram for Component Cohesion\nConclusion\nChapter 14   Component Coupling\nThe Acyclic Dependencies Principle\nTop-Down Design\nThe Stable Dependencies Principle\nThe Stable Abstractions Principle\nConclusion\nPART V       Architecture\nChapter 15   What Is Architecture?\nDevelopment\nDeployment\nOperation\nMaintenance\nKeeping Options Open\nDevice Independence\nJunk Mail\nPhysical Addressing\nConclusion\nChapter 16   Independence\nUse Cases\nOperation\n", "page": 9, "type": "text", "section": "Page 9"}
{"text": "Development\nDeployment\nLeaving Options Open\nDecoupling Layers\nDecoupling Use Cases\nDecoupling Mode\nIndependent Develop-ability\nIndependent Deployability\nDuplication\nDecoupling Modes (Again)\nConclusion\nChapter 17   Boundaries: Drawing Lines\nA Couple of Sad Stories\nFitNesse\nWhich Lines Do You Draw, and When Do You Draw Them?\nWhat About Input and Output?\nPlugin Architecture\nThe Plugin Argument\nConclusion\nChapter 18   Boundary Anatomy\nBoundary Crossing\nThe Dreaded Monolith\nDeployment Components\nThreads\nLocal Processes\nServices\nConclusion\nChapter 19   Policy and Level\nLevel\nConclusion\nChapter 20   Business Rules\nEntities\nUse Cases\nRequest and Response Models\nConclusion\nChapter 21   Screaming Architecture\nThe Theme of an Architecture\nThe Purpose of an Architecture\n", "page": 10, "type": "text", "section": "Page 10"}
{"text": "But What About the Web?\nFrameworks Are Tools, Not Ways of Life\nTestable Architectures\nConclusion\nChapter 22   The Clean Architecture\nThe Dependency Rule\nA Typical Scenario\nConclusion\nChapter 23   Presenters and Humble Objects\nThe Humble Object Pattern\nPresenters and Views\nTesting and Architecture\nDatabase Gateways\nData Mappers\nService Listeners\nConclusion\nChapter 24   Partial Boundaries\nSkip the Last Step\nOne-Dimensional Boundaries\nFacades\nConclusion\nChapter 25   Layers and Boundaries\nHunt the Wumpus\nClean Architecture?\nCrossing the Streams\nSplitting the Streams\nConclusion\nChapter 26   The Main Component\nThe Ultimate Detail\nConclusion\nChapter 27   Services: Great and Small\nService Architecture?\nService Benefits?\nThe Kitty Problem\nObjects to the Rescue\nComponent-Based Services\nCross-Cutting Concerns\n", "page": 11, "type": "text", "section": "Page 11"}
{"text": "Conclusion\nChapter 28   The Test Boundary\nTests as System Components\nDesign for Testability\nThe Testing API\nConclusion\nChapter 29   Clean Embedded Architecture\nApp-titude Test\nThe Target-Hardware Bottleneck\nConclusion\nPART VI      Details\nChapter 30   The Database Is a Detail\nRelational Databases\nWhy Are Database Systems So Prevalent?\nWhat If There Were No Disk?\nDetails\nBut What about Performance?\nAnecdote\nConclusion\nChapter 31   The Web Is a Detail\nThe Endless Pendulum\nThe Upshot\nConclusion\nChapter 32   Frameworks Are Details\nFramework Authors\nAsymmetric Marriage\nThe Risks\nThe Solution\nI Now Pronounce You \u2026\nConclusion\nChapter 33   Case Study: Video Sales\nThe Product\nUse Case Analysis\nComponent Architecture\nDependency Management\nConclusion\n", "page": 12, "type": "text", "section": "Page 12"}
{"text": "Chapter 34   The Missing Chapter\nPackage by Layer\nPackage by Feature\nPorts and Adapters\nPackage by Component\nThe Devil Is in the Implementation Details\nOrganization versus Encapsulation\nOther Decoupling Modes\nConclusion: The Missing Advice\nPART VII     Appendix\nAppendix A   Architecture Archaeology\nIndex\n", "page": 13, "type": "text", "section": "Page 13"}
{"text": "FOREWORD\nWhat do we talk about when we talk about architecture?\nAs with any metaphor, describing software through the lens of architecture can hide as much as it can\nreveal. It can both promise more than it can deliver and deliver more than it promises.\nThe obvious appeal of architecture is structure, and structure is something that dominates the\nparadigms and discussions of software development\u2014components, classes, functions, modules,\nlayers, and services, micro or macro. But the gross structure of so many software systems often defies\neither belief or understanding\u2014Enterprise Soviet schemes destined for legacy, improbable Jenga\ntowers reaching toward the cloud, archaeological layers buried in a big-ball-of-mud slide. It\u2019s not\nobvious that software structure obeys our intuition the way building structure does.\nBuildings have an obvious physical structure, whether rooted in stone or concrete, whether arching\nhigh or sprawling wide, whether large or small, whether magnificent or mundane. Their structures\nhave little choice but to respect the physics of gravity and their materials. On the other hand\u2014except\nin its sense of seriousness\u2014software has little time for gravity. And what is software made of?\nUnlike buildings, which may be made of bricks, concrete, wood, steel, and glass, software is made of\nsoftware. Large software constructs are made from smaller software components, which are in turn\nmade of smaller software components still, and so on. It\u2019s coding turtles all the way down.\nWhen we talk about software architecture, software is recursive and fractal in nature, etched and\nsketched in code. Everything is details. Interlocking levels of detail also contribute to a building\u2019s\narchitecture, but it doesn\u2019t make sense to talk about physical scale in software. Software has structure\n\u2014many structures and many kinds of structures\u2014but its variety eclipses the range of physical\nstructure found in buildings. You can even argue quite convincingly that there is more design activity\nand focus in software than in building architecture\u2014in this sense, it\u2019s not unreasonable to consider\nsoftware architecture more architectural than building architecture!\nBut physical scale is something humans understand and look for in the world. Although appealing and\nvisually obvious, the boxes on a PowerPoint diagram are not a software system\u2019s architecture.\nThere\u2019s no doubt they represent a particular view of an architecture, but to mistake boxes for the big\npicture\u2014for the architecture\u2014is to miss the big picture and the architecture: Software architecture\ndoesn\u2019t look like anything. A particular visualization is a choice, not a given. It is a choice founded on\na further set of choices: what to include; what to exclude; what to emphasize by shape or color; what\nto de-emphasize through uniformity or omission. There is nothing natural or intrinsic about one view\nover another.\n", "page": 14, "type": "text", "section": "Page 14"}
{"text": "Although it might not make sense to talk about physics and physical scale in software architecture, we\ndo appreciate and care about certain physical constraints. Processor speed and network bandwidth\ncan deliver a harsh verdict on a system\u2019s performance. Memory and storage can limit the ambitions of\nany code base. Software may be such stuff as dreams are made on, but it runs in the physical world.\nThis is the monstrosity in love, lady, that the will is infinite, and the execution confined; that the desire is boundless,\nand the act a slave to limit.\n\u2014William Shakespeare\nThe physical world is where we and our companies and our economies live. This gives us another\ncalibration we can understand software architecture by, other less physical forces and quantities\nthrough which we can talk and reason.\nArchitecture represents the significant design decisions that shape a system, where significant is measured by cost of\nchange.\n\u2014Grady Booch\nTime, money, and effort give us a sense of scale to sort between the large and the small, to distinguish\nthe architectural stuff from the rest. This measure also tells us how we can determine whether an\narchitecture is good or not: Not only does a good architecture meet the needs of its users, developers,\nand owners at a given point in time, but it also meets them over time.\nIf you think good architecture is expensive, try bad architecture.\n\u2014Brian Foote and Joseph Yoder\nThe kinds of changes a system\u2019s development typically experiences should not be the changes that are\ncostly, that are hard to make, that take managed projects of their own rather than being folded into the\ndaily and weekly flow of work.\nThat point leads us to a not-so-small physics-related problem: time travel. How do we know what\nthose typical changes will be so that we can shape those significant decisions around them? How do\nwe reduce future development effort and cost without crystal balls and time machines?\nArchitecture is the decisions that you wish you could get right early in a project, but that you are not necessarily more\nlikely to get them right than any other.\n\u2014Ralph Johnson\nUnderstanding the past is hard enough as it is; our grasp of the present is slippery at best; predicting\nthe future is nontrivial.\nThis is where the road forks many ways.\nDown the darkest path comes the idea that strong and stable architecture comes from authority and\nrigidity. If change is expensive, change is eliminated\u2014its causes subdued or headed off into a\nbureaucratic ditch. The architect\u2019s mandate is total and totalitarian, with the architecture becoming a\ndystopia for its developers and a constant source of frustration for all.\nDown another path comes a strong smell of speculative generality. A route filled with hard-coded\nguesswork, countless parameters, tombs of dead code, and more accidental complexity than you can\nshake a maintenance budget at.\n", "page": 15, "type": "text", "section": "Page 15"}
{"text": "The path we are most interested is the cleanest one. It recognizes the softness of software and aims to\npreserve it as a first-class property of the system. It recognizes that we operate with incomplete\nknowledge, but it also understands that, as humans, operating with incomplete knowledge is\nsomething we do, something we\u2019re good at. It plays more to our strengths than to our weaknesses. We\ncreate things and we discover things. We ask questions and we run experiments. A good architecture\ncomes from understanding it more as a journey than as a destination, more as an ongoing process of\nenquiry than as a frozen artifact.\nArchitecture is a hypothesis, that needs to be proven by implementation and measurement.\n\u2014Tom Gilb\nTo walk this path requires care and attention, thought and observation, practice and principle. This\nmight at first sound slow, but it\u2019s all in the way that you walk.\nThe only way to go fast, is to go well.\n\u2014Robert C. Martin\nEnjoy the journey.\n\u2014Kevlin Henney\nMay 2017\n", "page": 16, "type": "text", "section": "Page 16"}
{"text": "PREFACE\nThe title of this book is Clean Architecture. That\u2019s an audacious name. Some would even call it\narrogant. So why did I choose that title, and why did I write this book?\nI wrote my very first line of code in 1964, at the age of 12. The year is now 2016, so I have been\nwriting code for more than half a century. In that time, I have learned a few things about how to\nstructure software systems\u2014things that I believe others would likely find valuable.\nI learned these things by building many systems, both large and small. I have built small embedded\nsystems and large batch processing systems. I have built real-time systems and web systems. I have\nbuilt console apps, GUI apps, process control apps, games, accounting systems, telecommunications\nsystems, design tools, drawing apps, and many, many others.\nI have built single-threaded apps, multithreaded apps, apps with few heavy-weight processes, apps\nwith many light-weight processes, multiprocessor apps, database apps, mathematical apps,\ncomputational geometry apps, and many, many others.\nI\u2019ve built a lot of apps. I\u2019ve built a lot of systems. And from them all, and by taking them all into\nconsideration, I\u2019ve learned something startling.\nThe architecture rules are the same!\nThis is startling because the systems that I have built have all been so radically different. Why should\nsuch different systems all share similar rules of architecture? My conclusion is that the rules of\nsoftware architecture are independent of every other variable.\nThis is even more startling when you consider the change that has taken place in hardware over the\nsame half-century. I started programming on machines the size of kitchen refrigerators that had half-\nmegahertz cycle times, 4K of core memory, 32K of disk memory, and a 10 character per second\nteletype interface. I am writing this preface on a bus while touring in South Africa. I am using a\nMacBook with four i7 cores running at 2.8 gigahertz each. It has 16 gigabytes of RAM, a terabyte of\nSSD, and a 2880\u00d71800 retina display capable of showing extremely high-definition video. The\ndifference in computational power is staggering. Any reasonable analysis will show that this\nMacBook is at least 1022 more powerful than those early computers that I started using half a century\nago.\nTwenty-two orders of magnitude is a very large number. It is the number of angstroms from Earth to\nAlpha-Centuri. It is the number of electrons in the change in your pocket or purse. And yet that number\n", "page": 17, "type": "text", "section": "Page 17"}
{"text": "\u2014that number at least\u2014is the computational power increase that I have experienced in my own\nlifetime.\nAnd with all that vast change in computational power, what has been the effect on the software I\nwrite? It\u2019s gotten bigger certainly. I used to think 2000 lines was a big program. After all, it was a full\nbox of cards that weighed 10 pounds. Now, however, a program isn\u2019t really big until it exceeds\n100,000 lines.\nThe software has also gotten much more performant. We can do things today that we could scarcely\ndream about in the 1960s. The Forbin Project, The Moon Is a Harsh Mistress, and 2001: A Space\nOdyssey all tried to imagine our current future, but missed the mark rather significantly. They all\nimagined huge machines that gained sentience. What we have instead are impossibly small machines\nthat are still \u2026 just machines.{xx}\nAnd there is one thing more about the software we have now, compared to the software from back\nthen: It\u2019s made of the same stuff. It\u2019s made of if statements, assignment statements, and while loops.\nOh, you might object and say that we\u2019ve got much better languages and superior paradigms. After all,\nwe program in Java, or C#, or Ruby, and we use object-oriented design. True\u2014and yet the code is\nstill just an assemblage of sequence, selection, and iteration, just as it was back in the 1960s and\n1950s.\nWhen you really look closely at the practice of programming computers, you realize that very little\nhas changed in 50 years. The languages have gotten a little better. The tools have gotten fantastically\nbetter. But the basic building blocks of a computer program have not changed.\nIf I took a computer programmer from 1966 forward in time to 2016 and put her1 in front of my\nMacBook running IntelliJ and showed her Java, she might need 24 hours to recover from the shock.\nBut then she would be able to write the code. Java just isn\u2019t that different from C, or even from\nFortran.\nAnd if I transported you back to 1966 and showed you how to write and edit PDP-8 code by punching\npaper tape on a 10 character per second teletype, you might need 24 hours to recover from the\ndisappointment. But then you would be able to write the code. The code just hasn\u2019t changed that\nmuch.\nThat\u2019s the secret: This changelessness of the code is the reason that the rules of software architecture\nare so consistent across system types. The rules of software architecture are the rules of ordering and\nassembling the building blocks of programs. And since those building blocks are universal and\nhaven\u2019t changed, the rules for ordering them are likewise universal and changeless.\nYounger programmers might think this is nonsense. They might insist that everything is new and\ndifferent nowadays, that the rules of the past are past and gone. If that is what they think, they are\nsadly mistaken. The rules have not changed. Despite all the new languages, and all the new\nframeworks, and all the paradigms, the rules are the same now as they were when Alan Turing wrote\nthe first machine code in 1946.\n", "page": 18, "type": "text", "section": "Page 18"}
{"text": "But one thing has changed: Back then, we didn\u2019t know what the rules were. Consequently, we broke\nthem, over and over again. Now, with half a century of experience behind us, we have a grasp of\nthose rules.\nAnd it is those rules\u2014those timeless, changeless, rules\u2014that this book is all about.\nRegister your copy of Clean Architecture on the InformIT site for convenient access to updates\nand/or corrections as they become available. To start the registration process, go to\ninformit.com/register and log in or create an account. Enter the product ISBN (9780134494166)\nand click Submit. Look on the Registered Products tab for an Access Bonus Content link next to\nthis product, and follow that link to access the bonus materials.\n1. And she very likely would be female since, back then, women made up a large fraction of programmers.\n", "page": 19, "type": "text", "section": "Page 19"}
{"text": "ACKNOWLEDGMENTS\nThe people who played a part in the creation of this book\u2014in no particular order:{xxiii}\nChris Guzikowski\nChris Zahn\nMatt Heuser\nJeff Overbey\nMicah Martin\nJustin Martin\nCarl Hickman\nJames Grenning\nSimon Brown\nKevlin Henney\nJason Gorman\nDoug Bradbury\nColin Jones\nGrady Booch\nKent Beck\nMartin Fowler\nAlistair Cockburn\nJames O. Coplien\nTim Conrad\nRichard Lloyd\nKen Finder\nKris Iyer (CK)\nMike Carew\nJerry Fitzpatrick\n", "page": 20, "type": "text", "section": "Page 20"}
{"text": "Jim Newkirk\nEd Thelen\nJoe Mabel\nBill Degnan\nAnd many others too numerous to name.\nIn my final review of this book, as I was reading the chapter on Screaming Architecture, Jim\nWeirich\u2019s bright-eyed smile and melodic laugh echoed through my mind. Godspeed, Jim!\n", "page": 21, "type": "text", "section": "Page 21"}
{"text": "ABOUT THE AUTHOR\nRobert C. Martin (Uncle Bob) has been a programmer since 1970. He is the co-founder of\ncleancoders.com, which offers online video training for software developers, and is the founder of\nUncle Bob Consulting LLC, which offers software consulting, training, and skill development\nservices to major corporations worldwide. He served as the Master Craftsman at 8th Light, Inc., a\nChicago-based software consulting firm. He has published dozens of articles in various trade journals\nand is a regular speaker at international conferences and trade shows. He served three years as the\neditor-in-chief of the C++ Report and served as the first chairman of the Agile Alliance.\nMartin has authored and edited many books, including The Clean Coder, Clean Code, UML for Java\nProgrammers, Agile Software Development, Extreme Programming in Practice, More C++ Gems,\nPattern Languages of Program Design 3, and Designing Object Oriented C++ Applications Using\nthe Booch Method.\n", "page": 22, "type": "text", "section": "Page 22"}
{"text": "I\nINTRODUCTION\nIt doesn\u2019t take a huge amount of knowledge and skill to get a program working. Kids in high school\ndo it all the time. Young men and women in college start billion-dollar businesses based on\nscrabbling together a few lines of PHP or Ruby. Hoards of junior programmers in cube farms around\nthe world slog through massive requirements documents held in huge issue tracking systems to get\ntheir systems to \u201cwork\u201d by the sheer brute force of will. The code they produce may not be pretty; but\nit works. It works because getting something to work\u2014once\u2014just isn\u2019t that hard.\nGetting it right is another matter entirely. Getting software right is hard. It takes knowledge and skills\nthat most young programmers haven\u2019t yet acquired. It requires thought and insight that most\nprogrammers don\u2019t take the time to develop. It requires a level of discipline and dedication that most\nprogrammers never dreamed they\u2019d need. Mostly, it takes a passion for the craft and the desire to be a\nprofessional.\nAnd when you get software right, something magical happens: You don\u2019t need hordes of programmers\nto keep it working. You don\u2019t need massive requirements documents and huge issue tracking systems.\nYou don\u2019t need global cube farms and 24/7 programming.\nWhen software is done right, it requires a fraction of the human resources to create and maintain.\nChanges are simple and rapid. Defects are few and far between. Effort is minimized, and functionality\nand flexibility are maximized.\nYes, this vision sounds a bit utopian. But I\u2019ve been there; I\u2019ve seen it happen. I\u2019ve worked in projects\nwhere the design and architecture of the system made it easy to write and easy to maintain. I\u2019ve\nexperienced projects that required a fraction of the anticipated human resources. I\u2019ve worked on\nsystems that had extremely low defect rates. I\u2019ve seen the extraordinary effect that good software\narchitecture can have on a system, a project, and a team. I\u2019ve been to the promised land.\nBut don\u2019t take my word for it. Look at your own experience. Have you experienced the opposite?\nHave you worked on systems that are so interconnected and intricately coupled that every change,\nregardless of how trivial, takes weeks and involves huge risks? Have you experienced the impedance\nof bad code and rotten design? Has the design of the systems you\u2019ve worked on had a huge negative\neffect on the morale of the team, the trust of the customers, and the patience of the managers? Have\nyou seen teams, departments, and even companies that have been brought down by the rotten structure\nof their software? Have you been to programming hell?\n", "page": 23, "type": "text", "section": "Page 23"}
{"text": "I have\u2014and to some extent, most of the rest of us have, too. It is far more common to fight your way\nthrough terrible software designs than it is to enjoy the pleasure of working with a good one.\n", "page": 24, "type": "text", "section": "Page 24"}
{"text": "1\nWHAT IS DESIGN AND ARCHITECTURE?\nThere has been a lot of confusion about design and architecture over the years. What is design? What\nis architecture? What are the differences between the two?\nOne of the goals of this book is to cut through all that confusion and to define, once and for all, what\ndesign and architecture are. For starters, I\u2019ll assert that there is no difference between them. None at\nall.\nThe word \u201carchitecture\u201d is often used in the context of something at a high level that is divorced from\nthe lower-level details, whereas \u201cdesign\u201d more often seems to imply structures and decisions at a\nlower level. But this usage is nonsensical when you look at what a real architect does.\nConsider the architect who designed my new home. Does this home have an architecture? Of course it\ndoes. And what is that architecture? Well, it is the shape of the home, the outward appearance, the\nelevations, and the layout of the spaces and rooms. But as I look through the diagrams that my\narchitect produced, I see an immense number of low-level details. I see where every outlet, light\nswitch, and light will be placed. I see which switches control which lights. I see where the furnace is\nplaced, and the size and placement of the water heater and the sump pump. I see detailed depictions\nof how the walls, roofs, and foundations will be constructed.\nIn short, I see all the little details that support all the high-level decisions. I also see that those low-\nlevel details and high-level decisions are part of the whole design of the house.\n", "page": 25, "type": "text", "section": "Page 25"}
{"text": "And so it is with software design. The low-level details and the high-level structure are all part of the\nsame whole. They form a continuous fabric that defines the shape of the system. You can\u2019t have one\nwithout the other; indeed, no clear dividing line separates them. There is simply a continuum of\ndecisions from the highest to the lowest levels.\nTHE GOAL?\nAnd the goal of those decisions? The goal of good software design? That goal is nothing less than my\nutopian description:\nThe goal of software architecture is to minimize the human resources required to build and maintain the required\nsystem.\nThe measure of design quality is simply the measure of the effort required to meet the needs of the\ncustomer. If that effort is low, and stays low throughout the lifetime of the system, the design is good.\nIf that effort grows with each new release, the design is bad. It\u2019s as simple as that.\nCASE STUDY\nAs an example, consider the following case study. It includes real data from a real company that\nwishes to remain anonymous.\nFirst, let\u2019s look at the growth of the engineering staff. I\u2019m sure you\u2019ll agree that this trend is very\nencouraging. Growth like that shown in Figure 1.1 must be an indication of significant success!\n", "page": 26, "type": "text", "section": "Page 26"}
{"text": "Figure 1.1 Growth of the engineering staff\nReproduced with permission from a slide presentation by Jason Gorman\nNow let\u2019s look at the company\u2019s productivity over the same time period, as measured by simple lines\nof code (Figure 1.2).\nFigure 1.2 Productivity over the same period of time\nClearly something is going wrong here. Even though every release is supported by an ever-increasing\nnumber of developers, the growth of the code looks like it is approaching an asymptote.\n", "page": 27, "type": "text", "section": "Page 27"}
{"text": "Now here\u2019s the really scary graph: Figure 1.3 shows how the cost per line of code has changed over\ntime.\nThese trends aren\u2019t sustainable. It doesn\u2019t matter how profitable the company might be at the moment:\nThose curves will catastrophically drain the profit from the business model and drive the company\ninto a stall, if not into a downright collapse.\nWhat caused this remarkable change in productivity? Why was the code 40 times more expensive to\nproduce in release 8 as opposed to release 1?\nFigure 1.3 Cost per line of code over time\nTHE SIGNATURE OF A MESS\nWhat you are looking at is the signature of a mess. When systems are thrown together in a hurry, when\nthe sheer number of programmers is the sole driver of output, and when little or no thought is given to\nthe cleanliness of the code or the structure of the design, then you can bank on riding this curve to its\nugly end.\nFigure 1.4 shows what this curve looks like to the developers. They started out at nearly 100%\nproductivity, but with each release their productivity declined. By the fourth release, it was clear that\ntheir productivity was going to bottom out in an asymptotic approach to zero.\n", "page": 28, "type": "text", "section": "Page 28"}
{"text": "Figure 1.4 Productivity by release\nFrom the developers\u2019 point of view, this is tremendously frustrating, because everyone is working\nhard. Nobody has decreased their effort.\nAnd yet, despite all their heroics, overtime, and dedication, they simply aren\u2019t getting much of\nanything done anymore. All their effort has been diverted away from features and is now consumed\nwith managing the mess. Their job, such as it is, has changed into moving the mess from one place to\nthe next, and the next, and the next, so that they can add one more meager little feature.\nTHE EXECUTIVE VIEW\nIf you think that\u2019s bad, imagine what this picture looks like to the executives! Consider Figure 1.5,\nwhich depicts monthly development payroll for the same period.\nFigure 1.5 Monthly development payroll by release\nRelease 1 was delivered with a monthly payroll of a few hundred thousand dollars. The second\nrelease cost a few hundred thousand more. By the eighth release monthly payroll was $20 million,\n", "page": 29, "type": "text", "section": "Page 29"}
{"text": "and climbing.\nJust this chart alone is scary. Clearly something startling is happening. One hopes that revenues are\noutpacing costs and therefore justifying the expense. But no matter how you look at this curve, it\u2019s\ncause for concern.\nBut now compare the curve in Figure 1.5 with the lines of code written per release in Figure 1.2. That\ninitial few hundred thousand dollars per month bought a lot of functionality\u2014but the final $20 million\nbought almost nothing! Any CFO would look at these two graphs and know that immediate action is\nnecessary to stave off disaster.\nBut which action can be taken? What has gone wrong? What has caused this incredible decline in\nproductivity? What can executives do, other than to stamp their feet and rage at the developers?\nWHAT WENT WRONG?\nNearly 2600 years ago, Aesop told the story of the Tortoise and the Hare. The moral of that story has\nbeen stated many times in many different ways:\n\u2022 \u201cSlow and steady wins the race.\u201d\n\u2022 \u201cThe race is not to the swift, nor the battle to the strong.\u201d\n\u2022 \u201cThe more haste, the less speed.\u201d\nThe story itself illustrates the foolishness of overconfidence. The Hare, so confident in its intrinsic\nspeed, does not take the race seriously, and so naps while the Tortoise crosses the finish line.\nModern developers are in a similar race, and exhibit a similar overconfidence. Oh, they don\u2019t sleep\n\u2014far from it. Most modern developers work their butts off. But a part of their brain does sleep\u2014the\npart that knows that good, clean, well-designed code matters.\nThese developers buy into a familiar lie: \u201cWe can clean it up later; we just have to get to market\nfirst!\u201d Of course, things never do get cleaned up later, because market pressures never abate. Getting\nto market first simply means that you\u2019ve now got a horde of competitors on your tail, and you have to\nstay ahead of them by running as fast as you can.\nAnd so the developers never switch modes. They can\u2019t go back and clean things up because they\u2019ve\ngot to get the next feature done, and the next, and the next, and the next. And so the mess builds, and\nproductivity continues its asymptotic approach toward zero.\nJust as the Hare was overconfident in its speed, so the developers are overconfident in their ability to\nremain productive. But the creeping mess of code that saps their productivity never sleeps and never\nrelents. If given its way, it will reduce productivity to zero in a matter of months.\nThe bigger lie that developers buy into is the notion that writing messy code makes them go fast in the\nshort term, and just slows them down in the long term. Developers who accept this lie exhibit the\nhare\u2019s overconfidence in their ability to switch modes from making messes to cleaning up messes\n", "page": 30, "type": "text", "section": "Page 30"}
{"text": "sometime in the future, but they also make a simple error of fact. The fact is that making messes is\nalways slower than staying clean, no matter which time scale you are using.\nConsider the results of a remarkable experiment performed by Jason Gorman depicted in Figure 1.6.\nJason conducted this test over a period of six days. Each day he completed a simple program to\nconvert integers into Roman numerals. He knew his work was complete when his predefined set of\nacceptance tests passed. Each day the task took a little less than 30 minutes. Jason used a well-known\ncleanliness discipline named test-driven development (TDD) on the first, third, and fifth days. On the\nother three days, he wrote the code without that discipline.\nFigure 1.6 Time to completion by iterations and use/non-use of TDD\nFirst, notice the learning curve apparent in Figure 1.6. Work on the latter days is completed more\nquickly than the former days. Notice also that work on the TDD days proceeded approximately 10%\nfaster than work on the non-TDD days, and that even the slowest TDD day was faster than the fastest\nnon-TDD day.\nSome folks might look at that result and think it\u2019s a remarkable outcome. But to those who haven\u2019t\nbeen deluded by the Hare\u2019s overconfidence, the result is expected, because they know this simple\ntruth of software development:\nThe only way to go fast, is to go well.\nAnd that\u2019s the answer to the executive\u2019s dilemma. The only way to reverse the decline in productivity\nand the increase in cost is to get the developers to stop thinking like the overconfident Hare and start\ntaking responsibility for the mess that they\u2019ve made.\nThe developers may think that the answer is to start over from scratch and redesign the whole system\n\u2014but that\u2019s just the Hare talking again. The same overconfidence that led to the mess is now telling\nthem that they can build it better if only they can start the race over. The reality is less rosy:\nTheir overconfidence will drive the redesign into the same mess as the original project.\n", "page": 31, "type": "text", "section": "Page 31"}
{"text": "CONCLUSION\nIn every case, the best option is for the development organization to recognize and avoid its own\noverconfidence and to start taking the quality of its software architecture seriously.\nTo take software architecture seriously, you need to know what good software architecture is. To\nbuild a system with a design and an architecture that minimize effort and maximize productivity, you\nneed to know which attributes of system architecture lead to that end.\nThat\u2019s what this book is about. It describes what good clean architectures and designs look like, so\nthat software developers can build systems that will have long profitable lifetimes.\n", "page": 32, "type": "text", "section": "Page 32"}
{"text": "2\nA TALE OF TWO VALUES\nEvery software system provides two different values to the stakeholders: behavior and structure.\nSoftware developers are responsible for ensuring that both those values remain high. Unfortunately,\nthey often focus on one to the exclusion of the other. Even more unfortunately, they often focus on the\nlesser of the two values, leaving the software system eventually valueless.\nBEHA\nVIOR\nThe first value of software is its behavior. Programmers are hired to make machines behave in a way\nthat makes or saves money for the stakeholders. We do this by helping the stakeholders develop a\nfunctional specification, or requirements document. Then we write the code that causes the\nstakeholder\u2019s machines to satisfy those requirements.\nWhen the machine violates those requirements, programmers get their debuggers out and fix the\nproblem.\nMany programmers believe that is the entirety of their job. They believe their job is to make the\nmachine implement the requirements and to fix any bugs. They are sadly mistaken.\nARCHITECTURE\n", "page": 33, "type": "text", "section": "Page 33"}
{"text": "The second value of software has to do with the word \u201csoftware\u201d\u2014a compound word composed of\n\u201csoft\u201d and \u201cware.\u201d The word \u201cware\u201d means \u201cproduct\u201d; the word \u201csoft\u201d\u2026 Well, that\u2019s where the\nsecond value lies.\nSoftware was invented to be \u201csoft.\u201d It was intended to be a way to easily change the behavior of\nmachines. If we\u2019d wanted the behavior of machines to be hard to change, we would have called it\nhardware.\nTo fulfill its purpose, software must be soft\u2014that is, it must be easy to change. When the stakeholders\nchange their minds about a feature, that change should be simple and easy to make. The difficulty in\nmaking such a change should be proportional only to the scope of the change, and not to the shape of\nthe change.\nIt is this difference between scope and shape that often drives the growth in software development\ncosts. It is the reason that costs grow out of proportion to the size of the requested changes. It is the\nreason that the first year of development is much cheaper than the second, and the second year is much\ncheaper than the third.\nFrom the stakeholders\u2019 point of view, they are simply providing a stream of changes of roughly\nsimilar scope. From the developers\u2019 point of view, the stakeholders are giving them a stream of\njigsaw puzzle pieces that they must fit into a puzzle of ever-increasing complexity. Each new request\nis harder to fit than the last, because the shape of the system does not match the shape of the request.\nI\u2019m using the word \u201cshape\u201d here in a unconventional way, but I think the metaphor is apt. Software\ndevelopers often feel as if they are forced to jam square pegs into round holes.\nThe problem, of course, is the architecture of the system. The more this architecture prefers one shape\nover another, the more likely new features will be harder and harder to fit into that structure.\nTherefore architectures should be as shape agnostic are practical.\nTHE GREATER VALUE\nFunction or architecture? Which of these two provides the greater value? Is it more important for the\nsoftware system to work, or is it more important for the software system to be easy to change?\nIf you ask the business managers, they\u2019ll often say that it\u2019s more important for the software system to\nwork. Developers, in turn, often go along with this attitude. But it\u2019s the wrong attitude. I can prove\nthat it is wrong with the simple logical tool of examining the extremes.\n\u2022 If you give me a program that works perfectly but is impossible to change, then it won\u2019t work\nwhen the requirements change, and I won\u2019t be able to make it work. Therefore the program will\nbecome useless.\n\u2022 If you give me a program that does not work but is easy to change, then I can make it work, and\nkeep it working as requirements change. Therefore the program will remain continually useful.\n", "page": 34, "type": "text", "section": "Page 34"}
{"text": "You may not find this argument convincing. After all, there\u2019s no such thing as a program that is\nimpossible to change. However, there are systems that are practically impossible to change, because\nthe cost of change exceeds the benefit of change. Many systems reach that point in some of their\nfeatures or configurations.\nIf you ask the business managers if they want to be able to make changes, they\u2019ll say that of course\nthey do, but may then qualify their answer by noting that the current functionality is more important\nthan any later flexibility. In contrast, if the business managers ask you for a change, and your estimated\ncosts for that change are unaffordably high, the business managers will likely be furious that you\nallowed the system to get to the point where the change was impractical.\nEISENHOWER\u2019S MATRIX\nConsider President Dwight D. Eisenhower\u2019s matrix of importance versus urgency (Figure 2.1). Of\nthis matrix, Eisenhower said:\nI have two kinds of problems, the urgent and the important. The urgent are not important, and the important are never\nurgent.1\nFigure 2.1 Eisenhower matrix\nThere is a great deal of truth to this old adage. Those things that are urgent are rarely of great\nimportance, and those things that are important are seldom of great urgency.\nThe first value of software\u2014behavior\u2014is urgent but not always particularly important.\nThe second value of software\u2014architecture\u2014is important but never particularly urgent.\nOf course, some things are both urgent and important. Other things are not urgent and not important.\nUltimately, we can arrange these four couplets into priorities:\n1. Urgent and important\n", "page": 35, "type": "text", "section": "Page 35"}
{"text": "2. Not urgent and important\n3. Urgent and not important\n4. Not urgent and not important\nNote that the architecture of the code\u2014the important stuff\u2014is in the top two positions of this list,\nwhereas the behavior of the code occupies the first and third positions.\nThe mistake that business managers and developers often make is to elevate items in position 3 to\nposition 1. In other words, they fail to separate those features that are urgent but not important from\nthose features that truly are urgent and important. This failure then leads to ignoring the important\narchitecture of the system in favor of the unimportant features of the system.\nThe dilemma for software developers is that business managers are not equipped to evaluate the\nimportance of architecture. That\u2019s what software developers were hired to do. Therefore it is the\nresponsibility of the software development team to assert the importance of architecture over the\nurgency of features.\nFIGHT FOR THE ARCHITECTURE\nFulfilling this responsibility means wading into a fight\u2014or perhaps a better word is \u201cstruggle.\u201d\nFrankly, that\u2019s always the way these things are done. The development team has to struggle for what\nthey believe to be best for the company, and so do the management team, and the marketing team, and\nthe sales team, and the operations team. It\u2019s always a struggle.\nEffective software development teams tackle that struggle head on. They unabashedly squabble with\nall the other stakeholders as equals. Remember, as a software developer, you are a stakeholder. You\nhave a stake in the software that you need to safeguard. That\u2019s part of your role, and part of your duty.\nAnd it\u2019s a big part of why you were hired.\nThis challenge is doubly important if you are a software architect. Software architects are, by virtue\nof their job description, more focused on the structure of the system than on its features and functions.\nArchitects create an architecture that allows those features and functions to be easily developed,\neasily modified, and easily extended.\nJust remember: If architecture comes last, then the system will become ever more costly to develop,\nand eventually change will become practically impossible for part or all of the system. If that is\nallowed to happen, it means the software development team did not fight hard enough for what they\nknew was necessary.\n1. From a speech at Northwestern University in 1954.\n", "page": 36, "type": "text", "section": "Page 36"}
{"text": "II\nSTARTING WITH THE BRICKS: PROGRAMMING\nPARADIGMS\nSoftware architecture begins with the code\u2014and so we will begin our discussion of architecture by\nlooking at what we\u2019ve learned about code since code was first written.\nIn 1938, Alan Turing laid the foundations of what was to become computer programming. He was not\nthe first to conceive of a programmable machine, but he was the first to understand that programs\nwere simply data. By 1945, Turing was writing real programs on real computers in code that we\nwould recognize (if we squinted enough). Those programs used loops, branches, assignment,\nsubroutines, stacks, and other familiar structures. Turing\u2019s language was binary.\nSince those days, a number of revolutions in programming have occurred. One revolution with which\nwe are all very familiar is the revolution of languages. First, in the late 1940s, came assemblers.\nThese \u201clanguages\u201d relieved the programmers of the drudgery of translating their programs into binary.\nIn 1951, Grace Hopper invented A0, the first compiler. In fact, she coined the term compiler. Fortran\nwas invented in 1953 (the year after I was born). What followed was an unceasing flood of new\nprogramming languages\u2014COBOL, PL/1, SNOBOL, C, Pascal, C++, Java, ad infinitum.\nAnother, probably more significant, revolution was in programming paradigms. Paradigms are ways\nof programming, relatively unrelated to languages. A paradigm tells you which programming\nstructures to use, and when to use them. To date, there have been three such paradigms. For reasons\nwe shall discuss later, there are unlikely to be any others.\n", "page": 37, "type": "text", "section": "Page 37"}
{"text": "3\nPARADIGM OVERVIEW\nThe three paradigms included in this overview chapter are structured programming, object-orient\nprogramming, and functional programming.\nSTRUCTURED PROGRAMMING\nThe first paradigm to be adopted (but not the first to be invented) was structured programming, which\nwas discovered by Edsger Wybe Dijkstra in 1968. Dijkstra showed that the use of unrestrained jumps\n(goto statements) is harmful to program structure. As we\u2019ll see in the chapters that follow, he\nreplaced those jumps with the more familiar if/then/else and do/while/until constructs.\nWe can summarize the structured programming paradigm as follows:\nStructured programming imposes discipline on direct transfer of control.\nOBJECT-ORIENTED PROGRAMMING\nThe second paradigm to be adopted was actually discovered two years earlier, in 1966, by Ole Johan\nDahl and Kristen Nygaard. These two programmers noticed that the function call stack frame in the\nALGOL language could be moved to a heap, thereby allowing local variables declared by a function to\n", "page": 38, "type": "text", "section": "Page 38"}
{"text": "exist long after the function returned. The function became a constructor for a class, the local\nvariables became instance variables, and the nested functions became methods. This led inevitably to\nthe discovery of polymorphism through the disciplined use of function pointers.\nWe can summarize the object-oriented programming paradigm as follows:\nObject-oriented programming imposes discipline on indirect transfer of control.\nFUNCTIONAL PROGRAMMING\nThe third paradigm, which has only recently begun to be adopted, was the first to be invented. Indeed,\nits invention predates computer programming itself. Functional programming is the direct result of the\nwork of Alonzo Church, who in 1936 invented l-calculus while pursuing the same mathematical\nproblem that was motivating Alan Turing at the same time. His l-calculus is the foundation of the LISP\nlanguage, invented in 1958 by John McCarthy. A foundational notion of l-calculus is immutability\u2014\nthat is, the notion that the values of symbols do not change. This effectively means that a functional\nlanguage has no assignment statement. Most functional languages do, in fact, have some means to alter\nthe value of a variable, but only under very strict discipline.\nWe can summarize the functional programming paradigm as follows:\nFunctional programming imposes discipline upon assignment.\nFOOD FOR THOUGHT\nNotice the pattern that I\u2019ve quite deliberately set up in introducing these three programming\nparadigms: Each of the paradigms removes capabilities from the programmer. None of them adds new\ncapabilities. Each imposes some kind of extra discipline that is negative in its intent. The paradigms\ntell us what not to do, more than they tell us what to do.\nAnother way to look at this issue is to recognize that each paradigm takes something away from us.\nThe three paradigms together remove goto statements, function pointers, and assignment. Is there\nanything left to take away?\nProbably not. Thus these three paradigms are likely to be the only three we will see\u2014at least the only\nthree that are negative. Further evidence that there are no more such paradigms is that they were all\ndiscovered within the ten years between 1958 and 1968. In the many decades that have followed, no\nnew paradigms have been added.\nCONCLUSION\nWhat does this history lesson on paradigms have to do with architecture? Everything. We use\npolymorphism as the mechanism to cross architectural boundaries; we use functional programming to\n", "page": 39, "type": "text", "section": "Page 39"}
{"text": "impose discipline on the location of and access to data; and we use structured programming as the\nalgorithmic foundation of our modules.\nNotice how well those three align with the three big concerns of architecture: function, separation of\ncomponents, and data management.\n", "page": 40, "type": "text", "section": "Page 40"}
{"text": "4\nSTRUCTURED PROGRAMMING\nEdsger Wybe Dijkstra was born in Rotterdam in 1930. He survived the bombing of Rotterdam during\nWorld War II, along with the German occupation of the Netherlands, and in 1948 graduated from high\nschool with the highest possible marks in math, physics, chemistry, and biology. In March 1952, at the\nage of 21 (and just 9 months before I was born), Dijkstra took a job with the Mathematical Center of\nAmsterdam as the Netherlands\u2019 very first programmer.\nIn 1955, having been a programmer for three years, and while still a student, Dijkstra concluded that\nthe intellectual challenge of programming was greater than the intellectual challenge of theoretical\nphysics. As a result, he chose programming as his long-term career.\nIn 1957, Dijkstra married Maria Debets. At the time, you had to state your profession as part of the\nmarriage rites in the Netherlands. The Dutch authorities were unwilling to accept \u201cprogrammer\u201d as\nDijkstra\u2019s profession; they had never heard of such a profession. To satisfy them, Dijkstra settled for\n\u201ctheoretical physicist\u201d as his job title.\nAs part of deciding to make programming his career, Dijkstra conferred with his boss, Adriaan van\nWijngaarden. Dijkstra was concerned that no one had identified a discipline, or science, of\nprogramming, and that he would therefore not be taken seriously. His boss replied that Dijkstra might\nvery well be one of the people who would discover such disciplines, thereby evolving software into\na science.\nDijkstra started his career in the era of vacuum tubes, when computers were huge, fragile, slow,\n", "page": 41, "type": "text", "section": "Page 41"}
{"text": "unreliable, and (by today\u2019s standards) extremely limited. In those early years, programs were written\nin binary, or in very crude assembly language. Input took the physical form of paper tape or punched\ncards. The edit/compile/test loop was hours\u2014if not days\u2014long.\nIt was in this primitive environment that Dijkstra made his great discoveries.\nPROOF\nThe problem that Dijkstra recognized, early on, was that programming is hard, and that programmers\ndon\u2019t do it very well. A program of any complexity contains too many details for a human brain to\nmanage without help. Overlooking just one small detail results in programs that may seem to work,\nbut fail in surprising ways.\nDijkstra\u2019s solution was to apply the mathematical discipline of proof. His vision was the construction\nof a Euclidian hierarchy of postulates, theorems, corollaries, and lemmas. Dijkstra thought that\nprogrammers could use that hierarchy the way mathematicians do. In other words, programmers\nwould use proven structures, and tie them together with code that they would then prove correct\nthemselves.\nOf course, to get this going, Dijkstra realized that he would have to demonstrate the technique for\nwriting basic proofs of simple algorithms. This he found to be quite challenging.\nDuring his investigation, Dijkstra discovered that certain uses of goto statements prevent modules\nfrom being decomposed recursively into smaller and smaller units, thereby preventing use of the\ndivide-and-conquer approach necessary for reasonable proofs.\nOther uses of goto, however, did not have this problem. Dijkstra realized that these \u201cgood\u201d uses of\ngoto corresponded to simple selection and iteration control structures such as if/then/else and\ndo/while. Modules that used only those kinds of control structures could be recursively subdivided\ninto provable units.\nDijkstra knew that those control structures, when combined with sequential execution, were special.\nThey had been identified two years before by B\u00f6hm and Jacopini, who proved that all programs can\nbe constructed from just three structures: sequence, selection, and iteration.\nThis discovery was remarkable: The very control structures that made a module provable were the\nsame minimum set of control structures from which all programs can be built. Thus structured\nprogramming was born.\nDijkstra showed that sequential statements could be proved correct through simple enumeration. The\ntechnique mathematically traced the inputs of the sequence to the outputs of the sequence. This\napproach was no different from any normal mathematical proof.\nDijkstra tackled selection through reapplication of enumeration. Each path through the selection was\nenumerated. If both paths eventually produced appropriate mathematical results, then the proof was\n", "page": 42, "type": "text", "section": "Page 42"}
{"text": "solid.\nIteration was a bit different. To prove an iteration correct, Dijkstra had to use induction. He proved\nthe case for 1 by enumeration. Then he proved the case that if N was assumed correct, N + 1 was\ncorrect, again by enumeration. He also proved the starting and ending criteria of the iteration by\nenumeration.\nSuch proofs were laborious and complex\u2014but they were proofs. With their development, the idea\nthat a Euclidean hierarchy of theorems could be constructed seemed reachable.\nA HARMFUL PROCLAMATION\nIn 1968, Dijkstra wrote a letter to the editor of CACM, which was published in the March issue. The\ntitle of this letter was \u201cGo To Statement Considered Harmful.\u201d The article outlined his position on the\nthree control structures.\nAnd the programming world caught fire. Back then we didn\u2019t have an Internet, so people couldn\u2019t\npost nasty memes of Dijkstra, and they couldn\u2019t flame him online. But they could, and they did, write\nletters to the editors of many published journals.\nThose letters weren\u2019t necessarily all polite. Some were intensely negative; others voiced strong\nsupport for his position. And so the battle was joined, ultimately to last about a decade.\nEventually the argument petered out. The reason was simple: Dijkstra had won. As computer\nlanguages evolved, the goto statement moved ever rearward, until it all but disappeared. Most\nmodern languages do not have a goto statement\u2014and, of course, LISP never did.\nNowadays we are all structured programmers, though not necessarily by choice. It\u2019s just that our\nlanguages don\u2019t give us the option to use undisciplined direct transfer of control.\nSome may point to named breaks in Java or exceptions as goto analogs. In fact, these structures are\nnot the utterly unrestricted transfers of control that older languages like Fortran or COBOL once had.\nIndeed, even languages that still support the goto keyword often restrict the target to within the scope\nof the current function.\nFUNCTIONAL DECOMPOSITION\nStructured programming allows modules to be recursively decomposed into provable units, which in\nturn means that modules can be functionally decomposed. That is, you can take a large-scale problem\nstatement and decompose it into high-level functions. Each of those functions can then be decomposed\ninto lower-level functions, ad infinitum. Moreover, each of those decomposed functions can be\nrepresented using the restricted control structures of structured programming.\nBuilding on this foundation, disciplines such as structured analysis and structured design became\n", "page": 43, "type": "text", "section": "Page 43"}
{"text": "popular in the late 1970s and throughout the 1980s. Men like Ed Yourdon, Larry Constantine, Tom\nDeMarco, and Meilir Page-Jones promoted and popularized these techniques throughout that period.\nBy following these disciplines, programmers could break down large proposed systems into modules\nand components that could be further broken down into tiny provable functions.\nNO FORMAL PROOFS\nBut the proofs never came. The Euclidean hierarchy of theorems was never built. And programmers\nat large never saw the benefits of working through the laborious process of formally proving each and\nevery little function correct. In the end, Dijkstra\u2019s dream faded and died. Few of today\u2019s programmers\nbelieve that formal proofs are an appropriate way to produce high-quality software.\nOf course, formal, Euclidian style, mathematical proofs are not the only strategy for proving\nsomething correct. Another highly successful strategy is the scientific method.\nSCIENCE TO THE RESCUE\nScience is fundamentally different from mathematics, in that scientific theories and laws cannot be\nproven correct. I cannot prove to you that Newton\u2019s second law of motion, F = ma, or law of gravity,\nF = Gm1m2/r2, are correct. I can demonstrate these laws to you, and I can make measurements that\nshow them correct to many decimal places, but I cannot prove them in the sense of a mathematical\nproof. No matter how many experiments I conduct or how much empirical evidence I gather, there is\nalways the chance that some experiment will show that those laws of motion and gravity are\nincorrect.\nThat is the nature of scientific theories and laws: They are falsifiable but not provable.\nAnd yet we bet our lives on these laws every day. Every time you get into a car, you bet your life that\nF = ma is a reliable description of the way the world works. Every time you take a step, you bet your\nhealth and safety that F = Gm1m2/r2 is correct.\nScience does not work by proving statements true, but rather by proving statements false. Those\nstatements that we cannot prove false, after much effort, we deem to be true enough for our purposes.\nOf course, not all statements are provable. The statement \u201cThis is a lie\u201d is neither true nor false. It is\none of the simplest examples of a statement that is not provable.\nUltimately, we can say that mathematics is the discipline of proving provable statements true.\nScience, in contrast, is the discipline of proving provable statements false.\nTESTS\n", "page": 44, "type": "text", "section": "Page 44"}
{"text": "Dijkstra once said, \u201cTesting shows the presence, not the absence, of bugs.\u201d In other words, a program\ncan be proven incorrect by a test, but it cannot be proven correct. All that tests can do, after sufficient\ntesting effort, is allow us to deem a program to be correct enough for our purposes.\nThe implications of this fact are stunning. Software development is not a mathematical endeavor,\neven though it seems to manipulate mathematical constructs. Rather, software is like a science. We\nshow correctness by failing to prove incorrectness, despite our best efforts.\nSuch proofs of incorrectness can be applied only to provable programs. A program that is not\nprovable\u2014due to unrestrained use of goto, for example\u2014cannot be deemed correct no matter how\nmany tests are applied to it.\nStructured programming forces us to recursively decompose a program into a set of small provable\nfunctions. We can then use tests to try to prove those small provable functions incorrect. If such tests\nfail to prove incorrectness, then we deem the functions to be correct enough for our purposes.\nCONCLUSION\nIt is this ability to create falsifiable units of programming that makes structured programming valuable\ntoday. This is the reason that modern languages do not typically support unrestrained goto statements.\nMoreover, at the architectural level, this is why we still consider functional decomposition to be one\nof our best practices.\nAt every level, from the smallest function to the largest component, software is like a science and,\ntherefore, is driven by falsifiability. Software architects strive to define modules, components, and\nservices that are easily falsifiable (testable). To do so, they employ restrictive disciplines similar to\nstructured programming, albeit at a much higher level.\nIt is those restrictive disciplines that we will study in some detail in the chapters to come.\n", "page": 45, "type": "text", "section": "Page 45"}
{"text": "5\nOBJECT-ORIENTED PROGRAMMING\nAs we will see, the basis of a good architecture is the understanding and application of the principles\nof object-oriented design (OO). But just what is OO?\nOne answer to this question is \u201cThe combination of data and function.\u201d Although often cited, this is a\nvery unsatisfying answer because it implies that o.f() is somehow different from f(o). This is\nabsurd. Programmers were passing data structures into functions long before 1966, when Dahl and\nNygaard moved the function call stack frame to the heap and invented OO.\nAnother common answer to this question is \u201cA way to model the real world.\u201d This is an evasive\nanswer at best. What does \u201cmodeling the real world\u201d actually mean, and why is it something we\nwould want to do? Perhaps this statement is intended to imply that OO makes software easier to\nunderstand because it has a closer relationship to the real world\u2014but even that statement is evasive\nand too loosely defined. It does not tell us what OO is.\nSome folks fall back on three magic words to explain the nature of OO: encapsulation, inheritance,\nand polymorphism. The implication is that OO is the proper admixture of these three things, or at\nleast that an OO language must support these three things.\nLet\u2019s examine each of these concepts in turn.\n", "page": 46, "type": "text", "section": "Page 46"}
{"text": "ENCAPSULATION?\nThe reason encapsulation is cited as part of the definition of OO is that OO languages provide easy\nand effective encapsulation of data and function. As a result, a line can be drawn around a cohesive\nset of data and functions. Outside of that line, the data is hidden and only some of the functions are\nknown. We see this concept in action as the private data members and the public member functions of\na class.\nThis idea is certainly not unique to OO. Indeed, we had perfect encapsulation in C. Consider this\nsimple C program:\nClick here to view code image\npoint.h\nstruct Point;\nstruct Point* makePoint(double x, double y);\ndouble distance (struct Point *p1, struct Point *p2);\nClick here to view code image\npoint.c\n#include \"point.h\"\n#include <stdlib.h>\n#include <math.h>\n \nstruct Point {\n  double x,y;\n};\n \nstruct Point* makepoint(double x, double y) {\n  struct Point* p = malloc(sizeof(struct Point));\n  p->x = x;\n  p->y = y;\n  return p;\n}\n \ndouble distance(struct Point* p1, struct Point* p2) {\n  double dx = p1->x - p2->x;\n  double dy = p1->y - p2->y;\n  return sqrt(dx*dx+dy*dy);\n}\nThe users of point.h have no access whatsoever to the members of struct Point. They can call\nthe makePoint() function, and the distance() function, but they have absolutely no knowledge of\nthe implementation of either the Point data structure or the functions.\nThis is perfect encapsulation\u2014in a non-OO language. C programmers used to do this kind of thing all\nthe time. We would forward declare data structures and functions in header files, and then implement\nthem in implementation files. Our users never had access to the elements in those implementation\nfiles.\n", "page": 47, "type": "text", "section": "Page 47"}
{"text": "But then came OO in the form of C++\u2014and the perfect encapsulation of C was broken.\nThe C++ compiler, for technical reasons,1 needed the member variables of a class to be declared in\nthe header file of that class. So our Point program changed to look like this:\nClick here to view code image\npoint.h\nclass Point {\npublic:\n  Point(double x, double y);\n  double distance(const Point& p) const;\n \nprivate:\n  double x;\n  double y;\n};\nClick here to view code image\npoint.cc\n#include \"point.h\"\n#include <math.h>\n \nPoint::Point(double x, double y)\n: x(x), y(y)\n{}\ndouble Point::distance(const Point& p) const {\n  double dx = x-p.x;\n  double dy = y-p.y;\n  return sqrt(dx*dx + dy*dy);\n}\nClients of the header file point.h know about the member variables x and y! The compiler will\nprevent access to them, but the client still knows they exist. For example, if those member names are\nchanged, the point.cc file must be recompiled! Encapsulation has been broken.\nIndeed, the way encapsulation is partially repaired is by introducing the public, private, and\nprotected keywords into the language. This, however, was a hack necessitated by the technical need\nfor the compiler to see those variables in the header file.\nJava and C# simply abolished the header/implementation split altogether, thereby weakening\nencapsulation even more. In these languages, it is impossible to separate the declaration and\ndefinition of a class.\nFor these reasons, it is difficult to accept that OO depends on strong encapsulation. Indeed, many OO\nlanguages2 have little or no enforced encapsulation.\nOO certainly does depend on the idea that programmers are well-behaved enough to not circumvent\n", "page": 48, "type": "text", "section": "Page 48"}
{"text": "encapsulated data. Even so, the languages that claim to provide OO have only weakened the once\nperfect encapsulation we enjoyed with C.\nINHERITANCE?\nIf OO languages did not give us better encapsulation, then they certainly gave us inheritance.\nWell\u2014sort of. Inheritance is simply the redeclaration of a group of variables and functions within an\nenclosing scope. This is something C programmers3 were able to do manually long before there was\nan OO language.\nConsider this addition to our original point.h C program:\nClick here to view code image\nnamedPoint.h\nstruct NamedPoint;\n \nstruct NamedPoint* makeNamedPoint(double x, double y, char* name);\nvoid setName(struct NamedPoint* np, char* name);\nchar* getName(struct NamedPoint* np);\nClick here to view code image\nnamedPoint.c\n#include \"namedPoint.h\"\n#include <stdlib.h>\n \nstruct NamedPoint {\n  double x,y;\n  char* name;\n};\n \nstruct NamedPoint* makeNamedPoint(double x, double y, char* name) {\n  struct NamedPoint* p = malloc(sizeof(struct NamedPoint));\n  p->x = x;\n  p->y = y;\n  p->name = name;\n  return p;\n}\n \nvoid setName(struct NamedPoint* np, char* name) {\n  np->name = name;\n}\n \nchar* getName(struct NamedPoint* np) {\n  return np->name;\n}\nClick here to view code image\nmain.c\n", "page": 49, "type": "text", "section": "Page 49"}
{"text": "#include \"point.h\"\n#include \"namedPoint.h\"\n#include <stdio.h>\n \nint main(int ac, char** av) {\n  struct NamedPoint* origin = makeNamedPoint(0.0, 0.0, \"origin\");\n  struct NamedPoint* upperRight = makeNamedPoint (1.0, 1.0, \"upperRight\");\n  printf(\"distance=%f\\n\",\n    distance(\n             (struct Point*) origin, \n             (struct Point*) upperRight));\n}\nIf you look carefully at the main program, you\u2019ll see that the NamedPoint data structure acts as though\nit is a derivative of the Point data structure. This is because the order of the first two fields in\nNamedPoint is the same as Point. In short, NamedPoint can masquerade as Point because\nNamedPoint is a pure superset of Point and maintains the ordering of the members that correspond\nto Point.\nThis kind of trickery was a common practice4 of programmers prior to the advent of OO. In fact, such\ntrickery is how C++ implements single inheritance.\nThus we might say that we had a kind of inheritance long before OO languages were invented. That\nstatement wouldn\u2019t quite be true, though. We had a trick, but it\u2019s not nearly as convenient as true\ninheritance. Moreover, multiple inheritance is a considerably more difficult to achieve by such\ntrickery.\nNote also that in main.c, I was forced to cast the NamedPoint arguments to Point. In a real OO\nlanguage, such upcasting would be implicit.\nIt\u2019s fair to say that while OO languages did not give us something completely brand new, it did make\nthe masquerading of data structures significantly more convenient.\nTo recap: We can award no point to OO for encapsulation, and perhaps a half-point for inheritance.\nSo far, that\u2019s not such a great score.\nBut there\u2019s one more attribute to consider.\nPOLYMORPHISM?\nDid we have polymorphic behavior before OO languages? Of course we did. Consider this simple C\ncopy program.\nClick here to view code image\n#include <stdio.h>\nvoid copy() {\n  int c;\n", "page": 50, "type": "text", "section": "Page 50"}
{"text": "  while ((c=getchar()) != EOF)\n    putchar(c);\n}\nThe function getchar() reads from STDIN. But which device is STDIN? The putchar() function\nwrites to STDOUT. But which device is that? These functions are polymorphic\u2014their behavior\ndepends on the type of STDIN and STDOUT.\nIt\u2019s as though STDIN and STDOUT are Java-style interfaces that have implementations for each device.\nOf course, there are no interfaces in the example C program\u2014so how does the call to getchar()\nactually get delivered to the device driver that reads the character?\nThe answer to that question is pretty straightforward. The UNIX operating system requires that every\nIO device driver provide five standard functions:5 open, close, read, write, and seek. The\nsignatures of those functions must be identical for every IO driver.\nThe FILE data structure contains five pointers to functions. In our example, it might look like this:\nClick here to view code image\nstruct FILE {\n  void (*open)(char* name, int mode);\n  void (*close)();\n  int (*read)();\n  void (*write)(char);\n  void (*seek)(long index, int mode);\n};\nThe IO driver for the console will define those functions and load up a FILE data structure with their\naddresses\u2014something like this:\nClick here to view code image\n#include \"file.h\"\n \nvoid open(char* name, int mode) {/*...*/}\nvoid close() {/*...*/};\nint read() {int c;/*...*/ return c;}\nvoid write(char c) {/*...*/}\nvoid seek(long index, int mode) {/*...*/}\n \nstruct FILE console = {open, close, read, write, seek};\nNow if STDIN is defined as a FILE*, and if it points to the console data structure, then getchar()\nmight be implemented this way:\nClick here to view code image\nextern struct FILE* STDIN;\n \nint getchar() {\n  return STDIN->read();\n", "page": 51, "type": "text", "section": "Page 51"}
{"text": "}\nIn other words, getchar() simply calls the function pointed to by the read pointer of the FILE data\nstructure pointed to by STDIN.\nThis simple trick is the basis for all polymorphism in OO. In C++, for example, every virtual function\nwithin a class has a pointer in a table called a vtable, and all calls to virtual functions go through\nthat table. Constructors of derivatives simply load their versions of those functions into the vtable of\nthe object being created.\nThe bottom line is that polymorphism is an application of pointers to functions. Programmers have\nbeen using pointers to functions to achieve polymorphic behavior since V\non Neumann architectures\nwere first implemented in the late 1940s. In other words, OO has provided nothing new.\nAh, but that\u2019s not quite correct. OO languages may not have given us polymorphism, but they have\nmade it much safer and much more convenient.\nThe problem with explicitly using pointers to functions to create polymorphic behavior is that\npointers to functions are dangerous. Such use is driven by a set of manual conventions. You have to\nremember to follow the convention to initialize those pointers. You have to remember to follow the\nconvention to call all your functions through those pointers. If any programmer fails to remember\nthese conventions, the resulting bug can be devilishly hard to track down and eliminate.\nOO languages eliminate these conventions and, therefore, these dangers. Using an OO language makes\npolymorphism trivial. That fact provides an enormous power that old C programmers could only\ndream of. On this basis, we can conclude that OO imposes discipline on indirect transfer of control.\nTHE POWER OF POLYMORPHISM\nWhat\u2019s so great about polymorphism? To better appreciate its charms, let\u2019s reconsider the example\ncopy program. What happens to that program if a new IO device is created? Suppose we want to use\nthe copy program to copy data from a handwriting recognition device to a speech synthesizer device:\nHow do we need to change the copy program to get it to work with those new devices?\nWe don\u2019t need any changes at all! Indeed, we don\u2019t even need to recompile the copy program. Why?\nBecause the source code of the copy program does not depend on the source code of the IO drivers.\nAs long as those IO drivers implement the five standard functions defined by FILE, the copy program\nwill be happy to use them.\nIn short, the IO devices have become plugins to the copy program.\nWhy did the UNIX operating system make IO devices plugins? Because we learned, in the late 1950s,\nthat our programs should be device independent. Why? Because we wrote lots of programs that were\ndevice dependent, only to discover that we really wanted those programs to do the same job but use a\ndifferent device.\n", "page": 52, "type": "text", "section": "Page 52"}
{"text": "For example, we often wrote programs that read input data from decks of cards,6 and then punched\nnew decks of cards as output. Later, our customers stopped giving us decks of cards and started\ngiving us reels of magnetic tape. This was very inconvenient, because it meant rewriting large\nportions of the original program. It would be very convenient if the same program worked\ninterchangeably with cards or tape.\nThe plugin architecture was invented to support this kind of IO device independence, and has been\nimplemented in almost every operating system since its introduction. Even so, most programmers did\nnot extend the idea to their own programs, because using pointers to functions was dangerous.\nOO allows the plugin architecture to be used anywhere, for anything.\nDEPENDENCY INVERSION\nImagine what software was like before a safe and convenient mechanism for polymorphism was\navailable. In the typical calling tree, main functions called high-level functions, which called mid-\nlevel functions, which called low-level functions. In that calling tree, however, source code\ndependencies inexorably followed the flow of control (Figure 5.1).\nFigure 5.1 Source code dependencies versus flow of control\nFor main to call one of the high-level functions, it had to mention the name of the module that\ncontained that function In C, this was a #include. In Java, it was an import statement. In C#, it was\na using statement. Indeed, every caller was forced to mention the name of the module that contained\nthe callee.\nThis requirement presented the software architect with few, if any, options. The flow of control was\ndictated by the behavior of the system, and the source code dependencies were dictated by that flow\nof control.\nWhen polymorphism is brought into play, however, something very different can happen (Figure 5.2).\n", "page": 53, "type": "text", "section": "Page 53"}
{"text": "Figure 5.2 Dependency inversion\nIn Figure 5.2, module HL1 calls the F() function in module ML1. The fact that it calls this function\nthrough an interface is a source code contrivance. At runtime, the interface doesn\u2019t exist. HL1 simply\ncalls F() within ML1.7\nNote, however, that the source code dependency (the inheritance relationship) between ML1 and the\ninterface I points in the opposite direction compared to the flow of control. This is called\ndependency inversion, and its implications for the software architect are profound.\nThe fact that OO languages provide safe and convenient polymorphism means that any source code\ndependency, no matter where it is, can be inverted.\nNow look back at that calling tree in Figure 5.1, and its many source code dependencies. Any of those\nsource code dependencies can be turned around by inserting an interface between them.\nWith this approach, software architects working in systems written in OO languages have absolute\ncontrol over the direction of all source code dependencies in the system. They are not constrained to\nalign those dependencies with the flow of control. No matter which module does the calling and\nwhich module is called, the software architect can point the source code dependency in either\ndirection.\nThat is power! That is the power that OO provides. That\u2019s what OO is really all about\u2014at least from\nthe architect\u2019s point of view.\nWhat can you do with that power? As an example, you can rearrange the source code dependencies of\nyour system so that the database and the user interface (UI) depend on the business rules (Figure 5.3),\nrather than the other way around.\n", "page": 54, "type": "text", "section": "Page 54"}
{"text": "Figure 5.3 The database and the user interface depend on the business rules\nThis means that the UI and the database can be plugins to the business rules. It means that the source\ncode of the business rules never mentions the UI or the database.\nAs a consequence, the business rules, the UI, and the database can be compiled into three separate\ncomponents or deployment units (e.g., jar files, DLLs, or Gem files) that have the same dependencies\nas the source code. The component containing the business rules will not depend on the components\ncontaining the UI and database.\nIn turn, the business rules can be deployed independently of the UI and the database. Changes to the\nUI or the database need not have any effect on the business rules. Those components can be deployed\nseparately and independently.\nIn short, when the source code in a component changes, only that component needs to be redeployed.\nThis is independent deployability.\nIf the modules in your system can be deployed independently, then they can be developed\nindependently by different teams. That\u2019s independent developability.\nCONCLUSION\nWhat is OO? There are many opinions and many answers to this question. To the software architect,\nhowever, the answer is clear: OO is the ability, through the use of polymorphism, to gain absolute\ncontrol over every source code dependency in the system. It allows the architect to create a plugin\narchitecture, in which modules that contain high-level policies are independent of modules that\ncontain low-level details. The low-level details are relegated to plugin modules that can be deployed\nand developed independently from the modules that contain high-level policies.\n1. The C++ compiler needs to know the size of the instances of each class.\n2. For example, Smalltalk, Python, JavaScript, Lua, and Ruby.\n3. Not just C programmers: Most languages of that era had the capability to masquerade one data structure as another.\n4. Indeed it still is.\n5. UNIX systems vary; this is just an example.\n6. Punched cards\u2014IBM Hollerith cards, 80 columns wide. I\u2019m sure many of you have never even seen one of these, but they were\ncommonplace in the 1950s, 1960s, and even 1970s.\n7. Albeit indirectly.\n", "page": 55, "type": "text", "section": "Page 55"}
{"text": "6\nFUNCTIONAL PROGRAMMING\nIn many ways, the concepts of functional programming predate programming itself. This paradigm is\nstrongly based on the l-calculus invented by Alonzo Church in the 1930s.\nSQUARES OF INTEGERS\nTo explain what functional programming is, it\u2019s best to examine some examples. Let\u2019s investigate a\nsimple problem: printing the squares of the first 25 integers.\nIn a language like Java, we might write the following:\nClick here to view code image\npublic class Squint {\n  public static void main(String args[]) {\n    for (int i=0; i<25; i++)\n      System.out.println(i*i);\n  }\n}\nIn a language like Clojure, which is a derivative of Lisp, and is functional, we might implement this\nsame program as follows:\n", "page": 56, "type": "text", "section": "Page 56"}
{"text": "Click here to view code image\n(println (take 25 (map (fn [x] (* x x)) (range))))\nIf you don\u2019t know Lisp, then this might look a little strange. So let me reformat it a bit and add some\ncomments.\nClick here to view code image\n(println ;___________________ Print\n  (take 25 ;_________________ the first 25\n    (map (fn [x] (* x x)) ;__ squares\n      (range)))) ;___________ of Integers\nIt should be clear that println, take, map, and range are all functions. In Lisp, you call a function\nby putting it in parentheses. For example, (range) calls the range function.\nThe expression (fn [x] (* x x)) is an anonymous function that calls the multiply function, passing\nits input argument in twice. In other words, it computes the square of its input.\nLooking at the whole thing again, it\u2019s best to start with the innermost function call.\n\u2022 The range function returns a never-ending list of integers starting with 0.\n\u2022 This list is passed into the map function, which calls the anonymous squaring function on each\nelement, producing a new never-ending list of all the squares.\n\u2022 The list of squares is passed into the take function, which returns a new list with only the first 25\nelements.\n\u2022 The println function prints its input, which is a list of the first 25 squares of integers.\nIf you find yourself terrified by the concept of never-ending lists, don\u2019t worry. Only the first 25\nelements of those never-ending lists are actually created. That\u2019s because no element of a never-ending\nlist is evaluated until it is accessed.\nIf you found all of that confusing, then you can look forward to a glorious time learning all about\nClojure and functional programming. It is not my goal to teach you about these topics here.\nInstead, my goal here is to point out something very dramatic about the difference between the\nClojure and Java programs. The Java program uses a mutable variable\u2014a variable that changes state\nduring the execution of the program. That variable is i\u2014the loop control variable. No such mutable\nvariable exists in the Clojure program. In the Clojure program, variables like x are initialized, but\nthey are never modified.\nThis leads us to a surprising statement: V\nariables in functional languages do not vary.\nIMMUTABILITY AND ARCHITECTURE\n", "page": 57, "type": "text", "section": "Page 57"}
{"text": "Why is this point important as an architectural consideration? Why would an architect be concerned\nwith the mutability of variables? The answer is absurdly simple: All race conditions, deadlock\nconditions, and concurrent update problems are due to mutable variables. You cannot have a race\ncondition or a concurrent update problem if no variable is ever updated. You cannot have deadlocks\nwithout mutable locks.\nIn other words, all the problems that we face in concurrent applications\u2014all the problems we face in\napplications that require multiple threads, and multiple processors\u2014cannot happen if there are no\nmutable variables.\nAs an architect, you should be very interested in issues of concurrency. You want to make sure that the\nsystems you design will be robust in the presence of multiple threads and processors. The question\nyou must be asking yourself, then, is whether immutability is practicable.\nThe answer to that question is affirmative, if you have infinite storage and infinite processor speed.\nLacking those infinite resources, the answer is a bit more nuanced. Yes, immutability can be\npracticable, if certain compromises are made.\nLet\u2019s look at some of those compromises.\nSEGREGATION OF MUTABILITY\nOne of the most common compromises in regard to immutability is to segregate the application, or the\nservices within the application, into mutable and immutable components. The immutable components\nperform their tasks in a purely functional way, without using any mutable variables. The immutable\ncomponents communicate with one or more other components that are not purely functional, and allow\nfor the state of variables to be mutated (Figure 6.1).\nFigure 6.1 Mutating state and transactional memory\nSince mutating state exposes those components to all the problems of concurrency, it is common\n", "page": 58, "type": "text", "section": "Page 58"}
{"text": "practice to use some kind of transactional memory to protect the mutable variables from concurrent\nupdates and race conditions.\nTransactional memory simply treats variables in memory the same way a database treats records on\ndisk.1 It protects those variables with a transaction- or retry-based scheme.\nA simple example of this approach is Clojure\u2019s atom facility:\nClick here to view code image\n(def counter (atom 0)) ; initialize counter to 0\n(swap! counter inc)    ; safely increment counter.\nIn this code, the counter variable is defined as an atom. In Clojure, an atom is a special kind of\nvariable whose value is allowed to mutate under very disciplined conditions that are enforced by the\nswap! function.\nThe swap! function, shown in the preceding code, takes two arguments: the atom to be mutated, and a\nfunction that computes the new value to be stored in the atom. In our example code, the counter atom\nwill be changed to the value computed by the inc function, which simply increments its argument.\nThe strategy used by swap! is a traditional compare and swap algorithm. The value of counter is\nread and passed to inc. When inc returns, the value of counter is locked and compared to the value\nthat was passed to inc. If the value is the same, then the value returned by inc is stored in counter\nand the lock is released. Otherwise, the lock is released, and the strategy is retried from the\nbeginning.\nThe atom facility is adequate for simple applications. Unfortunately, it cannot completely safeguard\nagainst concurrent updates and deadlocks when multiple dependent variables come into play. In those\ninstances, more elaborate facilities can be used.\nThe point is that well-structured applications will be segregated into those components that do not\nmutate variables and those that do. This kind of segregation is supported by the use of appropriate\ndisciplines to protect those mutated variables.\nArchitects would be wise to push as much processing as possible into the immutable components, and\nto drive as much code as possible out of those components that must allow mutation.\nEVENT SOURCING\nThe limits of storage and processing power have been rapidly receding from view. Nowadays it is\ncommon for processors to execute billions of instructions per second and to have billions of bytes of\nRAM. The more memory we have, and the faster our machines are, the less we need mutable state.\nAs a simple example, imagine a banking application that maintains the account balances of its\n", "page": 59, "type": "text", "section": "Page 59"}
{"text": "customers. It mutates those balances when deposit and withdrawal transactions are executed.\nNow imagine that instead of storing the account balances, we store only the transactions. Whenever\nanyone wants to know the balance of an account, we simply add up all the transactions for that\naccount, from the beginning of time. This scheme requires no mutable variables.\nObviously, this approach sounds absurd. Over time, the number of transactions would grow without\nbound, and the processing power required to compute the totals would become intolerable. To make\nthis scheme work forever, we would need infinite storage and infinite processing power.\nBut perhaps we don\u2019t have to make the scheme work forever. And perhaps we have enough storage\nand enough processing power to make the scheme work for the reasonable lifetime of the application.\nThis is the idea behind event sourcing.2 Event sourcing is a strategy wherein we store the\ntransactions, but not the state. When state is required, we simply apply all the transactions from the\nbeginning of time.\nOf course, we can take shortcuts. For example, we can compute and save the state every midnight.\nThen, when the state information is required, we need compute only the transactions since midnight.\nNow consider the data storage required for this scheme: We would need a lot of it. Realistically,\noffline data storage has been growing so fast that we now consider trillions of bytes to be small\u2014so\nwe have a lot of it.\nMore importantly, nothing ever gets deleted or updated from such a data store. As a consequence, our\napplications are not CRUD; they are just CR. Also, because neither updates nor deletions occur in the\ndata store, there cannot be any concurrent update issues.\nIf we have enough storage and enough processor power, we can make our applications entirely\nimmutable\u2014and, therefore, entirely functional.\nIf this still sounds absurd, it might help if you remembered that this is precisely the way your source\ncode control system works.\nCONCLUSION\nTo summarize:\n\u2022 Structured programming is discipline imposed upon direct transfer of control.\n\u2022 Object-oriented programming is discipline imposed upon indirect transfer of control.\n\u2022 Functional programming is discipline imposed upon variable assignment.\nEach of these three paradigms has taken something away from us. Each restricts some aspect of the\nway we write code. None of them has added to our power or our capabilities.\n", "page": 60, "type": "text", "section": "Page 60"}
{"text": "What we have learned over the last half-century is what not to do.\nWith that realization, we have to face an unwelcome fact: Software is not a rapidly advancing\ntechnology. The rules of software are the same today as they were in 1946, when Alan Turing wrote\nthe very first code that would execute in an electronic computer. The tools have changed, and the\nhardware has changed, but the essence of software remains the same.\nSoftware\u2014the stuff of computer programs\u2014is composed of sequence, selection, iteration, and\nindirection. Nothing more. Nothing less.\n1. I know... What\u2019s a disk?\n2. Thanks to Greg Young for teaching me about this concept.\n", "page": 61, "type": "text", "section": "Page 61"}
{"text": "III\nDESIGN PRINCIPLES\nGood software systems begin with clean code. On the one hand, if the bricks aren\u2019t well made, the\narchitecture of the building doesn\u2019t matter much. On the other hand, you can make a substantial mess\nwith well-made bricks. This is where the SOLID principles come in.\nThe SOLID principles tell us how to arrange our functions and data structures into classes, and how\nthose classes should be interconnected. The use of the word \u201cclass\u201d does not imply that these\nprinciples are applicable only to object-oriented software. A class is simply a coupled grouping of\nfunctions and data. Every software system has such groupings, whether they are called classes or not.\nThe SOLID principles apply to those groupings.\nThe goal of the principles is the creation of mid-level software structures that:\n\u2022 Tolerate change,\n\u2022 Are easy to understand, and\n\u2022 Are the basis of components that can be used in many software systems.\nThe term \u201cmid-level\u201d refers to the fact that these principles are applied by programmers working at\nthe module level. They are applied just above the level of the code and help to define the kinds of\nsoftware structures used within modules and components.\nJust as it is possible to create a substantial mess with well-made bricks, so it is also possible to\n", "page": 62, "type": "text", "section": "Page 62"}
{"text": "create a system-wide mess with well-designed mid-level components. For this reason, once we have\ncovered the SOLID principles, we will move on to their counterparts in the component world, and\nthen to the principles of high-level architecture.\nThe history of the SOLID principles is long. I began to assemble them in the late 1980s while\ndebating software design principles with others on USENET (an early kind of Facebook). Over the\nyears, the principles have shifted and changed. Some were deleted. Others were merged. Still others\nwere added. The final grouping stabilized in the early 2000s, although I presented them in a different\norder.\nIn 2004 or thereabouts, Michael Feathers sent me an email saying that if I rearranged the principles,\ntheir first words would spell the word SOLID\u2014and thus the SOLID principles were born.\nThe chapters that follow describe each principle more thoroughly. Here is the executive summary:\n\u2022 SRP: The Single Responsibility Principle\nAn active corollary to Conway\u2019s law: The best structure for a software system is heavily influenced\nby the social structure of the organization that uses it so that each software module has one, and only\none, reason to change.\n\u2022 OCP: The Open-Closed Principle\nBertrand Meyer made this principle famous in the 1980s. The gist is that for software systems to be\neasy to change, they must be designed to allow the behavior of those systems to be changed by\nadding new code, rather than changing existing code.\n\u2022 LSP: The Liskov Substitution Principle\nBarbara Liskov\u2019s famous definition of subtypes, from 1988. In short, this principle says that to build\nsoftware systems from interchangeable parts, those parts must adhere to a contract that allows those\nparts to be substituted one for another.\n\u2022 ISP: The Interface Segregation Principle\nThis principle advises software designers to avoid depending on things that they don\u2019t use.\n\u2022 DIP: The Dependency Inversion Principle\nThe code that implements high-level policy should not depend on the code that implements low-\nlevel details. Rather, details should depend on policies.\nThese principles have been described in detail in many different publications1 over the years. The\nchapters that follow will focus on the architectural implications of these principles instead of\nrepeating those detailed discussions. If you are not already familiar with these principles, what\nfollows is insufficient to understand them in detail and you would be well advised to study them in\nthe footnoted documents.\n1. For example, Agile Software Development, Principles, Patterns, and Practices, Robert C. Martin, Prentice Hall, 2002,\nhttp://www.butunclebob.com/ArticleS.UncleBob.PrinciplesOfOod, and https://en.wikipedia.org/wiki/SOLID_(object-oriented_design)\n(or just google SOLID).\n", "page": 63, "type": "text", "section": "Page 63"}
{"text": "7\nSRP: THE SINGLE RESPONSIBILITY PRINCIPLE\nOf all the SOLID principles, the Single Responsibility Principle (SRP) might be the least well\nunderstood. That\u2019s likely because it has a particularly inappropriate name. It is too easy for\nprogrammers to hear the name and then assume that it means that every module should do just one\nthing.\nMake no mistake, there is a principle like that. A function should do one, and only one, thing. We use\nthat principle when we are refactoring large functions into smaller functions; we use it at the lowest\nlevels. But it is not one of the SOLID principles\u2014it is not the SRP.\nHistorically, the SRP has been described this way:\nA module should have one, and only one, reason to change.\nSoftware systems are changed to satisfy users and stakeholders; those users and stakeholders are the\n\u201creason to change\u201d that the principle is talking about. Indeed, we can rephrase the principle to say\nthis:\nA module should be responsible to one, and only one, user or stakeholder.\nUnfortunately, the words \u201cuser\u201d and \u201cstakeholder\u201d aren\u2019t really the right words to use here. There\nwill likely be more than one user or stakeholder who wants the system changed in the same way.\nInstead, we\u2019re really referring to a group\u2014one or more people who require that change. We\u2019ll refer\n", "page": 64, "type": "text", "section": "Page 64"}
{"text": "to that group as an actor.\nThus the final version of the SRP is:\nA module should be responsible to one, and only one, actor.\nNow, what do we mean by the word \u201cmodule\u201d? The simplest definition is just a source file. Most of\nthe time that definition works fine. Some languages and development environments, though, don\u2019t use\nsource files to contain their code. In those cases a module is just a cohesive set of functions and data\nstructures.\nThat word \u201ccohesive\u201d implies the SRP. Cohesion is the force that binds together the code responsible\nto a single actor.\nPerhaps the best way to understand this principle is by looking at the symptoms of violating it.\nSYMPTOM 1: ACCIDENTAL DUPLICATION\nMy favorite example is the Employee class from a payroll application. It has three methods:\ncalculatePay(), reportHours(), and save() (Figure 7.1).\nFigure 7.1 The Employee class\nThis class violates the SRP because those three methods are responsible to three very different\nactors.\n\u2022 The calculatePay() method is specified by the accounting department, which reports to the CFO.\n\u2022 The reportHours() method is specified and used by the human resources department, which\nreports to the COO.\n\u2022 The save() method is specified by the database administrators (DBAs), who report to the CTO.\nBy putting the source code for these three methods into a single Employee class, the developers have\n", "page": 65, "type": "text", "section": "Page 65"}
{"text": "coupled each of these actors to the others. This coupling can cause the actions of the CFO\u2019s team to\naffect something that the COO\u2019s team depends on.\nFor example, suppose that the calculatePay() function and the reportHours() function share a\ncommon algorithm for calculating non-overtime hours. Suppose also that the developers, who are\ncareful not to duplicate code, put that algorithm into a function named regularHours() (Figure 7.2).\nFigure 7.2 Shared algorithm\nNow suppose that the CFO\u2019s team decides that the way non-overtime hours are calculated needs to be\ntweaked. In contrast, the COO\u2019s team in HR does not want that particular tweak because they use non-\novertime hours for a different purpose.\nA developer is tasked to make the change, and sees the convenient regularHours() function called\nby the calculatePay() method. Unfortunately, that developer does not notice that the function is also\ncalled by the reportHours() function.\nThe developer makes the required change and carefully tests it. The CFO\u2019s team validates that the\nnew function works as desired, and the system is deployed.\nOf course, the COO\u2019s team doesn\u2019t know that this is happening. The HR personnel continue to use the\nreports generated by the reportHours() function\u2014but now they contain incorrect numbers.\nEventually the problem is discovered, and the COO is livid because the bad data has cost his budget\nmillions of dollars.\nWe\u2019ve all seen things like this happen. These problems occur because we put code that different\nactors depend on into close proximity. The SRP says to separate the code that different actors\ndepend on.\nSYMPTOM 2: MERGES\nIt\u2019s not hard to imagine that merges will be common in source files that contain many different\n", "page": 66, "type": "text", "section": "Page 66"}
{"text": "methods. This situation is especially likely if those methods are responsible to different actors.\nFor example, suppose that the CTO\u2019s team of DBAs decides that there should be a simple schema\nchange to the Employee table of the database. Suppose also that the COO\u2019s team of HR clerks\ndecides that they need a change in the format of the hours report.\nTwo different developers, possibly from two different teams, check out the Employee class and begin\nto make changes. Unfortunately their changes collide. The result is a merge.\nI probably don\u2019t need to tell you that merges are risky affairs. Our tools are pretty good nowadays,\nbut no tool can deal with every merge case. In the end, there is always risk.\nIn our example, the merge puts both the CTO and the COO at risk. It\u2019s not inconceivable that the CFO\ncould be affected as well.\nThere are many other symptoms that we could investigate, but they all involve multiple people\nchanging the same source file for different reasons.\nOnce again, the way to avoid this problem is to separate code that supports different actors.\nSOLUTIONS\nThere are many different solutions to this problem. Each moves the functions into different classes.\nPerhaps the most obvious way to solve the problem is to separate the data from the functions. The\nthree classes share access to EmployeeData, which is a simple data structure with no methods\n(Figure 7.3). Each class holds only the source code necessary for its particular function. The three\nclasses are not allowed to know about each other. Thus any accidental duplication is avoided.\nFigure 7.3 The three classes do not know about each other\nThe downside of this solution is that the developers now have three classes that they have to\ninstantiate and track. A common solution to this dilemma is to use the Facade pattern (Figure 7.4).\n", "page": 67, "type": "text", "section": "Page 67"}
{"text": "Figure 7.4 The Facade pattern\nThe EmployeeFacade contains very little code. It is responsible for instantiating and delegating to the\nclasses with the functions.\nSome developers prefer to keep the most important business rules closer to the data. This can be done\nby keeping the most important method in the original Employee class and then using that class as a\nFacade for the lesser functions (Figure 7.5).\nFigure 7.5 The most important method is kept in the original Employee class and used as a Facade for the lesser functions\nYou might object to these solutions on the basis that every class would contain just one function. This\nis hardly the case. The number of functions required to calculate pay, generate a report, or save the\ndata is likely to be large in each case. Each of those classes would have many private methods in\nthem.\nEach of the classes that contain such a family of methods is a scope. Outside of that scope, no one\nknows that the private members of the family exist.\nCONCLUSION\nThe Single Responsibility Principle is about functions and classes\u2014but it reappears in a different\n", "page": 68, "type": "text", "section": "Page 68"}
{"text": "form at two more levels. At the level of components, it becomes the Common Closure Principle. At\nthe architectural level, it becomes the Axis of Change responsible for the creation of Architectural\nBoundaries. We\u2019ll be studying all of these ideas in the chapters to come.\n", "page": 69, "type": "text", "section": "Page 69"}
{"text": "8\nOCP: THE OPEN-CLOSED PRINCIPLE\nThe Open-Closed Principle (OCP) was coined in 1988 by Bertrand Meyer.1 It says:\nA software artifact should be open for extension but closed for modification.\nIn other words, the behavior of a software artifact ought to be extendible, without having to modify\nthat artifact.\nThis, of course, is the most fundamental reason that we study software architecture. Clearly, if simple\nextensions to the requirements force massive changes to the software, then the architects of that\nsoftware system have engaged in a spectacular failure.\nMost students of software design recognize the OCP as a principle that guides them in the design of\nclasses and modules. But the principle takes on even greater significance when we consider the level\nof architectural components.\nA thought experiment will make this clear.\nA THOUGHT EXPERIMENT\nImagine, for a moment, that we have a system that displays a financial summary on a web page. The\ndata on the page is scrollable, and negative numbers are rendered in red.\n", "page": 70, "type": "text", "section": "Page 70"}
{"text": "Now imagine that the stakeholders ask that this same information be turned into a report to be printed\non a black-and-white printer. The report should be properly paginated, with appropriate page\nheaders, page footers, and column labels. Negative numbers should be surrounded by parentheses.\nClearly, some new code must be written. But how much old code will have to change?\nA good software architecture would reduce the amount of changed code to the barest minimum.\nIdeally, zero.\nHow? By properly separating the things that change for different reasons (the Single Responsibility\nPrinciple), and then organizing the dependencies between those things properly (the Dependency\nInversion Principle).\nBy applying the SRP, we might come up with the data-flow view shown in Figure 8.1. Some analysis\nprocedure inspects the financial data and produces reportable data, which is then formatted\nappropriately by the two reporter processes.\nFigure 8.1 Applying the SRP\nThe essential insight here is that generating the report involves two separate responsibilities: the\ncalculation of the reported data, and the presentation of that data into a web- and printer-friendly\nform.\nHaving made this separation, we need to organize the source code dependencies to ensure that\nchanges to one of those responsibilities do not cause changes in the other. Also, the new organization\nshould ensure that the behavior can be extended without undo modification.\nWe accomplish this by partitioning the processes into classes, and separating those classes into\ncomponents, as shown by the double lines in the diagram in Figure 8.2. In this figure, the component\nat the upper left is the Controller. At the upper right, we have the Interactor. At the lower right, there\nis the Database. Finally, at the lower left, there are four components that represent the Presenters and\nthe Views.\n", "page": 71, "type": "text", "section": "Page 71"}
{"text": "Figure 8.2 Partitioning the processes into classes and separating the classes into components\nClasses marked with <I> are interfaces; those marked with <DS> are data structures. Open\narrowheads are using relationships. Closed arrowheads are implements or inheritance relationships.\nThe first thing to notice is that all the dependencies are source code dependencies. An arrow pointing\nfrom class A to class B means that the source code of class A mentions the name of class B, but class\nB mentions nothing about class A. Thus, in Figure 8.2, FinancialDataMapper knows about\nFinancialDataGateway through an implements relationship, but FinancialGateway knows nothing\nat all about FinancialDataMapper.\nThe next thing to notice is that each double line is crossed in one direction only. This means that all\ncomponent relationships are unidirectional, as shown in the component graph in Figure 8.3. These\narrows point toward the components that we want to protect from change.\n", "page": 72, "type": "text", "section": "Page 72"}
{"text": "Figure 8.3 The component relationships are unidirectional\nLet me say that again: If component A should be protected from changes in component B, then\ncomponent B should depend on component A.\nWe want to protect the Controller from changes in the Presenters. We want to protect the Presenters\nfrom changes in the Views. We want to protect the Interactor from changes in\u2014well, anything.\nThe Interactor is in the position that best conforms to the OCP. Changes to the Database, or the\nController, or the Presenters, or the Views, will have no impact on the Interactor.\nWhy should the Interactor hold such a privileged position? Because it contains the business rules.\nThe Interactor contains the highest-level policies of the application. All the other components are\ndealing with peripheral concerns. The Interactor deals with the central concern.\nEven though the Controller is peripheral to the Interactor, it is nevertheless central to the Presenters\nand Views. And while the Presenters might be peripheral to the Controller, they are central to the\nViews.\nNotice how this creates a hierarchy of protection based on the notion of \u201clevel.\u201d Interactors are the\nhighest-level concept, so they are the most protected. Views are among the lowest-level concepts, so\nthey are the least protected. Presenters are higher level than Views, but lower level than the\nController or the Interactor.\nThis is how the OCP works at the architectural level. Architects separate functionality based on how,\nwhy, and when it changes, and then organize that separated functionality into a hierarchy of\ncomponents. Higher-level components in that hierarchy are protected from the changes made to\nlower-level components.\nDIRECTIONAL CONTROL\nIf you recoiled in horror from the class design shown earlier, look again. Much of the complexity in\n", "page": 73, "type": "text", "section": "Page 73"}
{"text": "that diagram was intended to make sure that the dependencies between the components pointed in the\ncorrect direction.\nFor example, the FinancialDataGateway interface between the FinancialReportGenerator and\nthe FinancialDataMapper exists to invert the dependency that would otherwise have pointed from\nthe Interactor component to the Database component. The same is true of the\nFinancialReportPresenter interface, and the two View interfaces.\nINFORMATION HIDING\nThe FinancialReportRequester interface serves a different purpose. It is there to protect the\nFinancialReportController from knowing too much about the internals of the Interactor. If that\ninterface were not there, then the Controller would have transitive dependencies on the\nFinancialEntities.\nTransitive dependencies are a violation of the general principle that software entities should not\ndepend on things they don\u2019t directly use. We\u2019ll encounter that principle again when we talk about the\nInterface Segregation Principle and the Common Reuse Principle.\nSo, even though our first priority is to protect the Interactor from changes to the Controller, we also\nwant to protect the Controller from changes to the Interactor by hiding the internals of the Interactor.\nCONCLUSION\nThe OCP is one of the driving forces behind the architecture of systems. The goal is to make the\nsystem easy to extend without incurring a high impact of change. This goal is accomplished by\npartitioning the system into components, and arranging those components into a dependency hierarchy\nthat protects higher-level components from changes in lower-level components.\n1. Bertrand Meyer. Object Oriented Software Construction, Prentice Hall, 1988, p. 23.\n", "page": 74, "type": "text", "section": "Page 74"}
{"text": "9\nLSP: THE LISKOV SUBSTITUTION PRINCIPLE\nIn 1988, Barbara Liskov wrote the following as a way of defining subtypes.\nWhat is wanted here is something like the following substitution property: If for each object o1 of type S there is an\nobject o2 of type T such that for all programs P defined in terms of T, the behavior of P is unchanged when o1 is\nsubstituted for o2 then S is a subtype of T.1\nTo understand this idea, which is known as the Liskov Substitution Principle (LSP), let\u2019s look at some\nexamples.\nGUIDING THE USE OF INHERITANCE\nImagine that we have a class named License, as shown in Figure 9.1. This class has a method named\ncalcFee(), which is called by the Billing application. There are two \u201csubtypes\u201d of License:\nPersonalLicense and BusinessLicense. They use different algorithms to calculate the license fee.\n", "page": 75, "type": "text", "section": "Page 75"}
{"text": "Figure 9.1 License, and its derivatives, conform to LSP\nThis design conforms to the LSP because the behavior of the Billing application does not depend, in\nany way, on which of the two subtypes it uses. Both of the subtypes are substitutable for the License\ntype.\nTHE SQUARE/RECTANGLE PROBLEM\nThe canonical example of a violation of the LSP is the famed (or infamous, depending on your\nperspective) square/rectangle problem (Figure 9.2).\nFigure 9.2 The infamous square/rectangle problem\nIn this example, Square is not a proper subtype of Rectangle because the height and width of the\nRectangle are independently mutable; in contrast, the height and width of the Square must change\ntogether. Since the User believes it is communicating with a Rectangle, it could easily get confused.\nThe following code shows why:\n", "page": 76, "type": "text", "section": "Page 76"}
{"text": "Click here to view code image\nRectangle r = \u2026\nr.setW(5);\nr.setH(2);\nassert(r.area() == 10);\nIf the \u2026 code produced a Square, then the assertion would fail.\nThe only way to defend against this kind of LSP violation is to add mechanisms to the User (such as\nan if statement) that detects whether the Rectangle is, in fact, a Square. Since the behavior of the\nUser depends on the types it uses, those types are not substitutable.\nLSP AND ARCHITECTURE\nIn the early years of the object-oriented revolution, we thought of the LSP as a way to guide the use of\ninheritance, as shown in the previous sections. However, over the years the LSP has morphed into a\nbroader principle of software design that pertains to interfaces and implementations.\nThe interfaces in question can be of many forms. We might have a Java-style interface, implemented\nby several classes. Or we might have several Ruby classes that share the same method signatures. Or\nwe might have a set of services that all respond to the same REST interface.\nIn all of these situations, and more, the LSP is applicable because there are users who depend on\nwell-defined interfaces, and on the substitutability of the implementations of those interfaces.\nThe best way to understand the LSP from an architectural viewpoint is to look at what happens to the\narchitecture of a system when the principle is violated.\nEXAMPLE LSP VIOLATION\nAssume that we are building an aggregator for many taxi dispatch services. Customers use our\nwebsite to find the most appropriate taxi to use, regardless of taxi company. Once the customer makes\na decision, our system dispatches the chosen taxi by using a restful service.\nNow assume that the URI for the restful dispatch service is part of the information contained in the\ndriver database. Once our system has chosen a driver appropriate for the customer, it gets that URI\nfrom the driver record and then uses it to dispatch the driver.\nSuppose Driver Bob has a dispatch URI that looks like this:\nClick here to view code image\n", "page": 77, "type": "text", "section": "Page 77"}
{"text": "purplecab.com/driver/Bob\nOur system will append the dispatch information onto this URI and send it with a PUT, as follows:\nClick here to view code image\npurplecab.com/driver/Bob \n       /pickupAddress/24 Maple St.\n       /pickupTime/153\n       /destination/ORD\nClearly, this means that all the dispatch services, for all the different companies, must conform to the\nsame REST interface. They must treat the pickupAddress, pickupTime, and destination fields\nidentically.\nNow suppose the Acme taxi company hired some programmers who didn\u2019t read the spec very\ncarefully. They abbreviated the destination field to just dest. Acme is the largest taxi company in our\narea, and Acme\u2019s CEO\u2019s ex-wife is our CEO\u2019s new wife, and \u2026 Well, you get the picture. What\nwould happen to the architecture of our system?\nObviously, we would need to add a special case. The dispatch request for any Acme driver would\nhave to be constructed using a different set of rules from all the other drivers.\nThe simplest way to accomplish this goal would be to add an if statement to the module that\nconstructed the dispatch command:\nClick here to view code image\nif (driver.getDispatchUri().startsWith(\"acme.com\"))\u2026\nBut, of course, no architect worth his or her salt would allow such a construction to exist in the\nsystem. Putting the word \u201cacme\u201d into the code itself creates an opportunity for all kinds of horrible\nand mysterious errors, not to mention security breaches.\nFor example, what if Acme became even more successful and bought the Purple Taxi company. What\nif the merged company maintained the separate brands and the separate websites, but unified all of the\noriginal companies\u2019 systems? Would we have to add another if statement for \u201cpurple\u201d?\nOur architect would have to insulate the system from bugs like this by creating some kind of dispatch\ncommand creation module that was driven by a configuration database keyed by the dispatch URI.\nThe configuration data might look something like this:\nClick here to view code image\nURI\nDispatch Format\nAcme.com    /pickupAddress/%s/pickupTime/%s/dest/%s\n*.*    \n/pickupAddress/%s/pickupTime/%s/destination/%s\n", "page": 78, "type": "text", "section": "Page 78"}
{"text": "And so our architect has had to add a significant and complex mechanism to deal with the fact that the\ninterfaces of the restful services are not all substitutable.\nCONCLUSION\nThe LSP can, and should, be extended to the level of architecture. A simple violation of\nsubstitutability, can cause a system\u2019s architecture to be polluted with a significant amount of extra\nmechanisms.\n1. Barbara Liskov, \u201cData Abstraction and Hierarchy,\u201d SIGPLAN Notices 23, 5 (May 1988).\n", "page": 79, "type": "text", "section": "Page 79"}
{"text": "10\nISP: THE INTERFACE SEGREGATION PRINCIPLE\nThe Interface Segregation Principle (ISP) derives its name from the diagram shown in Figure 10.1.\nFigure 10.1 The Interface Segregation Principle\nIn the situation illustrated in Figure 10.1, there are several users who use the operations of the OPS\nclass. Let\u2019s assume that User1 uses only op1, User2 uses only op2, and User3 uses only op3.\nNow imagine that OPS is a class written in a language like Java. Clearly, in that case, the source code\n", "page": 80, "type": "text", "section": "Page 80"}
{"text": "of User1 will inadvertently depend on op2 and op3, even though it doesn\u2019t call them. This\ndependence means that a change to the source code of op2 in OPS will force User1 to be recompiled\nand redeployed, even though nothing that it cared about has actually changed.\nThis problem can be resolved by segregating the operations into interfaces as shown in Figure 10.2.\nAgain, if we imagine that this is implemented in a statically typed language like Java, then the source\ncode of User1 will depend on U1Ops, and op1, but will not depend on OPS. Thus a change to OPS that\nUser1 does not care about will not cause User1 to be recompiled and redeployed.\nFigure 10.2 Segregated operations\nISP AND LANGUAGE\nClearly, the previously given description depends critically on language type. Statically typed\nlanguages like Java force programmers to create declarations that users must import, or use, or\notherwise include. It is these included declarations in source code that create the source code\ndependencies that force recompilation and redeployment.\nIn dynamically typed languages like Ruby and Python, such declarations don\u2019t exist in source code.\nInstead, they are inferred at runtime. Thus there are no source code dependencies to force\nrecompilation and redeployment. This is the primary reason that dynamically typed languages create\nsystems that are more flexible and less tightly coupled than statically typed languages.\nThis fact could lead you to conclude that the ISP is a language issue, rather than an architecture issue.\nISP AND ARCHITECTURE\nIf you take a step back and look at the root motivations of the ISP, you can see a deeper concern\nlurking there. In general, it is harmful to depend on modules that contain more than you need. This is\n", "page": 81, "type": "text", "section": "Page 81"}
{"text": "obviously true for source code dependencies that can force unnecessary recompilation and\nredeployment\u2014but it is also true at a much higher, architectural level.\nConsider, for example, an architect working on a system, S. He wants to include a certain framework,\nF, into the system. Now suppose that the authors of F have bound it to a particular database, D. So S\ndepends on F. which depends on D (Figure 10.3).\nFigure 10.3 A problematic architecture\nNow suppose that D contains features that F does not use and, therefore, that S does not care about.\nChanges to those features within D may well force the redeployment of F and, therefore, the\nredeployment of S. Even worse, a failure of one of the features within D may cause failures in F and\nS.\nCONCLUSION\nThe lesson here is that depending on something that carries baggage that you don\u2019t need can cause you\ntroubles that you didn\u2019t expect.\nWe\u2019ll explore this idea in more detail when we discuss the Common Reuse Principle in Chapter 13,\n\u201cComponent Cohesion.\u201d\n", "page": 82, "type": "text", "section": "Page 82"}
{"text": "11\nDIP: THE DEPENDENCY INVERSION PRINCIPLE\nThe Dependency Inversion Principle (DIP) tells us that the most flexible systems are those in which\nsource code dependencies refer only to abstractions, not to concretions.\nIn a statically typed language, like Java, this means that the use, import, and include statements\nshould refer only to source modules containing interfaces, abstract classes, or some other kind of\nabstract declaration. Nothing concrete should be depended on.\nThe same rule applies for dynamically typed languages, like Ruby and Python. Source code\ndependencies should not refer to concrete modules. However, in these languages it is a bit harder to\ndefine what a concrete module is. In particular, it is any module in which the functions being called\nare implemented.\nClearly, treating this idea as a rule is unrealistic, because software systems must depend on many\nconcrete facilities. For example, the String class in Java is concrete, and it would be unrealistic to\ntry to force it to be abstract. The source code dependency on the concrete java.lang.string cannot,\nand should not, be avoided.\nBy comparison, the String class is very stable. Changes to that class are very rare and tightly\ncontrolled. Programmers and architects do not have to worry about frequent and capricious changes\nto String.\nFor these reasons, we tend to ignore the stable background of operating system and platform facilities\n", "page": 83, "type": "text", "section": "Page 83"}
{"text": "when it comes to DIP. We tolerate those concrete dependencies because we know we can rely on\nthem not to change.\nIt is the volatile concrete elements of our system that we want to avoid depending on. Those are the\nmodules that we are actively developing, and that are undergoing frequent change.\nSTABLE ABSTRACTIONS\nEvery change to an abstract interface corresponds to a change to its concrete implementations.\nConversely, changes to concrete implementations do not always, or even usually, require changes to\nthe interfaces that they implement. Therefore interfaces are less volatile than implementations.\nIndeed, good software designers and architects work hard to reduce the volatility of interfaces. They\ntry to find ways to add functionality to implementations without making changes to the interfaces. This\nis Software Design 101.\nThe implication, then, is that stable software architectures are those that avoid depending on volatile\nconcretions, and that favor the use of stable abstract interfaces. This implication boils down to a set\nof very specific coding practices:\n\u2022 Don\u2019t refer to volatile concrete classes. Refer to abstract interfaces instead. This rule applies in\nall languages, whether statically or dynamically typed. It also puts severe constraints on the creation\nof objects and generally enforces the use of Abstract Factories.\n\u2022 Don\u2019t derive from volatile concrete classes. This is a corollary to the previous rule, but it bears\nspecial mention. In statically typed languages, inheritance is the strongest, and most rigid, of all the\nsource code relationships; consequently, it should be used with great care. In dynamically typed\nlanguages, inheritance is less of a problem, but it is still a dependency\u2014and caution is always the\nwisest choice.\n\u2022 Don\u2019t override concrete functions. Concrete functions often require source code dependencies.\nWhen you override those functions, you do not eliminate those dependencies\u2014indeed, you inherit\nthem. To manage those dependencies, you should make the function abstract and create multiple\nimplementations.\n\u2022 Never mention the name of anything concrete and volatile. This is really just a restatement of\nthe principle itself.\nFACTORIES\nTo comply with these rules, the creation of volatile concrete objects requires special handling. This\ncaution is warranted because, in virtually all languages, the creation of an object requires a source\ncode dependency on the concrete definition of that object.\nIn most object-oriented languages, such as Java, we would use an Abstract Factory to manage this\nundesirable dependency.\n", "page": 84, "type": "text", "section": "Page 84"}
{"text": "The diagram in Figure 11.1 shows the structure. The Application uses the ConcreteImpl through\nthe Service interface. However, the Application must somehow create instances of the\nConcreteImpl. To achieve this without creating a source code dependency on the ConcreteImpl, the\nApplication calls the makeSvc method of the ServiceFactory interface. This method is\nimplemented by the ServiceFactoryImpl class, which derives from ServiceFactory. That\nimplementation instantiates the ConcreteImpl and returns it as a Service.\nFigure 11.1 Use of the Abstract Factory pattern to manage the dependency\nThe curved line in Figure 11.1 is an architectural boundary. It separates the abstract from the\nconcrete. All source code dependencies cross that curved line pointing in the same direction, toward\nthe abstract side.\nThe curved line divides the system into two components: one abstract and the other concrete. The\nabstract component contains all the high-level business rules of the application. The concrete\ncomponent contains all the implementation details that those business rules manipulate.\nNote that the flow of control crosses the curved line in the opposite direction of the source code\ndependencies. The source code dependencies are inverted against the flow of control\u2014which is why\nwe refer to this principle as Dependency Inversion.\nCONCRETE COMPONENTS\nThe concrete component in Figure 11.1 contains a single dependency, so it violates the DIP. This is\ntypical. DIP violations cannot be entirely removed, but they can be gathered into a small number of\nconcrete components and kept separate from the rest of the system.\nMost systems will contain at least one such concrete component\u2014often called main because it\ncontains the main1 function. In the case illustrated in Figure 11.1, the main function would instantiate\nthe ServiceFactoryImpl and place that instance in a global variable of type ServiceFactory. The\nApplication would then access the factory through that global variable.\n", "page": 85, "type": "text", "section": "Page 85"}
{"text": "CONCLUSION\nAs we move forward in this book and cover higher-level architectural principles, the DIP will show\nup again and again. It will be the most visible organizing principle in our architecture diagrams. The\ncurved line in Figure 11.1 will become the architectural boundaries in later chapters. The way the\ndependencies cross that curved line in one direction, and toward more abstract entities, will become\na new rule that we will call the Dependency Rule.\n1. In other words, the function that is invoked by the operating system when the application is first started up.\n", "page": 86, "type": "text", "section": "Page 86"}
{"text": "IV\nCOMPONENT PRINCIPLES\nIf the SOLID principles tell us how to arrange the bricks into walls and rooms, then the component\nprinciples tell us how to arrange the rooms into buildings. Large software systems, like large\nbuildings, are built out of smaller components.\nIn Part IV, we will discuss what software components are, which elements should compose them, and\nhow they should be composed together into systems.\n", "page": 87, "type": "text", "section": "Page 87"}
{"text": "12\nCOMPONENTS\nComponents are the units of deployment. They are the smallest entities that can be deployed as part of\na system. In Java, they are jar files. In Ruby, they are gem files. In .Net, they are DLLs. In compiled\nlanguages, they are aggregations of binary files. In interpreted languages, they are aggregations of\nsource files. In all languages, they are the granule of deployment.\nComponents can be linked together into a single executable. Or they can be aggregated together into a\nsingle archive, such as a .war file. Or they can be independently deployed as separate dynamically\nloaded plugins, such as.jar or .dll or .exe files. Regardless of how they are eventually deployed,\nwell-designed components always retain the ability to be independently deployable and, therefore,\nindependently developable.\nA BRIEF HISTORY OF COMPONENTS\nIn the early years of software development, programmers controlled the memory location and layout\nof their programs. One of the first lines of code in a program would be the origin statement, which\ndeclared the address at which the program was to be loaded.\nConsider the following simple PDP-8 program. It consists of a subroutine named GETSTR that inputs a\nstring from the keyboard and saves it in a buffer. It also has a little unit test program to exercise\nGETSTR.\n", "page": 88, "type": "text", "section": "Page 88"}
{"text": "Click here to view code image\n                *200\n                TLS\n     START,     CLA\n                TAD BUFR\n                JMS GETSTR\n                CLA\n                TAD BUFR\n                JMS PUTSTR\n                JMP START\n     BUFR,      3000\n \n     GETSTR,    0\n                DCA PTR\n     NXTCH,     KSF\n                JMP -1\n                KRB\n                DCA I PTR\n                TAD I PTR\n                AND K177\n                ISZ PTR\n                TAD MCR\n                SZA\n                JMP NXTCH\n \n     K177,      177\n     MCR,       -15\nNote the *200 command at the start of this program. It tells the compiler to generate code that will be\nloaded at address 2008.\nThis kind of programming is a foreign concept for most programmers today. They rarely have to think\nabout where a program is loaded in the memory of the computer. But in the early days, this was one of\nthe first decisions a programmer needed to make. In those days, programs were not relocatable.\nHow did you access a library function in those olden days? The preceding code illustrates the\napproach used. Programmers included the source code of the library functions with their application\ncode, and compiled them all as a single program.1 Libraries were kept in source, not in binary.\nThe problem with this approach was that, during this era, devices were slow and memory was\nexpensive and, therefore, limited. Compilers needed to make several passes over the source code, but\nmemory was too limited to keep all the source code resident. Consequently, the compiler had to read\nin the source code several times using the slow devices.\nThis took a long time\u2014and the larger your function library, the longer the compiler took. Compiling a\nlarge program could take hours.\nTo shorten the compile times, programmers separated the source code of the function library from the\napplications. They compiled the function library separately and loaded the binary at a known address\n\u2014say, 20008. They created a symbol table for the function library and compiled that with their\napplication code. When they wanted to run an application, they would load the binary function\n", "page": 89, "type": "text", "section": "Page 89"}
{"text": "library,2 and then load the application. Memory looked like the layout shown in Figure 12.1.\nFigure 12.1 Early memory layout\nThis worked fine so long as the application could fit between addresses 00008 and 17778. But soon\napplications grew to be larger than the space allotted for them. At that point, programmers had to split\ntheir applications into two address segments, jumping around the function library (Figure 12.2).\nFigure 12.2 Splitting the application into two address segments\nObviously, this was not a sustainable situation. As programmers added more functions to the function\nlibrary, it exceeded its bounds, and they had to allocate more space for it (in this example, near\n70008). This fragmentation of programs and libraries necessarily continued as computer memory\ngrew.\nClearly, something had to be done.\nRELOCATABILITY\n", "page": 90, "type": "text", "section": "Page 90"}
{"text": "The solution was relocatable binaries. The idea behind them was very simple. The compiler was\nchanged to output binary code that could be relocated in memory by a smart loader. The loader would\nbe told where to load the relocatable code. The relocatable code was instrumented with flags that\ntold the loader which parts of the loaded data had to be altered to be loaded at the selected address.\nUsually this just meant adding the starting address to any memory reference addresses in the binary.\nNow the programmer could tell the loader where to load the function library, and where to load the\napplication. In fact, the loader would accept several binary inputs and simply load them in memory\none right after the other, relocating them as it loaded them. This allowed programmers to load only\nthose functions that they needed.\nThe compiler was also changed to emit the names of the functions as metadata in the relocatable\nbinary. If a program called a library function, the compiler would emit that name as an external\nreference. If a program defined a library function, the compiler would emit that name as an external\ndefinition. Then the loader could link the external references to the external definitions once it had\ndetermined where it had loaded those definitions.\nAnd the linking loader was born.\nLINKERS\nThe linking loader allowed programmers to divide their programs up onto separately compilable and\nloadable segments. This worked well when relatively small programs were being linked with\nrelatively small libraries. However, in the late 1960s and early 1970s, programmers got more\nambitious, and their programs got a lot bigger.\nEventually, the linking loaders were too slow to tolerate. Function libraries were stored on slow\ndevices such a magnetic tape. Even the disks, back then, were quite slow. Using these relatively slow\ndevices, the linking loaders had to read dozens, if not hundreds, of binary libraries to resolve the\nexternal references. As programs grew larger and larger, and more library functions accumulated in\nlibraries, a linking loader could take more than an hour just to load the program.\nEventually, the loading and the linking were separated into two phases. Programmers took the slow\npart\u2014the part that did that linking\u2014and put it into a separate application called the linker. The output\nof the linker was a linked relocatable that a relocating loader could load very quickly. This allowed\nprogrammers to prepare an executable using the slow linker, but then they could load it quickly, at any\ntime.\nThen came the 1980s. Programmers were working in C or some other high-level language. As their\nambitions grew, so did their programs. Programs that numbered hundreds of thousands of lines of\ncode were not unusual.\nSource modules were compiled from .c files into .o files, and then fed into the linker to create\nexecutable files that could be quickly loaded. Compiling each individual module was relatively fast,\nbut compiling all the modules took a bit of time. The linker would then take even more time.\n", "page": 91, "type": "text", "section": "Page 91"}
{"text": "Turnaround had again grown to an hour or more in many cases.\nIt seemed as if programmers were doomed to endlessly chase their tails. Throughout the 1960s,\n1970s, and 1980s, all the changes made to speed up workflow were thwarted by programmers\u2019\nambitions, and the size of the programs they wrote. They could not seem to escape from the hour-long\nturnaround times. Loading time remained fast, but compile-link times were the bottleneck.\nWe were, of course, experiencing Murphy\u2019s law of program size:\nPrograms will grow to fill all available compile and link time.\nBut Murphy was not the only contender in town. Along came Moore,3 and in the late 1980s, the two\nbattled it out. Moore won that battle. Disks started to shrink and got significantly faster. Computer\nmemory started to get so ridiculously cheap that much of the data on disk could be cached in RAM.\nComputer clock rates increased from 1 MHz to 100 MHz.\nBy the mid-1990s, the time spent linking had begun to shrink faster than our ambitions could make\nprograms grow. In many cases, link time decreased to a matter of seconds. For small jobs, the idea of\na linking loader became feasible again.\nThis was the era of Active-X, shared libraries, and the beginnings of .jar files. Computers and\ndevices had gotten so fast that we could, once again, do the linking at load time. We could link\ntogether several .jar files, or several shared libraries in a matter of seconds, and execute the\nresulting program. And so the component plugin architecture was born.\nToday we routinely ship .jar files or DLLs or shared libraries as plugins to existing applications. If\nyou want to create a mod to Minecraft, for example, you simply include your custom .jar files in a\ncertain folder. If you want to plug Resharper into Visual Studio, you simply include the appropriate\nDLLs.\nCONCLUSION\nThese dynamically linked files, which can be plugged together at runtime, are the software\ncomponents of our architectures. It has taken 50 years, but we have arrived at a place where\ncomponent plugin architecture can be the casual default as opposed to the herculean effort it once\nwas.\n1. My first employer kept several dozen decks of the subroutine library source code on a shelf. When you wrote a new program, you\nsimply grabbed one of those decks and slapped it onto the end of your deck.\n2. Actually, most of those old machines used core memory, which did not get erased when you powered the computer down. We often\nleft the function library loaded for days at a time.\n3. Moore\u2019s law: Computer speed, memory, and density double every 18 months. This law held from the 1950s to 2000, but then, at least\nfor clock rates, stopped cold.\n", "page": 92, "type": "text", "section": "Page 92"}
{"text": "13\nCOMPONENT COHESION\nWhich classes belong in which components? This is an important decision, and requires guidance\nfrom good software engineering principles. Unfortunately, over the years, this decision has been made\nin an ad hoc manner based almost entirely on context.\nIn this chapter we will discuss the three principles of component cohesion:\n\u2022 REP: The Reuse/Release Equivalence Principle\n\u2022 CCP: The Common Closure Principle\n\u2022 CRP: The Common Reuse Principle\nTHE REUSE/RELEASE EQUIV\nALENCE PRINCIPLE\nThe granule of reuse is the granule of release.\nThe last decade has seen the rise of a menagerie of module management tools, such as Maven,\nLeiningen, and RVM. These tools have grown in importance because, during that time, a vast number\nof reusable components and component libraries have been created. We are now living in the age of\nsoftware reuse\u2014a fulfillment of one of the oldest promises of the object-oriented model.\nThe Reuse/Release Equivalence Principle (REP) is a principle that seems obvious, at least in\nhindsight. People who want to reuse software components cannot, and will not, do so unless those\n", "page": 93, "type": "text", "section": "Page 93"}
{"text": "components are tracked through a release process and are given release numbers.\nThis is not simply because, without release numbers, there would be no way to ensure that all the\nreused components are compatible with each other. Rather, it also reflects the fact that software\ndevelopers need to know when new releases are coming, and which changes those new releases will\nbring.\nIt is not uncommon for developers to be alerted about a new release and decide, based on the changes\nmade in that release, to continue to use the old release instead. Therefore the release process must\nproduce the appropriate notifications and release documentation so that users can make informed\ndecisions about when and whether to integrate the new release.\nFrom a software design and architecture point of view, this principle means that the classes and\nmodules that are formed into a component must belong to a cohesive group. The component cannot\nsimply consist of a random hodgepodge of classes and modules; instead, there must be some\noverarching theme or purpose that those modules all share.\nOf course, this should be obvious. However, there is another way to look at this issue that is perhaps\nnot quite so obvious. Classes and modules that are grouped together into a component should be\nreleasable together. The fact that they share the same version number and the same release tracking,\nand are included under the same release documentation, should make sense both to the author and to\nthe users.\nThis is weak advice: Saying that something should \u201cmake sense\u201d is just a way of waving your hands\nin the air and trying to sound authoritative. The advice is weak because it is hard to precisely explain\nthe glue that holds the classes and modules together into a single component. Weak though the advice\nmay be, the principle itself is important, because violations are easy to detect\u2014they don\u2019t \u201cmake\nsense.\u201d If you violate the REP, your users will know, and they won\u2019t be impressed with your\narchitectural skills.\nThe weakness of this principle is more than compensated for by the strength of the next two\nprinciples. Indeed, the CCP and the CRP strongly define the this principle, but in a negative sense.\nTHE COMMON CLOSURE PRINCIPLE\nGather into components those classes that change for the same reasons and at the same times. Separate into different\ncomponents those classes that change at different times and for different reasons.\nThis is the Single Responsibility Principle restated for components. Just as the SRP says that a class\nshould not contain multiples reasons to change, so the Common Closure Principle (CCP) says that a\ncomponent should not have multiple reasons to change.\nFor most applications, maintainability is more important than reusability. If the code in an application\nmust change, you would rather that all of the changes occur in one component, rather than being\ndistributed across many components.1 If changes are confined to a single component, then we need to\nredeploy only the one changed component. Other components that don\u2019t depend on the changed\n", "page": 94, "type": "text", "section": "Page 94"}
{"text": "component do not need to be revalidated or redeployed.\nThe CCP prompts us to gather together in one place all the classes that are likely to change for the\nsame reasons. If two classes are so tightly bound, either physically or conceptually, that they always\nchange together, then they belong in the same component. This minimizes the workload related to\nreleasing, revalidating, and redeploying the software.\nThis principle is closely associated with the Open Closed Principle (OCP). Indeed, it is \u201cclosure\u201d in\nthe OCP sense of the word that the CCP addresses. The OCP states that classes should be closed for\nmodification but open for extension. Because 100% closure is not attainable, closure must be\nstrategic. We design our classes such that they are closed to the most common kinds of changes that\nwe expect or have experienced.\nThe CCP amplifies this lesson by gathering together into the same component those classes that are\nclosed to the same types of changes. Thus, when a change in requirements comes along, that change\nhas a good chance of being restricted to a minimal number of components.\nSIMILARITY WITH SRP\nAs stated earlier, the CCP is the component form of the SRP. The SRP tells us to separate methods\ninto different classes, if they change for different reasons. The CCP tells us to separate classes into\ndifferent components, if they change for different reasons. Both principles can be summarized by the\nfollowing sound bite:\nGather together those things that change at the same times and for the same reasons. Separate those things that\nchange at different times or for different reasons.\nTHE COMMON REUSE PRINCIPLE\nDon\u2019t force users of a component to depend on things they don\u2019t need.\nThe Common Reuse Principle (CRP) is yet another principle that helps us to decide which classes\nand modules should be placed into a component. It states that classes and modules that tend to be\nreused together belong in the same component.\nClasses are seldom reused in isolation. More typically, reusable classes collaborate with other\nclasses that are part of the reusable abstraction. The CRP states that these classes belong together in\nthe same component. In such a component we would expect to see classes that have lots of\ndependencies on each other.\nA simple example might be a container class and its associated iterators. These classes are reused\ntogether because they are tightly coupled to each other. Thus they ought to be in the same component.\nBut the CRP tells us more than just which classes to put together into a component: It also tells us\nwhich classes not to keep together in a component. When one component uses another, a dependency\nis created between the components. Perhaps the using component uses only one class within the used\n", "page": 95, "type": "text", "section": "Page 95"}
{"text": "component\u2014but that still doesn\u2019t weaken the dependency. The using component still depends on the\nused component.\nBecause of that dependency, every time the used component is changed, the using component will\nlikely need corresponding changes. Even if no changes are necessary to the using component, it will\nlikely still need to be recompiled, revalidated, and redeployed. This is true even if the using\ncomponent doesn\u2019t care about the change made in the used component.\nThus when we depend on a component, we want to make sure we depend on every class in that\ncomponent. Put another way, we want to make sure that the classes that we put into a component are\ninseparable\u2014that it is impossible to depend on some and not on the others. Otherwise, we will be\nredeploying more components than is necessary, and wasting significant effort.\nTherefore the CRP tells us more about which classes shouldn\u2019t be together than about which classes\nshould be together. The CRP says that classes that are not tightly bound to each other should not be in\nthe same component.\nRELATION TO ISP\nThe CRP is the generic version of the ISP. The ISP advises us not to depend on classes that have\nmethods we don\u2019t use. The CRP advises us not to depend on components that have classes we don\u2019t\nuse.\nAll of this advice can be reduced to a single sound bite:\nDon\u2019t depend on things you don\u2019t need.\nTHE TENSION DIAGRAM FOR COMPONENT\nCOHESION\nYou may have already realized that the three cohesion principles tend to fight each other. The REP\nand CCP are inclusive principles: Both tend to make components larger. The CRP is an exclusive\nprinciple, driving components to be smaller. It is the tension between these principles that good\narchitects seek to resolve.\nFigure 13.1 is a tension diagram2 that shows how the three principles of cohesion interact with each\nother. The edges of the diagram describe the cost of abandoning the principle on the opposite vertex.\n", "page": 96, "type": "text", "section": "Page 96"}
{"text": "Figure 13.1 Cohesion principles tension diagram\nAn architect who focuses on just the REP and CRP will find that too many components are impacted\nwhen simple changes are made. In contrast, an architect who focuses too strongly on the CCP and\nREP will cause too many unneeded releases to be generated.\nA good architect finds a position in that tension triangle that meets the current concerns of the\ndevelopment team, but is also aware that those concerns will change over time. For example, early in\nthe development of a project, the CCP is much more important than the REP, because develop-ability\nis more important than reuse.\nGenerally, projects tend to start on the right hand side of the triangle, where the only sacrifice is\nreuse. As the project matures, and other projects begin to draw from it, the project will slide over to\nthe left. This means that the component structure of a project can vary with time and maturity. It has\nmore to do with the way that project is developed and used, than with what the project actually does.\nCONCLUSION\nIn the past, our view of cohesion was much simpler than the REP, CCP, and CRP implied. We once\nthought that cohesion was simply the attribute that a module performs one, and only one, function.\nHowever, the three principles of component cohesion describe a much more complex variety of\ncohesion. In choosing the classes to group together into components, we must consider the opposing\nforces involved in reusability and develop-ability. Balancing these forces with the needs of the\napplication is nontrivial. Moreover, the balance is almost always dynamic. That is, the partitioning\nthat is appropriate today might not be appropriate next year. As a consequence, the composition of the\ncomponents will likely jitter and evolve with time as the focus of the project changes from develop-\nability to reusability.\n1. See the section on \u201cThe Kitty Problem\u201d in Chapter 27, \u201cServices: Great and Small.\u201c\n2. Thanks to Tim Ottinger for this idea.\n", "page": 97, "type": "text", "section": "Page 97"}
{"text": "14\nCOMPONENT COUPLING\nThe next three principles deal with the relationships between components. Here again we will run\ninto the tension between develop-ability and logical design. The forces that impinge upon the\narchitecture of a component structure are technical, political, and volatile.\nTHE ACYCLIC DEPENDENCIES PRINCIPLE\nAllow no cycles in the component dependency graph.\nHave you ever worked all day, gotten some stuff working, and then gone home, only to arrive the next\nmorning to find that your stuff no longer works? Why doesn\u2019t it work? Because somebody stayed later\nthan you and changed something you depend on! I call this \u201cthe morning after syndrome.\u201d\nThe \u201cmorning after syndrome\u201d occurs in development environments where many developers are\nmodifying the same source files. In relatively small projects with just a few developers, it isn\u2019t too\nbig a problem. But as the size of the project and the development team grow, the mornings after can\nget pretty nightmarish. It is not uncommon for weeks to go by without the team being able to build a\nstable version of the project. Instead, everyone keeps on changing and changing their code trying to\nmake it work with the last changes that someone else made.\nOver the last several decades, two solutions to this problem have evolved, both of which came from\nthe telecommunications industry. The first is \u201cthe weekly build,\u201d and the second is the Acyclic\n", "page": 98, "type": "text", "section": "Page 98"}
{"text": "Dependencies Principle (ADP).\nTHE WEEKLY BUILD\nThe weekly build used to be common in medium-sized projects. It works like this: All the developers\nignore each other for the first four days of the week. They all work on private copies of the code, and\ndon\u2019t worry about integrating their work on a collective basis. Then, on Friday, they integrate all their\nchanges and build the system.\nThis approach has the wonderful advantage of allowing the developers to live in an isolated world\nfor four days out of five. The disadvantage, of course, is the large integration penalty that is paid on\nFriday.\nUnfortunately, as the project grows, it becomes less feasible to finish integrating the project on\nFriday. The integration burden grows until it starts to overflow into Saturday. A few such Saturdays\nare enough to convince the developers that integration should really begin on Thursday\u2014and so the\nstart of integration slowly creeps toward the middle of the week.\nAs the duty cycle of development versus integration decreases, the efficiency of the team decreases,\ntoo. Eventually this situation becomes so frustrating that the developers, or the project managers,\ndeclare that the schedule should be changed to a biweekly build. This suffices for a time, but the\nintegration time continues to grow with project size.\nEventually, this scenario leads to a crisis. To maintain efficiency, the build schedule has to be\ncontinually lengthened\u2014but lengthening the build schedule increases project risks. Integration and\ntesting become increasingly harder to do, and the team loses the benefit of rapid feedback.\nELIMINATING DEPENDENCY CYCLES\nThe solution to this problem is to partition the development environment into releasable components.\nThe components become units of work that can be the responsibility of a single developer, or a team\nof developers. When developers get a component working, they release it for use by the other\ndevelopers. They give it a release number and move it into a directory for other teams to use. They\nthen continue to modify their component in their own private areas. Everyone else uses the released\nversion.\nAs new releases of a component are made available, other teams can decide whether they will\nimmediately adopt the new release. If they decide not to, they simply continue using the old release.\nOnce they decide that they are ready, they begin to use the new release.\nThus no team is at the mercy of the others. Changes made to one component do not need to have an\nimmediate affect on other teams. Each team can decide for itself when to adapt its own components to\nnew releases of the components. Moreover, integration happens in small increments. There is no\nsingle point in time when all developers must come together and integrate everything they are doing.\nThis is a very simple and rational process, and it is widely used. To make it work successfully,\n", "page": 99, "type": "text", "section": "Page 99"}
{"text": "however, you must manage the dependency structure of the components. There can be no cycles. If\nthere are cycles in the dependency structure, then the \u201cmorning after syndrome\u201d cannot be avoided.\nConsider the component diagram in Figure 14.1. It shows a rather typical structure of components\nassembled into an application. The function of this application is unimportant for the purpose of this\nexample. What is important is the dependency structure of the components. Notice that this structure is\na directed graph. The components are the nodes, and the dependency relationships are the directed\nedges.\nFigure 14.1 Typical component diagram\nNotice one more thing: Regardless of which component you begin at, it is impossible to follow the\ndependency relationships and wind up back at that component. This structure has no cycles. It is a\ndirected acyclic graph (DAG).\nNow consider what happens when the team responsible for Presenters makes a new release of their\ncomponent. It is easy to find out who is affected by this release; you just follow the dependency\narrows backward. Thus View and Main will both be affected. The developers currently working on\nthose components will have to decide when they should integrate their work with the new release of\nPresenters.\nNotice also that when Main is released, it has utterly no effect on any of the other components in the\nsystem. They don\u2019t know about Main, and they don\u2019t care when it changes. This is nice. It means that\nthe impact of releasing Main is relatively small.\nWhen the developers working on the Presenters component would like to run a test of that\ncomponent, they just need to build their version of Presenters with the versions of the Interactors\nand Entities components that they are currently using. None of the other components in the system\nneed be involved. This is nice. It means that the developers working on Presenters have relatively\nlittle work to do to set up a test, and that they have relatively few variables to consider.\nWhen it is time to release the whole system, the process proceeds from the bottom up. First the\nEntities component is compiled, tested, and released. Then the same is done for Database and\n", "page": 100, "type": "text", "section": "Page 100"}
{"text": "Interactors. These components are followed by Presenters, View, Controllers, and then\nAuthorizer. Main goes last. This process is very clear and easy to deal with. We know how to build\nthe system because we understand the dependencies between its parts.\nTHE EFFECT OF A CYCLE IN THE COMPONENT DEPENDENCY GRAPH\nSuppose that a new requirement forces us to change one of the classes in Entities such that it makes\nuse of a class in Authorizer. For example, let\u2019s say that the User class in Entities uses the\nPermissions class in Authorizer. This creates a dependency cycle, as shown in Figure 14.2.\nThis cycle creates some immediate problems. For example, the developers working on the Database\ncomponent know that to release it, the component must be compatible with Entities. However, with\nthe cycle in place, the Database component must now also be compatible with Authorizer. But\nAuthorizer depends on Interactors. This makes Database much more difficult to release.\nEntities, Authorizer, and Interactors have, in effect, become one large component\u2014which\nmeans that all of the developers working on any of those components will experience the dreaded\n\u201cmorning after syndrome.\u201d They will be stepping all over one another because they must all use\nexactly the same release of one another\u2019s components.\nFigure 14.2 A dependency cycle\nBut this is just part of the trouble. Consider what happens when we want to test the Entities\ncomponent. To our chagrin, we find that we must build and integrate with Authorizer and\nInteractors. This level of coupling between components is troubling, if not intolerable.\nYou may have wondered why you have to include so many different libraries, and so much of\neverybody else\u2019s stuff, just to run a simple unit test of one of your classes. If you investigate the matter\na bit, you will probably discover that there are cycles in the dependency graph. Such cycles make it\nvery difficult to isolate components. Unit testing and releasing become very difficult and error prone.\nIn addition, build issues grow geometrically with the number of modules.\nMoreover, when there are cycles in the dependency graph, it can be very difficult to work out the\n", "page": 101, "type": "text", "section": "Page 101"}
{"text": "order in which you must build the components. Indeed, there probably is no correct order. This can\nlead to some very nasty problems in languages like Java that read their declarations from compiled\nbinary files.\nBREAKING THE CYCLE\nIt is always possible to break a cycle of components and reinstate the dependency graph as a DAG.\nThere are two primary mechanisms for doing so:\n1. Apply the Dependency Inversion Principle (DIP). In the case in Figure 14.3, we could create an\ninterface that has the methods that User needs. We could then put that interface into Entities and\ninherit it into Authorizer. This inverts the dependency between Entities and Authorizer,\nthereby breaking the cycle.\n", "page": 102, "type": "text", "section": "Page 102"}
{"text": "Figure 14.3 Inverting the dependency between Entities and Authorizer\n2. Create a new component that both Entities and Authorizer depend on. Move the class(es) that\nthey both depend on into that new component (Figure 14.4).\nFigure 14.4 The new component that both Entities and Authorizer depend on\nTHE \u201cJITTERS\u201d\nThe second solution implies that the component structure is volatile in the presence of changing\nrequirements. Indeed, as the application grows, the component dependency structure jitters and\ngrows. Thus the dependency structure must always be monitored for cycles. When cycles occur, they\nmust be broken somehow. Sometimes this will mean creating new components, making the\ndependency structure grow.\nTOP-DOWN DESIGN\nThe issues we have discussed so far lead to an inescapable conclusion: The component structure\n", "page": 103, "type": "text", "section": "Page 103"}
{"text": "cannot be designed from the top down. It is not one of the first things about the system that is\ndesigned, but rather evolves as the system grows and changes.\nSome readers may find this point to be counterintuitive. We have come to expect that large-grained\ndecompositions, like components, will also be high-level functional decompositions.\nWhen we see a large-grained grouping such as a component dependency structure, we believe that the\ncomponents ought to somehow represent the functions of the system. Yet this does not seem to be an\nattribute of component dependency diagrams.\nIn fact, component dependency diagrams have very little do to with describing the function of the\napplication. Instead, they are a map to the buildability and maintainability of the application. This is\nwhy they aren\u2019t designed at the beginning of the project. There is no software to build or maintain, so\nthere is no need for a build and maintenance map. But as more and more modules accumulate in the\nearly stages of implementation and design, there is a growing need to manage the dependencies so that\nthe project can be developed without the \u201cmorning after syndrome.\u201d Moreover, we want to keep\nchanges as localized as possible, so we start paying attention to the SRP and CCP and collocate\nclasses that are likely to change together.\nOne of the overriding concerns with this dependency structure is the isolation of volatility. We don\u2019t\nwant components that change frequently and for capricious reasons to affect components that\notherwise ought to be stable. For example, we don\u2019t want cosmetic changes to the GUI to have an\nimpact on our business rules. We don\u2019t want the addition or modification of reports to have an impact\non our highest-level policies. Consequently, the component dependency graph is created and molded\nby architects to protect stable high-value components from volatile components.\nAs the application continues to grow, we start to become concerned about creating reusable elements.\nAt this point, the CRP begins to influence the composition of the components. Finally, as cycles\nappear, the ADP is applied and the component dependency graph jitters and grows.\nIf we tried to design the component dependency structure before we designed any classes, we would\nlikely fail rather badly. We would not know much about common closure, we would be unaware of\nany reusable elements, and we would almost certainly create components that produced dependency\ncycles. Thus the component dependency structure grows and evolves with the logical design of the\nsystem.\nTHE STABLE DEPENDENCIES PRINCIPLE\nDepend in the direction of stability.\nDesigns cannot be completely static. Some volatility is necessary if the design is to be maintained. By\nconforming to the Common Closure Principle (CCP), we create components that are sensitive to\ncertain kinds of changes but immune to others. Some of these components are designed to be volatile.\nWe expect them to change.\nAny component that we expect to be volatile should not be depended on by a component that is\n", "page": 104, "type": "text", "section": "Page 104"}
{"text": "difficult to change. Otherwise, the volatile component will also be difficult to change.\nIt is the perversity of software that a module that you have designed to be easy to change can be made\ndifficult to change by someone else who simply hangs a dependency on it. Not a line of source code\nin your module need change, yet your module will suddenly become more challenging to change. By\nconforming to the Stable Dependencies Principle (SDP), we ensure that modules that are intended to\nbe easy to change are not depended on by modules that are harder to change.\nSTABILITY\nWhat is meant by \u201cstability\u201d? Stand a penny on its side. Is it stable in that position? You would likely\nsay \u201cno.\u201d However, unless disturbed, it will remain in that position for a very long time. Thus\nstability has nothing directly to do with frequency of change. The penny is not changing, but it is\ndifficult to think of it as stable.\nWebster\u2019s Dictionary says that something is stable if it is \u201cnot easily moved.\u201d Stability is related to\nthe amount of work required to make a change. On the one hand, the standing penny is not stable\nbecause it requires very little work to topple it. On the other hand, a table is very stable because it\ntakes a considerable amount of effort to turn it over.\nHow does this relate to software? Many factors may make a software component hard to change\u2014for\nexample, its size, complexity, and clarity, among other characteristics. We will ignore all those\nfactors and focus on something different here. One sure way to make a software component difficult to\nchange, is to make lots of other software components depend on it. A component with lots of incoming\ndependencies is very stable because it requires a great deal of work to reconcile any changes with all\nthe dependent components.\nThe diagram in Figure 14.5 shows X, which is a stable component. Three components depend on X, so\nit has three good reasons not to change. We say that X is responsible to those three components.\nConversely, X depends on nothing, so it has no external influence to make it change. We say it is\nindependent.\n", "page": 105, "type": "text", "section": "Page 105"}
{"text": "Figure 14.5 X: a stable component\nFigure 14.6 shows Y, which is a very unstable component. No other components depend on Y, so we\nsay that it is irresponsible. Y also has three components that it depends on, so changes may come from\nthree external sources. We say that Y is dependent.\nFigure 14.6 Y: a very unstable component\nSTABILITY METRICS\nHow can we measure the stability of a component? One way is to count the number of dependencies\nthat enter and leave that component. These counts will allow us to calculate the positional stability of\nthe component.\n\u2022 Fan-in: Incoming dependencies. This metric identifies the number of classes outside this component\nthat depend on classes within the component.\n\u2022 Fan-out: Outgoing depenencies. This metric identifies the number of classes inside this component\nthat depend on classes outside the component.\n", "page": 106, "type": "text", "section": "Page 106"}
{"text": "\u2022 I: Instability: I = Fan-out , (Fan-in + Fan-out). This metric has the range [0, 1]. I = 0 indicates a\nmaximally stable component. I = 1 indicates a maximally unstable component.\nThe Fan-in and Fan-out metrics1 are calculated by counting the number of classes outside the\ncomponent in question that have dependencies with the classes inside the component in question.\nConsider the example in Figure 14.7.\nFigure 14.7 Our example\nLet\u2019s say we want to calculate the stability of the component Cc. We find that there are three classes\noutside Cc that depend on classes in Cc. Thus, Fan-in = 3. Moreover, there is one class outside Cc\nthat classes in Cc depend on. Thus, Fan-out = 1 and I = 1/4.\nIn C++, these dependencies are typically represented by #include statements. Indeed, the I metric is\neasiest to calculate when you have organized your source code such that there is one class in each\nsource file. In Java, the I metric can be calculated by counting import statements and qualified\nnames.\nWhen the I metric is equal to 1, it means that no other component depends on this component (Fan-in\n= 0), and this component depends on other components (Fan-out > 0). This situation is as unstable as\na component can get; it is irresponsible and dependent. Its lack of dependents gives the component no\nreason not to change, and the components that it depends on may give it ample reason to change.\nIn contrast, when the I metric is equal to 0, it means that the component is depended on by other\ncomponents (Fan-in > 0), but does not itself depend on any other components (Fan-out = 0). Such a\ncomponent is responsible and independent. It is as stable as it can get. Its dependents make it hard to\nchange the component, and its has no dependencies that might force it to change.\nThe SDP says that the I metric of a component should be larger than the I metrics of the components\nthat it depends on. That is, I metrics should decrease in the direction of dependency.\nNOT ALL COMPONENTS SHOULD BE STABLE\n", "page": 107, "type": "text", "section": "Page 107"}
{"text": "If all the components in a system were maximally stable, the system would be unchangeable. This is\nnot a desirable situation. Indeed, we want to design our component structure so that some components\nare unstable and some are stable. The diagram in Figure 14.8 shows an ideal configuration for a\nsystem with three components.\nThe changeable components are on top and depend on the stable component at the bottom. Putting the\nunstable components at the top of the diagram is a useful convention because any arrow that points up\nis violating the SDP (and, as we shall see later, the ADP).\nFigure 14.8 An ideal configuration for a system with three components\nThe diagram in Figure 14.9 shows how the SDP can be violated.\nFigure 14.9 SDP violation\nFlexible is a component that we have designed to be easy to change. We want Flexible to be\nunstable. However, some developer, working in the component named Stable, has hung a\ndependency on Flexible. This violates the SDP because the I metric for Stable is much smaller\nthan the I metric for Flexible. As a result, Flexible will no longer be easy to change. A change to\n", "page": 108, "type": "text", "section": "Page 108"}
{"text": "Flexible will force us to deal with Stable and all its dependents.\nTo fix this problem, we somehow have to break the dependence of Stable on Flexible. Why does\nthis dependency exist? Let\u2019s assume that there is a class C within Flexible that another class U\nwithin Stable needs to use (Figure 14.10).\nFigure 14.10 U within Stable uses C within Flexible\nWe can fix this by employing the DIP. We create an interface class called US and put it in a component\nnamed UServer. We make sure that this interface declares all the methods that U needs to use. We then\nmake C implement this interface as shown in Figure 14.11. This breaks the dependency of Stable on\nFlexible, and forces both components to depend on UServer. UServer is very stable (I = 0), and\nFlexible retains its necessary instability (I = 1). All the dependencies now flow in the direction of\ndecreasing I.\nFigure 14.11 C implements the interface class US\nAbstract Components\nYou may find it strange that we would create a component\u2014in this example, UService\u2014that contains\nnothing but an interface. Such a component contains no executable code! It turns out, however, that\nthis is a very common, and necessary, tactic when using statically typed languages like Java and C#.\nThese abstract components are very stable and, therefore, are ideal targets for less stable components\nto depend on.\n", "page": 109, "type": "text", "section": "Page 109"}
{"text": "When using dynamically typed languages like Ruby and Python, these abstract components don\u2019t exist\nat all, nor do the dependencies that would have targeted them. Dependency structures in these\nlanguages are much simpler because dependency inversion does not require either the declaration or\nthe inheritance of interfaces.\nTHE STABLE ABSTRACTIONS PRINCIPLE\nA component should be as abstract as it is stable.\nWHERE DO WE PUT THE HIGH-LEVEL POLICY?\nSome software in the system should not change very often. This software represents high-level\narchitecture and policy decisions. We don\u2019t want these business and architectural decisions to be\nvolatile. Thus the software that encapsulates the high-level policies of the system should be placed\ninto stable components (I = 0). Unstable components (I = 1) should contain only the software that is\nvolatile\u2014software that we want to be able to quickly and easily change.\nHowever, if the high-level policies are placed into stable components, then the source code that\nrepresents those policies will be difficult to change. This could make the overall architecture\ninflexible. How can a component that is maximally stable (I = 0) be flexible enough to withstand\nchange? The answer is found in the OCP. This principle tells us that it is possible and desirable to\ncreate classes that are flexible enough to be extended without requiring modification. Which kind of\nclasses conform to this principle? Abstract classes.\nINTRODUCING THE STABLE ABSTRACTIONS PRINCIPLE\nThe Stable Abstractions Principle (SAP) sets up a relationship between stability and abstractness. On\nthe one hand, it says that a stable component should also be abstract so that its stability does not\nprevent it from being extended. On the other hand, it says that an unstable component should be\nconcrete since it its instability allows the concrete code within it to be easily changed.\nThus, if a component is to be stable, it should consist of interfaces and abstract classes so that it can\nbe extended. Stable components that are extensible are flexible and do not overly constrain the\narchitecture.\nThe SAP and the SDP combined amount to the DIP for components. This is true because the SDP says\nthat dependencies should run in the direction of stability, and the SAP says that stability implies\nabstraction. Thus dependencies run in the direction of abstraction.\nThe DIP, however, is a principle that deals with classes\u2014and with classes there are no shades of\ngray. Either a class is abstract or it is not. The combination of the SDP and the SAP deals with\ncomponents, and allows that a component can be partially abstract and partially stable.\nMEASURING ABSTRACTION\n", "page": 110, "type": "text", "section": "Page 110"}
{"text": "The A metric is a measure of the abstractness of a component. Its value is simply the ratio of\ninterfaces and abstract classes in a component to the total number of classes in the component.\n\u2022 Nc: The number of classes in the component.\n\u2022 Na: The number of abstract classes and interfaces in the component.\n\u2022 A: Abstractness. A = Na \u00f7 Nc.\nThe A metric ranges from 0 to 1. A value of 0 implies that the component has no abstract classes at\nall. A value of 1 implies that the component contains nothing but abstract classes.\nTHE MAIN SEQUENCE\nWe are now in a position to define the relationship between stability (I) and abstractness (A). To do\nso, we create a graph with A on the vertical axis and I on the horizontal axis (Figure 14.12). If we\nplot the two \u201cgood\u201d kinds of components on this graph, we will find the components that are\nmaximally stable and abstract at the upper left at (0, 1). The components that are maximally unstable\nand concrete are at the lower right at (1, 0).\nFigure 14.12 The I/A graph\nNot all components fall into one of these two positions, because components often have degrees of\nabstraction and stability. For example, it is very common for one abstract class to derive from another\nabstract class. The derivative is an abstraction that has a dependency. Thus, though it is maximally\nabstract, it will not be maximally stable. Its dependency will decrease its stability.\nSince we cannot enforce a rule that all components sit at either (0, 1) or (1, 0), we must assume that\nthere is a locus of points on the A/I graph that defines reasonable positions for components. We can\ninfer what that locus is by finding the areas where components should not be\u2014in other words, by\ndetermining the zones of exclusion (Figure 11.13).\n", "page": 111, "type": "text", "section": "Page 111"}
{"text": "Figure 14.13 Zones of exclusion\nThe Zone of Pain\nConsider a component in the area of (0, 0). This is a highly stable and concrete component. Such a\ncomponent is not desirable because it is rigid. It cannot be extended because it is not abstract, and it\nis very difficult to change because of its stability. Thus we do not normally expect to see well-\ndesigned components sitting near (0, 0). The area around (0, 0) is a zone of exclusion called the Zone\nof Pain.\nSome software entities do, in fact, fall within the Zone of Pain. An example would be a database\nschema. Database schemas are notoriously volatile, extremely concrete, and highly depended on. This\nis one reason why the interface between OO applications and databases is so difficult to manage, and\nwhy schema updates are generally painful.\nAnother example of software that sits near the area of (0, 0) is a concrete utility library. Although\nsuch a library has an I metric of 1, it may actually be nonvolatile. Consider the String component,\nfor example. Even though all the classes within it are concrete, it is so commonly used that changing it\nwould create chaos. Therefore String is nonvolatile.\nNonvolatile components are harmless in the (0, 0) zone since they are not likely to be changed. For\nthat reason, it is only volatile software components that are problematic in the Zone of Pain. The more\nvolatile a component in the Zone of Pain, the more \u201cpainful\u201d it is. Indeed, we might consider\nvolatility to be a third axis of the graph. With this understanding, Figure 14.13 shows only the most\npainful plane, where volatility = 1.\nThe Zone of Uselessness\nConsider a component near (1, 1). This location is undesirable because it is maximally abstract, yet\nhas no dependents. Such components are useless. Thus this area is called the Zone of Uselessness.\nThe software entities that inhabit this region are a kind of detritus. They are often leftover abstract\nclasses that no one ever implemented. We find them in systems from time to time, sitting in the code\n", "page": 112, "type": "text", "section": "Page 112"}
{"text": "base, unused.\nA component that has a position deep within the Zone of Uselessness must contain a significant\nfraction of such entities. Clearly, the presence of such useless entities is undesirable.\nAVOIDING THE ZONES OF EXCLUSION\nIt seems clear that our most volatile components should be kept as far from both zones of exclusion as\npossible. The locus of points that are maximally distant from each zone is the line that connects (1, 0)\nand (0, 1). I call this line the Main Sequence.2\nA component that sits on the Main Sequence is not \u201ctoo abstract\u201d for its stability, nor is it \u201ctoo\nunstable\u201d for its abstractness. It is neither useless nor particularly painful. It is depended on to the\nextent that it is abstract, and it depends on others to the extent that it is concrete.\nThe most desirable position for a component is at one of the two endpoints of the Main Sequence.\nGood architects strive to position the majority of their components at those endpoints. However, in\nmy experience, some small fraction of the components in a large system are neither perfectly abstract\nnor perfectly stable. Those components have the best characteristics if they are on, or close, to the\nMain Sequence.\nDISTANCE FROM THE MAIN SEQUENCE\nThis leads us to our last metric. If it is desirable for components to be on, or close, to the Main\nSequence, then we can create a metric that measures how far away a component is from this ideal.\n\u2022 D3: Distance. D = |A+I\u20131| . The range of this metric is [0, 1]. A value of 0 indicates that the\ncomponent is directly on the Main Sequence. A value of 1 indicates that the component is as far\naway as possible from the Main Sequence.\nGiven this metric, a design can be analyzed for its overall conformance to the Main Sequence. The D\nmetric for each component can be calculated. Any component that has a D value that is not near zero\ncan be reexamined and restructured.\nStatistical analysis of a design is also possible. We can calculate the mean and variance of all the D\nmetrics for the components within a design. We would expect a conforming design to have a mean and\nvariance that are close to zero. The variance can be used to establish \u201ccontrol limits\u201d so as to identify\ncomponents that are \u201cexceptional\u201d in comparison to all the others.\nIn the scatterplot in Figure 14.14, we see that the bulk of the components lie along the Main Sequence,\nbut some of them are more than one standard deviation (Z = 1) away from the mean. These aberrant\ncomponents are worth examining more closely. For some reason, they are either very abstract with\nfew dependents or very concrete with many dependents.\n", "page": 113, "type": "text", "section": "Page 113"}
{"text": "Figure 14.14 Scatterplot of the components\nAnother way to use the metrics is to plot the D metric of each component over time. The graph in\nFigure 14.15 is a mock-up of such a plot. You can see that some strange dependencies have been\ncreeping into the Payroll component over the last few releases. The plot shows a control threshold\nat D = 0.1. The R2.1 point has exceeded this control limit, so it would be worth our while to find out\nwhy this component is so far from the main sequence.\nFigure 14.15 Plot of D for a single component over time\nCONCLUSION\nThe dependency management metrics described in this chapter measure the conformance of a design\nto a pattern of dependency and abstraction that I think is a \u201cgood\u201d pattern. Experience has shown that\ncertain dependencies are good and others are bad. This pattern reflects that experience. However, a\nmetric is not a god; it is merely a measurement against an arbitrary standard. These metrics are\nimperfect, at best, but it is my hope that you find them useful.\n", "page": 114, "type": "text", "section": "Page 114"}
{"text": "1. In previous publications, I used the names Efferent and Afferent couplings (Ce and Ca) for Fan-out and Fan-in, respectively. That\nwas just hubris on my part: I liked the metaphor of the central nervous system.\n2. The author begs the reader\u2019s indulgence for the arrogance of borrowing such an important term from astronomy.\n3. In previous publications, I called this metric D\u2032. I see no reason to continue that practice.\n", "page": 115, "type": "text", "section": "Page 115"}
{"text": "V\nARCHITECTURE\n", "page": 116, "type": "text", "section": "Page 116"}
{"text": "15\nWHAT IS ARCHITECTURE?\nThe word \u201carchitecture\u201d conjures visions of power and mystery. It makes us think of weighty\ndecisions and deep technical prowess. Software architecture is at the pinnacle of technical\nachievement. When we think of a software architect, we think of someone who has power, and who\ncommands respect. What young aspiring software developer has not dreamed of one day becoming a\nsoftware architect?\nBut what is software architecture? What does a software architect do, and when does he or she do it?\nFirst of all, a software architect is a programmer; and continues to be a programmer. Never fall for\nthe lie that suggests that software architects pull back from code to focus on higher-level issues. They\ndo not! Software architects are the best programmers, and they continue to take programming tasks,\nwhile they also guide the rest of the team toward a design that maximizes productivity. Software\narchitects may not write as much code as other programmers do, but they continue to engage in\nprogramming tasks. They do this because they cannot do their jobs properly if they are not\nexperiencing the problems that they are creating for the rest of the programmers.\nThe architecture of a software system is the shape given to that system by those who build it. The\nform of that shape is in the division of that system into components, the arrangement of those\ncomponents, and the ways in which those components communicate with each other.\nThe purpose of that shape is to facilitate the development, deployment, operation, and maintenance of\nthe software system contained within it.\n", "page": 117, "type": "text", "section": "Page 117"}
{"text": "The strategy behind that facilitation is to leave as many options open as possible, for as long as possible.\nPerhaps this statement has surprised you. Perhaps you thought that the goal of software architecture\nwas to make the system work properly. Certainly we want the system to work properly, and certainly\nthe architecture of the system must support that as one of its highest priorities.\nHowever, the architecture of a system has very little bearing on whether that system works. There are\nmany systems out there, with terrible architectures, that work just fine. Their troubles do not lie in\ntheir operation; rather, they occur in their deployment, maintenance, and ongoing development.\nThis is not to say that architecture plays no role in supporting the proper behavior of the system. It\ncertainly does, and that role is critical. But the role is passive and cosmetic, not active or essential.\nThere are few, if any, behavioral options that the architecture of a system can leave open.\nThe primary purpose of architecture is to support the life cycle of the system. Good architecture\nmakes the system easy to understand, easy to develop, easy to maintain, and easy to deploy. The\nultimate goal is to minimize the lifetime cost of the system and to maximize programmer productivity.\nDEVELOPMENT\nA software system that is hard to develop is not likely to have a long and healthy lifetime. So the\narchitecture of a system should make that system easy to develop, for the team(s) who develop it.\nDifferent team structures imply different architectural decisions. On the one hand, a small team of five\ndevelopers can quite effectively work together to develop a monolithic system without well-defined\ncomponents or interfaces. In fact, such a team would likely find the strictures of an architecture\nsomething of an impediment during the early days of development. This is likely the reason why so\nmany systems lack good architecture: They were begun with none, because the team was small and\ndid not want the impediment of a superstructure.\nOn the other hand, a system being developed by five different teams, each of which includes seven\ndevelopers, cannot make progress unless the system is divided into well-defined components with\nreliably stable interfaces. If no other factors are considered, the architecture of that system will likely\nevolve into five components\u2014one for each team.\nSuch a component-per-team architecture is not likely to be the best architecture for deployment,\noperation, and maintenance of the system. Nevertheless, it is the architecture that a group of teams\nwill gravitate toward if they are driven solely by development schedule.\nDEPLOYMENT\nTo be effective, a software system must be deployable. The higher the cost of deployment, the less\nuseful the system is. A goal of a software architecture, then, should be to make a system that can be\neasily deployed with a single action.\n", "page": 118, "type": "text", "section": "Page 118"}
{"text": "Unfortunately, deployment strategy is seldom considered during initial development. This leads to\narchitectures that may make the system easy to develop, but leave it very difficult to deploy.\nFor example, in the early development of a system, the developers may decide to use a \u201cmicro-\nservice architecture.\u201d They may find that this approach makes the system very easy to develop since\nthe component boundaries are very firm and the interfaces relatively stable. However, when it comes\ntime to deploy the system, they may discover that the number of micro-services has become daunting;\nconfiguring the connections between them, and the timing of their initiation, may also turn out to be a\nhuge source of errors.\nHad the architects considered deployment issues early on, they might have decided on fewer\nservices, a hybrid of services and in-process components, and a more integrated means of managing\nthe interconnections.\nOPERATION\nThe impact of architecture on system operation tends to be less dramatic than the impact of\narchitecture on development, deployment, and maintenance. Almost any operational difficulty can be\nresolved by throwing more hardware at the system without drastically impacting the software\narchitecture.\nIndeed, we have seen this happen over and over again. Software systems that have inefficient\narchitectures can often be made to work effectively simply by adding more storage and more servers.\nThe fact that hardware is cheap and people are expensive means that architectures that impede\noperation are not as costly as architectures that impede development, deployment, and maintenance.\nThis is not to say that an architecture that is well tuned to the operation of the system is not desirable.\nIt is! It\u2019s just that the cost equation leans more toward development, deployment, and maintenance.\nHaving said that, there is another role that architecture plays in the operation of the system: A good\nsoftware architecture communicates the operational needs of the system.\nPerhaps a better way to say this is that the architecture of a system makes the operation of the system\nreadily apparent to the developers. Architecture should reveal operation. The architecture of the\nsystem should elevate the use cases, the features, and the required behaviors of the system to first-\nclass entities that are visible landmarks for the developers. This simplifies the understanding of the\nsystem and, therefore, greatly aids in development and maintenance.\nMAINTENANCE\nOf all the aspects of a software system, maintenance is the most costly. The never-ending parade of\nnew features and the inevitable trail of defects and corrections consume vast amounts of human\nresources.\n", "page": 119, "type": "text", "section": "Page 119"}
{"text": "The primary cost of maintenance is in spelunking and risk. Spelunking is the cost of digging through\nthe existing software, trying to determine the best place and the best strategy to add a new feature or\nto repair a defect. While making such changes, the likelihood of creating inadvertent defects is always\nthere, adding to the cost of risk.\nA carefully thought-through architecture vastly mitigates these costs. By separating the system into\ncomponents, and isolating those components through stable interfaces, it is possible to illuminate the\npathways for future features and greatly reduce the risk of inadvertent breakage.\nKEEPING OPTIONS OPEN\nAs we described in an earlier chapter, software has two types of value: the value of its behavior and\nthe value of its structure. The second of these is the greater of the two because it is this value that\nmakes software soft.\nSoftware was invented because we needed a way to quickly and easily change the behavior of\nmachines. But that flexibility depends critically on the shape of the system, the arrangement of its\ncomponents, and the way those components are interconnected.\nThe way you keep software soft is to leave as many options open as possible, for as long as possible.\nWhat are the options that we need to leave open? They are the details that don\u2019t matter.\nAll software systems can be decomposed into two major elements: policy and details. The policy\nelement embodies all the business rules and procedures. The policy is where the true value of the\nsystem lives.\nThe details are those things that are necessary to enable humans, other systems, and programmers to\ncommunicate with the policy, but that do not impact the behavior of the policy at all. They include IO\ndevices, databases, web systems, servers, frameworks, communication protocols, and so forth.\nThe goal of the architect is to create a shape for the system that recognizes policy as the most\nessential element of the system while making the details irrelevant to that policy. This allows\ndecisions about those details to be delayed and deferred.\nFor example:\n\u2022 It is not necessary to choose a database system in the early days of development, because the high-\nlevel policy should not care which kind of database will be used. Indeed, if the architect is careful,\nthe high-level policy will not care if the database is relational, distributed, hierarchical, or just\nplain flat files.\n\u2022 It is not necessary to choose a web server early in development, because the high-level policy\nshould not know that it is being delivered over the web. If the high-level policy is unaware of\nHTML, AJAX, JSP, JSF, or any of the rest of the alphabet soup of web development, then you don\u2019t\nneed to decide which web system to use until much later in the project. Indeed, you don\u2019t even have\nto decide if the system will be delivered over the web.\n", "page": 120, "type": "text", "section": "Page 120"}
{"text": "\u2022 It is not necessary to adopt REST early in development, because the high-level policy should be\nagnostic about the interface to the outside world. Nor is it necessary to adopt a micro-services\nframework, or a SOA framework. Again, the high-level policy should not care about these things.\n\u2022 It is not necessary to adopt a dependency injection framework early in development, because the\nhigh-level policy should not care how dependencies are resolved.\nI think you get the point. If you can develop the high-level policy without committing to the details that\nsurround it, you can delay and defer decisions about those details for a long time. And the longer you\nwait to make those decisions, the more information you have with which to make them properly.\nThis also leaves you the option to try different experiments. If you have a portion of the high-level\npolicy working, and it is agnostic about the database, you could try connecting it to several different\ndatabases to check applicability and performance. The same is true with web systems, web\nframeworks, or even the web itself.\nThe longer you leave options open, the more experiments you can run, the more things you can try, and\nthe more information you will have when you reach the point at which those decisions can no longer\nbe deferred.\nWhat if the decisions have already been made by someone else? What if your company has made a\ncommitment to a certain database, or a certain web server, or a certain framework? A good architect\npretends that the decision has not been made, and shapes the system such that those decisions can\nstill be deferred or changed for as long as possible.\nA good architect maximizes the number of decisions not made.\nDEVICE INDEPENDENCE\nAs an example of this kind of thinking, let\u2019s take a trip back to the 1960s, when computers were\nteenagers and most programmers were mathematicians or engineers from other disciplines (and-one\nthird or more were women).\nIn those days we made a lot of mistakes. We didn\u2019t know they were mistakes at the time, of course.\nHow could we?\nOne of those mistakes was to bind our code directly to the IO devices. If we needed to print\nsomething on a printer, we wrote code that used the IO instructions that would control the printer. Our\ncode was device dependent.\nFor example, when I wrote PDP-8 programs that printed on the teleprinter, I used a set of machine\ninstructions that looked like this:\nClick here to view code image\nPRTCHR, 0\n        TSF\n", "page": 121, "type": "text", "section": "Page 121"}
{"text": "        JMP .-1\n        TLS\n        JMP I PRTCHR\nPRTCHR is a subroutine that prints one character on the teleprinter. The beginning zero was used as the\nstorage for the return address. (Don\u2019t ask.) The TSF instruction skipped the next instruction if the\nteleprinter was ready to print a character. If the teleprinter was busy, then TSF just fell through to the\nJMP .-1 instruction, which just jumped back to the TSF instruction. If the teleprinter was ready, then\nTSF would skip to the TLS instruction, which sent the character in the A register to the teleprinter.\nThen the JMP I PRTCHR instruction returned to the caller.\nAt first this strategy worked fine. If we needed to read cards from the card reader, we used code that\ntalked directly to the card reader. If we needed to punch cards, we wrote code that directly\nmanipulated the punch. The programs worked perfectly. How could we know this was a mistake?\nBut big batches of punched cards are difficult to manage. They can be lost, mutilated, spindled,\nshuffled, or dropped. Individual cards can be lost and extra cards can be inserted. So data integrity\nbecame a significant problem.\nMagnetic tape was the solution. We could move the card images to tape. If you drop a magnetic tape,\nthe records don\u2019t get shuffled. You can\u2019t accidentally lose a record, or insert a blank record simply by\nhanding the tape. The tape is much more secure. It\u2019s also faster to read and write, and it is very easy\nto make backup copies.\nUnfortunately, all our software was written to manipulate card readers and card punches. Those\nprograms had to be rewritten to use magnetic tape. That was a big job.\nBy the late 1960s, we had learned our lesson\u2014and we invented device independence. The operating\nsystems of the day abstracted the IO devices into software functions that handled unit records that\nlooked like cards. The programs would invoke operating system services that dealt with abstract unit-\nrecord devices. Operators could tell the operating system whether those abstract services should be\nconnected to card readers, magnetic tape, or any other unit-record device.\nNow the same program could read and write cards, or read and write tape, without any change. The\nOpen\u2013Closed Principle was born (but not yet named).\nJUNK MAIL\nIn the late 1960s, I worked for a company that printed junk mail for clients. The clients would send us\nmagnetic tapes with unit records containing the names and addresses of their customers, and we\nwould write programs that printed nice personalized advertisements.\nYou know the kind:\nHello Mr. Martin,\n", "page": 122, "type": "text", "section": "Page 122"}
{"text": "Congratulations!\nWe chose YOU from everyone else who lives on Witchwood Lane to participate in our new fantastic\none-time-only offering\u2026\nThe clients would send us huge rolls of form letters with all the words except the name and address,\nand any other element they wanted us to print. We wrote programs that extracted the names,\naddresses, and other elements from the magnetic tape, and printed those elements exactly where they\nneeded to appear on the forms.\nThese rolls of form letters weighed 500 pounds and contained thousands of letters. Clients would\nsend us hundreds of these rolls. We would print each one individually.\nAt first, we had an IBM 360 doing the printing on its sole line printer. We could print a few thousand\nletters per shift. Unfortunately, this tied up a very expensive machine for a very long time. In those\ndays, IBM 360s rented for tens of thousands of dollars per month.\nSo we told the operating system to use magnetic tape instead of the line printer. Our programs didn\u2019t\ncare, because they had been written to use the IO abstractions of the operating system.\nThe 360 could pump out a full tape in 10 minutes or so\u2014enough to print several rolls of form letters.\nThe tapes were taken outside of the computer room and mounted on tape drives connected to offline\nprinters. We had five of them, and we ran those five printers 24 hours per day, seven days per week,\nprinting hundreds of thousands of pieces of junk mail every week.\nThe value of device independence was enormous! We could write our programs without knowing or\ncaring which device would be used. We could test those programs using the local line printer\nconnected to the computer. Then we could tell the operating system to \u201cprint\u201d to magnetic tape and run\noff hundreds of thousands of forms.\nOur programs had a shape. That shape disconnected policy from detail. The policy was the formatting\nof the name and address records. The detail was the device. We deferred the decision about which\ndevice we would use.\nPHYSICAL ADDRESSING\nIn the early 1970s, I worked on a large accounting system for a local truckers union. We had a 25MB\ndisk drive on which we stored records for Agents, Employers, and Members. The different records\nhad different sizes, so we formatted the first few cylinders of the disk so that each sector was just the\nsize of an Agent record. The next few cylinders were formatted to have sectors that fit the Employer\nrecords. The last few cylinders were formatted to fit the Member records.\nWe wrote our software to know the detailed structure of the disk. It knew that the disk had 200\ncylinders and 10 heads, and that each cylinder had several dozen sectors per head. It knew which\ncylinders held the Agents, Employers, and Members. All this was hard-wired into the code.\n", "page": 123, "type": "text", "section": "Page 123"}
{"text": "We kept an index on the disk that allowed us to look up each of the Agents, Employers, and\nMembers. This index was in yet another specially formatted set of cylinders on the disk. The Agent\nindex was composed of records that contained the ID of an agent, and the cylinder number, head\nnumber, and sector number of that Agent record. Employers and Members had similar indices.\nMembers were also kept in a doubly linked list on the disk. Each Member record held the cylinder,\nhead, and sector number of the next Member record, and of the previous Member record.\nWhat would happen if we needed to upgrade to a new disk drive\u2014one with more heads, or one with\nmore cylinders, or one with more sectors per cylinder? We had to write a special program to read in\nthe old data from the old disk, and then write it out to the new disk, translating all of the\ncylinder/head/sector numbers. We also had to change all the hard-wiring in our code\u2014and that hard-\nwiring was everywhere! All the business rules knew the cylinder/head/sector scheme in detail.\nOne day a more experienced programmer joined our ranks. When he saw what we had done, the\nblood drained from his face, and he stared aghast at us, as if we were aliens of some kind. Then he\ngently advised us to change our addressing scheme to use relative addresses.\nOur wiser colleague suggested that we consider the disk to be one huge linear array of sectors, each\naddressable by a sequential integer. Then we could write a little conversion routine that knew the\nphysical structure of the disk, and could translate the relative address to a cylinder/head/sector\nnumber on the fly.\nFortunately for us, we took his advice. We changed the high-level policy of the system to be agnostic\nabout the physical structure of the disk. That allowed us to decouple the decision about disk drive\nstructure from the application.\nCONCLUSION\nThe two stories in this chapter are examples, in the small, of a principle that architects employ in the\nlarge. Good architects carefully separate details from policy, and then decouple the policy from the\ndetails so thoroughly that the policy has no knowledge of the details and does not depend on the\ndetails in any way. Good architects design the policy so that decisions about the details can be\ndelayed and deferred for as long as possible.\n", "page": 124, "type": "text", "section": "Page 124"}
{"text": "16\nINDEPENDENCE\nAs we previously stated, a good architecture must support:\n\u2022 The use cases and operation of the system.\n\u2022 The maintenance of the system.\n\u2022 The development of the system.\n\u2022 The deployment of the system.\nUSE CASES\nThe first bullet\u2014use cases\u2014means that the architecture of the system must support the intent of the\nsystem. If the system is a shopping cart application, then the architecture must support shopping cart\nuse cases. Indeed, this is the first concern of the architect, and the first priority of the architecture. The\narchitecture must support the use cases.\nHowever, as we discussed previously, architecture does not wield much influence over the behavior\nof the system. There are very few behavioral options that the architecture can leave open. But\ninfluence isn\u2019t everything. The most important thing a good architecture can do to support behavior is\nto clarify and expose that behavior so that the intent of the system is visible at the architectural level.\nA shopping cart application with a good architecture will look like a shopping cart application. The\n", "page": 125, "type": "text", "section": "Page 125"}
{"text": "use cases of that system will be plainly visible within the structure of that system. Developers will\nnot have to hunt for behaviors, because those behaviors will be first-class elements visible at the top\nlevel of the system. Those elements will be classes or functions or modules that have prominent\npositions within the architecture, and they will have names that clearly describe their function.\nChapter 21, \u201cScreaming Architecture,\u201d will make this point much clearer.\nOPERATION\nArchitecture plays a more substantial, and less cosmetic, role in supporting the operation of the\nsystem. If the system must handle 100,000 customers per second, the architecture must support that\nkind of throughput and response time for each use case that demands it. If the system must query big\ndata cubes in milliseconds, then the architecture must be structured to allow this kind of operation.\nFor some systems, this will mean arranging the processing elements of the system into an array of\nlittle services can be run in parallel on many different servers. For other systems, it will mean a\nplethora of little lightweight threads sharing the address space of a single process within a single\nprocessor. Still other systems will need just a few processes running in isolated address spaces. And\nsome systems can even survive as simple monolithic programs running in a single process.\nAs strange as it may seem, this decision is one of the options that a good architect leaves open. A\nsystem that is written as a monolith, and that depends on that monolithic structure, cannot easily be\nupgraded to multiple processes, multiple threads, or micro-services should the need arise. By\ncomparison, an architecture that maintains the proper isolation of its components, and does not\nassume the means of communication between those components, will be much easier to transition\nthrough the spectrum of threads, processes, and services as the operational needs of the system change\nover time.\nDEVELOPMENT\nArchitecture plays a significant role in supporting the development environment. This is where\nConway\u2019s law comes into play. Conway\u2019s law says:\nAny organization that designs a system will produce a design whose structure is a copy of the organization\u2019s\ncommunication structure.\nA system that must be developed by an organization with many teams and many concerns must have an\narchitecture that facilitates independent actions by those teams, so that the teams do not interfere with\neach other during development. This is accomplished by properly partitioning the system into well-\nisolated, independently developable components. Those components can then be allocated to teams\nthat can work independently of each other.\nDEPLOYMENT\n", "page": 126, "type": "text", "section": "Page 126"}
{"text": "The architecture also plays a huge role in determining the ease with which the system is deployed.\nThe goal is \u201cimmediate deployment.\u201d A good architecture does not rely on dozens of little\nconfiguration scripts and property file tweaks. It does not require manual creation of directories or\nfiles that must be arranged just so. A good architecture helps the system to be immediately deployable\nafter build.\nAgain, this is achieved through the proper partitioning and isolation of the components of the system,\nincluding those master components that tie the whole system together and ensure that each component\nis properly started, integrated, and supervised.\nLEA\nVING OPTIONS OPEN\nA good architecture balances all of these concerns with a component structure that mutually satisfies\nthem all. Sounds easy, right? Well, it\u2019s easy for me to write that.\nThe reality is that achieving this balance is pretty hard. The problem is that most of the time we don\u2019t\nknow what all the use cases are, nor do we know the operational constraints, the team structure, or the\ndeployment requirements. Worse, even if we did know them, they will inevitably change as the system\nmoves through its life cycle. In short, the goals we must meet are indistinct and inconstant. Welcome\nto the real world.\nBut all is not lost: Some principles of architecture are relatively inexpensive to implement and can\nhelp balance those concerns, even when you don\u2019t have a clear picture of the targets you have to hit.\nThose principles help us partition our systems into well-isolated components that allow us to leave as\nmany options open as possible, for as long as possible.\nA good architecture makes the system easy to change, in all the ways that it must change, by leaving\noptions open.\nDECOUPLING LAYERS\nConsider the use cases. The architect wants the structure of the system to support all the necessary use\ncases, but does not know what all those use cases are. However, the architect does know the basic\nintent of the system. It\u2019s a shopping cart system, or it\u2019s a bill of materials system, or it\u2019s an order\nprocessing system. So the architect can employ the Single Responsibility Principle and the Common\nClosure Principle to separate those things that change for different reasons, and to collect those things\nthat change for the same reasons\u2014given the context of the intent of the system.\nWhat changes for different reasons? There are some obvious things. User interfaces change for\nreasons that have nothing to do with business rules. Use cases have elements of both. Clearly, then, a\ngood architect will want to separate the UI portions of a use case from the business rule portions in\nsuch a way that they can be changed independently of each other, while keeping those use cases\nvisible and clear.\n", "page": 127, "type": "text", "section": "Page 127"}
{"text": "Business rules themselves may be closely tied to the application, or they may be more general. For\nexample, the validation of input fields is a business rule that is closely tied to the application itself. In\ncontrast, the calculation of interest on an account and the counting of inventory are business rules that\nare more closely associated with the domain. These two different kinds of rules will change at\ndifferent rates, and for different reasons\u2014so they should be separated so that they can be\nindependently changed.\nThe database, the query language, and even the schema are technical details that have nothing to do\nwith the business rules or the UI. They will change at rates, and for reasons, that are independent of\nother aspects of the system. Consequently, the architecture should separate them from the rest of the\nsystem so that they can be independently changed.\nThus we find the system divided into decoupled horizontal layers\u2014the UI, application-specific\nbusiness rules, application-independent business rules, and the database, just to mention a few.\nDECOUPLING USE CASES\nWhat else changes for different reasons? The use cases themselves! The use case for adding an order\nto an order entry system almost certainly will change at a different rate, and for different reasons, than\nthe use case that deletes an order from the system. Use cases are a very natural way to divide the\nsystem.\nAt the same time, use cases are narrow vertical slices that cut through the horizontal layers of the\nsystem. Each use case uses some UI, some application-specific business rules, some application-\nindependent business rules, and some database functionality. Thus, as we are dividing the system in to\nhorizontal layers, we are also dividing the system into thin vertical use cases that cut through those\nlayers.\nTo achieve this decoupling, we separate the UI of the add-order use case from the UI of the delete-\norder use case. We do the same with the business rules, and with the database. We keep the use cases\nseparate down the vertical height of the system.\nYou can see the pattern here. If you decouple the elements of the system that change for different\nreasons, then you can continue to add new use cases without interfering with old ones. If you also\ngroup the UI and database in support of those use cases, so that each use case uses a different aspect\nof the UI and database, then adding new use cases will be unlikely to affect older ones.\nDECOUPLING MODE\nNow think of what all that decoupling means for the second bullet: operations. If the different aspects\nof the use cases are separated, then those that must run at a high throughput are likely already\nseparated from those that must run at a low throughput. If the UI and the database have been separated\nfrom the business rules, then they can run in different servers. Those that require higher bandwidth\ncan be replicated in many servers.\n", "page": 128, "type": "text", "section": "Page 128"}
{"text": "In short, the decoupling that we did for the sake of the use cases also helps with operations. However,\nto take advantage of the operational benefit, the decoupling must have the appropriate mode. To run in\nseparate servers, the separated components cannot depend on being together in the same address\nspace of a processor. They must be independent services, which communicate over a network of\nsome kind.\nMany architects call such components \u201cservices\u201d or \u201cmicro-services,\u201d depending upon some vague\nnotion of line count. Indeed, an architecture based on services is often called a service-oriented\narchitecture.\nIf that nomenclature set off some alarm bells in your mind, don\u2019t worry. I\u2019m not going to tell you that\nSoA is the best possible architecture, or that micro-services are the wave of the future. The point\nbeing made here is that sometimes we have to separate our components all the way to the service\nlevel.\nRemember, a good architecture leaves options open. The decoupling mode is one of those options.\nBefore we explore that topic further, let\u2019s look to the other two bullets.\nINDEPENDENT DEVELOP-ABILITY\nThe third bullet was development. Clearly when components are strongly decoupled, the interference\nbetween teams is mitigated. If the business rules don\u2019t know about the UI, then a team that focuses on\nthe UI cannot much affect a team that focuses on the business rules. If the use cases themselves are\ndecoupled from one another, then a team that focuses on the addOrder use case is not likely to\ninterfere with a team that focuses on the deleteOrder use case.\nSo long as the layers and use cases are decoupled, the architecture of the system will support the\norganization of the teams, irrespective of whether they are organized as feature teams, component\nteams, layer teams, or some other variation.\nINDEPENDENT DEPLOYABILITY\nThe decoupling of the use cases and layers also affords a high degree of flexibility in deployment.\nIndeed, if the decoupling is done well, then it should be possible to hot-swap layers and use cases in\nrunning systems. Adding a new use case could be a simple as adding a few new jar files or services\nto the system while leaving the rest alone.\nDUPLICATION\nArchitects often fall into a trap\u2014a trap that hinges on their fear of duplication.\nDuplication is generally a bad thing in software. We don\u2019t like duplicated code. When code is truly\n", "page": 129, "type": "text", "section": "Page 129"}
{"text": "duplicated, we are honor-bound as professionals to reduce and eliminate it.\nBut there are different kinds of duplication. There is true duplication, in which every change to one\ninstance necessitates the same change to every duplicate of that instance. Then there is false or\naccidental duplication. If two apparently duplicated sections of code evolve along different paths\u2014if\nthey change at different rates, and for different reasons\u2014then they are not true duplicates. Return to\nthem in a few years, and you\u2019ll find that they are very different from each other.\nNow imagine two use cases that have very similar screen structures. The architects will likely be\nstrongly tempted to share the code for that structure. But should they? Is that true duplication? Or it is\naccidental?\nMost likely it is accidental. As time goes by, the odds are that those two screens will diverge and\neventually look very different. For this reason, care must be taken to avoid unifying them. Otherwise,\nseparating them later will be a challenge.\nWhen you are vertically separating use cases from one another, you will run into this issue, and your\ntemptation will be to couple the use cases because they have similar screen structures, or similar\nalgorithms, or similar database queries and/or schemas. Be careful. Resist the temptation to commit\nthe sin of knee-jerk elimination of duplication. Make sure the duplication is real.\nBy the same token, when you are separating layers horizontally, you might notice that the data\nstructure of a particular database record is very similar to the data structure of a particular screen\nview. You may be tempted to simply pass the database record up to the UI, rather than to create a\nview model that looks the same and copy the elements across. Be careful: This duplication is almost\ncertainly accidental. Creating the separate view model is not a lot of effort, and it will help you keep\nthe layers properly decoupled.\nDECOUPLING MODES (AGAIN)\nBack to modes. There are many ways to decouple layers and use cases. They can be decoupled at the\nsource code level, at the binary code (deployment) level, and at the execution unit (service) level.\n\u2022 Source level. We can control the dependencies between source code modules so that changes to one\nmodule do not force changes or recompilation of others (e.g., Ruby Gems).\nIn this decoupling mode the components all execute in the same address space, and communicate\nwith each other using simple function calls. There is a single executable loaded into computer\nmemory. People often call this a monolithic structure.\n\u2022 Deployment level. We can control the dependencies between deployable units such as jar files,\nDLLs, or shared libraries, so that changes to the source code in one module do not force others to\nbe rebuilt and redeployed.\nMany of the components may still live in the same address space, and communicate through function\ncalls. Other components may live in other processes in the same processor, and communicate\nthrough interprocess communications, sockets, or shared memory. The important thing here is that\n", "page": 130, "type": "text", "section": "Page 130"}
{"text": "the decoupled components are partitioned into independently deployable units such as jar files,\nGem files, or DLLs.\n\u2022 Service level. We can reduce the dependencies down to the level of data structures, and\ncommunicate solely through network packets such that every execution unit is entirely independent\nof source and binary changes to others (e.g., services or micro-services).\nWhat is the best mode to use?\nThe answer is that it\u2019s hard to know which mode is best during the early phases of a project. Indeed,\nas the project matures, the optimal mode may change.\nFor example, it\u2019s not difficult to imagine that a system that runs comfortably on one server right now\nmight grow to the point where some of its components ought to run on separate servers. While the\nsystem runs on a single server, the source-level decoupling might be sufficient. Later, however, it\nmight require decoupling into deployable units, or even services.\nOne solution (which seems to be popular at the moment) is to simply decouple at the service level by\ndefault. A problem with this approach is that it is expensive and encourages coarse-grained\ndecoupling. No matter how \u201cmicro\u201d the micro-services get, the decoupling is not likely to be fine-\ngrained enough.\nAnother problem with service-level decoupling is that it is expensive, both in development time and\nin system resources. Dealing with service boundaries where none are needed is a waste of effort,\nmemory, and cycles. And, yes, I know that the last two are cheap\u2014but the first is not.\nMy preference is to push the decoupling to the point where a service could be formed. should it\nbecome necessary; but then to leave the components in the same address space as long as possible.\nThis leaves the option for a service open.\nWith this approach, initially the components are separated at the source code level. That may be good\nenough for the duration of the project\u2019s lifetime. If, however, deployment or development issues arise,\ndriving some of the decoupling to a deployment level may be sufficient\u2014at least for a while.\nAs the development, deployment, and operational issues increase, I carefully choose which\ndeployable units to turn into services, and gradually shift the system in that direction.\nOver time, the operational needs of the system may decline. What once required decoupling at the\nservice level may now require only deployment-level or even source-level decoupling.\nA good architecture will allow a system to be born as a monolith, deployed in a single file, but then\nto grow into a set of independently deployable units, and then all the way to independent services\nand/or micro-services. Later, as things change, it should allow for reversing that progression and\nsliding all the way back down into a monolith.\nA good architecture protects the majority of the source code from those changes. It leaves the\ndecoupling mode open as an option so that large deployments can use one mode, whereas small\n", "page": 131, "type": "text", "section": "Page 131"}
{"text": "deployments can use another.\nCONCLUSION\nYes, this is tricky. And I\u2019m not saying that the change of decoupling modes should be a trivial\nconfiguration option (though sometimes that is appropriate). What I\u2019m saying is that the decoupling\nmode of a system is one of those things that is likely to change with time, and a good architect\nforesees and appropriately facilitates those changes.\n", "page": 132, "type": "text", "section": "Page 132"}
{"text": "17\nBOUNDARIES: DRAWING LINES\nSoftware architecture is the art of drawing lines that I call boundaries. Those boundaries separate\nsoftware elements from one another, and restrict those on one side from knowing about those on the\nother. Some of those lines are drawn very early in a project\u2019s life\u2014even before any code is written.\nOthers are drawn much later. Those that are drawn early are drawn for the purposes of deferring\ndecisions for as long as possible, and of keeping those decisions from polluting the core business\nlogic.\nRecall that the goal of an architect is to minimize the human resources required to build and maintain\nthe required system. What it is that saps this kind of people-power? Coupling\u2014and especially\ncoupling to premature decisions.\nWhich kinds of decisions are premature? Decisions that have nothing to do with the business\nrequirements\u2014the use cases\u2014of the system. These include decisions about frameworks, databases,\nweb servers, utility libraries, dependency injection, and the like. A good system architecture is one in\nwhich decisions like these are rendered ancillary and deferrable. A good system architecture does not\ndepend on those decisions. A good system architecture allows those decisions to be made at the latest\npossible moment, without significant impact.\nA COUPLE OF SAD STORIES\n", "page": 133, "type": "text", "section": "Page 133"}
{"text": "Here\u2019s the sad story of company P, which serves as a warning about making premature decisions. In\nthe 1980s the founders of P wrote a simple monolithic desktop application. They enjoyed a great deal\nof success and grew the product through the 1990s into a popular and successful desktop GUI\napplication.\nBut then, in the late 1990s, the web emerged as a force. Suddenly everybody had to have a web\nsolution, and P was no exception. P\u2019s customers clamored for a version of the product on the web. To\nmeet this demand, the company hired a bunch of hotshot twenty-something Java programmers and\nembarked upon a project to webify their product.\nThe Java guys had dreams of server farms dancing in their heads, so they adopted a rich three-tiered\n\u201carchitecture\u201d1 that they could distribute through such farms. There would be servers for the GUI,\nservers for the middleware, and servers for the database. Of course.\nThe programmers decided, very early on, that all domain objects would have three instantiations: one\nin the GUI tier, one in the middleware tier, and one in the database tier. Since these instantiations\nlived on different machines, a rich system of interprocessor and inter-tier communications was set up.\nMethod invocations between tiers were converted to objects, serialized, and marshaled across the\nwire.\nNow imagine what it took to implement a simple feature like adding a new field to an existing record.\nThat field had to be added to the classes in all three tiers, and to several of the inter-tier messages.\nSince data traveled in both directions, four message protocols needed to be designed. Each protocol\nhad a sending and receiving side, so eight protocol handlers were required. Three executables had to\nbe built, each with three updated business objects, four new messages, and eight new handlers.\nAnd think of what those executables had to do to implement the simplest of features. Think of all the\nobject instantiations, all the serializations, all the marshaling and de-marshaling, all the building and\nparsing of messages, all the socket communications, timeout managers, retry scenarios, and all the\nother extra stuff that you have to do just to get one simple thing done.\nOf course, during development the programmers did not have a server farm. Indeed, they simply ran\nall three executables in three different processes on a single machine. They developed this way for\nseveral years. But they were convinced that their architecture was right. And so, even though they\nwere executing in a single machine, they continued all the object instantiations, all the serializations,\nall the marshaling and de-marshaling, all the building and parsing of messages, all the socket\ncommunications, and all the extra stuff in a single machine.\nThe irony is that company P never sold a system that required a server farm. Every system they ever\ndeployed was a single server. And in that single server all three executables continued all the object\ninstantiations, all the serializations, all the marshaling and de-marshaling, all the building and parsing\nof messages, all the socket communications, and all the extra stuff, in anticipation of a server farm that\nnever existed, and never would.\nThe tragedy is that the architects, by making a premature decision, multiplied the development effort\nenormously.\n", "page": 134, "type": "text", "section": "Page 134"}
{"text": "The story of P is not isolated. I\u2019ve seen it many times and in many places. Indeed, P is a superposition\nof all those places.\nBut there are worse fates than P.\nConsider W, a local business that manages fleets of company cars. They recently hired an \u201cArchitect\u201d\nto get their rag-tag software effort under control. And, let me tell you, control was this guy\u2019s middle\nname. He quickly realized that what this little operation needed was a full-blown, enterprise-scale,\nservice-oriented \u201cARCHITECTURE.\u201d He created a huge domain model of all the different\n\u201cobjects\u201d in the business, designed a suite of services to manage these domain objects, and put all the\ndevelopers on a path to Hell. As a simple example, suppose you wanted to add the name, address,\nand phone number of a contact person to a sales record. You had to go to the ServiceRegistry and\nask for the service ID of the ContactService. Then you had to send a CreateContact message to\nthe ContactService. Of course, this message had dozens of fields that all had to have valid data in\nthem\u2014data to which the programmer had no access, since all the programmer had was a name,\naddress, and phone number. After faking the data, the programmer had to jam the ID of the newly\ncreated contact into the sales record and send the UpdateContact message to the\nSaleRecordService.\nOf course, to test anything you had to fire up all the necessary services, one by one, and fire up the\nmessage bus, and the BPel server, and \u2026 And then, there were the propagation delays as these\nmessages bounced from service to service, and waited in queue after queue.\nAnd then if you wanted to add a new feature\u2014well, you can imagine the coupling between all those\nservices, and the sheer volume of WSDLs that needed changing, and all the redeployments those\nchanges necessitated \u2026\nHell starts to seem like a nice place by comparison.\nThere\u2019s nothing intrinsically wrong with a software system that is structured around services. The\nerror at W was the premature adoption and enforcement of a suite of tools that promised SoA\u2014that\nis, the premature adoption of a massive suite of domain object services. The cost of those errors was\nsheer person-hours\u2014person-hours in droves\u2014flushed down the SoA vortex.\nI could go on describing one architectural failure after another. But let\u2019s talk about an architectural\nsuccess instead.\nFITNESSE\nMy Son, Micah, and I started work on FitNesse in 2001. The idea was to create a simple wiki that\nwrapped Ward Cunningham\u2019s FIT tool for writing acceptance tests.\nThis was back in the days before Maven \u201csolved\u201d the jar file problem. I was adamant that anything\nwe produced should not require people to download more than one jar file. I called this rule,\n\u201cDownload and Go.\u201d This rule drove many of our decisions.\n", "page": 135, "type": "text", "section": "Page 135"}
{"text": "One of the first decisions was to write our own web server, specific to the needs of FitNesse. This\nmight sound absurd. Even in 2001 there were plenty of open source web servers that we could have\nused. Yet writing our own turned out to be a really good decision because a bare-bones web server is\na very simple piece of software to write and it allowed us to postpone any web framework decision\nuntil much later.2\nAnother early decision was to avoid thinking about a database. We had MySQL in the back of our\nminds, but we purposely delayed that decision by employing a design that made the decision\nirrelevant. That design was simply to put an interface between all data accesses and the data\nrepository itself.\nWe put the data access methods into an interface named WikiPage. Those methods provided all the\nfunctionality we needed to find, fetch, and save pages. Of course, we didn\u2019t implement those methods\nat first; we simply stubbed them out while we worked on features that didn\u2019t involve fetching and\nsaving the data.\nIndeed, for three months we simply worked on translating wiki text into HTML. This didn\u2019t require\nany kind of data storage, so we created a class named MockWikiPage that simply left the data access\nmethods stubbed.\nEventually, those stubs became insufficient for the features we wanted to write. We needed real data\naccess, not stubs. So we created a new derivative of WikiPage named InMemoryPage. This\nderivative implemented the data access method to manage a hash table of wiki pages, which we kept\nin RAM.\nThis allowed us to write feature after feature for a full year. In fact, we got the whole first version of\nthe FitNesse program working this way. We could create pages, link to other pages, do all the fancy\nwiki formatting, and even run tests with FIT. What we couldn\u2019t do was save any of our work.\nWhen it came time to implement persistence, we thought again about MySQL, but decided that wasn\u2019t\nnecessary in the short term, because it would be really easy to write the hash tables out to flat files.\nSo we implemented FileSystemWikiPage, which just moved the functionality out to flat files, and\nthen we continued developing more features.\nThree months later, we reached the conclusion that the flat file solution was good enough; we decided\nto abandon the idea of MySQL altogether. We deferred that decision into nonexistence and never\nlooked back.\nThat would be the end of the story if it weren\u2019t for one of our customers who decided that he needed\nto put the wiki into MySQL for his own purposes. We showed him the architecture of WikiPages that\nhad allowed us to defer the decision. He came back a day later with the whole system working in\nMySQL. He simply wrote a MySqlWikiPage derivative and got it working.\nWe used to bundle that option with FitNesse, but nobody else ever used it, so eventually we dropped\nit. Even the customer who wrote the derivative eventually dropped it.\n", "page": 136, "type": "text", "section": "Page 136"}
{"text": "Early in the development of FitNesse, we drew a boundary line between business rules and\ndatabases. That line prevented the business rules from knowing anything at all about the database,\nother than the simple data access methods. That decision allowed us to defer the choice and\nimplementation of the database for well over a year. It allowed us to try the file system option, and it\nallowed us to change direction when we saw a better solution. Yet it did not prevent, or even impede,\nmoving in the original direction (MySQL) when someone wanted it.\nThe fact that we did not have a database running for 18 months of development meant that, for 18\nmonths, we did not have schema issues, query issues, database server issues, password issues,\nconnection time issues, and all the other nasty issues that raise their ugly heads when you fire up a\ndatabase. It also meant that all our tests ran fast, because there was no database to slow them down.\nIn short, drawing the boundary lines helped us delay and defer decisions, and it ultimately saved us\nan enormous amount of time and headaches. And that\u2019s what a good architecture should do.\nWHICH LINES DO YOU DRAW, AND WHEN DO\nYOU DRAW THEM?\nYou draw lines between things that matter and things that don\u2019t. The GUI doesn\u2019t matter to the\nbusiness rules, so there should be a line between them. The database doesn\u2019t matter to the GUI, so\nthere should be a line between them. The database doesn\u2019t matter to the business rules, so there\nshould be a line between them.\nSome of you may have rejected one or more of those statements, especially the part about the business\nrules not caring about the database. Many of us have been taught to believe that the database is\ninextricably connected to the business rules. Some of us have even been convinced that the database\nis the embodiment of the business rules.\nBut, as we shall see in another chapter, this idea is misguided. The database is a tool that the business\nrules can use indirectly. The business rules don\u2019t need to know about the schema, or the query\nlanguage, or any of the other details about the database. All the business rules need to know is that\nthere is a set of functions that can be used to fetch or save data. This allows us to put the database\nbehind an interface.\nYou can see this clearly in Figure 17.1. The BusinessRules use the DatabaseInterface to load\nand save data. The DatabaseAccess implements the interface and directs the operation of the actual\nDatabase.\n", "page": 137, "type": "text", "section": "Page 137"}
{"text": "Figure 17.1 The database behind an interface\nThe classes and interfaces in this diagram are symbolic. In a real application, there would be many\nbusiness rule classes, many database interface classes, and many database access implementations.\nAll of them, though, would follow roughly the same pattern.\nWhere is the boundary line? The boundary is drawn across the inheritance relationship, just below\nthe DatabaseInterface (Figure 17.2).\nFigure 17.2 The boundary line\nNote the two arrows leaving the DatabaseAccess class. Those two arrows point away from the\nDatabaseAccess class. That means that none of these classes knows that the DatabaseAccess class\nexists.\nNow let\u2019s pull back a bit. We\u2019ll look at the component that contains many business rules, and the\ncomponent that contains the database and all its access classes (Figure 17.3).\n", "page": 138, "type": "text", "section": "Page 138"}
{"text": "Figure 17.3 The business rules and database components\nNote the direction of the arrow. The Database knows about the BusinessRules. The\nBusinessRules do not know about the Database. This implies that the DatabaseInterface classes\nlive in the BusinessRules component, while the DatabaseAccess classes live in the Database\ncomponent.\nThe direction of this line is important. It shows that the Database does not matter to the\nBusinessRules, but the Database cannot exist without the BusinessRules.\nIf that seems strange to you, just remember this point: The Database component contains the code that\ntranslates the calls made by the BusinessRules into the query language of the database. It is that\ntranslation code that knows about the BusinessRules.\nHaving drawn this boundary line between the two components, and having set the direction of the\narrow toward the BusinessRules, we can now see that the BusinessRules could use any kind of\ndatabase. The Database component could be replaced with many different implementations\u2014the\nBusinessRules don\u2019t care.\nThe database could be implemented with Oracle, or MySQL, or Couch, or Datomic, or even flat files.\nThe business rules don\u2019t care at all. And that means that the database decision can be deferred and\nyou can focus on getting the business rules written and tested before you have to make the database\ndecision.\nWHAT ABOUT INPUT AND OUTPUT?\nDevelopers and customers often get confused about what the system is. They see the GUI, and think\nthat the GUI is the system. They define a system in terms of the GUI, so they believe that they should\nsee the GUI start working immediately. They fail to realize a critically important principle: The IO is\nirrelevant.\n", "page": 139, "type": "text", "section": "Page 139"}
{"text": "This may be hard to grasp at first. We often think about the behavior of the system in terms of the\nbehavior of the IO. Consider a video game, for example. Your experience is dominated by the\ninterface: the screen, the mouse, the buttons, and the sounds. You forget that behind that interface there\nis a model\u2014a sophisticated set of data structures and functions\u2014driving it. More importantly, that\nmodel does not need the interface. It would happily execute its duties, modeling all the events in the\ngame, without the game ever being displayed on the screen. The interface does not matter to the model\n\u2014the business rules.\nAnd so, once again, we see the GUI and BusinessRules components separated by a boundary line\n(Figure 17.4). Once again, we see that the less relevant component depends on the more relevant\ncomponent. The arrows show which component knows about the other and, therefore, which\ncomponent cares about the other. The GUI cares about the BusinessRules.\nFigure 17.4 The boundary between GUI and BusinessRules components\nHaving drawn this boundary and this arrow, we can now see that the GUI could be replaced with any\nother kind of interface\u2014and the BusinessRules would not care.\nPLUGIN ARCHITECTURE\nTaken together, these two decisions about the database and the GUI create a kind of pattern for the\naddition of other components. That pattern is the same pattern that is used by systems that allow third-\nparty plugins.\nIndeed, the history of software development technology is the story of how to conveniently create\nplugins to establish a scalable and maintainable system architecture. The core business rules are kept\nseparate from, and independent of, those components that are either optional or that can be\nimplemented in many different forms (Figure 17.5).\n", "page": 140, "type": "text", "section": "Page 140"}
{"text": "Figure 17.5 Plugging in to the business rules\nBecause the user interface in this design is considered to be a plugin, we have made it possible to\nplug in many different kinds of user interfaces. They could be web based, client/server based, SOA\nbased, Console based, or based on any other kind of user interface technology.\nThe same is true of the database. Since we have chosen to treat it as a plugin, we can replace it with\nany of the various SQL databases, or a NOSQL database, or a file system-based database, or any\nother kind of database technology we might deem necessary in the future.\nThese replacements might not be trivial. If the initial deployment of our system was web-based, then\nwriting the plugin for a client-server UI could be challenging. It is likely that some of the\ncommunications between the business rules and the new UI would have to be reworked. Even so, by\nstarting with the presumption of a plugin structure, we have at very least made such a change\npractical.\nTHE PLUGIN ARGUMENT\nConsider the relationship between ReSharper and Visual Studio. These components are produced by\ncompletely different development teams in completely different companies. Indeed, JetBrains, the\nmaker of ReSharper, lives in Russia. Microsoft, of course, resides in Redmond, Washington. It\u2019s hard\nto imagine two development teams that are more separate.\nWhich team can damage the other? Which team is immune to the other? The dependency structure tells\nthe story (Figure 17.6). The source code of ReSharper depends on the source code of Visual Studio.\nThus there is nothing that the ReSharper team can do to disturb the Visual Studio team. But the Visual\nStudio team could completely disable the ReSharper team if they so desired.\n", "page": 141, "type": "text", "section": "Page 141"}
{"text": "Figure 17.6 ReSharper depends on Visual Studio\nThat\u2019s a deeply asymmetric relationship, and it is one that we desire to have in our own systems. We\nwant certain modules to be immune to others. For example, we don\u2019t want the business rules to break\nwhen someone changes the format of a web page, or changes the schema of the database. We don\u2019t\nwant changes in one part of the system to cause other unrelated parts of the system to break. We don\u2019t\nwant our systems to exhibit that kind of fragility.\nArranging our systems into a plugin architecture creates firewalls across which changes cannot\npropagate. If the GUI plugs in to the business rules, then changes in the GUI cannot affect those\nbusiness rules.\nBoundaries are drawn where there is an axis of change. The components on one side of the boundary\nchange at different rates, and for different reasons, than the components on the other side of the\nboundary.\nGUIs change at different times and at different rates than business rules, so there should be a\nboundary between them. Business rules change at different times and for different reasons than\ndependency injection frameworks, so there should be a boundary between them.\nThis is simply the Single Responsibility Principle again. The SRP tells us where to draw our\nboundaries.\nCONCLUSION\nTo draw boundary lines in a software architecture, you first partition the system into components.\nSome of those components are core business rules; others are plugins that contain necessary functions\nthat are not directly related to the core business. Then you arrange the code in those components such\nthat the arrows between them point in one direction\u2014toward the core business.\nYou should recognize this as an application of the Dependency Inversion Principle and the Stable\n", "page": 142, "type": "text", "section": "Page 142"}
{"text": "Abstractions Principle. Dependency arrows are arranged to point from lower-level details to higher-\nlevel abstractions.\n1. The word \u201carchitecture\u201d appears in quotes here because three-tier is not an architecture; it\u2019s a topology. It\u2019s exactly the kind of\ndecision that a good architecture strives to defer.\n2. Many years later we were able to slip the Velocity framework into FitNesse.\n", "page": 143, "type": "text", "section": "Page 143"}
{"text": "18\nBOUNDARY ANATOMY\nThe architecture of a system is defined by a set of software components and the boundaries that\nseparate them. Those boundaries come in many different forms. In this chapter we\u2019ll look at some of\nthe most common.\nBOUNDARY CROSSING\nAt runtime, a boundary crossing is nothing more than a function on one side of the boundary calling a\nfunction on the other side and passing along some data. The trick to creating an appropriate boundary\ncrossing is to manage the source code dependencies.\nWhy source code? Because when one source code module changes, other source code modules may\nhave to be changed or recompiled, and then redeployed. Managing and building firewalls against this\nchange is what boundaries are all about.\nTHE DREADED MONOLITH\nThe simplest and most common of the architectural boundaries has no strict physical representation. It\nis simply a disciplined segregation of functions and data within a single processor and a single\naddress space. In a previous chapter, I called this the source-level decoupling mode.\n", "page": 144, "type": "text", "section": "Page 144"}
{"text": "From a deployment point of view, this amounts to nothing more than a single executable file\u2014the so-\ncalled monolith. This file might be a statically linked C or C++ project, a set of Java class files\nbound together into an executable jar file, a set of .NET binaries bound into a single .EXE file, and so\non.\nThe fact that the boundaries are not visible during the deployment of a monolith does not mean that\nthey are not present and meaningful. Even when statically linked into a single executable, the ability\nto independently develop and marshal the various components for final assembly is immensely\nvaluable.\nSuch architectures almost always depend on some kind of dynamic polymorphism1 to manage their\ninternal dependencies. This is one of the reasons that object-oriented development has become such\nan important paradigm in recent decades. Without OO, or an equivalent form of polymorphism,\narchitects must fall back on the dangerous practice of using pointers to functions to achieve the\nappropriate decoupling. Most architects find prolific use of pointers to functions to be too risky, so\nthey are forced to abandon any kind of component partitioning.\nThe simplest possible boundary crossing is a function call from a low-level client to a higher-level\nservice. Both the runtime dependency and the compile-time dependency point in the same direction,\ntoward the higher-level component.\nIn Figure 18.1, the flow of control crosses the boundary from left to right. The Client calls function\nf() on the Service. It passes along an instance of Data. The <DS> marker simply indicates a data\nstructure. The Data may be passed as a function argument or by some other more elaborate means.\nNote that the definition of the Data is on the called side of the boundary.\nFigure 18.1 Flow of control crosses the boundary from a lower level to a higher level\nWhen a high-level client needs to invoke a lower-level service, dynamic polymorphism is used to\ninvert the dependency against the flow of control. The runtime dependency opposes the compile-time\ndependency.\nIn Figure 18.2, the flow of control crosses the boundary from left to right as before. The high-level\n", "page": 145, "type": "text", "section": "Page 145"}
{"text": "Client calls the f() function of the lower-level ServiceImpl through the Service interface. Note,\nhowever, that all dependencies cross the boundary from right to left toward the higher-level\ncomponent. Note, also, that the definition of the data structure is on the calling side of the boundary.\nFigure 18.2 Crossing the boundary against the flow of control\nEven in a monolithic, statically linked executable, this kind of disciplined partitioning can greatly aid\nthe job of developing, testing, and deploying the project. Teams can work independently of each other\non their own components without treading on each other\u2019s toes. High-level components remain\nindependent of lower-level details.\nCommunications between components in a monolith are very fast and inexpensive. They are typically\njust function calls. Consequently, communications across source-level decoupled boundaries can be\nvery chatty.\nSince the deployment of monoliths usually requires compilation and static linking, components in\nthese systems are typically delivered as source code.\nDEPLOYMENT COMPONENTS\nThe simplest physical representation of an architectural boundary is a dynamically linked library like\na .Net DLL, a Java jar file, a Ruby Gem, or a UNIX shared library. Deployment does not involve\ncompilation. Instead, the components are delivered in binary, or some equivalent deployable form.\nThis is the deployment-level decoupling mode. The act of deployment is simply the gathering of these\ndeployable units together in some convenient form, such as a WAR file, or even just a directory.\nWith that one exception, deployment-level components are the same as monoliths. The functions\ngenerally all exist in the same processor and address space. The strategies for segregating the\ncomponents and managing their dependencies are the same.2\nAs with monoliths, communications across deployment component boundaries are just function calls\n", "page": 146, "type": "text", "section": "Page 146"}
{"text": "and, therefore, are very inexpensive. There may be a one-time hit for dynamic linking or runtime\nloading, but communications across these boundaries can still be very chatty.\nTHREADS\nBoth monoliths and deployment components can make use of threads. Threads are not architectural\nboundaries or units of deployment, but rather a way to organize the schedule and order of execution.\nThey may be wholly contained within a component, or spread across many components.\nLOCAL PROCESSES\nA much stronger physical architectural boundary is the local process. A local process is typically\ncreated from the command line or an equivalent system call. Local processes run in the same\nprocessor, or in the same set of processors within a multicore, but run in separate address spaces.\nMemory protection generally prevents such processes from sharing memory, although shared memory\npartitions are often used.\nMost often, local processes communicate with each other using sockets, or some other kind of\noperating system communications facility such as mailboxes or message queues.\nEach local process may be a statically linked monolith, or it may be composed of dynamically linked\ndeployment components. In the former case, several monolithic processes may have the same\ncomponents compiled and linked into them. In the latter, they may share the same dynamically linked\ndeployment components.\nThink of a local process as a kind of uber-component: The process consists of lower-level\ncomponents that manage their dependencies through dynamic polymorphism.\nThe segregation strategy between local processes is the same as for monoliths and binary\ncomponents. Source code dependencies point in the same direction across the boundary, and always\ntoward the higher-level component.\nFor local processes, this means that the source code of the higher-level processes must not contain the\nnames, or physical addresses, or registry lookup keys of lower-level processes. Remember that the\narchitectural goal is for lower-level processes to be plugins to higher-level processes.\nCommunication across local process boundaries involve operating system calls, data marshaling and\ndecoding, and interprocess context switches, which are moderately expensive. Chattiness should be\ncarefully limited.\nSERVICES\nThe strongest boundary is a service. A service is a process, generally started from the command line\n", "page": 147, "type": "text", "section": "Page 147"}
{"text": "or through an equivalent system call. Services do not depend on their physical location. Two\ncommunicating services may, or may not, operate in the same physical processor or multicore. The\nservices assume that all communications take place over the network.\nCommunications across service boundaries are very slow compared to function calls. Turnaround\ntimes can range from tens of milliseconds to seconds. Care must be taken to avoid chatting where\npossible. Communications at this level must deal with high levels of latency.\nOtherwise, the same rules apply to services as apply to local processes. Lower-level services should\n\u201cplug in\u201d to higher-level services. The source code of higher-level services must not contain any\nspecific physical knowledge (e.g., a URI) of any lower-level service.\nCONCLUSION\nMost systems, other than monoliths, use more than one boundary strategy. A system that makes use of\nservice boundaries may also have some local process boundaries. Indeed, a service is often just a\nfacade for a set of interacting local processes. A service, or a local process, will almost certainly be\neither a monolith composed of source code components or a set of dynamically linked deployment\ncomponents.\nThis means that the boundaries in a system will often be a mixture of local chatty boundaries and\nboundaries that are more concerned with latency.\n1. Static polymorphism (e.g., generics or templates) can sometimes be a viable means of dependency management in monolithic systems,\nespecially in languages like C++. However, the decoupling afforded by generics cannot protect you from the need for recompilation\nand redeployment the way dynamic polymorphism can.\n2. Although static polymorphism is not an option in this case.\n", "page": 148, "type": "text", "section": "Page 148"}
{"text": "19\nPOLICY AND LEVEL\nSoftware systems are statements of policy. Indeed, at its core, that\u2019s all a computer program actually\nis. A computer program is a detailed description of the policy by which inputs are transformed into\noutputs.\nIn most nontrivial systems, that policy can be broken down into many different smaller statements of\npolicy. Some of those statements will describe how particular business rules are to be calculated.\nOthers will describe how certain reports are to be formatted. Still others will describe how input\ndata are to be validated.\nPart of the art of developing a software architecture is carefully separating those policies from one\nanother, and regrouping them based on the ways that they change. Policies that change for the same\nreasons, and at the same times, are at the same level and belong together in the same component.\nPolicies that change for different reasons, or at different times, are at different levels and should be\nseparated into different components.\nThe art of architecture often involves forming the regrouped components into a directed acyclic\ngraph. The nodes of the graph are the components that contain policies at the same level. The directed\nedges are the dependencies between those components. They connect components that are at different\nlevels.\nThose dependencies are source code, compile-time dependencies. In Java, they are import\nstatements. In C#, they are using statements. In Ruby, they are require statements. They are the\n", "page": 149, "type": "text", "section": "Page 149"}
{"text": "dependencies that are necessary for the compiler to function.\nIn a good architecture, the direction of those dependencies is based on the level of the components\nthat they connect. In every case, low-level components are designed so that they depend on high-level\ncomponents.\nLEVEL\nA strict definition of \u201clevel\u201d is \u201cthe distance from the inputs and outputs.\u201d The farther a policy is from\nboth the inputs and the outputs of the system, the higher its level. The policies that manage input and\noutput are the lowest-level policies in the system.\nThe data flow diagram in Figure 19.1 depicts a simple encryption program that reads characters from\nan input device, translates the characters using a table, and then writes the translated characters to an\noutput device. The data flows are shown as curved solid arrows. The properly designed source code\ndependencies are shown as straight dashed lines.\nFigure 19.1 A simple encryption program\nThe Translate component is the highest-level component in this system because it is the component\nthat is farthest from the inputs and outputs.1\nNote that the data flows and the source code dependencies do not always point in the same direction.\nThis, again, is part of the art of software architecture. We want source code dependencies to be\ndecoupled from data flow and coupled to level.\nIt would be easy to create an incorrect architecture by writing the encryption program like this:\nClick here to view code image\nfunction encrypt() {\n  while(true)\n", "page": 150, "type": "text", "section": "Page 150"}
{"text": "    writeChar(translate(readChar()));\n}\nThis is incorrect architecture because the high-level encrypt function depends on the lower-level\nreadChar and writeChar functions.\nA better architecture for this system is shown in the class diagram in Figure 19.2. Note the dashed\nborder surrounding the Encrypt class, and the CharWriter and CharReader interfaces. All\ndependencies crossing that border point inward. This unit is the highest-level element in the system.\nFigure 19.2 Class diagram showing a better architecture for the system\nConsoleReader and ConsoleWriter are shown here as classes. They are low level because they are\nclose to the inputs and outputs.\nNote how this structure decouples the high-level encryption policy from the lower-level input/output\npolicies. This makes the encryption policy usable in a wide range of contexts. When changes are\nmade to the input and output policies, they are not likely to affect the encryption policy.\nRecall that policies are grouped into components based on the way that they change. Policies that\nchange for the same reasons and at the same times are grouped together by the SRP and CCP. Higher-\nlevel policies\u2014those that are farthest from the inputs and outputs\u2014tend to change less frequently, and\nfor more important reasons, than lower-level policies. Lower-level policies\u2014those that are closest\nto the inputs and outputs\u2014tend to change frequently, and with more urgency, but for less important\nreasons.\nFor example, even in the trivial example of the encryption program, it is far more likely that the IO\ndevices will change than that the encryption algorithm will change. If the encryption algorithm does\nchange, it will likely be for a more substantive reason than a change to one of the IO devices.\nKeeping these policies separate, with all source code dependencies pointing in the direction of the\nhigher-level policies, reduces the impact of change. Trivial but urgent changes at the lowest levels of\nthe system have little or no impact on the higher, more important, levels.\n", "page": 151, "type": "text", "section": "Page 151"}
{"text": "Another way to look at this issue is to note that lower-level components should be plugins to the\nhigher-level components. The component diagram in Figure 19.3 shows this arrangement. The\nEncryption component knows nothing of the IODevices component; the IODevices component\ndepends on the Encryption component.\nFigure 19.3 Lower-level components should plug in to higher-level components\nCONCLUSION\nAt this point, this discussion of policies has involved a mixture of the Single Responsibility\nPrinciple, the Open-Closed Principle, the Common Closure Principle, the Dependency Inversion\nPrinciple, the Stable Dependencies Principle, and the Stable Abstractions Principle. Look back and\nsee if you can identify where each principle was used, and why.\n1. Meilir Page-Jones called this component the \u201cCentral Transform\u201d in his book The Practical Guide to Structured Systems Design,\n2nd ed. (Yourdon Press, 1988).\n", "page": 152, "type": "text", "section": "Page 152"}
{"text": "20\nBUSINESS RULES\nIf we are going to divide our application into business rules and plugins, we\u2019d better get a good grasp\non just what business rules actually are. It turns out there are several different kinds.\nStrictly speaking, business rules are rules or procedures that make or save the business money. V\nery\nstrictly speaking, these rules would make or save the business money, irrespective of whether they\nwere implemented on a computer. They would make or save money even if they were executed\nmanually.\nThe fact that a bank charges N% interest for a loan is a business rule that makes the bank money. It\ndoesn\u2019t matter if a computer program calculates the interest, or if a clerk with an abacus calculates\nthe interest.\nWe shall call these rules Critical Business Rules, because they are critical to the business itself, and\nwould exist even if there were no system to automate them.\nCritical Business Rules usually require some data to work with. For example, our loan requires a\nloan balance, an interest rate, and a payment schedule.\nWe shall call this data Critical Business Data. This is the data that would exist even if the system\nwere not automated.\nThe critical rules and critical data are inextricably bound, so they are a good candidate for an object.\n", "page": 153, "type": "text", "section": "Page 153"}
{"text": "We\u2019ll call this kind of object an Entity.1\nENTITIES\nAn Entity is an object within our computer system that embodies a small set of critical business rules\noperating on Critical Business Data. The Entity object either contains the Critical Business Data or\nhas very easy access to that data. The interface of the Entity consists of the functions that implement\nthe Critical Business Rules that operate on that data.\nFor example, Figure 20.1 shows what our Loan entity might look like as a class in UML. It has three\npieces of Critical Business Data, and presents three related Critical Business Rules at its interface.\nFigure 20.1 Loan entity as a class in UML\nWhen we create this kind of class, we are gathering together the software that implements a concept\nthat is critical to the business, and separating it from every other concern in the automated system we\nare building. This class stands alone as a representative of the business. It is unsullied with concerns\nabout databases, user interfaces, or third-party frameworks. It could serve the business in any system,\nirrespective of how that system was presented, or how the data was stored, or how the computers in\nthat system were arranged. The Entity is pure business and nothing else.\nSome of you may be concerned that I called it a class. Don\u2019t be. You don\u2019t need to use an object-\noriented language to create an Entity. All that is required is that you bind the Critical Business Data\nand the Critical Business Rules together in a single and separate software module.\nUSE CASES\nNot all business rules are as pure as Entities. Some business rules make or save money for the\nbusiness by defining and constraining the way that an automated system operates. These rules would\nnot be used in a manual environment, because they make sense only as part of an automated system.\n", "page": 154, "type": "text", "section": "Page 154"}
{"text": "For example, imagine an application that is used by bank officers to create a new loan. The bank may\ndecide that it does not want the loan officers to offer loan payment estimates until they have first\ngathered, and validated, contact information and ensured that the candidate\u2019s credit score is 500 or\nhigher. For this reason, the bank may specify that the system will not proceed to the payment\nestimation screen until the contact information screen has been filled out and verified, and the credit\nscore has been confirmed to be greater than the cutoff.\nThis is a use case.2 A use case is a description of the way that an automated system is used. It\nspecifies the input to be provided by the user, the output to be returned to the user, and the processing\nsteps involved in producing that output. A use case describes application-specific business rules as\nopposed to the Critical Business Rules within the Entities.\nFigure 20.2 shows an example of a use case. Notice that in the last line it mentions the Customer. This\nis a reference to the Customer entity, which contains the Critical Business Rules that govern the\nrelationship between the bank and its customers.\nFigure 20.2 Example use case\nUse cases contain the rules that specify how and when the Critical Business Rules within the Entities\nare invoked. Use cases control the dance of the Entities.\nNotice also that the use case does not describe the user interface other than to informally specify the\ndata coming in from that interface, and the data going back out through that interface. From the use\ncase, it is impossible to tell whether the application is delivered on the web, or on a thick client, or\non a console, or is a pure service.\nThis is very important. Use cases do not describe how the system appears to the user. Instead, they\ndescribe the application-specific rules that govern the interaction between the users and the Entities.\nHow the data gets in and out of the system is irrelevant to the use cases.\nA use case is an object. It has one or more functions that implement the application-specific business\nrules. It also has data elements that include the input data, the output data, and the references to the\nappropriate Entities with which it interacts.\n", "page": 155, "type": "text", "section": "Page 155"}
{"text": "Entities have no knowledge of the use cases that control them. This is another example of the\ndirection of the dependencies following the Dependency Inversion Principle. High-level concepts,\nsuch as Entities, know nothing of lower-level concepts, such as use cases. Instead, the lower-level\nuse cases know about the higher-level Entities.\nWhy are Entities high level and use cases lower level? Because use cases are specific to a single\napplication and, therefore, are closer to the inputs and outputs of that system. Entities are\ngeneralizations that can be used in many different applications, so they are farther from the inputs and\noutputs of the system. Use cases depend on Entities; Entities do not depend on use cases.\nREQUEST AND RESPONSE MODELS\nUse cases expect input data, and they produce output data. However, a well-formed use case object\nshould have no inkling about the way that data is communicated to the user, or to any other component.\nWe certainly don\u2019t want the code within the use case class to know about HTML or SQL!\nThe use case class accepts simple request data structures for its input, and returns simple response\ndata structures as its output. These data structures are not dependent on anything. They do not derive\nfrom standard framework interfaces such as HttpRequest and HttpResponse. They know nothing of\nthe web, nor do they share any of the trappings of whatever user interface might be in place.\nThis lack of dependencies is critical. If the request and response models are not independent, then the\nuse cases that depend on them will be indirectly bound to whatever dependencies the models carry\nwith them.\nYou might be tempted to have these data structures contain references to Entity objects. You might\nthink this makes sense because the Entities and the request/response models share so much data.\nAvoid this temptation! The purpose of these two objects is very different. Over time they will change\nfor very different reasons, so tying them together in any way violates the Common Closure and Single\nResponsibility Principles. The result would be lots of tramp data, and lots of conditionals in your\ncode.\nCONCLUSION\nBusiness rules are the reason a software system exists. They are the core functionality. They carry the\ncode that makes, or saves, money. They are the family jewels.\nThe business rules should remain pristine, unsullied by baser concerns such as the user interface or\ndatabase used. Ideally, the code that represents the business rules should be the heart of the system,\nwith lesser concerns being plugged in to them. The business rules should be the most independent and\nreusable code in the system.\n1. This is Ivar Jacobson\u2019s name for this concept (I. Jacobson et al., Object Oriented Software Engineering, Addison-Wesley, 1992).\n", "page": 156, "type": "text", "section": "Page 156"}
{"text": "2. Ibid.\n", "page": 157, "type": "text", "section": "Page 157"}
{"text": "21\nSCREAMING ARCHITECTURE\nImagine that you are looking at the blueprints of a building. This document, prepared by an architect,\nprovides the plans for the building. What do these plans tell you?\nIf the plans you are viewing are for a single-family residence, then you\u2019ll likely see a front entrance, a\nfoyer leading to a living room, and perhaps a dining room. There will likely be a kitchen a short\ndistance away, close to the dining room. Perhaps there is a dinette area next to the kitchen, and\nprobably a family room close to that. When you looked at those plans, there would be no question that\nyou were looking at a single family home. The architecture would scream: \u201cHOME.\u201d\nNow suppose you were looking at the architecture of a library. You would likely see a grand\nentrance, an area for check-in/out clerks, reading areas, small conference rooms, and gallery after\ngallery capable of holding bookshelves for all the books in the library. That architecture would\nscream: \u201cLIBRARY\n.\u201d\nSo what does the architecture of your application scream? When you look at the top-level directory\nstructure, and the source files in the highest-level package, do they scream \u201cHealth Care System,\u201d or\n\u201cAccounting System,\u201d or \u201cInventory Management System\u201d? Or do they scream \u201cRails,\u201d or\n\u201cSpring/Hibernate,\u201d or \u201cASP\u201d?\nTHE THEME OF AN ARCHITECTURE\n", "page": 158, "type": "text", "section": "Page 158"}
{"text": "Go back and read Ivar Jacobson\u2019s seminal work on software architecture: Object Oriented Software\nEngineering. Notice the subtitle of the book: A Use Case Driven Approach. In this book Jacobson\nmakes the point that software architectures are structures that support the use cases of the system. Just\nas the plans for a house or a library scream about the use cases of those buildings, so should the\narchitecture of a software application scream about the use cases of the application.\nArchitectures are not (or should not be) about frameworks. Architectures should not be supplied by\nframeworks. Frameworks are tools to be used, not architectures to be conformed to. If your\narchitecture is based on frameworks, then it cannot be based on your use cases.\nTHE PURPOSE OF AN ARCHITECTURE\nGood architectures are centered on use cases so that architects can safely describe the structures that\nsupport those use cases without committing to frameworks, tools, and environments. Again, consider\nthe plans for a house. The first concern of the architect is to make sure that the house is usable\u2014not to\nensure that the house is made of bricks. Indeed, the architect takes pains to ensure that the homeowner\ncan make decisions about the exterior material (bricks, stone, or cedar) later, after the plans ensure\nthat the use cases are met.\nA good software architecture allows decisions about frameworks, databases, web servers, and other\nenvironmental issues and tools to be deferred and delayed. Frameworks are options to be left open.\nA good architecture makes it unnecessary to decide on Rails, or Spring, or Hibernate, or Tomcat, or\nMySQL, until much later in the project. A good architecture makes it easy to change your mind about\nthose decisions, too. A good architecture emphasizes the use cases and decouples them from\nperipheral concerns.\nBUT WHAT ABOUT THE WEB?\nIs the web an architecture? Does the fact that your system is delivered on the web dictate the\narchitecture of your system? Of course not! The web is a delivery mechanism\u2014an IO device\u2014and\nyour application architecture should treat it as such. The fact that your application is delivered over\nthe web is a detail and should not dominate your system structure. Indeed, the decision that your\napplication will be delivered over the web is one that you should defer. Your system architecture\nshould be as ignorant as possible about how it will be delivered. You should be able to deliver it as a\nconsole app, or a web app, or a thick client app, or even a web service app, without undue\ncomplication or change to the fundamental architecture.\nFRAMEWORKS ARE TOOLS, NOT WAYS OF LIFE\nFrameworks can be very powerful and very useful. Framework authors often believe very deeply in\ntheir frameworks. The examples they write for how to use their frameworks are told from the point of\nview of a true believer. Other authors who write about the framework also tend to be disciples of the\n", "page": 159, "type": "text", "section": "Page 159"}
{"text": "true belief. They show you the way to use the framework. Often they assume an all-encompassing, all-\npervading, let-the-framework-do-everything position.\nThis is not the position you want to take.\nLook at each framework with a jaded eye. View it skeptically. Yes, it might help, but at what cost?\nAsk yourself how you should use it, and how you should protect yourself from it. Think about how\nyou can preserve the use-case emphasis of your architecture. Develop a strategy that prevents the\nframework from taking over that architecture.\nTESTABLE ARCHITECTURES\nIf your system architecture is all about the use cases, and if you have kept your frameworks at arm\u2019s\nlength, then you should be able to unit-test all those use cases without any of the frameworks in place.\nYou shouldn\u2019t need the web server running to run your tests. You shouldn\u2019t need the database\nconnected to run your tests. Your Entity objects should be plain old objects that have no dependencies\non frameworks or databases or other complications. Your use case objects should coordinate your\nEntity objects. Finally, all of them together should be testable in situ, without any of the complications\nof frameworks.\nCONCLUSION\nYour architecture should tell readers about the system, not about the frameworks you used in your\nsystem. If you are building a health care system, then when new programmers look at the source\nrepository, their first impression should be, \u201cOh, this is a heath care system.\u201d Those new\nprogrammers should be able to learn all the use cases of the system, yet still not know how the system\nis delivered. They may come to you and say:\n\u201cWe see some things that look like models\u2014but where are the views and controllers?\u201d\nAnd you should respond:\n\u201cOh, those are details that needn\u2019t concern us at the moment. We\u2019ll decide about them later.\u201d\n", "page": 160, "type": "text", "section": "Page 160"}
{"text": "22\nTHE CLEAN ARCHITECTURE\nOver the last several decades we\u2019ve seen a whole range of ideas regarding the architecture of\nsystems. These include:\n\u2022 Hexagonal Architecture (also known as Ports and Adapters), developed by Alistair Cockburn, and\nadopted by Steve Freeman and Nat Pryce in their wonderful book Growing Object Oriented\nSoftware with Tests\n\u2022 DCI from James Coplien and Trygve Reenskaug\n\u2022 BCE, introduced by Ivar Jacobson from his book Object Oriented Software Engineering: A Use-\nCase Driven Approach\nAlthough these architectures all vary somewhat in their details, they are very similar. They all have\nthe same objective, which is the separation of concerns. They all achieve this separation by dividing\nthe software into layers. Each has at least one layer for business rules, and another layer for user and\nsystem interfaces.\nEach of these architectures produces systems that have the following characteristics:\n\u2022 Independent of frameworks. The architecture does not depend on the existence of some library of\nfeature-laden software. This allows you to use such frameworks as tools, rather than forcing you to\ncram your system into their limited constraints.\n\u2022 Testable. The business rules can be tested without the UI, database, web server, or any other\n", "page": 161, "type": "text", "section": "Page 161"}
{"text": "external element.\n\u2022 Independent of the UI. The UI can change easily, without changing the rest of the system. A web UI\ncould be replaced with a console UI, for example, without changing the business rules.\n\u2022 Independent of the database. You can swap out Oracle or SQL Server for Mongo, BigTable,\nCouchDB, or something else. Your business rules are not bound to the database.\n\u2022 Independent of any external agency. In fact, your business rules don\u2019t know anything at all about\nthe interfaces to the outside world.\nThe diagram in Figure 22.1 is an attempt at integrating all these architectures into a single actionable\nidea.\nFigure 22.1 The clean architecture\nTHE DEPENDENCY RULE\nThe concentric circles in Figure 22.1 represent different areas of software. In general, the further in\nyou go, the higher level the software becomes. The outer circles are mechanisms. The inner circles\nare policies.\nThe overriding rule that makes this architecture work is the Dependency Rule:\nSource code dependencies must point only inward, toward higher-level policies.\nNothing in an inner circle can know anything at all about something in an outer circle. In particular,\nthe name of something declared in an outer circle must not be mentioned by the code in an inner\ncircle. That includes functions, classes, variables, or any other named software entity.\nBy the same token, data formats declared in an outer circle should not be used by an inner circle,\nespecially if those formats are generated by a framework in an outer circle. We don\u2019t want anything in\nan outer circle to impact the inner circles.\n", "page": 162, "type": "text", "section": "Page 162"}
{"text": "ENTITIES\nEntities encapsulate enterprise-wide Critical Business Rules. An entity can be an object with\nmethods, or it can be a set of data structures and functions. It doesn\u2019t matter so long as the entities can\nbe used by many different applications in the enterprise.\nIf you don\u2019t have an enterprise and are writing just a single application, then these entities are the\nbusiness objects of the application. They encapsulate the most general and high-level rules. They are\nthe least likely to change when something external changes. For example, you would not expect these\nobjects to be affected by a change to page navigation or security. No operational change to any\nparticular application should affect the entity layer.\nUSE CASES\nThe software in the use cases layer contains application-specific business rules. It encapsulates and\nimplements all of the use cases of the system. These use cases orchestrate the flow of data to and from\nthe entities, and direct those entities to use their Critical Business Rules to achieve the goals of the\nuse case.\nWe do not expect changes in this layer to affect the entities. We also do not expect this layer to be\naffected by changes to externalities such as the database, the UI, or any of the common frameworks.\nThe use cases layer is isolated from such concerns.\nWe do, however, expect that changes to the operation of the application will affect the use cases and,\ntherefore, the software in this layer. If the details of a use case change, then some code in this layer\nwill certainly be affected.\nINTERFACE ADAPTERS\nThe software in the interface adapters layer is a set of adapters that convert data from the format most\nconvenient for the use cases and entities, to the format most convenient for some external agency such\nas the database or the web. It is this layer, for example, that will wholly contain the MVC architecture\nof a GUI. The presenters, views, and controllers all belong in the interface adapters layer. The\nmodels are likely just data structures that are passed from the controllers to the use cases, and then\nback from the use cases to the presenters and views.\nSimilarly, data is converted, in this layer, from the form most convenient for entities and use cases, to\nthe form most convenient for whatever persistence framework is being used (i.e., the database). No\ncode inward of this circle should know anything at all about the database. If the database is a SQL\ndatabase, then all SQL should be restricted to this layer\u2014and in particular to the parts of this layer\nthat have to do with the database.\nAlso in this layer is any other adapter necessary to convert data from some external form, such as an\nexternal service, to the internal form used by the use cases and entities.\n", "page": 163, "type": "text", "section": "Page 163"}
{"text": "FRAMEWORKS AND DRIVERS\nThe outermost layer of the model in Figure 22.1 is generally composed of frameworks and tools such\nas the database and the web framework. Generally you don\u2019t write much code in this layer, other than\nglue code that communicates to the next circle inward.\nThe frameworks and drivers layer is where all the details go. The web is a detail. The database is a\ndetail. We keep these things on the outside where they can do little harm.\nONLY FOUR CIRCLES?\nThe circles in Figure 22.1 are intended to be schematic: You may find that you need more than just\nthese four. There\u2019s no rule that says you must always have just these four. However, the Dependency\nRule always applies. Source code dependencies always point inward. As you move inward, the level\nof abstraction and policy increases. The outermost circle consists of low-level concrete details. As\nyou move inward, the software grows more abstract and encapsulates higher-level policies. The\ninnermost circle is the most general and highest level.\nCROSSING BOUNDARIES\nAt the lower right of the diagram in Figure 22.1 is an example of how we cross the circle boundaries.\nIt shows the controllers and presenters communicating with the use cases in the next layer. Note the\nflow of control: It begins in the controller, moves through the use case, and then winds up executing in\nthe presenter. Note also the source code dependencies: Each points inward toward the use cases.\nWe usually resolve this apparent contradiction by using the Dependency Inversion Principle. In a\nlanguage like Java, for example, we would arrange interfaces and inheritance relationships such that\nthe source code dependencies oppose the flow of control at just the right points across the boundary.\nFor example, suppose the use case needs to call the presenter. This call must not be direct because\nthat would violate the Dependency Rule: No name in an outer circle can be mentioned by an inner\ncircle. So we have the use case call an interface (shown in Figure 22.1 as \u201cuse case output port\u201d) in\nthe inner circle, and have the presenter in the outer circle implement it.\nThe same technique is used to cross all the boundaries in the architectures. We take advantage of\ndynamic polymorphism to create source code dependencies that oppose the flow of control so that we\ncan conform to the Dependency Rule, no matter which direction the flow of control travels.\nWHICH DATA CROSSES THE BOUNDARIES\nTypically the data that crosses the boundaries consists of simple data structures. You can use basic\nstructs or simple data transfer objects if you like. Or the data can simply be arguments in function\ncalls. Or you can pack it into a hashmap, or construct it into an object. The important thing is that\nisolated, simple data structures are passed across the boundaries. We don\u2019t want to cheat and pass\nEntity objects or database rows. We don\u2019t want the data structures to have any kind of dependency\n", "page": 164, "type": "text", "section": "Page 164"}
{"text": "that violates the Dependency Rule.\nFor example, many database frameworks return a convenient data format in response to a query. We\nmight call this a \u201crow structure.\u201d We don\u2019t want to pass that row structure inward across a boundary.\nDoing so would violate the Dependency Rule because it would force an inner circle to know\nsomething about an outer circle.\nThus, when we pass data across a boundary, it is always in the form that is most convenient for the\ninner circle.\nA TYPICAL SCENARIO\nThe diagram in Figure 22.2 shows a typical scenario for a web-based Java system using a database.\nThe web server gathers input data from the user and hands it to the Controller on the upper left. The\nController packages that data into a plain old Java object and passes this object through the\nInputBoundary to the UseCaseInteractor. The UseCaseInteractor interprets that data and uses\nit to control the dance of the Entities. It also uses the DataAccessInterface to bring the data used\nby those Entities into memory from the Database. Upon completion, the UseCaseInteractor\ngathers data from the Entities and constructs the OutputData as another plain old Java object. The\nOutputData is then passed through the OutputBoundary interface to the Presenter.\nFigure 22.2 A typical scenario for a web-based Java system utilizing a database\nThe job of the Presenter is to repackage the OutputData into viewable form as the ViewModel,\nwhich is yet another plain old Java object. The ViewModel contains mostly Strings and flags that the\nView uses to display the data. Whereas the OutputData may contain Date objects, the Presenter\nwill load the ViewModel with corresponding Strings already formatted properly for the user. The\nsame is true of Currency objects or any other business-related data. Button and MenuItem names\nare placed in the ViewModel, as are flags that tell the View whether those Buttons and MenuItems\nshould be gray.\n", "page": 165, "type": "text", "section": "Page 165"}
{"text": "This leaves the View with almost nothing to do other than to move the data from the ViewModel into\nthe HTML page.\nNote the directions of the dependencies. All dependencies cross the boundary lines pointing inward,\nfollowing the Dependency Rule.\nCONCLUSION\nConforming to these simple rules is not difficult, and it will save you a lot of headaches going\nforward. By separating the software into layers and conforming to the Dependency Rule, you will\ncreate a system that is intrinsically testable, with all the benefits that implies. When any of the\nexternal parts of the system become obsolete, such as the database, or the web framework, you can\nreplace those obsolete elements with a minimum of fuss.\n", "page": 166, "type": "text", "section": "Page 166"}
{"text": "23\nPRESENTERS AND HUMBLE OBJECTS\nIn Chapter 22, we introduced the notion of presenters. Presenters are a form of the Humble Object\npattern, which helps us identify and protect architectural boundaries. Actually, the Clean Architecture\nin the last chapter was full of Humble Object implementations.\nTHE HUMBLE OBJECT PATTERN\nThe Humble Object pattern1 is a design pattern that was originally identified as a way to help unit\ntesters to separate behaviors that are hard to test from behaviors that are easy to test. The idea is very\nsimple: Split the behaviors into two modules or classes. One of those modules is humble; it contains\nall the hard-to-test behaviors stripped down to their barest essence. The other module contains all the\ntestable behaviors that were stripped out of the humble object.\nFor example, GUIs are hard to unit test because it is very difficult to write tests that can see the\nscreen and check that the appropriate elements are displayed there. However, most of the behavior of\na GUI is, in fact, easy to test. Using the Humble Object pattern, we can separate these two kinds of\nbehaviors into two different classes called the Presenter and the View.\nPRESENTERS AND VIEWS\n", "page": 167, "type": "text", "section": "Page 167"}
{"text": "The View is the humble object that is hard to test. The code in this object is kept as simple as\npossible. It moves data into the GUI but does not process that data.\nThe Presenter is the testable object. Its job is to accept data from the application and format it for\npresentation so that the View can simply move it to the screen. For example, if the application wants a\ndate displayed in a field, it will hand the Presenter a Date object. The Presenter will then format that\ndata into an appropriate string and place it in a simple data structure called the View Model, where\nthe View can find it.\nIf the application wants to display money on the screen, it might pass a Currency object to the\nPresenter. The Presenter will format that object with the appropriate decimal places and currency\nmarkers, creating a string that it can place in the View Model. If that currency value should be turned\nred if it is negative, then a simple boolean flag in the View model will be set appropriately.\nEvery button on the screen will have a name. That name will be a string in the View Model, placed\nthere by the presenter. If those buttons should be grayed out, the Presenter will set an appropriate\nboolean flag in the View model. Every menu item name is a string in the View model, loaded by the\nPresenter. The names for every radio button, check box, and text field are loaded, by the Presenter,\ninto appropriate strings and booleans in the View model. Tables of numbers that should be displayed\non the screen are loaded, by the Presenter, into tables of properly formatted strings in the View\nmodel.\nAnything and everything that appears on the screen, and that the application has some kind of control\nover, is represented in the View Model as a string, or a boolean, or an enum. Nothing is left for the\nView to do other than to load the data from the View Model into the screen. Thus the View is humble.\nTESTING AND ARCHITECTURE\nIt has long been known that testability is an attribute of good architectures. The Humble Object\npattern is a good example, because the separation of the behaviors into testable and non-testable parts\noften defines an architectural boundary. The Presenter/View boundary is one of these boundaries, but\nthere are many others.\nDATABASE GATEW\nAYS\nBetween the use case interactors and the database are the database gateways.2 These gateways are\npolymorphic interfaces that contain methods for every create, read, update, or delete operation that\ncan be performed by the application on the database. For example, if the application needs to know\nthe last names of all the users who logged in yesterday, then the UserGateway interface will have a\nmethod named getLastNamesOfUsersWhoLoggedInAfter that takes a Date as its argument and\nreturns a list of last names.\nRecall that we do not allow SQL in the use cases layer; instead, we use gateway interfaces that have\n", "page": 168, "type": "text", "section": "Page 168"}
{"text": "appropriate methods. Those gateways are implemented by classes in the database layer. That\nimplementation is the humble object. It simply uses SQL, or whatever the interface to the database is,\nto access the data required by each of the methods. The interactors, in contrast, are not humble\nbecause they encapsulate application-specific business rules. Although they are not humble, those\ninteractors are testable, because the gateways can be replaced with appropriate stubs and test-\ndoubles.\nDATA MAPPERS\nGoing back to the topic of databases, in which layer do you think ORMs like Hibernate belong?\nFirst, let\u2019s get something straight: There is no such thing as an object relational mapper (ORM). The\nreason is simple: Objects are not data structures. At least, they are not data structures from their\nusers\u2019 point of view. The users of an object cannot see the data, since it is all private. Those users see\nonly the public methods of that object. So, from the user\u2019s point of view, an object is simply a set of\noperations.\nA data structure, in contrast, is a set of public data variables that have no implied behavior. ORMs\nwould be better named \u201cdata mappers,\u201d because they load data into data structures from relational\ndatabase tables.\nWhere should such ORM systems reside? In the database layer of course. Indeed, ORMs form another\nkind of Humble Object boundary between the gateway interfaces and the database.\nSERVICE LISTENERS\nWhat about services? If your application must communicate with other services, or if your\napplication provides a set of services, will we find the Humble Object pattern creating a service\nboundary?\nOf course! The application will load data into simple data structures and then pass those structures\nacross the boundary to modules that properly format the data and send it to external services. On the\ninput side, the service listeners will receive data from the service interface and format it into a\nsimple data structure that can be used by the application. That data structure is then passed across the\nservice boundary.\nCONCLUSION\nAt each architectural boundary, we are likely to find the Humble Object pattern lurking somewhere\nnearby. The communication across that boundary will almost always involve some kind of simple\ndata structure, and the boundary will frequently divide something that is hard to test from something\nthat is easy to test. The use of this pattern at architectural boundaries vastly increases the testability of\n", "page": 169, "type": "text", "section": "Page 169"}
{"text": "the entire system.\n1. xUnit Patterns, Meszaros, Addison-Wesley, 2007, p. 695.\n2. Patterns of Enterprise Application Architecture, Martin Fowler, et. al., Addison-Wesley, 2003, p. 466.\n", "page": 170, "type": "text", "section": "Page 170"}
{"text": "24\nPARTIAL BOUNDARIES\nFull-fledged architectural boundaries are expensive. They require reciprocal polymorphic Boundary\ninterfaces, Input and Output data structures, and all of the dependency management necessary to\nisolate the two sides into independently compilable and deployable components. That takes a lot of\nwork. It\u2019s also a lot of work to maintain.\nIn many situations, a good architect might judge that the expense of such a boundary is too high\u2014but\nmight still want to hold a place for such a boundary in case it is needed later.\nThis kind of anticipatory design is often frowned upon by many in the Agile community as a violation\nof YAGNI: \u201cYou Aren\u2019t Going to Need It.\u201d Architects, however, sometimes look at the problem and\nthink, \u201cYeah, but I might.\u201d In that case, they may implement a partial boundary.\nSKIP THE LAST STEP\nOne way to construct a partial boundary is to do all the work necessary to create independently\ncompilable and deployable components, and then simply keep them together in the same component.\nThe reciprocal interfaces are there, the input/output data structures are there, and everything is all set\nup\u2014but we compile and deploy all of them as a single component.\nObviously, this kind of partial boundary requires the same amount of code and preparatory design\n", "page": 171, "type": "text", "section": "Page 171"}
{"text": "work as a full boundary. However, it does not require the administration of multiple components.\nThere\u2019s no version number tracking or release management burden. That difference should not be\ntaken lightly.\nThis was the early strategy behind FitNesse. The web server component of FitNesse was designed\nto be separable from the wiki and testing part of FitNesse. The idea was that we might want to\ncreate other web-based applications by using that web component. At the same, we did not want\nusers to have to download two components. Recall that one of our design goals was \u201cdownload and\ngo.\u201d It was our intent that users would download one jar file and execute it without having to hunt for\nother jar files, work out version compatibilities, and so on.\nThe story of FitNesse also points out one of the dangers of this approach. Over time, as it became\nclear that there would never be a need for a separate web component, the separation between the web\ncomponent and the wiki component began to weaken. Dependencies started to cross the line in the\nwrong direction. Nowadays, it would be something of a chore to re-separate them.\nONE-DIMENSIONAL BOUNDARIES\nThe full-fledged architectural boundary uses reciprocal boundary interfaces to maintain isolation in\nboth directions. Maintaining separation in both directions is expensive both in initial setup and in\nongoing maintenance.\nA simpler structure that serves to hold the place for later extension to a full-fledged boundary is\nshown in Figure 24.1. It exemplifies the traditional Strategy pattern. A ServiceBoundary interface is\nused by clients and implemented by ServiceImpl classes.\nFigure 24.1 The Strategy pattern\nIt should be clear that this sets the stage for a future architectural boundary. The necessary\ndependency inversion is in place in an attempt to isolate the Client from the ServiceImpl. It should\nalso be clear that the separation can degrade pretty rapidly, as shown by the nasty dotted arrow in the\n", "page": 172, "type": "text", "section": "Page 172"}
{"text": "diagram. Without reciprocal interfaces, nothing prevents this kind of backchannel other than the\ndiligence and discipline of the developers and architects.\nFACADES\nAn even simpler boundary is the Facade pattern, illustrated in Figure 24.2. In this case, even the\ndependency inversion is sacrificed. The boundary is simply defined by the Facade class, which lists\nall the services as methods, and deploys the service calls to classes that the client is not supposed to\naccess.\nFigure 24.2 The Facade pattern\nNote, however, that the Client has a transitive dependency on all those service classes. In static\nlanguages, a change to the source code in one of the Service classes will force the Client to\nrecompile. Also, you can imagine how easy backchannels are to create with this structure.\nCONCLUSION\nWe\u2019ve seen three simple ways to partially implement an architectural boundary. There are, of course,\nmany others. These three strategies are simply offered as examples.\nEach of these approaches has its own set of costs and benefits. Each is appropriate, in certain\ncontexts, as a placeholder for an eventual full-fledged boundary. Each can also be degraded if that\nboundary never materializes.\nIt is one of the functions of an architect to decide where an architectural boundary might one day\nexist, and whether to fully or partially implement that boundary.\n", "page": 173, "type": "text", "section": "Page 173"}
{"text": "25\nLAYERS AND BOUNDARIES\nIt is easy to think of systems as being composed of three components: UI, business rules, and\ndatabase. For some simple systems, this is sufficient. For most systems, though, the number of\ncomponents is larger than that.\nConsider, for example, a simple computer game. It is easy to imagine the three components. The UI\nhandles all messages from the player to the game rules. The game rules store the state of the game in\nsome kind of persistent data structure. But is that all there is?\nHUNT THE WUMPUS\nLet\u2019s put some flesh on these bones. Let\u2019s assume that the game is the venerable Hunt the Wumpus\nadventure game from 1972. This text-based game uses very simple commands like GO EAST and\nSHOOT WEST. The player enters a command, and the computer responds with what the player sees,\nsmells, hears, and experiences. The player is hunting for a Wumpus in a system of caverns, and must\navoid traps, pits, and other dangers lying in wait. If you are interested, the rules of the game are easy\nto find on the web.\nLet\u2019s assume that we\u2019ll keep the text-based UI, but decouple it from the game rules so that our version\ncan use different languages in different markets. The game rules will communicate with the UI\ncomponent using a language-independent API, and the UI will translate the API into the appropriate\n", "page": 174, "type": "text", "section": "Page 174"}
{"text": "human language.\nIf the source code dependencies are properly managed, as shown in Figure 25.1, then any number of\nUI components can reuse the same game rules. The game rules do not know, nor do they care, which\nhuman language is being used.\nFigure 25.1 Any number of UI components can reuse the game rules\nLet\u2019s also assume that the state of the game is maintained on some persistent store\u2014perhaps in flash,\nor perhaps in the cloud, or maybe just in RAM. In any of those cases, we don\u2019t want the game rules to\nknow the details. So, again, we\u2019ll create an API that the game rules can use to communicate with the\ndata storage component.\nWe don\u2019t want the game rules to know anything about the different kinds of data storage, so the\ndependencies have to be properly directed following the Dependency Rule, as shown in Figure 25.2.\nFigure 25.2 Following the Dependency Rule\nCLEAN ARCHITECTURE?\nIt should be clear that we could easily apply the clean architecture approach in this context,1 with all\nthe use cases, boundaries, entities, and corresponding data structures. But have we really found all the\n", "page": 175, "type": "text", "section": "Page 175"}
{"text": "significant architectural boundaries?\nFor example, language is not the only axis of change for the UI. We also might want to vary the\nmechanism by which we communicate the text. For example, we might want to use a normal shell\nwindow, or text messages, or a chat application. There are many different possibilities.\nThat means that there is a potential architectural boundary defined by this axis of change. Perhaps we\nshould construct an API that crosses that boundary and isolates the language from the communications\nmechanism; that idea is illustrated in Figure 25.3.\nFigure 25.3 The revised diagram\nThe diagram in Figure 25.3 has gotten a little complicated, but should contain no surprises. The\ndashed outlines indicate abstract components that define an API that is implemented by the\ncomponents above or below them. For example, the Language API is implemented by English and\nSpanish.\nGameRules communicates with Language through an API that GameRules defines and Language\nimplements. Language communicates with TextDelivery using an API that Language defines but\nTextDelivery implements. The API is defined and owned by the user, rather than by the\nimplementer.\nIf we were to look inside GameRules, we would find polymorphic Boundary interfaces used by the\ncode inside GameRules and implemented by the code inside the Language component. We would\nalso find polymorphic Boundary interfaces used by Language and implemented by code inside\nGameRules.\nIf we were to look inside of Language, we would find the same thing: Polymorphic Boundary\ninterfaces implemented by the code inside TextDelivery, and polymorphic Boundary interfaces\nused by TextDelivery and implemented by Language.\nIn each case, the API defined by those Boundary interfaces is owned by the upstream component.\nThe variations, such as English, SMS, and CloudData, are provided by polymorphic interfaces\n", "page": 176, "type": "text", "section": "Page 176"}
{"text": "defined in the abstract API component, and implemented by the concrete components that serve them.\nFor example, we would expect polymorphic interfaces defined in Language to be implemented by\nEnglish and Spanish.\nWe can simplify this diagram by eliminating all the variations and focusing on just the API\ncomponents. Figure 25.4 shows this diagram.\nFigure 25.4 Simplified diagram\nNotice that the diagram is oriented in Figure 25.4 so that all the arrows point up. This puts\nGameRules at the top. This orientation makes sense because GameRules is the component that\ncontains the highest-level policies.\nConsider the direction of information flow. All input comes from the user through the TextDelivery\ncomponent at the bottom left. That information rises through the Language component, getting\ntranslated into commands to GameRules. GameRules processes the user input and sends appropriate\ndata down to DataStorage at the lower right.\nGameRules then sends output back down to Language, which translates the API back to the\nappropriate language and then delivers that language to the user through TextDelivery.\nThis organization effectively divides the flow of data into two streams.2 The stream on the left is\nconcerned with communicating with the user, and the stream on the right is concerned with data\npersistence. Both streams meet at the top3 at GameRules, which is the ultimate processor of the data\nthat goes through both streams.\nCROSSING THE STREAMS\nAre there always two data streams as in this example? No, not at all. Imagine that we would like to\nplay Hunt the Wumpus on the net with multiple players. In this case, we would need a network\ncomponent, like that shown in Figure 25.5. This organization divides the data flow into three streams,\n", "page": 177, "type": "text", "section": "Page 177"}
{"text": "all controlled by the GameRules.\nFigure 25.5 Adding a network component\nSo, as systems become more complex, the component structure may split into many such streams.\nSPLITTING THE STREAMS\nAt this point you may be thinking that all the streams eventually meet at the top in a single component.\nIf only life were so simple! The reality, of course, is much more complex.\nConsider the GameRules component for Hunt the Wumpus. Part of the game rules deal with the\nmechanics of the map. They know how the caverns are connected, and which objects are located in\neach cavern. They know how to move the player from cavern to cavern, and how to determine the\nevents that the player must deal with.\nBut there is another set of policies at an even higher level\u2014policies that know the health of the\nplayer, and the cost or benefit of a particular event. These policies could cause the player to gradually\nlose health, or to gain health by discovering food. The lower-level mechanics policy would declare\nevents to this higher-level policy, such as FoundFood or FellInPit. The higher-level policy would\nthen manage the state of the player (as shown in Figure 25.6). Eventually that policy would decide\nwhether the player wins or loses.\n", "page": 178, "type": "text", "section": "Page 178"}
{"text": "Figure 25.6 The higher-level policy manages the player\nIs this an architectural boundary? Do we need an API that separates MoveManagement from\nPlayerManagement? Well, let\u2019s make this a bit more interesting and add micro-services.\nLet\u2019s assume that we\u2019ve got a massive multiplayer version of Hunt the Wumpus. MoveManagement is\nhandled locally within the player\u2019s computer, but PlayerManagement is handled by a server.\nPlayerManagement offers a micro-service API to all the connected MoveManagement components.\nThe diagram in Figure 25.7 depicts this scenario in a somewhat abbreviated fashion. The Network\nelements are a bit more complex than depicted\u2014but you can probably still get the idea. A full-fledged\narchitectural boundary exists between MoveManagement and PlayerManagement in this case.\nFigure 25.7 Adding a micro-service API\nCONCLUSION\nWhat does all this mean? Why have I taken this absurdly simply program, which could be\n", "page": 179, "type": "text", "section": "Page 179"}
{"text": "implemented in 200 lines of Kornshell, and extrapolated it out with all these crazy architectural\nboundaries?\nThis example is intended to show that architectural boundaries exist everywhere. We, as architects,\nmust be careful to recognize when they are needed. We also have to be aware that such boundaries,\nwhen fully implemented, are expensive. At the same time, we have to recognize that when such\nboundaries are ignored, they are very expensive to add in later\u2014even in the presence of\ncomprehensive test-suites and refactoring discipline.\nSo what do we do, we architects? The answer is dissatisfying. On the one hand, some very smart\npeople have told us, over the years, that we should not anticipate the need for abstraction. This is the\nphilosophy of YAGNI: \u201cYou aren\u2019t going to need it.\u201d There is wisdom in this message, since over-\nengineering is often much worse than under-engineering. On the other hand, when you discover that\nyou truly do need an architectural boundary where none exists, the costs and risks can be very high to\nadd such a boundary.\nSo there you have it. O Software Architect, you must see the future. You must guess\u2014intelligently.\nYou must weigh the costs and determine where the architectural boundaries lie, and which should be\nfully implemented, and which should be partially implemented, and which should be ignored.\nBut this is not a one-time decision. You don\u2019t simply decide at the start of a project which boundaries\nto implement and which to ignore. Rather, you watch. You pay attention as the system evolves. You\nnote where boundaries may be required, and then carefully watch for the first inkling of friction\nbecause those boundaries don\u2019t exist.\nAt that point, you weigh the costs of implementing those boundaries versus the cost of ignoring them\n\u2014and you review that decision frequently. Your goal is to implement the boundaries right at the\ninflection point where the cost of implementing becomes less than the cost of ignoring.\nIt takes a watchful eye.\n1. It should be just as clear that we would not apply the clean architecture approach to something as trivial as this game. After all, the\nentire program can probably be written in 200 lines of code or less. In this case, we\u2019re using a simple program as a proxy for a much\nlarger system with significant architectural boundaries.\n2. If you are confused by the direction of the arrows, remember that they point in the direction of source code dependencies, not in the\ndirection of data flow.\n3. In days long past, we would have called that top component the Central Transform. See Practical Guide to Structured Systems\nDesign, 2nd ed., Meilir Page-Jones, 1988.\n", "page": 180, "type": "text", "section": "Page 180"}
{"text": "26\nTHE MAIN COMPONENT\nIn every system, there is at least one component that creates, coordinates, and oversees the others. I\ncall this component Main.\nTHE ULTIMATE DETAIL\nThe Main component is the ultimate detail\u2014the lowest-level policy. It is the initial entry point of the\nsystem. Nothing, other than the operating system, depends on it. Its job is to create all the Factories,\nStrategies, and other global facilities, and then hand control over to the high-level abstract portions of\nthe system.\nIt is in this Main component that dependencies should be injected by a Dependency Injection\nframework. Once they are injected into Main, Main should distribute those dependencies normally,\nwithout using the framework.\nThink of Main as the dirtiest of all the dirty components.\nConsider the following Main component from a recent version of Hunt the Wumpus. Notice how it\nloads up all the strings that we don\u2019t want the main body of the code to know about.\nClick here to view code image\n", "page": 181, "type": "text", "section": "Page 181"}
{"text": "public class Main implements HtwMessageReceiver {\n  private static HuntTheWumpus game;\n  private static int hitPoints = 10;\n  private static final List<String> caverns = new   ArrayList<>();\n  private static final String[] environments = new String[]{\n    \"bright\",\n    \"humid\",\n    \"dry\",\n    \"creepy\",\n    \"ugly\",\n    \"foggy\",\n    \"hot\",\n    \"cold\",\n    \"drafty\",\n    \"dreadful\"\n  };\n \n  private static final String[] shapes = new String[] {\n    \"round\",\n    \"square\",\n    \"oval\",\n    \"irregular\",\n    \"long\",\n    \"craggy\",\n    \"rough\",\n    \"tall\",\n    \"narrow\"\n  };\n \n  private static final String[] cavernTypes = new String[] {\n    \"cavern\",\n    \"room\",\n    \"chamber\",\n    \"catacomb\",\n    \"crevasse\",\n    \"cell\",\n    \"tunnel\",\n    \"passageway\",\n    \"hall\",\n    \"expanse\"\n  };\n \n  private static final String[] adornments = new String[] {\n   \"smelling of sulfur\",\n    \"with engravings on the walls\",\n    \"with a bumpy floor\",\n    \"\",\n    \"littered with garbage\",\n    \"spattered with guano\",\n    \"with piles of Wumpus droppings\",\n    \"with bones scattered around\",\n    \"with a corpse on the floor\",\n    \"that seems to vibrate\",\n    \"that feels stuffy\",\n    \"that fills you with dread\"\n  };\nNow here\u2019s the main function. Notice how it uses the HtwFactory to create the game. It passes in the\nname of the class, htw.game.HuntTheWumpusFacade, because that class is even dirtier than Main.\n", "page": 182, "type": "text", "section": "Page 182"}
{"text": "This prevents changes in that class from causing Main to recompile/redeploy.\nClick here to view code image\npublic static void main(String[] args) throws IOException {\n   game = HtwFactory.makeGame(\"htw.game.HuntTheWumpusFacade\",\n                                 new Main());\n   createMap();\n   BufferedReader br = \n     new BufferedReader(new InputStreamReader(System.in));\n   game.makeRestCommand().execute();\n   while (true) {\n     System.out.println(game.getPlayerCavern());\n     System.out.println(\"Health: \" + hitPoints + \" arrows: \" + \n                           game.getQuiver());\n     HuntTheWumpus.Command c = game.makeRestCommand();\n      System.out.println(\">\");\n      String command = br.readLine();\n      if (command.equalsIgnoreCase(\"e\"))\n        c = game.makeMoveCommand(EAST);\n      else if (command.equalsIgnoreCase(\"w\"))\n        c = game.makeMoveCommand(WEST);\n      else if (command.equalsIgnoreCase(\"n\"))\n        c = game.makeMoveCommand(NORTH);\n      else if (command.equalsIgnoreCase(\"s\"))\n        c = game.makeMoveCommand(SOUTH);\n      else if (command.equalsIgnoreCase(\"r\"))\n        c = game.makeRestCommand();\n      else if (command.equalsIgnoreCase(\"sw\"))\n        c = game.makeShootCommand(WEST);\n      else if (command.equalsIgnoreCase(\"se\"))\n        c = game.makeShootCommand(EAST);\n      else if (command.equalsIgnoreCase(\"sn\"))\n        c = game.makeShootCommand(NORTH);\n      else if (command.equalsIgnoreCase(\"ss\"))\n        c = game.makeShootCommand(SOUTH);\n      else if (command.equalsIgnoreCase(\"q\"))\n        return;\n \n      c.execute();\n    }\n  }\nNotice also that main creates the input stream and contains the main loop of the game, interpreting the\nsimple input commands, but then defers all processing to other, higher-level components.\nFinally, notice that main creates the map.\nClick here to view code image\nprivate static void createMap() {\n   int nCaverns = (int) (Math.random() * 30.0 + 10.0);\n   while (nCaverns-- > 0)\n     caverns.add(makeName());\n \n    for (String cavern : caverns) {\n      maybeConnectCavern(cavern, NORTH);\n      maybeConnectCavern(cavern, SOUTH);\n", "page": 183, "type": "text", "section": "Page 183"}
{"text": "      maybeConnectCavern(cavern, EAST);\n      maybeConnectCavern(cavern, WEST);\n    }\n \n    String playerCavern = anyCavern();\n    game.setPlayerCavern(playerCavern);\n    game.setWumpusCavern(anyOther(playerCavern));\n    game.addBatCavern(anyOther(playerCavern));\n    game.addBatCavern(anyOther(playerCavern));\n    game.addBatCavern(anyOther(playerCavern));\n \n    game.addPitCavern(anyOther(playerCavern));\n    game.addPitCavern(anyOther(playerCavern));\n    game.addPitCavern(anyOther(playerCavern));\n \n    game.setQuiver(5);\n  }\n \n  // much code removed\u2026\n}\nThe point is that Main is a dirty low-level module in the outermost circle of the clean architecture. It\nloads everything up for the high level system, and then hands control over to it.\nCONCLUSION\nThink of Main as a plugin to the application\u2014a plugin that sets up the initial conditions and\nconfigurations, gathers all the outside resources, and then hands control over to the high-level policy\nof the application. Since it is a plugin, it is possible to have many Main components, one for each\nconfiguration of your application.\nFor example, you could have a Main plugin for Dev, another for Test, and yet another for Production.\nYou could also have a Main plugin for each country you deploy to, or each jurisdiction, or each\ncustomer.\nWhen you think about Main as a plugin component, sitting behind an architectural boundary, the\nproblem of configuration becomes a lot easier to solve.\n", "page": 184, "type": "text", "section": "Page 184"}
{"text": "27\nSERVICES: GREAT AND SMALL\nService-oriented \u201carchitectures\u201d and micro-service \u201carchitectures\u201d have become very popular of\nlate. The reasons for their current popularity include the following:\n\u2022 Services seem to be strongly decoupled from each other. As we shall see, this is only partially true.\n\u2022 Services appear to support independence of development and deployment. Again, as we shall see,\nthis is only partially true.\nSERVICE ARCHITECTURE?\nFirst, let\u2019s consider the notion that using services, by their nature, is an architecture. This is patently\nuntrue. The architecture of a system is defined by boundaries that separate high-level policy from\nlow-level detail and follow the Dependency Rule. Services that simply separate application\nbehaviors are little more than expensive function calls, and are not necessarily architecturally\nsignificant.\nThis is not to say that all services should be architecturally significant. There are often substantial\nbenefits to creating services that separate functionality across processes and platforms\u2014whether they\nobey the Dependency Rule or not. It\u2019s just that services, in and of themselves, do not define an\narchitecture.\n", "page": 185, "type": "text", "section": "Page 185"}
{"text": "A helpful analogy is the organization of functions. The architecture of a monolithic or component-\nbased system is defined by certain function calls that cross architectural boundaries and follow the\nDependency Rule. Many other functions in those systems, however, simply separate one behavior\nfrom another and are not architecturally significant.\nSo it is with services. Services are, after all, just function calls across process and/or platform\nboundaries. Some of those services are architecturally significant, and some aren\u2019t. Our interest, in\nthis chapter, is with the former.\nSERVICE BENEFITS?\nThe question mark in the preceding heading indicates that this section is going to challenge the current\npopular orthodoxy of service architecture. Let\u2019s tackle the benefits one at a time.\nTHE DECOUPLING FALLACY\nOne of the big supposed benefits of breaking a system up into services is that services are strongly\ndecoupled from each other. After all, each service runs in a different process, or even a different\nprocessor; therefore those services do not have access to each other\u2019s variables. What\u2019s more, the\ninterface of each service must be well defined.\nThere is certainly some truth to this\u2014but not very much truth. Yes, services are decoupled at the level\nof individual variables. However, they can still be coupled by shared resources within a processor,\nor on the network. What\u2019s more, they are strongly coupled by the data they share.\nFor example, if a new field is added to a data record that is passed between services, then every\nservice that operates on the new field must be changed. The services must also strongly agree about\nthe interpretation of the data in that field. Thus those services are strongly coupled to the data record\nand, therefore, indirectly coupled to each other.\nAs for interfaces being well defined, that\u2019s certainly true\u2014but it is no less true for functions. Service\ninterfaces are no more formal, no more rigorous, and no better defined than function interfaces.\nClearly, then, this benefit is something of an illusion.\nTHE FALLACY OF INDEPENDENT DEVELOPMENT AND DEPLOYMENT\nAnother of the supposed benefits of services is that they can be owned and operated by a dedicated\nteam. That team can be responsible for writing, maintaining, and operating the service as part of a\ndev-ops strategy. This independence of development and deployment is presumed to be scalable. It is\nbelieved that large enterprise systems can be created from dozens, hundreds, or even thousands of\nindependently developable and deployable services. Development, maintenance, and operation of the\nsystem can be partitioned between a similar number of independent teams.\nThere is some truth to this belief\u2014but only some. First, history has shown that large enterprise\n", "page": 186, "type": "text", "section": "Page 186"}
{"text": "systems can be built from monoliths and component-based systems as well as service-based systems.\nThus services are not the only option for building scalable systems.\nSecond, the decoupling fallacy means that services cannot always be independently developed,\ndeployed, and operated. To the extent that they are coupled by data or behavior, the development,\ndeployment, and operation must be coordinated.\nTHE KITTY PROBLEM\nAs an example of these two fallacies, let\u2019s look at our taxi aggregator system again. Remember, this\nsystem knows about many taxi providers in a given city, and allows customers to order rides. Let\u2019s\nassume that the customers select taxis based on a number of criteria, such as pickup time, cost, luxury,\nand driver experience.\nWe wanted our system to be scalable, so we chose to build it out of lots of little micro-services. We\nsubdivided our development staff into many small teams, each of which is responsible for\ndeveloping, maintaining, and operating a correspondingly1 small number of services.\nThe diagram in Figure 27.1 shows how our fictitious architects arranged services to implement this\napplication. The TaxiUI service deals with the customers, who use mobile devices to order taxis.\nThe TaxiFinder service examines the inventories of the various TaxiSuppliers and determines\nwhich taxies are possible candidates for the user. It deposits these into a short-term data record\nattached to that user. The TaxiSelector service takes the user\u2019s criteria of cost, time, luxury, and so\nforth, and chooses an appropriate taxi from among the candidates. It hands that taxi off to the\nTaxiDispatcher service, which orders the appropriate taxi.\nFigure 27.1 Services arranged to implement the taxi aggregator system\nNow let us suppose that this system has been in operation for more than a year. Our staff of\ndevelopers have been happily developing new features while maintaining and operating all these\nservices.\n", "page": 187, "type": "text", "section": "Page 187"}
{"text": "One bright and cheerful day, the marketing department holds a meeting with the development team. In\nthis meeting, they announce their plans to offer a kitten delivery service to the city. Users can order\nkittens to be delivered to their homes or to their places of business.\nThe company will set up several kitten collection points across the city. When a kitten order is\nplaced, a nearby taxi will be selected to collect a kitten from one of those collection points, and then\ndeliver it to the appropriate address.\nOne of the taxi suppliers has agreed to participate in this program. Others are likely to follow. Still\nothers may decline.\nOf course, some drivers may be allergic to cats, so those drivers should never be selected for this\nservice. Also, some customers will undoubtedly have similar allergies, so a vehicle that has been\nused to deliver kittens within the last 3 days should not be selected for customers who declare such\nallergies.\nLook at that diagram of services. How many of those services will have to change to implement this\nfeature? All of them. Clearly, the development and deployment of the kitty feature will have to be very\ncarefully coordinated.\nIn other words, the services are all coupled, and cannot be independently developed, deployed, and\nmaintained.\nThis is the problem with cross-cutting concerns. Every software system must face this problem,\nwhether service oriented or not. Functional decompositions, of the kind depicted in the service\ndiagram in Figure 27.1, are very vulnerable to new features that cut across all those functional\nbehaviors.\nOBJECTS TO THE RESCUE\nHow would we have solved this problem in a component-based architecture? Careful consideration\nof the SOLID design principles would have prompted us to create a set of classes that could be\npolymorphically extended to handle new features.\nThe diagram in Figure 27.2 shows the strategy. The classes in this diagram roughly correspond to the\nservices shown in Figure 27.1. However, note the boundaries. Note also that the dependencies follow\nthe Dependency Rule.\nMuch of the logic of the original services is preserved within the base classes of the object model.\nHowever, that portion of the logic that was specific to rides has been extracted into a Rides\ncomponent. The new feature for kittens has been placed into a Kittens component. These two\ncomponents override the abstract base classes in the original components using a pattern such as\nTemplate Method or Strategy.\nNote again that the two new components, Rides and Kittens, follow the Dependency Rule. Note\n", "page": 188, "type": "text", "section": "Page 188"}
{"text": "also that the classes that implement those features are created by factories under the control of the UI.\nClearly, in this scheme, when the Kitty feature is implemented, the TaxiUI must change. But nothing\nelse needs to be changed. Rather, a new jar file, or Gem, or DLL is added to the system and\ndynamically loaded at runtime.\nThus the Kitty feature is decoupled, and independently developable and deployable.\nFigure 27.2 Using an object-oriented approach to deal with cross-cutting concerns\nCOMPONENT-BASED SERVICES\nThe obvious question is: Can we do that for services? And the answer is, of course: Yes! Services do\nnot need to be little monoliths. Services can, instead, be designed using the SOLID principles, and\ngiven a component structure so that new components can be added to them without changing the\n", "page": 189, "type": "text", "section": "Page 189"}
{"text": "existing components within the service.\nThink of a service in Java as a set of abstract classes in one or more jar files. hink of each new\nfeature or feature extension as another jar file that contains classes that extend the abstract classes in\nthe first jar files. Deploying a new feature then becomes not a matter of redeploying the services, but\nrather a matter of simply adding the new jar files to the load paths of those services. In other words,\nadding new features conforms to the Open-Closed Principle.\nThe service diagram in Figure 27.3 shows the structure. The services still exist as before, but each\nhas its own internal component design, allowing new features to be added as new derivative classes.\nThose derivative classes live within their own components.\nFigure 27.3 Each service has its own internal component design, enabling new features to be added as new derivative classes\nCROSS-CUTTING CONCERNS\nWhat we have learned is that architectural boundaries do not fall between services. Rather, those\nboundaries run through the services, dividing them into components.\n", "page": 190, "type": "text", "section": "Page 190"}
{"text": "To deal with the cross-cutting concerns that all significant systems face, services must be designed\nwith internal component architectures that follow the Dependency Rule, as shown in the diagram in\nFigure 27.4. Those services do not define the architectural boundaries of the system; instead, the\ncomponents within the services do.\nFigure 27.4 Services must be designed with internal component architectures that follow the Dependency Rule\nCONCLUSION\nAs useful as services are to the scalability and develop-ability of a system, they are not, in and of\nthemselves, architecturally significant elements. The architecture of a system is defined by the\nboundaries drawn within that system, and by the dependencies that cross those boundaries. That\narchitecture is not defined by the physical mechanisms by which elements communicate and execute.\nA service might be a single component, completely surrounded by an architectural boundary.\nAlternatively, a service might be composed of several components separated by architectural\nboundaries. In rare2 cases, clients and services may be so coupled as to have no architectural\nsignificance whatever.\n1. Therefore the number of micro-services will be roughly equal to the number of programmers.\n2. We hope they are rare. Unfortunately, experience suggests otherwise.\n", "page": 191, "type": "text", "section": "Page 191"}
{"text": "28\nTHE TEST BOUNDARY\nYes, that\u2019s right: The tests are part of the system, and they participate in the architecture just like\nevery other part of the system does. In some ways, that participation is pretty normal. In other ways, it\ncan be pretty unique.\nTESTS AS SYSTEM COMPONENTS\nThere is a great deal of confusion about tests. Are they part of the system? Are they separate from the\nsystem? Which kinds of tests are there? Are unit tests and integration tests different things? What\nabout acceptance tests, functional tests, Cucumber tests, TDD tests, BDD tests, component tests, and\nso on?\nIt is not the role of this book to get embroiled in that particular debate, and fortunately it isn\u2019t\nnecessary. From an architectural point of view, all tests are the same. Whether they are the tiny little\ntests created by TDD, or large FitNesse, Cucumber, SpecFlow, or JBehave tests, they are\narchitecturally equivalent.\nTests, by their very nature, follow the Dependency Rule; they are very detailed and concrete; and they\nalways depend inward toward the code being tested. In fact, you can think of the tests as the outermost\ncircle in the architecture. Nothing within the system depends on the tests, and the tests always depend\ninward on the components of the system.\n", "page": 192, "type": "text", "section": "Page 192"}
{"text": "Tests are also independently deployable. In fact, most of the time they are deployed in test systems,\nrather than in production systems. So, even in systems where independent deployment is not\notherwise necessary, the tests will still be independently deployed.\nTests are the most isolated system component. They are not necessary for system operation. No user\ndepends on them. Their role is to support development, not operation. And yet, they are no less a\nsystem component than any other. In fact, in many ways they represent the model that all other system\ncomponents should follow.\nDESIGN FOR TESTABILITY\nThe extreme isolation of the tests, combined with the fact that they are not usually deployed, often\ncauses developers to think that tests fall outside of the design of the system. This is a catastrophic\npoint of view. Tests that are not well integrated into the design of the system tend to be fragile, and\nthey make the system rigid and difficult to change.\nThe issue, of course, is coupling. Tests that are strongly coupled to the system must change along with\nthe system. Even the most trivial change to a system component can cause many coupled tests to break\nor require changes.\nThis situation can become acute. Changes to common system components can cause hundreds, or even\nthousands, of tests to break. This is known as the Fragile Tests Problem.\nIt is not hard to see how this can happen. Imagine, for example, a suite of tests that use the GUI to\nverify business rules. Such tests may start on the login screen and then navigate through the page\nstructure until they can check particular business rules. Any change to the login page, or the navigation\nstructure, can cause an enormous number of tests to break.\nFragile tests often have the perverse effect of making the system rigid. When developers realize that\nsimple changes to the system can cause massive test failures, they may resist making those changes.\nFor example, imagine the conversation between the development team and a marketing team that\nrequests a simple change to the page navigation structure that will cause 1000 tests to break.\nThe solution is to design for testability. The first rule of software design\u2014whether for testability or\nfor any other reason\u2014is always the same: Don\u2019t depend on volatile things. GUIs are volatile. Test\nsuites that operate the system through the GUI must be fragile. Therefore design the system, and the\ntests, so that business rules can be tested without using the GUI.\nTHE TESTING API\nThe way to accomplish this goal is to create a specific API that the tests can use to verify all the\nbusiness rules. This API should have superpowers that allow the tests to avoid security constraints,\nbypass expensive resources (such as databases), and force the system into particular testable states.\nThis API will be a superset of the suite of interactors and interface adapters that are used by the\n", "page": 193, "type": "text", "section": "Page 193"}
{"text": "user interface.\nThe purpose of the testing API is to decouple the tests from the application. This decoupling\nencompasses more than just detaching the tests from the UI: The goal is to decouple the structure of\nthe tests from the structure of the application.\nSTRUCTURAL COUPLING\nStructural coupling is one of the strongest, and most insidious, forms of test coupling. Imagine a test\nsuite that has a test class for every production class, and a set of test methods for every production\nmethod. Such a test suite is deeply coupled to the structure of the application.\nWhen one of those production methods or classes changes, a large number of tests must change as\nwell. Consequently, the tests are fragile, and they make the production code rigid.\nThe role of the testing API is to hide the structure of the application from the tests. This allows the\nproduction code to be refactored and evolved in ways that don\u2019t affect the tests. It also allows the\ntests to be refactored and evolved in ways that don\u2019t affect the production code.\nThis separation of evolution is necessary because as time passes, the tests tend to become\nincreasingly more concrete and specific. In contrast, the production code tends to become\nincreasingly more abstract and general. Strong structural coupling prevents\u2014or at least impedes\u2014\nthis necessary evolution, and prevents the production code from being as general, and flexible, as it\ncould be.\nSECURITY\nThe superpowers of the testing API could be dangerous if they were deployed in production systems.\nIf this is a concern, then the testing API, and the dangerous parts of its implementation, should be kept\nin a separate, independently deployable component.\nCONCLUSION\nTests are not outside the system; rather, they are parts of the system that must be well designed if they\nare to provide the desired benefits of stability and regression. Tests that are not designed as part of\nthe system tend to be fragile and difficult to maintain. Such tests often wind up on the maintenance\nroom floor\u2014discarded because they are too difficult to maintain.\n", "page": 194, "type": "text", "section": "Page 194"}
{"text": "29\nCLEAN EMBEDDED ARCHITECTURE\nBy James Grenning\nA while ago I read an article entitled \u201cThe Growing Importance of Sustaining Software for the DoD\u201d1\non Doug Schmidt\u2019s blog. Doug made the following claim:\n\u201cAlthough software does not wear out, firmware and hardware become obsolete, thereby requiring software\nmodifications.\u201d\nIt was a clarifying moment for me. Doug mentioned two terms that I would have thought to be obvious\n\u2014but maybe not. Software is this thing that can have a long useful life, but firmware will become\nobsolete as hardware evolves. If you have spent any time in embedded systems development, you\nknow the hardware is continually evolving and being improved. At the same time, features are added\nto the new \u201csoftware,\u201d and it continually grows in complexity.\nI\u2019d like to add to Doug\u2019s statement:\nAlthough software does not wear out, it can be destroyed from within by unmanaged dependencies on firmware and\nhardware.\nIt is not uncommon for embedded software to be denied a potentially long life due to being infected\nwith dependencies on hardware.\nI like Doug\u2019s definition of firmware, but let\u2019s see which other definitions are out there. I found these\n", "page": 195, "type": "text", "section": "Page 195"}
{"text": "alternatives:\n\u2022 \u201cFirmware is held in non-volatile memory devices such as ROM, EPROM, or flash memory.\u201d\n(https://en.wikipedia.org/wiki/Firmware)\n\u2022 \u201cFirmware is a software program or set of instructions programmed on a hardware device.\u201d\n(https://techterms.com/definition/firmware)\n\u2022 \u201cFirmware is software that is embedded in a piece of hardware.\u201d (https://www.lifewire.com/what-\nis-firmware-2625881)\n\u2022 Firmware is \u201cSoftware (programs or data) that has been written onto read-only memory (ROM).\u201d\n(http://www.webopedia.com/TERM/F/firmware.html)\nDoug\u2019s statement makes me realize that these accepted definitions of firmware are wrong, or at least\nobsolete. Firmware does not mean code lives in ROM. It\u2019s not firmware because of where it is\nstored; rather, it is firmware because of what it depends on and how hard it is to change as hardware\nevolves. Hardware does evolve (pause and look at your for phone for evidence), so we should\nstructure our embedded code with that reality in mind.\nI have nothing against firmware, or firmware engineers (I\u2019ve been known to write some firmware\nmyself). But what we really need is less firmware and more software. Actually, I am disappointed\nthat firmware engineers write so much firmware!\nNon-embedded engineers also write firmware! You non-embedded developers essentially write\nfirmware whenever you bury SQL in your code or when you spread platform dependencies throughout\nyour code. Android app developers write firmware when they don\u2019t separate their business logic\nfrom the Android API.\nI\u2019ve been involved in a lot of efforts where the line between the product code (the software) and the\ncode that interacts with the product\u2019s hardware (the firmware) is fuzzy to the point of nonexistence.\nFor example, in the late 1990s I had the fun of helping redesign a communications subsystem that was\ntransitioning from time-division multiplexing (TDM) to voice over IP (VOIP). VOIP is how things are\ndone now, but TDM was considered the state of the art from the 1950s and 1960s, and was widely\ndeployed in the 1980s and 1990s.\nWhenever we had a question for the systems engineer about how a call should react to a given\nsituation, he would disappear and a little later emerge with a very detailed answer. \u201cWhere did he get\nthat answer?\u201d we asked. \u201cFrom the current product\u2019s code,\u201d he\u2019d answer. The tangled legacy code\nwas the spec for the new product! The existing implementation had no separation between TDM and\nthe business logic of making calls. The whole product was hardware/technology dependent from top\nto bottom and could not be untangled. The whole product had essentially become firmware.\nConsider another example: Command messages arrive to this system via serial port. Unsurprisingly,\nthere is a message processor/dispatcher. The message processor knows the format of messages, is\nable to parse them, and can then dispatch the message to the code that can handle the request. None of\nthis is surprising, except that the message processor/dispatcher resides in the same file as code that\ninteracts with a UART2 hardware. The message processor is polluted with UART details. The\n", "page": 196, "type": "text", "section": "Page 196"}
{"text": "message processor could have been software with a potentially long useful life, but instead it is\nfirmware. The message processor is denied the opportunity to become software\u2014and that is just not\nright!\nI\u2019ve known and understood the need for separating software from hardware for a long time, but\nDoug\u2019s words clarified how to use the terms software and firmware in relationship to each other.\nFor engineers and programmers, the message is clear: Stop writing so much firmware and give your\ncode a chance at a long useful life. Of course, demanding it won\u2019t make it so. Let\u2019s look at how we\ncan keep embedded software architecture clean to give the software a fighting chance of having a long\nand useful life.\nAPP-TITUDE TEST\nWhy does so much potential embedded software become firmware? It seems that most of the\nemphasis is on getting the embedded code to work, and not so much emphasis is placed on structuring\nit for a long useful life. Kent Beck describes three activities in building software (the quoted text is\nKent\u2019s words and the italics are my commentary):\n1. \u201cFirst make it work.\u201d Y\nou are out of business if it doesn\u2019t work.\n2. \u201cThen make it right.\u201d Refactor the code so that you and others can understand it and evolve it as\nneeds change or are better understood.\n3. \u201cThen make it fast.\u201d Refactor the code for \u201cneeded\u201d performance.\nMuch of the embedded systems software that I see in the wild seems to have been written with \u201cMake\nit work\u201d in mind\u2014and perhaps also with an obsession for the \u201cMake it fast\u201d goal, achieved by adding\nmicro-optimizations at every opportunity. In The Mythical Man-Month, Fred Brooks suggests we\n\u201cplan to throw one away.\u201d Kent and Fred are giving virtually the same advice: Learn what works,\nthen make a better solution.\nEmbedded software is not special when it comes to these problems. Most non-embedded apps are\nbuilt just to work, with little regard to making the code right for a long useful life.\nGetting an app to work is what I call the App-titude test for a programmer. Programmers, embedded\nor not, who just concern themselves with getting their app to work are doing their products and\nemployers a disservice. There is much more to programming than just getting an app to work.\nAs an example of code produced while passing the App-titude test, check out these functions located\nin one file of a small embedded system:\nClick here to view code image\nISR(TIMER1_vect) { ... }\nISR(INT2_vect) { ... }\nvoid btn_Handler(void) { ... }\nfloat calc_RPM(void) { ... }\n", "page": 197, "type": "text", "section": "Page 197"}
{"text": "static char Read_RawData(void) { ... }\nvoid Do_Average(void) { ... }\nvoid Get_Next_Measurement(void) { ... }\nvoid Zero_Sensor_1(void) { ... }\nvoid Zero_Sensor_2(void) { ... }\nvoid Dev_Control(char Activation) { ... }\nchar Load_FLASH_Setup(void) { ... }\nvoid Save_FLASH_Setup(void) { ... }\nvoid Store_DataSet(void) { ... }\nfloat bytes2float(char bytes[4]) { ... }\nvoid Recall_DataSet(void) { ... }\nvoid Sensor_init(void) { ... }\nvoid uC_Sleep(void) { ... }\nThat list of functions is in the order I found them in the source file. Now I\u2019ll separate them and group\nthem by concern:\n\u2022 Functions that have domain logic\nClick here to view code image\nfloat calc_RPM(void) { ... }\nvoid Do_Average(void) { ... }\nvoid Get_Next_Measurement(void) { ... }\nvoid Zero_Sensor_1(void) { ... }\nvoid Zero_Sensor_2(void) { ... }\n\u2022 Functions that set up the hardware platform\nClick here to view code image\nISR(TIMER1_vect) { ... }*\nISR(INT2_vect) { ... }\nvoid uC_Sleep(void) { ... }\nFunctions that react to the on off button press\nvoid btn_Handler(void) { ... }\nvoid Dev_Control(char Activation) { ... }\nA Function that can get A/D input readings from the hardware\nstatic char Read_RawData(void) { ... }\n\u2022 Functions that store values to the persistent storage\nClick here to view code image\nchar Load_FLASH_Setup(void) { ... }\nvoid Save_FLASH_Setup(void) { ... }\nvoid Store_DataSet(void) { ... }\nfloat bytes2float(char bytes[4]) { ... }\nvoid Recall_DataSet(void) { ... }\n\u2022 Function that does not do what its name implies\nClick here to view code image\nvoid Sensor_init(void) { ... }\nLooking at some of the other files in this application, I found many impediments to understanding the\ncode. I also found a file structure that implied that the only way to test any of this code is in the\n", "page": 198, "type": "text", "section": "Page 198"}
{"text": "embedded target. Virtually every bit of this code knows it is in a special microprocessor architecture,\nusing \u201cextended\u201d C constructs3 that tie the code to a particular tool chain and microprocessor. There\nis no way for this code to have a long useful life unless the product never needs to be moved to a\ndifferent hardware environment.\nThis application works: The engineer passed the App-titude test. But the application can\u2019t be said to\nhave a clean embedded architecture.\nTHE TARGET-HARDW\nARE BOTTLENECK\nThere are many special concerns that embedded developers have to deal with that non-embedded\ndevelopers do not\u2014for example, limited memory space, real-time constraints and deadlines, limited\nIO, unconventional user interfaces, and sensors and connections to the real world. Most of the time\nthe hardware is concurrently developed with the software and firmware. As an engineer developing\ncode for this kind of system, you may have no place to run the code. If that\u2019s not bad enough, once you\nget the hardware, it is likely that the hardware will have its own defects, making software\ndevelopment progress even slower than usual.\nYes, embedded is special. Embedded engineers are special. But embedded development is not so\nspecial that the principles in this book are not applicable to embedded systems.\nOne of the special embedded problems is the target-hardware bottleneck. When embedded code is\nstructured without applying clean architecture principles and practices, you will often face the\nscenario in which you can test your code only on the target. If the target is the only place where testing\nis possible, the target-hardware bottleneck will slow you down.\nA CLEAN EMBEDDED ARCHITECTURE IS A TESTABLE EMBEDDED\nARCHITECTURE\nLet\u2019s see how to apply some of the architectural principles to embedded software and firmware to\nhelp you eliminate the target-hardware bottleneck.\nLayers\nLayering comes in many flavors. Let\u2019s start with three layers, as shown in Figure 29.1. At the bottom,\nthere is the hardware. As Doug warns us, due to technology advances and Moore\u2019s law, the hardware\nwill change. Parts become obsolete, and new parts use less power or provide better performance or\nare cheaper. Whatever the reason, as an embedded engineer, I don\u2019t want to have a bigger job than is\nnecessary when the inevitable hardware change finally happens.\n", "page": 199, "type": "text", "section": "Page 199"}
{"text": "Figure 29.1 Three layers\nThe separation between hardware and the rest of the system is a given\u2014at least once the hardware is\ndefined (Figure 29.2). Here is where the problems often begin when you are trying to pass the App-\ntitude test. There is nothing that keeps hardware knowledge from polluting all the code. If you are not\ncareful about where you put things and what one module is allowed to know about another module,\nthe code will be very hard to change. I\u2019m not just talking about when the hardware changes, but when\nthe user asks for a change, or when a bug needs to be fixed.\nFigure 29.2 Hardware must be separated from the rest of the system\nSoftware and firmware intermingling is an anti-pattern. Code exhibiting this anti-pattern will resist\nchanges. In addition, changes will be dangerous, often leading to unintended consequences. Full\nregression tests of the whole system will be needed for minor changes. If you have not created\nexternally instrumented tests, expect to get bored with manual tests\u2014and then you can expect new bug\nreports.\nThe Hardware Is a Detail\n", "page": 200, "type": "text", "section": "Page 200"}
{"text": "The line between software and firmware is typically not so well defined as the line between code\nand hardware, as shown in Figure 29.3.\nFigure 29.3 The line between software and firmware is a bit fuzzier than the line between code and hardware\nOne of your jobs as an embedded software developer is to firm up that line. The name of the\nboundary between the software and the firmware is the hardware abstraction layer (HAL) (Figure\n29.4). This is not a new idea: It has been in PCs since the days before Windows.\nFigure 29.4 The hardware abstraction layer\nThe HAL exists for the software that sits on top of it, and its API should be tailored to that software\u2019s\nneeds. As an example, the firmware can store bytes and arrays of bytes into flash memory. In contrast,\nthe application needs to store and read name/value pairs to some persistence mechanism. The\nsoftware should not be concerned that the name/value pairs are stored in flash memory, a spinning\ndisk, the cloud, or core memory. The HAL provides a service, and it does not reveal to the software\nhow it does it. The flash implementation is a detail that should be hidden from software.\nAs another example, an LED is tied to a GPIO bit. The firmware could provide access to the GPIO\n", "page": 201, "type": "text", "section": "Page 201"}
{"text": "bits, where a HAL might provide Led_TurnOn(5). That is a pretty low-level hardware abstraction\nlayer. Let\u2019s consider raising the level of abstraction from a hardware perspective to the\nsoftware/product perspective. What is the LED indicating? Suppose that it indicated low battery\npower. At some level, the firmware (or a board support package) could provide Led_TurnOn(5),\nwhile the HAL provides Indicate_LowBattery(). You can see the HAL expressing services needed\nby the application. You can also see that layers may contain layers. It is more of a repeating fractal\npattern than a limited set of predefined layers. The GPIO assignments are details that should be\nhidden from the software.\nDON\u2019T REVEAL HARDW\nARE DETAILS TO THE USER OF THE HAL\nA clean embedded architecture\u2019s software is testable off the target hardware. A successful HAL\nprovides that seam or set of substitution points that facilitate off-target testing.\nThe Processor Is a Detail\nWhen your embedded application uses a specialized tool chain, it will often provide header files to\n<i>help you</i>.4 These compilers often take liberties with the C language, adding new keywords to\naccess their processor features. The code will look like C, but it is no longer C.\nSometimes vendor-supplied C compilers provide what look like global variables to give access\ndirectly to processor registers, IO ports, clock timers, IO bits, interrupt controllers, and other\nprocessor functions. It is helpful to get access to these things easily, but realize that any of your code\nthat uses these helpful facilities is no longer C. It won\u2019t compile for another processor, or maybe even\nwith a different compiler for the same processor.\nI would hate to think that the silicon and tool provider is being cynical, tying your product to the\ncompiler. Let\u2019s give the provider the benefit of a doubt by assuming that it is truly trying to help. But\nnow it\u2019s up to you to use that help in a way that does not hurt in the future. You will have to limit\nwhich files are allowed to know about the C extensions.\nLet\u2019s look at this header file designed for the ACME family of DSPs\u2014you know, the ones used by\nWile E. Coyote:\nClick here to view code image\n#ifndef _ACME_STD_TYPES\n#define _ACME_STD_TYPES\n \nClick here to view code image\n#if defined(_ACME_X42)\n    typedef unsigned int        Uint_32;\n    typedef unsigned short      Uint_16;\n    typedef unsigned char       Uint_8;\n \n    typedef int                 Int_32;\n    typedef short               Int_16;\n", "page": 202, "type": "text", "section": "Page 202"}
{"text": "    typedef char                Int_8;\n \n#elif defined(_ACME_A42)\n    typedef unsigned long       Uint_32;\n    typedef unsigned int        Uint_16;\n    typedef unsigned char       Uint_8;\n \n    typedef long                Int_32;\n    typedef int                 Int_16;\n    typedef char                Int_8;\n#else\n    #error <acmetypes.h> is not supported for this environment\n#endif\n \n#endif\nThe acmetypes.h header file should not be used directly. If you do, your code gets tied to one of the\nACME DSPs. You are using an ACME DSP, you say, so what is the harm? You can\u2019t compile your\ncode unless you include this header. If you use the header and define _ACME_X42 or _ACME_A42, your\nintegers will be the wrong size if you try to test your code off-target. If that is not bad enough, one day\nyou\u2019ll want to port your application to another processor, and you will have made that task much\nmore difficult by not choosing portability and by not limiting what files know about ACME.\nInstead of using acmetypes.h, you should try to follow a more standardized path and use stdint.h.\nBut what if the target compiler does not provide stdint.h? You can write this header file. The\nstdint.h you write for target builds uses the acmetypes.h for target compiles like this:\nClick here to view code image\n#ifndef _STDINT_H_\n#define _STDINT_H_\n \n#include <acmetypes.h>\n \ntypedef Uint_32 uint32_t;\ntypedef Uint_16 uint16_t;\ntypedef Uint_8  uint8_t;\n \ntypedef Int_32  int32_t;\ntypedef Int_16  int16_t;\ntypedef Int_8   int8_t;\n \n#endif\nHaving your embedded software and firmware use stdint.h helps keep your code clean and\nportable. Certainly, all of the software should be processor independent, but not all of the firmware\ncan be. This next code snippet takes advantage of special extensions to C that gives your code access\nto the peripherals in the micro-controller. It\u2019s likely your product uses this micro-controller so that\nyou can use its integrated peripherals. This function outputs a line that says \"hi\" to the serial output\nport. (This example is based on real code from the wild.)\nClick here to view code image\n", "page": 203, "type": "text", "section": "Page 203"}
{"text": "void say_hi()\n{\n  IE = 0b11000000;\n  SBUF0 = (0x68);\n  while(TI_0 == 0);\n  TI_0 = 0;\n  SBUF0 = (0x69);\n  while(TI_0 == 0);\n  TI_0 = 0;\n  SBUF0 = (0x0a);\n  while(TI_0 == 0);\n  TI_0 = 0;\n  SBUF0 = (0x0d);\n  while(TI_0 == 0);\n  TI_0 = 0;\n  IE = 0b11010000;\n}\nThere are lots of problems with this small function. One thing that might jump out at you is the\npresence of 0b11000000. This binary notation is cool; can C do that? Unfortunately, no. A few other\nproblems relate to this code directly using the custom C extensions:\nIE: Interrupt enable bits.\nSBUF0: Serial output buffer.\nTI_0: Serial transmit buffer empty interrupt. Reading a 1 indicates the buffer is empty.\nThe uppercase variables actually access micro-controller built-in peripherals. If you want to control\ninterrupts and output characters, you must use these peripherals. Yes, this is convenient\u2014but it\u2019s not\nC.\nA clean embedded architecture would use these device access registers directly in very few places\nand confine them totally to the firmware. Anything that knows about these registers becomes firmware\nand is consequently bound to the silicon. Tying code to the processor will hurt you when you want to\nget code working before you have stable hardware. It will also hurt you when you move your\nembedded application to a new processor.\nIf you use a micro-controller like this, your firmware could isolate these low-level functions with\nsome form of a processor abstraction layer (PAL). Firmware above the PAL could be tested off-\ntarget, making it a little less firm.\nThe Operating System Is a Detail\nA HAL is necessary, but is it sufficient? In bare-metal embedded systems, a HAL may be all you need\nto keep your code from getting too addicted to the operating environment. But what about embedded\nsystems that use a real-time operating system (RTOS) or some embedded version of Linux or\nWindows?\nTo give your embedded code a good chance at a long life, you have to treat the operating system as a\n", "page": 204, "type": "text", "section": "Page 204"}
{"text": "detail and protect against OS dependencies.\nThe software accesses the services of the operating environment through the OS. The OS is a layer\nseparating the software from firmware (Figure 29.5). Using an OS directly can cause problems. For\nexample, what if your RTOS supplier is bought by another company and the royalties go up, or the\nquality goes down? What if your needs change and your RTOS does not have the capabilities that you\nnow require? You\u2019ll have to change lots of code. These won\u2019t just be simple syntactical changes due\nto the new OS\u2019s API, but will likely have to adapt semantically to the new OS\u2019s different capabilities\nand primitives.\nFigure 29.5 Adding in an operating system\nA clean embedded architecture isolates software from the operating system, through an operating\nsystem abstraction layer (OSAL) (Figure 29.6). In some cases, implementing this layer might be as\nsimple as changing the name of a function. In other cases, it might involve wrapping several functions\ntogether.\nFigure 29.6 The operating system abstraction layer\n", "page": 205, "type": "text", "section": "Page 205"}
{"text": "If you have ever moved your software from one RTOS to another, you know it is painful. If your\nsoftware depended on an OSAL instead of the OS directly, you would largely be writing a new OSAL\nthat is compatible with the old OSAL. Which would you rather do: modify a bunch of complex\nexisting code, or write new code to a defined interface and behavior? This is not a trick question. I\nchoose the latter.\nYou might start worrying about code bloat about now. Really, though, the layer becomes the place\nwhere much of the duplication around using an OS is isolated. This duplication does not have to\nimpose a big overhead. If you define an OSAL, you can also encourage your applications to have a\ncommon structure. You might provide message passing mechanisms, rather than having every thread\nhandcraft its concurrency model.\nThe OSAL can help provide test points so that the valuable application code in the software layer can\nbe tested off-target and off-OS. A clean embedded architecture\u2019s software is testable off the target\noperating system. A successful OSAL provides that seam or set of substitution points that facilitate\noff-target testing.\nPROGRAMMING TO INTERFACES AND SUBSTITUTABILITY\nIn addition to adding a HAL and potentially an OSAL inside each of the major layers (software, OS,\nfirmware, and hardware), you can\u2014and should\u2014apply the principles described throughout this book.\nThese principles encourage separation of concerns, programming to interfaces, and substitutability.\nThe idea of a layered architecture is built on the idea of programming to interfaces. When one module\ninteracts with another though an interface, you can substitute one service provider for another. Many\nreaders will have written their own small version of printf for deployment in the target. As long as\nthe interface to your printf is the same as the standard version of printf, you can override the\nservice one for the other.\nOne basic rule of thumb is to use header files as interface definitions. When you do so, however, you\nhave to be careful about what goes in the header file. Limit header file contents to function\ndeclarations as well as the constants and struct names that are needed by the function.\nDon\u2019t clutter the interface header files with data structures, constants, and typedefs that are needed by\nonly the implementation. It\u2019s not just a matter of clutter: That clutter will lead to unwanted\ndependencies. Limit the visibility of the implementation details. Expect the implementation details to\nchange. The fewer places where code knows the details, the fewer places where code will have to be\ntracked down and modified.\nA clean embedded architecture is testable within the layers because modules interact through\ninterfaces. Each interface provides that seam or substitution point that facilitates off-target testing.\nDRY CONDITIONAL COMPILATION DIRECTIVES\nOne use of substitutability that is often overlooked relates to how embedded C and C++ programs\nhandle different targets or operating systems. There is a tendency to use conditional compilation to\n", "page": 206, "type": "text", "section": "Page 206"}
{"text": "turn on and off segments of code. I recall one especially problematic case where the statement\n#ifdef BOARD_V2 was mentioned several thousand times in a telecom application.\nThis repetition of code violates the Don\u2019t Repeat Yourself (DRY) principle.5 If I see #ifdef\nBOARD_V2 once, it\u2019s not really a problem. Six thousand times is an extreme problem. Conditional\ncompilation identifying the target-hardware\u2019s type is often repeated in embedded systems. But what\nelse can we do?\nWhat if there is a hardware abstraction layer? The hardware type would become a detail hidden\nunder the HAL. If the HAL provides a set of interfaces, instead of using conditional compilation, we\ncould use the linker or some form of runtime binding to connect the software to the hardware.\nCONCLUSION\nPeople who are developing embedded software have a lot to learn from experiences outside of\nembedded software. If you are an embedded developer who has picked up this book, you will find a\nwealth of software development wisdom in the words and ideas.\nLetting all code become firmware is not good for your product\u2019s long-term health. Being able to test\nonly in the target hardware is not good for your product\u2019s long-term health. A clean embedded\narchitecture is good for your product\u2019s long-term health.\n1. https://insights.sei.cmu.edu/sei_blog/2011/08/the-growing-importance-of-sustaining-software-for-thedod.html\n2. The hardware device that controls the serial port.\n3. Some silicon providers add keywords to the C language to make accessing the registers and IO ports simple from C. Unfortunately,\nonce that is done, the code is no longer C.\n4. This statement intentionally uses HTML.\n5. Hunt and Thomas, The Pragmatic Programmer.\n", "page": 207, "type": "text", "section": "Page 207"}
{"text": "VI\nDETAILS\n", "page": 208, "type": "text", "section": "Page 208"}
{"text": "30\nTHE DATABASE IS A DETAIL\nFrom an architectural point of view, the database is a non-entity\u2014it is a detail that does not rise to\nthe level of an architectural element. Its relationship to the architecture of a software system is rather\nlike the relationship of a doorknob to the architecture of your home.\nI realize that these are fighting words. Believe me, I\u2019ve had the fight. So let me be clear: I am not\ntalking about the data model. The structure you give to the data within your application is highly\nsignificant to the architecture of your system. But the database is not the data model. The database is\npiece of software. The database is a utility that provides access to the data. From the architecture\u2019s\npoint of view, that utility is irrelevant because it\u2019s a low-level detail\u2014a mechanism. And a good\narchitect does not allow low-level mechanisms to pollute the system architecture.\nRELATIONAL DATABASES\nEdgar Codd defined the principles of relational databases in 1970. By the mid-1980s, the relational\nmodel had grown to become the dominant form of data storage. There was a good reason for this\npopularity: The relational model is elegant, disciplined, and robust. It is an excellent data storage and\naccess technology.\nBut no matter how brilliant, useful, and mathematically sound a technology it is, it is still just a\ntechnology. And that means it\u2019s a detail.\n", "page": 209, "type": "text", "section": "Page 209"}
{"text": "While relational tables may be convenient for certain forms of data access, there is nothing\narchitecturally significant about arranging data into rows within tables. The use cases of your\napplication should neither know nor care about such matters. Indeed, knowledge of the tabular\nstructure of the data should be restricted to the lowest-level utility functions in the outer circles of the\narchitecture.\nMany data access frameworks allow database rows and tables to be passed around the system as\nobjects. Allowing this is an architectural error. It couples the use cases, business rules, and in some\ncases even the UI to the relational structure of the data.\nWHY ARE DATABASE SYSTEMS SO PREV\nALENT?\nWhy are software systems and software enterprises dominated by database systems? What accounts\nfor the preeminence of Oracle, MySQL, and SQL Server? In a word: disks.\nThe rotating magnetic disk was the mainstay of data storage for five decades. Several generations of\nprogrammers have known no other form of data storage. Disk technology has grown from huge stacks\nof massive platters 48 inches in diameter that weighed thousands of pounds and held 20 megabytes, to\nsingle thin circles, 3 inches in diameter, that weigh just a few grams and hold a terabyte or more. It\u2019s\nbeen a wild ride. And throughout that ride programmers have been plagued by one fatal trait of disk\ntechnology: Disks are slow.\nOn a disk, data is stored within circular tracks. Those tracks are divided into sectors that hold a\nconvenient number of bytes, often 4K. Each platter may have hundreds of tracks, and there can be a\ndozen or so platters. If you want to read a particular byte off the disk, you have to move the head to\nthe proper track, wait for the disk to rotate to the proper sector, read all 4K of that sector into RAM,\nand then index into that RAM buffer to get the byte you want. And all that takes time\u2014milliseconds of\ntimes.\nMilliseconds might not seem like a lot, but a millisecond is a million times longer than the cycle time\nof most processors. If that data was not on a disk, it could be accessed in nanoseconds, instead of\nmilliseconds.\nTo mitigate the time delay imposed by disks, you need indexes, caches, and optimized query schemes;\nand you need some kind of regular means of representing the data so that these indexes, caches, and\nquery schemes know what they are working with. In short, you need a data access and management\nsystem. Over the years these systems have split into two distinct kinds: file systems and relational\ndatabase management systems (RDBMS).\nFile systems are document based. They provide a natural and convenient way to store whole\ndocuments. They work well when you need to save and retrieve a set of documents by name, but they\ndon\u2019t offer a lot of help when you\u2019re searching the content of those documents. It\u2019s easy to find a file\nnamed login.c, but it\u2019s hard, and slow, to find every .c file that has a variable named x in it.\nDatabase systems are content based. They provide a natural and convenient way to find records\n", "page": 210, "type": "text", "section": "Page 210"}
{"text": "based on their content. They are very good at associating multiple records based on some bit of\ncontent that they all share. Unfortunately, they are rather poor at storing and retrieving opaque\ndocuments.\nBoth of these systems organize the data on disk so that it can be stored and retrieved in as efficient a\nway as possible, given their particular access needs. Each has their own scheme for indexing and\narranging the data. In addition, each eventually brings the relevant data into RAM, where it can be\nquickly manipulated.\nWHAT IF THERE WERE NO DISK?\nAs prevalent as disks once were, they are now a dying breed. Soon they will have gone the way of\ntape drives, floppy drives, and CDs. They are being replaced by RAM.\nAsk yourself this question: When all the disks are gone, and all your data is stored in RAM, how will\nyou organize that data? Will you organize it into tables and access it with SQL? Will you organize it\ninto files and access it through a directory?\nOf course not. You\u2019ll organize it into linked lists, trees, hash tables, stacks, queues, or any of the other\nmyriad data structures, and you\u2019ll access it using pointers or references\u2014because that\u2019s what\nprogrammers do.\nIn fact, if you think carefully about this issue, you\u2019ll realize that this is what you already do. Even\nthough the data is kept in a database or a file system, you read it into RAM and then you reorganize it,\nfor your own convenience, into lists, sets, stacks, queues, trees, or whatever data structure meets your\nfancy. It is very unlikely that you leave the data in the form of files or tables.\nDETAILS\nThis reality is why I say that the database is a detail. It\u2019s just a mechanism we use to move the data\nback and forth between the surface of the disk and RAM. The database is really nothing more than a\nbig bucket of bits where we store our data on a long-term basis. But we seldom use the data in that\nform.\nThus, from an architectural viewpoint, we should not care about the form that the data takes while it\nis on the surface of a rotating magnetic disk. Indeed, we should not acknowledge that the disk exists at\nall.\nBUT WHAT ABOUT PERFORMANCE?\nIsn\u2019t performance an architectural concern? Of course it is\u2014but when it comes to data storage, it\u2019s a\nconcern that can be entirely encapsulated and separated from the business rules. Yes, we need to get\nthe data in and out of the data store quickly, but that\u2019s a low-level concern. We can address that\n", "page": 211, "type": "text", "section": "Page 211"}
{"text": "concern with low-level data access mechanisms. It has nothing whatsoever to do with the overall\narchitecture of our systems.\nANECDOTE\nIn the late 1980s, I led a team of software engineers at a startup company that was trying to build and\nmarket a network management system that measured the communications integrity of T1\ntelecommunication lines. The system retrieved data from the devices at the endpoints of those lines,\nand then ran a series of predictive algorithms to detect and report problems.\nWe were using UNIX platforms, and we stored our data in simple random access files. We had no\nneed of a relational database because our data had few content-based relationships. It was better kept\nin trees and linked lists in those random access files. In short, we kept the data in a form that was\nmost convenient to load into RAM where it could be manipulated.\nWe hired a marketing manager for this startup\u2014a nice and knowledgeable guy. But he immediately\ntold me that we had to have a relational database in the system. It wasn\u2019t an option and it wasn\u2019t an\nengineering issue\u2014it was a marketing issue.\nThis made no sense to me. Why in the world would I want to rearrange my linked lists and trees into\na bunch of rows and tables accessed through SQL? Why would I introduce all the overhead and\nexpense of a massive RDBMS when a simple random access file system was more than sufficient? So\nI fought him, tooth and nail.\nWe had a hardware engineer at this company who took up the RDBMS chant. He became convinced\nthat our software system needed an RDBMS for technical reasons. He held meetings behind my back\nwith the executives of the company, drawing stick figures on the whiteboard of a house balancing on a\npole, and he would ask the executives, \u201cWould you build a house on a pole?\u201d His implied message\nwas that an RDBMS that keeps its tables in random access files was somehow more reliable than the\nrandom access files that we were using.\nI fought him. I fought the marketing guy. I stuck to my engineering principles in the face of incredible\nignorance. I fought, and fought, and fought.\nIn the end, the hardware developer was promoted over my head to become the software manager. In\nthe end, they put a RDBMS into that poor system. And, in the end, they were absolutely right and I\nwas wrong.\nNot for engineering reasons, mind you: I was right about that. I was right to fight against putting an\nRDBMS into the architectural core of the system. The reason I was wrong was because our customers\nexpected us to have a relational database. They didn\u2019t know what they would do with it. They didn\u2019t\nhave any realistic way of using the relational data in our system. But it didn\u2019t matter: Our customers\nfully expected an RDBMS. It had become a check box item that all the software purchasers had on\ntheir list. There was no engineering rationale\u2014rationality had nothing to do with it. It was an\nirrational, external, and entirely baseless need, but it was no less real.\n", "page": 212, "type": "text", "section": "Page 212"}
{"text": "Where did that need come from? It originated from the highly effective marketing campaigns\nemployed by the database vendors at the time. They had managed to convince high-level executives\nthat their corporate \u201cdata assets\u201d needed protection, and that the database systems they offered were\nthe ideal means of providing that protection.\nWe see the same kind of marketing campaigns today. The word \u201centerprise\u201d and the notion of\n\u201cService-Oriented Architecture\u201d have much more to do with marketing than with reality.\nWhat should I have done in that long-ago scenario? I should have bolted an RDBMS on the side of\nthe system and provided some narrow and safe data access channel to it, while maintaining the\nrandom access files in the core of the system. What did I do? I quit and became a consultant.\nCONCLUSION\nThe organizational structure of data, the data model, is architecturally significant. The technologies\nand systems that move data on and off a rotating magnetic surface are not. Relational database\nsystems that force the data to be organized into tables and accessed with SQL have much more to do\nwith the latter than with the former. The data is significant. The database is a detail.\n", "page": 213, "type": "text", "section": "Page 213"}
{"text": "31\nTHE WEB IS A DETAIL\nWere you a developer in the 1990s? Do you remember how the web changed everything? Do you\nremember how we looked at our old client\u2013server architectures with disdain in the face of the shiny\nnew technology of The Web?\nActually the web didn\u2019t change anything. Or, at least, it shouldn\u2019t have. The web is just the latest in a\nseries of oscillations that our industry has gone through since the 1960s. These oscillations move\nback and forth between putting all the computer power in central servers and putting all computer\npower out at the terminals.\nWe\u2019ve seen several of these oscillations just in the last decade or so since the web became\nprominent. At first we thought all the computer power would be in server farms, and the browsers\nwould be stupid. Then we started putting applets in the browsers. But we didn\u2019t like that, so we\nmoved dynamic content back to the servers. But then we didn\u2019t like that, so we invented Web 2.0 and\nmoved lots of processing back into the browser with Ajax and JavaScript. We went so far as to create\nwhole huge applications written to execute in the browsers. And now we\u2019re all excited about pulling\nthat JavaScript back into the server with Node.\n(Sigh.)\nTHE ENDLESS PENDULUM\n", "page": 214, "type": "text", "section": "Page 214"}
{"text": "Of course, it would be incorrect to think that those oscillations started with the web. Before the web,\nthere was client\u2013server architecture. Before that, there were central minicomputers with arrays of\ndumb terminals. Before that, there were mainframes with smart green-screen terminals (that were\nvery much analogous to modern-day browsers). Before that, there were computer rooms and punched\ncards \u2026\nAnd so the story goes. We can\u2019t seem to figure out where we want the computer power. We go back\nand forth between centralizing it and distributing it. And, I imagine, those oscillations will continue\nfor some time to come.\nWhen you look at it in the overall scope of IT history, the web didn\u2019t change anything at all. The web\nwas simply one of many oscillations in a struggle that began before most of us were born and will\ncontinue well after most of us have retired.\nAs architects, though, we have to look at the long term. Those oscillations are just short-term issues\nthat we want to push away from the central core of our business rules.\nLet me tell you the story of company Q. Company Q built a very popular personal finance system. It\nwas a desktop app with a very useful GUI. I loved using it.\nThen came the web. In its next release, company Q changed the GUI to look, and behave, like a\nbrowser. I was thunderstruck! What marketing genius decided that personal finance software, running\non a desktop, should have the look and feel of a web browser?\nOf course, I hated the new interface. Apparently everyone else did, too\u2014because after a few\nreleases, company Q gradually removed the browser-like feel and turned its personal finance system\nback into a regular desktop GUI.\nNow imagine you were a software architect at Q. Imagine that some marketing genius convinces\nupper management that the whole UI has to change to look more like the web. What do you do? Or,\nrather, what should you have done before this point to protect your application from that marketing\ngenius?\nYou should have decoupled your business rules from your UI. I don\u2019t know whether the Q architects\nhad done that. One day I\u2019d love to hear their story. Had I been there at the time, I certainly would have\nlobbied very hard to isolate the business rules from the GUI, because you never know what the\nmarketing geniuses will do next.\nNow consider company A, which makes a lovely smartphone. Recently it released an upgraded\nversion of its \u201coperating system\u201d (it\u2019s so strange that we can talk about the operating system inside a\nphone). Among other things, that \u201coperating system\u201d upgrade completely changed the look and feel of\nall the applications. Why? Some marketing genius said so, I suppose.\nI\u2019m not an expert on the software within that device, so I don\u2019t know if that change caused any\nsignificant difficulties for the programmers of the apps that run in company A\u2019s phone. I do hope the\narchitects at A, and the architects of the apps, keep their UI and business rules isolated from each\n", "page": 215, "type": "text", "section": "Page 215"}
{"text": "other, because there are always marketing geniuses out there just waiting to pounce on the next little\nbit of coupling you create.\nTHE UPSHOT\nThe upshot is simply this: The GUI is a detail. The web is a GUI. So the web is a detail. And, as an\narchitect, you want to put details like that behind boundaries that keep them separate from your core\nbusiness logic.\nThink about it this way: The WEB is an IO device. In the 1960s, we learned the value of writing\napplications that were device independent. The motivation for that independence has not changed.\nThe web is not an exception to that rule.\nOr is it? The argument can be made that a GUI, like the web, is so unique and rich that it is absurd to\npursue a device-independent architecture. When you think about the intricacies of JavaScript\nvalidation or drag-and-drop AJAX calls, or any of the plethora of other widgets and gadgets you can\nput on a web page, it\u2019s easy to argue that device independence is impractical.\nTo some extent, this is true. The interaction between the application and the GUI is \u201cchatty\u201d in ways\nthat are quite specific to the kind of GUI you have. The dance between a browser and a web\napplication is different from the dance between a desktop GUI and its application. Trying to abstract\nout that dance, the way devices are abstracted out of UNIX, seems unlikely to be possible.\nBut another boundary between the UI and the application can be abstracted. The business logic can\nbe thought of as a suite of use cases, each of which performs some function on behalf of a user. Each\nuse case can be described based on the input data, the processing preformed, and the output data.\nAt some point in the dance between the UI and the application, the input data can be said to be\ncomplete, allowing the use case to be executed. Upon completion, the resultant data can be fed back\ninto the dance between the UI and the application.\nThe complete input data and the resultant output data can be placed into data structures and used as\nthe input values and output values for a process that executes the use case. With this approach, we can\nconsider each use case to be operating the IO device of the UI in a device-independent manner.\nCONCLUSION\nThis kind of abstraction is not easy, and it will likely take several iterations to get just right. But it is\npossible. And since the world is full of marketing geniuses, it\u2019s not hard to make the case that it\u2019s\noften very necessary.\n", "page": 216, "type": "text", "section": "Page 216"}
{"text": "32\nFRAMEWORKS ARE DETAILS\nFrameworks have become quite popular. Generally speaking, this is a good thing. There are many\nframeworks out there that are free, powerful, and useful.\nHowever, frameworks are not architectures\u2014though some try to be.\nFRAMEWORK AUTHORS\nMost framework authors offer their work for free because they want to be helpful to the community.\nThey want to give back. This is laudable. However, regardless of their high-minded motives, those\nauthors do not have your best interests at heart. They can\u2019t, because they don\u2019t know you, and they\ndon\u2019t know your problems.\nFramework authors know their own problems, and the problems of their coworkers and friends. And\nthey write their frameworks to solve those problems\u2014not yours.\nOf course, your problems will likely overlap with those other problems quite a bit. If this were not\nthe case, frameworks would not be so popular. To the extent that such overlap exists, frameworks can\nbe very useful indeed.\n", "page": 217, "type": "text", "section": "Page 217"}
{"text": "ASYMMETRIC MARRIAGE\nThe relationship between you and the framework author is extraordinarily asymmetric. You must\nmake a huge commitment to the framework, but the framework author makes no commitment to you\nwhatsoever.\nThink about this point carefully. When you use a framework, you read through the documentation that\nthe author of that framework provides. In that documentation, the author, and other users of that\nframework, advise you on how to integrate your software with the framework. Typically, this means\nwrapping your architecture around that framework. The author recommends that you derive from the\nframework\u2019s base classes, and import the framework\u2019s facilities into your business objects. The\nauthor urges you to couple your application to the framework as tightly as possible.\nFor the framework author, coupling to his or her own framework is not a risk. The author wants to\ncouple to that framework, because the author has absolute control over that framework.\nWhat\u2019s more, the author wants you to couple to the framework, because once coupled in this way, it is\nvery hard to break away. Nothing feels more validating to a framework author than a bunch of users\nwilling to inextricably derive from the author\u2019s base classes.\nIn effect, the author is asking you to marry the framework\u2014to make a huge, long-term commitment to\nthat framework. And yet, under no circumstances will the author make a corresponding commitment to\nyou. It\u2019s a one-directional marriage. You take on all the risk and burden; the framework author takes\non nothing at all.\nTHE RISKS\nWhat are the risks? Here are just a few for you to consider.\n\u2022 The architecture of the framework is often not very clean. Frameworks tend to violate he\nDependency Rule. They ask you to inherit their code into your business objects\u2014your Entities!\nThey want their framework coupled into that innermost circle. Once in, that framework isn\u2019t coming\nback out. The wedding ring is on your finger; and it\u2019s going to stay there.\n\u2022 The framework may help you with some early features of your application. However, as your\nproduct matures, it may outgrow the facilities of the framework. If you\u2019ve put on that wedding ring,\nyou\u2019ll find the framework fighting you more and more as time passes.\n\u2022 The framework may evolve in a direction that you don\u2019t find helpful. You may be stuck upgrading to\nnew versions that don\u2019t help you. You may even find old features, which you made use of,\ndisappearing or changing in ways that are difficult for you to keep up with.\n\u2022 A new and better framework may come along that you wish you could switch to.\nTHE SOLUTION\n", "page": 218, "type": "text", "section": "Page 218"}
{"text": "What is the solution?\nDon\u2019t marry the framework!\nOh, you can use the framework\u2014just don\u2019t couple to it. Keep it at arm\u2019s length. Treat the framework\nas a detail that belongs in one of the outer circles of the architecture. Don\u2019t let it into the inner circles.\nIf the framework wants you to derive your business objects from its base classes, say no! Derive\nproxies instead, and keep those proxies in components that are plugins to your business rules.\nDon\u2019t let frameworks into your core code. Instead, integrate them into components that plug in to your\ncore code, following the Dependency Rule.\nFor example, maybe you like Spring. Spring is a good dependency injection framework. Maybe you\nuse Spring to auto-wire your dependencies. That\u2019s fine, but you should not sprinkle @autowired\nannotations all throughout your business objects. Your business objects should not know about Spring.\nInstead, you can use Spring to inject dependencies into your Main component. It\u2019s OK for Main to\nknow about Spring since Main is the dirtiest, lowest-level component in the architecture.\nI NOW PRONOUNCE YOU \u2026\nThere are some frameworks that you simply must marry. If you are using C++, for example, you will\nlikely have to marry STL\u2014it\u2019s hard to avoid. If you are using Java, you will almost certainly have to\nmarry the standard library.\nThat\u2019s normal\u2014but it should still be a decision. You must understand that when you marry a\nframework to your application, you will be stuck with that framework for the rest of the life cycle of\nthat application. For better or for worse, in sickness and in health, for richer, for poorer, forsaking all\nothers, you will be using that framework. This is not a commitment to be entered into lightly.\nCONCLUSION\nWhen faced with a framework, try not to marry it right away. See if there aren\u2019t ways to date it for a\nwhile before you take the plunge. Keep the framework behind an architectural boundary if at all\npossible, for as long as possible. Perhaps you can find a way to get the milk without buying the cow.\n", "page": 219, "type": "text", "section": "Page 219"}
{"text": "33\nCASE STUDY: VIDEO SALES\nNow it\u2019s time to put these rules and thoughts about architecture together into a case study. This case\nstudy will be short and simple, yet will depict both the process a good architect uses and the\ndecisions that such an architect makes.\nTHE PRODUCT\nFor this case study, I\u2019ve chosen a product with which I am rather intimately familiar: the software for\na website that sells videos. Of course, it is reminiscent of cleancoders.com, the site where I sell my\nsoftware tutorial videos.\nThe basic idea is trivial. We have a batch of videos we want to sell. We sell them, on the web, to\nboth individuals and businesses. Individuals can pay one price to stream the videos, and another,\nhigher price to download those videos and own them permanently. Business licenses are streaming\nonly, and are purchased in batches that allow quantity discounts.\nIndividuals typically act as both the viewers and the purchasers. Businesses, in contrast, often have\npeople who buy the videos that other people will watch.\nVideo authors need to supply their video files, written descriptions, and ancillary files with exams,\nproblems, solutions, source code, and other materials.\n", "page": 220, "type": "text", "section": "Page 220"}
{"text": "Administrators need to add new video series, add and delete videos to and from the series, and\nestablish prices for various licenses.\nOur first step in determining the initial architecture of the system is to identify the actors and use\ncases.\nUSE CASE ANALYSIS\nFigure 33.1 shows a typical use-case analysis.\nFigure 33.1 A typical use-case analysis\nThe four main actors are evident. According to the Single Responsibility Principle, these four actors\nwill be the four primary sources of change for the system. Every time some new feature is added, or\nsome existing feature is changed, that step will be taken to serve one of these actors. Therefore we\n", "page": 221, "type": "text", "section": "Page 221"}
{"text": "want to partition the system such that a change to one actor does not affect any of the other actors.\nThe use cases shown in Figure 33.1 are not a complete list. For example, you won\u2019t find log-in or\nlog-out use cases. The reason for this omission is simply to manage the size of the problem in this\nbook. If I were to include all the different use cases, then this chapter would have to turn into a book\nin its own right.\nNote the dashed use cases in the center of Figure 33.1. They are abstract1 use cases. An abstract use\ncase is one that sets a general policy that another use case will flesh out. As you can see, the View\nCatalog as Viewer and View Catalog as Purchaser use cases both inherit from the View Catalog\nabstract use case.\nOn the one hand, it was not strictly necessary for me to create that abstraction. I could have left the\nabstract use case out of the diagram without compromising any of the features of the overall product.\nOn the other hand, these two use cases are so similar that I thought it wise to recognize the similarity\nand find a way to unify it early in the analysis.\nCOMPONENT ARCHITECTURE\nNow that we know the actors and use cases, we can create a preliminary component architecture\n(Figure 33.2).\nThe double lines in the drawing represent architectural boundaries as usual. You can see the typical\npartitioning of views, presenters, interactors, and controllers. You can also see that I\u2019ve broken each\nof those categories up by their corresponding actors.\nEach of the components in Figure 33.2 represents a potential .jar file or .dll file. Each of those\ncomponents will contain the views, presenters, interactors, and controllers that have been allocated to\nit.\nNote the special components for the Catalog View and the Catalog Presenter. This is how I dealt\nwith the abstract View Catalog use case. I assume that those views and presenters will be coded into\nabstract classes within those components, and that the inheriting components will contain view and\npresenter classes that will inherit from those abstract classes.\n", "page": 222, "type": "text", "section": "Page 222"}
{"text": "Figure 33.2 A preliminary component architecture\nWould I really break the system up into all these components, and deliver them as .jar or .dll files?\nYes and no. I would certainly break the compile and build environment up this way, so that I could\nbuild independent deliverables like that. I would also reserve the right to combine all those\ndeliverables into a smaller number of deliverables if necessary. For example, given the partitioning\nin Figure 33.2, it would be easy to combine them into five .jar files\u2014one for views, presenters,\ninteractors, controllers, and utilities, respectively. I could then independently deploy the components\nthat are most likely to change independently of each other.\nAnother possible grouping would be to put the views and presenters together into the same .jar file,\nand put the interactors, controllers, and utilities in their own .jar file. Still another, even more\nprimitive, grouping would be to create two .jar files, with views and presenters in one file, and\neverything else in the other.\nKeeping these options open will allow us to adapt the way we deploy the system based on how the\nsystem changes over time.\nDEPENDENCY MANAGEMENT\n", "page": 223, "type": "text", "section": "Page 223"}
{"text": "The flow of control in Figure 33.2 proceeds from right to left. Input occurs at the controllers, and that\ninput is processed into a result by the interactors. The presenters then format the results, and the\nviews display those presentations.\nNotice that the arrows do not all flow from the right to the left. In fact, most of them point from left to\nright. This is because the architecture is following the Dependency Rule. All dependencies cross the\nboundary lines in one direction, and they always point toward the components containing the higher-\nlevel policy.\nAlso notice that the using relationships (open arrows) point with the flow of control, and that the\ninheritance relationships (closed arrows) point against the flow of control. This depicts our use of\nthe Open\u2013Closed Principle to make sure that the dependencies flow in the right direction, and that\nchanges to low-level details do not ripple upward to affect high-level policies.\nCONCLUSION\nThe architecture diagram in Figure 33.2 includes two dimensions of separation. The first is the\nseparation of actors based on the Single Responsibility Principle; the second is the Dependency Rule.\nThe goal of both is to separate components that change for different reasons, and at different rates.\nThe different reasons correspond to the actors; the different rates correspond to the different levels of\npolicy.\nOnce you have structured the code this way, you can mix and match how you want to actually deploy\nthe system. You can group the components into deployable deliverables in any way that makes sense,\nand easily change that grouping when conditions change.\n1. This is my own notation for \u201cabstract\u201d use cases. It would have been more standard to use a UML stereotype such as <<abstract>>,\nbut I don\u2018t find adhering to such standards very useful nowadays.\n", "page": 224, "type": "text", "section": "Page 224"}
{"text": "34\nTHE MISSING CHAPTER\nBy Simon Brown\nAll of the advice you\u2019ve read so far will certainly help you design better software, composed of\nclasses and components with well-defined boundaries, clear responsibilities, and controlled\ndependencies. But it turns out that the devil is in the implementation details, and it\u2019s really easy to fall\nat the last hurdle if you don\u2019t give that some thought, too.\nLet\u2019s imagine that we\u2019re building an online book store, and one of the use cases we\u2019ve been asked to\nimplement is about customers being able to view the status of their orders. Although this is a Java\nexample, the principles apply equally to other programming languages. Let\u2019s put the Clean\nArchitecture to one side for a moment and look at a number of approaches to design and code\norganization.\nPACKAGE BY LAYER\nThe first, and perhaps simplest, design approach is the traditional horizontal layered architecture,\nwhere we separate our code based on what it does from a technical perspective. This is often called\n\u201cpackage by layer.\u201d Figure 34.1 shows what this might look like as a UML class diagram.\nIn this typical layered architecture, we have one layer for the web code, one layer for our \u201cbusiness\nlogic,\u201d and one layer for persistence. In other words, code is sliced horizontally into layers, which\n", "page": 225, "type": "text", "section": "Page 225"}
{"text": "are used as a way to group similar types of things. In a \u201cstrict layered architecture,\u201d layers should\ndepend only on the next adjacent lower layer. In Java, layers are typically implemented as packages.\nAs you can see in Figure 34.1, all of the dependencies between layers (packages) point downward. In\nthis example, we have the following Java types:\n\u2022 OrdersController: A web controller, something like a Spring MVC controller, that handles\nrequests from the web.\n\u2022 OrdersService: An interface that defines the \u201cbusiness logic\u201d related to orders.\n\u2022 OrdersServiceImpl: The implementation of the orders service.1\n\u2022 OrdersRepository: An interface that defines how we get access to persistent order information.\n\u2022 JdbcOrdersRepository: An implementation of the repository interface.\nFigure 34.1 Package by layer\nIn \u201cPresentation Domain Data Layering,\u201d2 Martin Fowler says that adopting such a layered\n", "page": 226, "type": "text", "section": "Page 226"}
{"text": "architecture is a good way to get started. He\u2019s not alone. Many of the books, tutorials, training\ncourses, and sample code you\u2019ll find will also point you down the path of creating a layered\narchitecture. It\u2019s a very quick way to get something up and running without a huge amount of\ncomplexity. The problem, as Martin points out, is that once your software grows in scale and\ncomplexity, you will quickly find that having three large buckets of code isn\u2019t sufficient, and you will\nneed to think about modularizing further.\nAnother problem is that, as Uncle Bob has already said, a layered architecture doesn\u2019t scream\nanything about the business domain. Put the code for two layered architectures, from two very\ndifferent business domains, side by side and they will likely look eerily similar: web, services, and\nrepositories. There\u2019s also another huge problem with layered architectures, but we\u2019ll get to that later.\nPACKAGE BY FEATURE\nAnother option for organizing your code is to adopt a \u201cpackage by feature\u201d style. This is a vertical\nslicing, based on related features, domain concepts, or aggregate roots (to use domain-driven design\nterminology). In the typical implementations that I\u2019ve seen, all of the types are placed into a single\nJava package, which is named to reflect the concept that is being grouped.\nWith this approach, as shown in Figure 34.2, we have the same interfaces and classes as before, but\nthey are all placed into a single Java package rather than being split among three packages. This is a\nvery simple refactoring from the \u201cpackage by layer\u201d style, but the top-level organization of the code\nnow screams something about the business domain. We can now see that this code base has something\nto do with orders rather than the web, services, and repositories. Another benefit is that it\u2019s\npotentially easier to find all of the code that you need to modify in the event that the \u201cview orders\u201d\nuse case changes. It\u2019s all sitting in a single Java package rather than being spread out.3\nI often see software development teams realize that they have problems with horizontal layering\n(\u201cpackage by layer\u201d) and switch to vertical layering (\u201cpackage by feature\u201d) instead. In my opinion,\nboth are suboptimal. If you\u2019ve read this book so far, you might be thinking that we can do much better\n\u2014and you\u2019re right.\n", "page": 227, "type": "text", "section": "Page 227"}
{"text": "Figure 34.2 Package by feature\nPORTS AND ADAPTERS\nAs Uncle Bob has said, approaches such as \u201cports and adapters,\u201d the \u201chexagonal architecture,\u201d\n\u201cboundaries, controllers, entities,\u201d and so on aim to create architectures where business/domain-\nfocused code is independent and separate from the technical implementation details such as\nframeworks and databases. To summarize, you often see such code bases being composed of an\n\u201cinside\u201d (domain) and an \u201coutside\u201d (infrastructure), as suggested in Figure 34.3.\n", "page": 228, "type": "text", "section": "Page 228"}
{"text": "Figure 34.3 A code base with an inside and an outside\nThe \u201cinside\u201d region contains all of the domain concepts, whereas the \u201coutside\u201d region contains the\ninteractions with the outside world (e.g., UIs, databases, third-party integrations). The major rule here\nis that the \u201coutside\u201d depends on the \u201cinside\u201d\u2014never the other way around. Figure 34.4 shows a\nversion of how the \u201cview orders\u201d use case might be implemented.\nThe com.mycompany.myapp.domain package here is the \u201cinside,\u201d and the other packages are the\n\u201coutside.\u201d Notice how the dependencies flow toward the \u201cinside.\u201d The keen-eyed reader will notice\nthat the OrdersRepository from previous diagrams has been renamed to simply be Orders. This\ncomes from the world of domain-driven design, where the advice is that the naming of everything on\nthe \u201cinside\u201d should be stated in terms of the \u201cubiquitous domain language.\u201d To put that another way,\nwe talk about \u201corders\u201d when we\u2019re having a discussion about the domain, not the \u201corders repository.\u201d\n", "page": 229, "type": "text", "section": "Page 229"}
{"text": "Figure 34.4 View orders use case\nIt\u2019s also worth pointing out that this is a simplified version of what the UML class diagram might look\nlike, because it\u2019s missing things like interactors and objects to marshal the data across the dependency\nboundaries.\nPACKAGE BY COMPONENT\nAlthough I agree wholeheartedly with the discussions about SOLID, REP, CCP, and CRP and most of\nthe advice in this book, I come to a slightly different conclusion about how to organize code. So I\u2019m\ngoing to present another option here, which I call \u201cpackage by component.\u201d To give you some\nbackground, I\u2019ve spent most of my career building enterprise software, primarily in Java, across a\nnumber of different business domains. Those software systems have varied immensely, too. A large\nnumber have been web-based, but others have been client\u2013server4, distributed, message-based, or\nsomething else. Although the technologies differed, the common theme was that the architecture for\n", "page": 230, "type": "text", "section": "Page 230"}
{"text": "most of these software systems was based on a traditional layered architecture.\nI\u2019ve already mentioned a couple of reasons why layered architectures should be considered bad, but\nthat\u2019s not the whole story. The purpose of a layered architecture is to separate code that has the same\nsort of function. Web stuff is separated from business logic, which is in turn separated from data\naccess. As we saw from the UML class diagram, from an implementation perspective, a layer\ntypically equates to a Java package. From a code accessibility perspective, for the\nOrdersController to be able to have a dependency on the OrdersService interface, the\nOrdersService interface needs to be marked as public, because they are in different packages.\nLikewise, the OrdersRepository interface needs to be marked as public so that it can be seen\noutside of the repository package, by the OrdersServiceImpl class.\nIn a strict layered architecture, the dependency arrows should always point downward, with layers\ndepending only on the next adjacent lower layer. This comes back to creating a nice, clean, acyclic\ndependency graph, which is achieved by introducing some rules about how elements in a code base\nshould depend on each other. The big problem here is that we can cheat by introducing some\nundesirable dependencies, yet still create a nice, acyclic dependency graph.\nSuppose that you hire someone new who joins your team, and you give the newcomer another\norders-related use case to implement. Since the person is new, he wants to make a big impression\nand get this use case implemented as quickly as possible. After sitting down with a cup of coffee for a\nfew minutes, the newcomer discovers an existing OrdersController class, so he decides that\u2019s\nwhere the code for the new orders-related web page should go. But it needs some orders data from\nthe database. The newcomer has an epiphany: \u201cOh, there\u2019s an OrdersRepository interface already\nbuilt, too. I can simply dependency-inject the implementation into my controller. Perfect!\u201d After a few\nmore minutes of hacking, the web page is working. But the resulting UML diagram looks like Figure\n34.5.\nThe dependency arrows still point downward, but the OrdersController is now additionally\nbypassing the OrdersService for some use cases. This organization is often called a relaxed layered\narchitecture, as layers are allowed to skip around their adjacent neighbor(s). In some situations, this\nis the intended outcome\u2014if you\u2019re trying to follow the CQRS5 pattern, for example. In many other\ncases, bypassing the business logic layer is undesirable, especially if that business logic is\nresponsible for ensuring authorized access to individual records, for example.\nWhile the new use case works, it\u2019s perhaps not implemented in the way that we were expecting. I see\nthis happen a lot with teams that I visit as a consultant, and it\u2019s usually revealed when teams start to\nvisualize what their code base really looks like, often for the first time.\n", "page": 231, "type": "text", "section": "Page 231"}
{"text": "Figure 34.5 Relaxed layered architecture\nWhat we need here is a guideline\u2014an architectural principle\u2014that says something like, \u201cWeb\ncontrollers should never access repositories directly.\u201d The question, of course, is enforcement. Many\nteams I\u2019ve met simply say, \u201cWe enforce this principle through good discipline and code reviews,\nbecause we trust our developers.\u201d This confidence is great to hear, but we all know what happens\nwhen budgets and deadlines start looming ever closer.\nA far smaller number of teams tell me that they use static analysis tools (e.g., NDepend, Structure101,\nCheckstyle) to check and automatically enforce architecture violations at build time. You may have\nseen such rules yourself; they usually manifest themselves as regular expressions or wildcard strings\nthat state \u201ctypes in package **/web should not access types in **/data\u201d; and they are executed after\nthe compilation step.\nThis approach is a little crude, but it can do the trick, reporting violations of the architecture\nprinciples that you\u2019ve defined as a team and (you hope) failing the build. The problem with both\n", "page": 232, "type": "text", "section": "Page 232"}
{"text": "approaches is that they are fallible, and the feedback loop is longer than it should be. If left\nunchecked, this practice can turn a code base into a \u201cbig ball of mud.\u201d6 I\u2019d personally like to use the\ncompiler to enforce my architecture if at all possible.\nThis brings us to the \u201cpackage by component\u201d option. It\u2019s a hybrid approach to everything we\u2019ve seen\nso far, with the goal of bundling all of the responsibilities related to a single coarse-grained\ncomponent into a single Java package. It\u2019s about taking a service-centric view of a software system,\nwhich is something we\u2019re seeing with micro-service architectures as well. In the same way that ports\nand adapters treat the web as just another delivery mechanism, \u201cpackage by component\u201d keeps the\nuser interface separate from these coarse-grained components. Figure 34.6 shows what the \u201cview\norders\u201d use case might look like.\nIn essence, this approach bundles up the \u201cbusiness logic\u201d and persistence code into a single thing,\nwhich I\u2019m calling a \u201ccomponent.\u201d Uncle Bob presented his definition of \u201ccomponent\u201d earlier in the\nbook, saying:\nComponents are the units of deployment. They are the smallest entities that can be deployed as part of a system. In\nJava, they are jar files.\n", "page": 233, "type": "text", "section": "Page 233"}
{"text": "Figure 34.6 View orders use case\nMy definition of a component is slightly different: \u201cA grouping of related functionality behind a nice\nclean interface, which resides inside an execution environment like an application.\u201d This definition\ncomes from my \u201cC4 software architecture model,\u201d7 which is a simple hierarchical way to think about\nthe static structures of a software system in terms of containers, components, and classes (or code). It\nsays that a software system is made up of one or more containers (e.g., web applications, mobile\napps, stand-alone applications, databases, file systems), each of which contains one or more\ncomponents, which in turn are implemented by one or more classes (or code). Whether each\ncomponent resides in a separate jar file is an orthogonal concern.\nA key benefit of the \u201cpackage by component\u201d approach is that if you\u2019re writing code that needs to do\nsomething with orders, there\u2019s just one place to go\u2014the OrdersComponent. Inside the component,\nthe separation of concerns is still maintained, so the business logic is separate from data persistence,\nbut that\u2019s a component implementation detail that consumers don\u2019t need to know about. This is akin to\nwhat you might end up with if you adopted a micro-services or Service-Oriented Architecture\u2014a\n", "page": 234, "type": "text", "section": "Page 234"}
{"text": "separate OrdersService that encapsulates everything related to handling orders. The key difference\nis the decoupling mode. You can think of well-defined components in a monolithic application as\nbeing a stepping stone to a micro-services architecture.\nTHE DEVIL IS IN THE IMPLEMENTATION\nDETAILS\nOn the face of it, the four approaches do all look like different ways to organize code and, therefore,\ncould be considered different architectural styles. This perception starts to unravel very quickly if\nyou get the implementation details wrong, though.\nSomething I see on a regular basis is an overly liberal use of the public access modifier in languages\nsuch as Java. It\u2019s almost as if we, as developers, instinctively use the public keyword without\nthinking. It\u2019s in our muscle memory. If you don\u2019t believe me, take a look at the code samples for\nbooks, tutorials, and open source frameworks on GitHub. This tendency is apparent, regardless of\nwhich architectural style a code base is aiming to adopt\u2014horizontal layers, vertical layers, ports and\nadapters, or something else. Marking all of your types as public means you\u2019re not taking advantage\nof the facilities that your programming language provides with regard to encapsulation. In some cases,\nthere\u2019s literally nothing preventing somebody from writing some code to instantiate a concrete\nimplementation class directly, violating the intended architecture style.\nORGANIZATION VERSUS ENCAPSULATION\nLooking at this issue another way, if you make all types in your Java application public, the\npackages are simply an organization mechanism (a grouping, like folders), rather than being used for\nencapsulation. Since public types can be used from anywhere in a code base, you can effectively\nignore the packages because they provide very little real value. The net result is that if you ignore the\npackages (because they don\u2019t provide any means of encapsulation and hiding), it doesn\u2019t really matter\nwhich architectural style you\u2019re aspiring to create. If we look back at the example UML diagrams, the\nJava packages become an irrelevant detail if all of the types are marked as public. In essence, all\nfour architectural approaches presented earlier in this chapter are exactly the same when we overuse\nthis designation (Figure 34.7).\nTake a close look at the arrows between each of the types in Figure 34.7: They\u2019re all identical\nregardless of which architectural approach you\u2019re trying to adopt. Conceptually the approaches are\nvery different, but syntactically they are identical. Furthermore, you could argue that when you make\nall of the types public, what you really have are just four ways to describe a traditional horizontally\nlayered architecture. This is a neat trick, and of course nobody would ever make all of their Java\ntypes public. Except when they do. And I\u2019ve seen it.\nThe access modifiers in Java are not perfect,8 but ignoring them is just asking for trouble. The way\nJava types are placed into packages can actually make a huge difference to how accessible (or\n", "page": 235, "type": "text", "section": "Page 235"}
{"text": "inaccessible) those types can be when Java\u2019s access modifiers are applied appropriately. If I bring\nthe packages back and mark (by graphically fading) those types where the access modifier can be\nmade more restrictive, the picture becomes pretty interesting (Figure 34.8).\nFigure 34.7 All four architectural approaches are the same\nMoving from left to right, in the \u201cpackage by layer\u201d approach, the OrdersService and\nOrdersRepository interfaces need to be public, because they have inbound dependencies from\nclasses outside of their defining package. In contrast, the implementation classes\n(OrdersServiceImpl and JdbcOrdersRepository) can be made more restrictive (package\nprotected). Nobody needs to know about them; they are an implementation detail.\nIn the \u201cpackage by feature\u201d approach, the OrdersController provides the sole entry point into the\npackage, so everything else can be made package protected. The big caveat here is that nothing else in\nthe code base, outside of this package, can access information related to orders unless they go through\nthe controller. This may or may not be desirable.\nIn the ports and adapters approach, the OrdersService and Orders interfaces have inbound\ndependencies from other packages, so they need to be made public. Again, the implementation\nclasses can be made package protected and dependency injected at runtime.\n", "page": 236, "type": "text", "section": "Page 236"}
{"text": "Figure 34.8 Grayed-out types are where the access modifier can be made more restrictive\nFinally, in the \u201cpackage by component\u201d approach, the OrdersComponent interface has an inbound\ndependency from the controller, but everything else can be made package protected. The fewer\npublic types you have, the smaller the number of potential dependencies. There\u2019s now no way9 that\ncode outside this package can use the OrdersRepository interface or implementation directly, so we\ncan rely on the compiler to enforce this architectural principle. You can do the same thing in .NET\nwith the internal keyword, although you would need to create a separate assembly for every\ncomponent.\nJust to be absolutely clear, what I\u2019ve described here relates to a monolithic application, where all of\nthe code resides in a single source code tree. If you are building such an application (and many\npeople are), I would certainly encourage you to lean on the compiler to enforce your architectural\nprinciples, rather than relying on self-discipline and post-compilation tooling.\nOTHER DECOUPLING MODES\nIn addition to the programming language you\u2019re using, there are often other ways that you can\n", "page": 237, "type": "text", "section": "Page 237"}
{"text": "decouple your source code dependencies. With Java, you have module frameworks like OSGi and the\nnew Java 9 module system. With module systems, when used properly, you can make a distinction\nbetween types that are public and types that are published. For example, you could create an Orders\nmodule where all of the types are marked as public, but publish only a small subset of those types\nfor external consumption. It\u2019s been a long time coming, but I\u2019m enthusiastic that the Java 9 module\nsystem will give us another tool to build better software, and spark people\u2019s interest in design\nthinking once again.\nAnother option is to decouple your dependencies at the source code level, by splitting code across\ndifferent source code trees. If we take the ports and adapters example, we could have three source\ncode trees:\n\u2022 The source code for the business and domain (i.e., everything that is independent of technology and\nframework choices): OrdersService, OrdersServiceImpl, and Orders\n\u2022 The source code for the web: OrdersController\n\u2022 The source code for the data persistence: JdbcOrdersRepository\nThe latter two source code trees have a compile-time dependency on the business and domain code,\nwhich itself doesn\u2019t know anything about the web or the data persistence code. From an\nimplementation perspective, you can do this by configuring separate modules or projects in your build\ntool (e.g., Maven, Gradle, MSBuild). Ideally you would repeat this pattern, having a separate source\ncode tree for each and every component in your application. This is very much an idealistic solution,\nthough, because there are real-world performance, complexity, and maintenance issues associated\nwith breaking up your source code in this way.\nA simpler approach that some people follow for their ports and adapters code is to have just two\nsource code trees:\n\u2022 Domain code (the \u201cinside\u201d)\n\u2022 Infrastructure code (the \u201coutside\u201d)\nThis maps on nicely to the diagram (Figure 34.9) that many people use to summarize the ports and\nadapters architecture, and there is a compile-time dependency from the infrastructure to the domain.\n", "page": 238, "type": "text", "section": "Page 238"}
{"text": "Figure 34.9 Domain and infrastructure code\nThis approach to organizing source code will also work, but be aware of the potential trade-off. It\u2019s\nwhat I call the \u201cP\u00e9riph\u00e9rique anti-pattern of ports and adapters.\u201d The city of Paris, France, has a ring\nroad called the Boulevard P\u00e9riph\u00e9rique, which allows you to circumnavigate Paris without entering\nthe complexities of the city. Having all of your infrastructure code in a single source code tree means\nthat it\u2019s potentially possible for infrastructure code in one area of your application (e.g., a web\ncontroller) to directly call code in another area of your application (e.g., a database repository),\nwithout navigating through the domain. This is especially true if you\u2019ve forgotten to apply appropriate\naccess modifiers to that code.\nCONCLUSION: THE MISSING ADVICE\nThe whole point of this chapter is to highlight that your best design intentions can be destroyed in a\nflash if you don\u2019t consider the intricacies of the implementation strategy. Think about how to map your\ndesired design on to code structures, how to organize that code, and which decoupling modes to apply\nduring runtime and compile-time. Leave options open where applicable, but be pragmatic, and take\ninto consideration the size of your team, their skill level, and the complexity of the solution in\nconjunction with your time and budgetary constraints. Also think about using your compiler to help\nyou enforce your chosen architectural style, and watch out for coupling in other areas, such as data\nmodels. The devil is in the implementation details.\n1. This is arguably a horrible way to name a class, but as we\u2019ll see later, perhaps it doesn\u2019t really matter.\n2. https://martinfowler.com/bliki/PresentationDomainDataLayering.html.\n", "page": 239, "type": "text", "section": "Page 239"}
{"text": "3. This benefit is much less relevant with the navigation facilities of modern IDEs, but it seems there has been a renaissance moving\nback to lightweight text editors, for reasons I am clearly too old to understand.\n4. My first job after graduating from university in 1996 was building client\u2013server desktop applications with a technology called\nPowerBuilder, a super-productive 4GL that exceled at building database-driven applications. A couple of years later, I was building\nclient\u2013server applications with Java, where we had to build our own database connectivity (this was pre-JDBC) and our own GUI\ntoolkits on top of AWT. That\u2019s \u201cprogress\u201c for you!\n5. In the Command Query Responsibility Segregation pattern, you have separate patterns for updating and reading data.\n6. http://www.laputan.org/mud/\n7. See https://www.structurizr.com/help/c4 for more information.\n8. In Java, for example, although we tend to think of packages as being hierarchical, it\u2019s not possible to create access restrictions based\non a package and subpackage relationship. Any hierarchy that you create is in the name of those packages, and the directory structure\non disk, only.\n9. Unless you cheat and use Java\u2019s reflection mechanism, but please don\u2019t do that!\n", "page": 240, "type": "text", "section": "Page 240"}
{"text": "VII\nAppendix\n", "page": 241, "type": "text", "section": "Page 241"}
{"text": "A\nARCHITECTURE ARCHAEOLOGY\nTo unearth the principles of good architecture, let\u2019s take a 45-year journey through some of the\nprojects I have worked on since 1970. Some of these projects are interesting from an architectural\npoint of view. Others are interesting because of the lessons learned and because of how they fed into\nsubsequent projects.\nThis appendix is somewhat autobiographical. I\u2019ve tried to keep the discussion relevant to the topic of\narchitecture; but, as in anything autobiographical, other factors sometimes intrude. ;-)\nUNION ACCOUNTING SYSTEM\nIn the late 1960s, a company by the name of ASC Tabulating signed a contract with Local 705 of the\nTeamsters Union to provide an accounting system. The computer ASC chose to implement this system\non was a GE Datanet 30, as shown in Figure A.1.\n", "page": 242, "type": "text", "section": "Page 242"}
{"text": "Figure A.1 GE Datanet 30\nCourtesy Ed Thelen, ed-thelen.org\nAs you can see from the picture, this was a huge1 machine. It filled a room, and the room needed strict\nenvironmental controls.\nThis computer was built in the days before integrated circuits. It was built out of discrete transistors.\nThere were even some vacuum tubes in it (albeit only in the sense amplifiers of the tape drives).\nBy today\u2019s standards the machine was huge, slow, small, and primitive. It had 16K \u00d7 18 bits of core,\nwith a cycle time of about 7 microseconds.2 It filled a big, environmentally controlled room. It had 7\ntrack magnetic tape drives and a disk drive with a capacity of 20 megabytes or so.\nThat disk was a monster. You can see it in the picture in Figure A.2\u2014but that doesn\u2019t quite give you\nthe scale of the beast. The top of that cabinet was over my head. The platters were 36 inches in\ndiameter, and 3/8 of an inch thick. One of the platters is pictured in Figure A.3.\nNow count the platters in that first picture. There are more than a dozen. Each one had its own\nindividual seek arm that was driven by pneumatic actuators. You could watch those seek heads move\nacross the platters. The seek time was probably about half a second to a second.\nWhen this beast was turned on, it sounded like a jet engine. The floor would rumble and shake until it\ngot up to speed.3\n", "page": 243, "type": "text", "section": "Page 243"}
{"text": "Figure A.2 The data storage unit with its platters\nCourtesy Ed Thelen, ed-thelen.org\nThe great claim to fame of the Datanet 30 was its capability to drive a large number of asynchronous\nterminals at relatively high speed. That\u2019s exactly what ASC needed.\nASC was based in Lake Bluff, Illinois, 30 miles north of Chicago. The Local 705 office was in\ndowntown Chicago. The union wanted a dozen or so of their data entry clerks to use CRT4 terminals\n(Figure A.4) to enter data into the system. They would print reports on ASR35 teletypes (Figure A.5).\nFigure A.3 One platter of that disk: 3/8 inch thick, 36 inches in diameter\nCourtesy, Ed Thelen, ed-thelen.org\nThe CRT terminals ran at 30 characters per second. This was a pretty good rate for the late 1960s\nbecause modems were relatively unsophisticated in those days.\nASC leased a dozen or so dedicated phone lines and twice that number of 300 baud modems from the\nphone company to connect the Datanet 30 to these terminals.\n", "page": 244, "type": "text", "section": "Page 244"}
{"text": "These computers did not come with operating systems. They didn\u2019t even come with file systems.\nWhat you got was an assembler.\nIf you needed to store data on the disk, you stored data on the disk. Not in a file. Not in a directory.\nYou figured out which track, platter, and sector to put the data into, and then you operated the disk to\nput the data there. Yes, that means we wrote our own disk driver.\nFigure A.4 Datapoint CRT terminal\nCourtesy of Bill Degnan, vintagecomputer.net\nThe Union Accounting system had three kinds of records: Agents, Employers, and Members. The\nsystem was a CRUD system for these records, but also included operations for posting dues,\ncomputing changes in the general ledger, and so on.\nThe original system was written in assembler by a consultant who somehow managed to cram the\nwhole thing into 16K.\nAs you might imagine, that big Datanet 30 was an expensive machine to operate and maintain. The\nsoftware consultant who kept the software running was expensive, too. What\u2019s more, minicomputers\nwere becoming popular, and were much cheaper.\n", "page": 245, "type": "text", "section": "Page 245"}
{"text": "Figure A.5 ASR35 teletype\nJoe Mabel, with permission\nIn 1971, when I was 18 years old, ASC hired me and two of my geeky friends to replace the whole\nunion accounting system with one that was based on a V\narian 620/f minicomputer (Figure A.6). The\ncomputer was cheap. We were cheap. So it seemed like a good deal for ASC.\nThe V\narian machine had a 16-bit bus and 32K * 16 core memory. It had a cycle time of about 1\nmicrosecond. It was much more powerful than the Datanet 30. It used IBM\u2019s wildly successful 2314\ndisk technology, allowing us to store 30 megabytes on platters that were only 14 inches in diameter\nand could not explode through concrete block walls!\nOf course, we still had no operating system. No file system. No high-level language. All we had was\nan assembler. But we made do.\nFigure A.6 Varian 620/f minicomputer\nThe Minicomputer Orphanage\n", "page": 246, "type": "text", "section": "Page 246"}
{"text": "Instead of trying to cram the whole system into 32K, we created an overlay system. Applications\nwould be loaded from disk into a block of memory dedicated to overlays. They would be executed in\nthat memory, and preemptively swapped back out onto disk, with their local RAM, to allow other\nprograms to execute.\nPrograms would get swapped into the overlay area, execute enough to fill the output buffers, and then\nget swapped out so that another program could be swapped in.\nOf course, when your UI runs at 30 characters per second, your programs spend a lot of time waiting.\nWe had plenty of time to swap the programs in and off the disk to keep all of the terminals running as\nfast as they could go. Nobody ever complained of response time issues.\nWe wrote a preemptive supervisor that managed the interrupts and IO. We wrote the applications; we\nwrote the disk drivers, the terminal drivers, the tape drivers, and everything else in that system. There\nwas not a single bit in that system that we did not write. Though it was a struggle involving far too\nmany 80-hour weeks, we got the beast up and running in a matter of 8 or 9 months.\nThe architecture of the system was simple (Figure A.7). When an application was started, it would\ngenerate output until its particular terminal buffer was full. Then the supervisor would swap the\napplication out, and swap a new application in. The supervisor would continue to dribble out the\ncontents of the terminal buffer at 30 cps until it was nearly empty. Then it would swap the application\nback in to fill the buffer again.\nFigure A.7 The system architecture\nThere are two boundaries in this system. The first is the character output boundary. The applications\nhad no idea that their output was going to a 30-cps terminal. Indeed, the character output was entirely\nabstract from the applications\u2019 point of view. The applications simply passed strings to the\nsupervisor, and the supervisor took care of loading the buffers, sending the characters to the\nterminals, and swapping the applications in and out of memory.\nThis boundary was dependency normal\u2014that is, dependencies pointed with the flow of control. The\napplications had compile-time dependencies on the supervisor, and the flow of control passed from\n", "page": 247, "type": "text", "section": "Page 247"}
{"text": "the applications to the supervisor. The boundary prevented the applications from knowing which kind\nof device the output was going to.\nThe second boundary was dependency inverted. The supervisor could start the applications, but had\nno compile-time dependencies upon them. The flow of control passed from the supervisor to the\napplications. The polymorphic interface that inverted the dependency was simply this: Every\napplication was started by jumping to the exact same memory address within the overlay area. The\nboundary prevented the supervisor from knowing anything about the applications other than the\nstarting point.\nLASER TRIM\nIn 1973, I joined a company in Chicago called Teradyne Applied Systems (TAS). This was a division\nof Teradyne Inc., which was headquartered in Boston. Our product was a system that used relatively\nhigh-powered lasers to trim electronic components to very fine tolerances.\nBack in those days, manufacturers would silk-screen electronic components onto ceramic substrates.\nThose substrates were on the order of 1 inch square. The components were typically resistors\u2014\ndevices that resist the flow of current.\nThe resistance of a resistor depends on a number of factors, including its composition and its\ngeometry. The wider the resistor, the less resistance it has.\nOur system would position the ceramic substrate in a harness that had probes that made contact with\nthe resistors. The system would measure the resistance of the resistors, and then use a laser to burn\noff parts of the resistor, making it thinner and thinner until it reached the desired resistance value\nwithin a tenth of a percent or so.\nWe sold these systems to manufacturers. We also used some in-house systems to trim relatively small\nbatches for small manufacturers.\nThe computer was an M365. This was in the days when many companies built their own computers:\nTeradyne built the M365 and supplied it to all its divisions. The M365 was an enhanced version of a\nPDP-8\u2014a popular minicomputer of the day.\nThe M365 controlled the positioning table, which moved the ceramic substrates under the probes. It\ncontrolled the measurement system and the laser. The laser was positioned using X-Y mirrors that\ncould rotate under program control. The computer could also control the power setting of the laser.\nThe development environment of the M365 was relatively primitive. There was no disk. Mass\nstorage was on tape cartridges that looked like old 8-track audio tape cassettes. The tapes and drives\nwere made by Tri-Data.\nLike the 8-track audio cassettes of the day, the tape was oriented in a loop. The drive moved the tape\nin only one direction\u2014there was no rewind! If you wanted to position the tape at the beginning, you\n", "page": 248, "type": "text", "section": "Page 248"}
{"text": "had to send it forward to its \u201cload point.\u201d\nThe tape moved at a speed of approximately 1 foot per second. Thus, if the tape loop was 25 feet\nlong, it could take as long as 25 seconds to send it to the load point. For this reason Tridata made\ncartridges in several lengths, ranging from 10 feet to 100 feet.\nThe M365 had a button on the front that would load memory with a little bootstrap program and\nexecute it. This program would read the first block of data from the tape, and execute that. Typically\nthis block held a loader that loaded the operating system that lived on the rest of the tape.\nThe operating system would prompt the user for the name of a program to run. Those programs were\nstored on the tape, just after the operating system. We would type in the name of the program\u2014for\nexample, the ED-402 Editor\u2014and the operating system would search the tape for that program, load\nit, and execute it.\nThe console was an ASCII CRT with green phosphors, 72 characters wide5 by 24 lines. The\ncharacters were all uppercase.\nTo edit a program, you would load the ED-402 Editor, and then insert the tape that held your source\ncode. You would read one tape block of that source code into memory, and it would be displayed on\nthe screen. The tape block might hold 50 lines of code. You would make your edits by moving the\ncursor around on the screen and typing in a manner similar to vi. When you were done, you would\nwrite that block onto a different tape, and read the next block from the source tape. You kept on doing\nthis until you were done.\nThere was no scrolling back to previous blocks. You edited your program in a straight line, from\nbeginning to end. Going back to the beginning forced you to finish copying the source code onto the\noutput tape and then start a new editing session on that tape. Perhaps not surprisingly, given these\nconstraints, we printed our programs out on paper, marked all the edits by hand in red ink, and then\nedited our programs block by block by consulting our markups on the listing.\nOnce the program was edited, we returned to the OS and invoked the assembler. The assembler read\nthe source code tape, and wrote a binary tape, while also producing a listing on our data products line\nprinter.\nThe tapes weren\u2019t 100% reliable, so we always wrote two tapes at the same time. That way, at least\none of them had a high probability of being free of errors.\nOur program was approximately 20,000 lines of code, and took nearly 30 minutes to compile. The\nodds that we would get a tape read error during that time were roughly 1 in 10. If the assembler got a\ntape error, it would ring the bell on the console and then start printing a stream of errors on the\nprinter. You could hear this maddening bell all across the lab. You could also hear the cursing of the\npoor programmer who just learned that the 30-minute compile needed to start over.\nThe architecture of the program was typical for those days. There was a Master Operating Program,\nappropriately called \u201cthe MOP.\u201d Its job was to manage basic IO functions and provide the rudiments\n", "page": 249, "type": "text", "section": "Page 249"}
{"text": "of a console \u201cshell.\u201d Many of the divisions of Teradyne shared the MOP source code, but each had\nforked it for its own uses. Consequently, we would send source code updates around to each other in\nthe form of marked-up listings that we would then integrate manually (and very carefully).\nA special-purpose utility layer controlled the measurement hardware, the positioning tables, and the\nlaser. The boundary between this layer and the MOP was muddled at best. While the utility layer\ncalled the MOP, the MOP had been specifically modified for that layer, and often called back into it.\nIndeed, we didn\u2019t really think of these two as separate layers. To us, it was just some code that we\nadded to the MOP in a highly coupled way.\nNext came the isolation layer. This layer provided a virtual machine interface for the application\nprograms, which were written in a completely different domain-specific data-driven language (DSL).\nThe language had operations for moving the laser, moving the table, making cuts, making\nmeasurements, and so on. Our customers would write their laser trimming application programs in\nthis language, and the isolation layer would execute them.\nThis approach was not intended to create a machine-independent laser trim language. Indeed, the\nlanguage had many idiosyncrasies that were deeply coupled to the layers below. Rather, this approach\ngave the application programmers a \u201csimpler\u201d language than M356 assembler in which to program\ntheir trim jobs.\nTrim jobs could be loaded from tape and executed by the system. Essentially, our system was an\noperating system for trim applications.\nThe system was written in M365 assembler and compiled in a single compilation unit that produced\nabsolute binary code.\nThe boundaries in this application were soft at best. Even the boundary between the system code and\nthe applications written in the DSL was not well enforced. There were couplings everywhere.\nBut that was typical of software in the early 1970s.\nALUMINUM DIE-CAST MONITORING\nIn the middle of the 1970s, while OPEC was placing an embargo on oil, and gasoline shortages were\ncausing angry drivers to get into fights at gas stations, I began working at Outboard Marine\nCorporation (OMC). This is the parent company of Johnson Motors and Lawnboy lawnmowers.\nOMC maintained a huge facility in Waukegan, Illinois, for creating die-cast aluminum parts for all of\nthe company\u2019s motors and products. Aluminum was melted down in huge furnaces, and then carried in\nlarge buckets to dozens upon dozens of individually operated aluminum die-cast machines. Each\nmachine had a human operator responsible for setting the molds, cycling the machine, and extracting\nthe newly cast parts. These operators were paid based on how many parts they produced.\nI was hired to work on a shop-floor automation project. OMC had purchased an IBM System/7\u2014\n", "page": 250, "type": "text", "section": "Page 250"}
{"text": "which was IBM\u2019s answer to the minicomputer. They tied this computer to all the die-cast machines on\nthe floor, so that we could count, and time, the cycles of each machine. Our role was to gather all that\ninformation and present it on 3270 green-screen displays.\nThe language was assembler. And, again, every bit of code that executed in this computer was code\nthat we wrote. There was no operating system, no subroutine libraries, and no framework. It was just\nraw code.\nIt was also interrupt-driven real-time code. Every time a die-cast machine cycled, we had to update a\nbatch of statistics, and send messages to a great IBM 370 in-the-sky, running a CICS-COBOL program\nthat presented those statistics on the green screens.\nI hated this job. Oh, boy, did I. Oh, the work was fun! But the culture \u2026 Suffice it to say that I was\nrequired to wear a tie.\nOh, I tried. I really did. But I was clearly unhappy working there, and my colleagues knew it. They\nknew it because I couldn\u2019t remember critical dates or manage to get up early enough to attend\nimportant meetings. This was the only programming job I was ever fired from\u2014and I deserved it.\nFrom an architectural point of view, there\u2019s not a lot to learn here except for one thing. The System/7\nhad a very interesting instruction called set program interrupt (SPI). This allowed you to trigger an\ninterrupt of the processor, allowing it to handle any other queued lower-priority interrupts.\nNowadays, in Java we call this Thread.yield().\n4-TEL\nIn October 1976, having been fired from OMC, I returned to a different division of Teradyne\u2014a\ndivision I would stay with for 12 years. The product I worked on was named 4-TEL. Its purpose was\nto test every telephone line in a telephone service area, every night, and produce a report of all lines\nrequiring repair. It also allowed telephone test personnel to test specific telephone lines in detail.\nThis system started its life with the same kind of architecture as the Laser Trim system. It was a\nmonolithic application written in assembly language without any significant boundaries. But at the\ntime I joined the company, that was about to change.\nThe system was used by testers located in a service center (SC). A service center covered many\ncentral offices (CO), each of which could handle as many as 10,000 phone lines. The dialing and\nmeasurement hardware had to be located inside the CO. So that\u2019s where the M365 computers were\nput. We called those computers the central office line testers (COL\nTs). Another M365 was placed at\nthe SC; it was called the service area computer (SAC). The SAC had several modems that it could\nuse to dial up the COL\nTs and communicate at 300 baud (30 cps).\nAt first, the COL\nT computers did everything, including all the console communication, menus, and\nreports. The SAC was just a simple multiplexor that took the output from the COL\nTs and put it on a\nscreen.\n", "page": 251, "type": "text", "section": "Page 251"}
{"text": "The problem with this setup was that 30 cps is really slow. The testers didn\u2019t like watching the\ncharacters trickle across the screen, especially since they were only interested in a few key bits of\ndata. Also, in those days, the core memory in the M365 was expensive, and the program was big.\nThe solution was to separate the part of the software that dialed and measured lines from the part that\nanalyzed the results and printed the reports. The latter would be moved into the SAC, and the former\nwould remain behind in the COL\nTs. This would allow the COL\nT to be a smaller machine, with much\nless memory, and would greatly speed up the response at the terminal, since the reports would be\ngenerated in the SAC.\nThe result was remarkably successful. Screen updates were very fast (once the appropriate COL\nT\nhad been dialed), and the memory footprint of the COL\nTs shrank a lot.\nThe boundary was very clean and highly decoupled. V\nery short packets of data were exchanged\nbetween the SAC and COL\nT. These packets were a very simple form of DSL, representing primitive\ncommands like \u201cDIAL XXXX\u201d or \u201cMEASURE.\u201d\nThe M365 was loaded from tape. Those tape drives were expensive and weren\u2019t very reliable\u2014\nespecially in the industrial environment of a telephone central office. Also, the M365 was an\nexpensive machine relative to the rest of the electronics within the COL\nT. So we embarked upon a\nproject to replace the M365 with a microcomputer based on an 8085 \u00b5processor.\nThe new computer was composed of a processor board that held the 8085, a RAM board that held\n32K of RAM, and three ROM boards that held 12K of read-only memory apiece. All these boards fit\ninto the same chassis as the measurement hardware, thereby eliminating the bulky extra chassis that\nhad housed the M365.\nThe ROM boards held 12 Intel 2708 EPROM (Erasable Programmable Read-Only Memory) chips.6\nFigure A.8 shows an example of such a chip. We loaded those chips with software by inserting them\ninto special devices called PROM burners that were driven by our development environment. The\nchips could be erased by exposing them to high-intensity ultraviolet light.7\nMy buddy CK and I translated the M365 assembly language program for the COL\nT into 8085\nassembly language. This translation was done by hand and took us about 6 months. The end result was\napproximately 30K of 8085 code.\nOur development environment had 64K of RAM and no ROM, so we could quickly download our\ncompiled binaries into RAM and test them.\nOnce we got the program working, we switched to using the EPROMs. We burned 30 chips and\ninserted them into just the right slots in the three ROM boards. Each chip was labeled so we could\ntell which chip went into which slot.\nThe 30K program was a single binary, 30K long. To burn the chips, we simply divided that binary\nimage into 30 different 1K segments, and burned each segment onto the appropriately labeled chip.\n", "page": 252, "type": "text", "section": "Page 252"}
{"text": "Figure A.8 EPROM chip\nThis worked very well, and we began to mass-produce the hardware and deploy the system into the\nfield.\nBut software is soft.8 Features needed to be added. Bugs needed to be fixed. And as the installed base\ngrew, the logistics of updating the software by burning 30 chips per installation, and having field\nservice people replace all 30 chips at each site became a nightmare.\nThere were all kinds of problems. Sometimes chips would be mislabeled, or the labels would fall\noff. Sometimes the field service engineer would mistakenly replace the wrong chip. Sometimes the\nfield service engineer would inadvertently break a pin off one of the new chips. Consequently, the\nfield engineers had to carry extras of all 30 chips with them.\nWhy did we have to change all 30 chips? Every time we added or removed code from our 30K\nexecutable, it changed the addresses in which each instruction was loaded. It also changed the\naddresses of the subroutines and functions that we called. So every chip was affected, no matter how\ntrivial the change.\nOne day, my boss came to me and asked me to solve that problem. He said we needed a way to make\na change to the firmware without replacing all 30 chips every time. We brainstormed this issue for a\nwhile, and then embarked upon the \u201cV\nectorization\u201d project. It took me three months.\nThe idea was beautifully simple. We divided the 30K program into 32 independently compilable\nsource files, each less than 1K. At the beginning of each source file, we told the compiler in which\naddress to load the resulting program (e.g., ORG C400 for the chip that was to be inserted into the C4\nposition).\nAlso at the beginning of each source file, we created a simple, fixed-size data structure that contained\nall the addresses of all the subroutines on that chip. This data structure was 40 bytes long, so it could\nhold no more than 20 addresses. This meant that no chip could have more than 20 subroutines.\nNext, we created a special area in RAM known as the vectors. It contained 32 tables of 40 bytes\u2014\n", "page": 253, "type": "text", "section": "Page 253"}
{"text": "exactly enough RAM to hold the pointers at the start of each chip.\nFinally, we changed every call to every subroutine on every chip into an indirect call through the\nappropriate RAM vector.\nWhen our processor booted, it would scan each chip and load the vector table at the start of each chip\ninto the RAM vectors. Then it would jump into the main program.\nThis worked very well. Now, when we fixed a bug, or added a feature, we could simply recompile\none or two chips, and send just those chips to the field service engineers.\nWe had made the chips independently deployable. We had invented polymorphic dispatch. We had\ninvented objects.\nThis was a plugin architecture, quite literally. We plugged those chips in. We eventually engineered it\nso that a feature could be installed into our products by plugging the chip with that feature into one of\nthe open chip sockets. The menu control would automatically appear, and the binding into the main\napplication would happen automatically.\nOf course, we didn\u2019t know about object-oriented principles at the time, and we knew nothing about\nseparating user interface from business rules. But the rudiments were there, and they were very\npowerful.\nOne unexpected side benefit of the approach was that we could patch the firmware over a dial-up\nconnection. If we found a bug in the firmware, we could dial up our devices and use the on-board\nmonitor program to alter the RAM vector for the faulty subroutine to point to a bit of empty RAM.\nThen we\u2019d enter the repaired subroutine into that RAM area, by typing it in machine code, in\nhexadecimal.\nThis was a great boon to our field service operation, and to our customers. If they had a problem, they\ndidn\u2019t need us to ship new chips and schedule an urgent field service call. The system could be\npatched, and a new chip could be installed at the next regularly scheduled maintenance visit.\nTHE SERVICE AREA COMPUTER\nThe 4-TEL service area computer (SAC) was based on an M365 minicomputer. This system\ncommunicated with all the COL\nTs out in the field, through either dedicated or dial-up modems. It\nwould command those COL\nTs to measure telephone lines, would receive back the raw results, and\nwould then perform a complex analysis of those results to identify and locate any faults.\nDISPATCH DETERMINATION\nOne of the economic foundations for this system was based on the correct allocation of repair\ncraftsmen. Repair craft were separated, by union rules, into three categories: central office, cable,\nand drop. CO craftsmen fixed problems inside the central office. Cable craftsmen fixed problems in\n", "page": 254, "type": "text", "section": "Page 254"}
{"text": "the cable plant that connected the CO to the customer. Drop craftsmen fixed problems inside the\ncustomer\u2019s premises, and in the lines connecting the external cable to that premises (the \u201cdrop\u201d).\nWhen a customer complained about a problem, our system could diagnose that problem and\ndetermine which kind of craftsman to dispatch. This saved the phone companies lots of money\nbecause incorrect dispatches meant delays for the customer and wasted trips for the craftsmen.\nThe code that made this dispatch determination was designed and written by someone who was very\nbright, but a terrible communicator. The process of writing the code has been described as \u201cThree\nweeks of staring at the ceiling and two days of code pouring out of every orifice of his body\u2014after\nwhich he quit.\u201d\nNobody understood this code. Every time we tried to add a feature or fix a defect, we broke it in\nsome way. And since it was upon this code that one of the primary economic benefits our system\nrested, every new defect was deeply embarrassing to the company.\nIn the end, our management simply told us to lock that code down and never modify it. That code\nbecame officially rigid.\nThis experience impressed upon me the value of good, clean code.\nARCHITECTURE\nThe system was written in 1976 in M365 assembler. It was a single, monolithic program of roughly\n60,000 lines. The operating system was a home-grown, nonpreemptive, task-switcher based on\npolling. We called it MPS for multiprocessing system. The M365 computer had no built-in stack, so\ntask-specific variables were kept in a special area of memory and swapped out at every context\nswitch. Shared variables were managed with locks and semaphores. Reentrancy issues and race\nconditions were constant problems.\nThere was no isolation of device control logic, or UI logic, from the business rules of the system. For\nexample, modem control code could be found smeared throughout the bulk of the business rules and\nUI code. There was no attempt to gather it into a module or abstract the interface. The modems were\ncontrolled, at the bit level, by code that was scattered everywhere around the system.\nThe same was true for the terminal UI. Messages and formatting control code were not isolated. They\nranged far and wide throughout the 60,000-line code base.\nThe modem modules we were using were designed to be mounted on PC boards. We bought those\nunits from a third party, and integrated them with other circuitry onto a board that fit into our custom\nbackplane. These units were expensive. So, after a few years, we decided to design our own\nmodems. We, in the software group, begged the hardware designer to use the same bit formats for\ncontrolling the new modem. We explained that the modem control code was smeared everywhere, and\nthat our system would have to deal with both kinds of modems in the future. So, we begged and\ncajoled, \u201cPlease make the new modem look just like the old modem from a software control point of\nview.\u201d\n", "page": 255, "type": "text", "section": "Page 255"}
{"text": "But when we got the new modem, the control structured was entirely different. It was not just a little\ndifferent. It was entirely, and completely, different.\nThanks, hardware engineer.\nWhat were we to do? We were not simply replacing all the old modems with new modems. Instead,\nwe were mixing old and new modems in our systems. The software needed to be able to handle both\nkinds of modems at the same time. Were we doomed to surround every place in the code that\nmanipulated the modems with flags and special cases? There were hundreds of such places!\nIn the end, we opted for an even worse solution.\nOne particular subroutine wrote data to the serial communication bus that was used to control all our\ndevices, including our modems. We modified that subroutine to recognize the bit patterns that were\nspecific to the old modem, and translate them into the bit patterns needed by the new modem.\nThis was not straightforward. Commands to the modem consisted of sequences of writes to different\nIO addresses on the serial bus. Our hack had to interpret these commands, in sequence, and translate\nthem into a different sequence using different IO addresses, timings, and bit positions.\nWe got it to work, but it was the worst hack imaginable. It was because of this fiasco that I learned\nthe value of isolating hardware from business rules, and of abstracting interfaces.\nTHE GRAND REDESIGN IN THE SKY\nBy the time the 1980s rolled around, the idea of producing your own minicomputer and your own\ncomputer architecture was beginning to fall out of fashion. There were many microcomputers on the\nmarket, and getting them to work was cheaper and more standard then continuing to rely on\nproprietary computer architectures from the late 1960s. That, plus the horrible architecture of the\nSAC software, induced our technical management to start a complete re-architecture of the SAC\nsystem.\nThe new system was to be written in C using a UNIX O/S on disk, running on an Intel 8086\nmicrocomputer. Our hardware guys started working on the new computer hardware, and a select\ngroup of software developers, \u201cThe Tiger Team,\u201d was commissioned with the rewrite.\nI won\u2019t bore you with the details of the initial fiasco. Suffice it to say that the first Tiger Team failed\nentirely after burning two or three man-years on a software project that never delivered anything.\nA year or two later, probably 1982, the process was started again. The goal was the total and\ncomplete redesign of the SAC in C and UNIX on our own, newly designed, impressively powerful\n80286 hardware. We called that computer \u201cDeep Thought.\u201d\nIt took years, then more years, and then even more years. I don\u2019t know when the first UNIX-based\nSAC was finally deployed; I believe I had left the company by then (1988). Indeed, I\u2019m not at all sure\nit ever was deployed.\n", "page": 256, "type": "text", "section": "Page 256"}
{"text": "Why the delay? In short, it is very difficult for a redesign team to catch up with a large staff of\nprogrammers who are actively maintaining the old system. Here\u2019s just one example of the difficulties\nthey encountered.\nEUROPE\nAt about the same time that the SAC was being redesigned in C, the company started to expand sales\ninto Europe. They could not wait for the redesigned software to be finished, so of course, they\ndeployed the old M365 systems into Europe.\nThe problem was that the phone systems in Europe were very different from the phone systems in the\nUnited States. The organization of the craft and of the bureaucracies were different as well. So one of\nour best programmers was sent to the United Kingdom to lead a team of U.K. developers to modify\nthe SAC software to deal with all these European issues.\nOf course, no serious attempt was made to integrate these changes into the U.S.-based software. This\nwas long before networks made it feasible to transmit large code bases across the ocean. These U.K.\ndevelopers simply forked the U.S.-based code and modified it as needed.\nThis, of course, caused difficulties. Bugs were found on both sides of the Atlantic that needed repair\non the other side. But the modules had changed significantly, so it was very difficult to determine\nwhether the fix made in the United States would work in the United Kingdom.\nAfter a few years of heartburn, and the installation of a high-throughput line connecting the U.S. and\nU.K. offices, a serious attempt was made to integrate these two forks back together again, making the\ndifferences a matter of configuration. This effort failed the first, second, and third times it was tried.\nThe two code bases, though remarkably similar, were still too different to reintegrate\u2014especially in\nthe rapidly changing market environment that existed at that time.\nMeanwhile, the \u201cTiger Team,\u201d trying to rewrite everything in C and UNIX, realized that it also had to\ndeal with this European/US dichotomy. And, of course, that did nothing to accelerate their progress.\nSAC CONCLUSION\nThere are many other stories I could tell you about this system, but it\u2019s just too depressing for me to\ncontinue. Suffice it to say that many of the hard lessons of my software life were learned while\nimmersed in the horrible assembler code of the SAC.\nC LANGUAGE\nThe 8085 computer hardware that we used in the 4-Tel Micro project gave us a relatively low-cost\ncomputing platform for many different projects that could be embedded into industrial environments.\nWe could load it up with 32K of RAM and another 32K of ROM, and we had an extremely flexible\nand powerful scheme for controlling peripherals. What we did not have was a flexible and\n", "page": 257, "type": "text", "section": "Page 257"}
{"text": "convenient language with which to program the machine. The 8085 assembler was simply not fun to\nwrite code in.\nOn top of that, the assembler we were using was written by our own programmers. It ran on our\nM365 computers, using the cartridge tape operating system described in the \u201cLaser Trim\u201d section.\nAs fate would have it, our lead hardware engineer convinced our CEO that we needed a real\ncomputer. He didn\u2019t actually know what he would do with it, but he had a lot of political clout. So we\npurchased a PDP-11/60.\nI, a lowly software developer at the time, was ecstatic. I knew precisely what I wanted to do with\nthat computer. I was determined that this was going to be my machine.\nWhen the manuals arrived, many months before the delivery of the machine, I took them home and\ndevoured them. By the time the computer was delivered, I knew how to operate both the hardware\nand the software at an intimate level\u2014at least, as intimate as home study can make it.\nI helped to write the purchase order. In particular, I specified the disk storage that the new computer\nwould have. I decided we should buy two disk drives that could take removable disk packs that held\n25 megabytes each.9\nFifty megabytes! The number seemed infinite! I remember walking through the halls of the office, late\nat night, cackling like the Wicked Witch of the West: \u201cFifty megabytes! Hahahahahahahahahah!\u201d\nI had the facilities manager build a little room that would house six VT100 terminals. I decorated it\nwith pictures from space. Our software developers would use this room to write and compile code.\nWhen the machine arrived, I spent several days setting it up, wiring all the terminals, and getting\neverything to work. It was a joy\u2014a labor of love.\nWe purchased standard assemblers for the 8085 from Boston Systems Office, and we translated the 4-\nTel Micro code into that syntax. We built a cross-compilation system that allowed us to download\ncompiled binaries from the PDP-11 to our 8085 development environments, and ROM burners. And\n\u2014Bob\u2019s your Uncle\u2014it all worked like a champ.\nC\nBut that left us with the problem of still using 8085 assembler. That was not a situation that I was\nhappy with. I had heard that there was this \u201cnew\u201d language that was heavily used at Bell Labs. They\ncalled it \u201cC.\u201d So I purchased a copy of The C Programming Language by Kernighan and Ritchie.\nLike the PDP-11 manuals a few months before, I inhaled this book.\nI was astounded by the simple elegance of this language. It sacrificed none of the power of assembly\nlanguage, and provided access to that power with a much more convenient syntax. I was sold.\nI purchased a C compiler from Whitesmiths, and got it running on the PDP-11. The output of the\n", "page": 258, "type": "text", "section": "Page 258"}
{"text": "compiler was assembler syntax that was compatible with the Boston Systems Office 8085 compiler.\nSo we had a pathway to go from C to the 8085 hardware! We were in business.\nNow the only problem was convincing a group of embedded assembly language programmers that\nthey should be using C. But that\u2019s a nightmare tale for another time \u2026\nBOSS\nOur 8085 platform had no operating system. My experience with the MPS system of the M365, and\nthe primitive interrupt mechanisms of the IBM System 7, convinced me that we needed a simple task\nswitcher for the 8085. So I conceived of BOSS: Basic Operating System and Scheduler.10\nThe vast majority of BOSS was written in C. It provided the ability to create concurrent tasks. Those\ntasks were not preemptive\u2014task switching did not take place based on interrupts. Instead, and just\nlike with the MPS system on the M365, tasks were switched based on a simple polling mechanism.\nThe polling happened whenever a task blocked for an event.\nThe BOSS call to block a task looked like this:\nblock(eventCheckFunction);\nThis call suspended the current task, placed the eventCheckFunction in the polling list, and\nassociated it with the newly blocked task. It then waited in the polling loop, calling each of the\nfunctions in the polling list until one of them returned true. The task associated with that function was\nthen allowed to run.\nIn other words, as I said before, it was a simple, nonpreemptive task switcher.\nThis software became the basis for a vast number of projects over the following years. But one of the\nfirst was the pCCU.\npCCU\nThe late 1970s and early 1980s were a tumultuous time for telephone companies. One of the sources\nof that tumult was the digital revolution.\nFor the preceding century, the connection between the central switching office and the customer\u2019s\ntelephone had been a pair of copper wires. These wires were bundled into cables that spread in a\nhuge network across the countryside. They were sometimes carried on poles, and sometimes buried\nunderground.\nCopper is a precious metal, and the phone company had tons (literally tons) of it covering the country.\nThe capital investment was enormous. Much of that capital could be reclaimed by transporting the\ntelephone conversation over digital connections. A single pair of copper wires could carry hundreds\n", "page": 259, "type": "text", "section": "Page 259"}
{"text": "of conversations in digital form.\nIn response, the phone companies embarked upon the process of replacing their old analog central\nswitching equipment with modern digital switches.\nOur 4-Tel product tested copper wires, not digital connections. There were still plenty of copper\nwires in a digital environment, but they were much shorter than before, and they were localized near\nthe customer\u2019s telephones. The signal would be carried digitally from the central office to a local\ndistribution point, where it would be converted back to an analog signal and distributed to the\ncustomer over standard copper wires. This meant that our measurement device needed to be located\nout where the copper wires began, but our dialing device needed to remain at the central office. The\nproblem was that all our COL\nTs embodied both dialing and measurement in the same device. (We\ncould have saved ourselves a fortune had we recognized that obvious architectural boundary a few\nyears earlier!)\nThus we conceived of a new product architecture: the CCU/CMU (the COL\nT control unit and the\nCOL\nT measurement unit). The CCU would be located at the central switching office, and would\nhandle the dialing of the phone lines to be tested. The CMU would be located at the local distribution\npoints, and would measure the copper wires that led to the customer\u2019s phone.\nThe problem was that for each CCU, there were many CMUs. The information about which CMU\nshould be used for each phone number was held by the digital switch itself. Thus the CCU had to\ninterrogate the digital switch to determine which CMU to communicate with and control.\nWe promised the phone companies that we would have this new architecture working in time for their\ntransition. We knew they were months, if not years away, so we did not feel rushed. We also knew\nthat it would take several man-years to develop this new CCU/CMU hardware and software.\nTHE SCHEDULE TRAP\nAs time went on, we found that there were always urgent matters that required us to postpone\ndevelopment of the CCU/CMU architecture. We felt safe about this decision because the phone\ncompanies were consistently delaying the deployment of digital switches. As we looked at their\nschedules, we felt confident that we had plenty of time, so we consistently delayed our development.\nThen came the day that my boss called me into his office and said: \u201cOne of our customers is\ndeploying a digital switch next month. We have to have a working CCU/CMU by then.\u201d\nI was aghast! How could we possibly do man-years of development in a month? But my boss had a\nplan \u2026\nWe did not, in fact, need a full CCU/CMU architecture. The phone company that was deploying the\ndigital switch was tiny. They had only one central office, and only two local distribution points. More\nimportantly, the \u201clocal\u201d distribution points were not particularly local. They actually had regular-old\nanalog switches in them that switched to several hundred customers. Better yet, those switches were\nof a kind that could be dialed by a normal COL\nT. Better even still, the customer\u2019s phone number\n", "page": 260, "type": "text", "section": "Page 260"}
{"text": "contained all the information necessary to decide which local distribution point to use. If the phone\nnumber had a 5, 6, or 7 in a certain position, it went to distribution point 1; otherwise, it went to\ndistribution point 2.\nSo, as my boss explained to me, we did not actually need a CCU/CMU. What we needed was a\nsimple computer at the central office connected by modem lines to two standard COL\nTs at the\ndistribution points. The SAC would communicate with our computer at the central office, and that\ncomputer would decode the phone number and then relay the dialing and measurement commands to\nthe COL\nT at the appropriate distribution point.\nThus was born the pCCU.\nThis was the first product written in C and using BOSS that was deployed to a customer. It took me\nabout a week to develop. There is no deep architectural significance to this tale, but it makes a nice\npreface to the next project.\nDLU/DRU\nIn the early 1980s, one of our customers was a telephone company in Texas. They had large\ngeographic areas to cover. In fact, the areas were so large that a single service area required several\ndifferent offices from which to dispatch craftsmen. Those offices had test craftspeople who needed\nterminals into our SAC.\nYou might think that this was a simple problem to solve\u2014but remember that this story takes place in\nthe early 1980s. Remote terminals were not very common. To make matters worse, the hardware of\nthe SAC presumed that all the terminals were local. Our terminals actually sat on a proprietary, high-\nspeed, serial bus.\nWe had remote terminal capability, but it was based on modems, and in the early 1980s modems were\ngenerally limited to 300 bits per second. Our customers were not happy with that slow speed.\nHigh-speed modems were available, but they were very expensive, and they needed to run on\n\u201cconditioned\u201d permanent connections. Dial-up quality was definitely not good enough.\nOur customers demanded a solution. Our response was DLU/DRU.\nDLU/DRU stood for \u201cDisplay Local Unit\u201d and \u201cDisplay Remote Unit.\u201d The DLU was a computer\nboard that plugged into the SAC computer chassis and pretended to be a terminal manager board.\nInstead of controlling the serial bus for local terminals, however, it took the character stream and\nmultiplexed it over a single 9600-bps conditioned modem link.\nThe DRU was a box placed at the customer\u2019s remote location. It connected to the other end of the\n9600-bps link, and had the hardware to control the terminals on our proprietary serial bus. It\ndemultiplexed the characters received from the 9600-bps link and sent them to the appropriate local\nterminals.\n", "page": 261, "type": "text", "section": "Page 261"}
{"text": "Strange, isn\u2019t it? We had to engineer a solution that nowadays is so ubiquitous we never even think\nabout it. But back then \u2026\nWe even had to invent our own communications protocol because, in those days, standard\ncommunications protocols were not open source shareware. Indeed, this was long before we had any\nkind of Internet connection.\nARCHITECTURE\nThe architecture of this system was very simple, but there are some interesting quirks I want to\nhighlight. First, both units used our 8085 technology, and both were written in C and used BOSS. But\nthat\u2019s where the similarity ended.\nThere were two of us on the project. I was the project lead, and Mike Carew was my close associate.\nI took on the design and coding of the DLU; Mike did the DRU.\nThe architecture of the DLU was based on a dataflow model. Each task did a small and focused job,\nand then passed its output to the next task in line, using a queue. Think of a pipes and filters model in\nUNIX. The architecture was intricate. One task might feed a queue that many others would service.\nOther tasks would feed a queue that just one task would service.\nThink of an assembly line. Each position on the assembly line has a single, simple, highly focused job\nto perform. Then the product moves to the next position in line. Sometimes the assembly line splits\ninto many lines. Sometimes those lines merge back into a single line. That was the DLU.\nMike\u2019s DRU used a remarkably different scheme. He created one task per terminal, and simply did\nthe entire job for that terminal in that task. No queues. No data flow. Just many identical large tasks,\neach managing its own terminal.\nThis is the opposite of an assembly line. In this case the analogy is many expert builders, each of\nwhom builds an entire product.\nAt the time I thought my architecture was superior. Mike, of course, thought his was better. We had\nmany entertaining discussions about this. In the end, of course, both worked quite well. And I was left\nwith the realization that software architectures can be wildly different, yet equally effective.\nVRS\nAs the 1980s progressed, newer and newer technologies appeared. One of those technologies was the\ncomputer control of voice.\nOne of the features of the 4-Tel system was the ability of the craftsman to locate a fault in a cable. The\nprocedure was as follows:\n\u2022 The tester, in the central office, would use our system to determine the approximate distance, in feet,\n", "page": 262, "type": "text", "section": "Page 262"}
{"text": "to the fault. This would be accurate to within 20% or so. The tester would dispatch a cable repair\ncraftsman to an appropriate access point near that position.\n\u2022 The cable repair craftsman, upon arrival, would call the tester and ask to begin the fault location\nprocess. The tester would invoke the fault location feature of the 4-Tel system. The system would\nbegin measuring the electronic characteristics of that faulty line, and would print messages on the\nscreen requesting that certain operations be performed, such as opening the cable or shorting the\ncable.\n\u2022 The tester would tell the craftsman which operations the system wanted, and the craftsman would\ntell the tester when the operation was complete. The tester would then tell the system that the\noperation was complete, and the system would continue with the test.\n\u2022 After two or three such interactions, the system would calculate a new distance to the fault. The\ncable craftsman would then drive to that location and begin the process again.\nImagine how much better that would be if the cable craftsmen, up on the pole or standing at a\npedestal, could operate the system themselves. And that is exactly what the new voice technologies\nallowed us to do. The cable craftsmen could call directly into our system, direct the system with\ntouch tones, and listen to the results being read back to them in a pleasant voice.\nTHE NAME\nThe company held a little contest to select a name for the new system. One of the most creative of the\nnames suggested was SAM CARP. This stood for \u201cStill Another Manifestation of Capitalist Avarice\nRepressing the Proletariat.\u201d Needless to say, that wasn\u2019t selected.\nAnother was the Teradyne Interactive Test System. That one was also not selected.\nStill another was Service Area Test Access Network. That, too, was not selected.\nThe winner, in the end, was VRS: V\noice Response System.\nARCHITECTURE\nI did not work on this system, but I heard about what happened. The story I am going to relate to you\nis second-hand, but for the most part, I believe it to be correct.\nThese were the heady days of microcomputers, UNIX operating systems, C, and SQL databases. We\nwere determined to use them all.\nFrom the many database vendors out there, we eventually chose UNIFY\n. UNIFY was a database\nsystem that worked with UNIX, which was perfect for us.\nUNIFY also supported a new technology called Embedded SQL. This technology allowed us to\nembed SQL commands, as strings, right into our C code. And so we did\u2014everywhere.\nI mean, it was just so cool that you could put your SQL right into your code, anywhere you wanted.\n", "page": 263, "type": "text", "section": "Page 263"}
{"text": "And where did we want to? Everywhere! And so there was SQL smeared throughout the body of that\ncode.\nOf course, in those days SQL was hardly a solid standard. There were lots of special vendor-specific\nquirks. So the special SQL and special UNIFY API calls were also smeared throughout the code.\nThis worked great! The system was a success. The craftsmen used it, and the telephone companies\nloved it. Life was all smiles.\nThen the UNIFY product we were using was cancelled.\nOh. Oh.\nSo we decided to switch to SyBase. Or was it Ingress? I don\u2019t remember. Suffice it to say, we had to\nsearch through all that C code, find all the embedded SQL and special API calls, and replace them\nwith corresponding gestures for the new vendor.\nAfter three months of effort or so, we gave up. We couldn\u2019t make it work. We were so coupled to\nUNIFY that there was no serious hope of restructuring the code at any practical expense.\nSo, we hired a third party to maintain UNIFY for us, based on a maintenance contract. And, of course,\nthe maintenance rates went up year after year after year.\nVRS CONCLUSION\nThis is one of the ways that I learned that databases are details that should be isolated from the\noverall business purpose of the system. This is also one of the reasons that I don\u2019t like strongly\ncoupling to third-party software systems.\nTHE ELECTRONIC RECEPTIONIST\nIn 1983, our company sat at the confluence of computer systems, telecommunications systems, and\nvoice systems. Our CEO thought this might be a fertile position from which to develop new products.\nTo address this goal, he commissioned a team of three (which included me) to conceive, design, and\nimplement a new product for the company.\nIt didn\u2019t take us long to come up with The Electronic Receptionist (ER).\nThe idea was simple. When you called a company, ER would answer and ask you who you wanted to\nspeak with. You would use touch tones to spell the name of that person, and ER would then connect\nyou. The users of ER could dial in and, by using simple touch-tone commands, tell it which phone\nnumber the desired person could be reached at, anywhere in the world. In fact, the system could list\nseveral alternate numbers.\nWhen you called ER and dialed RMART (my code), ER would call the first number on my list. If I\n", "page": 264, "type": "text", "section": "Page 264"}
{"text": "failed to answer and identify myself, it would call the next number, and the next. If I still wasn\u2019t\nreached, ER would record a message from the caller.\nER would then, periodically, try to find me to deliver that message, and any other message left for me\nby anyone else.\nThis was the first voice mail system ever, and we11 held the patent to it.\nWe built all the hardware for this system\u2014the computer board, the memory board, the voice/telecom\nboards, everything. The main computer board was Deep Thought, the Intel 80286 processor that I\nmentioned earlier.\nThe voice boards each supported one telephone line. They consisted of a telephone interface, a voice\nencoder/decoder, some memory, and an Intel 80186 microcomputer.\nThe software for the main computer board was written in C. The operating system was MP/M-86, an\nearly command-line\u2013driven, multiprocessing, disk operating system. MP/M was the poor man\u2019s\nUNIX.\nThe software for the voice boards was written in assembler, and had no operating system.\nCommunication between Deep Thought and the voice boards occurred through shared memory.\nThe architecture of this system would today be called service oriented. Each telephone line was\nmonitored by a listener process running under MP/M. When a call came in, an initial handler process\nwas started and the call was passed to it. As the call proceeded from state to state, the appropriate\nhandler process would be started and take control.\nMessages were passed between these services through disk files. The currently running service\nwould determine what the next service should be; would write the necessary state information into a\ndisk file; would issue the command line to start that service; and then would exit.\nThis was the first time I had built a system like this. Indeed, this was the first time I had been the\nprincipal architect of an entire product. Everything having to do with software was mine\u2014and it\nworked like a champ.\nI would not say that the architecture of this system was \u201cclean\u201d in the sense of this book; it was not a\n\u201cplugin\u201d architecture. However, it definitely showed signs of true boundaries. The services were\nindependently deployable, and lived within their own domain of responsibility. There were high-\nlevel processes and low-level processes, and many of the dependencies ran in the right direction.\nER DEMISE\nUnfortunately, the marketing of this product did not go very well. Teradyne was a company that sold\ntest equipment. We did not understand how to break into the office equipment market.\nAfter repeated attempts over two years, our CEO gave up and\u2014unfortunately\u2014dropped the patent\n", "page": 265, "type": "text", "section": "Page 265"}
{"text": "application. The patent was picked up by the company that filed three months after we filed; thus we\nsurrendered the entire voice mail and electronic call-forwarding market.\nOuch!\nOn the other hand, you can\u2019t blame me for those annoying machines that now plague our existence.\nCRAFT DISPATCH SYSTEM\nER had failed as a product, but we still had all this hardware and software that we could use to\nenhance our existing product lines. Moreover, our marketing success with VRS convinced us that we\nshould offer a voice response system for interacting with telephone craftsmen that did not depend on\nour test systems.\nThus was born CDS, the Craft Dispatch System. CDS was essentially ER, but specifically focused on\nthe very narrow domain of managing the deployment of telephone repairmen in the field.\nWhen a problem was discovered in a phone line, a trouble ticket was created in the service center.\nTrouble tickets were kept in an automated system. When a repairman in the field finished a job, he\nwould call the service center for the next assignment. The service center operator would pull up the\nnext trouble ticket and read it off to the repairman.\nWe set about to automate that process. Our goal was for the repairman in the field to call into CDS\nand ask for the next assignment. CDS would consult the trouble ticket system, and read off the results.\nCDS would keep track of which repairman was assigned to which trouble ticket, and would inform\nthe trouble ticket system of the status of the repair.\nThere were quite a few interesting features of this system having to do with interacting with the\ntrouble ticket system, the plant management system, and any automated testing systems.\nThe experience with the service-oriented architecture of ER made me want to try the same idea more\naggressively. The state machine for a trouble ticket was much more involved than the state machine\nfor handling a call with ER. I set about to create what would now be called a micro-service\narchitecture.\nEvery state transition of any call, no matter how insignificant, caused the system to start up a new\nservice. Indeed, the state machine was externalized into a text file that the system read. Each event\ncoming into the system from a phone line turned into a transition in that finite state machine. The\nexisting process would start a new process dictated by the state machine to handle that event; then the\nexisting process would either exit or wait on a queue.\nThis externalized state machine allowed us to change the flow of the application without changing any\ncode (the Open-Closed Principle). We could easily add a new service, independently of any of the\nothers, and wire it into the flow by modifying the text file that contained the state machine. We could\neven do this while the system was running. In other words we had hot-swapping and an effective\n", "page": 266, "type": "text", "section": "Page 266"}
{"text": "BPEL (Business Process Execution Language).\nThe old ER approach of using disk files to communicate between services was too slow for this much\nmore rapid flip-flopping of services, so we invented a shared memory mechanism that we called the\n3DBB.12 The 3DBB allowed data to be accessed by name; the names we used were names assigned\nto each state machine instance.\nThe 3DBB was great for storing strings and constants, but couldn\u2019t be used for holding complex data\nstructures. The reason for this is technical but easy to understand. Each process in MP/M lived in its\nown memory partition. Pointers to data in one memory partition had no meaning in another memory\npartition. As a consequence, the data in the 3DBB could not contain pointers. Strings were fine, but\ntrees, linked lists, or any data structure with pointers would not work.\nThe trouble tickets in the trouble ticket system came from many different sources. Some were\nautomated, and some were manual. The manual entries were created by operators who were talking to\ncustomers about their troubles. As the customers described their problems, the operators would type\nin their complaints and observations in a structured text stream. It looked something like this:\nClick here to view code image\n/pno 8475551212 /noise /dropped-calls\nYou get the idea. The / character started a new topic. Following the slash was a code, and following\nthe code were parameters. There were thousands of codes, and an individual trouble ticket could\nhave dozens of them in the description. Worse, since they were manually entered, they were often\nmisspelled or improperly formatted. They were meant for humans to interpret, not for machines to\nprocess.\nOur problem was to decode these semi-free-form strings, interpret and fix any errors, and then turn\nthem into voice output so we could read them to the repairman, up on a pole, listening with a handset.\nThis required, among other things, a very flexible parsing and data representation technique. That data\nrepresentation had to be passed through the 3DBB, which could handle only strings.\nAnd so, on an airplane, flying between customer visits, I invented a scheme that I called FLD: Field\nLabeled Data. Nowadays we would call this XML or JSON. The format was different, but the idea\nwas the same. FLDs were binary trees that associated names with data in a recursive hierarchy. FLDs\ncould be queried by a simple API, and could be translated to and from a convenient string format that\nwas ideal for the 3DBB.\nSo, micro-services communicating through shared memory analog of sockets using an XML analog\u2014\nin 1985.\nThere is nothing new under the Sun.\nCLEAR COMMUNICATIONS\n", "page": 267, "type": "text", "section": "Page 267"}
{"text": "In 1988, a group of Teradyne employees left the company to form a startup named Clear\nCommunications. I joined them a few months later. Our mission was to build the software for a\nsystem that would monitor the communications quality of T1 lines\u2014the digital lines that carried long-\ndistance communications across the country. The vision was a huge monitor with a map of the United\nStates crisscrossed by T1 lines flashing red if they were degrading.\nRemember, graphical user interfaces were brand new in 1988. The Apple Macintosh was only five\nyears old. Windows was a joke back then. But Sun Microsystems was building Sparcstations that had\ncredible X-Windows GUIs. So we went with Sun\u2014and therefore with C and UNIX.\nThis was a startup. We worked 70 to 80 hours per week. We had the vision. We had the motivation.\nWe had the will. We had the energy. We had the expertise. We had equity. We had dreams of being\nmillionaires. We were full of shit.\nThe C code poured out of every orifice of our bodies. We slammed it here, and shoved it there. We\nconstructed huge castles in the air. We had processes, and message queues, and grand, superlative\narchitectures. We wrote a full seven-layer ISO communications stack from scratch\u2014right down to the\ndata link layer.\nWe wrote GUI code. GOOEY CODE! OMG! We wrote GOOOOOEY code.\nI personally wrote a 3000-line C function named gi(); its name stood for Graphic Interpreter. It was\na masterpiece of goo. It was not the only goo I wrote at Clear, but it was my most infamous.\nArchitecture? Are you joking? This was a startup. We didn\u2019t have time for architecture. Just code,\ndammit! Code for your very lives!\nSo we coded. And we coded. And we coded. But, after three years, what we failed to do was sell.\nOh, we had an installation or two. But the market was not particularly interested in our grand vision,\nand our venture capital financiers were getting pretty fed up.\nI hated my life at this point. I saw all my effort and dreams crashing down. I had conflicts at work,\nconflicts at home because of work, and conflicts with myself.\nAnd then I got a phone call that changed everything.\nTHE SETUP\nTwo years before that phone call, two things of significance happened.\nFirst, I managed to set up a uucp connection to a nearby company that had a uucp connection to\nanother facility that was connected to the Internet. These connections were dial-up, of course. Our\nmain Sparcstation (the one on my desk) used a 1200-bps modem to call up our uucp host twice per\nday. This gave us email and Netnews (an early social network where people discussed interesting\nissues).\n", "page": 268, "type": "text", "section": "Page 268"}
{"text": "Second, Sun released a C++ compiler. I had been interested in C++ and OO since 1983, but\ncompilers were difficult to come by. So when the opportunity presented itself, I changed languages\nright away. I left the 3000-line C functions behind, and started to write C++ code at Clear. And I\nlearned \u2026\nI read books. Of course, I read The C++ Programming Language and The Annotated C++\nReference Manual (The ARM) by Bjarne Stroustrup. I read Rebecca Wirfs-Brock\u2019s lovely book on\nresponsibility-driven design: Designing Object Oriented Software. I read OOA and OOD and OOP\nby Peter Coad. I read Smalltalk-80 by Adele Goldberg. I read Advanced C++ Programming Styles\nand Idioms by James O. Coplien. But perhaps most significantly of all, I read Object Oriented\nDesign with Applications by Grady Booch.\nWhat a name! Grady Booch. How could anyone forget a name like that. What\u2019s more, he was the\nChief Scientist at a company called Rational! How I wanted to be a Chief Scientist! And so I read\nhis book. And I learned, and I learned, and I learned \u2026\nAs I learned, I also began debating on Netnews, the way people now debate on Facebook. My\ndebates were about C++ and OO. For two years, I relieved the frustrations that were building at work\nby debating with hundreds of folks on Usenet about the best language features and the best principles\nof design. After a while, I even started making a certain amount of sense.\nIt was in one of those debates that the foundations of the SOLID principles were laid.\nAnd all that debating, and perhaps even some of the sense, got me noticed \u2026\nUNCLE BOB\nOne of the engineers at Clear was a young fellow by the name of Billy V\nogel. Billy gave nicknames to\neveryone. He called me Uncle Bob. I suspect, despite my name being Bob, that he was making an\noffhand reference to J. R. \u201cBob\u201d Dobbs (see https://en.wikipedia.org/wiki/File:Bobdobbs.png).\nAt first I tolerated it. But as the months went by, his incessant chattering of \u201cUncle Bob, \u2026 Uncle\nBob,\u201d in the context of the pressures and disappointments of the startup, started to wear pretty thin.\nAnd then, one day, the phone rang.\nTHE PHONE CALL\nIt was a recruiter. He had gotten my name as someone who knew C++ and object-oriented design. I\u2019m\nnot sure how, but I suspect it had something to do with my Netnews presence.\nHe said he had an opportunity in Silicon V\nalley, at a company named Rational. They were looking for\nhelp building a CASE13 tool.\nThe blood drained from my face. I knew what this was. I don\u2019t know how I knew, but I knew. This\nwas Grady Booch\u2019s company. I saw before me the opportunity to join forces with Grady Booch!\n", "page": 269, "type": "text", "section": "Page 269"}
{"text": "ROSE\nI joined Rational, as a contract programmer, in 1990. I was working on the ROSE product. This was\na tool that allowed programmers to draw Booch diagrams\u2014the diagrams that Grady had written\nabout in Object-Oriented Analysis and Design with Applications (Figure A.9 shows an example).\nFigure A.9 A Booch diagram\nThe Booch notation was very powerful. It presaged notations like UML.\nROSE had an architecture\u2014a real architecture. It was constructed in true layers, and the\ndependencies between layers were properly controlled. The architecture made it releasable,\ndevelopable, and independently deployable.\nOh, it wasn\u2019t perfect. There were a lot of things we still didn\u2019t understand about architectural\nprinciples. We did not, for example, create a true plugin structure.\nWe also fell for one of the most unfortunate fads of the day\u2014we used a so-called object-oriented\ndatabase.\nBut, overall, the experience was a great one. I spent a lovely year and a half working with the\nRational team on ROSE. This was one of the most intellectually stimulating experiences of my\nprofessional life.\nTHE DEBATES CONTINUED\nOf course, I did not stop debating on Netnews. In fact, I drastically increased my network presence. I\nstarted writing articles for C++ Report. And, with Grady\u2019s help, I started working on my first book:\nDesigning Object-Oriented C++ Applications Using the Booch Method.\nOne thing bothered me. It was perverse, but it was true. No one was calling me \u201cUncle Bob.\u201d I found\nthat I missed it. So I made the mistake of putting \u201cUncle Bob\u201d in my email and Netnews signatures.\n", "page": 270, "type": "text", "section": "Page 270"}
{"text": "And the name stuck. Eventually I realized that it was a pretty good brand.\n... BY ANY OTHER NAME\nROSE was a gigantic C++ application. It was composed of layers, with a strictly enforced\ndependency rule. That rule is not the rule that I have described in this book. We did not point our\ndependencies toward high-level policies. Rather, we pointed our dependencies in the more\ntraditional direction of flow control. The GUI pointed at the representation, which pointed at the\nmanipulation rules, which pointed at the database. In the end, it was this failure to direct our\ndependencies toward policy that aided the eventual demise of the product.\nThe architecture of ROSE was similar to the architecture of a good compiler. The graphical notation\nwas \u201cparsed\u201d into an internal representation; that representation was then manipulated by rules and\nstored in an object-oriented database.\nObject-oriented databases were a relatively new idea, and the OO world was all abuzz with the\nimplications. Every object-oriented programmer wanted to have an object-oriented database in his or\nher system. The idea was relatively simple, and deeply idealistic. The database stores objects, not\ntables. The database was supposed to look like RAM. When you accessed an object, it simply\nappeared in memory. If that object pointed to another object, the other object would appear in\nmemory as soon as you accessed it. It was like magic.\nThat database was probably our biggest practical mistake. We wanted the magic, but what we got\nwas a big, slow, intrusive, expensive third-party framework that made our lives hell by impeding our\nprogress on just about every level.\nThat database was not the only mistake we made. The biggest mistake, in fact, was over-architecture.\nThere were many more layers than I have described here, and each had its own brand of\ncommunications overhead. This served to significantly reduce the productivity of the team.\nIndeed, after many man-years of work, immense struggles, and two tepid releases, the whole tool was\nscrapped and replaced with a cute little application written by a small team in Wisconsin.\nAnd so I learned that great architectures sometimes lead to great failures. Architecture must be\nflexible enough to adapt to the size of the problem. Architecting for the enterprise, when all you really\nneed is a cute little desktop tool, is a recipe for failure.\nARCHITECTS REGISTRY EXAM\nIn the early 1990s, I became a true consultant. I traveled the world teaching people what this new OO\nthing was. My consulting was focused strongly on the design and architecture of object-oriented\nsystems.\nOne of my first consulting clients was Educational Testing Service (ETS). It was under contract with\nthe National Council of Architects Registry Board (NCARB) to conduct the registration exams for\n", "page": 271, "type": "text", "section": "Page 271"}
{"text": "new architect candidates.\nAnyone desiring to be a registered architect (the kind who design buildings) in the United States or\nCanada must pass the registration exam. This exam involved having the candidate solve a number of\narchitectural problems involving building design. The candidate might be given a set of requirements\nfor a public library, or a restaurant, or a church, and then asked to draw the appropriate architectural\ndiagrams.\nThe results would be collected and saved until such time as a group of senior architects could be\ngathered together as jurors, to score the submissions. These gatherings were big, expensive events\nand were the source of much ambiguity and delay.\nNCARB wanted to automate the process by having the candidates take the exams using a computer,\nand then have another computer do the evaluation and scoring. NCARB asked ETS to develop that\nsoftware, and ETS hired me to gather a team of developers to produce the product.\nETS had broken the problem down into 18 individual test vignettes. Each would require a CAD-like\nGUI application that the candidate would use to express his or her solution. A separate scoring\napplication would take in the solutions and produce scores.\nMy partner, Jim Newkirk, and I realized that these 36 applications had vast amounts of similarity. The\n18 GUI apps all used similar gestures and mechanisms. The 18 scoring applications all used the same\nmathematical techniques. Given these shared elements, Jim and I were determined to develop a\nreusable framework for all 36 applications. Indeed, we sold this idea to ETS by saying that we\u2019d\nspend a long time working on the first application, but then the rest would just pop out every few\nweeks.\nAt this point you should be face-palming or banging your head on this book. Those of you who are old\nenough may remember the \u201creuse\u201d promise of OO. We were all convinced, back then, that if you just\nwrote good clean object-oriented C++ code, you would just naturally produce lots and lots of\nreusable code.\nSo we set about to write the first application\u2014which was the most complicated of the batch. It was\ncalled Vignette Grande.\nThe two of us worked full time on Vignette Grande with an eye toward creating a reusable\nframework. It took us a year. At the end of that year we had 45,000 lines of framework code and 6000\nlines of application code. We delivered this product to ETS, and they contracted with us to write the\nother 17 applications post-haste.\nSo Jim and I recruited a team of three other developers and we began to work on the next few\nvignettes.\nBut something went wrong. We found that the reusable framework we had created was not\nparticularly reusable. It did not fit well into the new applications being written. There were subtle\nfrictions that just didn\u2019t work.\n", "page": 272, "type": "text", "section": "Page 272"}
{"text": "This was deeply discouraging, but we believed we knew what to do about it. We went to ETS and\ntold them that there would be a delay\u2014that the 45,000-line framework needed to be rewritten, or at\nleast readjusted. We told them that it would take a while longer to get that done.\nI don\u2019t need to tell you that ETS was not particularly happy with this news.\nSo we began again. We set the old framework aside and began writing four new vignettes\nsimultaneously. We would borrow ideas and code from the old framework but rework them so that\nthey fit into all four without modification. This effort took another year. It produced another 45,000-\nline framework, plus four vignettes that were on the order of 3000 to 6000 lines each.\nNeedless to say, the relationship between the GUI applications and the framework followed the\nDependency Rule. The vignettes were plugins to the framework. All the high-level GUI policy was in\nthe framework. The vignette code was just glue.\nThe relationship between the scoring applications and the framework was a bit more complex. The\nhigh-level scoring policy was in the vignette. The scoring framework plugged into the scoring\nvignette.\nOf course, both of these applications were statically linked C++ applications, so the notion of plugin\nwas nowhere in our minds. And yet, the way the dependencies ran was consistent with the\nDependency Rule.\nHaving delivered those four applications, we began on the next four. And this time they started\npopping out the back end every few weeks, just as we had predicted. The delay had cost us nearly a\nyear on our schedule, so we hired another programmer to speed the process along.\nWe met our dates and our commitments. Our customer was happy. We were happy. Life was good.\nBut we learned a good lesson: You can\u2019t make a reusable framework until you first make a usable\nframework. Reusable frameworks require that you build them in concert with several reusing\napplications.\nCONCLUSION\nAs I said at the start, this appendix is somewhat autobiographical. I\u2019ve hit the high points of the\nprojects that I felt had an architectural impact. And, of course, I mentioned a few episodes that were\nnot exactly relevant to the technical content of this book, but were significant nonetheless.\nOf course, this was a partial history. There were many other projects that I worked on over the\ndecades. I also purposely stopped this history in the early 1990s\u2014because I have another book to\nwrite about the events of the late 1990s.\nMy hope is that you enjoyed this little trip down my memory lane; and that you were able to learn\nsome things along the way.\n", "page": 273, "type": "text", "section": "Page 273"}
{"text": "1. One of the stories we heard about the particular machine at ASC was that it was shipped in a large semi-trailer truck along with a\nhousehold of furniture. On the way, the truck hit a bridge at high speed. The computer was fine, but it slid forward and crushed the\nfurniture into splinters.\n2. Today we would say that it had a clock rate of 142 kHz.\n3. Imagine the mass of that disk. Imagine the kinetic energy! One day we came in and saw little metal shavings dropping out from the\nbutton of the cabinet. We called the maintenance man. He advised us to shut the unit down. When he came to repair it, he said that\none of the bearings had worn out. Then he told us stories about how these disks, if not repaired, could tear loose from their moorings,\nplow through concrete block walls, and embed themselves into cars in the parking lot.\n4. Cathode ray tube: monochrome, green-screen, ASCII displays.\n5. The magic number 72 came from Hollerith punched cards, which held 80 characters each. The last 8 characters were \u201creserved\u201d for\nsequence numbers in case you dropped the deck.\n6. Yes, I understand that\u2019s an oxymoron.\n7. They had a little clear plastic window that allowed you to see the silicon chip inside, and allowed the UV to erase the data.\n8. Yes, I know that when software is burned into ROM, it\u2019s called firmware\u2014but even firmware is really still soft.\n9. RKO7.\n10. This was later renamed as Bob\u2019s Only Successful Software.\n11. Our company held the patent. Our employment contract made it clear that anything we invented belonged to our company. My boss\ntold me: \u201cYou sold it to us for one dollar, and we didn\u2019t pay you that dollar.\u201d\n12. Three-Dimensional Black Board. If you were born in the 1950s, you likely get this reference: Drizzle, Drazzle, Druzzle, Drone.\n13. Computer Aided Software Engineering\n", "page": 274, "type": "text", "section": "Page 274"}
{"text": "INDEX\nNumbers\n3DBB shared memory system, Craft Dispatch System archaeology project, 363\n4-TEL, archaeology projects\nBOSS, 351\u2013352\nC language, 349\u2013351\nDLU/DRU, 354\u2013356\noverview of, 339\u2013344\npCCU, 352\u2013354\nSAC (service area computer), 344\u2013349\nVRS, 357\u2013359\n8085 computer, archaeological projects\n4-TEL, 341\nBOSS, 351\nC language and, 349\u2013351\nDLU/DRU, 356\n8086 Intel microcomputer, SAC archaeology project, 347\u2013348\nA\nAbstract classes\nconclusion, 132\nDependency Inversion Principle and, 87\nleftover in Zone of Uselessness, 129\u2013130\nplacing high-level policy, 126\u2013128\nservices in Java as set of, 246\nAbstract components, 125\u2013126\nAbstract Factories, 89\u201390\nAbstractions\nprinciple of stable. See SAP (Stable Abstractions Principle)\nsource code dependencies and, 87\nstable, 88\u201389\nAccess modifiers, architectural packages, 316\u2013319\nAccidental duplication, 154\u2013155\nActors, 62\u201365\n", "page": 275, "type": "text", "section": "Page 275"}
{"text": "Address segments, relocatable binaries, 99\u2013100\nADP (Acyclic Dependencies Principle)\nbreaking cycle, 117\u2013118\ncomponent dependency graph effected by, 118\neffect of cycle in component dependency graph, 115\u2013117\neliminating dependency cycles, 113\u2013115\njitters, 118\noverview of, 112\nweekly build, 112\u2013113\nAluminum die-cast monitoring, archaeology project, 338\u2013339\nAPIs, testing, 252\u2013253\nApp-titude test, 258\u2013261\nApplication-specific business rules, use cases, 192\u2013193, 204\nArchitects\ngoal to minimize human resources, 160\nregistry exam archaeology project, 370\u2013373\nseparate details from policy, 142\nArchitecture\nclean. See Clean architecture\nclean embedded. See Clean embedded architecture\ndesign vs., 4\nin DLU/DRU archaeology project, 356\nEisenhower\u2019s matrix of importance vs. urgency, 16\u201317\ngetting software right, 2\nimmutability and, 52\nindependence. See Independence\nISP and, 86\nLSP and, 80\nplugin, 170\u2013171\nin ROSE archaeology product, 368\u2013370\nin SAC archaeology project, 345\u2013347\nas senior to function, 18\nas software value, 14\u201315\nstability, 122\u2013126\ntesting, 213\nthree big concerns in, 24\nvalue of function vs., 15\u201316\nin VRS archaeology project, 358\u2013359\nArchitecture archaeology projects\n4-TEL, 339\u2013344\naluminum die-cast monitoring, 338\u2013339\n", "page": 276, "type": "text", "section": "Page 276"}
{"text": "architects registry exam, 370\u2013373\nby author since 1970, 325\u2013326\nBasic Operating System and Scheduler, 351\u2013352\nC language, 349\u2013351\nconclusion, 373\nCraft Dispatch System, 361\u2013367\nDLU/DRU, 354\u2013356\nElectronic Receptionist, 359\u2013361\nLaser Trim, 334\u2013338\npCCU, 352\u2013354\nROSE product, 368\u2013370\nservice area computer, 344\u2013349\nUnion Accounting system, 326\u2013334\nVRS, 357\u2013359\nArchitecture, defining\nconclusion, 146\ndeployment, 138\ndevelopment, 137\u2013138\ndevice independence, 142\u2013143\njunk mail example, 144\u2013145\nkeeping options open, 140\u2013142\nmaintenance, 139\u2013140\noperation, 138\u2013139\nphysical addressing example, 145\u2013146\nunderstanding, 135\u2013137\nArchitecture, screaming\nabout the web, 197\u2013198\nconclusion, 199\nframeworks as tools, not ways of life, 198\noverview of, 195\u2013196\npurpose of, 197\ntestable architectures, 198\ntheme, 196\u2013197\nArchives, as aggregation of components, 96\nArtifacts, OCP, 70\nASC Tabulating, Union Accounting archaeology project, 326\u2013334\nAssignment, and functional programming, 23\nAsymmetric marriage, to framework authors, 292\u2013293\nAuthors, framework, 292\u2013293\nAutomated systems, business rules, 191\u2013192\n", "page": 277, "type": "text", "section": "Page 277"}
{"text": "B\nBase classes, frameworks, 293\nBCE system architecture, 202\nBeck, Kent, 258\u2013261\nBehavior (function)\narchitecture supports system, 137, 148\nEisenhower\u2019s matrix of importance vs. urgency, 16\u201317\nfighting for seniority of architecture over function, 18\nkeeping options open, 140\u2013142\nas software value, 14\nvalue of function vs. architecture, 15\u201316\nBinaries, relocatability, 99\u2013100\nBooch, Grady\nintroduction to, 366\nworking for, 367\nworking on ROSE product, 368\u2013369\nBOSS (Basic Operating System and Scheduler)\narchaeology project, 351\u2013352\nin DLU/DRU archaeology project, 356\nBoundaries\nin 4-TEL archaeology project, 340\u2013341\nconclusion, 173\ndividing services into components, 246\nin Electronic Receptionist archaeology project, 361\nFitNesse program, 163\u2013165\ninput and output, 169\u2013170\nin Laser Trim archaeology project, 338\nlayers and. See Layers and boundaries\noverview of, 159\u2013160\npartial, 218\u2013220\nplugin architecture, 170\u2013171\nplugin argument, 172\u2013173\nsad stories of architectural failures, 160\u2013163\nservices as function calls across, 240\ntest, 249\u2013253\nUnion Accounting system archaeology project, 333\u2013334\nwhich lines to draw, and when, 165\u2013169\nBoundary anatomy\nboundary crossing, 176\nconclusion, 181\ndeployment components, 178\u2013179\n", "page": 278, "type": "text", "section": "Page 278"}
{"text": "dreaded monolith, 176\u2013178\nlocal processes, 179\u2013180\nservices, 180\u2013181\nthreads, 179\nBoundary crossing\nin clean architecture, 206\nclean architecture scenario, 207\u2013208\ncreating appropriate, 176\nDependency Rule for data in, 207\nBreaking cycle, Acyclic Dependencies Principle, 117\u2013118\nBusiness managers\nEisenhower\u2019s matrix of importance vs. urgency, 17\npreference for function vs. architecture, 15\u201316\nBusiness rules\nboundaries between GUI and, 169\u2013170\nclean architecture for, 202\u2013203\nconclusion, 194\ncreating Entities, 190\u2013191\ndecoupling from UI, 287\u2013289\ndecoupling layers, 152\u2013153\ndecoupling use cases, 153\ndesigning for testability, 251\nin Hunt the Wumpus adventure game, 222\u2013223\nindependent developability, 47\nkeeping close to data, 67\nplugging into, 170\u2013173\npolicy statements calculating, 184\nrequest/response models and, 193\u2013194\nin SAC archaeology project, 346\u2013347\nseparating components with boundary lines, 165\u2013169\nunderstanding, 189\u2013190\nuse cases for, 191\u2013193, 204\nC\nC++ language\ninheritance in, 40\nlearning, 366\nmarrying STL framework in, 293\npolymorphism in, 42\nROSE application, 369\u2013370\nweakening encapsulation, 36\u201337\n", "page": 279, "type": "text", "section": "Page 279"}
{"text": "C language\nBOSS archaeology project using, 351\u2013352\nDLU/DRU archaeology project using, 356\nencapsulation in, 34\u201336\ninheritance in, 38\u201340\npolymorphism in, 40\u201342\nredesigning SAC in, 347\u2013348\nC language, archaeology project, 349\u2013351\nC# programming language\nabstract components in, 125\ndependency inversion, 45\nusing statements for dependencies, 184\nweakening encapsulation, 36\u201337\nC Programming Language (Kernighan & Ritchie), 351\nC4 software architecture model, 314\u2013315\nCarew, Mike, 356\nCASE (Computer Aided Software Engineering) tool, 368\nCase study. See Video sales case study\nCathode ray tube (CRT) terminals, Union Accounting archaeology project, 328\u2013329\nCCP (Common Closure Principle)\ndecoupling layers, 152\ngrouping policies into components, 186\u2013187\nkeeping changes localized, 118\noverview of, 105\u2013107\nStable Dependencies Principle and, 120\ntension diagram, 108\u2013110\nCCU/CMU (COL\nT control unit/COL\nT measurement unit), pCCU archaeology project, 353\u2013354\nCDS (Craft Dispatch System), archaeology project\noverview of, 361\u2013363\nCentral office line testers. See COL\nTs (central office line testers)\nCentral offices (COs), 4-TEL archaeology project, 339\u2013340\nChange, ease of software, 14\u201315\nChurch, Alonzo, 22\u201323, 50\nCICS-COBOL program, aluminum die-cast archaeology project, 339\nClasses\nabstract. See Abstract classes\nCommon Reuse Principle, 107\u2013108\nDIP and, 89\nLSP use in guiding inheritance, 78\npartitioning processes into, 71\u201372\nReuse/Release Equivalence Principle, 105\n", "page": 280, "type": "text", "section": "Page 280"}
{"text": "SRP examples, 67\nClean architecture\ncharacteristics of, 201\u2013203\nconclusion, 209\nDependency Rule, 203\u2013207\nframeworks tend to violate, 293\ntypical scenario, 208\nusing layers and boundaries, 223\u2013226\nClean embedded architecture\napp-titude test, 258\u2013261\nconclusion, 273\ndon\u2019t reveal hardware details to user of HAL, 265\u2013269\nDRY conditional compilation directives, 272\nhardware is detail, 263\u2013264\nis testable embedded architecture, 262\nlayers, 262\u2013263\noperating system is detail, 269\u2013271\noverview of, 255\u2013258\nprogramming to interfaces and substitutability, 271\u2013272\ntarget-hardware bottleneck, 261\nCleancoders.com, 297\nClear Communications, 364\u2013367\nphone call, 367\nsetup, 366\nUncle Bob, 367\nClojure, 50\u201351, 53\u201354\nCodd, Edgar, 278\nCode\nin aluminum die-cast archaeology project, 338\u2013339\ndecreasing productivity/increased cost of, 5\u20137\nfoolishness of overconfidence, 9\u201312\nrising costs of development payroll, 8\u20139\nin SAC archaeology project, 345\nsignature of messy, 7\u20138\nsource code dependencies. See Source code dependencies\nCode organization\nconclusion, 321\ndevil is in the details, 315\u2013316\nother decoupling modes, 319\u2013320\noverview of, 303\u2013304\npackage by component, 310\u2013315\n", "page": 281, "type": "text", "section": "Page 281"}
{"text": "package by feature, 306\u2013307\npackage by layer, 304\u2013306\nports and adapters, 308\u2013310\nvs. encapsulation, 316\u2013319\nCohesion, Single Responsibility Principle, 63\nCOL\nTs (central office line testers)\nin 4-TEL archaeology project, 340\u2013344\npCCU archaeology project, 352\u2013354\nin service area computer archaeology project, 344\u2013349\nCommon Closure Principle. See CCP (Common Closure Principle)\nCommon Reuse Principle. See CRP (Common Reuse Principle)\nCommunications\nacross deployment component boundaries, 179\nacross local process boundaries, 180\nacross service boundaries, 180\u2013181\nacross source-level decoupled boundaries, 178\nConway\u2019s law, 149\nas function calls between components in monoliths, 178\nin types of decoupling modes, 155\u2013157\nCompare and swap algorithm, 54\nCompiled languages, 96\nCompilers\nenforce architectural principles with, 319\nlocation of source code, 97\u201398\nrelocatable binaries, 99\u2013100\nComponent architecture, video sales case study, 300\u2013302\nComponent-based systems\nbuilding scalable, 241\ndesigning services using SOLID, 245\u2013246\nfunction calls, 240\nOO approach for cross-cutting concerns, 244\u2013245\nComponent cohesion\nCommon Closure Principle, 105\u2013107\nCommon Reuse Principle, 107\u2013108\nconclusion, 110\noverview of, 104\nReuse/Release Equivalence Principle, 104\u2013105\ntension diagram, 108\u2013110\nComponent coupling\nADP. See ADP (Acyclic Dependencies Principle)\nconclusion, 132\n", "page": 282, "type": "text", "section": "Page 282"}
{"text": "Fragile Tests Problem, 251\noverview of, 111\nStable Abstractions Principle. See SAP (Stable Abstractions Principle)\nStable Dependencies Principle, 120\u2013126\ntop-down design, 118\u2013119\nComponent dependency graph\nbreak cycle of components/reinstate as DAG, 117\u2013118\neffect of cycle in, 115\u2013117\nComponent-per-team architecture, 137\u2013138\nComponents\nconcrete, 91\ndeployment of, 178\u2013179\nhistory of, 96\u201399\nlinkers, 100\u2013102\noverview of, 96\npackage by, 313\u2013315\npartitioning processes into classes/separating classes into, 71\u201372\nprinciples, 93\nrelocatability, 99\u2013100\ntests as system, 250\nComputer Aided Software Engineering (CASE) tool, 368\nConcrete components, Dependency Inversion Principle, 91\nConcurrent tasks, BOSS archaeology project, 351\u2013352\nConcurrent updates, 52\u201353\nConstantine, Larry, 29\nControl, flow of. See Flow of control\nControl structures, program, 27\u201328\nControl, transfer of, 22\nControllers\nin clean architecture, 203, 205\nclean architecture scenario, 207\u2013208\ncrossing circle boundaries, 206\nConway\u2019s law, 149\nCopper wires, pCCU archaeology project, 352\u2013354\nCore code, avoid frameworks in, 293\nCOs (central offices), 4-TEL archaeology project, 339\u2013340\nCoupling. See also Component coupling\navoid allowing framework, 293\nto premature decisions, 160\nCraft Dispatch System. See CDS (Craft Dispatch System), archaeology project\nCritical Business Data, 190\u2013191\n", "page": 283, "type": "text", "section": "Page 283"}
{"text": "Critical Business Rules, 190\u2013193\nCross-cutting concerns\ndesigning services to deal with, 247\nobject-oriented approach to, 244\u2013245\nCrossing data streams, 226\nCRP (Common Reuse Principle)\ninfluencing composition of components, 118\noverview of, 107\u2013108\ntension diagram, 108\u2013110\nCRT (cathode ray tube) terminals, Union Accounting archaeology project, 328\u2013329\nCycles\nbreaking, 117\u2013118\neffect of in dependency graph, 115\u2013117\neliminating dependency, 113\u2013115\nweekly build issues, 112\u2013113\nD\nD metric, distance from Main Sequence, 130\u2013132\nDAGs (directed acyclic graphs)\narchitectural framework for policy, 184\nbreaking cycle of components, 117\u2013118\ndefined, 114\nDahl, Ole Johan, 22\nData\nclean architecture scenario, 207\u2013208\nDependency Rule for crossing boundaries, 207\nmanagement concerns in architecture, 24\nmappers, 214\u2013215\nseparating from functions, 66\nData model, database vs., 278\nData storage\nin Laser Trim archaeology project, 335\nprevalence of database systems due to disks, 279\u2013280\nin Union Accounting archaeology project, 327\u2013328\nDatabase\nclean architecture independent from, 202\nclean architecture scenario, 207\u2013208\ncreating testable architecture without, 198\ndecoupling layers, 153\ndecoupling use cases, 153\nDependency Rule, 205\n", "page": 284, "type": "text", "section": "Page 284"}
{"text": "drawing boundary line between business rules and, 165\ngateways, 214\nin Hunt the Wumpus adventure game, 222\u2013223\nindependent developability, 47\nleaving options open in development, 141, 197\nplugin architecture, 171\nrelational, 278\nschema in Zone of Pain, 129\nseparating components with boundary lines, 165\u2013169\nDatabase is detail\nanecdote, 281\u2013283\nconclusion, 283\ndetails, 281\nif there were no disks, 280\u2013281\noverview of, 277\u2013278\nperformance, 281\nrelational databases, 278\nwhy database systems are so prevalent, 279\u2013280\nDCI system architecture, 202\nDeadlocks, from mutable variables, 52\nDecoupling\nas fallacy of services, 240\u2013241\nindependent deployment, 154, 241\nindependent development, 153\u2013154, 241\nkitty problem example, 242\u2013243\nlayers, 151\u2013152\nmodes, 153, 155\u2013158\nOO approach for cross-cutting concerns, 244\u2013245\npurpose of testing API, 252\u2013253\nsource code dependencies, 319\nuse cases, 152\nDeMarco, Tom, 29\nDependencies\nADP. See ADP (Acyclic Dependencies Principle)\narchitectural framework for policy, 184\ncalculating stability metrics, 123\ncase study. See Video sales case study\nCommon Reuse Principle and, 107\u2013108\nDIP. See DIP (Dependency Inversion Principle)\nin Laser Trim archaeology project, 338\nmanaging undesirable, 89\u201390\n", "page": 285, "type": "text", "section": "Page 285"}
{"text": "OCP example, 72\nin package by layer, 304\u2013306, 310\u2013311\nsoftware destroyed by unmanaged, 256\nstable. See SDP (Stable Dependencies Principle)\ntransitive, 75\nunderstanding component, 121\nin Union Accounting archaeology project, 333\u2013334\nDependency graph, 115\u2013118\nDependency Injection framework, Main component, 232\nDependency inversion, 44\u201347\nDependency management\nmetrics. See ADP (Acyclic Dependencies Principle)\nvia full-fledged architectural boundaries, 218\nvia polymorphism in monolithic systems, 177\nvideo sales case study, 302\nDependency Rule\nclean architecture and, 203\u2013206\nclean architecture scenario, 207\u2013208\ncrossing boundaries, 206\ndefined, 91\ndependency management, 302\ndesigning services to follow, 247\nEntities, 204\nframeworks and drivers, 205\nframeworks tending to violate, 293\nin Hunt the Wumpus adventure game, 223\ninterface adapters, 205\nOO approach for cross-cutting concerns, 244\u2013245\nservices may follow, 240\ntests following, 250\nuse cases, 204\nwhich data crosses boundaries, 207\nDeployment\narchitecture determines ease of, 150\ncomponents, 178\u2013180\ncomponents as units of, 96\nimpact of architecture on, 138\ntests use independent, 250\nDeployment-level decoupling mode, 156\u2013157, 178\u2013179\nDesign\napproaches to. See Code organization\n", "page": 286, "type": "text", "section": "Page 286"}
{"text": "architecture vs., 4\ndecreasing productivity/increases cost of code, 5\u20137\ngetting it right, 2\ngoal of good, 4\u20135\nreducing volatility of interfaces, 88\nsignature of a mess, 7\u20138\nSOLID principles of, 57\u201359\nfor testability, 251\nDesigning Object-Oriented C++ Applications Using the Booch Method, 369\nDetail\ndatabase is. See Database is detail\ndon\u2019t reveal hardware, to user of HAL, 265\u2013269\nframework is, 291\u2013295\nhardware is, 263\u2013264\nseparating from policy, 140\u2013142\nstory of architectural success, 163\u2013165\nweb is, 285\u2013289\nDevelopers\ndecreasing productivity/increasing cost of code, 5\u20137\nEisenhower\u2019s matrix of importance vs. urgency, 17\nfoolishness of overconfidence, 9\u201312\npreference for function vs. architecture, 15\u201316\nscope vs. shape in determining cost of change, 15\nsignature of a mess, 8\u20139\nas stakeholders, 18\nDevelopment\nimpact of architecture on, 137\u2013138\nindependent. See Independent developability\nrole of architecture in supporting, 149\u2013150\nrole of test to support, 250\nDevice independence\ndefined, 142\u2013143\nIO device of UI as, 288\u2013289\njunk mail example, 144\u2013145\nphysical addressing example, 145\u2013146\nin programming, 44\nDigital revolution, and telephone companies, 352\u2013354\nDijkstra, Edsger Wybe\napplying discipline of proof to programming, 27\ndiscovery of structured programming, 22\nhistory of, 26\n", "page": 287, "type": "text", "section": "Page 287"}
{"text": "proclamation on goto statements, 28\u201329\non testing, 31\nDIP (Dependency Inversion Principle)\nbreaking cycle of components, 117\u2013118\nconclusion, 91\nconcrete components, 91\ncrossing circle boundaries, 206\ndefined, 59\ndrawing boundary lines, 173\nEntities without knowledge of use cases as, 193\nfactories, 89\u201390\nin good software architecture, 71\nnot all components should be stable, 125\noverview of, 87\u201388\nstable abstractions, 88\u201389\nStable Abstractions Principle, 127\nDirected acyclic graph. See DAGs (directed acyclic graphs)\nDirectional control, Open-Closed Principle, 74\nDisks\nif there were no, 280\u2013281\nprevalence of database systems due to, 279\u2013280\nin Union Accounting archaeology project, 326\u2013330\nDispatch code, service area computer project, 345\nDisplay local unit/display remote unit (DLU/DRU) archaeology project, 354\u2013356\nDLU/DRU (display local unit/display remote unit), archaeology project, 354\u2013356\nDo/while/until statements, 22, 27\nDon\u2019t Repeat Yourself (DRY) principle, conditional compilation directives, 272\nDrawing lines. See Boundaries\nDrivers, Dependency Rule, 205\nDRY (Don\u2019t Repeat Yourself) principle, conditional compilation directives, 272\nDSL (domain-specific data-driven language), Laser Trim archaeology project, 337\nDuplication\naccidental, 63\u201365\ntrue vs. accidental, 154\u2013155\nDynamic polymorphism, 177\u2013178, 206\nDynamically linked libraries, as architectural boundaries, 178\u2013179\nDynamically typed languages\nDIP and, 88\nISP and, 85\nE\n", "page": 288, "type": "text", "section": "Page 288"}
{"text": "Editing, Laser Trim archaeology project, 336\nEducational Testing Service (ETS), 370\u2013372\nEisenhower, matrix of importance vs. urgency, 16\u201317\nEmbedded architecture. See Clean embedded architecture\nEncapsulation\nin defining OOP, 35\u201337\norganization vs., 316\u2013319\noveruse of public and, 316\nEntities\nbusiness rules and, 190\u2013191\nclean architecture scenario, 207\u2013208\ncreating testable architecture, 198\nDependency Rule for, 204\nrisks of frameworks, 293\nuse cases vs., 191\u2013193\nEnumeration, Dijkstra\u2019s proof for sequence/selection, 28\nEPROM (Erasable Programmable Read-Only Memory) chips, 4-TEL archaeology project, 341\u2013343\nER (Electronic Receptionist)\narchaeology project, 359\u2013361\nCraft Dispatch System was, 362\u2013364\nETS (Educational Testing Service), 370\u2013372\nEurope, redesigning SAC for US and, 347\u2013348\nEvent sourcing, storing transactions, 54\u201355\nExecutables\ndeployment of monoliths, 176\u2013178\nlinking components as, 96\nExternal agency, clean architecture independence from, 202\nExternal definition, compilers, 100\nExternal reference, compilers, 100\nF\nFacade pattern, partial boundaries, 220\nFan-in/fan-out metrics, component stability, 122\u2013123\nFeathers, Michael, 58\nFile systems, mitigating time delay, 279\u2013280\nFirewalls, boundary crossings via, 176\nFirmware\nin 4-TEL archaeology project, 343\u2013344\ndefinitions of, 256\u2013257\neliminating target-hardware bottleneck, 262\u2013263\nfuzzy line between software and, 263\u2013264\n", "page": 289, "type": "text", "section": "Page 289"}
{"text": "obsolete as hardware evolves, 256\nstop writing so much, 257\u2013258\nFitNesse program\noverview of, 163\u2013165\npartial boundary, 218\u2013219\nFLD (Field Labeled Data), Craft Dispatch System archaeology project, 363\nFlow of control\ncrossing circle boundaries, 206\ndependency management, case study, 302\ndynamic polymorphism, 177\u2013178\nin Union Accounting archaeology project, 334\nFowler, Martin, 305\u2013306\nFragile Tests Problem, 251\nFrameworks\navoid basing architecture on, 197\nclean architecture independent from, 202\ncreating testable architecture without, 198\nDependency Rule for, 205\nas option to be left open, 197\nas tools, not a way of life, 198\nFrameworks are details\nasymmetric marriage and, 292\u2013293\nconclusion, 295\nframework authors, 292\nframeworks you must simply marry, 295\npopularity of, 292\nrisks, 293\u2013294\nsolution, 294\nFunction calls, services as, 240\nFunctional decomposition\nprogramming best practice, 32\nin structured programming, 29\nFunctional pointers, OOP, 22, 23\nFunctional programming\nconclusion, 56\nevent sourcing, 54\u201355\nhistory of, 22\u201323\nimmutability, 52\noverview of, 50\nsegregation of mutability, 52\u201354\nsquares of integers example, 50\u201351\n", "page": 290, "type": "text", "section": "Page 290"}
{"text": "Functions\navoid overriding concrete, 89\nbreaking down into parts (functional decomposition), 29\none of three big concerns in architecture, 24\nprinciple of doing one thing, 62\nseparating from data, 66\nSRP examples, 67\nG\nGateways, database, 214\nGE Datanet 30 computer, Union Accounting archaeology project, 326\u2013330\nGoto statements\nDijkstra replaces with iteration control structures, 27\nDijkstra\u2019s proclamation on harmfulness of, 28\u201329\nhistory of structured programming, 22\nremoved in structured programming, 23\nGrowing Object Oriented Software with Tests (Freeman & Pryce), 202\nGUI (graphical user interface). See also UI (user interface)\ndecoupling business rules from, 287\u2013289\ndesigning for testability, 251\ndeveloping architects registry exam, 371\u2013372\ninput/output and boundary lines, 169\u2013170\nplugin architecture, 170\u2013171\nplugin argument, 172\u2013173\nseparating from business rules with boundaries, 165\u2013169\nunit testing, 212\nweb is, 288\nH\nHAL (hardware abstraction layer)\navoid revealing hardware details to user of, 265\u2013269\nas boundary line between software/firmware, 264\nDRY conditional compilation directives, 272\noperating system is detail and, 269\u2013271\nHardware\neliminating target-hardware bottleneck with layers, 262\u2013263\nfirmware becomes obsolete through evolution of, 256\nin SAC archaeology project, 346\u2013347\nHeader files, programming to interfaces with, 272\nHexagonal Architecture (Ports and Adapters), 202\nHigh-level policy\n", "page": 291, "type": "text", "section": "Page 291"}
{"text": "decoupling from lower level input/output policies, 185\u2013186\nseparating details from, 140\u2013142\nsplitting data streams, 227\u2013228\nwhere to place, 126\nHuman resources, goal of architect to minimize, 160\nHumble Object pattern\ndata mappers, 214\u2013215\ndatabase getaways, 214\nPresenters and Views, 212\u2013213\nPresenters as form of, 212\ntesting and architecture, 213\nunderstanding, 212\nHunt the Wumpus game\nlayers and boundaries. See Layers and boundaries\nMain component from, 232\u2013237\nI\nIBM System/7, aluminum die-cast archaeology project, 338\u2013339\nIf/then/else statements, 22, 27\nImmutability, 52\u201354\nImplementation strategy. See Code organization\nImportance, urgency vs. Eisenhower\u2019s matrix of, 16\u201317\nIncoming dependencies, stability metrics, 122\u2013123\nIndependence\nconclusion, 158\ndecoupling layers, 151\u2013152\ndecoupling mode, 153\ndecoupling use cases, 152\ndeployment, 150\ndevelopment, 149\u2013150\nduplication, 154\u2013155\nindependent deployability, 154\nindependent developability, 153\u2013154\nleaving options open, 150\u2013151\noperation, 149\noverview of, 147\u2013148\ntypes of decoupling modes, 155\u2013158\nuse cases, 148\nIndependent components\ncalculating stability metrics, 123\nunderstanding, 121\n", "page": 292, "type": "text", "section": "Page 292"}
{"text": "Independent deployability\nin 4-TEL archaeology project, 344\nas fallacy of services, 241\nkitty problem example, 242\u2013243\nin OO approach for cross-cutting concerns, 244\u2013245\noverview of, 154\nIndependent developability\nas fallacy of services, 241\nkitty problem example, 242\u2013243\nin OO approach for cross-cutting concerns, 244\u2013245\noverview of, 153\u2013154\nof UI and database, 47\nInduction, Dijkstra\u2019s proof related to iteration, 28\nInformation hiding, Open-Closed Principle, 74\u201375\nInheritance relationships\ncrossing circle boundaries, 206\ndefining OOP, 37\u201340\ndependency inversion, 46\ndependency management, 302\nguiding use of, 78\nInput/output\nbusiness rules for use cases, 193\u2013194\ndecoupling higher-level policy from lower level, 185\u2013187\npolicy level defined as distance from, 184\nseparating components with boundary lines, 169\u2013170\nIntegers, functional programming example, 50\u201351\nIntegration, weekly build issues, 112\u2013113\nInterface adapters, Dependency Rule for, 205\nInterface Segregation Principle. See ISP (Interface Segregation Principle)\nIO device\nUNIX functions, 41\u201344\nweb is, 288\u2013289\nIsolation, test, 250\u2013251\nISP (Interface Segregation Principle)\narchitecture and, 86\nCommon Reuse Principle compared with, 108\nconclusion, 86\ndefined, 59\nlanguage type and, 85\noverview of, 84\u201385\nIteration, 27\u201328\n", "page": 293, "type": "text", "section": "Page 293"}
{"text": "J\nJacobson, Ivar, 196, 202\nJar files\ncomponent architecture, 301\ncomponents as, 96\ncreating partial boundary, 219\ndefining function of components, 313\ndesigning component-based services, 245\u2013246\nDownload and Go rule for, 163\nin source-level decoupling mode, 176\nJava\nabstract components in, 125\ncode organization approaches in. See Code organization\ncomponents as jar files in, 96\nDIP and, 87\nimport statements for dependencies, 184\nISP example, 84\u201385\nmarrying standard library framework in, 293\nmodule frameworks in, 319\npackage by layer in, 304\u2013306\nsquares of integers example in, 50\u201351\nweakening encapsulation, 36\u201337\nJitters, breaking cycle of components, 118\nJunk mail example, 144\u2013145\nK\nKitty problem example, 242\u2013245\nL\nLanguages\nclean architecture and, 223\u2013226\nHunt the Wumpus adventure game, 222\u2013223\nLaser Trim, archaeology project\n4-TEL project, 339\noverview of, 334\u2013338\nLayered architecture\npackage by layer code organization, 304\u2013306\nrelaxed, 311\u2013312\nwhy it is considered bad, 310\u2013311\nLayers\napproach to code organization, 304\u2013306\n", "page": 294, "type": "text", "section": "Page 294"}
{"text": "clean architecture using, 202\u2013203\ndecoupling, 151\u2013152\nduplication of, 155\neliminating target-hardware bottleneck, 262\u2013263\nindependent developability, 154\nLayers and boundaries\nclean architecture, 223\u2013226\nconclusion, 228\ncrossing streams, 226\nHunt the Wumpus adventure game, 222\u2013223\noverview of, 221\u2013222\nsplitting streams, 227\u2013228\nLeiningen tool, module management, 104\nLevel\nhierarchy of protection and, 74\npolicy and, 184\u2013187\nLibraries\nlocation of source code, 97\u201398\nrelocatable binaries, 99\u2013100\nLife cycle, architecture supports system, 137\nLinkers, separating from loaders, 100\u2013102\nLiskov, Barbara, 78\nLiskov Substitution Principle (LSP). See LSP (Liskov Substitution Principle)\nLISP langauge, functional programming, 23\nLisp language, squares of integers example, 50\u201351\nLoaders\nlinking, 100\u2013102\nrelocatable binaries, 99\u2013100\nLocal process boundaries, 179\u2013180\nLSP (Liskov Substitution Principle)\narchitecture and, 80\nconclusion, 82\ndefined, 59\nguiding use of inheritance, 78\noverview of, 78\nsquare/rectangle example problem, 79\nviolation of, 80\u201382\nM\nM365 computer\n4-TEL archaeology project, 340\u2013341\n", "page": 295, "type": "text", "section": "Page 295"}
{"text": "Laser Trim archaeology project, 335\u2013338\nSAC archaeology project, 345\u2013347\nMailboxes, local processes communicate via, 180\nMain component\nconclusion, 237\nas concrete component, 91\ndefined, 232\nobject-oriented programming, 40\npolymorphism, 45\nsmall impact of releasing, 115\nas ultimate detail, 232\u2013237\nMain Sequence\navoiding Zones of Exclusion via, 130\ndefining relationship between abstraction/stability, 127\u2013128\nmeasuring distance from, 130\u2013132\nZone of Pain, 129\nZone of Uselessness, 129\u2013130\nMaintenance, impact of architecture on, 139\u2013140\nMarketing campaigns, database vendor, 283\nMaster Operating Program (MOP), Laser Trim archaeology project, 336\nMathematics\ncontrasting science with, 30\ndiscipline of proof, 27\u201328\nMaven tool, module management, 104\nMcCarthy, John, 23\nMemory\nearly layout of, 98\u201399\nlocal processes and, 179\nRAM. See RAM\nMerges, SRP examples, 65\nMessage queues, local processes communicate via, 180\nMetrics\nabstraction, 127\ndistance from Main Sequence, 130\u2013132\nMeyer, Bertrand, 70\nMicro-service architecture\nin Craft Dispatch System archaeology project, 362\u2013363\ndecoupling mode, 153\ndeployment strategy, 138\npopularity of, 239\nModems, SAC archaeology project, 346\u2013347\n", "page": 296, "type": "text", "section": "Page 296"}
{"text": "Modules\nCommon Reuse Principle, 107\u2013108\ndefined, 62\nmanagement tools, 104\npublic types vs. published types, 319\nReuse/Release Equivalence Principle, 105\nMonoliths\nbuilding scalable systems, 241\ndeployment-level components vs., 179\ndeployment of, 176\u2013178\nfunction calls, 240\nlocal processes as statically linked, 180\nthreads, 179\nMoore\u2019s Law, 101\nMOP (Master Operating Program), Laser Trim archaeology project, 336\nMorning after syndrome\neliminating dependency cycles to solve, 113\u2013115\nmanaging dependencies to prevent, 118\noverview of, 112\nweekly build issues, 112\u2013113\nMPS (multiprocessing system), SAC archaeology project, 345\u2013346\nMutability, 52\u201354\nMutable variables, 51, 54\u201355\nN\nNational Council of Architects Registry Board (NCARB), 370\u2013372\n.NET, components as DLLs, 96\nNetNews, presence of author on, 367\u2013369\nNewkirk, Jim, 371\u2013372\nNygaard, Kristen, 22\nO\nObject-oriented databases, ROSE product, 368\u2013370\nObject Oriented Design with Applications (Booch), 366, 368\nObject-oriented programming\nconclusion, 47\nfor cross-cutting concerns, 244\u2013245\ndependency inversion, 44\u201347\ndeployment of monoliths, 177\nencapsulation, 35\u201337\nhistory of, 22\n", "page": 297, "type": "text", "section": "Page 297"}
{"text": "inheritance, 37\u201340\noverview of, 34\u201335\npolymorphism, 40\u201343\npower of polymorphism, 43\u201344\nObject Oriented Software Engineering (Jacobson), 196, 202\nObject relational mappers (ORMs), 214\u2013215\nObjects, invented in 4-TEL archaeology project, 344\nOCP (Open-Closed Principle)\nbirth of, 142\nCommon Closure Principle compared with, 106\nconclusion, 75\nin Craft Dispatch System archaeology project, 363\ndefined, 59\ndependency management, 302\ndesigning component-based services, 246\ndirectional control, 74\ninformation hiding, 74\u201375\noverview of, 70\nthought experiment, 71\u201374\nOMC (Outboard Marine Corporation), aluminum die-cast archaeology project, 338\u2013339\nOne-dimensional boundaries, 219\nOpen-Closed Principle. See OCP (Open-Closed Principle)\nOperating system abstraction layer (OSAL), clean embedded architecture, 270\u2013271\nOperating system (OS), is detail, 269\u2013271\nOperations\narchitecture supports system, 138\u2013139, 149\ndecoupling use cases for, 153\nuse cases affected by changes in, 204\nOptions, keeping open\ngood architecture makes system easy to change, 150\u2013151\noperational architecture, 149\npurpose of architecture, 140\u2013142, 197\nvia decoupling mode, 153\nOrganization vs. encapsulation, 316\u2013319\nORMs (object relational mappers), 214\u2013215\nOS (operating system), is detail, 269\u2013271\nOSAL (operating system abstraction layer), clean embedded architecture, 270\u2013271\nOscillations, web as one of many, 285\u2013289\nOutgoing dependencies, stability metrics, 122\u2013123\nOverconfidence, foolishness of, 9\u201312\n", "page": 298, "type": "text", "section": "Page 298"}
{"text": "P\nPackage by component, 310\u2013315, 318\nPackage by feature, 306\u2013307, 317\nPackage by layer\naccess modifiers, 317\u2013318\nhorizontal layering of code, 304\u2013306\nwhy it is considered bad, 310\u2013311\nPackages, organization vs. encapsulation, 316\u2013319\nPage-Jones, Meilir, 29\nPartial boundaries\nconclusion, 220\nfacades, 220\none-dimensional boundaries, 219\nreasons to implement, 217\u2013218\nskip last step, 218\u2013219\nPatches, in 4-TEL archaeology project, 344\nPCCU, archaeology project, 352\u2013354\nPDP-11/60 computer, 349\u2013351\nPerformance, as low-level concern, 281\nP\u00e9riph\u00e9rique anti-pattern of ports and adapters, 320\u2013321\nPhysical addressing example, 145\u2013146\nPlugin architecture\nin 4-TEL archaeology project, 344\nfor device independence, 44\ndrawing boundaries for axis of change, 173\nof lower-level components into higher-level components, 187\nMain component as, 237\nstart with presumption of, 170\u2013171\nPointers\nin creating polymorphic behavior, 43\nfunctional, 22\u201323\nPolicy\nin clean architecture, 203\nhigh-level. See High-level policy\noverview of, 183\u2013184\nsoftware systems as statements of, 183\nsplitting data streams, 227\u2013228\nPolymorphic dispatch, 4-TEL archaeology project, 344\nPolymorphism\ncrossing circle boundaries with dynamic, 206\ndependency inversion, 44\u201347\n", "page": 299, "type": "text", "section": "Page 299"}
{"text": "flow of control in dynamic, 177\u2013178\nin object-oriented programming, 22, 40\u201343\npower of, 43\u201344\nPorts and adapters\naccess modifiers, 318\napproach to code organization, 308\u2013310\ndecouple dependencies with source code trees, 319\u2013320\nP\u00e9riph\u00e9rique anti-pattern of, 320\u2013321\nPositional stability, component, 122\u2013123\nPremature decisions, coupling to, 160\u2013163\n\u201cPresentation Domain Data Layering\u201d (Fowler), 305\u2013306\nPresenters\nin clean architecture, 203, 205\nclean architecture scenario, 207\u2013208\ncomponent architecture, 301\ncrossing circle boundaries, 206\nPresenters and humble objects\nconclusion, 215\ndata mappers, 214\u2013215\ndatabase getaways, 214\nHumble Object pattern, 212\noverview of, 211\u2013212\nPresenters and Views, 212\u2013213\nservice listeners, 215\ntesting and architecture, 213\nProcesses, partitioning into classes/separating classes, 71\u201372\nProcessor\nis detail, 265\u2013269\nmutability and, 52\nProduct, video sales case study, 298\nProductivity\ndecreasing, increasing cost of code, 5\u20137\nsignature of a mess, 8\u20139\nProgramming languages\nabstract components in, 125\u2013126\ncomponents, 96\ndynamically typed, 88\nISP and, 85\nstatically typed, 87\nvariables in functional languages, 51\nProgramming paradigms\n", "page": 300, "type": "text", "section": "Page 300"}
{"text": "functional programming. See Functional programming\nhistory of, 19\u201320\nobject-oriented programming. See Object-oriented programming\noverview of, 21\u201324\nstructured programming. See Structured programming\nProof\ndiscipline of, 27\u201328\nstructured programming lacking, 30\u201331\nProxies, using with frameworks, 293\nPublic types\nmisuse of, 315\u2013316\nvs. types that are published in modules, 319\nPython\nDIP and, 88\nISP and, 85\nR\nRace conditions\ndue to mutable variables, 52\nprotecting against concurrent updates and, 53\nRAM\n4-TEL archaeology project, 341, 343\u2013344\nreplacing disks, 280\u2013281\nRational (company), 367, 368\nRDBMS (relational database management systems), 279\u2013283\nReal-time operating system (RTOS) is detail, 269\u2013271\nRelational database management systems (RDBMS), 279\u2013283\nRelational databases, 278, 281\u2013283\nRelaxed layered architecture, 311\u2013312\nReleases\neffect of cycle in component dependency graph, 115\u2013117\neliminating dependency cycles, 113\u2013115\nnumbering new component, 113\nReuse/Release Equivalence Principle for new, 104\u2013105\nRemote terminals, DLU/DRU archaeology project, 354\u2013356\nREP (Reuse/Release Equivalence Principle), 104\u2013105, 108\u2013110\nRequest models, business rules, 193\u2013194\nReSharper, plugin argument, 172\u2013173\nResponse models, business rules, 193\u2013194\nREST, leave options open in development, 141\nReusability. See CRP (Common Reuse Principle)\n", "page": 301, "type": "text", "section": "Page 301"}
{"text": "Reuse/Release Equivalence Principle (REP), 104\u2013105, 108\u2013110\nRisks\narchitecture should mitigate costs of, 139\u2013140\nof frameworks, 293\u2013294\nROM boards, 4-TEL archaeology project, 341\nROSE product, archaeology project, 368\u2013370\nRTOS (real-time operating system) is detail, 269\u2013271\nRuby\ncomponents as gem files, 96\nDIP and, 88\nISP and, 85\nRVM tool, module management, 104\nS\nSAC (service area computer), archaeology project\n4-TEL using, 340\u2013341\narchitecture, 345\u2013347\nconclusion, 349\ndispatch determination, 345\nDLU/DRU archaeology project, 354\u2013356\nEurope, 348\u2013349\ngrand redesign, 347\u2013348\noverview of, 344\nSAP (Stable Abstractions Principle)\navoiding zones of exclusion, 130\ndistance from main sequence, 130\u2013132\ndrawing boundary lines, 173\nintroduction to, 126\u2013127\nmain sequence, 127\u2013130\nmeasuring abstraction, 127\nwhere to put high-level policy, 126\nSC (service center), 4-TEL archaeology project, 339\u2013340\nScalability\nkitty problem and services, 242\u2013243\nservices not only option for building, 241\nSchmidt, Doug, 256\u2013258\nScientific methods, proving statements false, 30\u201331\nScope, of changing architecture, 15\nScreaming architecture. See Architecture, screaming\nSDP (Stable Dependencies Principle)\nabstract components, 125\u2013126\n", "page": 302, "type": "text", "section": "Page 302"}
{"text": "not all components should be, 124\u2013125\nnot all components should be stable, 123\u2013125\noverview of, 120\nstability, 120\u2013121\nstability metrics, 122\u2013123\nStable Abstractions Principle, 127\nSecurity, testing API, 253\nSelection, as program control structure, 27\u201328\nSeparation of components, as big concern in architecture, 24\nSequence, as program control structure, 27\u201328\nSerial communication bus, SAC archaeology project, 347\nService area computer. See SAC (service area computer), archaeology project\nService center (SC), 4-TEL archaeology project, 339\u2013340\nService-level decoupling mode, 153, 156\u2013157\nServices\ncomponent-based, 245\u2013246\nconclusion, 247\ncross-cutting concerns, 246\u2013247\ndecoupling fallacy, 240\u2013241\nas function calls vs. architecture, 240\nHumble Object boundaries for, 214\u2013215\nindependent development/deployment fallacy, 241\nkitty problem, 242\u2013243\nobjects to the rescue, 244\u2013245\noverview of, 239\nas strongest boundary, 180\u2013181\nSet program interrupt (SPI) instruction, aluminum die-cast archaeology project, 339\nShape, of change, 15\nSingle Responsibility Principle. See SRP (Single Responsibility Principle)\nSOA (service-oriented architecture)\ndecoupling mode, 153\nin Electronic Receptionist archaeology project, 360\u2013361\nreasons for popularity of, 239\nSockets, local processes communicate via, 180\nSoftware\nclean embedded architecture isolates OS from, 270\ncomponents. See Components\neliminating target-hardware bottleneck with layers, 262\u2013263\nfuzzy line between firmware and, 263\u2013264\ngetting it right, 1\u20132\nSOLID principles, 58\n", "page": 303, "type": "text", "section": "Page 303"}
{"text": "value of architecture vs. behavior, 14\u201318\nSoftware development\nfighting for architecture over function, 18\nlike a science, 31\nSoftware reuse\nCommon Reuse Principle, 107\u2013108\nreusable components and, 104\nReuse/Release Equivalence Principle, 104\u2013105\nSOLID principles\nDependency Inversion Principle. See DIP (Dependency Inversion Principle)\ndesigning component-based services using, 245\u2013246\nhistory of, 57\u201359\nInterface Segregation Principle. See ISP (Interface Segregation Principle)\nLiskov Substitution Principle. See LSP (Liskov Substitution Principle)\nOO approach for cross-cutting concerns, 244\u2013245\nOpen-Closed Principle. See OCP (Open-Closed Principle)\nSingle Responsibility Principle. See SRP (Single Responsibility Principle)\nSource code, compiling, 97\u201398\nSource code dependencies\ncreating boundary crossing via, 176\ncrossing circle boundaries, 206\ndecoupling, 184\u2013185, 319\ndependency inversion, 44\u201347\nlocal processes as, 180\nOCP example, 72\nreferring only to abstractions, 87\u201388\nUI components reuse game rules via, 222\u2013223\nSource code trees, decoupling dependencies, 319\u2013321\nSource-level decoupling mode, 155\u2013157, 176\u2013178\nSpelunking, architecture mitigates costs of, 139\u2013140\nSPI (set program interrupt) instruction, aluminum die-cast archaeology project, 339\nSplitting data streams, 227\u2013228\nSquare/rectangle problem, LSP, 79\nSquares of integers, functional programming, 50\u201351\nSRP (Single Responsibility Principle)\naccidental duplication example, 63\u201365\nCommon Closure Principle vs., 106\u2013107\nconclusion, 66\u201367\ndecoupling layers, 152\ndefined, 59\ndependency management, 302\n", "page": 304, "type": "text", "section": "Page 304"}
{"text": "in good software architecture, 71\ngrouping policies into components, 186\u2013187\nkeeping changes localized, 118\nmerges, 65\noverview of, 61\u201363\nsolutions, 66\u201367\nuse case analysis, 299\nwhere to draw boundaries, 172\u2013173\nStability, component\nmeasuring, 122\u2013123\nrelationship between abstraction and, 127\u2013130\nSAP. See SAP (Stable Abstractions Principle)\nunderstanding, 120\u2013121\nStable Abstractions Principle. See SAP (Stable Abstractions Principle)\nStable components\nabstract components as, 125\u2013126\nas harmless in Zone of Pain, 129\nnot all components should be, 123\u2013125\nplacing high-level policies in, 126\nStable Abstractions Principle, 126\u2013127\nStable Dependencies Principle. See SDP (Stable Dependencies Principle)\nStakeholders\nscope vs. shape for cost of change, 15\nseniority of architecture over function, 18\nvalues provided by software systems, 14\nState\nconcurrency issues from mutation, 53\nstoring transactions but not, 54\u201355\nStatic analysis tools, architecture violations, 313\nStatic vs. dynamic polymorphism, 177\nStrategy pattern\ncreating one-dimensional boundaries, 219\nOO approach for cross-cutting concerns, 244\u2013245\nStreams, data\nclean architecture and, 224\u2013226\ncrossing, 226\nsplitting, 227\u2013228\nStructural coupling, testing API, 252\nStructure. See Architecture\nStructured programming\nDijkstra\u2019s proclamation on goto statements, 28\u201329\n", "page": 305, "type": "text", "section": "Page 305"}
{"text": "discipline of proof, 27\u201328\nfunctional decomposition in, 29\nhistory of, 22\nlack of formal proofs, 30\noverview of, 26\nrole of science in, 30\u201331\nrole of tests in, 31\nvalue of, 31\u201332\nSubstitution\nLSP. See LSP (Liskov Substitution Principle)\nprogramming to interfaces and, 271\u2013272\nSubtypes, defining, 78\nT\nTarget-hardware bottleneck, 261, 262\u2013272\nTAS (Teradyne Applied Systems), 334\u2013338, 339\u2013344\nTemplate Method pattern, OO approach for cross-cutting concerns, 244\u2013245\nTest boundary\nconclusion, 253\ndesigning for testability, 251\nFragile Tests Problem, 251\noverview of, 249\u2013250\ntesting API, 252\u2013253\ntests as system components, 250\nTestable architecture\nclean architecture creating, 202\nclean embedded architecture as, 262\u2013272\noverview of, 198\nTesting\nand architecture, 213\nPresenters and Views, 212\u2013213\nin structured programming, 31\nunit. See Unit testing\nvia Humble Object pattern, 212\nThreads\nmutability and, 52\nschedule/order of execution, 179\nThree-tiered \u201carchitecture\u201d (as topology), 161\nTop-down design, component structure, 118\u2013119\nTransactional memory, 53\nTransactions, storing, 54\u201355\n", "page": 306, "type": "text", "section": "Page 306"}
{"text": "Transitive dependencies, violating software principles, 75\nTrouble tickets, CDS archaeology project, 362\u2013364\nTrue duplication, 154\u2013155\nTurning, Alan, 23\nU\nUI (user interface). See also GUI (graphical user interface)\napplying LSP to, 80\nclean architecture independent from, 202\ncrossing circle boundaries, 206\ndecoupling business rules from, 287\u2013289\ndecoupling layers, 152\u2013153\ndecoupling use cases, 153\nHunt the Wumpus adventure game, 222\u2013223\nindependent developability, 47, 154\nInterface Segregation Principle, 84\nprogramming to, 271\u2013272\nreducing volatility of, 88\nSAC archaeology project, 346\nUML class diagram\npackage by layer, 304\u2013305, 310\nports and adapters, 308\u2013310\nrelaxed layered architecture, 311\u2013312\nUncle Bob, 367, 369\nUNIFY database system, VRS archaeology project, 358\u2013359\nUnion Accounting system, archaeology project, 326\u2013334\nUnit testing\ncreating testable architecture, 198\neffect of cycle in component dependency graph, 116\u2013117\nvia Humble Object pattern, 212\nUNIX, IO device driver functions, 41\u201344\nUpgrades, risks of frameworks, 293\nUrgency, Eisenhower\u2019s matrix of importance vs., 16\u201317\nUse cases\narchitecture must support, 148\nbusiness rules for, 191\u2013194\nclean architecture scenario, 207\u2013208\ncoupling to premature decisions with, 160\ncreating testable architecture, 198\ncrossing circle boundaries, 206\ndecoupling, 152\n", "page": 307, "type": "text", "section": "Page 307"}
{"text": "decoupling mode, 153\nDependency Rule for, 204\nduplication of, 155\ngood architecture centered on, 196, 197\nindependent developability and, 154\nvideo sales case study, 298\u2013300\nUser interface\nGUI. See GUI (graphical user interface)\nUI. See UI (user interface)\nUtility library, Zone of Pain, 129\nUucp connection, 366\nV\nV\nalues, software system\narchitecture (structure), 14\u201315\nbehavior, 14\nEisenhower\u2019s matrix of importance vs. urgency, 16\u201317\nfighting for seniority of architecture, 18\nfunction vs. architecture, 15\u201316\noverview of, 14\nV\nariables, functional language, 51\nV\narian 620/f minicomputer, Union Accounting archaeology project, 331\u2013334\nVideo sales case study\ncomponent architecture, 300\u2013302\nconclusion, 302\ndependency management, 302\non process/decisions of good architect, 297\u2013298\nproduct, 298\nuse case analysis, 298\u2013300\nView Model, Presenters and Views, 213\nViews\ncomponent architecture, 301\nPresenters and, 212\u2013213\nVignette Grande, architects registry exam, 371\u2013372\nVisual Studio, plugin argument, 172\u2013173\nV\noice technologies, archaeology projects\nElectronic Receptionist, 359\u2013361\nV\noice Response System, 357\u2013359\nV\nolatile components\ndependency graph and, 118\ndesign for testability, 251\n", "page": 308, "type": "text", "section": "Page 308"}
{"text": "placing in volatile software, 126\nas problematic in Zone of Pain, 129\nStable Dependencies Principle and, 120\nV\non Neumann, 34\nVRS (V\noice Response System), archaeology projects, 357\u2013359, 362\u2013363\nW\nWeb\nas delivery system for your application, 197\u2013198\nDependency Rule for, 205\nis detail, 285\u2013289\nWeb servers\ncreating testable architecture without, 198\nas option to be left open, 141, 197\nwriting own, 163\u2013165\nWeekly build, 112\u2013113\nWiki text, architectural success story, 164\nY\nYourdon, Ed, 29\nZ\nZones of exclusion\navoiding, 130\nrelationship between abstraction/stability, 128\nZone of Pain, 129\nZone of Uselessness, 129\u2013130\n", "page": 309, "type": "text", "section": "Page 309"}
{"text": "Code Snippets\n", "page": 311, "type": "text", "section": "Page 311"}
{"text": "Code Snippets\n", "page": 323, "type": "text", "section": "Page 323"}
{"text": "Code Snippets\n", "page": 328, "type": "text", "section": "Page 328"}
{"text": "Code Snippets\n", "page": 334, "type": "text", "section": "Page 334"}
{"text": "Code Snippets\n", "page": 337, "type": "text", "section": "Page 337"}
{"text": "Code Snippets\n", "page": 339, "type": "text", "section": "Page 339"}
{"text": "Code Snippets\n", "page": 341, "type": "text", "section": "Page 341"}
{"text": "Code Snippets\n", "page": 351, "type": "text", "section": "Page 351"}
{"text": "Code Snippets\n", "page": 363, "type": "text", "section": "Page 363"}
