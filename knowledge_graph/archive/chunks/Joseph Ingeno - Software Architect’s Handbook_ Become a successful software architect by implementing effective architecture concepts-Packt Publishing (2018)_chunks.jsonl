{"text": "About the reviewers: Gaurav Aroraa has done an MPhil in computer science. He is a Microsoft MVP, a lifetime\nmember of Computer Society of India (CSI),  an advisory member of IndiaMentor,\ncertified as a Scrum trainer/coach, XEN for ITIL-F, and APMG for PRINCE-F and PRINCE-\nP. He is an open source developer, a contributor to TechNet Wiki, and the founder of\nOvatic Systems Private Limited. In his career of over 20 years, he has mentored thousands\nof students and industry professionals. Apart from that, he's written over 100 white papers\nfor research scholars and various universities across the globe. I'd like to thank my wife, Shuby Arora, and my angel daughter, Aarchi Arora, as well as\nthe team at PACKT. Anand B Pillai is a technophile by profession with 20 years' of experience in software\ndevelopment, design, and architecture. Over the years, he has worked with numerous\ncompanies in fields ranging from security, search engines, large-scale web portals and big\ndata. He is a founder of the Bangalore Python Users' Group and is the author of Software\nArchitecture with Python (PacktPub, April 2017). Anand is currently a VP of an engineering\nat the early-stage legal technology startup, Klarity Law. He happily resides with his family\nin Bangalore, India. Packt is searching for authors like you\nIf you're interested in becoming an author for Packt, please visit authors.packtpub.com\nand apply today. We have worked with thousands of developers and tech professionals,\njust like you, to help them share their insight with the global tech community. You can\nmake a general application, apply for a specific hot topic that we are recruiting an author\nfor, or submit your own idea.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 7", "position": 7, "chunk_type": "semantic", "token_estimate": 278}
{"text": "Table of Contents: Preface\n1\nChapter 1: The Meaning of Software Architecture\n8\nWhat is software architecture? 8\nISO/IEC/IEEE 42010 standard definition\n9\nWhat makes up a software architecture? 10\nSoftware architecture is an abstraction\n10\nSoftware architecture is about the important stuff\n10\nWhy is software architecture important? 11\nDefining a solution to meet requirements\n11\nEnabling and inhibiting quality attributes \n12\nGiving you the ability to predict software system qualities\n12\nEasing communication among stakeholders\n13\nManaging change\n13\nProviding a reusable model\n14\nImposing implementation constraints\n14\nImproving cost and effort estimates\n14\nServes as training for team members\n15\nSoftware architecture is not a silver bullet\n16\nWho are the consumers of software architectures? 16\nWhat is the software architect role? 17\nSoftware architects are technical leaders\n17\nSoftware architects perform a number of duties\n18\nIvory tower software architects\n18\nWhat are software architects expected to know? 19\nDon't be overwhelmed\n20\nIs the software architect role right for you? 21\nSummary\n22\nChapter 2: Software Architecture in an Organization\n23\nTypes of software architects\n24\nEnterprise architect\n24\nSolution architect\n25\nApplication architect\n25\nData architect/information architect\n26\nInfrastructure architect\n27\nInformation security architect\n28\nCloud architect\n28\nSoftware development methodologies\n29\nThe Waterfall model\n30", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 8", "position": 8, "chunk_type": "semantic", "token_estimate": 212}
{"text": "Table of Contents: [ ii ]\nPhases of the Waterfall methodology\n31\nIssues with the Waterfall methodology\n32\nAgile software development methodologies\n32\nAgile values and principles\n33\nAn iterative methodology\n34\nAdaptive rather than predictive\n34\nDaily stand-up meetings\n35\nProject management\n36\nThe importance of software project estimation\n36\nPutting effort into the estimates\n36\nBeing a realist (or even a pessimist)\n37\nTeam and situational factors to consider\n37\nProject schedule changes\n37\nGetting a project back on schedule\n38\nWorking overtime\n38\nReducing scope\n38\nAdding resources\n39\nReallocating resources\n39\nIdentifying problem areas\n40\nActing as early as possible\n40\nOffice politics\n40\nUnderstanding your organization's goals\n42\nAddressing the concerns of others\n42\nAssisting people with their goals\n42\nKnowing when to compromise\n43\nBeing aware of cultural differences\n43\nSoftware risk management\n43\nRisk avoidance\n45\nTransferring the risk to another party\n45\nRisk mitigation\n46\nRisk acceptance\n46\nConfiguration management\n46\nChanging management\n47\nSoftware product lines\n49\nBenefits of a software product line\n50\nCore assets of an organization\n50\nRisks of product line engineering\n51\nSummary\n51\nChapter 3: Understanding the Domain\n53\nDeveloping business acumen\n53\nFamiliarity with general business topics\n54\nUnderstanding your organization's business\n55\nDomain-driven design\n56\nEncourages and improves communication\n56\nWhat is a ubiquitous language? 57", "domains": ["Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 9", "position": 9, "chunk_type": "semantic", "token_estimate": 222}
{"text": "Table of Contents: [ iii ]\nEntities, value objects, and aggregates\n59\nEntities\n59\nValue objects\n59\nAggregates and root entities\n60\nSeparating the domain into subdomains\n60\nWhat are bounded contexts? 61\nRequirements engineering\n63\nTypes of software requirements\n63\nBusiness requirements\n64\nFunctional requirements\n64\nNon-functional requirements\n65\nConstraints\n65\nThe importance of requirements engineering\n66\nSoftware requirements must be measurable and testable\n67\nSoftware requirements that affect architecture\n67\nRequirements elicitation\n68\nTechniques to elicit requirements\n69\nInterviews\n69\nRequirements workshops\n69\nBrainstorming\n70\nObservation\n71\nFocus groups\n72\nSurveys\n72\nDocument analysis\n73\nPrototyping\n73\nReverse engineering\n74\nGet access to the proper stakeholders\n74\nSummary\n75\nChapter 4: Software Quality Attributes\n76\nQuality attributes\n76\nExternal or internal\n77\nQuality attributes and the SDLC\n78\nTesting quality attributes\n78\nMaintainability\n79\nTypes of software maintenance\n80\nCorrective maintenance\n80\nPerfective maintenance\n80\nAdaptive maintenance\n81\nPreventive maintenance\n81\nModifiability\n81\nExtensibility and flexibility\n82\nScope of modifications\n82\nDesigning for maintainability\n82\nReducing size\n83\nIncreasing cohesion\n83\nReducing coupling\n83\nMeasuring maintainability\n83", "domains": ["Software Quality Attributes", "Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 10", "position": 10, "chunk_type": "semantic", "token_estimate": 179}
{"text": "Table of Contents: [ iv ]\nLines of code (LOC)\n84\nCyclomatic complexity\n84\nDepth of inheritance tree (DIT)\n86\nUsability\n87\nAllowing users to complete their tasks efficiently\n87\nLearnability\n87\nProviding useful feedback\n88\nAccessibility   \n88\nUsability needs to be considered during requirements\n89\nUsability testing\n89\nAppealing visual design\n89\nProviding a good help system\n91\nSoftware must be useful, and not just usable\n91\nAvailability\n91\nCalculating availability based on time\n92\nCalculating availability based on request success rate\n93\nFaults, errors, and failures\n93\nDetecting faults\n94\nPing/echo reply\n94\nHeartbeat\n94\nTimestamp\n94\nVoting\n94\nSanity test/sanity checking\n95\nCondition monitoring\n95\nSelf-tests\n95\nRecovering from faults\n95\nException handling\n95\nRetry strategy\n96\nVarying levels of redundancy\n96\nRollback\n97\nGraceful degradation\n97\nIgnoring faulty behavior\n97\nPreventing faults\n98\nRemoval from service\n98\nTransactions\n98\nIncreasing competence sets\n98\nException prevention\n98\nPortability\n99\nAdaptability\n99\nInstallability\n99\nReplaceability\n100\nInternationalization and localization\n100\nMaintaining portability\n102\nInteroperability\n102\nChallenges with interoperability\n102\nLocating and exchanging information with another system\n103\nInteroperability standards\n104", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 11", "position": 11, "chunk_type": "semantic", "token_estimate": 182}
{"text": "Table of Contents: [ v ]\nInteroperability testing\n104\nTestability\n105\nControllability\n105\nObservability\n106\nIsolability\n106\nAutomatability\n106\nComplexity of the software\n107\nImportance of test documentation\n108\nWhat makes a good tester? 109\nSummary\n110\nChapter 5: Designing Software Architectures\n111\nSoftware architecture design\n112\nMaking design decisions\n112\nSoftware architecture design terms\n113\nStructure\n113\nElement\n114\nSystem\n114\nSubsystem\n114\nModule\n114\nComponent\n115\nThe importance of software architecture design\n115\nMaking key decisions\n115\nAvoiding design decisions can incur technical debt\n116\nCommunicating the architecture to others\n116\nProviding guidance to developers\n117\nInfluencing non-technical parts of the project\n117\nTop-down versus bottom-up design approaches\n117\nTop-down approach\n117\nAdvantages of the top-down approach\n118\nDisadvantages of the top-down approach\n118\nBottom-up approach\n119\nAdvantages of the bottom-up approach\n120\nDisadvantages of the bottom-up approach\n120\nWhich approach should I use? 121\nGreenfield versus brownfield software systems\n123\nGreenfield systems\n123\nBrownfield systems\n124\nArchitectural drivers\n124\nDesign objectives\n125\nPrimary functional requirements\n126\nQuality attribute scenarios\n126\nPrioritizing quality attribute scenarios\n127\nConstraints\n127\nArchitectural concerns\n128\nLeveraging design principles and existing solutions\n129", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 12", "position": 12, "chunk_type": "semantic", "token_estimate": 190}
{"text": "Table of Contents: [ viii ]\nControl coupling\n170\nStamp coupling (data-structured coupling)\n170\nData coupling\n170\nMessage coupling\n171\nNo coupling\n171\nThe Law of Demeter (LoD) / principle of least knowledge\n171\nDesigning for loose coupling\n171\nHigh cohesion\n172\nTypes of cohesion\n172\nCoincidental cohesion\n172\nLogical cohesion\n173\nTemporal cohesion\n173\nProcedural cohesion\n173\nCommunicational cohesion\n174\nSequential cohesion\n174\nFunctional cohesion\n174\nDesigning for high cohesion\n175\nMinimizing complexity\n175\nKISS principle \u2013 \"Keep It Simple, Stupid\"\n176\nOrigin of KISS\n176\nApplying KISS to software\n177\nDon't make it overly simple\n177\nDRY \u2013 \"Don't Repeat Yourself\"\n178\nCopy-and-paste programming\n178\nMagic strings\n178\nHow to avoid duplication\n180\nDon't make things overly DRY\n180\nInformation hiding\n180\nReasons for information hiding\n181\nWhat needs to be exposed/hidden? 181\nYAGNI \u2013 \"You Aren't Gonna Need It\"\n182\nAvoid over-engineering a solution\n182\nSituations where YAGNI doesn't apply\n182\nSeparation of Concerns (SoC)\n183\nFollowing SOLID design principles\n184\nSingle Responsibility Principle (SRP)\n184\nOpen/Closed Principle (OCP)\n187\nLiskov Substitution Principle (LSP)\n189\nInterface Segregation Principle (ISP)\n191\nDependency Inversion Principle (DIP)\n194\nInversion of Control (IoC)\n195\nDependency Injection (DI)\n196\nBenefits of DI\n196\nDI patterns\n197\nDI containers\n200\nHelping your team succeed\n201\nUnit testing\n201\nWhat are unit tests? 201\nBenefits of unit testing\n202\nProperties of a good unit test\n202", "domains": ["Design Principles", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 15", "position": 15, "chunk_type": "semantic", "token_estimate": 232}
{"text": "Table of Contents: [ ix ]\nAtomic\n202\nDeterministic\n203\nAutomated and repeatable\n203\nIsolated and independent\n203\nEasy to set up and implement\n203\nFast\n203\nThe AAA pattern\n204\nArrange\n204\nAct\n204\nAssert\n204\nNaming conventions for unit tests\n204\nUnit test class names\n205\nUnit test method names\n205\nCode coverage for unit tests\n205\nKeeping unit tests up to date\n206\nSetting up development environments\n206\nProviding a README file\n207\nPair programming\n208\nBenefits of pair programming\n208\nUsing pair programming when it is needed\n209\nReviewing deliverables\n209\nCode reviews\n209\nFormal inspections\n210\nRoles for a formal inspection\n211\nInspection meeting and follow-up\n211\nWalkthroughs\n212\nSummary\n212\nChapter 7: Software Architecture Patterns\n213\nSoftware architecture patterns\n214\nUsing software architecture patterns\n214\nOverusing architecture patterns\n215\nUnderstanding the difference between architecture styles and architecture\npatterns\n215\nLayered architecture\n216\nOpen versus closed layers\n216\nTiers versus layers\n217\nAdvantages of layered architectures\n218\nDisadvantages of layered architectures\n219\nClient-server architecture (two-tier architecture)\n220\nUsing stored procedures for application logic\n221\nN-tier architecture\n222\nPresentation tier\n223\nBusiness tier\n223\nData tier\n224\nEvent-driven architecture\n224\nEvent channels\n224\nMessage queues\n225\nThe point-to-point channel pattern\n225\nMessage topics\n225", "domains": ["Architectural Patterns and Styles"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 16", "position": 16, "chunk_type": "semantic", "token_estimate": 208}
{"text": "Table of Contents: [ x ]\nThe publish-subscribe pattern\n225\nEvent-driven architecture topologies\n226\nThe mediator topology\n226\nEvent mediator implementations\n227\nThe broker topology\n227\nEvent processing styles\n228\nSimple event processing (SEP)\n229\nEvent stream processing (ESP)\n229\nComplex event processing (CEP)\n229\nTypes of event-driven functionality\n230\nEvent notification\n230\nEvent-carried state transfer\n230\nEvent-sourcing\n231\nThe Model-View-Controller pattern\n232\nModel\n233\nView\n233\nController\n234\nAdvantages of the MVC pattern\n234\nDisadvantages of the MVC pattern\n234\nThe Model-View-Presenter pattern\n235\nModel\n235\nView\n235\nPresenter\n236\nThe Model-View-ViewModel pattern\n236\nModel\n237\nView\n237\nViewModel\n237\nThe Command Query Responsibility Segregation pattern\n238\nThe query model and the command model\n239\nUsing event-sourcing with CQRS\n240\nAdvantages of CQRS\n240\nDisadvantages of CQRS\n241\nService-oriented architecture\n241\nWhat makes SOA different from other distributed solutions? 242\nBenefits of using a SOA\n242\nIncreases alignment between business and technology\n243\nPromotes federation within an organization\n243\nAllows for vendor diversity\n243\nIncreases intrinsic interoperability\n244\nWorks well with agile development methodologies\n244\nCost-benefit analysis of SOA\n244\nChallenges with SOA\n245\nKey principles for service orientation\n246\nStandardized service contract\n246\nService loose coupling\n246\nService abstraction\n247\nService reusability\n247", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 17", "position": 17, "chunk_type": "semantic", "token_estimate": 205}
{"text": "Table of Contents: [ xi ]\nService autonomy\n247\nService statelessness\n248\nService discoverability\n248\nService composability\n248\nSOA delivery strategies\n248\nThe top-down strategy\n249\nThe bottom-up strategy\n249\nThe agile strategy\n250\nService-oriented analysis\n250\nDefining business automation requirements\n250\nIdentifying existing automation systems\n251\nModeling candidate services\n251\nService layers and service models\n251\nTask service\n253\nEntity service\n253\nUtility service\n253\nService-oriented design\n253\nService interface design\n254\nService interface granularity\n255\nService registries\n255\nService descriptions\n256\nStructuring namespaces\n257\nOrchestration and choreography\n258\nSummary\n259\nChapter 8: Architecting Modern Applications\n260\nMonolithic architecture\n260\nBenefits of a monolithic architecture\n261\nDrawbacks of a monolithic architecture\n262\nMicroservice architecture\n263\nSOA done right\n263\nCharacteristics of microservice architecture\n264\nSmall, focused services\n264\nWell-defined service interfaces\n265\nAutonomous and independently deployable services\n265\nIndependent data storage\n265\nBetter fault isolation\n266\nCommunicating with lightweight message protocols\n266\nDesigning polyglot microservices\n267\nPolyglot programming\n268\nPolyglot persistence\n268\nUsing too many technologies\n269\nConsidering service granularity\n269\nNanoservices\n269\nSharing dependencies between microservices\n270\nStateless versus stateful microservices\n271\nService discovery\n271\nUsing a service registry\n271\nSelf-registration pattern\n272", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Domain-Driven Design", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 18", "position": 18, "chunk_type": "semantic", "token_estimate": 195}
{"text": "Table of Contents: [ xii ]\nThird-party registration pattern\n272\nTypes of service discovery\n273\nClient-side discovery pattern\n274\nServer-side discovery pattern\n274\nUsing microservices is not for everyone\n275\nServerless architecture\n276\nFunction as a Service (FaaS)\n277\nBackend as a Service (BaaS)\n278\nAdvantages of serverless architectures\n279\nCost savings\n279\nScalable and flexible\n279\nFocus on building your core products\n279\nPolyglot development\n280\nDisadvantages of serverless architectures\n280\nDifficulties with debugging and monitoring\n280\nMultitenancy issues\n280\nVendor lock-in\n281\nComplexity of designing many functions\n281\nNot as many runtime optimizations\n281\nStill immature\n281\nTaking a hybrid approach to serverless\n282\nFunction deployment\n282\nFunction invocation\n283\nSynchronous request\n283\nAsynchronous request (message queue)\n284\nMessage stream\n284\nBatch job\n285\nCloud-native applications\n285\nReasons to move to the cloud\n286\nReducing costs\n286\nGreater flexibility and scalability\n286\nAutomatic updates\n286\nDisaster recovery\n287\nWhat are cloud-native applications? 287\nContainerized\n287\nDynamically orchestrated\n288\nMicroservices-oriented\n289\nNo downtime\n289\nContinuous delivery\n289\nSupport for a variety of devices\n290\nTwelve-factor apps\n290\nCodebase\n291\nDependencies\n291\nConfiguration\n292\nBacking services\n292\nBuild/Release/Run\n292\nProcesses\n293\nPort binding\n293\nConcurrency\n294", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 19", "position": 19, "chunk_type": "semantic", "token_estimate": 196}
{"text": "Table of Contents: [ xiii ]\nDisposability\n294\nDevelopment/production parity\n294\nLogs\n295\nAdministrative processes\n295\nSummary\n296\nChapter 9: Cross-Cutting Concerns\n297\nCross-cutting concerns\n298\nGeneral guidelines for cross-cutting concerns\n299\nIdentifying cross-cutting concerns\n299\nUsing open-source and third-party solutions\n299\nMaintaining consistency\n299\nAvoiding scattered solutions\n300\nAvoiding tangled solutions\n300\nImplementing cross-cutting concerns\n301\nUsing dependency injection (DI)\n301\nUsing the decorator pattern\n302\nAspect-oriented programming\n306\nTypes of advice\n307\nWeaving\n308\nCompile-time weaving\n309\nRuntime weaving\n309\nTypes of cross-cutting concerns\n310\nCaching\n310\nConfiguration management\n311\nAuditing\n312\nSecurity\n312\nException management\n312\nLogging\n313\nUnderstanding log levels\n313\nRouting log entries\n314\nUsing Elastic Stack\n315\nElasticsearch\n315\nLogstash\n316\nKibana\n316\nBeats\n317\nCross-cutting concerns for microservices\n317\nLeveraging a microservice chassis\n317\nUsing the sidecar pattern\n318\nSummary\n320\nChapter 10: Performance Considerations\n321\nThe importance of performance\n321\nPerformance affects user experience\n322\nBounce rate\n322\nConversion rate\n322\nPerformance is a requirement\n323", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 20", "position": 20, "chunk_type": "semantic", "token_estimate": 164}
{"text": "Table of Contents: [ xiv ]\nPage speed affects search rankings\n323\nDefining performance terminology\n323\nLatency\n324\nThroughput\n324\nBandwidth\n324\nProcessing time\n325\nResponse time\n325\nWorkload\n325\nUtilization\n325\nTaking a systematic approach to performance improvement\n326\nProfiling an application\n326\nInstrumentation\n327\nStatistical profilers\n327\nAnalyzing the results\n328\nImplementing changes\n328\nMonitoring results\n329\nServer-side caching\n329\nCaching data in distributed applications\n330\nUsing a private caching strategy\n330\nUsing a shared caching strategy\n330\nPriming the cache\n331\nInvalidating cached data\n331\nExpiring data\n331\nEvicting data\n331\nCache usage patterns\n332\nCache-aside pattern\n332\nRead-through pattern\n332\nWrite-through pattern\n333\nWrite-behind pattern\n333\nImproving web application performance\n333\nLeveraging HTTP caching\n333\nUsing a validation token\n334\nSpecifying cache-control directives\n334\nTaking advantage of compression\n335\nFile compression\n335\nLossless compression\n336\nLossy compression\n336\nContent-encoding (end-to-end) compression\n337\nMinifying resources\n337\nBundling resources\n338\n Using HTTP/2\n341\nMultiplexing\n342\nServer push\n342\nHeader compression\n343\nImplementing HTTP/2\n343\nUsing content delivery networks (CDNs)\n344\nOptimizing web fonts\n344", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 21", "position": 21, "chunk_type": "semantic", "token_estimate": 176}
{"text": "Table of Contents: [ xv ]\nOptimizing the critical rendering path\n346\nDatabase performance\n347\nDesigning an efficient database schema\n347\nNormalizing a database\n347\nDenormalizing a database\n348\nIdentifying primary and foreign keys\n348\nSelecting the most appropriate data types\n349\nUsing database indexes\n349\nPrimary/clustered indexes\n349\nSecondary/non-clustered indexes\n350\nHaving too many indexes\n350\nScaling up and out\n351\nDatabase concurrency\n352\nDatabase transactions\n352\nOptimistic versus pessimistic concurrency control\n352\nCAP theorem\n353\nACID model\n354\nAtomicity\n354\nConsistency\n354\nIsolation\n354\nDurability\n355\nBASE model\n355\nBasic availability\n355\nSoft state\n355\nEventual consistency\n355\nSummary\n356\nChapter 11: Security Considerations\n357\nSecuring software systems\n358\nThe three states of information\n358\nThe CIA triad\n359\nConfidentiality\n359\nIntegrity\n359\nAvailability\n360\nThreat modeling\n360\nDecomposing an application\n361\nIdentifying and categorizing potential threats\n361\nSTRIDE threat model\n362\nSpoofing identity\n362\nTampering with data\n362\nRepudiation\n362\nInformation disclosure\n363\nDenial-of-service\n363\nElevation of Privilege\n363\nPrioritizing potential threats\n363\nDREAD risk assessment model\n364\nDamage potential\n364\nReproducibility\n364\nExploitability\n365\nAffected users\n365", "domains": ["Design Patterns", "Software Quality Attributes", "Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 22", "position": 22, "chunk_type": "semantic", "token_estimate": 181}
{"text": "Table of Contents: [ xvi ]\nDiscoverability\n365\nResponses to threats\n365\nAvoiding the risk\n366\nTransferring the risk\n366\nAccepting the risk\n366\nMitigating the risk\n367\nTypes of security control\n367\nPhysical security controls\n367\nAdministrative controls\n368\nTechnical security controls\n369\nPrevention\n369\nDetection\n370\nResponse\n370\nSecure by design\n370\nMinimizing the attack surface\n371\nDefense in depth\n371\nPrinciple of least privilege (PoLP)\n371\nAvoiding security by obscurity\n372\nKeep software designs simple\n372\nSecure by default\n372\nDefault deny\n373\nValidating input\n373\nSecure the weakest link\n373\nSecurity must be usable\n373\nFail securely\n374\nCryptography\n374\nEncryption\n374\nSymmetric (secret key) encryption\n375\nAsymmetric (public key) encryption\n375\nCryptographic hash functions\n375\nIdentity and access management (IAM)\n377\nAuthentication\n377\nWhat is multi-factor authentication (MFA)? 378\nAuthorization\n378\nStoring plaintext passwords\n379\nStoring encrypted passwords\n379\nStoring hashed passwords\n379\nUsing domain authentication\n380\nImplementing a centralized identity provider (IdP)\n380\nOAuth 2/OpenID Connect (OIDC)\n381\nOAuth 2 roles\n381\nJSON web token (JWT)\n382\nHeader\n383\nPayload\n383\nSignature\n384\nAuthorizing with the authorization server\n384\nMost common web application security risks\n385\nInjection\n385", "domains": ["Software Quality Attributes", "Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 23", "position": 23, "chunk_type": "semantic", "token_estimate": 194}
{"text": "Table of Contents: [ xvii ]\nBroken authentication\n385\nSensitive data exposure\n386\nXML external entity (XXE) attack\n386\nBroken access control\n388\nSecurity misconfiguration\n388\nCross-site scripting (XSS)\n389\nInsecure deserialization\n389\nUsing components with known vulnerable components\n390\nInsufficient logging and monitoring\n390\nUnvalidated redirects and forwards\n391\nSummary\n391\nChapter 12: Documenting and Reviewing Software Architectures\n393\nUses of software architecture documentation\n394\nCommunicating your architecture to others\n394\nAssisting the development team\n395\nEducates team members\n395\nProviding input for software architecture reviews\n395\nAllowing for the reuse of architectural knowledge\n396\nHelp the software architect\n396\nCreating architecture descriptions (ADs)\n397\nSoftware architecture views\n397\nSoftware architecture notations\n398\nInformal software architecture notations\n398\nSemiformal software architecture notations\n399\nFormal software architecture notations\n399\nIncluding design rationales\n400\nOverview of the Unified Modeling Language (UML)\n401\nTypes of modeling\n401\nClass diagrams\n401\nVisibility\n402\nAssociation\n402\nAggregation\n403\nComposition\n404\nMultiplicity\n404\nDependency\n405\nGeneralization/specialization\n405\nRealization\n406\nComponent diagrams\n407\nPackage diagrams\n409\nDeployment diagrams\n410\nUse case diagrams\n411\nSequence diagrams\n413\nLifeline\n414\nActivation boxes\n414\nMessages\n414\nLoops\n415\nOptional flows\n416\nAlternative flows\n416\nActivity diagrams\n417", "domains": ["Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 24", "position": 24, "chunk_type": "semantic", "token_estimate": 198}
{"text": "Table of Contents: [ xix ]\nContinuous deployment\n448\nArchitecting for DevOps\n449\nImportant quality attributes for DevOps\n449\nSome architecture patterns complement DevOps\n451\nDeploying to the cloud\n451\nCloud types\n452\nPublic cloud\n452\nPrivate cloud\n452\nHybrid cloud\n453\nCloud models\n454\nInfrastructure as a service (IaaS)\n455\nContainers as a Service (CaaS)\n456\nPlatform as a Service (PaaS)\n458\nServerless/Function as a Service (FaaS)\n459\nSoftware as a Service (SaaS)\n459\nSummary\n460\nChapter 14: Architecting Legacy Applications\n461\nLegacy applications\n461\nIssues with legacy applications\n462\nWhy are legacy applications used? 463\nMore than just code\n464\nRefactoring legacy applications\n464\nMaking legacy code testable\n465\nBenefits of unit testing\n465\nRefactoring for unit tests\n466\nWhere to start writing tests? 466\nRemoving redundant code\n467\nUnreachable code\n467\nDead code\n468\nCommented-out code\n468\nDuplicate code\n468\nUsing tools to refactor\n469\nMaking small, incremental changes\n469\nTransforming monoliths to microservices\n469\nMigrating to the cloud\n470\nThe 6 R's\n471\nRemove (or retire)\n471\nRetain\n471\nReplatform\n472\nRehost\n472\nRepurchase\n472\nRefactor (or re-architect)\n472\nMoving to an agile approach\n473\nModernizing build and deployment processes\n473\nAutomating the build and deployment processes\n474\nPracticing continuous integration (CI)\n474", "domains": ["Architectural Patterns and Styles"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 26", "position": 26, "chunk_type": "semantic", "token_estimate": 207}
{"text": "Preface: Modern software systems are complex, and the software architect role is a challenging one. This book was written to help software developers transition into the role of a software\narchitect, and to assist existing software architects to be successful in their role. It\nhelps readers understand how being a software architect is different than being a developer\nand what it takes to be an effective software architect. This comprehensive guide to software architecture begins by explaining what software\narchitecture entails, the responsibilities of the software architect position, and what you will\nbe expected to know. Software architects must have technical and non-technical skills, and\nthey must have both breadth and depth of knowledge. The book progress to covering non-technical topics such as the importance of\nunderstanding your organization's business, working in the context of an organization, and\ngathering requirements for software systems. It then takes a deep dive into technical topics\nsuch as software quality attributes, software architecture design, software development\nbest practices, architecture patterns, how to improve performance, and security\nconsiderations. After reading this book, you should have a familiarity with the many topics related to\nsoftware architecture and understand how to be a software architect. Technologies and\npractices may change over time, but the book lays a strong foundation on which you can\nbuild a successful career as a software architect. Who this book is for\nThis book is aimed at senior developers and software architects who want to learn how to\nbe a successful software architect. Readers should be experienced software development\nprofessionals who want to advance in their career and become a software architect. It\ncovers a wide range of topics that will help readers learn what it takes to be effective in the\nsoftware architect role.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 30", "position": 30, "chunk_type": "semantic", "token_estimate": 292}
{"text": "Preface: [ 2 ]\nWhat this book covers\nChapter 1, The Meaning of Software Architecture, begins the book by providing a definition\nof software architecture. The book establishes what makes up a software architecture and\nthe reasons why it is important to a software system. It also details the software architect\nrole, including the responsibilities of software architects and what they are expected to\nknow. Chapter 2, Software Architecture in an Organization, focuses on software architecture in the\ncontext of an organization. It covers the different types of software architect roles and\nsoftware development methodologies that you may encounter. Non-technical topics such\nas project management, office politics, and risk management are explained. The\ndevelopment of software product lines and the creation of architectural core assets are also\ncovered. Chapter 3, Understanding the Domain, discusses the business aspects of being a software\narchitect. It covers topics such as familiarizing yourself with your organization's business,\ndomain-driven design (DDD), and how to effectively elicit requirements for the software\nsystem from stakeholders. Chapter 4, Software Quality Attributes, covers software quality attributes and their\nimportance to a software architecture. Some common software quality attributes are\npresented, including maintainability, usability, availability, portability, interoperability,\nand testability. Chapter 5, Designing Software Architectures, concentrates on the important topic of software\narchitecture design. It details what is involved with architecture design and its importance\nto a software system. The chapter discusses different approaches to architecture design, the\ndrivers for it, and the design principles that can be leveraged during the process. The chapter presents the use of various systematic approaches to software architecture\ndesign, including attribute-driven design (ADD), Microsoft's technique for architecture\nand design, the architecture-centric design method (ACDM), and the architecture\ndevelopment method (ADM). Chapter 6, Software Development Principles and Practices, describes proven software\ndevelopment principles and practices that can be used to build high-quality software\nsystems. Concepts such as loose coupling and high cohesion are covered, as well as\nprinciples such as KISS, DRY, information hiding, YAGNI, and the Separation of Concerns\n(SoC).", "domains": ["Design Principles", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 31", "position": 31, "chunk_type": "semantic", "token_estimate": 335}
{"text": "Preface: [ 3 ]\nThe chapter includes a discussion of the SOLID principles, which include the single\nresponsibility, open/closed, Liskov substitution, interface segregation, and dependency\ninversion principles. The chapter closes with topics related to helping your team succeed,\nincluding unit testing, setting up development environments, pair programming, and\nreviewing deliverables. Chapter 7, Software Architecture Patterns, discusses one of the most useful software\narchitecture design concepts. Learning the architecture patterns that are available to you\nand when to properly apply them is a key skill for software architects. The chapter details a\nnumber of software architecture patterns, including layered architecture, event-driven\narchitecture (EDA), Model-View-Controller (MVC), Model-View-Presenter (MVP),\nModel-View-ViewModel (MVVM), Command Query Responsibility Segregation\n(CQRS), and Service-Oriented Architecture (SOA). Chapter 8, Architecting Modern Applications, explains the software architecture patterns and\nparadigms that are used with modern applications deployed to the cloud. After describing\na monolithic architecture, the chapter details microservices architecture, serverless\narchitecture, and cloud-native applications. Chapter 9, Cross-Cutting Concerns, places its focus on functionality that is used in multiple\nareas of the system. It explains how to handle cross-cutting concerns in your applications. Topics covered include using Dependency Injection (DI), the decorator pattern, and\naspect-oriented programming (AOP) to implement cross-cutting concerns. The chapter\nalso provides a look at different cross-cutting concerns, including caching, configuration\nmanagement, auditing, security, exception management, and logging. Chapter 10, Performance Considerations, takes a close look at performance. It describes the\nimportance of performance and techniques to improve it. Topics such as server-side\ncaching and database performance are discussed. An examination of web application\nperformance is included in the chapter, including coverage of HTTP caching, compression,\nminimizing and bundling of resources, HTTP/2, content delivery networks (CDNs),\noptimizing web fonts, and the critical rendering path. Chapter 11, Security Considerations, covers the critical topic of software application security. Security concepts such as the confidentiality, integrity, and availability (CIA) triad and\nthreat modeling are presented. The chapter provides readers with various principles and\npractices for creating software that is secure by design.", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Design Principles", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 32", "position": 32, "chunk_type": "semantic", "token_estimate": 331}
{"text": "The Meaning of Software Architecture: Chapter 1\n[ 12 ]\nSoftware architecture is the foundation for software, so software systems that lack a solid\narchitecture make it more difficult to meet all of the requirements. Poor architectures will\nlead to implementations that fail to meet the measurable goals of quality attributes, and\nthey are typically difficult to maintain, deploy, and manage. Enabling and inhibiting quality attributes \nSoftware architecture either enables quality attributes or inhibits them. Quality attributes\nare measurable and testable properties of a system. Some examples of quality attributes\ninclude maintainability, interoperability, security, and performance. They are non-functional requirements of a software system as opposed to its features, which\nare functional requirements. Quality attributes and how they satisfy the stakeholders of the\nsystem are critical, and software architecture plays a large role in ensuring that quality\nattributes are satisfied. The design of a software architecture can be made to focus on\ncertain quality attributes at the cost of others. Quality attributes may be in conflict with\neach other. A software architecture, when designed properly, sets out to achieve agreed-\nupon and validated requirements related to quality attributes. Giving you the ability to predict software system\nqualities\nWhen you look at a software architecture and its documentation, you can predict the\nsoftware system's qualities. Making architecture decisions based on quality attributes\nmakes it easier to fulfill those requirements. You want to start thinking about quality\nattributes as early as possible in the software development process as it is much more\ndifficult (and costly) to make changes to fulfill them later. By thinking about them up front,\nand using modeling and analysis techniques, we can ensure that the software architecture\ncan meet its non-functional requirements. If you are not able to predict if a software system will fulfill quality attributes until it is\nimplemented and tested, then costly and time-consuming rework may be necessary. A\nsoftware architecture allows you to predict a software system's qualities and avoid costly\nrework.", "domains": ["Design Principles", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 41", "position": 41, "chunk_type": "semantic", "token_estimate": 328}
{"text": "The Meaning of Software Architecture: Chapter 1\n[ 16 ]\nThe maintenance phase of a software system can be one of the longest and costliest phases\nof a software project. Like new team members introduced during development, it is\ncommon for different developers to work on the system over time, including those\nintroduced to maintain it. Having a solid architecture available to teach and bring aboard\nnew developers can provide an important advantage. Software architecture is not a silver bullet\nThe Mythical Man-Month by Frederick P. Brooks is one of the seminal texts in software\nproject management. It contains various essays on software engineering. Although this\nbook was written some time ago, and some of the references are now outdated, it provides\nthought-provoking advice about software development that is timeless and still applicable\ntoday:\n\"There is no single development, in either technology or management technique, which by\nitself promises even one order-of-magnitude improvement within a decade in productivity,\nin reliability, in simplicity.\" Fred Brooks 1986 essay, No Silver Bullet \u2013 Essence and Accident in Software Engineering, which\nis included in the twentieth anniversary edition of the book, begins with this quote. It\nessentially conveys the idea that there is no silver bullet in software development. Software architecture, as well, is not a silver bullet. Although we have covered a number of\nreasons why software architecture is important, there is no specific architecture or\ncombination of components that will serve as a silver bullet. It can't be thought of as a\nmagical solution that will solve all problems. As we will learn in more detail later, software\narchitectures are about compromises between different and sometimes conflicting\nrequirements. Each architectural approach has pros and cons that must be weighed and\nevaluated. No one approach should be viewed as a silver bullet. Who are the consumers of software\narchitectures? When we create a software architecture, who is it for? There are a variety of stakeholders in\na software system, such as the end users of the system, business analysts, domain experts,\nquality assurance personnel, managers, those who may integrate with the system, and\noperations staff members. Each of these stakeholders is affected by the software\narchitecture to some degree. While certain stakeholders will have access to, and be\ninterested in, examining the software architecture and its documentation, others will not.", "domains": ["Design Principles", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 45", "position": 45, "chunk_type": "semantic", "token_estimate": 387}
{"text": "The Meaning of Software Architecture: Chapter 1\n[ 20 ]\nHelping to select team members\nUnderstanding the business domain\nParticipating in gathering and analyzing requirements\nCommunicating with a variety of technical and non-technical stakeholders\nHaving a vision for future products\nTechnical topics that software architects should be familiar with include:\nUnderstanding non-functional requirements and quality attributes\nBeing able to effectively design software architectures\nUnderstanding patterns and best practices for software development\nHaving a deep knowledge of software architecture patterns, their pros and cons,\nand knowing when to choose one over another\nKnowing how to handle cross-cutting concerns\nEnsuring performance and security requirements are met\nBeing able to document and review software architectures\nHaving an understanding of DevOps and the deployment process\nKnowing how to integrate and work with legacy applications\nBeing able to design software architectures that adapt to change and evolve over\ntime\nDon't be overwhelmed\nIf you find yourself in the software architect role for the first time, or if you are joining a\nteam that has been working on an existing software system for some time, it can be natural\nto feel overwhelmed by all that you do not know. It will take time to wrap your head\naround everything that you will eventually need to know. As your experience grows, you'll feel more comfortable when you start on a new project. Just like anything, experience in different situations will make you more comfortable with\ntaking on new challenges. You'll also understand that it will take some time to become\nacquainted with the business domain, people, processes, technology, details, and intricacies\nthat come with each software system.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 49", "position": 49, "chunk_type": "semantic", "token_estimate": 270}
{"text": "The Meaning of Software Architecture: Chapter 1\n[ 21 ]\nIs the software architect role right for you? If you care about the software that you are working on and all of its stakeholders, including\nthe software's end users and developers, then you care about the important design\ndecisions that go into building the software. Ultimately, that means you care about its\narchitecture. Concerning yourself with the most important decisions can be challenging,\nbut it can be enjoyable and rewarding for that very reason. Software architects need to communicate with a variety of stakeholders and sometimes\nserve as a bridge between management, technical staff, and non-technical staff. If this is not\nsomething you want to get involved with, being a software architect may not be the best fit\nfor you. Software architects are passionate about technology. They have a deep understanding of\nthe technologies they are working with and keep those skills fresh by practicing their craft\nand being involved with projects. They must have a large breadth of knowledge and have a\nfamiliarity with technologies that they may not be currently using on a project. It is\nnecessary to keep up with the fast pace of change in areas such as languages, tools, and\nframeworks. Being aware of a range of technologies will allow you to recommend the best\nsolution to a particular problem. Software architects should love to learn and play with new technologies because being a\nsoftware architect requires continuous learning. As someone with a lot of wisdom to share,\nand who will be a leader on a team, you should enjoy mentoring and teaching others. Making those who work around you better at their jobs is a part of your job. All software applications have a purpose. Good software architects make every effort to\nensure that the software applications they work on serve their purpose as best that they\ncan. If this is something you care about, the software architect role may be right for you.", "domains": ["Design Patterns"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 50", "position": 50, "chunk_type": "semantic", "token_estimate": 330}
{"text": "Software Architecture in an Organization: Chapter 2\n[ 24 ]\nTypes of software architects\nThe role of a software architect can vary from organization to organization. You may have\nalso heard of a variety of job titles related to software architects, such as the following:\nEnterprise architect\nSolution architect\nApplication architect\nData architect/Information architect\nSolution architect\nSecurity architect\nCloud architect\nSome organizations have one or more architects who perform a combination of these roles. They may go by the title of software architect or by the title of one of these roles. In other\norganizations, different individuals play different architectural roles. Some companies\norganize their software architects so that they are in an architecture team. They collaborate\nwith the team on architecture tasks but also work on other teams to design and implement\nsoftware products. This book does not focus on any one type of software architect. It deals with mostly\ntechnical topics, and so is geared toward a number of technical architect roles. Many of the\ntechnical, non-technical, and soft skills described in this book are required by more than\none type of software architect. Even in organizations that have different types of architects,\nthere is an overlap in their responsibilities and duties. Let's take a closer look at the\ndifferent types of software architect roles and what they typically mean. Enterprise architect\nEnterprise architects are responsible for the technical solutions and strategic direction of an\norganization. They must work with a variety of stakeholders to understand an\norganization's market, customers, products, business domain, requirements, and\ntechnology. The enterprise architect ensures that an organization's business and strategic goals are in\nsync with the technical solutions. They need to take a holistic view to ensure that their\narchitecture designs, and the designs of other architects, are in line with the overall\norganization.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 53", "position": 53, "chunk_type": "semantic", "token_estimate": 302}
{"text": "Software Architecture in an Organization: Chapter 2\n[ 25 ]\nThey should have both a deep and broad understanding of technology so that they can\nmake the proper recommendations and architecture designs. They must also look to the\nfuture to ensure that solutions are in line with both existing needs as well as anticipated\nones. In addition to high-level architecture design documents, enterprise architects work with\nother architects, such as application architects, to ensure that solutions meet all of the\ndefined requirements. Enterprise architects come up with and maintain best practices for\nthings such as designs, implementations, and policies. For organizations that have multiple\nsoftware products, they will analyze them to identify areas for architectural reuse. Enterprise architects provide guidance, mentorship, advice, and technical leadership for\nother architects and developers. Solution architect\nA solution architect converts requirements into an architecture for a solution. They work\nclosely with business analysts and product owners to understand the requirements so that\nthey can design a solution that will satisfy those requirements. Solution architects select the most appropriate technologies for the problem that needs to be\nsolved. They may work with enterprise architects, or if such a role does not exist in the\norganization, take on the responsibilities of an enterprise architect, to consider an\norganization's overall strategic goals and enterprise architecture principles when designing\ntheir solution. The designs created by solution architects may be reused in multiple projects. It is common\nin an organization to reuse architectural components and to reuse patterns across\narchitectures in different solution areas. In large organizations that have architects playing\ndifferent roles, solution architects bridge a gap between enterprise architects and\napplication architects. Application architect\nApplication architects focus on one or more applications and their architecture. They\nensure that the requirements for their application are satisfied by the design of that\napplication. They may serve as a liaison between the technical and non-technical staff\nworking on an application.", "domains": ["Design Patterns", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 54", "position": 54, "chunk_type": "semantic", "token_estimate": 320}
{"text": "Software Architecture in an Organization: Chapter 2\n[ 26 ]\nMost of the time, application architects are involved in all the steps in the software\ndevelopment process. They may recommend solutions or technologies for an application,\nand evaluate alternative approaches to problems. Individuals in this role need to keep up\nwith trends and new technologies. They know when to use them in order to solve a\nproblem or take advantage of an opportunity. When appropriate, they are involved with\nhow applications within an organization will work and integrate with each other. Application architects ensure that the development team is following best practices and\nstandards during implementation. They provide guidance and leadership for team\nmembers. They may be involved in reviewing designs and code. Application architects\nwork with enterprise architects to ensure that the solutions designed for an individual\napplication align with the overall strategy of the organization. Data architect/information architect\nData architects are responsible for designing, deploying, and managing an organization's\ndata architecture. They focus on data management systems, and their goal is to ensure that\nthe appropriate consumers of an organization's data have access to the data in the right\nplace at the right time. Data architects are responsible for all of an organization's data sources, both internal and\nexternal. They ensure that an organization's strategic data requirements are met. They\ncreate designs and models and decide how data will be stored, consumed, and integrated\ninto the organization's various software systems. Data architects also ensure the security of\nthe organization's data, and define processes for data backup, data archiving, and database\nrecovery. They maintain database performance by monitoring environments and may be tasked with\nidentifying and resolving various issues, including problems in production environments. Data architects may support developers by assisting with their database design and coding\nwork. Some organizations have the role of an information architect. Although the data architect\nand information architect roles are related, and may even be fulfilled by the same person,\nthere is a difference between the two roles. While data architects focus their attention on databases and data structures, information\narchitects place their focus on users. They are concerned with user intent related to data\nand how data affects the user experience. They are primarily interested in how the data is\ngoing to be used and what is going to be done with it.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 55", "position": 55, "chunk_type": "semantic", "token_estimate": 391}
{"text": "Software Architecture in an Organization: Chapter 2\n[ 27 ]\nInformation architects want to provide a positive user experience and ensure that users can\neasily interact with the data. They want to design solutions so that users have the ability to\nintuitively find the information that they need. They may conduct usability testing to\ngather feedback so that they can determine what changes, if any, should be made to a\nsystem. They work with UX designers and others to develop strategies that will improve\nthe user experience. Infrastructure architect\nInfrastructure architects focus on the design and implementation of an organization's\nenterprise infrastructure. This type of architect is responsible for the infrastructure\nenvironment meeting the organization's business goals, and provide hardware,\nnetworking, operating system, and software solutions to satisfy them. The infrastructure must support the business processes and software applications of the\norganization. These architects are involved with infrastructure components such as the\nfollowing:\nServers: Physical or virtual servers for either cloud or on-premises environments\nNetwork elements: Elements such as routers, switches, firewalls, cabling, and\nload balancers\nStorage systems: Data storage systems such as storage area networks (SAN) and\nnetwork-attached storage (NAS)\nFacilities: The physical location of the infrastructure equipment, and ensuring\npower, cooling, and security needs are met\nInfrastructure architects support the delivery of an enterprise's software applications. This\nincludes designing and implementing infrastructure solutions and integrating new\nsoftware systems with an existing or new infrastructure. Once in production, they also\nensure that existing software systems continue to fulfill requirements affected by\ninfrastructure, and run at optimal levels. Infrastructure architects may make\nrecommendations, such as using new technologies or hardware, which will improve an\norganization's infrastructure. To fulfill the demands of the enterprise, they monitor and analyze characteristics such as\nworkload, throughput, latency, capacity, and redundancy so that a proper balance is\nachieved and desired performance levels are met. They use infrastructure management\ntools and services to assist them with the management of the infrastructure.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 56", "position": 56, "chunk_type": "semantic", "token_estimate": 325}
{"text": "Software Architecture in an Organization: Chapter 2\n[ 28 ]\nInformation security architect\nA security architect is responsible for an organization's computer and network security. They build, oversee, and maintain an organization's security implementations. Security\narchitects must have a full understanding of an organization's systems and infrastructure so\nthat they can design secure systems. Security architects conduct security assessments and vulnerability testing to identify and\nevaluate potential threats. Security architects should be familiar with security standards,\nbest practices, and techniques that can be used to combat any identified threats. They\nrecognize security gaps in existing and proposed software architectures and recommend\nsolutions to close those gaps. Once security components are put into place, security architects are involved in testing\nthem to ensure that they work as expected. When a security incident does occur, security\narchitects are involved in their resolution and conduct a post-incident analysis. The results\nof the analysis are used to take proper action so that a similar incident will not occur again. A security architect may oversee an organization's security awareness program and help to\nimplement an organization's corporate security policies and procedures. Cloud architect\nNow that cloud deployments are the norm, having someone in an organization dedicated\nto cloud adoption, with the relevant expertise, has become increasingly common and\nnecessary. A cloud architect is someone who is responsible for an organization's cloud\ncomputing strategy and initiatives. They are responsible for the cloud architecture used for\nthe deployment of software systems. An organization that has someone who is focused on\ncloud architecture leads to increased levels of success with cloud adoption. The responsibilities of cloud architects include selecting a cloud provider and selecting the\nmodel (for example, SaaS, PaaS, or IaaS) that is most appropriate for the organization's\nneeds. They create cloud migration plans for existing applications not already in the cloud,\nincluding the coordination of the adoption process. They may also be involved in designing\nnew cloud-native applications that are built from the ground up for the cloud.", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 57", "position": 57, "chunk_type": "semantic", "token_estimate": 332}
{"text": "Software Architecture in an Organization: Chapter 2\n[ 29 ]\nCloud architects oversee cloud management, and create policies and procedures for\ngovernance. They use tools and services to monitor and manage cloud deployments. The\nexpertise that cloud architects possess typically means they are involved in negotiating\ncontracts with cloud service providers and ensuring that service-level agreements (SLAs)\nare satisfied. Cloud architects should have a firm understanding of security concerns, such as protecting\ndata deployed into different types of cloud and cloud/hybrid systems. They work with\nsecurity architects, or if such a role does not exist in the organization, take on the\nresponsibilities of a security architect, to ensure that systems deployed to the cloud are\nsecure. For organizations that have not fully migrated to the cloud, one of the tasks for a cloud\narchitect is to lead a cultural change within the organization for cloud adoption. Cloud\nstrategies can fail if the organization's culture does not fully embrace them. Part of the\ncloud architect's job is to evangelize cloud adoption by communicating the many benefits,\nand influence behavior changes toward cloud adoption that will ultimately lead to cultural\nchanges. Software development methodologies\nAs a software architect working in an organization, you will typically be required to use the\nsoftware development methodology that has been adopted by the organization. However,\nin some cases, the software architect may play a role in deciding which software\ndevelopment methodology is used. Either way, software architects may be able to provide\ninput to an organization's processes, giving them the ability to make suggestions that may\nimprove these processes. For these reasons, it is good to have an understanding of the more common software\ndevelopment methodologies. There are a variety of different types that can be employed for\na software project, each of which has its own strengths and weaknesses. Today, Agile\nmethodologies are much more widely used than traditional ones, but even among the\nmethodologies that are considered Agile, there are numerous variations. Unfortunately, sometimes, a software project moves forward with a software development\nmethodology that is not appropriate for the project. Prior to choosing one, you should\nconsider which one would be the most appropriate to use. In the following sections, we'll\ntake a look at two types of software development methodologies: the Waterfall model and\nAgile.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 58", "position": 58, "chunk_type": "semantic", "token_estimate": 383}
{"text": "Software Architecture in an Organization: Chapter 2\n[ 34 ]\nAn iterative methodology\nUnlike the Waterfall approach, which is sequential, Agile methodologies are iterative:\nIterative methodology\nSoftware is built incrementally, with each iteration satisfying a portion of the total\nrequirements. At the end of each iteration is a working version of the software, rather than\nattempting to deliver all of the software at once. Testing takes place in the same iteration as\nthe coding. There is continuous feedback, and if something needs to be changed, it becomes\napparent sooner. Adaptive rather than predictive\nAgile methodologies are adaptive rather than predictive. While methodologies such as\nWaterfall focus on a large amount of upfront planning to detail and predict outcomes in the\nfuture, Agile methodologies focus on adapting to changes as they occur. Although it becomes difficult to know what the state of the software will be in the future,\nand increasingly so the further into the future you want to examine, Agile methodologies\nallow you to make changes after the initial planning takes place. Feedback from users and\ntesters can be taken into consideration immediately, and incorporated into the work of\nsubsequent iterations.", "domains": ["Design Patterns"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 63", "position": 63, "chunk_type": "semantic", "token_estimate": 192}
{"text": "Software Architecture in an Organization: Chapter 2\n[ 36 ]\nProject management\nSoftware architects typically assist project management throughout the life cycle of a\nsoftware system. One of the ways in which you will be asked to do so is in estimating the\nlevel of effort for technical tasks. How estimates are derived and who participates in\nestimation varies from organization to organization, but typically, software architects, with\ntheir expertise and experience, will be expected to provide some level of input during the\nprocess of estimating tasks. The importance of software project estimation\nThe importance of estimation cannot be understated. Estimates and project planning are\nmajor factors in the success of a project. Estimation can be difficult and must take into account a number of factors. However, the\nteam should work together to come up with accurate estimates as project management\nrelies on them. Estimates are used to organize work and plan releases. Inaccurate estimates\nand poor project planning are among the main reasons software projects fail. Putting effort into the estimates\nSometimes, estimation is done informally. You may be asked to provide an estimate or an\nopinion on the spot without an opportunity to analyze the work further. You want to\nestablish a culture within the organization that values putting effort into the estimates,\nrather than off-the-cuff estimates that amount to little more than guesswork. If you are asked by someone to provide an estimate for a task extemporaneously, and you\nare not comfortable doing so, ask whether it's acceptable to get back to them. This will slow\nthe process down, and provide you with an opportunity to conduct a proper analysis\nbefore providing an estimate. Sometimes, if you are on the spot to provide an estimate, it is\neasy to be inaccurate and later regret it. When estimating tasks, some will involve work that is understood, making it easier to\nprovide estimates. It is the work that involves unknowns that can be challenging. If you are\nput in that position, see whether you can afford some time to perform some analysis, or, if\nappropriate, even create a proof of concept (POC). A POC will allow you to prototype a\nsolution to gain a better understanding of the effort involved, or whether the solution is\neven feasible.", "domains": ["Design Patterns"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 65", "position": 65, "chunk_type": "semantic", "token_estimate": 378}
{"text": "Software Architecture in an Organization: Chapter 2\n[ 40 ]\nIdentifying problem areas\nTry to understand why the project is late. The first step in coming up with a solution is to\nfigure out the problem. If you are going to make suggestions to project management about\nhow to get back on schedule, make every attempt to identify the main problems. Consider getting thoughts from the team as to what improvements could be done. You\ndon't want to turn it into a finger-pointing exercise, but the team can provide good insight. There could be some process that is causing a delay, which could be adjusted or eliminated. For example, maybe there is a regular meeting that is not adding much value but taking up\nvaluable time, and the meeting could be eliminated or made to happen less frequently. If there are any delays caused by external forces, examine whether anything can be done to\nalleviate those types of issues. Acting as early as possible\nWhatever approach you take, one thing you do not want to do is to cut testing time short. Not performing the adequate amount of testing can have serious implications for the\nsuccess of the project. Software that does not meet requirements and/or contains defects can\nput the whole project in jeopardy. The cost of inadequate testing may be far greater than\nwhatever time or cost saving it offers. The absolute best thing you can do is act as early as possible. Communicate with project\nmanagement so that they have a detailed understanding of the current state of the project. Use your experience to recognize warning signs and potential pitfalls for the team. Don't\nput off acting on potential problems, because the earlier in the project you realize there is an\nissue, the more options you have to resolve it. Office politics\nAlmost everyone in an organization has to deal with office politics to some degree. As you\ncan see from the following pie chart, the majority of people think it is best to have an\nawareness of the office politics in your own organization:", "domains": ["Design Patterns"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 69", "position": 69, "chunk_type": "semantic", "token_estimate": 348}
{"text": "Software Architecture in an Organization: Chapter 2\n[ 45 ]\nAfter risks are evaluated, software architects may be asked by the organization to assist\nwith a plan on how to manage the risks. There are four main techniques that can be used to\nhandle risks: risk avoidance, transferring the risk to another party, risk mitigation, and risk\nacceptance. Risk avoidance\nOne technique for handling risk is risk avoidance, which involves changing the project in\nsome way so as to eliminate the risk altogether. An example of a risk that could be dealt\nwith in this way would be a project team that is considering using a programming\nlanguage and/or technology that the team is not familiar with. Moving forward with such a\nchoice brings with it some potentially significant risks. The project may take longer than anticipated due to the learning curve, or it may be\ndiscovered that features the team wants to implement are not well suited to the chosen\ntechnology. One way to handle this risk would be to select a technology that the team is\nalready familiar with, as long as it can satisfy the needs of the project. This would allow the\nteam to effectively avoid the risk. Keep in mind that not all risks can be avoided, and avoiding a risk could lead to other risks. In addition, taking risks is sometimes necessary in order to take advantage of an\nopportunity. These should be some of your thoughts when considering whether or not to\navoid a risk. Transferring the risk to another party\nAnother technique in risk management is to transfer the risk to another party. An example\nof this might be a project team that hires a subcontractor to implement parts of a project,\nsuch as the design and development of a user interface for a web application. There is a risk\nthat the subcontractor may not complete their deliverables on time or that their deliverables\nmay not meet certain quality standards. One way to handle this risk would be to transfer it\nto the subcontractor by building penalties into the contract that will be incurred if the\ndeliverables are late or do not meet the level of quality desired.", "domains": ["Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 74", "position": 74, "chunk_type": "semantic", "token_estimate": 366}
{"text": "Software Architecture in an Organization: Chapter 2\n[ 51 ]\nRisks of product line engineering\nAlthough product line engineering for software may result in benefits, be aware that there\nare risks related to its successful implementation. Moving to this type of approach requires\nadopting a new technical strategy for the organization as a whole. It requires coordination\nand managerial support. It needs to be clear to development teams what core assets are available to them, and what\nthe variation points are so that they can utilize them properly. The scope of the product line\nneeds to be set appropriately so that it is neither too broad nor too narrow. If the organization is creating all of the products in the same product line, it requires\nsuccessful execution by the software architects and development teams, as well as the\nproper organizational management. If the organization is acquiring software products to be\nincluded in a software product line, then the proper resources, both technical and\nmanagerial, must be available and prepared to put forth the effort required to identify and\nutilize the common components of those products. Organizations that are not prepared to fully adopt this approach may experience failure. Summary\nWhen working as an architect in an organization, you should be aware of the various types\nof software architect roles. The various roles have similarities and differences in terms of\ntheir responsibilities and duties. If you are required to use a software development\nmethodology that has already been selected for your organization, it is important to\nbecome familiar with it and its practices. If you are in a position to select the software\ndevelopment methodology, be sure to be knowledgeable about the choices so that an\nappropriate one can be selected for the project's needs. Software architects use their expertise and experience to work with project management on\nestimates for tasks, project planning, and controlling and monitoring the project during its\nlife cycle. Due to communicating with a larger number of stakeholders and management, office\npolitics is a reality that a software architect will have to deal with more than those in other\ndevelopment roles. Organizations will want involvement from software architects for risk management. Software architects can identify risks and come up with approaches to handle them. Software architects work with the SCM team to coordinate builds, development\nenvironments, and to provide input for change management.", "domains": ["Design Patterns", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 80", "position": 80, "chunk_type": "semantic", "token_estimate": 396}
{"text": "Understanding the Domain: Chapter 3\n[ 57 ]\nIn addition to encouraging communication with domain experts, DDD improves\ncommunication among all team members and stakeholders by introducing the concept of a\nubiquitous language. What is a ubiquitous language? The development team may not have a strong understanding of the domain, and may not\nbe familiar with terms and concepts used by stakeholders, including the domain experts. They may use their own language when discussing the functionality and discuss the\ndomain in terms of their technical design. The stakeholders, including the domain experts,\nwill use their own jargon when discussing their domain, and may not have a good\nunderstanding of technical terms. Because different people may use different language to\ndescribe the same concepts in a particular domain, it can take longer to communicate ideas,\nand it can lead to misunderstandings. In Domain-Driven Design: Tackling Complexity in the Heart of Software, Eric Evans described\nthis problem:\n\"The terminology of day-to-day discussions is disconnected from the terminology\nembedded in the code (ultimately the most important product of a software project). And\neven the same person uses different language in speech and in writing, so that the most\nincisive expressions of the domain often emerge in a transient form that is never captured\nin the code or even in the writing. Translation blunts communication and makes knowledge crunching anemic. Yet none of these dialects can be a common language because none serves all needs.\" Some team members may become familiar with the domain terminology and act as\ntranslators for the rest of the team, but they can become bottlenecks.", "domains": ["Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 86", "position": 86, "chunk_type": "semantic", "token_estimate": 266}
{"text": "Understanding the Domain: Chapter 3\n[ 58 ]\nIn order to mitigate these types of risks, Eric Evans created the concept of a ubiquitous\nlanguage. It is a common language among all team members and stakeholders based on the\ndomain model:\nDeveloping a ubiquitous language can take time, and can evolve and grow as the team's\nunderstanding of the domain changes. Domain experts should use their understanding of\nthe domain to point out terms that don't correctly express an idea, and everyone can look\nfor inconsistencies and ambiguities in an effort to improve the ubiquitous language.", "domains": ["Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 87", "position": 87, "chunk_type": "semantic", "token_estimate": 96}
{"text": "Understanding the Domain: Chapter 3\n[ 59 ]\nAlthough it takes effort, once you have a ubiquitous language, it simplifies communication\nand leads to a greater understanding among everyone involved with the project. No\ntranslation will be needed because everyone has agreed on and understands the various\nterms. The important thing is to use it consistently and throughout the project. The\nubiquitous language should be used during discussions and in all of the project artifacts\nsuch as documentation, diagrams, code, and tests. Entities, value objects, and aggregates\nSome of the basic building blocks of DDD are entities, value objects, and aggregates. When\nmodeling them, the ubiquitous language should be used. Entities\nEntities are objects that are defined by their identity and not their attributes. They are\nmutable because the values of their attributes can change without changing their identity. If\ntwo objects have the same values for their attributes, other than their unique identifier, they\nare not considered equal. For example, if you had two Person objects with the same first and last name values for\nthose corresponding attributes, they are still two different objects because they have\ndifferent identities. This also means that a value for an attribute such as last name can be\nchanged on a Person object and it still represents the same person. Value objects\nUnlike entities, value objects are objects that describe some characteristic or attribute, but\nhave no concept of identity. They are defined by the values of their attributes and are\nimmutable. If two objects have the same values assigned to their properties, they can be\nconsidered equal. For example, if two objects that represent points on a graph using Cartesian coordinates\nhave the same x and y values, they can be considered equal and would be modeled as a\nvalue object.", "domains": ["Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 88", "position": 88, "chunk_type": "semantic", "token_estimate": 299}
{"text": "Understanding the Domain: Chapter 3\n[ 60 ]\nAggregates and root entities\nAggregates are groupings of entities and value objects that are treated as a single unit. A\nboundary is defined that groups them together. Without aggregates, complicated domain\nmodels can become unwieldy to manage, as the many entities and their dependencies grow\nlarge in number. Retrieving and saving an entity and all of its dependent objects can\nbecome difficult and error-prone. An example of an aggregate is an order object that contains an address object and a\ncollection of line item objects. The address object and the collection of line item objects are\nall separate objects, but they are treated as a single unit for data retrieval and changes. Separating the domain into subdomains\nOne practice of DDD is to separate the domain model into multiple subdomains. While a\ndomain is the entire problem space that the software solution is being developed for, a\nsubdomain is a partitioned piece of the overall domain. This is particularly useful for large\ndomains, where it is not feasible to have one large and unwieldy domain model. By focusing on one subdomain at a time, it reduces complexity and makes the overall work\nmore digestible. Rather than attempting to address too many issues at once, dividing your\ndomain into subdomains provides more of a divide and conquer approach. For example, in a student information system, you may have subdomains for contact\nmanagement, admissions, financial aid, student accounts, and academics, among others. One or more of the subdomains may be designated as a core domain, which is typically the\npart of the domain that is fundamental to the organization. If there is a part of the domain\nthat differentiates the organization from competitors, it is probably one of the core\ndomains. Core domains are the reason that the software is worth writing, rather than\nbuying existing software off the shelf or outsourcing the work. The domain experts on the project can help with identifying the core domains, as well as\nthe division of domains into subdomains.", "domains": ["Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 89", "position": 89, "chunk_type": "semantic", "token_estimate": 342}
{"text": "Understanding the Domain: Chapter 3\n[ 61 ]\nWhat are bounded contexts? A domain model is a conceptual model based on the domain and includes both behaviors\nand data. It represents a part of the overall solution that fulfills the goals of the business. Bounded contexts are a pattern in DDD that represent partitions in the domain model. Similar to subdomains, which are partitions in the domain, bounded contexts are partitions\nin the domain model. As is the case with subdomains, creating partitions and boundaries\nreduces the overall complexity. A bounded context may map to a single subdomain, but keep in mind that is not always the\ncase. The domain model for a subdomain may require multiple bounded contexts for the\noverall solution of that subdomain. For example, if we were creating a software system for a business that sells clothing online,\nwe might allow customers to sign up for a newsletter that contains deals and discounts. Another part of the application would allow customers to place orders and provide\npayment information. With these two pieces of functionality, some concepts are shared, while some are not. If\ndifferent development teams, or different developers on a single team, are working on\nthese two sets of functionality, it is not clear what overlap, if any, exists. If there is overlap,\nwhat should or should not be shared between these two pieces of functionality? This is\nwhere the concept of bounded contexts is applicable. A domain model applies to a\nparticular context, so we can define the various contexts to clear up some of the ambiguities\nthat exist.", "domains": ["Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 90", "position": 90, "chunk_type": "semantic", "token_estimate": 265}
{"text": "Understanding the Domain: Chapter 3\n[ 62 ]\nIn this example, we could create one bounded context for marketing (Marketing Context),\nand one for order processing (Order Processing Context). Each bounded context may have\nentities that are unique to itself. For example, the Order Processing Context has the concept\nof an order line item, whereas the Contact Management Context does not. However, both\nbounded contexts have the concept of a Customer. Is Customer referring to the same\nconcept in both bounded contexts? By separating them out, we can begin to answer this\nquestion:\nIn the context of marketing, all that may be required for a Customer entity is an identity\n(unique identifier), first name, last name, and email address. However, in the context of\nplacing an order, the Customer entity would require additional information, such as a\nshipping address and payment information. You could create one Customer entity, but using it for different contexts adds complexity\nand can lead to inconsistencies. Validation that requires payment information only applies\nin the Order Processing Context, and not the Marketing Context. The behavior required\nfor a Customer in the Order Processing Context should not prevent a Customer from\nbeing created in the Marketing Context, where only the first name, last name, and email\naddress are required.", "domains": ["Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 91", "position": 91, "chunk_type": "semantic", "token_estimate": 213}
{"text": "Understanding the Domain: Chapter 3\n[ 63 ]\nWe will discuss the single responsibility principle (SRP) later in this book, but the basic\nidea is that each class should be responsible for a single aspect of the functionality. The\nCustomer entity is still small now, but you can begin to see how it could grow quickly. If it\nwere to be used in multiple contexts, it would attempt to fulfill too many disparate\nresponsibilities and break the SRP. The context for each model should be clearly defined and there should be an explicit\nboundary between bounded contexts. They are created so that everyone on the team, or\nacross multiple teams, can have the same understanding of what belongs in each context. While the example used is a simplistic one, a large domain model will have many entities\nand contexts, and it usually isn't immediately clear what is unique or common across\ndifferent contexts, and how each context should interact with each other. DDD and the concept of bounded contexts work well with microservices, which we will be\ndiscussing in further detail later in this book. Now that we have a better understanding of\nDDD concepts, let's go into detail about requirements. Working with domain experts and\nother stakeholders, we need to have an understanding of the requirements prior to design. Requirements engineering\nIn order to model your domain and design an appropriate architecture, you will need to\nknow the requirements for the software you are building. Requirements engineering\ninvolves establishing the functionality that is required by the stakeholders, along with the\nconstraints under which the software must be developed and operate. It encompasses all of\nthe tasks involved in eliciting, analyzing, documenting, validating, and maintaining the\nrequirements of the software system. As a software architect, you will be participating in\nthese tasks so it is helpful to be familiar with them. Types of software requirements\nThere are different types of software requirements, and software architects should be\nknowledgeable about them. The main types of software requirements include:\nBusiness requirements\nFunctional requirements\nNon-functional requirements\nConstraints", "domains": ["Architectural Patterns and Styles", "Design Principles", "Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 92", "position": 92, "chunk_type": "semantic", "token_estimate": 344}
{"text": "Understanding the Domain: Chapter 3\n[ 65 ]\nExternal requirements: Requirements that originate externally, such as\nrequirements from external systems that must integrate with the software system\nbeing designed\nNon-functional requirements\nNon-functional requirements are conditions that must be met in order for the solution to\nbe effective, or constraints that must be taken into consideration. Business analysts and\nother stakeholders tend to do a good job at capturing functional requirements, but they\ndon't always focus as much on non-functional requirements. However, non-functional\nrequirements are an important part of requirements engineering. The success of the project\nis dependent on the non-functional requirements and whether or not they are met. When a software architecture is designed, the software architect must ensure that the non-\nfunctional requirements can be satisfied. Non-functional requirements can have a\nsignificant impact on the design of the architecture. For that reason, they are of great\nimportance to software architects. Software architects need to play an active role in eliciting\nnon-functional requirements from stakeholders and ensuring that they are captured. Quality attributes are a subset of non-functional requirements and include the ilities, such\nas maintainability, usability, testability, and interoperability. We'll go into more detail on\nsome of the different quality attributes in the next chapter. Constraints\nConstraints are some type of restriction on the solution and may be technical or non-\ntechnical in nature. Some constraints on a project might be captured and classified as a\nfunctional or non-functional requirement, or they might be explicitly categorized as a\nconstraint. Either way, the important thing is that they are decisions that have already been\nmade and must be honored. Typically, a constraint cannot be changed, and the software\narchitect does not have any control over it. However, if you have reasons why you believe a\nconstraint should be changed or removed, there may be situations in which you could\nprovide your input. Constraints can pertain to a number of aspects of a software project. The following are\nsome examples of constraints:\nAn organization might have an existing agreement with a particular vendor or\nhas already purchased a certain technology or tool that you will be required to\nuse\nThere may be a law or regulation that the software must follow", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 94", "position": 94, "chunk_type": "semantic", "token_estimate": 370}
{"text": "Understanding the Domain: Chapter 3\n[ 71 ]\nEither the facilitator or another person should take notes, preferably on a whiteboard so\nthat everyone can see the ideas that have previously been given. For remote meetings\nwhere everyone is not in a room together, someone should share their screen so that the\nideas are visible. There should be a time limit so that everyone is aware when the session will end. If there is\na clear ending where no more new ideas are being generated, the session could be called off\nearly. Observation\nObservation is a technique where someone studies a stakeholder in their work\nenvironment, performing tasks related to the software project. It is particularly useful when\nyou are attempting to understand a current process. It is effective because the observer may\nnotice things that aren't mentioned through other elicitation techniques. Stakeholders may\nforget certain requirements or may not even be aware that what they are doing is a\nrequirement that needs to be documented. By observing the actual work performed, you\ncan sometimes gather important information. This technique can either be performed in a passive way or in an active way, depending on\nwhat is agreed upon and what would be most effective. If the observer is passive, then he\nor she makes every attempt not to be disruptive. The observer does not ask many\nquestions, nor do they interrupt the tasks that the stakeholder is performing. If the observer\nis active, then they can have an ongoing dialog with the stakeholder and ask questions\nwhile they are performing their tasks. There are disadvantages to this technique. It can be time-consuming to observe someone\nperforming his or her daily work. The person who is being observed may find it disruptive\nto their work, so you may only get limited time to perform the observation. Even if a lot of time is spent observing, you may not witness all of the possible scenarios,\nand being aware of scenarios that do not happen as frequently is still important for the\nrequirements of the software. Although you shouldn't use this technique as the only one, it can be useful as a supplement\nto other techniques because it may draw out requirements that would not be revealed using\nother elicitation techniques.", "domains": ["Design Patterns"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 100", "position": 100, "chunk_type": "semantic", "token_estimate": 379}
{"text": "Understanding the Domain: Chapter 3\n[ 73 ]\nDocument analysis\nDocument analysis utilizes existing documentation to obtain information and\nrequirements. The documentation may cover the relevant business processes or existing\nsoftware systems. If there is an existing system in use, it can serve as a starting point for the\nrequirements of the new system. The documentation may come in the form of technical\ndocumentation, user manuals, contracts, statements of work, emails, training materials, and\nanything else that may be of use. The documentation may even be commercial off-the-shelf (COTS) software package\nmanuals. There may be existing software that provides part or all of the functionality you\nare seeking to implement, and by analyzing that documentation you can get the\nrequirements for your software system. Analyzing documents is particularly useful if stakeholders are not available for other\nrequirements elicitation techniques. Prototyping\nPrototyping is a requirements elicitation technique that involves building a prototype that\nstakeholders can use to some degree, or at least see. Some people are more visually-\noriented than others, and having a prototype can trigger ideas regarding requirements. The disadvantage of prototyping is that it can take time to build a prototype. However,\nwith most modern technologies a prototype can be built quickly. There is also the option of\nsimply creating visual diagrams of the software, rather than a prototype. For web\napplications, this involves creating wireframes, which are visual representations of web\npages that let a person see the layout and structure of web pages. The scope of a prototype can be as broad or narrow as you want it to be. While it can\ndemonstrate an entire application, it could be focused on a specific piece of functionality. Prototyping can be useful in conjunction with other techniques so that you can validate\nrequirements and uncover things that had not already been discussed. Prototyping can also be taken to a different level in which a working version of the\nsoftware is produced. In a situation where the direction and purpose of the software have\nnot been fully evaluated yet, perhaps because the stakeholders don't know where to begin,\nor they have many ideas but cannot agree among themselves, an initial prototype can be\ndeveloped. If you are using an agile methodology, a few initial iterations can take place,\neach ending with a working version of the software that can be shared with the\nstakeholders.", "domains": ["Design Patterns", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 102", "position": 102, "chunk_type": "semantic", "token_estimate": 396}
{"text": "Understanding the Domain: Chapter 3\n[ 75 ]\nSummary\nBeing an effective software architect means understanding the domain of the software you\nare building. Gaining knowledge of general business topics and a deep understanding of\nthe organization's business is the foundation for becoming an expert on the problem space\nfor which you will be designing a solution. DDD is a proven approach to modeling a domain. Creating a ubiquitous language that will\nsimplify and facilitate communication between everyone involved in the software project,\nand working with domain experts, will facilitate learning a particular domain. Other practices, such as separating your domain into subdomains and creating bounded\ncontexts in your domain model, will minimize complexity and allow you and your team to\nfirmly grasp even complex domains. One of the keys to building software successfully is proper requirements engineering,\nincluding knowing how to effectively elicit requirements from stakeholders. Knowing the\nrequirements for the software is crucial to designing an appropriate solution. In the next chapter, we will further explore one of the most important types of software\nrequirements: quality attributes. Building quality software requires the software architect to\nknow and understand the details of the quality attributes that are important to the\nstakeholders.", "domains": ["Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 104", "position": 104, "chunk_type": "semantic", "token_estimate": 202}
{"text": "4: Software Quality Attributes\nQuality attributes are of the utmost importance to software architects because they affect\narchitectural decisions. In this chapter, we will begin by explaining what quality attributes\nare, and why it is important to consider them throughout the software development life\ncycle. Some quality attributes can be difficult to test, so we'll explore what it takes to test\nthem. We will go into detail on some more common software quality attributes, such as\nmaintainability, usability, availability, portability, interoperability, and testability. After\nreading this chapter, you will know the considerations that need to be taken into account\nfor a software system to meet these quality attributes. In this chapter, we will cover the following topics:\nQuality attributes\nMaintainability\nUsability\nAvailability\nPortability\nInteroperability\nTestability\nQuality attributes\nQuality attributes are properties of a software system and a subset of its non-functional\nrequirements. Like other requirements, they should be measurable and testable. Software\nquality attributes are benchmarks that describe the software system's quality and measure\nthe fitness of the system. A software system is made up of a combination of quality\nattributes, and the degree to which they are fulfilled describes the overall quality of the\nsoftware.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 105", "position": 105, "chunk_type": "semantic", "token_estimate": 196}
{"text": "Software Quality Attributes: Chapter 4\n[ 77 ]\nQuality attributes can have a significant impact on the design of the architecture, so they\nare of great interest to software architects. They affect multiple facets of a software system,\nsuch as its design, degree of maintainability, runtime behavior, and the overall user\nexperience. When designing your architecture, it is imperative to understand that software quality\nattributes can affect each other, and the degree to which one is met can affect the degree to\nwhich others can be met. It's important to identify potential conflicts between quality\nattributes. For example, a need for ultra-fast performance may conflict with the ability to\nachieve extreme scalability, and having a high level of security could decrease the level of\nusability. Such trade-offs need to be analyzed to provide a balance so that an acceptable solution can\nbe created. The priority of each quality attribute will be a factor in your overall design. External or internal\nQuality attributes can be internal or external. Internal quality attributes can be measured by\nthe software system itself and are visible to the development team. Consequently, they are\nmeasurable both during and after development. Examples of internal quality attributes are\naspects of the software system such as lines of code (LOC), level of cohesion, readability of\nthe code, and the degree of coupling between modules. These attributes reflect the complexity of the software system. Although internal quality\nattributes are not visible to users directly, they affect external quality attributes. A greater\nlevel of internal quality often leads to a greater level of external quality. External quality attributes are properties that are externally visible; hence, they are\nnoticeable to end users. These quality attributes are measured with respect to how the\nsoftware system relates to its environment. Unlike internal quality attributes, a working\nversion of the software must be deployed so that it can be tested. Examples of external\nquality attributes are the performance, reliability, availability, and usability of the system.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 106", "position": 106, "chunk_type": "semantic", "token_estimate": 330}
{"text": "Software Quality Attributes: Chapter 4\n[ 78 ]\nQuality attributes and the SDLC\nQuality attributes should be considered throughout the software development life cycle\n(SDLC). The process begins during requirements engineering by ensuring that they are\ncaptured completely and correctly. In the previous chapter, we discussed the importance of\nmeasurable and testable requirements. This should be stressed further in relation to quality\nattributes, as some quality attributes can be difficult to measure. Quality attributes must be\nmeasurable and testable in order to determine if the software system satisfies the needs of\nstakeholders. The software architecture design must ensure that it can meet the quality objectives. During\ntesting, quality attributes must be verified to ensure that the software system satisfies the\nrequirements. Testing quality attributes\nWhat kinds of testing techniques should be used to verify quality attributes? Beyond the\nsimplest of software applications, it can be challenging to test software quality attributes\nand all of the possible scenarios that are affected by them. A variety of testing techniques\nmust be employed in order to test different properties. For example, the following are some\ntests you may need to conduct:\nManually testing the software for usability\nCreating benchmarks and using tools for performance testing\nPerforming code reviews and calculating code metrics to test for maintainability\nExecuting automated unit tests to ensure the system behaves as expected\nEach testing technique comes with its own set of strengths and weaknesses. Some software\nquality attributes are difficult to assess, so you may need to combine multiple testing\ntechniques in order to effectively test a quality attribute. Given unlimited resources, we\nmight perform an exhaustive amount of testing, but projects are typically constrained by\nsome combination of cost and time. As a result, a balance sometimes needs to be reached\nbetween the amount of testing and the time available. Automating as much of the testing as possible is key to being able to execute tests in a short\namount of time while maximizing test coverage. Automated tests can be executed on-\ndemand or as part of a continuous delivery process (for example, as part of an automated\nbuild). Development teams can be assured that they still meet quality attribute objectives as\nthey continue to make changes to the code.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 107", "position": 107, "chunk_type": "semantic", "token_estimate": 373}
{"text": "Software Quality Attributes: Chapter 4\n[ 79 ]\nNow that we know more about quality attributes, let's go into detail about some common\nones (performance and security will be covered in Chapter 10, Performance Considerations,\nand Chapter 11, Security Considerations). Maintainability\nMaintainability focuses on the ease with which a software system can be maintained. Maintenance of a software system takes place as changes are made to it after the software is\nin operation. Maintenance is necessary to preserve the value of the software over time. Change is constant in the real world. Sometimes it is expected and can be planned for,\nwhile other times it is not. Either way, it is inevitable that software systems will experience\nchange. With the knowledge that change is unavoidable, it is important to build\nmaintainable systems. Decades ago, the greater part of a software project's costs went into software development. However, over the years there has been a shift in the cost ratio from development to\nmaintenance. Today, the majority of a system's lifetime costs can typically be attributed to\nmaintenance. Doing whatever is possible to keep those costs down can make a significant\ndifference in the total money spent on the software over its lifetime. Code that is easy to maintain allows maintenance work to be completed more quickly, and\nin turn will help to keep maintenance costs down. When a developer is writing code, he or\nshe has to take into consideration not just the end user of the software, but also those who\nwill be maintaining it. Even if the original developer ends up also being responsible for the maintenance of a\nparticular piece of code, consider the fact that the developer could leave the organization. Further, a developer may have a need to revisit their own code after some time passes, only\nto have forgotten the intricacies of it. In some cases, a developer may not even remember at\nfirst that they were the original developer of the code! Maintainable code benefits whoever\nneeds to maintain it, even if it is the original developer. Maintainability also affects how easily a software system can be reverse-engineered. There\nmay be a need for a software system to be reverse-engineered, possibly so that it can be\nmigrated to a newer technology. An architecture that exhibits a high level of modifiability\nwill be easier to understand and reason about, making it easier to reverse-engineer.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 108", "position": 108, "chunk_type": "semantic", "token_estimate": 400}
{"text": "Software Quality Attributes: Chapter 4\n[ 80 ]\nTypes of software maintenance\nSoftware maintenance is performed for a variety of reasons, such as correcting defects,\nimproving quality in some way, or meeting new requirements. As a result, software\nmaintenance work can be categorized into the following different types of software\nmaintenance:\nCorrective\nPerfective\nAdaptive\nPreventive maintenance\nCorrective maintenance\nCorrective maintenance is the work involved in analyzing and fixing defects in the\nsoftware. Although it isn't the only kind of maintenance, it is the type that people associate\nmost with maintenance work. Defects might be found internally, or by users in production. The severity and priority of defects vary depending on the nature of the bug. Severity represents the level of impact the bug has on the operation of the software. Organizations have various classification systems for severity, but categories such as\ncritical, high, medium, and low are common examples. The priority of a defect is the order in which it will be fixed. Typically, the higher the\npriority, the quicker it will be fixed. Like severity, organizations may have different\nclassification systems for priority, but categories such as high, medium, and low are common. Another common classification system is P0, P1, P2, P3, and P4, with P0 being the highest\npriority. P0 defects are critical and are considered blockers. A release will be put on hold\nuntil all P0 defects are fixed. Maintainability can be measured by the time it takes to analyze and fix a particular defect. Higher levels of maintainability allow these tasks to be completed in a shorter amount of\ntime. Perfective maintenance\nPerfective maintenance is necessary when the software needs to implement new or\nupdated requirements. These types of changes are mostly focused on the functionality of\nthe software. An example of perfective maintenance is a new enhancement to the software\nsystem.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 109", "position": 109, "chunk_type": "semantic", "token_estimate": 306}
{"text": "Software Quality Attributes: Chapter 4\n[ 81 ]\nSoftware that has a higher level of maintainability will allow for perfective changes to be\nmade with less effort, and therefore at a lower total cost. Adaptive maintenance\nAdaptive maintenance is defined as the work required to adapt a software system to\nchanges in the software environment. Examples of this may be to adapt the software system\nfor a new operating system (OS), a new version of the same OS, or to use a new database\nmanagement system (DBMS). The duration of time it takes to adapt a software system to changes in the environment is a\nmeasure of the maintainability of the software. Preventive maintenance\nThe goal of preventive maintenance tasks is to prevent problems in the future, by\nincreasing quality. This may include improving quality attributes of the software system,\nsuch as increasing maintainability and reliability. Preventive maintenance may take place to make maintenance easier in the future. An\nexample of this is refactoring a software component to make it less complex. Modifiability\nModifiability is one aspect of maintainability. Modifiability is the ease with which changes\ncan be made to the software without introducing defects or reducing quality. It is an\nimportant quality attribute because there are a number of reasons why software needs to be\nchanged. Some software can remain useful in production for years, or even decades. Inevitably, that\ncode will need to be modified for the different types of maintenance that were described\npreviously. The time required from when a change is specified until it is deployed is an\nindication of the modifiability of the system. In today's world, agile software development methodologies are the most common. These\nsoftware projects embrace change. In addition to new functionality, each iteration of the\nproject can involve changes to existing code. Improving modifiability is not just beneficial\nfor maintenance but also for the entire development of the software.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 110", "position": 110, "chunk_type": "semantic", "token_estimate": 318}
{"text": "Software Quality Attributes: Chapter 4\n[ 82 ]\nExtensibility and flexibility\nExtensibility and flexibility are additional characteristics related to maintainability. The\nlevel of extensibility reflects how easy it is to extend or enhance the software system. Software systems that are designed to be extensible take future growth into consideration\nby anticipating the need to add new functionality. Flexibility is similar but mostly focuses on how easy it is to change a capability so that it can\nbe used in a way that wasn't originally designed. Both extensibility and flexibility are\ncharacteristics that dictate the level of ease with which someone can perform perfective\nmaintenance. Scope of modifications\nNot all changes to software are equal, and the scope of a particular modification is a factor\nin how difficult the change will be to implement. The larger and the more complex the\nmodification, the greater the effort to complete it. In addition to size, if the changes require architecture level changes, that will increase the\nlevel and scale of effort involved. Some components and their interactions may need to be\nrefactored extensively for large changes. Designing for maintainability\nIn order to design a software architecture that exhibits maintainability, you must reduce the\ndifficulty in implementing changes. Making it easier to implement changes, in large part,\nmeans reducing the complexity of the architecture and its components. Higher levels of complexity make software systems harder to understand, test, and\nmaintain. For this reason, the level of complexity is a predictive measure of some quality\nattributes, including maintainability. Evidence shows that modules with greater complexity are also more likely to contain\nerrors. What makes this situation worse is that such modules are also more difficult to test,\nmeaning that it may be more likely that an error will go undetected. Although taking measurements and designing architectures to lower complexity may\nrequire more time (and therefore cost more money) during development, cost savings will\nbe realized in the long term due to higher quality and a greater level of maintainability.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 111", "position": 111, "chunk_type": "semantic", "token_estimate": 334}
{"text": "Software Quality Attributes: Chapter 4\n[ 83 ]\nSome techniques used to reduce complexity and increase maintainability are to:\nReduce size\nIncrease cohesion\nReduce coupling\nReducing size\nModules that are large tend to be more complex, and are responsible for a larger share of\nthe logic. Therefore, the larger a module is, the more difficult it can be to change. Your\ndesign should seek to reduce the size of individual modules. One way to accomplish this\nwould be to split up a module into multiple ones. Increasing cohesion\nCohesion represents how interrelated the elements in a particular module are. Designs\nshould seek to increase cohesion by not allowing a particular module to have too many\ndisparate elements. Increasing cohesion reduces complexity and enables a number of\ndifferent quality attributes, including maintainability. High cohesion often correlates to\nloose coupling. Reducing coupling\nCoupling refers to how dependent different modules are on each other. Designs should\nseek loose coupling, such that different modules are independent of each other or almost\nindependent. If it becomes necessary to make changes to a highly coupled module, it will\nbe more likely that other modules will be affected and therefore also require changes. Loose\ncoupling reduces complexity and enables a number of different quality attributes, including\nmaintainability. Loose coupling often correlates to high cohesion. Measuring maintainability\nA number of software metrics can help to measure the complexity of software, and\ntherefore also the maintainability of software. Although measuring complexity and\nmaintainability can be difficult, there are software metrics that can provide you with\ninsights about the level of complexity and maintainability of your software.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 112", "position": 112, "chunk_type": "semantic", "token_estimate": 267}
{"text": "Software Quality Attributes: Chapter 4\n[ 84 ]\nLines of code (LOC)\nOne such software metric is lines of code (LOC), also known as source lines of code\n(SLOC). This measurement simply represents the size of a software system by determining\nthe number of lines in its source code. Typically, software systems with more lines of code are greater in complexity and more\ndifficult to maintain than those with fewer lines of code. However, comparisons of LOC\nbetween different software systems are really only useful if they involve an order of\nmagnitude difference in lines of code. For example, if one software system has 50,000 lines\nof code, and another has 48,000 lines of code, you won't be able to make a determination on\nwhich software is more maintainable. However, if you were to compare a software system\nwith 10,000 lines of code with one that has 100,000 lines of code, then you are more likely to\nmake a useful determination. Development tools and integrated development environments (IDEs) can count the \nnumber of lines of code for you. However, there are different ways that it can be calculated,\nand it is debatable which is the most useful. There is also a challenge if a software system\nuses multiple languages. The two major ways that it can be calculated are the physical LOC and the logical LOC. The\nphysical LOC typically is just a count of all of the source code lines, excluding comment\nlines. The logical LOC takes into consideration the actual number of program language\nstatements in an attempt to only count effective lines of code. While the physical LOC is\neasier to calculate, it is more sensitive to being affected by such things as line spacing and\nother formatting. Cyclomatic complexity\nCyclomatic complexity is a quantitative software metric that reflects the complexity of a\nsoftware module. It was developed by Thomas J. McCabe and is sometimes referred to as\nMcCabe's cyclomatic complexity. It measures the number of linearly independent paths\nthrough a module or detailed design element. Higher cyclomatic complexities indicate that\nthe software is more complex. Cyclomatic complexity can be calculated in a few different ways, but one of the most\ncommon is by using the following formula:", "domains": ["Design Patterns", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 113", "position": 113, "chunk_type": "semantic", "token_estimate": 371}
{"text": "Software Quality Attributes: Chapter 4\n[ 89 ]\nUsability needs to be considered during\nrequirements\nDuring requirements engineering, use the requirement elicitation technique of observation\nthat was previously mentioned to observe how users go about their work. This includes the\nprocess they go through as well as the current software that they use (if that software is\nrelevant to the one being built). Through this technique, you can glean what works for the\nusers, and just as importantly, what does not work. Other requirement elicitation techniques, such as interviews where you can ask questions,\ncan provide information regarding usability as well. The results of requirements elicitation\ncan be used to create requirements related to usability. Usability testing\nOnce a working version of the software is available for testing, usability testing should\ntake place. Some of the same techniques used for requirements elicitation can also be used\nto get usability feedback. Arrange for users to operate the software and perform tasks with\nit while observing what works well and what pain points they may be having. Use\ninterviews and focus groups to get feedback from users on what they liked and did not like\nabout the software. Appealing visual design\nThe aesthetics, or the visual appearance, of the software system, is a component of\nusability. The appearance of the application can have a great effect on users, so it is worth\nspending time ensuring that the application is designed to be visually appealing to users. For web applications, there are a number of design choices that can be made to increase\nusability. When designing web applications, consider the following:\nPlace emphasis on readability, including using headers, proper spacing, readable\nfonts, attractive colors, and appropriately formatted text\nMake sure the page layouts for all pages are well thought out\nKeep content concise by avoiding excessive text\nEnsure navigation and menus are not overly complicated to avoid making it\ndifficult for users to find what they want\nEnsure that any links are not broken", "domains": ["Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 118", "position": 118, "chunk_type": "semantic", "token_estimate": 331}
{"text": "Software Quality Attributes: Chapter 4\n[ 90 ]\nProvide tooltips, such as using the title attribute on hyperlinks so that helpful\npop-up text will let users know more about a link\nBe consistent throughout the application with things such as colors, icons, fonts,\nand captions/terms\nDisplay progress indicators when the user has to wait for a response, such as a\nwait animation or a percentage complete indicator\nThe following are some things to consider when designing user interfaces for Windows\ndesktop applications:\nDesign with proper spacing of controls so that they are not too close or too far\nfrom each other\nSize windows and controls such as buttons, dropdowns, grids, and list views\nappropriately based on their expected content\nIf a window is made to be resizable, and content is truncated, make sure that\nmore content is shown as the window is made larger\nConsider setting a minimum window size if content is no longer usable at a\nparticular size\nLabel every control and every group of controls\nMake sure that the tab order of the controls on a window is correct\nDesignate shortcut keys (a single key or key combination) that will allow users to\nquickly perform frequently executed commands\nAssign access keys (alphanumeric keys used with the Alt key) so that users can\nquickly activate a command\nEnsure proper capitalization in window headers, controls, labels, and content\nProvide helpful tooltips\nBe consistent throughout the application with things such as colors, icons, fonts,\nand captions/terms\nProvide the user with feedback on system status when it is busy with a long-\nrunning process, such as showing a progress bar\nThe application should display informative messages to the user, such as\nconfirmation, warning, and error messages, when appropriate", "domains": ["Design Patterns", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 119", "position": 119, "chunk_type": "semantic", "token_estimate": 287}
{"text": "Software Quality Attributes: Chapter 4\n[ 91 ]\nProviding a good help system\nDesigning a system that is highly usable also means that it should be designed to allow\nusers to easily learn how to use it. A thorough and up-to-date help system makes an\napplication easier to learn. Whether you are developing a desktop application or a web-\nbased one, ensure that you have a help system that is extensive enough for the given\napplication so that it will be of use to people. Depending on the software, who is using it,\nand how it is being used, an overall help system may include tooltips, online help, product\nmanuals, tutorials, frequently asked questions (FAQs), a knowledge base/discussion\nforum, training events, and ways to submit support cases. This is not to say that a help system is the primary way of making an application usable. The application should be intuitive and easy to use based on other factors, but an\nappropriate help system does supplement the overall effort to increase learnability and\nusability. Software must be useful, and not just usable\nKeep in mind that, regardless of how usable the software is, it must have utility. For a\nsystem to have utility, it must provide the functionality that its users need. Being usable is\ngreat, but in order to be useful, it must also have utility. Users must be able to accomplish\ntheir goals and the software must serve its purpose. Is the software system meeting the needs of the users? Is it useful to them? Even if the\nsoftware system provides a nice visual experience, without usefulness it will not be a good\nuser experience. Availability\nAvailability describes the degree to which the software system will work as required when\nit is needed. It is the probability that the software system is operating properly when\nneeded by a user and is not experiencing unplanned downtime due to failure and repair. It is typically measured in terms of nines, as in how many nines represent the availability\nlevel (99.9%, 99.99%, or 99.999%).", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 120", "position": 120, "chunk_type": "semantic", "token_estimate": 342}
{"text": "Software Quality Attributes: Chapter 4\n[ 92 ]\nCalculating availability based on time\nAvailability can be calculated with the following formula:\nMTBF is the mean time between failures, which is the average time between two failures of\nthe system. MTTR is the mean time to repair, which is the average time to troubleshoot and\nrepair the system back to its operational state. Only unplanned outages are considered\nwhen calculating system downtime. To achieve high availability, which is considered five nines, or 99.999% availability, your\nsoftware system can only be down for 5 minutes and 15 seconds or less during the course of\na whole year! The following table shows some sample availability calculations:\nAvailability\nDowntime per year\nDowntime per month\nDowntime per week\n99.0%\n3.65 days\n7.2 hours\n1.68 hours\n99.9%\n8.76 hours\n43.2 minutes\n10.1 minutes\n99.99%\n52.6 minutes\n4.32 minutes\n60.5 seconds\n99.999%\n5.26 minutes\n25.9 seconds\n6.05 seconds\nExtreme levels of availability are not necessarily ideal because, at a certain point, you will\nexperience diminishing returns. Consider that each additional nine requires a whole\nmagnitude improvement of availability. This comes at a cost, in terms of time and money. This can have a negative effect on the software because it will limit how much time and\neffort can be spent on other things, such as releasing new features. Also, consider the fact that a user won't be able to tell the difference between 99.99% and\n99.999% availability. For example, in the case of a web or mobile application, there are other\nfactors such as the user's device and the network that the user is on, which may not be as\nreliable as the software system. The software architect and the stakeholders need to weigh the benefits of higher availability\nlevels against the increased costs of providing them to determine which level suits them\nbest.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 121", "position": 121, "chunk_type": "semantic", "token_estimate": 307}
{"text": "Software Quality Attributes: Chapter 4\n[ 93 ]\nCalculating availability based on request success\nrate\nIf your software is globally distributed, perhaps by taking advantage of cloud services\nprovided by companies such as Amazon, Microsoft, or Google, you may be experiencing\nvery high availability, such that at least some traffic is being served at any given time. As a\nresult, calculating availability based on time as we did previously may not be very\nmeaningful. An alternate calculation would be to base availability on the request success rate. This can\nbe calculated as follows:\nYour software system will have a variety of requests, some of which are more important\nthan others, but looking at all requests should provide a good estimation of unplanned\ndowntime. Availability calculations based on time take into consideration the total failure duration,\nincluding the time to repair the software system. It doesn't distinguish between, for\nexample, two failures of 30 minutes and one failure of an hour, but a calculation based on\nthe request success rate does. Considering both of these metrics provides insights into\ndifferent facets of your software's availability, which gives you a more complete picture. Faults, errors, and failures\nA part of availability involves how a software system can handle and overcome faults so\nthat the duration of an unplanned outage does not exceed the specified value over a\nparticular amount of time. A system fault is a characteristic of a software system that can lead, either on its own or in\nconjunction with other faults, to a system failure. A system fault exists somewhere in the\ncode. An error is an erroneous state of the software system caused by one or more faults. A\nsystem failure is an event experienced by a user in which the software system does not\nbehave as expected. System failures are caused by one or more faults. In order to handle and overcome faults and prevent failures, we can attempt to detect\nfaults, recover from faults, or prevent faults.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 122", "position": 122, "chunk_type": "semantic", "token_estimate": 331}
{"text": "Software Quality Attributes: Chapter 4\n[ 94 ]\nDetecting faults\nBefore a software system can recover from a fault, it must first detect a fault. The following\nare some techniques that can be used to detect faults. Ping/echo reply\nIn this method of detecting faults, a component, acting as a system monitor, sends an\nInternet Control Message Protocol (ICMP) echo request to another component (it pings\nthe component) and waits for an ICMP echo reply. If the target does not respond to the\nping in a predefined amount of time, the component acting as the system monitor reports\nthat the other component has failed. Heartbeat\nThis technique requires one component to periodically send a message (heartbeat) to\nindicate that it is operating normally. If the listening component does not receive the\nheartbeat message within the predefined amount of time, it determines that a system fault\nhas occurred and takes the appropriate action. Timestamp\nThis strategy of detecting faults focuses on catching an incorrect sequence of events. By\nusing a timestamp, or even just a sequence of numbers, faults can be reported if the\nsequence is in the incorrect order. Voting\nA voting system can be used to report faults. One such approach is triple modular\nredundancy (TMR), sometimes called triple-mode redundancy. It utilizes three\ncomponents to perform the same process. Voting logic will compare the results to produce a single output. If all three components\nproduce the same output, then everything worked as expected. If two out of three\ncomponents agree, they can correct the fault by outvoting the third component. A fault is\nthen reported on the third component.", "domains": ["Design Patterns", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 123", "position": 123, "chunk_type": "semantic", "token_estimate": 270}
{"text": "Software Quality Attributes: Chapter 4\n[ 95 ]\nSanity test/sanity checking\nThis fault detection technique uses a test to evaluate if a result from a process is reasonable\nand possible. If the test determines that the result is not, it can report a fault. Such a test is a\nsimple one, and not intended to test for a wide variety of issues. It is used to detect obvious\nfaults in a quick way. Condition monitoring\nCondition monitoring is a method whereby conditions are checked in a software system in\norder to detect a fault or a situation in which a fault may develop. When a problem is\ndetected, the fault can be handled. One benefit of condition monitoring is that it may detect a problem before a fault occurs, or\nat least before it becomes an even bigger issue. Self-tests\nSoftware systems can incorporate self-tests, allowing their components the ability to detect\nfaults. Such a test is sometimes referred to as a built-in self-test (BIST) or a built-in test\n(BIT). If this technique is used, software components or combinations of components are \ndeveloped with the logic that gives them the ability to test themselves for correct operation. Self-tests can be initiated by the component they are testing or by a separate component\nacting as a system monitor. If a test fails, the fault is reported. Recovering from faults\nOnce faults are detected, a strategy can be undertaken to recover from them. The following\nare some ways to recover from faults. Exception handling\nOnce an exception is detected, the software system can utilize an exception handling\nmechanism. How an exception is handled is dependent on the language, framework, and\ntype of exception, but it can involve anything from simply returning an error code to\nreturning an instance of some error class that contains helpful information about the\nexception, such as an error code, a message, and a stack trace.", "domains": ["Design Patterns", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 124", "position": 124, "chunk_type": "semantic", "token_estimate": 318}
{"text": "Software Quality Attributes: Chapter 4\n[ 96 ]\nThe software can use exception information to potentially recover from the fault, such as\ncorrecting the cause of the exception and retrying the operation, and/or showing the user a\nuser-friendly message about the problem. Retry strategy\nTransient faults are errors that occur due to some temporary conditions, such as network\nconnectivity issues, temporary unavailability of a service, timeout of a service, or\ninfrastructure-level faults. Retry strategies can be used to attempt to retry an operation when it encounters a transient\nfault. A retry policy can be tailored based on the nature of the component that caused the\nerror, which can dictate things such as how many retry attempts are made, and the length\nof time to wait in between attempts. Some common retry interval types include:\nRegular intervals: The software system waits the same amount of time in\nbetween each interval attempt. Incremental intervals: The software system waits a short amount of time before\nthe first retry, and then incrementally increases the amount of time before each\nsubsequent retry. For example, retry attempts may occur at 2 seconds, 6 seconds,\n12 seconds, and so on. Exponential backoff: The software system waits a short amount of time before\nthe first retry, and then exponentially increases the amount of time before each\nsubsequent retry. Immediate retry: A retry can take place immediately. However, there should not\nbe multiple immediate retry attempts. If a single immediate retry attempt fails,\nthen any subsequent attempts should use one of the other interval types. Randomization: Any of the aforementioned interval types can be used in\nconjunction with randomization to prevent retry attempts from being sent at the\nsame time from multiple instances of a client. Varying levels of redundancy\nOne way to recover from faults and achieve availability is to have a failover mechanism. There are varying levels of redundancy that can be provided.", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 125", "position": 125, "chunk_type": "semantic", "token_estimate": 316}
{"text": "Software Quality Attributes: Chapter 4\n[ 97 ]\nIn active/hot spare environments, each component has another one that performs the same\nprocesses with the same inputs, so that, if one fails, the other component can take over at\nany time. Failover is generally transparent because the recovery time is nearly\ninstantaneous. A passive/warm spare environment is one in which only active components perform\nprocesses from inputs, but the active components provide the backup components with\nperiodic state updates. A passive redundancy approach is not as highly available as an\nactive/hot spare, but it is less expensive to operate. Depending on how frequent state updates\nare provided to the backup, recovery time can be in terms of seconds or minutes. With a cold spare approach, redundant components are kept out of service until they are\nneeded. The component that failed will be out of operation until it can be repaired or\nreplaced. Using a cold spare takes more time to bring the redundant component into\noperation as compared with hot or warm spares. Recovery time could be a few hours. Rollback\nThe rollback technique returns the system to a checkpoint in which the system is known to\nhave been in a good state. This does require that checkpoints be persisted in some manner. Once the system is rolled back, regular operations can continue once again. This approach\ncan be used with active or passive redundancy so that a failed component can be made\nactive again after a rollback. Graceful degradation\nGraceful degradation is a fault recovery approach in which some functionality is made\nunavailable in order to prevent the entire system from becoming unusable. If failures are\npreventing the entire software system from being operational, then some functions can be\ndropped in favor of others. For example, if a system is running low on resources, the most\ncritical functions can be kept working while others are shut down. Ignoring faulty behavior\nAnother approach to fault handling is to simply ignore the fault. If a particular type of fault\nfrom a certain source is known to be one that can be ignored, then the system may simply\ndisregard the fault and processing can continue.", "domains": ["Design Patterns", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 126", "position": 126, "chunk_type": "semantic", "token_estimate": 362}
{"text": "Software Quality Attributes: Chapter 4\n[ 98 ]\nPreventing faults\nAn alternative to detecting and recovering from faults is to prevent them from occurring in\nthe first place. There are a number of strategies that can be employed to prevent faults. Removal from service\nOne way to prevent faults is to remove the offending component from service. A software\ncomponent can be removed from operation in anticipation of faults. It can then be restored\nwhen it is operational again. Transactions\nTransactions can be used to prevent faults. Multiple steps in a process can be bundled\ntogether in a transaction, so that if one step fails then the entire bundle can be undone. This\napproach can prevent data from being saved in an incorrect state, or prevent race\nconditions when more than one process attempts to access or change the same data. Increasing competence sets\nThe competence set for a given component in a software system determines what states and\nconditions it can handle. A component's level of fault tolerance and the cases that it is\ndesigned to handle depend on the collection of logic that make up its competence set. A\ncomponent can be modified so that it handles more cases, reducing the exceptions that are\nthrown. For example, if a method does not handle a null value for a particular parameter, an\nexception may be thrown, leading to a system failure. The code could be modified to\nhandle null values, increasing the competence set of the software and preventing a fault. Exception prevention\nThis strategy involves writing the code in such a way that exceptions can be prevented. For\nexample, methods could perform bounds checking on arguments and handle them\ngracefully, which would prevent exceptions from being thrown.", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 127", "position": 127, "chunk_type": "semantic", "token_estimate": 289}
{"text": "Software Quality Attributes: Chapter 4\n[ 99 ]\nPortability\nPortability describes how efficiently and effectively a software system can be transferred\nfrom one environment to another. Some factors that influence portability are adaptability,\ninstallability, and replaceability. Adaptability\nAdaptability is the degree to which a software system can be adapted for different\nenvironments, such as different hardware, operating systems, or other operational\ncharacteristics. In order to test for adaptability, functional testing must be conducted to ensure that the\nsoftware system can perform all of its tasks in all target environments. Installability\nInstallability is the ease with which a software system can be installed or uninstalled in a\nspecified environment. The installation process should be easy to understand, and may also\nprovide configuration options as part of the installation. For example, the process may\nprompt the user to configure things such as the location for the installation, database\nconnection information, and other configuration options for the software. Another aspect of installability is how the software handles an update/upgrade process. The software system should provide a friendly and usable update process so that a user can\nupgrade the software to a newer version. The update process should clean up the older\nversion, possibly by automatically uninstalling it first. Installability also encompasses any functionality to uninstall the application. When a\nsoftware system is uninstalled, it should remove the software system and any related\ncomponents entirely from the machine. It may also delete any folders that are no longer\nneeded. A feature you may consider including that increases installability is providing users with\nthe ability to cancel an install, update, or uninstall process once it has been initiated. Ideally, the process will clean-up after itself properly if the user aborts it or if it fails.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 128", "position": 128, "chunk_type": "semantic", "token_estimate": 290}
{"text": "Software Quality Attributes: Chapter 4\n[ 100 ]\nInstallability testing should ensure that all of the functionality related to installing,\nupdating, and uninstalling the software works properly and without error. It should test\nthat any configuration options work properly, and check how the process handles\nsituations such as when there is an insufficient amount of disk space available to install the\nsoftware or an update. Replaceability\nReplaceability is the capability of a software system to replace another software system for\nthe same purpose, in the same environment. This may entail only replacing one or more\nsoftware components within a software system. A good example of replaceability is when a software system is upgraded. Customers desire\na system that is highly replaceable so that an upgrade to a newer version of the software\nfrom an older version goes smoothly. Testing for replaceability should confirm that, after any replacement, the software system\nstill works properly. All relevant functionality should be tested to verify that it works as\nexpected. Internationalization and localization\nInternationalization and localization are part of portability and consist of adapting a\nsoftware system for use with different languages, taking into consideration cultural\ndifferences, and meeting other requirements for various locales. As a software architect, you will need to know if there are internationalization/localization\nrequirements, so that you may consider them in the design. Even if there are no\nrequirements currently, if your organization has business goals for international business\nexpansion, the software may eventually be used in different areas around the world. If that\nis the case, you will want to consider that fact in your initial design, as it is more difficult to\nprovide this capability later. Internationalization is designing software in such a way as to enable localization and\nsupport for different languages, cultures, and regions. Internationalization in this context is\noften abbreviated to i18n, where 18 is the number of letters between i and n in the word\ninternationalization. Sometimes the word globalization is used instead of\ninternationalization. When that is the case, it is sometimes abbreviated as g11n.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 129", "position": 129, "chunk_type": "semantic", "token_estimate": 342}
{"text": "Software Quality Attributes: Chapter 4\n[ 101 ]\nAs part of internationalization, the software should be designed so that adapting it to\ndifferent locales later will not require modifications to the code. For example, all\ntranslatable strings, such as captions, messages, tooltips, and other content, should be\nplaced in resource files. The code can then reference those resource files when it needs that\ncontent. Later, while localizing the application, these strings can be translated for different\nlocales without having to change any code. Unicode should be used as the character set for applications that require\ninternationalization, as it supports all of the characters from any written language\nworldwide. If a development team were to use different character sets for different\nlanguages, localizing an application would be much more difficult. Unicode will facilitate\ndisplaying characters from any language that needs to be supported, and because there is a\nunique encoding for each character, data can be sorted, searched, and manipulated in the\nsame way. Once an application is ready to be localized, the following are some things which need to be\nconsidered:\nTranslating captions, messages, tooltips, and other content for another language\nEnsuring the user interface can accommodate words properly after they are\ntranslated (for example, spacing and the wrapping of words could be different)\nVariation in the spelling of words, even for countries which use the same\nlanguage, such as aeroplane/airplane, capitalise/capitalize, and\norganisations/organizations\nLanguage and cultural differences may require content to be rewritten to ensure\nthat nothing will be misunderstood or taken the wrong way\nWhether text is read from left to right or right to left\nDifferences in telephone numbers\nDifferences in weights and measures\nDifferent date/time formats, such as whether 6/1/2019 should be interpreted as\nJune 1st or January 6th\nTime zone differences (UTC)\nFormatting numbers, such as decimal mark symbols, digit group separators, and\ndigit groupings\nDifferent currencies, such as currency symbols and whether the currency symbol\nshould appear before or after the monetary value\nAny legal requirements", "domains": ["Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 130", "position": 130, "chunk_type": "semantic", "token_estimate": 332}
{"text": "Software Quality Attributes: Chapter 4\n[ 102 ]\nMaintaining portability\nOnce a software system is made portable for different environments, it is important that\nportability is maintained. As changes are made to the software as part of maintenance, it is\npossible those modifications could affect portability. While making modifications, consideration must be given as to whether or not it will affect\nportability. During testing, it must be verified that any changes made did not adversely\naffect any requirements for portability. Interoperability\nInteroperability is the degree to which a software system can exchange and use\ninformation from another software system. In order for two systems to interoperate in a\nuseful manner, they must be capable of communicating with each other (syntactic\ninteroperability) as well as be able to interpret the information exchanged in a meaningful\nand correct way (semantic interoperability). The term interoperability is related to, and sometimes used interchangeably with, the term\nintegration. Challenges with interoperability\nThere are a number of difficulties in providing interoperability between existing or new\nsoftware systems. When dealing with requirements related to interoperability, keep in\nmind the following potential pitfalls:\nEven when a software system follows a particular standard when developing\ninteroperability with another software system, the desired level of\ninteroperability may not be met due to the two systems interpreting the\nstandards specifications in different ways. Interoperability between two software systems is sometimes scaled back or put\non hold because of the involvement of an existing legacy system, either directly\nor indirectly, when there is a desire to maintain compatibility with the legacy\nsystem. An organization may not want to make modifications to the legacy\nsystem.", "domains": ["Software Quality Attributes", "Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 131", "position": 131, "chunk_type": "semantic", "token_estimate": 272}
{"text": "Software Quality Attributes: Chapter 4\n[ 103 ]\nTests for interoperability are sometimes insufficient and may miss certain\ninteroperability issues. Software systems are sometimes released even when there are known\ninteroperability issues. Even if interoperability is achieved between two software systems, it can be\ndifficult to maintain as new versions of the two software systems get released. It\nis not uncommon for interoperability to be broken, requiring ongoing\nmaintenance to maintain it. There can be legal issues for interoperability, so be aware of any that may affect\nyour system when interoperating with another one. This may be particularly true\nif the two software systems are owned by different organizations, and even more\nso if they operate in different jurisdictions, such as two different countries. One\nexample of this may be privacy and security laws related to healthcare\ninformation being exchanged. Locating and exchanging information with\nanother system\nTo achieve interoperability between two software systems, the consumer system must be\nable to locate the other and the two systems must be able to exchange information in a\nsemantically meaningful way. The consumer system may or may not need to discover the other system at runtime. If it\ndoes need to happen at runtime, a known directory service is searched by one or more\nattributes (for example, a URL) in order to locate the system. It should be noted that there\ncan be n levels of indirection during the location process. For example, once one location is\nfound, it may lead to another location, and so on, until the service is discovered. Once the system is located, the response must be handled. The service will send back a\nresponse to the requester, forward a response on to another system, or broadcast a response\nto any interested services that may be listening. In order for the two systems to exchange information, interoperability tactics, such as\norchestration and the management of interfaces, are used. Orchestration involves directing\nand managing the services that are called and ensuring that the necessary steps take place\nin the correct sequence.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 132", "position": 132, "chunk_type": "semantic", "token_estimate": 342}
{"text": "Software Quality Attributes: Chapter 4\n[ 104 ]\nIn order to manage interfaces to facilitate information exchange, capabilities may be added\nor removed for the purposes of interoperability. A capability may be added to an interface\nspecifically to enable data exchange. An example of this may be functionality related to the\nbuffering of data. In addition, capabilities may be removed from an interface if we do not\nwant those capabilities to be exposed to other systems. For example, functionality to delete\ncertain data may be removed from the interface if we do not want any external clients to\nhave that capability. Interoperability standards\nWhen interoperability involves software systems from different organizations, one of the\nways to achieve interoperability is to follow a common standard. The standard may be\ncreated by the two organizations working together as part of a partnership, or it may be an\nexisting industry, national, international, or open standard. In some cases, an entire industry will work together to come up with a standard. The\nstandard may be one that is newly created by the industry, or some existing standard might\nbe used as a starting point. The interoperability standard that is agreed upon can then be\nfollowed during the course of development and used during testing. Another aspect of standards that may facilitate interoperability is using a common\ntechnology, such as agreeing on the data interchange format and the communication\nprotocol. For example, it may be agreed upon that JavaScript Object Notation (JSON) is\nused as the data interchange format and that Hypertext Transfer Protocol Secure (HTTPS)\nis used as the web transfer protocol. Interoperability testing\nIn addition to unit and system testing the two systems that must interoperate, it is critical to\nperform integration testing between the two systems. Tests must be conducted to ensure\nthat the consumer system can locate the other system and that they can exchange\ninformation properly. The testing environment should be as close to what will be experienced in production as\npossible. Keep in mind that, even if both systems follow a particular standard for\ninteroperability, and even if both systems have passed their own conformance tests based\non those standards, it is not uncommon for there to be issues with interoperability. Some\nissues may not be uncovered until both systems are tested together.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 133", "position": 133, "chunk_type": "semantic", "token_estimate": 384}
{"text": "Software Quality Attributes: Chapter 4\n[ 105 ]\nTestability\nTestability is the degree to which a software system supports testing in its given context. The higher the level of testability, the easier the software system and its components are to\ntest. If a component is not easy to test, it may indicate that the design is not ideal, leading to\nan implementation that is unnecessarily complex. A significant part of the development\ncosts for software systems is related to testing, so if the software architecture can play a role\nin testability there can be noticeable cost benefits. A software system that has a higher level of testability makes it easier to test, which results\nin an increase in both the efficiency as well as the effectiveness of testing. Testing efficiency\nincreases because it takes less time and effort to create and execute tests. Testing\neffectiveness is increased because you are more likely to find existing defects in the\nsoftware system, and you are more likely to find them sooner. Finding defects sooner has a tremendous effect on the software system's overall level of\nquality. Not only is it more likely that defects won't make it into production, but the sooner\na defect is found, the less costly it is to repair. Some factors that influence testability are as follows:\nControllability\nObservability\nIsolability\nAutomatability\nComplexity of the software\nControllability\nControllability represents the level to which it is possible to control the state of the\ncomponent being tested. The component being tested is sometimes referred to as the\nsystem under test (SUT) or the component under test (CUT). Controlling the state of a\ncomponent involves being able to dictate the inputs of the component as well as the level to\nwhich those inputs exercise its capabilities. When designing components, you should seek to increase their controllability, as this\nincreases testability.", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 134", "position": 134, "chunk_type": "semantic", "token_estimate": 308}
{"text": "Software Quality Attributes: Chapter 4\n[ 106 ]\nObservability\nObservability represents the level to which it is possible to observe the state of the\ncomponent being tested. This includes being able to observe inputs and outputs so that we\ncan determine if the component is working correctly. If it is not possible for the testing framework being used to see the inputs and outputs of a\ncomponent, then the component is not observable, and it won't be possible to confirm if the\nresults are correct. When designing components, you should seek to increase their observability, as it is\ndirectly correlated to testability. Isolability\nIsolability is the degree to which a component can be isolated. The goal is to have tests that\ncan focus on specific pieces of functionality (for example, unit tests) that do not have\ndependencies on other components. This allows us to create and execute tests on a\ncompleted component, even if other components are not complete. We want to avoid a situation where we have to write a great deal of code before any of it\ncan be tested since it is desirable to get feedback as quickly as possible. If a problem is\nfound, it will be easier to determine its source and cause if components exhibit isolability. An increase in isolability increases the testability of a component. Automatability\nAutomatability is the level to which a process or action can be automated. If a system\nexhibits automatability, then automated tests can be created and executed for the system. Automated testing utilizes pre-scripted tests that can then be executed automatically. These\ntests can be run at any time, such as before a code check-in or prior to a build taking place. If automated builds are in place, automated tests can be executed as part of that process. Automated tests give you quick feedback on whether a new defect was introduced into the\nsystem. As we just noted, discovering a defect as close to when it was introduced is highly\nbeneficial. When designing a software system, you should seek to increase automatability,\nas the ability to have automated tests increases testability. Automated builds and\nautomated testing will be covered further in Chapter 13, DevOps and Software Architecture.", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 135", "position": 135, "chunk_type": "semantic", "token_estimate": 369}
{"text": "Software Quality Attributes: Chapter 4\n[ 107 ]\nComplexity of the software\nAs is the case with other quality attributes, the complexity of the software plays a role in\ntestability. Reducing dependencies and isolating modules (isolability), decreases the\ncomplexity of the software. When we discussed maintainability, we mentioned that minimizing the complexity of\narchitectural elements can be accomplished by reducing the number of lines of code in\nmodules, increasing cohesion and reducing coupling. Techniques that increase\nmaintainability also increase the controllability of elements. This has a direct and positive\nimpact on testability. In addition to the structural complexity of components, there is behavioral complexity. Non-\ndeterminism is one type of behavioral complexity. If some algorithm we are testing is non-\ndeterministic, then it means that given the same inputs, it can exhibit different behavior\nwith each execution. This is in contrast with a deterministic algorithm, which, if given the\nsame inputs, will behave in the same way each time it is executed. Non-deterministic code is more difficult to test, so the first step would be to identify any\nareas in the software system that are non-deterministic. For such areas, the ideal approach,\nif possible, would be to refactor the logic to make it deterministic, or allow the logic to be\nmocked as deterministic. A simple example often seen in practice involves tests and the current time. In most\nprogramming languages, there is a way to get the current date and time of day. Doing so\ncan tightly couple your code to the host environment. It is non-deterministic because each\ntime we make a call to get the current date and time, the result will be different. If there is\nlogic that uses this value in a way that would affect a unit test, your test results will vary\ndue to the non-deterministic nature of the result. This logic can be refactored to wrap the\ncall to get the current date and time into some other class. Doing so will allow you to inject\nthis dependency into the class that needs this logic. At that point, a testing framework can\nmock the dependency, specifying what the result will be when the call to get the current\ndate and time is made. However, there are cases where it will not be possible to refactor logic to make it\ndeterministic, or to make it so that the logic can be mocked as deterministic.", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 136", "position": 136, "chunk_type": "semantic", "token_estimate": 400}
{"text": "Software Quality Attributes: At that point, a testing framework can\nmock the dependency, specifying what the result will be when the call to get the current\ndate and time is made. However, there are cases where it will not be possible to refactor logic to make it\ndeterministic, or to make it so that the logic can be mocked as deterministic. An example of\nthis is if you had a multi-threaded system that is required to interact with an external\ncomponent, and that external component raises events in a non-deterministic manner.", "domains": ["Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 136", "position": 136, "chunk_type": "semantic", "token_estimate": 91}
{"text": "Software Quality Attributes: Chapter 4\n[ 108 ]\nImportance of test documentation\nAn aspect of testability is the ease with which tests can be executed, including testers who\ndid not originally write the test cases. The reusability and maintainability of tests, part of\ntestability, are improved with the existence of quality test documentation. When dealing\nwith a large system, it can be difficult to remember all of the business rules and alternative\npaths related to a particular scenario. With agile software development methodologies, excessive documentation is viewed as a\nproject risk rather than something that will reduce risk. Project teams aim to be efficient\nwith documentation and produce only as much as is needed. In addition, automated tests\nshould be fairly self-documenting through things such as the test method names, test class\nnames, and comments. However, there are many different types of testing beyond automated tests that need to be\nconducted, and some can be quite involved. There can be complexity in the business logic,\nin the inputs and outputs of tests, in the data that is needed to set up particular tests, and to\nensure adequate coverage on a variety of business scenarios. Some form of test\ndocumentation can really be of benefit. Resources can, and more than likely will, change over time. An employee may leave an\norganization, or testers simply get different assignments over time and a piece of\nfunctionality isn't necessarily owned by a single tester. Testing documentation allows\ndifferent testers to use the test case over time and it facilitates such transitions so that\nquality is not sacrificed. To add to that point, it is increasingly common for development departments to outsource\ndevelopment work, including testing. Not only may you have to deal with outsourced\nresources changing, but testing tasks may need to move from internal resources to\noutsourced resources, and vice versa. Documentation of artifacts such as designs and tests is very useful so that new resources\ncan get up to speed quickly. It's basically the concept of tribal knowledge and being able to\npass on things that have been previously learned to others. Projects with good\ndocumentation typically demonstrate a higher level of organizational maturity and\ncontribute to the project's overall success.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 137", "position": 137, "chunk_type": "semantic", "token_estimate": 369}
{"text": "Software Quality Attributes: Chapter 4\n[ 109 ]\nWhat makes a good tester? While we are on the topic of testability, I wanted to touch upon the characteristics of a good\ntester. As a software architect, you may be providing guidance to testers. Good testers\nperform their tasks with a greater level of efficiency, effectiveness, and thoroughness. Testers that operate efficiently do so in an organized way and their efficiency allows them\nto complete test cases faster, and find defects quicker. They are not just using the software\nand happening upon bugs, but using a systematic approach to finding defects. Testers who achieve a greater level of effectiveness do so because they focus particular\nattention on problems that users will care about in the released version. Part of the process\nof finding defects involves documenting the problem and giving them to developers so that\nthey can be fixed. Exceptional testers are thorough in their work. They plan their testing activities and they\ndocument their test cases. Testers attempt to be as thorough as possible, exercising all\naspects of a software's functionality and considering a variety of scenarios. They use a\nvariety of inputs and test things such as edge cases, which are scenarios that involve\nextreme input values, such as those at the minimum or maximum of a range of possibilities. Another characteristic of a good tester is the ability to understand the software's behavior,\nenvironments, and capabilities. Because a tester cannot test all possible inputs and\nscenarios, an effective software tester must understand the software's behavior. This\nincludes what the software is doing, and what things could potentially cause it to fail. Software operates in an environment that interacts with a variety of inputs and outputs. There is the user interface (UI) that people interact with, but there is also the kernel\ninterface (operating system), software interfaces (for example, in things such as database\nsystems), and file system interfaces (for example, errors related to accessing, reading, and\nwriting to files). A good tester must consider the totality of the environment in which the\nsoftware operates. Finally, the tester must understand the software's capabilities. Although there may be a\nlimited number of basic capabilities (for example, accepting input, producing output,\nstoring data, and executing calculations), these capabilities can be combined into complex\nfunctionality.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 138", "position": 138, "chunk_type": "semantic", "token_estimate": 383}
{"text": "Software Quality Attributes: Chapter 4\n[ 110 ]\nSummary\nSoftware architects should pay particular attention to quality attributes, as they influence\nthe architecture of software. The software must meet the designated quality attributes, so\nidentifying and specifying them in a way that they can be measured and tested is\nimportant. Although some stakeholders might focus on functionality, non-functional requirements\nsuch as quality attributes are a principal factor in whether or not a software system will be a\nsuccess. Some of the more important quality attributes include maintainability, usability,\navailability, portability, interoperability, and testability. Now that we understand more about quality attributes and the fact that they influence\nsoftware architecture, we are ready to explore software architecture design. Architecture\ndesign involves making decisions to create a solution for functional requirements, quality\nattributes, and constraints. You will learn what is involved with architecture design, about\ndesign principles that you can leverage in your designs, and about architecture design\nprocesses.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 139", "position": 139, "chunk_type": "semantic", "token_estimate": 157}
{"text": "Designing Software Architectures: Chapter 5\n[ 112 ]\nUsing a systematic approach to software architecture design\nAttribute-driven design (ADD)\nMicrosoft's technique for architecture and design\nArchitecture-centric design method (ACDM)\nArchitecture development method (ADM)\nTracking the progress of the software architecture's design\nSoftware architecture design\nSoftware architecture design involves making decisions in order to satisfy functional\nrequirements, quality attributes, and constraints. It is a problem-solving process that leads\nto the creation of an architecture design. Software architecture design comprises defining the structures that will make up the\nsolution and documenting them. The structures of a software system are made up of\nelements, and the relationships between the elements. The properties and behaviors of the\nelements that are publicly exposed, through an interface, should be identified as part of the\ndesign. The design allows you to understand how the elements behave and interact with\neach other. Private implementations of the elements are not architecturally significant and\nneed not be considered as part of the design. The software architecture design serves as technical guidance for development and\ntypically occurs iteratively until the initial architecture is at a point where the development\nteam can begin their work. Once an initial architecture is designed, it can continue to evolve\nas development is taking place. For example, additional design iterations may occur to\nrefactor an architecture to fulfill new requirements or quality attributes. Software architecture design is a creative process. Software architects have the privilege of\ncoming up with solutions to complex problems and can use creativity to do it. It can be one\nof the most fun and rewarding parts of a software project. Making design decisions\nThe set of software requirements consists of a series of design issues that must be solved. For each of these design issues, such as providing certain business functionality, respecting\na particular constraint, meeting performance objectives, or providing a certain level of\navailability, there may be numerous ways to solve the problem. You will need to consider\nthe strengths and weaknesses of these alternatives in order to select the most appropriate\nchoice.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 141", "position": 141, "chunk_type": "semantic", "token_estimate": 343}
{"text": "Designing Software Architectures: Chapter 5\n[ 113 ]\nA large part of software architecture design is making design decisions to resolve issues so\nthat a solution can be implemented. As the software architect, you will be leading the\ndecision-making process. It is a collaborative process, and usually the best designs incorporate knowledge and\nfeedback from multiple people, such as other software architects and experienced\ndevelopers. Joint designs and reviewing the architecture with others is beneficial in coming\nup with a solid software architecture design. The result of the design is a set of decisions that shape your software architecture. The\ndesign is documented in artifacts that can be used for the implementation of a solution. Software architects should keep in mind that a decision that is made for one design issue\nmay affect another one. This is why software architecture design is an iterative process. Each decision for a design issue may not be optimal for another issue, but the overall\nsolution must be acceptable by satisfying all of the requirements. Perfect is the enemy of good, an aphorism that has its origins in the thoughts of the French\nphilosopher, Voltaire, and others, is applicable to software architecture design. A\ncompleted design may not be perfect, as there will be conflicting requirements that need to\nbe met, and trade-offs made in order to meet them. If the design satisfies all of the\nrequirements, then it is a good one, even if it is not perfect. Software architecture design terms\nBefore we go any further, let's define some of the terms that we will be using while\ndetailing the process of software architecture design. These terms can vary, depending on\nthe organization and the team. Regardless of the terms used, the important thing is that\nthey are used consistently by the team members and that they are understood by all team\nmembers. For the purposes of this book, we'll be using the terms structure, element, system,\nsubsystem, module, and component. Structure\nStructures are groupings of, and relations between, elements. Anything that is complex and\nmade up of elements can be referred to as a structure. We previously defined software\narchitecture, in part, by saying it is made up of the structures, their elements, and the\nrelationships of those elements with each other.", "domains": ["Design Principles", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 142", "position": 142, "chunk_type": "semantic", "token_estimate": 381}
{"text": "Designing Software Architectures: Chapter 5\n[ 114 ]\nElement\nAn element is a generic term that can be used to represent any of the following terms:\nsystem, subsystem, module, or component. If we want to refer to pieces of a software\napplication in a general way, we can refer to them as elements. System\nThe software system represents the entire software project, including all of its subsystems. A system consists of one or more subsystems. It is the highest level of abstraction in a\nsoftware architecture design. Subsystem\nSubsystems are logical groupings of elements that make up a larger system. The\nsubsystems can be created in a variety of ways, including partitioning a system by\nfunctionality. Although they do not have to be, subsystems can represent standalone software\napplications. An overall software system may be composed of multiple subsystems, and\nany number of them might be a standalone application. These standalone applications can\nbe external applications that were not developed by the organization. Organizing a larger software system into subsystems lowers complexity, and allows for\nsoftware development to be better managed. In some cases, one or more development\nteams may be formed for each subsystem. Each subsystem is made up of one or more\nmodules. Module\nModules, like subsystems, are logical groupings of elements. Each module is contained\nwithin a subsystem and consists of other modules and/or components. They are typically\nfocused on a single logical area of responsibility. Development teams assigned to a particular subsystem will be responsible for the modules\nthat make up that subsystem.", "domains": ["Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 143", "position": 143, "chunk_type": "semantic", "token_estimate": 257}
{"text": "Designing Software Architectures: Chapter 5\n[ 115 ]\nComponent\nComponents are execution units that represent some well-defined functionality. They\ntypically encapsulate their implementation and expose their properties and behaviors\nthrough an interface. Components are the smallest level of abstraction and typically have a relatively small\nscope. Components can be grouped together to form more complex elements, such as\nmodules. The importance of software architecture\ndesign\nA software architecture is the foundation of a software system. The design of the\narchitecture is significant to the quality and long-term success of the software. A proper\ndesign determines whether the requirements and quality attributes can be satisfied. There are a number of reasons why a good software architecture design is critical to\nbuilding useful software. In this section, we will explore the following reasons:\nSoftware architecture design is when key decisions are made regarding the\narchitecture. Avoiding design decisions can incur technical debt. A software architecture design communicates the architecture to others. The design provides guidance to the developers. The impact of the software architecture design is not limited to technical\nconcerns. It also influences the non-technical parts of the project. Making key decisions\nIt is during software architecture design that key decisions are made that will determine\nwhether requirements, including quality attributes, can be satisfied. Software architecture\nenables or inhibits quality attributes, so design decisions play a large role in whether or not\nthey can be met. Some of the earliest decisions are made during design. If these decisions need to change, it\nis easier and less costly to change architectural decisions early, before coding has even\nbegun, than to make changes later.", "domains": ["Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 144", "position": 144, "chunk_type": "semantic", "token_estimate": 271}
{"text": "Designing Software Architectures: Chapter 5\n[ 116 ]\nAvoiding design decisions can incur technical\ndebt\nCritical decisions are made during the design, and for that reason, there is a cost to either\ndelaying a design decision or not making one at all. Delaying or avoiding certain design\ndecisions can incur technical debt. Technical debt is similar to financial debt. In the context of design, it is the cost and effort\nfor the additional work that will be necessary later due to decisions that are made now, or\nbecause decisions have not been made. In addition to delaying or avoiding decisions, a decision may be made knowing that it will\ncost some amount of technical debt. As a software architect, you may decide to take an\neasier route to a solution, incurring technical debt, even though there is a better solution. As\nis the case with financial debt, technical debt is not always a bad thing. Sometimes, you will\nwant to pay a debt later in order to get something now. For example, designing a better\nlong-term solution may take more time and effort, and you may decide on a solution that\ntakes less time in order to get the software in production to take advantage of a market\nopportunity. It can be difficult to measure the impact of technical debt accurately. Keep in mind that in\naddition to the time and effort that might be required later to make up for a decision that is\nmade or avoided now, technical debt can have other negative repercussions. For example, a\ndesign that is not optimal, leading to lower levels of modifiability and extensibility, can\nhinder the team's ability to deliver other functionality. This is an additional cost that should\nbe added to the technical debt. The software architect needs to take all of these factors into consideration when deciding\nwhether or not to incur a technical debt. Communicating the architecture to others\nThe results of the architecture design allows you to communicate the software architecture\nto others. There will be a variety of people who will potentially be interested in the design\nof the architecture. The design will also improve cost and effort estimates since it influences what tasks will be\nrequired for implementation. Understanding the nature of the work that lies ahead and\nwhat types of tasks will be needed to complete the project will assist project managers with\ntheir planning.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 145", "position": 145, "chunk_type": "semantic", "token_estimate": 401}
{"text": "Designing Software Architectures: Chapter 5\n[ 117 ]\nProviding guidance to developers\nA software architecture design provides guidance to the development team, by steering\nimplementation choices as well as providing training on the technical details of the project. The design imposes implementation constraints, making it important for coding tasks. Knowing the software architecture design helps developers be aware of the implementation\nchoices available to them, and minimizes the possibility of making an incorrect\nimplementation decision. It can also be used as training for developers. At the start of the project, the development\nteam will need to understand the design decisions that have been made, and the structures\nthat have been designed. Creating detailed designs for components and implementing\nthem requires an understanding of the architecture design. If new developers join the team\nlater, they can also use the architecture design as part of their onboarding. Influencing non-technical parts of the project\nAnother reason that software architecture design is important is the fact that design\ndecisions affect aspects of the software project other than the architecture. For example,\ncertain architecture design decisions could affect the purchasing of tools and licenses, the\nhiring of team members, the organization of the development environment, and how the\nsoftware will eventually be deployed. Top-down versus bottom-up design\napproaches\nThere are two fundamental approaches to the design of software architecture. One is a top-\ndown design approach, and the other is a bottom-up approach. These strategies apply to a\nvariety of disciplines, including software architecture design. Let's look at both of them in\nmore detail. Top-down approach\nA top-down approach starts with the entire system at the highest level, and then a process\nof decomposition begins to work downward toward more detail. The starting point is the\nhighest level of abstraction. As decomposition progresses, the design becomes more\ndetailed, until the component level is reached.", "domains": ["Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 146", "position": 146, "chunk_type": "semantic", "token_estimate": 309}
{"text": "Designing Software Architectures: Chapter 5\n[ 118 ]\nWhile the detailed design and implementation details of the components are not part of the\narchitecture design, the public interfaces of the components are part of the design. It is the\npublic interfaces that allow us to reason about how components will interact with each\nother. A design using the top-down approach is typically performed iteratively, with increasing\nlevels of decomposition. It is particularly effective if the domain is well understood. This systematic approach has been favored by enterprises since it can handle large and\ncomplex projects and because the method of design is planned. A systematic approach to\narchitecture design is attractive to enterprises because it can help with time and budget\nestimates. However, a strict top-down approach, which requires a lot of upfront\narchitecture design, has become less common in modern software architecture. Advantages of the top-down approach\nThere are a number of benefits to using a top-down approach. It is a systematic approach to\ndesign and breaks the system down into smaller parts. As a system is decomposed, it lends\nitself well to the division of work. On larger projects with multiple teams, this work can be\ndivided among the teams. As further decomposition takes place, tasks can be created for individual team members. This supports project management in the assignment of tasks, scheduling, and budgeting. This type of ability to plan is attractive to enterprises. The management teams of\norganizations may prefer, or even insist on, a top-down approach. Earlier in the book, we\ndiscussed how, as a software architect, you may be asked to assist with project estimates,\nand a top-down approach will allow you to do that with more accuracy. Although this approach works well on both small and large projects, it can be particularly\nuseful for large projects. By decomposing a system into smaller components, a large project\ncan become more manageable as the size and complexity of each component is reduced. Disadvantages of the top-down approach\nA strictly top-down approach runs the risk of a big design up front (BDUF), sometimes\nreferred to as a big up-front design (BUFD). Software is complex and it can be difficult to\ncreate the entire architecture up front. Design flaws or missing functionality in the\narchitecture may not be uncovered until later in the process, when components are\ndesigned or implemented.", "domains": ["Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 147", "position": 147, "chunk_type": "semantic", "token_estimate": 393}
{"text": "Designing Software Architectures: Chapter 5\n[ 119 ]\nA top-down approach works best when the domain is well understood, which is not always\nthe case. Plenty of projects begin without the domain being fully understood. Even when it\nis understood, stakeholders and users can sometimes be unclear as to what the software\nshould do, and how it should work. If multiple teams are working on a project, each responsible for a particular subsystem or\nmodule, knowledge sharing and reuse can be difficult with this approach. Each team can\nwork independently of the others, which has its advantages, but it does not facilitate the\nsharing of code or knowledge. The software architect may have to recognize areas of reuse,\nabstract them out, and communicate them to the teams. Another way to mitigate this issue\nis to provide opportunities and collaboration tools for teams to communicate with each\nother. If you use the top-down approach, be careful not to become an ivory tower architect. If you\ndesign the higher levels of an architecture and then hand them off to developers to handle\nthe lower-level detailed design, it is easy to become disengaged. As much as your\norganization and the project permits, make an effort to stay involved with the team. If\narchitectural changes are required later, you will already be familiar with the ongoing\nimplementation, which will help you to make the correct changes. Bottom-up approach\nIn contrast with the top-down approach, the bottom-up approach begins with the\ncomponents that are needed for the solution, and then the design works upward into\nhigher levels of abstraction. Various components can then be used together, like building\nblocks, to create other components and eventually larger structures. The process continues\nuntil all the requirements have been met. Unlike the top-down approach, which begins with the high-level structure, there is no up-\nfront architecture design with the bottom-up approach. The architecture emerges as more\nwork is completed. Hence, this is sometimes referred to as emergent design or emergent\narchitecture. The bottom-up approach does not require that the domain be well-understood, as the team\nonly focuses on a small piece at a time. The system grows incrementally as the team learns\nmore about the problem domain as well as the solution.", "domains": ["Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 148", "position": 148, "chunk_type": "semantic", "token_estimate": 374}
{"text": "Designing Software Architectures: Chapter 5\n[ 120 ]\nAdvantages of the bottom-up approach\nOne advantage of a bottom-up approach is the greater level of simplicity. The team only\nhas to focus on individual pieces and builds only what it needs for a particular iteration. This approach works well with agile development methodologies. With an iterative\napproach that handles change, refactoring can take place to add new functionality or to\nchange existing functionality. Each iteration ends with a working version of the software\nuntil eventually the entire system is built. Agile practices, such as automated unit testing\nand continuous integration, are encouraged and can lead to higher quality software. A bottom-up approach avoids the possibility of a big design up front, which can lead to\noverdesigning a solution. Some in the agile community feel that a lot of design effort up\nfront is wasted time and that an emergent design, or no design up front (NDUF), would be\nmore effective. The bottom-up approach allows the development team to begin coding very early in the\nprocess, which also means testing can occur earlier. This includes automated unit testing as\nwell as manual testing by team members such as QA analysts and other users. Getting\nfeedback earlier in the process allows the team to identify any necessary changes earlier. This approach facilitates code reuse. As the team is focused on a limited number of\ncomponents at any given time, recognizing opportunities for reuse becomes easier. Disadvantages of the bottom-up approach\nA bottom-up, or emergent, approach, assumes that change is cheap. Agile methodologies\nand practices provide an approach that anticipates change and can adapt to it. However,\ndepending on the nature of the change, refactoring software architecture design can be very\ncostly. A bottom-up approach, with no initial architecture, can lead to lower levels of\nmaintainability. With the refactoring that may be necessary with this approach, issues can\narise. If the team is not diligent, this problem can become worse over time. The entire scope of work may not be known when using this approach. This makes it more\ndifficult to plan and estimate the entire project, which may be unacceptable for enterprise\nsoftware.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 149", "position": 149, "chunk_type": "semantic", "token_estimate": 360}
{"text": "Designing Software Architectures: Chapter 5\n[ 123 ]\nGreenfield versus brownfield software\nsystems\nWhen you are starting the design process, one of the first considerations is whether you are\ndesigning a greenfield or a brownfield system. The terms greenfield and brownfield are\nused in a number of disciplines. It is an analogy to a construction project, and whether it\nwill begin on greenfield land, as in land that is undeveloped, or brownfield land, referring to\nland that was previously developed but is not currently in use. Greenfield systems\nA greenfield software system is a completely new software application, one in which you\ncan start with a clean slate. There are no constraints based on any prior work. A greenfield\nsystem can be designed for a well-understood domain or for a novel domain. A well-understood domain is one that is mature, and the possibilities for innovation are\nvery limited. Examples include Windows desktop applications, standard mobile\napplications, and enterprise web applications. There will be existing frameworks, tools, and\nsample architectures for the software that you need to build. The software architectures of\nexisting applications can be used as a guide. It will be more likely that you are developing software for a well-understood domain, and\nthe benefit is that there will be a tremendous amount of knowledge that you can leverage\nfrom the experience of those who have built similar applications. A greenfield system for a novel domain is also a new software application that does not\nneed to take into consideration any prior work. The difference between a greenfield system\nfor a mature domain and one for a new domain lies in the fact that a new domain is not as\nwell understood, and requires a lot more innovation. Unlike a well-understood domain, you will not find as much supporting information for a\nnew domain. Rather than relying on a plethora of reference architectures or referring to a\nlarge knowledge base, you will find yourself spending time building prototypes to test out\nyour solutions. For novel domains, it may be beneficial to design a throwaway prototype initially. These are\nprototypes of some piece of a software system so that you can test it out, such as getting\nfeedback from users or testing quality attributes. They will help you to gain an\nunderstanding of what will make a viable solution for a novel domain.", "domains": ["Design Patterns"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 152", "position": 152, "chunk_type": "semantic", "token_estimate": 393}
{"text": "Designing Software Architectures: Chapter 5\n[ 124 ]\nThrowaway prototypes are not built for long-term use, hence the term throwaway, so\nqualities such as maintainability and reusability are not the focus of such prototypes. If you\nare using new technologies, or technologies that are not already familiar to you, a prototype\ncan be a good way to try out a solution. Brownfield systems\nA brownfield software system is an existing software system. If changes to an existing\nsystem require architectural changes, architecture design will be needed. Modifications\nmay be necessary for purposes such as correcting defects, implementing new functionality,\nor changing existing functionality. Architectural changes may also be performed on existing software to improve it in some\nway without changing any functionality. For example, an architecture for an existing\nsoftware system might be refactored to improve a particular quality attribute. Most of the\ntime, work on brownfield systems does not involve wholesale changes in the overall\narchitecture unless major rework is required. One of the crucial first steps for the software architecture design of brownfield systems is to\ngain an understanding of the existing architecture. You need to understand the overall\nstructure, the elements, and the relationships between those elements. From there, the\ndesign is not so different from a greenfield system that has been through some iterations to\nestablish an initial architecture. We will explore the topic of architecture for legacy systems in Chapter 14, Architecting\nLegacy Applications. Architectural drivers\nArchitectural drivers are considerations that need to be made for the software system that\nare architecturally significant. They drive and guide the design of the software architecture. Architectural drivers describe what you are doing and why you are doing it. Software\narchitecture design satisfies architectural drivers. Architectural drivers are inputs into the design process, and include:\nDesign objectives\nPrimary functional requirements", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 153", "position": 153, "chunk_type": "semantic", "token_estimate": 302}
{"text": "Designing Software Architectures: Chapter 5\n[ 125 ]\nQuality attribute scenarios\nConstraints\nArchitectural concerns\nDesign objectives\nDesign objectives focus on the purpose of the specific architecture design. For the\nparticular design in question, what are the reasons behind why the software is being\ndesigned? The design objectives influence the design and are therefore one of the architectural drivers. A common design objective is to design an architecture for a solution, prior to\ndevelopment. The overall objective is to facilitate the implementation of a solution that will\nsatisfy requirements. This type of design objective might be for a greenfield or a brownfield type of system. As\nwe already explored, the differences between these types of systems might lead you to\nfocus on different design objectives. Designing a software architecture for development is not the only type of design objective. As a software architect, you may find yourself involved with project proposals. For such\npre-sales activity, the design objective may focus on coming up with the software's\ncapabilities, the possible timeframe for delivery, a breakdown of work tasks, and the\nfeasibility of the proposed project. If this is the purpose of the design, then this type of\ninitial design will not be nearly as detailed as one that you are designing for development. For the purposes of a project proposal, it will not be necessary to be as detailed as you\nwould be in preparation for development. You may be required to produce a design for a\nproject proposal in a short amount of time to meet a particular sales deadline. In addition,\nuntil the sale is complete, you will probably not be allocated the funds or time for a full-\nscale design. Similarly, software architects may need to create a prototype. This may be for a project\nproposal, but it could also be to test out a new technology or framework, to create a proof\nof concept (POC) for some solution to a particular problem, or to explore how a certain\nquality attribute might be effectively met. As with project proposals, if the design objective\nis to build a prototype, the focus and scope of the software architecture design will be\ndifferent from one that is being done for development. It is important to keep the design objectives in mind as an architectural driver when\nsoftware architecture design is about to begin.", "domains": ["Design Patterns"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 154", "position": 154, "chunk_type": "semantic", "token_estimate": 391}
{"text": "Designing Software Architectures: Chapter 5\n[ 126 ]\nPrimary functional requirements\nAnother important type of input into the architecture design is the primary functional\nrequirements that need to be satisfied. Primary functional requirements are those that are\ncritical to the organization's business goals. In Chapter 3, Understanding the Domain, we\ndiscussed core domains, which refer to the part of the domain that makes the software\nworth writing. Some of the primary functionality will come from the core domain. It is\nwhat differentiates the organization from competitors. Although satisfying functional requirements is a goal of software architecture design, keep\nin mind that not all functionality is affected by the architecture. While some functionality\nis highly affected by the architecture, other functionality can be delivered equally as well\nwith different architectures. Even in cases where functionality is not influenced by the architecture directly, functional\nrequirements may be an architectural driver for other reasons. One example of this would\nbe the need to make modifications to the functionality later. Maintainability and\nmodifiability of the software are affected by the software architecture. Quality attribute scenarios\nQuality attributes are measurable properties of a software system. They are the ilities, such\nas maintainability, usability, testability, and interoperability. We have been stressing the\nimportance of quality attributes since they play such an important part in the success of\nsoftware systems, and because software architecture decisions will affect them. This makes quality attributes one of the main architectural drivers for software architecture\ndesign. The design decisions that are made will determine what quality attributes will be\nmet. As an architectural driver, quality attributes are typically described in the context of a\nparticular scenario. A quality attribute scenario is a short description of how the software system should\nrespond to a particular stimulus. Scenarios make quality attributes measurable and testable. For example, a quality attribute of performance or a requirement that states a particular\nfunction should be fast is not measurable or testable. An actual example of a valid quality\nattribute related to performance would be as follows: When the user selects the Login option, a\nLogin page is displayed within two seconds.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 155", "position": 155, "chunk_type": "semantic", "token_estimate": 354}
{"text": "Designing Software Architectures: Chapter 5\n[ 128 ]\nConstraints are generally fixed from the beginning of the project and might be technical or\nnon-technical. Examples of technical constraints include being required to use a specific\ntechnology, having the ability to deploy to a particular type of target environment, or using\na specific programming language. Examples of non-technical constraints are being required\nto abide by a certain regulation, or that the project must meet a particular deadline. Constraints may also be classified by whether they are internal or external. Internal\nconstraints originate from within the organization and you may have some control over\nthem. In contrast, external constraints come from outside of the business and you may not\nhave any control over them. Like the other architectural drivers, constraints need to be considered in the design as an\ninput into the design process. Architectural concerns\nArchitectural concerns are interests of the software architect that impact the software\narchitecture. As a result, they are an architectural driver. Just as functional requirements\nand quality attributes are design issues important to stakeholders, architectural concerns\nare design issues important to the software architect. Architectural concerns need to be considered part of the design, but are not captured as\nfunctional requirements. In some cases, they may be captured as quality attributes rather\nthan architectural concerns, or an architectural concern may lead to new quality attribute\nscenarios that need to be met. For example, a software architect may have concerns related to software instrumentation or\nlogging. If not already recorded as part of a quality attribute, such as maintainability, the\narchitectural concern may lead to a new quality attribute. Good software architects will be able to recognize possible architectural concerns based on\nthe type of software they are designing. Architectural concerns may also arise from\nprevious architecture design iterations, so be aware that architecture changes may lead to\nnew concerns being created from them.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 157", "position": 157, "chunk_type": "semantic", "token_estimate": 317}
{"text": "Designing Software Architectures: Chapter 5\n[ 132 ]\nTactics\nTactics are proven techniques to influence quality attribute scenarios. They focus on a\nsingle quality attribute, so they are simpler than other design concepts, such as architecture\npatterns and reference architectures, which aim to solve a greater number of design issues. Tactics provide options to satisfy quality attributes, and the use of other design concepts,\nsuch as architecture patterns or an externally built framework, along with code, are\nrequired to fully complete the tactic. We went over some tactics when we explored quality attributes, such as:\nSatisfying a maintainability quality attribute scenario by reducing complexity in\na component by increasing cohesion and reducing coupling\nIncreasing usability in a scenario by providing friendly and informative\nmessages to the user\nImplementing a retry strategy in a process to handle a possible transient fault in\norder to improve an availability quality attribute scenario\nSatisfying a portability quality attribute scenario by increasing installability by\nensuring that a software update process to a newer version properly cleans up\nthe older version\nExternally developed software\nWhen designing a software architecture, you will be making design decisions for a number\nof design issues. Some of these design issues already have solutions in the form of concrete\nimplementations that have been developed externally. Rather than build a solution in-\nhouse to solve a particular design issue, you can leverage software that has already been\ndeveloped outside of the organization. The externally developed software can come in different forms, such as a component, an\napplication framework, a software product, or a platform. There are many examples of\nexternally developed software, such as a logging library for logging functionality, a UI\nframework for creating user interfaces, or a development platform for server-side logic.", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 161", "position": 161, "chunk_type": "semantic", "token_estimate": 292}
{"text": "Designing Software Architectures: Chapter 5\n[ 135 ]\nShould I use open source software (OSS)? When searching for an externally developed solution that will solve a design problem, one\npossibility is to find open source software (OSS) that will fulfill your needs. OSS is written\nby the community and is intended for use by the community. Given the wide availability and range of open source software, there are many solutions\navailable for a variety of problems. It is much more common now to use open source\nsolutions as part of a software application. Some organizations do not permit the use of\nopen source software but if your organization does, then you should give it consideration\nas a viable alternative for a given task. One consideration when selecting open source software is the license that is associated with\nit. The license dictates the terms and conditions under which the software can be used,\nmodified, and shared. One set of open source licenses that are popular is a group of licenses\nthat have been approved by the Open Source Initiative (OSI). Some of the OSI-approved\nlicenses include (in alphabetical order):\nApache License 2.0\nBSD 2-clause Simplified or FreeBSD license\nBSD 3-clause New or Revised license\nCommon Development and Distribution License\nEclipse Public License\nGNU General Public License (GPL)\nGNU Lesser General Public License (LGPL)\nMIT license\nMozilla Public License 2.0\nThere are differences in the terms and conditions of the various licenses. For example, your\napplication can incorporate open source software that uses the MIT license and you will be\nable to distribute your application without making it open source. In contrast, if your application incorporates software that uses the GNU General Public\nLicense and you then distribute your application, your application would need to be made\nopen source. This is true even if your application is free and you do not change the open\nsource software you are using in any way. If your software is for internal use only and it is\nnot distributed, then your application could remain proprietary and closed source.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 164", "position": 164, "chunk_type": "semantic", "token_estimate": 343}
{"text": "Designing Software Architectures: Chapter 5\n[ 136 ]\nAdvantages of using open source software\nThere are benefits to using open source software, which explains its popularity. Using an\nopen source solution for a design problem provides many of the same advantages as using\none that has been purchased. You don't have to spend time building the solution, it may\nhave a robust set of features, and it may already be a tested and proven solution with many\nother users. Unlike software that must be purchased, open source software is freely available so there\nare cost savings. You just have to keep in mind the license that comes with the software. If the open source software is a popular solution with an active community, it might be\ncontinuously improved with bug fixes and new features. You will be able to take advantage\nof this work. Bugs may be detected and fixed quickly because many people are using and\nworking on the code. This is the idea behind Linus's Law, which is named after Linus\nTorvalds, the creator of the Linux kernel. Linus's Law basically states that given enough\neyeballs, or people looking at the code, all bugs are shallow. In other words, with many\npeople looking at the source code, problems will be detected sooner rather than later, and\nsomeone will be able to provide a fix. Although some view open source software as less secure due to the availability of the code,\nsome people see it as more secure because there are many eyes that are using, looking at,\nand fixing the code. Another advantage of open source software is the fact that you have access to the source\ncode. If necessary, your development team will be able to modify it just as you would with\nan in-house solution. Disadvantages of using open source software\nDespite its advantages, there are some disadvantages to using open source software that\nyou should consider. Even though the software is free, there are still costs related to using\nit. Someone has to spend time integrating the solution into the software system, and there is\nan associated cost for that effort. If the open source software has to be modified in any way\nto suit the needs of the project, there is a cost related to that work as well.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 165", "position": 165, "chunk_type": "semantic", "token_estimate": 385}
{"text": "Designing Software Architectures: Chapter 5\n[ 137 ]\nEven for a popular open source project with an active community, there is no guarantee\nthat the software will continue to experience support. There is always a risk of the software\ngoing out of favor. If the project is abandoned, you won't be able to rely on support for bug\nfixes or new features unless the development team performs that work for themselves. One reason an open source software project may become less secure is if no one is actively\nworking on it. Even if the project has not been abandoned, no one is necessarily reading the\ncode. The average programmer writes much more code than they read. The existence of\nsome prominent security bugs has shown that it is possible for critical security\nvulnerabilities to go undetected for some time. Despite Linus's Law, the fact that the source code is readily available introduces a degree of\nsecurity risk. Malicious individuals can analyze the source code to identify security\nvulnerabilities and attempt to take advantage of them. Documenting the software architecture\ndesign\nAn important part of architecture design is documenting the design, including the many\ndesign decisions that are made during the process. This typically comes in the form of\nsketching architecture views and documenting the design rationale. Sketching the architecture design\nSoftware architectures are commonly documented through the creation of architecture\nviews. Architecture views are representations of a software architecture that are used to\ndocument it and communicate it to various stakeholders. Multiple views of an architecture\nare typically required, as a software architecture is too complex to be represented in a\nsingle, comprehensive model. Formal documentation of a software architecture through views will be covered Chapter\n12, Documenting and Reviewing Software Architectures, and is not typically done as part of the\ndesign process. While that type of documentation comes afterward, informal\ndocumentation, in the form of sketches, should take place during the architecture design. Sketches can record the structures, elements, relationships between the elements, and the\ndesign concepts used.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 166", "position": 166, "chunk_type": "semantic", "token_estimate": 339}
{"text": "Designing Software Architectures: Chapter 5\n[ 141 ]\nCapturing design rationale can facilitate successful architectural reuse. It can help designers\nunderstand what parts of the application can be reused. It may also provide some insight\ninto where modifications can be made to a component in order to reuse it in the application. Due to the variation between software products, reusable components are typically\ndesigned with variation points, or places where modifications can be made in order to adapt\nthe component for use in a particular software product. Understanding the design rationale\nwill help designers use the component properly, and prevent harmful modifications from\nbeing made. Using a systematic approach to software\narchitecture design\nIf you are going to dedicate some time to designing the architecture of a software system,\nand not just let it emerge after implementing features, you should do so in a systematic way. Software architects need to ensure that the architecture they are designing will satisfy the\narchitectural drivers, and a systematic approach can assist in accomplishing that goal. In\nDesigning Software Architectures, A Practical Approach, the following is said about using an\narchitecture design process:\n\"The question is, how do you actually perform design? Performing design to ensure that\nthe drivers are satisfied requires a principled method. By \"principled\", we refer to a\nmethod that takes into account all of the relevant aspects that are needed to produce an\nadequate design. Such a method provides guidance that is necessary to guarantee that your\ndrivers are satisfied.\" Using an established architecture design process will provide you, as the software architect,\nwith guidance on how to go about designing an architecture that will satisfy functional\nrequirements and quality attribute scenarios. There are a number of design processes that\ncan be used for software architecture. Although they differ from each other, including\ndifferences in terminology, they also have some fundamental commonalities.", "domains": ["Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 170", "position": 170, "chunk_type": "semantic", "token_estimate": 312}
{"text": "Designing Software Architectures: Chapter 5\n[ 158 ]\nTOGAF architecture domains\nFour standard architecture areas, or architecture domains, for enterprise architecture are\ndefined by TOGAF. They are business, data, applications, and technology architectures. These domains are sometimes referred to as the BDAT domains. All four of these architecture domains are considered during the ADM and we will explore\nthem in more detail when we go over the various phases of the ADM.\nTOGAF documentation\nThe TOGAF documentation is broken up into the following seven sections:\nPart I \u2013 Introduction: The first part is an introduction to the concepts of \nenterprise architecture, the TOGAF approach, and definitions of relevant terms\nused in TOGAF. Part II \u2013 Architecture development method: This section details the architecture\ndevelopment method (ADM), which is the core of TOGAF. We will be focusing\nour attention on the ADM part of the TOGAF. Part III \u2013 ADM guidelines and techniques: This part of the documentation\nprovides guidelines and techniques for applying TOGAF and the ADM.\nPart IV \u2013 Architecture content framework: In this part, information is provided\non the TOGAF content framework, including the architectural artifacts and\ndeliverables that are part of the process. Part V \u2013 Enterprise continuum and tools: This section covers the architecture\nrepository for an enterprise, including the categorization and storage of\narchitecture artifacts. Part VI \u2013 TOGAF reference models: In this section, various architectural\nreference models are provided, including the TOGAF Foundation Architecture\nand the Integrated Information Infrastructure Reference Model (III-RM). Part VII \u2013 Architecture capability framework: The final part provides guidelines\non establishing and operating an enterprise architecture capability within an\nenterprise, including processes, skills, roles, and responsibilities.", "domains": ["Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 187", "position": 187, "chunk_type": "semantic", "token_estimate": 276}
{"text": "Designing Software Architectures: Chapter 5\n[ 160 ]\nEach phase continuously checks with requirements to ensure that they are being met. Organizations can modify or extend the process to meet their needs, and it is usable with\nthe deliverables of other frameworks if it is decided that those deliverables are more\nsuitable. Phase A \u2013 Architecture vision\nIn this step of the ADM, the team defines the overall vision for the enterprise architecture,\nincluding its capabilities and business value. The team agrees on items such as scope,\nbusiness goals, business drivers, constraints, requirements, roles, responsibilities, and\nscheduling. These decisions are documented in the Statement of Architecture Work, which\nis a deliverable for this phase. The document typically contains the following:\nArchitecture project request and background information\nProject description and scope of the architecture\nAn overview of the architecture vision\nChange of scope procedures\nRoles, responsibilities, and deliverables for the project\nDetails on the acceptance criteria and procedures\nProject plan and schedule\nPhase B \u2013 Business architecture\nBusiness architecture is one of the four architecture domains defined in TOGAF. The\nbusiness architecture focuses on the business and/or service strategies of the organization,\nalong with its business environment. An understanding of the business architecture is a\nprerequisite to perform architecture work on the other three domains defined in TOGAF\n(data, application, and technology). The goal of this phase is to determine the target business architecture for how the enterprise\nachieves its business objectives and its strategic drivers. In order to create a roadmap on\nhow to reach the target state, the following four steps are undertaken:\nGain an understanding of the current state of the architecture\n1. Refine and validate the target state of the architecture\n2. Determine the gap that exists between the current and target states of the\n3.\narchitecture\nCreate a roadmap to transition between the current and target architecture states\n4.", "domains": ["Design Patterns"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 189", "position": 189, "chunk_type": "semantic", "token_estimate": 313}
{"text": "Designing Software Architectures: Chapter 5\n[ 161 ]\nPhase C \u2013 Information systems architectures\nData and application architecture are two of the other architecture domains defined in\nTOGAF. The data architecture focuses on an organization's data and how it is managed,\nwhile application architecture involves the enterprise's software applications. The results from the architecture vision and business architecture phases are used to\ndetermine the architectural changes that will be necessary to an enterprise's data and\napplication architectures. As was the case with Phase B, the current and target states of the architecture are compared\nso as to determine the gap between the two. This allows for an architecture roadmap to be\ncreated for the candidate application and data components that will be needed to bridge the\ngap. Phase D \u2013 Technology architecture\nTechnology architecture, one of the other architecture domains defined in TOGAF, involves\nthe enterprise's infrastructure components. This includes the hardware and software\nnecessary to support the enterprise's business, data, and application architectures. The goal of this phase is to develop the target technology architecture that will support the\nenterprise's solutions. An assessment of the enterprise's current infrastructure capabilities is\ncompleted and compared with the desired target state so that the gap between them can be\nidentified. From there, a roadmap of the target state for the technology architecture can be\ncreated along with the candidate components. Phase E \u2013 Opportunities and solutions\nThis phase focuses on how to deliver the target architecture as we move from a conceptual\nview of the target architecture toward implementation. The roadmaps created in Phase B,\nPhase C, and Phase D are consolidated into an overall architecture roadmap. The candidate\nsolutions that were created in the previous phases are organized into high-level candidate\nwork packages. The overall architecture roadmap, which includes all of the gaps between the current and\ntarget states of the architecture, is used to determine the best approach on how to deliver\nthe target architecture. If an incremental approach is to be taken, transition architectures are identified so that\nbusiness value continues to be delivered.", "domains": ["Design Patterns", "Design Principles", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 190", "position": 190, "chunk_type": "semantic", "token_estimate": 345}
{"text": "Designing Software Architectures: Chapter 5\n[ 164 ]\nDIVE criteria\nDIVE is an acronym that stands for the types of criteria that are used to prioritize product\nbacklog items. It focuses on Dependencies, to Insure against risks, business Value, and\nestimated Effort as the factors used to determine priority. Dependencies\nSome product backlog items will be dependent on others, and therefore those dependencies\nwill need to be completed first. For example, if item A depends on item B, B would be\nprioritized higher than item A.\nInsure against risks\nWhen prioritizing backlog items, you want to insure against risks, which include both\nbusiness and technical risks. Taking potential risks into consideration may lead the team to\nprioritize a backlog item higher or lower when compared to other backlog items. Business value\nThe business value of a product backlog item is an important criterion for prioritization. Product backlog items with greater levels of business value may be deemed a higher\npriority. The input of relevant stakeholders can help to determine the business value of a\nproduct backlog item. Estimated e\ufb00ort\nThe estimated level of effort for a product backlog item may be a factor when prioritizing\nwork. This may be due to factors such as scheduling or resource availability. There may be\ncases where a product backlog item has a large estimated effort, and the team wants to\ntackle the item sooner rather than later to ensure that it will be completed in time. Active and dynamic architecture backlogs\nAs with any product backlog, the architecture backlog is not static and will evolve as the\narchitecture design takes place. As architecture design iterations are completed, new\narchitectural drivers may be uncovered, necessitating the need for new items to be added to\nthe backlog.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 193", "position": 193, "chunk_type": "semantic", "token_estimate": 292}
{"text": "Designing Software Architectures: Chapter 5\n[ 165 ]\nAnother reason that items may be added to the architecture backlog is when issues are\ndiscovered with the architecture. When the design is reviewed, a problem may become\napparent, requiring further work to be done. As architectural design decisions are made, it may cause the creation of new architecture\nbacklog items. When a design decision is made, new concerns may arise from that decision. For example, if it is decided that the application will be a web application, backlog items\nrelated to security, session management, and performance that are specific to web\napplications may need to be added to the architecture backlog if they did not already exist. Changes to the architecture backlog may prompt you to revisit the priorities of the backlog\nitems. The architecture backlog should be made available to anyone who may need to be aware of\nthe design's progress. If you do have separate backlogs for the architecture and the rest of\nthe project, keep in mind that the audience for the two backlogs may be different. It really\ndepends on the project and the level of involvement and transparency that exists between\nthe project team and other stakeholders involved with the project. In some cases, clients may have access to the product backlog to track functionality, but the\nteam may want to keep the architecture backlog private. Summary\nSoftware architecture design plays a critical part in the creation and success of software\narchitectures. At its core, architecture design involves making design decisions to produce\nsolutions to design problems. The result is an architecture design that can be validated,\nformally documented, and eventually used by development teams. There are two main approaches to architecture design, the top-down and bottom-up\napproaches. We examined situations in which one would be used over the other and\nlearned how a combination of the two approaches often works best. Architectural drivers, which are the inputs into the architecture design process, guide the\narchitecture design. They include design objectives, primary functional requirements,\nquality attribute scenarios, constraints, and architectural concerns. Designing a software architecture can be challenging, but we can leverage design concepts,\nsuch as software architecture patterns, reference architectures, tactics, and externally\ndeveloped software, to assist with the design of solutions.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 194", "position": 194, "chunk_type": "semantic", "token_estimate": 377}
{"text": "6: Software Development\nPrinciples and Practices\nOne of the main goals for software architects is to design high-quality software\napplications. There are a number of software design principles and best practices that can\nbe applied to achieve that goal. Software architects can apply these principles and practices when designing software\narchitectures and encourage developers to use them in their implementations. These\nprinciples and practices are used to improve quality, simplify maintenance, increase\nreusability, find defects, and make software systems easier to test. In this chapter, we will cover the following topics:\nDesigning orthogonal software systems, including a focus on loose coupling and\nhigh cohesion\nMinimizing complexity in a software system by following principles such as\nKISS, DRY, information hiding, YAGNI, and Separation of Concerns (SoC)\nThe SOLID design principles, which include the Single Responsibility Principle\n(SRP), Open/Closed Principle (OCP), Liskov Substitution Principle (LSP),\nInterface Segregation Principle (ISP), and the Dependency Inversion Principle\n(DIP)\nUsing Dependency Injection (DI) to provide dependencies to a class\nUsing unit testing to improve the quality of a software system\nEnsuring that development environments can be set up easily\nPractice of pair programming\nReviewing deliverables, such as code reviews, formal inspections, and\nwalkthroughs", "domains": ["Design Principles", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 196", "position": 196, "chunk_type": "semantic", "token_estimate": 197}
{"text": "Software Development Principles and Practices: Chapter 6\n[ 168 ]\nDesigning orthogonal software systems\nIn geometry, two Euclidean vectors are orthogonal if they are perpendicular (form a right\nangle of 90 degrees). The two vectors meet at the origin point, but do not intersect. The two\nvectors are independent of each other:\nSoftware that is well designed is orthogonal in that its modules are independent of each\nother. Ideally, changes to one module in a software system should not require changes to\nanother module. Software systems will undergo many changes during their lifetime and\ndesigning them with this in mind provides a number of benefits, including increased\nproductivity for those who work on them and lowered risk of introducing defects when\nchanges are made. Designing orthogonal systems may have higher upfront costs, but over\ntime, a highly maintainable and extendable system will be worth it. Orthogonal systems are designed so that their elements are loosely coupled and highly\ncohesive. Let's look at the concepts of coupling and cohesion in more detail. Loose coupling\nCoupling is the degree to which a software module depends on another software module. Coupling between modules is a measure of how closely connected they are, and it can\neither be loose, sometimes described as low or weak, or it can be tight, sometimes referred\nto as high or strong. The degree of coupling between modules reflects the quality of their\ndesign. Software modules that are tightly coupled are more complex, which decreases their\nmaintainability. Tight coupling makes modifying the code more difficult because a change\nin a tightly coupled module will likely require changes in other modules. This introduces a\nhigher degree of risk as there is a greater likelihood that a new defect could be introduced if\na software module is modified.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 197", "position": 197, "chunk_type": "semantic", "token_estimate": 297}
{"text": "Software Development Principles and Practices: Chapter 6\n[ 169 ]\nIt is also easy to engage in parallel development if the code is loosely coupled. One\ndeveloper can work on one part of the application independent of another developer who\nis working on a different part of the application. Modules that are loosely coupled do not have as many dependencies with other modules. Changes to tightly coupled modules will take more time and effort due to the\ninterdependencies with other modules. As the number of modules that are affected by a\nchange increases, it will take longer for developers to make the modifications, and for\ntesters to test the changes. Tight coupling also reduces reusability. It is harder to reuse a\nmodule when dependent modules must be included in the reuse. Types of coupling\nThere are different types of coupling. The following are details regarding those types, in\norder of the tightest (least desirable) to loosest (most desirable) coupling. It should be noted\nthat it is possible for two modules to be coupled in more than one way. In those situations,\nthe coupling type is determined by the worst, or tightest, coupling type. Content coupling\nContent coupling is the highest type of coupling. It is considered so bad that it is also\nreferred to as pathological coupling. It occurs when one module directly references the\ninternal or private information in another module. For example, it exists when one module\naccesses or changes private data in another module. Modules should never be designed to have this type of coupling. If modules have content\ncoupling, they should be refactored so that there is a proper level of abstraction. The\nmodules should not directly rely on the internal workings of each other. Common coupling\nCommon coupling, also known as global coupling, is a high level of coupling. This type of\ncoupling is highly undesirable. Although sometimes it is unavoidable, modules should be\ndesigned to minimize the existence of this type of coupling. Modules exhibit common coupling when they share the same global data, such as a global\nvariable. It is perfectly acceptable to share configuration data throughout an application. However, as a general rule, if you are going to use other types of global data, it is better to\nuse something that has a fixed value, such as a constant, rather than a variable whose value\ncan vary at runtime.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 198", "position": 198, "chunk_type": "semantic", "token_estimate": 397}
{"text": "Software Development Principles and Practices: Chapter 6\n[ 170 ]\nExternal coupling\nExternal coupling is another type of high coupling. It exists when multiple modules share\nthe same part of an environment that is external to the software. This could come in the\nform of having to use an external data format, interface, communication format, tool, or\ndevice. Sometimes, external dependencies are imposed and unavoidable, but we should still seek\nto limit the number of modules that have those dependencies. Doing so will ensure that if\nthe external dependency changes, only a limited number of modules are affected. Control coupling\nControl coupling is a moderate type of coupling. Two modules exhibit control coupling\nwhen one module controls the internal logic of the other by passing it information. An\nexample of this is when a module passes a control flag to another module, which uses it to\ncontrol its flow. This type of coupling may be acceptable, but an effort should be made to make it known\nthat the coupling exists so that the modules can be tested together. It is beneficial to detect\nany problems with either of the modules earlier rather than later. Stamp coupling (data-structured coupling)\nStamp coupling is a fairly low type of coupling. It is also known as data-structure coupling\nbecause it occurs when modules share a composite data structure. By composite data\nstructure, we mean that it is data that has some internal structure to it, such as a record. When a composite data structure is shared between two modules, some of the fields in the\ndata structure may not even be used. For example, a module passes a composite data\nstructure to another module, which then just uses one field in it. It is similar to data coupling, except that the data shared is a composite data type rather\nthan primitive data values and that not all of the values shared may be used. Data coupling\nData coupling occurs when two modules share some data which are just primitive data\nvalues. It is another low type of coupling. A common type of data coupling is when a\nmodule calls a method on another module, and inputs and outputs are shared in the form\nof method parameters and the return value.", "domains": ["Design Patterns", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 199", "position": 199, "chunk_type": "semantic", "token_estimate": 376}
{"text": "Software Development Principles and Practices: Chapter 6\n[ 171 ]\nWhen two modules need to interact, this is a common and acceptable type of coupling. Unlike stamp collecting, where some of the values in the shared composite data structure\nmay not be used, all of the parameters in data coupling are used. If any parameters are not\nneeded, they should be removed. Message coupling\nModules exhibit message coupling when one module calls a method on another and does\nnot send any parameters. The only coupling is on the name of the method, but nothing else. It is the lowest type of coupling. No coupling\nThere are, of course, situations where there is no coupling between modules. This is when\ntwo modules have no direct communication at all. It is an ideal that allows the two modules\nto be implemented, tested, and maintained independently. The Law of Demeter (LoD) / principle of least knowledge\nThe Law of Demeter (LoD), or principle of least knowledge, is a design principle related\nto loose coupling. In order to minimize coupling between software modules, the principle\ncan be followed when designing software. The principle follows the only talk to your friends idiom, which keeps coupling loose by\nlimiting a module's communication with other modules. Ideally, a method should only call\nother methods in the same object, in objects that were passed into it, in direct component\nobjects, in objects that it created/instantiated, or in objects in a global variable that are\naccessible. Another one of the tenets of LoD is that a software module should know as little as possible\nabout other modules. This will ensure its independence from other modules, allowing\ncoupling to remain loose. See the Information hiding section for a principle that helps to\nachieve this goal. Designing for loose coupling\nDuring designs and implementations, your goal as a software architect should be to\nminimize the amount of coupling that exists between modules. Modules should be\ndesigned to be as independent as possible.", "domains": ["Design Patterns", "Design Principles", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 200", "position": 200, "chunk_type": "semantic", "token_estimate": 331}
{"text": "Software Development Principles and Practices: Chapter 6\n[ 172 ]\nCoupling can be reduced by eliminating or reducing the number of unnecessary\ndependencies. For any coupling that must exist, it should be the lowest type that is\nnecessary. Loose coupling reduces complexity and increases maintainability and\nreusability. Coupling typically affects the level of cohesion, so that loose coupling correlates with high\ncohesion and tight coupling correlates with low cohesion. High cohesion\nCohesion is the degree to which the elements inside a module belong together. It is the\nstrength of the relationships of elements within a module, and how united they are in their\npurpose. Cohesion is a qualitative measure of the consistency of purpose within a module. There are different types of cohesion, and those that reflect a higher level of cohesion are\npreferable. Highly cohesive modules have a single, well-defined purpose, and reflect a\nbetter quality of design. Software modules with low cohesion are harder to maintain. If a module contains multiple\nunrelated functions, changes to it are more likely to require changes in other modules. This\nwill require extra time and effort, not just in development, but also testing. The extra\ncomplexity in modules with low cohesion make it more likely that defects may be\nintroduced when they are modified. They may also be harder to understand, making them\nmore difficult to modify. Reusability is lessened for modules with low cohesion. Modules with low cohesion,\nperforming many disparate functions, are less likely to be reused for other purposes. A\nmodule that works together as a logical unit with a clear purpose is more likely to be\nreused. Types of cohesion\nThe level of cohesion in a module is represented by the type of cohesion. Let's examine the\ndifferent types of cohesion, from lowest (least desirable) to highest (most desirable). Coincidental cohesion\nCoincidental cohesion occurs when elements in a module are grouped arbitrarily. There is\nno relationship among the different elements, making it the lowest (worst) type of cohesion. Sometimes, you will see this type of cohesion in a utilities or helpers class where a number of\nunrelated functions have been placed together.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 201", "position": 201, "chunk_type": "semantic", "token_estimate": 354}
{"text": "Software Development Principles and Practices: Chapter 6\n[ 173 ]\nCoincidental cohesion should be avoided and, if it is encountered in a module, the module\nshould be refactored. Each part of the module should be moved to an existing or new \nmodule where it would make logical sense for it to exist. Logical cohesion\nModules exhibit logical cohesion when elements are grouped together because they are\nrelated in some way logically. Even though the functionality of logically cohesive modules\nmight be of the same general category, they may be different in other ways. For this reason,\nthis type of cohesion is considered low. While better than coincidental cohesion, these types\nof modules are not very cohesive. An example of logical cohesion would be a module that contains a set of functions that\nhandles I/O for the application. While they are related logically, the nature of the various\nfunctions would be quite different. They would be more cohesive if each type of I/O was\nhandled by a separate module. Temporal cohesion\nTemporal cohesion exists when the elements of a module are grouped together based on\nwhen they are processed. This can occur when different elements are grouped together\nsimply because they need to be executed at a single moment in time. This is another type of\nlow cohesion. An example of temporal cohesion is grouping a bunch of elements together because they\nare all related to system startup, system shutdown, or the handling of a system error. Even\nthough the elements are related temporally, they are only weakly related to each other. This\nmakes the module harder to maintain and reuse. The elements should be grouped into different modules, with each module designed for a\nsingle purpose. Procedural cohesion\nA module exhibits procedural cohesion when its elements have been grouped together\nbecause they always execute in a particular sequence. For example, payment processing for\na customer placing an order might involve the following steps being executed in a\nparticular sequence:\nGathering payment information\nValidating payment method details\nChecking whether funds are available or whether there is enough available credit", "domains": ["Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 202", "position": 202, "chunk_type": "semantic", "token_estimate": 348}
{"text": "Software Development Principles and Practices: Chapter 6\n[ 174 ]\nPersisting the order in a database\nChecking inventory levels\nCreating a back order or canceling an order based on inventory\nSending the order for fulfillment\nSending an email confirmation to the customer\nAlthough the various parts are all related by the order of execution, some of the individual\nactivities are quite distinct from each other. This type of cohesion is considered moderate. Although it is an acceptable level of cohesion,\nit is not ideal. If possible, refactoring can be performed to improve the level of cohesion. Communicational cohesion\nCommunicational cohesion occurs when parts of a module are grouped together because\nthey use the same set of inputs and outputs. If a module has different elements that have\nbeen grouped together because they access and modify the same data structure, it would\ndemonstrate communicational cohesion. For example, a data structure that represents the contents of a customer's shopping basket\nmight be used by a variety of elements in a single module. The elements might calculate\ndiscounts, shipping, and taxes based on the same data structure. This level of cohesion is moderate and usually considered acceptable. Sequential cohesion\nSequential cohesion exists when the different parts of a module are grouped together\nbecause the output of one part serves as the input for another part. Modules of this type\nhave a moderate level of cohesion. An example of a module that is sequentially cohesive would be one that is responsible for\nformatting and validating a file. The output of an activity that formats a raw record\nbecomes the input for an activity that then validates the fields in that record. Functional cohesion\nFunctional cohesion occurs when elements of a module are grouped together because they\nare united for a single, well-defined purpose. All of the elements in the module work\ntogether to fulfill that purpose. Functional cohesion in a module is ideal and is the highest\ntype of cohesion.", "domains": ["Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 203", "position": 203, "chunk_type": "semantic", "token_estimate": 326}
{"text": "Software Development Principles and Practices: Chapter 6\n[ 175 ]\nFunctional cohesion promotes the reusability of a module and makes it easier to\nmaintain. Examples of functionally cohesive modules include one that is responsible for\nreading a particular file and one that is responsible for calculating shipping costs for an\norder. Designing for high cohesion\nSoftware architects should design modules to have high cohesion. Each module should\nhave a single, well-defined purpose. The elements contained in the module should be\nrelated and contribute to that purpose. If there are auxiliary elements contained in a module that are not directly related to the\nmain purpose, consider moving them to either a new module or an existing module that\nhas the same purpose of the element being moved. Cohesion and coupling are related in that high cohesion correlates with loose coupling and\nlow cohesion correlates with tight coupling. Minimizing complexity\nBuilding software is inherently complex and a number of problems result from complexity. Higher levels of complexity in software:\nCause delays in schedules\nLead to cost overruns\nMay cause the software to behave in unintended ways or lead to an\nunanticipated application state\nMay create security loopholes or prevent security issues from being discovered\nin a timely fashion\nAre a predictive measure of lower levels of some quality attributes, such as lower\nmaintainability, extendibility, and reusability\nIn The Mythical Man-Month, Fred Brooks divides the problems facing software engineering\ninto two categories, essential and accidental:\n\"All software construction involves essential tasks, the fashioning of the complex\nconceptual structures that compose the abstract software entity, and accidental tasks, the\nrepresentation of these abstract entities in programming languages and the mapping of\nthese onto machine languages within space and speed constraints.\"", "domains": ["Design Patterns", "Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 204", "position": 204, "chunk_type": "semantic", "token_estimate": 286}
{"text": "Software Development Principles and Practices: Chapter 6\n[ 176 ]\nAccidental difficulties are problems that are just inherent to the production of software in\ngeneral. They are problems that software engineers can fix and may not even be directly\nrelated to the problem they are trying to solve. Improvements in programming languages,\nframeworks, design patterns, integrated development environments (IDEs), and software\ndevelopment methodologies are just some examples of progress over the years in\neliminating or reducing accidental difficulties. Essential difficulties are the core problems that you are trying to solve and they can't\nsimply be removed to reduce complexity. Software development teams spend more time on\nessential complexities than accidental ones. We try to manage and minimize the complexity, whether it is accidental or essential. As it\nhas probably become apparent by now, a recurring theme in this book is the importance of\nmanaging and minimizing complexity. It has a direct relationship with the quality of the\nsoftware and is therefore a major focus for software architects. Minimizing complexity in software helps to eliminate or manage both accidental and\nessential difficulties. Some of the principles related to minimizing complexity include KISS,\nDRY, information hiding, YAGNI, and SoC. KISS principle \u2013 \"Keep It Simple, Stupid\"\nThe KISS principle, which is an acronym for Keep It Simple, Stupid, has been used in\nnumerous contexts to convey the idea that systems generally work best if they are kept\nsimple. The principle is applicable to the design of software systems. A development team\nshould strive to not overcomplicate their solutions. Variations of the acronym include Keep It Short, Simple, Keep It  Simple, Stupid, Keep It Simple,\nStraightforward, and Keep It Simple, Silly. All of them have the same basic meaning, which is\nto express the value of simplicity in designs. Origin of KISS\nThe creation of the principle is typically credited to the late Kelly Johnson, who was an\naeronautical and systems engineer. Among other accomplishments, he contributed to\naircraft designs for the Lockheed Corporation (now known as Lockheed Martin after its\nmerger with Martin Marietta).", "domains": ["Design Principles", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 205", "position": 205, "chunk_type": "semantic", "token_estimate": 342}
{"text": "Software Development Principles and Practices: Chapter 6\n[ 177 ]\nThough the principle is commonly communicated as Keep It Simple, Stupid,\nKelly's original version didn't have a comma in it. The word Stupid wasn't\nintended to refer to a person. Kelly introduced the principle by explaining to the engineers that the jet aircraft they were\ndesigning needed to be something that a man in the field could fix with basic training and\ncommon tools. The design needed to satisfy this requirement, which is understandable\ngiven that the aircraft might need to be repaired quickly in a combat situation. Applying KISS to software\nSimplicity is a highly desirable quality in software systems, and this includes their designs\nas well as their implementations. Making software more complicated than it needs to be\nlowers its overall quality. Greater complexity reduces maintainability, hinders reusability,\nand may lead to an increase in the number of defects. Some ways to follow the KISS principle in software include:\nEliminating duplication as much as possible (see the DRY \u2013 \"Don't Repeat\nYourself\" section)\nEliminating unnecessary features (see the YAGNI \u2013 \"You Aren't Gonna Need It\"\nsection)\nHiding complexity and design decisions (see the Information hiding section)\nFollowing known standards when possible and minimizing deviations and\nsurprises\nEven after a module is implemented, if you see a method or class that could be made\nsimpler, consider refactoring it if you have the opportunity. Don't make it overly simple\nIn a quest for simplicity, we cannot oversimplify a design or implementation, though. If we\nreach a point that it negatively affects the ability to deliver on required functionality or \nquality attributes, we have gone too far. Keep the following quote, attributed to Albert Einstein, in mind when designing software:\nEverything should be made as simple as possible, but not simpler.", "domains": ["Design Principles", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 206", "position": 206, "chunk_type": "semantic", "token_estimate": 300}
{"text": "Software Development Principles and Practices: Chapter 6\n[ 178 ]\nDRY \u2013 \"Don't Repeat Yourself\"\nThe DRY principle stands for Don't Repeat Yourself and strives to reduce duplication in a\ncodebase. Duplication is wasteful and makes a codebase unnecessarily larger and more\ncomplex. This makes maintenance more difficult. When code that has been duplicated\nneeds to be changed, modifications are required in multiple locations. If the changes\napplied everywhere are not consistent, defects may be introduced. Software architects and\ndevelopers should avoid duplication whenever possible. When a design violates the DRY principle, it is sometimes referred to as a WET (Write\nEverything Twice) solution (or Waste Everyone's Time or We Enjoy Typing). Copy-and-paste programming\nIn poorly written codebases, code duplication often results from copy-and-paste\nprogramming. This happens when a developer needs the exact same or very similar logic,\nwhich exists somewhere else in the system, so they duplicate (copy and paste) the code. This violates the DRY principle and lowers the quality of the code. Copy-and-paste programming can sometimes be acceptable and serve a useful purpose. Code snippets, which are small blocks of reusable code, may speed up development. Many\nIDEs and text editors provide snippet management to make the use of snippets easier for\ndevelopers. However, beyond the appropriate application of snippets, it is usually not a\ngood idea to copy and paste your application code in multiple places. Magic strings\nMagic strings are strings that appear directly in your code. Sometimes, these strings are\nneeded in multiple places and are duplicated, violating the DRY principle. Maintenance of\nthese strings can become a nightmare because if you want to change the value of the string,\nyou have to change it in multiple places. The problem is exacerbated when the string is\nused, not just in multiple places within the same class, but within multiple classes. There are many examples of magic strings, from exception messages, settings in\nconfiguration files, parts of a file path, or a web URL. Let's look at an example where the\nmagic string value represents a cache key. This serves as a good example because this is a\ncase where a magic string might be duplicated multiple times within the same class and\neven within the same method:\npublic string GetFilePath()\n{\n    string result = _cache.Get(\"FilePathCacheKey\");", "domains": ["Design Principles"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 207", "position": 207, "chunk_type": "semantic", "token_estimate": 381}
{"text": "Software Development Principles and Practices: Chapter 6\n[ 179 ]\n    if (string.IsNullOrEmpty(result))\n    {\n        _cache.Put(\"FilePathCacheKey\", DetermineFilePath());\n        result = _cache.Get(\"FilePathCacheKey\");\n    }\n    return result;\n}\nThis key is repeated multiple times, increasing the possibility of a typo resulting in a defect. In addition, if we ever want to change the cache key, we will have to update it in multiple\nplaces. To follow the DRY principle, let's refactor this code so that the cache key is not repeated. First, let's declare a constant at the class level for the magic string:\nprivate const string FilePathCacheKey = \"FilePathCacheKey\";\nNow, we can use that constant in our GetFilePath method:\npublic string GetFilePath()\n{\n    string result = _cache.Get(FilePathCacheKey);\n    if (string.IsNullOrEmpty(result))\n    {\n        _cache.Put(FilePathCacheKey, DetermineFilePath());\n        result = _cache.Get(FilePathCacheKey);\n    }\n    return result;\n}\nNow, the string is declared in just one location. If you are going to place a magic string in a\nconstant, you should think about where the constant should be declared. One consideration\nis the scope of its use. It may be appropriate to declare it within the scope of a particular\nclass, but in some cases, a broader or narrower scope will make more sense. Although placing a magic string in a constant is a good technique for a variety of situations,\nit is not always ideal. This decision also depends on the type of string and its purpose. For\nexample, if the string is a validation message, you might want to place it in a resource file. If there are any internationalization requirements, placing translatable strings, such as\nvalidation messages, in a resource file will facilitate translating the messages into different\nlanguages.", "domains": ["Design Principles"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 208", "position": 208, "chunk_type": "semantic", "token_estimate": 268}
{"text": "Software Development Principles and Practices: Chapter 6\n[ 180 ]\nHow to avoid duplication\nDRYness can be achieved by being mindful and taking action when appropriate. If you find\nyourself copying and pasting code, or simply writing code that is identical or similar to\nexisting code, think about what you are trying to accomplish and how it can be made\nreusable. Duplication in logic can be eliminated by abstraction. This concept is referred to as the\nabstraction principle (or the principle of abstraction). The principle is consistent with the\nDRY principle and is a way to reduce duplication. The code that is needed in multiple\nplaces should be abstracted out, and the locations that need it can then be routed through\nthe abstraction. Some refactoring may be necessary to make it generic enough to be reused,\nbut it is worth the effort. Once the logic is centralized, if it needs to be modified in the\nfuture, perhaps to fix a defect or to enhance it in some way, you will be able to make the\nchanges in a single location. As we saw in the case of magic strings, duplication with values can be eliminated by\nplacing the value in a central location, such as the declaration of a constant. If there is duplication in a process, it may be possible to reduce it through automation. Manual unit testing, builds, and integration processes can be eliminated by automating\nthose processes. The automation of tests and builds will be discussed further in Chapter\n13, DevOps and Software Architecture. Don't make things overly DRY\nWhen attempting to follow the DRY principle, be careful not to consolidate disparate items\nthat just happen to be duplicates in some way. If two or more things are duplicates, it may\nbe that they are just coincidentally repetitive. For example, if two constants have the same value, that does not mean they should be\ncombined into one constant for the sake of eliminating duplication. If the constants\nrepresent distinct concepts, they should remain separate. Information hiding\nInformation hiding is a principle that advocates for software modules to be designed such\nthat they hide implementation details from the rest of the software system. The idea of\ninformation hiding was introduced by D.L. Parnas in On the Criteria to Be Used in\nDecomposing Systems into Modules, which was published in 1972.", "domains": ["Design Principles", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 209", "position": 209, "chunk_type": "semantic", "token_estimate": 392}
{"text": "Software Development Principles and Practices: Chapter 6\n[ 181 ]\nInformation hiding decouples the internal workings of a module from the places in the\nsystem that call it. The details of a module that do not need to be revealed should be made\ninaccessible. Information hiding defines constraints related to what properties and\nbehaviors can be accessed. Callers interact with the module's public interface and are\nprotected from the implementation details. There are a number of reasons to abide by the principle of information hiding. Reasons for information hiding\nInformation hiding is useful at all levels of design. Only exposing the details that need to be\nknown reduces complexity, which improves maintainability. Unless you are specifically\ninterested in the internal details, you do not need to concern yourself with them. Another one of the key reasons for information hiding is to hide design decisions from the\nrest of the software system. This is particularly beneficial if the design decision might\nchange. By hiding a design decision, if the decision needs to be changed, it minimizes the\namount and extent of the modifications that will be necessary. It provides the flexibility to\nmake changes later if it is necessary to do so. Whether the design decision is to use a particular API, represent data in a certain way, or\nuse a particular algorithm, the modifications necessary to change that design decision\nshould be kept as localized as possible. What needs to be exposed/hidden? You and your team should really think about the properties and behaviors (methods) that\nneed to be exposed for a module. Everything else can be hidden. Through the use of a\npublic interface, we can define what we want to make available. Information hiding assists with defining public interfaces. Rather than lazily exposing most\nof a class, it forces us to consider what really needs to be made public. The public interface\ndefines a contract that the implementation must follow, and allows others to know what is\navailable. It is up to the implementation to decide how it is accomplished.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 210", "position": 210, "chunk_type": "semantic", "token_estimate": 342}
{"text": "Software Development Principles and Practices: Chapter 6\n[ 182 ]\nYAGNI \u2013 \"You Aren't Gonna Need It\"\nYAGNI, which stands for You Aren't Gonna Need It, or You Ain't Gonna Need It, is a\nprinciple from the software development methodology of Extreme Programming (XP). XP\nis one of the first agile methods and was the dominant one until the rise of the popularity of\nScrum. YAGNI is similar to the KISS principle in that they both aim for simpler solutions,\nwith YAGNI focusing on a specific aspect, which is the removal of unnecessary\nfunctionality and logic. Avoid over-engineering a solution\nThe idea behind YAGNI is that you should only implement functionality when you need it\nand not just because you think you may need it some day. Ron Jeffries, one of the co-\nfounders of XP, once said:\n\"Always implement things when you actually need them, never when you just foresee that\nyou need them.\" Following the YAGNI principle helps you to avoid over-engineering a solution. You don't\nwant to spend time on future scenarios that are unknown. The problem with implementing\na feature that you think might eventually be needed is that quite often the feature ends up\nnot being needed or the requirements for it change. Code that is not written equates to time and money that is saved. Spending time and\nmoney on a feature you don't need takes away from time and money you could have spent\non something that you do need. Resources are finite, and using them on something that is\nunnecessary is a waste. As was the case with code duplication, adding unnecessary logic to\nan application increases its size and complexity, which reduces maintainability. Situations where YAGNI doesn't apply\nYAGNI applies to presumptive features, as in functionality that is not currently needed. It\ndoes not apply to code that would make the software system easier to maintain and modify\nlater. In fact, following YAGNI means you may be changing the system later to add a\nfeature, so the system should be well designed for this purpose. If a software system is not\nmaintainable, making changes later may be difficult. You may come across times where, in hindsight, a change made sooner would have\nprevented more expensive changes later. This may be particularly true for software\narchitects if the change is architecture related.", "domains": ["Design Principles", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 211", "position": 211, "chunk_type": "semantic", "token_estimate": 390}
{"text": "Software Development Principles and Practices: Chapter 6\n[ 183 ]\nIt can sometimes be difficult to foresee which changes should have been made before they\nwere needed. However, for the most part, following YAGNI is beneficial. Even in the case\nof an architecture change, a good architecture design reduces complexity and makes it\neasier to make changes. It also makes it more likely that when a change is needed, it can be\nlimited in scope and may not even require architectural changes. As software architects gain more experience, they become more adept at spotting\nexceptions to the YAGNI principle where a particular change should be made before it is\nneeded. Separation of Concerns (SoC)\nConcerns are the different aspects of functionality that the software system\nprovides. Separation of Concerns (SoC) is a design principle that manages complexity by\npartitioning the software system so that each partition is responsible for a separate concern,\nminimizing the overlap of concerns as much as possible. Following the principle involves decomposing a larger problem into smaller, more\nmanageable concerns. SoC reduces complexity in a software system, which reduces the\neffort needed to make changes and improves the overall quality of the software. When the DRY principle is followed, and logic is not repeated, a SoC is usually a natural\nresult as long as the logic is organized properly. SoC is a principle that can be applied to multiple levels in a software application. At the\narchitecture level, software applications can follow a SoC by separating different logic such\nas user-interface functionality, business logic, and infrastructure logic. An example of an\narchitecture pattern that separates concerns at this level is the Model-View-Controller\n(MVC) pattern, which we will cover in the next chapter. We can apply SoC at a lower level, such as with classes. If we were providing order\nprocessing functionality in a software system, the concern of validating credit card\ninformation shouldn't exist in the same place as the concern for updating inventory. They\nare distinct concerns that should not be placed together. At this level, it is related to the\nSingle Responsibility Principle, which we will discuss shortly. An example of separating concerns by language in web programming is Hypertext\nMarkup Language (HTML), Cascading Style Sheets (CSS), and JavaScript. They\ncomplement each other with one being focused on the content of web pages, one for the\npresentation, and one for the behavior.", "domains": ["Architectural Patterns and Styles", "Design Principles", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 212", "position": 212, "chunk_type": "semantic", "token_estimate": 398}
{"text": "Software Development Principles and Practices: Chapter 6\n[ 184 ]\nFollowing SOLID design principles\nSOLID design principles focus on creating code that is more understandable, maintainable,\nreusable, testable, and flexible. SOLID is an acronym that represents five separate software\ndesign principles:\nSingle Responsibility Principle (SRP)\nOpen/Closed Principle (OCP)\nLiskov Substitution Principle (LSP)\nInterface Segregation Principle (ISP)\nDependency Inversion Principle (DIP)\nSoftware architects should be familiar with SOLID principles and apply them in their\ndesigns and implementations. They should realize, though, that the principles are\nguidelines, and while you should strive to follow them, you may not always be able to\naccomplish that fully. Use your judgement as to when, and to what degree, these principles\nshould be followed. Now, let's explore the five design principles that make up SOLID in more detail. Single Responsibility Principle (SRP)\nThe Single Responsibility Principle (SRP) states that each class should have only one\nresponsibility, meaning it should do one thing and do that thing well. A responsibility is a\nreason to change, so each class should have only one reason to change. If we group together\nthe functions that need to change for the same reason, and separate out the things that\nchange for other reasons, we can create a class that follows this principle. If a class has multiple responsibilities, there is a likelihood that it is used in a greater\nnumber of places. When one responsibility is changed, not only do we run a higher risk of\nintroducing defects into other responsibilities in the same class, but there is a greater\nnumber of other classes that might be impacted. By following the single responsibility principle, if we need to change a particular\nresponsibility, that change would be located in a single class. This is the way to create an\northogonal software system. Applying this principle does not necessarily mean, as some posit, that each class should\nonly have a single public method. Although it does reduce the size of classes, the goal is to\nhave each class have a single responsibility. Fulfilling a single responsibility may require\nmultiple public methods.", "domains": ["Design Patterns", "Design Principles", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 213", "position": 213, "chunk_type": "semantic", "token_estimate": 348}
{"text": "Software Development Principles and Practices: Chapter 6\n[ 185 ]\nThis principle is related to the SoC principle because, as concerns are separated from each\nother, it facilitates the creation of classes that have a single responsibility. Following the\nDRY principle also helps us to abide by the SRP. By removing duplicate code and placing it\nin a single location so that it can be reused, the classes that need the logic do not have to\nrepeat it and therefore do not need to be responsible for it. Let's take a look at an example of the SRP. It is written in C#, although you will get the idea\neven if you do not use that particular programming language. We have an email service\nthat is responsible for sending out emails. There is a requirement to log information to a log\nfile, so it also contains logic to open, write to, and close a log file on a file system:\npublic class EmailService : IEmailService\n{\n    public SendEmailResponse SendEmail(SendEmailRequest request)\n    {\n        if (request == null)\n            throw new ArgumentNullException(nameof(request));\n        SendEmailResponse response = null;\n        try\n        {\n            // Logic to send email\n            // Log info about sent email\n            LogInfo(\"Some info message\");\n        }\n        catch (Exception ex)\n        {\n            // Log details about error\n            LogError(\"Some error message\");\n        }\n        return response;\n    }\n    private void LogInfo(string message)\n    {\n        // Logic to write to file system for logging\n    }\n    private void LogError(string message)\n    {\n        // Logic to write to file system for logging\n    }\n}", "domains": ["Design Principles"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 214", "position": 214, "chunk_type": "semantic", "token_estimate": 246}
{"text": "Software Development Principles and Practices: Chapter 6\n[ 186 ]\nAs you can see in this simple example, this class has more than one responsibility: sending\nout emails as well as handling the logging. This means that it has more than one reason to\nchange. If we wanted to change how emails were sent out, or allow for the logging to target\ncloud file storage instead of a file on a local file system, both would require changes to the\nsame class. This violates the SRP. Let's refactor this class so that it is only responsible for one thing:\npublic class EmailService : IEmailService\n{\n    private readonly ILogger _logger;\n    public EmailService(ILogger logger)\n    {\n        if (_logger == null)\n            throw new ArgumentNullException(nameof(logger));\n        _logger = logger;\n    }\n    public SendEmailResponse SendEmail(SendEmailRequest request)\n    {\n        if (request == null)\n            throw new ArgumentNullException(nameof(request));\n        SendEmailResponse response = null;\n        try\n        {\n            // Logic to send email\n            // Log info about sent email\n            _logger.LogInfo(\"Info message\");\n        }\n        catch (Exception ex)\n        {\n            // Log details about error\n            _logger.LogError($\"Error message: {ex.Message}\");\n        }\n        return response;\n    }\n}\nNow, the EmailService class is only responsible for sending out emails. The logic for\nlogging has been abstracted out to an interface. This dependency is injected in through the\nclass's constructor, and the implementation will be responsible for how logging works.", "domains": ["Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 215", "position": 215, "chunk_type": "semantic", "token_estimate": 214}
{"text": "Software Development Principles and Practices: Chapter 6\n[ 187 ]\nThis class is now only responsible for a single thing and therefore only has one reason to\nchange. Only changes related to the sending of emails will require modifications to this\nclass. It no longer violates the SRP. Open/Closed Principle (OCP)\nThe Open/Closed Principle (OCP) states that software components, such as classes, should\nbe open for extension but closed for modification. When requirements change, the design\nshould minimize the amount of changes that need to occur on existing code. We should be\nable to extend a component by adding new code without having to modify existing code\nthat already works. When Dr. Bertrand Meyer first came up with the principle in his book Object Oriented\nSoftware Construction, it focused on using implementation inheritance as the solution. If new\nfunctionality is needed, a new subtype is created and the base class and any existing\nsubtypes could remain unchanged. Software engineer Robert C. Martin, popularly known as Uncle Bob, redefined the principle\nin his article The Open-Closed Principle, and later in his book Agile Software Development,\nPrinciples, Patterns, and Practices, by stressing the importance of abstraction and the use of\ninterfaces. Using interfaces, we can change implementations as needed. In this way, we can\nchange behavior without having to modify existing code that relies on the interfaces. Let's take a look at an example. In this program, we have a Shape class with Rectangle\nand Circle classes that inherit from it. A Canvas class has methods that allow us to draw\nthe shapes:\npublic class Canvas\n{\n    public void DrawShape(Shape shape)\n    {\n        if (shape is Rectangle)\n            DrawRectangle((Rectangle)shape);\n        if (shape is Circle)\n            DrawCircle((Circle)shape);\n    }\n    public void DrawRectangle(Rectangle r)\n    {\n        // Logic to draw a rectangle\n    }\n    public void DrawCircle(Circle c)", "domains": ["Design Patterns", "Design Principles", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 216", "position": 216, "chunk_type": "semantic", "token_estimate": 297}
{"text": "Software Development Principles and Practices: Chapter 6\n[ 188 ]\n    {\n        // Logic to draw a circle\n    }\n}\nIf a new request comes in that requires us to be able to draw a new shape, such as a\ntriangle, we will have to modify the Canvas class. This class has not been designed to be\nclosed for modification and violates the OCP. Developers will need to understand the Canvas class in order to add a new shape. Modifications to the Canvas class will require unit tests to be revisited and introduces the\npossibility that existing functionality could be broken. Let's refactor this poor design so that it no longer violates the OCP:\npublic interface IShape\n{\n    void Draw();\n}\npublic class Rectangle : IShape\n{\n    public void Draw()\n    {\n        // Logic to draw rectangle\n    }\n}\npublic class Circle : IShape\n{\n    public void Draw()\n    {\n        // Logic to draw circle\n    }\n}\npublic class Canvas\n{\n    public void DrawShape(IShape shape)\n    {\n        shape.Draw();\n    }\n}", "domains": ["Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 217", "position": 217, "chunk_type": "semantic", "token_estimate": 166}
{"text": "Software Development Principles and Practices: Chapter 6\n[ 189 ]\nThe Canvas class is now much smaller, with each shape now having its own\nimplementation of how to draw itself. If there was a requirement to add a new shape, we\ncan create a new Shape class that implements the IShape interface without having to make\nany changes to the Canvas class or any of the other shapes. It is now open for extension but\nclosed for modification. Liskov Substitution Principle (LSP)\nInheritance is one of the four pillars of object-oriented programming (OOP). It allows\nsubclasses to inherit from a base class (sometimes referred to as the parent class), which\nincludes the properties and methods of the base class. When you first learned about\ninheritance, you may have been taught about \"is a\" relationships. For example, \"Car is a\nVehicle,\" if Car was a base class and Vehicle was a subtype of that base class. The Liskov Substitution Principle (LSP) is an object-oriented principle that states that\nsubtypes must be substitutable for their base types without having to alter the base type. If\na subtype is inherited from a base class, we should be able to substitute the subclass for that\nbase class without any issues. Subtypes extending a base class should do so without\nchanging the behavior of the base class. When the LSP is violated, it makes for confusing\ncode that is hard to understand. For a given base class or an interface the base class implements, the subtypes of that base\nclass should be usable through the base class or an interface the base class implements. The\nmethods and properties of the base class should make sense and work as intended for all of\nthe subtypes. If the classes work without an issue and behave as expected, the subtypes are\nsubstitutable for the base class. When the LSP is violated, this is not the case. Although the\ncode may compile, unexpected behavior or runtime errors may be experienced. A classic example to illustrate the LSP is that of a Rectangle class and a Square class. In\ngeometry, a square is a type of rectangle, so every square is a rectangle. The only difference\nis that with a square all of the sides have the same length.", "domains": ["Design Patterns", "Design Principles", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 218", "position": 218, "chunk_type": "semantic", "token_estimate": 379}
{"text": "Software Development Principles and Practices: Chapter 6\n[ 191 ]\n    Height = 2\n};\nConsole.WriteLine(sqrSubstitutedForRect.CalculateArea());\nConsole.ReadLine();\nThe area of the rect object will calculate to 20 and the area of the sqr object will calculate\nto 16, as expected. However, the area of the sqrSubstitutedForRect object will calculate\nto 4 and not 6. As these classes are currently designed, the Square subtype is really not\nsubstitutable for the Rectangle base class. This code will compile but it violates the SRP and leads to confusing results. This is a\nsimplistic example, but you start to get the idea. With complex class hierarchies, violations\nof the SRP can lead to defects, some of which can be difficult to solve. Interface Segregation Principle (ISP)\nInterfaces define methods and properties, but do not provide any implementation. Classes\nthat implement an interface provide the implementation. Interfaces define a contract, and\nclients can use them without concerning themselves with their implementation details. The\nimplementation can change and as long as a breaking change is not made to the interface,\nthe client does not need to change their logic. The Interface Segregation Principle (ISP) states that clients should not be forced to depend\non properties and methods that they do not use. When designing software, we prefer\nsmaller, more cohesive interfaces. If an interface is too large, we can logically split it up into\nmultiple interfaces so that clients can focus on only the properties and methods that are of\ninterest to them. When interfaces are too large and attempt to cover too many aspects of functionality, they\nare known as fat interfaces. The ISP is violated when classes are dependent on an interface\nwith methods they do not need. Violation of the ISP increases coupling and makes\nmaintenance more difficult. Let's look at an example where we are creating a system for a business that sells books. We\ncreate the following interface and class for the products:\npublic interface IProduct\n{\n    int ProductId { get; set; }\n    string Title { get; set; }\n    int AuthorId { get; set; }\n    decimal Price { get; set; }", "domains": ["Design Patterns", "Design Principles", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 220", "position": 220, "chunk_type": "semantic", "token_estimate": 349}
{"text": "Software Development Principles and Practices: Chapter 6\n[ 192 ]\n}\npublic class Book : IProduct\n{\n    public int ProductId { get; set; }\n    public string Title { get; set; }\n    public int AuthorId { get; set; }\n    public decimal Price { get; set; }\n}\nNow, let's say that the business owners want to start selling physical disks of movies. The\nproperties needed are very similar to IProduct, so an inexperienced developer might use\nthe IProduct interface for their Movie class:\npublic class Movie : IProduct\n{\n    public int ProductId { get; set; }\n    public string Title { get; set; }\n    public int AuthorId {\n        get => throw new NotSupportedException();\n        set => throw new NotSupportedException();\n    }\n    public decimal Price { get; set; }\n    public int RunningTime { get; set; }\n}\nAuthorId doesn't make sense for movies, but in this example, the developer decides to just\nmark the property as not supported. This is one of the code smells for violation of the ISP. If\na developer of a class that is implementing an interface finds themselves having to mark\nproperties or methods as not supported/not implemented, perhaps the interface needs to be\nsegregated. The Movie class also needs to represent the running time of the movie, which isn't a\nproperty needed by the Book class. If it is added to the IProduct interface, all classes that\nimplement that interface will need to be modified. This is another code smell indicating that\nthere may be an issue with the design of the interface. If we were to refactor this code so that it no longer violates the ISP, we would separate the\nIProduct interface into more than one so that the single \"fat\" interface is separated into\nmultiple smaller and more cohesive ones:\npublic interface IProduct\n{\n    int ProductId { get; set; }\n    string Title { get; set; }\n    decimal Price { get; set; }", "domains": ["Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 221", "position": 221, "chunk_type": "semantic", "token_estimate": 316}
{"text": "Software Development Principles and Practices: Chapter 6\n[ 193 ]\n}\npublic interface IBook : IProduct\n{\n    int AuthorId { get; set; }\n}\npublic interface IMovie : IProduct\n{\n    int RunningTime { get; set; }\n}\npublic class Book : IBook\n{\n    public int ProductId { get; set; }\n    public string Title { get; set; }\n    public int AuthorId { get; set; }\n    public decimal Price { get; set; }\n}\npublic class Movie : IMovie\n{\n    public int ProductId { get; set; }\n    public string Title { get; set; }\n    public decimal Price { get; set; }\n    public int RunningTime { get; set; }\n}\nWe can see that the IProduct interface only contains the properties needed by all\nproducts. The IBook interface inherits from IProduct, so that any class implementing\nIBook will need to implement the AuthorId property in addition to everything in\nIProduct. Similarly, IMovie inherits from IProduct and contains the RunningTime\nproperty that is only needed for movie implementations. We are no longer in violation of the ISP, and classes implementing these interfaces do not\nhave to deal with properties and methods that are not of interest to them.", "domains": ["Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 222", "position": 222, "chunk_type": "semantic", "token_estimate": 194}
{"text": "Software Development Principles and Practices: Chapter 6\n[ 194 ]\nDependency Inversion Principle (DIP)\nThe Dependency Inversion Principle (DIP) is a principle that describes how to handle\ndependencies and write loosely coupled software. In their book Agile Principles, Patterns,\nand Practices in C#, Robert C. Martin and Micah Martin state the principle as follows:\n\"The high-level modules should not depend on low-level modules. Both should depend on\nabstractions. Abstractions should not depend upon details. Details should depend upon abstractions.\" For example, let's say that Class A depends on Class B and Class B depends on Class C:\nIn a direct dependency graph, at compile time, Class A references Class B which references\nClass C. At runtime, the control flow will go from Class A to Class B to Class C. Class A\nand Class B will have to instantiate, or new up, their dependencies. This creates tightly\ncoupled code that is difficult to maintain and test. Changes to one of the dependencies may\nrequire changes to the classes that use those dependencies. Another disadvantage of this approach is that the code is not unit-testable because of its\ndependencies. We will not be able to create mock objects for dependencies because we are\nreferencing concrete types rather than abstractions. There is no way to inject the mock\nobjects so that we can create true unit tests that are not dependent on other classes. Rather than high-level classes being dependent on lower-level classes, they should depend\non abstractions through an interface. The interfaces do not depend on their\nimplementations. Instead, the implementations depend on the interfaces:", "domains": ["Design Patterns", "Design Principles", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 223", "position": 223, "chunk_type": "semantic", "token_estimate": 264}
{"text": "Software Development Principles and Practices: Chapter 6\n[ 195 ]\nWith an inverted dependency graph, at compile time, Class A depends on an abstraction\n(Interface B), which in turn depends on an abstraction (Interface C). Class B and Class C\nimplement Interface B and Interface C, respectively:\nAt run time, the flow of control goes through the interfaces, and each interface has an\nimplementation. The DIP is closely related to the inversion of control principle, with the inversion of control\nbeing applied to dependencies. Inversion of Control (IoC)\nInversion of Control (IoC) is a design principle in which a software system receives the\nflow of control from reusable code, such as a framework. In traditional procedural\nprogramming, a software system would call into a reusable library. The IOC principle\ninverts this flow of control by allowing the reusable code to call into the software system.", "domains": ["Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 224", "position": 224, "chunk_type": "semantic", "token_estimate": 145}
{"text": "Software Development Principles and Practices: Chapter 6\n[ 196 ]\nDevelopers are now so familiar with using a variety of frameworks, and even multiple\nframeworks on a single project, that the principle of IoC is no longer a novel one. Although\nthe principle of IoC can be applied to many more things than just dependencies, it has\nbecome closely related to dependencies for its IOC over them. This is the reason why DI containers were originally, and sometimes still are, referred to as\nIoC containers. DI containers are frameworks that provide DI functionality, and we will\ndiscuss them shortly. Dependency Injection (DI)\nDependency Injection (DI) is a technique that provides dependencies to a class, thereby\nachieving dependency inversion. Dependencies are passed (injected) to a client that needs\nit. There are a number of benefits to using DI in a software application. Bene\ufb01ts of DI\nDI removes hardcoded dependencies and allows them to be changed, at either runtime or\ncompile-time. If the implementation of a dependency is determined at runtime rather than\ncompile-time, this is known as late binding, or runtime binding. As long as we are\nprogramming to an interface, the implementation can be swapped out. DI allows us to write loosely coupled code, making applications easier to maintain, extend,\nand test. As we know, when we need to make a change, loosely coupled code allows us to\nmake a change in one part of our application without it affecting other areas of our\napplication. Testability increases in software applications that use DI. Loosely coupled code can be\ntested more easily. Code is written to depend on abstractions and not concrete\nimplementations, so dependencies can be mocked with unit testing frameworks. Following\nthe LSP, a class is not dependent on a concrete type; it is only dependent on the interface. As a result, we can inject mock objects for the dependencies by using the interface and\nwriting unit tests. Parallel development is made easier with DI. Developers can work on different pieces of\nfunctionality in parallel. Since the implementations are independent of each other, as long\nas the shared interfaces are agreed upon, development can occur at the same time. This is\nparticularly beneficial on larger projects with multiple teams. Each team can work\nindependently and share interfaces for the functionality that must be integrated.", "domains": ["Design Principles", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 225", "position": 225, "chunk_type": "semantic", "token_estimate": 385}
{"text": "Software Development Principles and Practices: Chapter 6\n[ 200 ]\nUnlike the example for constructor injection, notice that the private class-level variable to\nhold the dependency (_orgService) does not have the readonly keyword. In order to use\nproperty injection, we need the ability to set the variable outside of the variable's\ndeclaration and the constructor. Method injection\nMethod injection is similar to property injection, except a dependency is provided through\na method rather than a property:\npublic class Person\n{\n    private IOrgService _orgService;\n    public void Initialize(IOrgService orgService)\n    {\n        if (orgService == null)\n            throw new ArgumentNullException(nameof(orgService));\n        _orgService = orgService;\n    }\n}\nService Locator \nThe service locator pattern uses a locator object that encapsulates logic to determine and\nprovide an instance of the dependencies that are needed. Although it will vary depending\non your implementation, a sample call with a service locator might look something like the\nfollowing, resulting in an instance being provided based on the specified interface:\nvar cache = ServiceLocator.GetInstance<ILogger>();\nUsing the service locator pattern to get dependencies is considered to be an anti-pattern by\nsome people because it hides a class's dependencies. As opposed to constructor injection,\nwhere we can see the dependencies in the public constructor, we would have to look at the\ncode to find dependencies being resolved through the service locator. Hiding dependencies\nin this way can lead to runtime or compile-time issues, and make it more difficult to reuse\nthe code. This is particularly true if we do not have access to the source code, which might\nbe the case if we are using code from a third party. It is preferable to use an explicit method\nto acquire an instance of a dependency. DI containers\nA Dependency Injection container, sometimes referred as DI container or IoC container, is\na framework that helps with DI. It creates and injects dependencies for us automatically.", "domains": ["Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 229", "position": 229, "chunk_type": "semantic", "token_estimate": 309}
{"text": "Software Development Principles and Practices: Chapter 6\n[ 202 ]\nSome of the principles we have discussed, such as creating loosely coupled code, following\nprinciples such as DRY, SoC, and single responsibility, and using techniques such as DI,\ncreate code that is independent and decoupled from dependencies. These qualities make\nour code testable and allow us to focus a unit test on a single unit. Benefits of unit testing\nThere are important benefits to unit testing your software, such as improving the quality of\nthe software. With good unit tests, defects can be found before code is checked in or before\na build is attempted. By testing early and often, bugs can be fixed without affecting other\ncode. One way to make it easier to test early and often is to automate the unit testing. Unit testing\nis ideal for automation. While developers will want to manually execute unit tests as they\nchange code or prior to check-ins, unit tests can be automatically executed as part of some\nprocess, such as a build process. We will discuss automation and build processes in more\ndetail in Chapter 13, DevOps and Software Architecture. Debugging is made easier with regular unit testing because the source of the bug can be\nnarrowed down to recent changes. Unit tests also serve as a form of documentation. By\nlooking at the unit tests, one can begin to understand what functionality a particular class\nprovides and how it is supposed to work. Properties of a good unit test\nThere are various properties that make for a good unit test. Keep them in mind when\nwriting your unit tests. Unit tests should be atomic, deterministic, automated and\nrepeatable, isolated and independent, easy to set up and implement, and fast. Atomic\nUnit tests should only test a single assumption about a small piece of functionality. This\nassumption should focus on a behavior of the unit being tested, with the unit typically\nbeing a single method. Therefore, multiple tests are necessary to check all of the\nassumptions for a given unit. If you are testing multiple assumptions in a single test or\ncalling multiple methods in a single test, the scope of your unit test is probably too large.", "domains": ["Design Principles"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 231", "position": 231, "chunk_type": "semantic", "token_estimate": 369}
{"text": "Software Development Principles and Practices: Chapter 6\n[ 212 ]\nWalkthroughs\nA walkthrough is an informal method of review. In a walkthrough, the author of a design\nor code deliverable hosts a meeting in which they guide reviewers through the deliverable. Unlike a formal inspection, participants are not assigned specific roles (other than the\nhost/author). Walkthroughs are flexible and an organization can choose how they want to\norganize a walkthrough based on their needs. Participants can prepare for a walkthrough by looking at the deliverable beforehand. The\nfocus of the walkthrough is to identify potential defects. Although the focus is not to correct\nany defects found, unlike formal inspections, the group can decide to allow suggestions of\nchanges that can be made to the deliverable. Similar to formal inspections, management\nshould not attend a walkthrough so as not to influence the meeting in any way. Walkthroughs have been found to not be as effective as other review methods for\nevaluating and improving deliverables. However, they do allow a larger number of\nreviewers to participate at once. This provides an opportunity to get feedback from a more\ndiverse group. Summary\nEven though software engineering is a relatively new discipline compared to other types of\nengineering, a number of principles and practices have been established to create high-\nquality software systems. We learned that to design orthogonal software systems that can be extended while\nminimizing the impact to existing functionality, we need to focus on loose coupling and\nhigh cohesion. To minimize complexity in our software applications, a number of principles\ncan be applied, such as KISS, DRY, information hiding, YAGNI, and SoC. The SOLID design principles, which include the SRP, OCP, LSP, ISP, and DIP, can be used\nto create code that is more understandable, maintainable, reusable, testable, and flexible. A\nnumber of practices, such as unit testing, pair programming, and reviewing deliverables\ncan be used to identify defects and improve the quality of software systems. Software architecture patterns are reusable solutions that can be used to solve recurring\nproblems. In the next chapter, we will go over some of the common software architecture\npatterns so that you will be aware of them and can apply them appropriately to your\nsoftware applications.", "domains": ["Design Principles", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 241", "position": 241, "chunk_type": "semantic", "token_estimate": 371}
{"text": "7: Software Architecture Patterns\nSoftware architecture patterns are one of the most useful tools that can be leveraged for\ndesigning a software architecture. Some of the design issues we face as software architects\nalready have proven solutions. Experienced software architects are knowledgeable about\navailable architecture patterns and can recognize when one can be applied to a given\ndesign scenario. This chapter begins by explaining what software architecture patterns are and how they\ncan be used. It then goes into detail about some commonly used architecture patterns,\nincluding layered architecture, event-driven architecture (EDA), Model-View-Controller\n(MVC), Model-View-Presenter (MVP), Model-View-ViewModel (MVVM), Command\nQuery Responsibility Segregation (CQRS), and service-oriented architecture (SOA). In this chapter, we will cover the following topics:\nSoftware architecture patterns\nLayered architecture\nEvent-driven architecture , including event notifications, event-carried state\ntransfer, and event-sourcing\nModel-View-Controller pattern\nModel-View-Presenter pattern\nModel-View-ViewModel pattern\nCommand Query Responsibility Segregation\nService-oriented architecture", "domains": ["Architectural Patterns and Styles", "Design Patterns"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 242", "position": 242, "chunk_type": "semantic", "token_estimate": 144}
{"text": "Software Architecture Patterns: Chapter 7\n[ 216 ]\nFor example, if we were to follow the microservice architecture style for an application,\nwhich comes with the constraint that each service should be independent of the others, we\ncan expect such a system to have certain properties. A system that follows a microservice\narchitecture style will be able to deploy services independently, isolate faults for a\nparticular service, and use a technology of the team's choosing for a given service. A software architecture pattern is a particular arrangement of the available elements into a\nsolution for a recurring problem given a certain context. Given a particular architecture\nstyle, we can use the vocabulary of that style to express how we want to use the elements\navailable in that style in a certain way. When this arrangement is a known solution to a\ncommon, recurring problem in a particular context, it is a software architecture pattern. This book does not focus on making a distinction between these two terms and, for the\nmost part, uses the term software architecture pattern, as evidenced by the title of this\nchapter. Now that we know what software architecture patterns (and styles) are, let's explore some\nof the commonly used ones, beginning with the layered architecture. Layered architecture\nWhen partitioning a complicated software system, layering is one of the most common\ntechniques. In a layered architecture, the software application is divided into various\nhorizontal layers, with each layer located on top of a lower layer. Each layer is dependent\non one or more layers below it (depending on whether the layers are open or closed), but is\nindependent of the layers above it. Open versus closed layers\nLayered architectures can have layers that are designed to be open or closed. With a closed\nlayer, requests that are flowing down the stack from the layer above must go through it and\ncannot bypass it. For example, in a three-layer architecture with presentation, business, and\ndata layers, if the business layer is closed, the presentation layer must send all requests to\nthe business layer and cannot bypass it to send a request directly to the data layer.", "domains": ["Architectural Patterns and Styles"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 245", "position": 245, "chunk_type": "semantic", "token_estimate": 359}
{"text": "Software Architecture Patterns: Chapter 7\n[ 217 ]\nClosed layers provide layers of isolation, which makes code easier to change, write, and\nunderstand. This makes the layers independent of each other, such that changes made to\none layer of the application will not affect components in the other layers. If the layers are\nopen, this increases complexity. Maintainability is lowered because multiple layers can now\ncall into another layer, increasing the number of dependencies and making changes more\ndifficult. However, there may be situations in which it is advantageous to have an open layer. One of\nthem is to solve a common problem with the layered architecture, in which unnecessary\ntraffic can result when each layer must be passed even if one or more of them is just passing\nrequests on to the next layer. In our example of a three-layer architecture with presentation, business, and data layers,\nlet's say that we introduce a shared services layer between the business and data layers. This shared services layer may contain reusable components needed by multiple\ncomponents in the business layer. We may choose to place it below the business layer so\nthat only the business layer has access to it. However, now all requests from the business\nlayer to the data layer must go through the shared services layer even though nothing is\nneeded from that layer. If we make the shared services layer open, requests to the data\nlayer can be made directly from the business layer. The important point for software architects to understand when designing a layered\narchitecture is that there are advantages to closed layers and achieving layers of isolation. However, experienced software architects understand when it might be appropriate to\nopen a layer. It is not necessary to make all of the layers open or closed. You may\nselectively choose which layers, if any, are open. Tiers versus layers\nYou may have heard the terms tier and layer in reference to layered architectures. Before\nwe proceed with discussing layered architectures, these terms should be clarified. Layers are logical separations of a software application and tiers are physical ones. When partitioning application logic, layers are a way to organize functionality and\ncomponents. For example, in a three-layered architecture, the logic may be separated into\npresentation, business, and data layers. When a software architecture is organized into\nmore than one layer, it is known as a multi-layer architecture.", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 246", "position": 246, "chunk_type": "semantic", "token_estimate": 400}
{"text": "Software Architecture Patterns: For example, in a three-layered architecture, the logic may be separated into\npresentation, business, and data layers. When a software architecture is organized into\nmore than one layer, it is known as a multi-layer architecture. Different layers do not\nnecessarily have to be located on different physical machines. It is possible to have multiple\nlayers on the same machine.", "domains": ["Architectural Patterns and Styles"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 246", "position": 246, "chunk_type": "semantic", "token_estimate": 62}
{"text": "Software Architecture Patterns: Chapter 7\n[ 218 ]\nTiers concern themselves with the physical location of the functionality and components. A\nthree-tiered architecture with presentation, business, and data tiers implies that those three\ntiers have been physically deployed to three separate machines and are each running on\nthose separate machines. When a software architecture is partitioned into multiple tiers, it\nis known as a multi-tier architecture. Keep in mind that some people use the two terms interchangeably. When communicating\nwith others, if the distinction is important, you may want to be precise in your language\nand you may need to confirm with the other person what they mean when they use one of\nthe terms. Advantages of layered architectures\nThere are some key benefits to using a layered architecture. This pattern reduces\ncomplexity by achieving a Separation of Concerns (SoC). Each layer is independent and\nyou can understand it on its own without the other layers. Complexity can be abstracted\naway in a layered application, allowing us to deal with more complex problems. Dependencies between layers can be minimized in a layered architecture, which further\nreduces complexity. For example, the presentation layer does not need to depend directly\non the data layer and the business layer does not depend on the presentation layer. Minimizing dependencies also allows you to substitute implementations for a particular\nlayer without affecting the other layers. Another advantage of layered architectures is the fact that they can make development\neasier. The pattern is pervasive and well known to many developers, which makes using it\neasy for the development team. Due to the way that the architecture separates the\napplication logic, it matches up well with how many organizations hire their resources and\nallocate tasks during a project. Each layer requires a particular skill set and suitable\nresources can be assigned to work on each layer. For example, UI developers for the\npresentation layer, and backend developers for the business and data layers. This architecture pattern increases the testability quality attribute of software applications. Partitioning the application into layers and using interfaces for the interaction between\nlayers allows us to isolate a layer for testing and either mock or stub the other layers. For\nexample, you can perform unit testing on classes in your business layer without the\npresentation and data layers.", "domains": ["Architectural Patterns and Styles", "Design Principles", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 247", "position": 247, "chunk_type": "semantic", "token_estimate": 385}
{"text": "Software Architecture Patterns: Partitioning the application into layers and using interfaces for the interaction between\nlayers allows us to isolate a layer for testing and either mock or stub the other layers. For\nexample, you can perform unit testing on classes in your business layer without the\npresentation and data layers. The business layer is not dependent on the presentation layer\nand the data layer can be mocked or stubbed. Applications using a layered architecture may have higher levels of reusability if more than\none application can reuse the same layer. For example, if multiple applications target the\nsame business and/or data layers, those layers are reusable.", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 247", "position": 247, "chunk_type": "semantic", "token_estimate": 107}
{"text": "Software Architecture Patterns: Chapter 7\n[ 219 ]\nWhen an application using a layered architecture is deployed to different tiers, there are\nadditional benefits:\nThere is increased scalability as more hardware can be added to each tier,\nproviding the ability to handle increased workloads. A multi-tier application can experience greater levels of availability when\nmultiple machines are used per layer. Uptime is increased because, if a hardware\nfailure takes place in a layer, other machines can take over. Having separate tiers enhances security as firewalls can be placed in between the\nvarious layers. If a layer can be reused for multiple applications, it means that the physical tier\ncan be reused as well. Disadvantages of layered architectures\nAlthough layered architectures are commonly used, and for good reasons, there are\ndisadvantages to using them. Although the layers can be designed to be independent, a\nrequirement change may require changes in multiple layers. This type of coupling lowers\nthe overall agility of the software application. For example, adding a new field will require changes to multiple layers: the presentation\nlayer so that it can be displayed, the business layer so that it can be\nvalidated/saved/processed, and the data layer because it will need to be added to the\ndatabase. This can complicate deployment because, even for a change such as this, an\napplication may require multiple parts (or even the entire application) to be deployed. Another minor disadvantage is the fact that more code will be necessary for layered\napplications. This is to provide the interfaces and other logic that are necessary for the\ncommunication between the multiple layers. Development teams have to be diligent about placing code in the correct layer so as not to\nleak logic to a layer that belongs in another layer. Examples of this include placing business\nlogic in the presentation layer or putting data-access logic in the business layer. Although applications with good performance can be designed with layered architectures,\nif you are designing a high-performance application, you should be aware that there can be\ninefficiencies in having a request go through multiple layers. In addition, moving from one\nlayer to another sometimes requires data representations to be transformed. One way to\nmitigate this disadvantage is to allow some layers to be open but this should only be done if\nit is appropriate to open a layer.", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 248", "position": 248, "chunk_type": "semantic", "token_estimate": 391}
{"text": "Software Architecture Patterns: Chapter 7\n[ 220 ]\nThere are some additional disadvantages to layered architectures when they are deployed\nto multiple tiers:\nThe performance disadvantage of layered architectures has already been\nmentioned, but when those layers are deployed to separate physical tiers, as is\ncommon, there is an additional performance cost. With modern hardware, this\ncost may be small but it still won't be faster than an application that runs on a\nsingle machine. There is a greater monetary cost associated with having a multi-tier architecture. The more machines are used for the application, the greater the overall cost. Unless the hosting of the software application is handled by a cloud provider or\nhas otherwise been outsourced, an internal team will be needed to manage the\nphysical hardware of a multi-tier application. Client-server architecture (two-tier architecture)\nLayered architectures became very prevalent with the popularity of client-server software\nsystems. In a distributed application that uses a client-server architecture, also known as a\ntwo-tier architecture, clients and servers communicate with each other directly. A client\nrequests some resource or calls some service provided by a server and the server responds\nto the requests of clients. There can be multiple clients connected to a single server:\nThe Client part of the application contains the user interface code and the Server contains\nthe database, which traditionally has been a relational database management system\n(RDBMS). The majority of application logic in a client-server architecture is located on the\nserver, but some of it could also be located in the client. The application logic located on the\nserver might exist in software components, in the database, or both. When the client contains a significant portion of the logic and is handling a large share of\nthe workload, it is known as a thick, or fat, client. When the server is doing that instead, the\nclient is known as a thin client.", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 249", "position": 249, "chunk_type": "semantic", "token_estimate": 316}
{"text": "Software Architecture Patterns: Chapter 7\n[ 221 ]\nIn some client-server applications, the business logic is spread out between the client and\nthe server. If consistency isn't applied, it can make it difficult to always know where a\nparticular piece of logic is located. If a team isn't diligent, business logic might be duplicated on the client and the server,\nviolating the DRY principle. There may be instances in which the same piece of logic is needed on both the client and the\nserver. For example, there may be business logic needed by the user interface to validate a\npiece of data prior to submitting the data to the server. The server may need this same\nbusiness logic because it also needs to perform this validation. While centralizing this logic\nmay require additional communication between the client and the server, the alternative\n(duplication) lowers maintainability. If the business logic were to change, it would have to\nbe modified in multiple places. Using stored procedures for application logic\nIf application logic does exist in the database, it is commonly found in stored procedures. A\nstored procedure is a grouping of one or more Structured Query Language (SQL)\nstatements that forms a logical unit to accomplish some task. It can be used to do a\ncombination of retrieving, inserting, updating, and deleting data. It used to be popular to use stored procedures with client-server applications because their\nuse reduced the amount of network traffic between the client and the server. Stored\nprocedures can contain any number of statements within them and can call other stored\nprocedures. A single call from the client to the server is all that is needed to execute a stored\nprocedure. If that logic was not encapsulated inside a stored procedure, multiple calls\nwould need to be made over the network between the client and the server to execute the\nsame logic. Stored procedures are compiled the first time they are executed, at which time an execution\nplan is created that a database's query engine can use to optimize its use on subsequent\ncalls. In addition to some performance benefits, there are security advantages to using\nstored procedures. Users and applications do not need to be granted permissions to the\nunderlying database objects that a stored procedure uses, such as database tables.", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Design Principles", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 250", "position": 250, "chunk_type": "semantic", "token_estimate": 384}
{"text": "Software Architecture Patterns: In addition to some performance benefits, there are security advantages to using\nstored procedures. Users and applications do not need to be granted permissions to the\nunderlying database objects that a stored procedure uses, such as database tables. A user or\napplication can execute a stored procedure but it is the stored procedure that has control\nover what logic is executed and which database objects are used. Stored procedures\nincrease reusability in that once one is written and compiled, it can be reused in multiple\nplaces.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 250", "position": 250, "chunk_type": "semantic", "token_estimate": 89}
{"text": "Software Architecture Patterns: Chapter 7\n[ 222 ]\nAlthough there are benefits to using stored procedures, there are drawbacks. There are\nlimited coding constructs, as compared with high-level programming languages, that are\navailable for use in application logic. Having some of your business logic located in stored\nprocedures also means that your business logic isn't centralized. In modern application development, application logic should not be placed in stored\nprocedures. It belongs outside the data layer, independent and decoupled from the\nmechanism used to store the data. Based on that point, some have relegated stored\nprocedures to simple CRUD operations. However, if that is the case, stored procedures are\nnot providing much of a benefit. Although application logic should not be placed in stored procedures, they can still be of\nuse in some situations. For complex queries (for example, SQL queries with complex table\njoins and WHERE clauses) and queries requiring multiple statements and large amounts of\ndata, the performance advantages of stored procedures can be useful. N-tier architecture\nWith an n-tier architecture, also known as a multitier architecture, there are multiple tiers\nin the architecture. One of the most widely-used variations of this type of layered\narchitecture is the three-tier architecture. The rise of the web coincided with a shift from\ntwo-tier (client-server) architectures to three-tier architectures. This change was not a\ncoincidence. With web applications and the use of web browsers, rich client applications\ncontaining business logic were not ideal. The three-tier architecture separates logic into presentation, business, and data layers:", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 251", "position": 251, "chunk_type": "semantic", "token_estimate": 252}
{"text": "Software Architecture Patterns: Chapter 7\n[ 223 ]\nPresentation tier\nThe presentation tier provides functionality for the application's UI. It should provide an\nappealing visual design as it is the part of the application that users interact with and see. Data is presented to the user and input is received from users in this tier. Aspects of the\nusability quality attribute, which were discussed in Chapter 4, Software Quality Attributes,\nshould be the concern of the presentation tier. This tier should contain logic to render the user interface, including the placement of data\nin the appropriate UI components, formatting the data that is presented appropriately, and\nhiding/showing UI components as required. It should also provide some basic validation to help users avoid or minimize mistakes, such\nas ensuring the correct type of data is being entered for a given control and that the data is\nin the correct format. Developers should be careful not to introduce business logic into the\nvalidation, which should be handled by the business tier. The presentation tier should provide users with useful feedback, such as friendly and\ninformative messages, tooltips, visual feedback, such as a progress bar for long-running\nprocesses, and notifications to inform users about the completion or failure of\nasynchronous operations. Software architects should strive to design thin clients that minimize the amount of logic\nthat exists in the presentation tier. The logic in the presentation tier should focus on user\ninterface concerns. A presentation tier devoid of business logic will be easier to test. Business tier\nThe business tier, which is sometimes referred to as the application tier, provides the\nimplementation for the business logic of the application, including such things as business\nrules, validations, and calculation logic. Business entities for the application's domain are\nplaced in this tier. The business tier coordinates the application and executes logic. It can perform detailed\nprocesses and makes logical decisions. The business tier is the center of the application and\nserves as an intermediary between the presentation and data tiers. It provides the\npresentation tier with services, commands, and data that it can use, and it interacts with the\ndata tier to retrieve and manipulate data.", "domains": ["Design Patterns", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 252", "position": 252, "chunk_type": "semantic", "token_estimate": 361}
{"text": "Software Architecture Patterns: Chapter 7\n[ 224 ]\nData tier\nThe data tier provides functionality to access and manage data. The data tier contains a\ndata store for persistent storage, such as an RDBMS. It provides services and data for the\nbusiness tier. There are variations of n-tier architectures that go beyond just three tiers. For example, in\nsome systems, there is a data access or persistence layer in addition to a data or database\nlayer. The persistence layer contains components for data access, such as an object-\nrelational mapping (ORM) tool, and the database layer contains the actual data store, such\nas an RDBMS. One reason to separate these into two distinct layers is if you wanted the\nability to switch out your data access or database technology for a different one. Event-driven architecture\nAn event is the occurrence of something deemed significant in a software application, such\nas a state change, that may be of interest to other applications or other components within\nthe same application. An example of an event is the placement of a purchase order or the\nposting of a letter grade for a course that a student is taking. An event-driven architecture (EDA) is a distributed, asynchronous software architecture\npattern that integrates applications and components through the production and handling\nof events. By tracking events, we don't miss anything of significance related to the business\ndomain. EDAs are loosely coupled. The producer of an event does not have any knowledge\nregarding the event subscribers or what actions may take place as a result of the event. SOA can complement EDA because service operations can be called based on events being\ntriggered. The converse can also be designed, such that service operations raise events. EDAs can be relatively complex given their inherent asynchronous, distributed processing. As with any distributed architecture, issues may occur due to a lack of responsiveness,\nperformance issues, or failures with event mediators and event brokers (these components\nwill be described shortly). Event channels\nBefore we cover the two main event topologies for EDAs, let's go over the concept of event\nchannels because both topologies make use of them.", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 253", "position": 253, "chunk_type": "semantic", "token_estimate": 357}
{"text": "Software Architecture Patterns: Chapter 7\n[ 225 ]\nEvent messages contain data about an event and are created by event producers. These\nevent messages use event channels, which are streams of event messages, to travel to an\nevent processor. Event channels are typically implemented as message queues, which use the point-to-point\nchannel pattern, or message topics, which use the publish-subscribe pattern. Message queues\nMessage queues ensure that there is one, and only one, receiver for a message. In the\ncontext of event channels, this means that only one event processor will receive an event\nfrom an event channel. The point-to-point channel pattern is utilized for message queue\nimplementations. The point-to-point channel pattern\nThe point-to-point channel pattern is a messaging pattern used when we want to ensure\nthat there will be exactly one receiver for a given message. If a channel has multiple\nreceivers so that more than one message can be consumed concurrently, and more than one\nreceiver attempts to consume a message, the event channel will ensure that only one of\nthem succeeds. Since the event channel is handling that, it removes any need for\ncoordination between the event processors. Message topics\nMessage topics allow multiple event consumers to receive an event message. The publish-\nsubscribe pattern is used for the implementation of message topics. The publish-subscribe pattern\nThe publish-subscribe pattern, which is sometimes referred to as pub/sub for short, is a\nmessaging pattern that provides a way for a sender (publisher) to broadcast a message to\ninterested parties (subscribers). Rather than publishers sending messages directly to specific receivers as in the point-to-\npoint channel pattern, the messages can be sent without any knowledge of the subscribers\nor even if there are no subscribers. Similarly, it allows subscribers to show interest in a\nparticular message without any knowledge of the publishers or even if there are no\npublishers.", "domains": ["Architectural Patterns and Styles"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 254", "position": 254, "chunk_type": "semantic", "token_estimate": 309}
{"text": "Software Architecture Patterns: Chapter 7\n[ 226 ]\nEvent-driven architecture topologies\nThe two main topologies for EDAs are the mediator and broker topologies. The mediator topology\nA mediator topology for EDAs uses a single event queue and an event mediator to route\nevents to the relevant event processors. This topology is commonly used when multiple\nsteps are required to process an event. With the mediator topology, event producers send events into an event queue. There can\nbe many event queues in an EDA. Event queues are responsible for sending event messages\non to the event mediator. All of these events, referred to as initial events, go through an\nevent mediator. The event mediator then performs any necessary orchestration:\nAfter the event mediator orchestrates each event from the event queue, it creates one or\nmore asynchronous processing events based on the orchestration. These processing events\nget sent out to an event channel, which can either be a message queue or a message topic.", "domains": ["Architectural Patterns and Styles", "Design Patterns"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 255", "position": 255, "chunk_type": "semantic", "token_estimate": 162}
{"text": "Software Architecture Patterns: Chapter 7\n[ 227 ]\nMessage topics are more commonly used with the mediator topology due to the\norchestration involved, which allows multiple event processors, which will perform\ndifferent tasks, to receive an event message. This can be seen in the preceding diagram\nwhere some of the event channels are sending an event message to multiple event\nprocessors. Event processors listen in on event channels to pick up events and process them in line\nwith their design. In the Event processing styles section later in this chapter, we will cover the\ndifferent processing styles typically used by event processors. Event mediator implementations\nAn event mediator can be implemented in a number of different ways. For simple\norchestrations, an integration hub can be used. These typically allow you to define\nmediation rules using a domain-specific language (DSL) for the routing of events. Domain-specific languages, unlike a general-purpose language, such as C#, Java, or UML,\nallow expressions to be written for a particular domain. For a more complex orchestration of events, Business Process Execution Language\n(BPEL) can be used in conjunction with a BPEL engine. BPEL is an XML-based language\nthat is used to define business processes and their behavior. It is frequently used with SOA\nand web services. Large software applications with complex orchestration needs, which may even include\nhuman interactions, may opt to implement an event mediator that uses a business process\nmanager (BPM). Business process management involves modeling, automating, and\nexecuting business workflows. Some business process managers use Business Process\nModel and Notation (BPMN) to define business processes. BPMN allows for business\nprocess modeling using a graphical notation. Business process diagrams created with\nBPMN are similar to activity diagrams in UML. Software architects must understand the needs of the software application in order to select\nan appropriate event mediator implementation. The broker topology\nIn a broker topology, the event messages created by event producers enter an event broker,\nsometimes referred to as an event bus. The event broker contains all of the event channels\nused for the event flow. The event channels may be message queues, message topics, or\nsome combination of the two.", "domains": ["Design Patterns"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 256", "position": 256, "chunk_type": "semantic", "token_estimate": 358}
{"text": "Software Architecture Patterns: Chapter 7\n[ 228 ]\nUnlike the mediator topology, there is no event queue with the broker topology. The event\nprocessors are responsible for picking up events from an event broker:\nThis topology is ideal when the processing flow is fairly simple and there is no need for\ncentralized event orchestration. Events flow to the event processor from the event broker\nand, as part of the processing, new events may be created. This can be seen in the preceding diagram, where in some cases, events are flowing from\nthe event processors back to the event broker. A key to the broker topology is chaining\nevents in order to execute a particular business task. Event processing styles\nEvent processors are components that have a specific task and contain logic to analyze and\ntake action on events. Each event processor should be independent and loosely coupled\nwith other event processors. Once event messages reach event processors, there are three prevalent styles for processing\nevents: simple event processing (SEP), event stream processing (ESP), and complex event\nprocessing (CEP). The type of event processing needed by a particular application depends\non the processing complexity that is required. EDAs may utilize a combination of these\nthree styles.", "domains": ["Design Patterns", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 257", "position": 257, "chunk_type": "semantic", "token_estimate": 205}
{"text": "Software Architecture Patterns: Chapter 7\n[ 230 ]\nTypes of event-driven functionality\nEDA can mean different things. There are three main types of functionality that can\ntypically be found in systems that have an EDA: event notification, event-carried state\ntransfer, and event sourcing. Event-driven software systems can provide a combination of\nthe three. Event notification\nAn architecture that provides event notification is one in which the software system sends\na message when an event takes place. This functionality is the most common in software\nsystems that have an EDA. The mediator and broker topologies allow us to implement\nevent notifications. There is a loose coupling between the event producer and any event consumers as well as\nbetween the logic that sends event messages and logic that responds to the events. This\nloose coupling allows us to change the logic in one without affecting the other. Event\nprocessor components are single-purpose and independent of other event processors,\nallowing them to be modified without affecting others. The drawback to the loose coupling between event producers and event consumers is that\nit can be difficult to see the logical flow of event notifications. This added complexity also\nmakes it more difficult to debug and maintain. There aren't specific statements you can look\nat to see what logic will be executed. A variety of event consumers, including ones in\nsoftware systems other than the one that produced the event notification, may react to an\nevent. Sometimes the only way to understand the logical flow is to monitor your systems to\nsee the flow of event messages. Event-carried state transfer\nEvent-carried state transfer is a variation on event notification. Its use is not as common as\nregular event notification. When an event consumer receives an event notification, it may\nneed more information from the event producer in order to take the action that they want\nto take.", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 259", "position": 259, "chunk_type": "semantic", "token_estimate": 312}
{"text": "Software Architecture Patterns: Chapter 7\n[ 231 ]\nFor example, a sales system may send a new order event notification and a shipping system\nmay subscribe to this type of event messages. However, in order to take appropriate action,\nthe shipping system now needs additional information about the order, such as the\nquantity and type of line items that are in the order. This requires the shipping system to\nquery the sales system in some way, such as through an API, for this information. While the event publisher may not need to know anything about their subscribers, the\nsubscriber is coupled to the producer in the sense that it needs to be aware of the producer\nand have a way to get more information from the producer. Callbacks to the system that produced an event notification for more data in order to\nhandle an event increase network load and traffic. One way to resolve this is to add state\ninformation to the events so that they contain enough information to be useful for potential\nconsumers. For example, an event notification for a new order could contain the line item\ndetails needed by the shipping system so that no callback is required. The shipping system\ncan keep its own copy of only the order details that it needs. Although more data is being passed around, we gain a greater level of availability and\nresilience. The shipping system can function, at least with orders it has already received,\neven if the order system is temporarily unavailable. The shipping system does not need to\ncall back to the order system after the initial event notification is received, which can be\nparticularly beneficial if contacting and receiving data from the order system is slow. However, with greater availability comes lower consistency. The replication of some data\nbetween the order and shipping systems lowers the consistency of the data. Event-sourcing\nA system can use a data store to read and update the application's current state, but what if\nthere are requirements to know the details of the state changes that got us to the current\npoint? With event-sourcing, the events that take place in a system, such as state changes,\nare persisted in an event store. Having a complete record of all the events that took place\nallows it to serve as a source of truth.", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 260", "position": 260, "chunk_type": "semantic", "token_estimate": 391}
{"text": "Software Architecture Patterns: With event-sourcing, the events that take place in a system, such as state changes,\nare persisted in an event store. Having a complete record of all the events that took place\nallows it to serve as a source of truth. Replaying events from an event log can be used to\nrecreate an application's state. Event-sourcing works in a similar way to a transaction log in a database system. The\ntransaction log records all of the modifications that have been made to a database. This\nallows for rollbacks of transactions and also allows us to recreate the system state up to a\nparticular point, such as right before a failure occurred.", "domains": ["Architectural Patterns and Styles", "Design Patterns"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 260", "position": 260, "chunk_type": "semantic", "token_estimate": 113}
{"text": "Software Architecture Patterns: Chapter 7\n[ 232 ]\nEvents should be immutable as they represent something that has already taken place. Actions may take place downstream as the result of an event, so if an event could be\nchanged after the fact, it could put your system in an inconsistent state. If an update or\ncancellation of an event is necessary, a compensating event should be created. Compensating logic can be executed based on such events and can apply the necessary\nbusiness rules to apply counter-operations. This will ensure that the event store is still a\nsource of truth and that we can replay all of the events to recreate an application's state. The benefits of event-sourcing include the fact that it can aid in debugging a system. It\nprovides the ability to take events and run them through the system to see how the system\nwill behave. This can be used to determine the cause of a problem. Event-sourcing also\nprovides detailed auditing. The complete record of events allows us to see what happened,\nhow it happened, when it happened, and other details. Although event-sourcing can be very useful, it does introduce some added complexity into\na software system. Multiple instances of an application and multithreaded applications\nmight be persisting events to an event store. The system must be designed to ensure that\nevents are processed in the correct order. The code that processes events and the event schema can change over time. Consideration\nmust be given to ensure that older events, possibly with different event schemas, can still\nbe replayed with the current logic. If part of the sequence of events includes the use of an external system, consideration has to\nbe given to storing the responses from the external system as events. This will ensure that\nwe can replay events accurately to rebuild the application's state without having to call the\nexternal system again. The Model-View-Controller pattern\nThe Model-View-Controller (MVC) pattern is a software architecture pattern that is\nwidely used for the UI of an application. It is particularly well suited to web applications,\nalthough it can also be used for other types of applications, such as desktop applications. The pattern provides a structure for building user interfaces and provides a separation of\nthe different responsibilities involved. A number of popular web and application\ndevelopment frameworks make use of this pattern.", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 261", "position": 261, "chunk_type": "semantic", "token_estimate": 394}
{"text": "Software Architecture Patterns: Chapter 7\n[ 233 ]\nThe MVC pattern consists of the Model, View, and Controller:\nThe model, view, and controller all have distinct responsibilities for the user interface. Let's\ntake a look at each of them more closely. Model\nThe model manages the application data and the state. Among its responsibilities is the\nprocessing of data to and from a data store, such as a database. A model is independent of\nthe controllers and the views, allowing them to be reused with different user interfaces. This also allows them to be tested independently. Models receive directives from controllers to retrieve and update data. Models also provide\napplication state updates. In some variations of MVC, the model is passive and must receive\na request to send out an application state update. In other variations, a view may be active\nand push notifications of model state changes to a view. View\nThe view is responsible for the presentation of the application. It is the part of the\napplication that is visible to the user. The view displays data to the user in an appropriate\ninterface based on information received from the controller. If the model is providing\napplication state updates directly to views, the views may also be updated based on these\nnotifications. As users manipulate a view, such as providing input or providing some user action, the\nview will send this information to a controller.", "domains": ["Design Patterns", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 262", "position": 262, "chunk_type": "semantic", "token_estimate": 237}
{"text": "Software Architecture Patterns: Chapter 7\n[ 234 ]\nController\nAs users navigate a web application, requests are routed to the appropriate controller based\non routing configuration. A controller acts as an intermediary between the model and the\nview. A controller executes application logic to select the appropriate view and sends it the\ninformation that it needs to render the user interface. Views notify controllers of user\nactions so that the controller can respond to them. Controllers will update the model based\non user actions. Advantages of the MVC pattern\nUsing the MVC pattern allows for a separation of concerns. By separating the presentation\nfrom the data, it makes it easier to change one of them without affecting the other. It also \nmakes each part easier to test. However, it is difficult to achieve a complete separation. For\nexample, adding a new field to the application will require a change in both the data and\nthe presentation. The MVC pattern makes presentation objects more reusable. Separating the user interface\nfrom the data allows UI components to be reused. It also means that a model can be reused\nwith more than one view. The separation of the presentation from the business logic and data allows developers to\nspecialize in either frontend or backend development. This can also speed up the\ndevelopment process as some tasks can take place in parallel. For example, one developer\ncan work on the user interface while another works on the business logic. Disadvantages of the MVC pattern\nIf your development team is not set up so that developers are focused on either frontend or\nbackend development, this does require developers to be skilled in both areas (full-stack\ndevelopers). This will require developers who are skilled in multiple technologies. If a model is very active and is providing notifications directly to views, frequent changes\nto a model may result in excessive updates to views.", "domains": ["Design Principles", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 263", "position": 263, "chunk_type": "semantic", "token_estimate": 316}
{"text": "Software Architecture Patterns: Chapter 7\n[ 235 ]\nThe Model-View-Presenter pattern\nThe Model-View-Presenter (MVP) pattern is a variation on the MVC pattern. Like the\nMVC pattern, it provides a separation between UI logic and business logic. However, the\npresenter takes the place of the controller in the MVP pattern. Each view in the MVP pattern typically has a corresponding interface (view interface). Presenters are coupled with the view interfaces. As compared with the MVC pattern, the\nview is more loosely coupled to the model because the two do not interact with each other\ndirectly:\nBoth web and desktop applications can use the MVP pattern. The main components of this\npattern are the Model, View, and Presenter. Model\nAs was the case with the MVC pattern, the model represents the business model and the\ndata. It interacts with the database to retrieve and update data. The model receives\nmessages from the presenter for updates and reports state changes back to the presenter. Models in the MVP pattern do not interact directly with views and only interact with the\npresenter. View\nThe view is responsible for displaying the user interface and data. Each view in the MVP\npattern implements an interface (view interface). As the user interacts with the view, the\nview will send messages to the presenter to act on the events and data.", "domains": ["Design Patterns", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 264", "position": 264, "chunk_type": "semantic", "token_estimate": 223}
{"text": "Software Architecture Patterns: Chapter 7\n[ 236 ]\nPresenters are loosely coupled with views through the view interface. Views are more\npassive in the MVP model and rely on the presenter to provide information on what to\ndisplay. Presenter\nThe presenter is the intermediary between the model and the view. It interacts with both of\nthem. Each view has a presenter and the view notifies the presenter of user actions. The\npresenter updates the model and receives state changes from the model. A presenter will receive data from the model and format it for the view to display, taking\nan active role in presentation logic. Presenters encapsulate presentation logic and views\nplay a more passive role. Unlike the MVC pattern, where a controller can interact with multiple views, in the MVP\npattern, each presenter typically handles one, and only one, view. The Model-View-ViewModel pattern\nThe Model-View-ViewModel (MVVM) pattern is another software architecture pattern\nand it shares similarities with MVC and MVP in that they all provide a SoC. Partitioning\nthe various responsibilities makes an application easier to maintain, extend, and test. The\nMVVM pattern separates the UI from the rest of the application:", "domains": ["Design Patterns", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 265", "position": 265, "chunk_type": "semantic", "token_estimate": 193}
{"text": "Software Architecture Patterns: Chapter 7\n[ 237 ]\nThere is typically a significant amount of interaction between views and ViewModels,\nfacilitated by data binding. The MVVM pattern works well for rich desktop applications,\nalthough it can be used for other types of application, such as web and mobile applications. An example of a framework that can be used to build MVVM applications is Windows\nPresentation Foundation (WPF). The main components of MVVM are the Model, View, and ViewModel. Let's take a look at\neach of these in more detail. Model\nThe model in the MVVM pattern plays a similar role as in MVC and MVP. It represents the\nbusiness domain object and the data. The model uses the database to retrieve and update\ndata. In MVVM applications, there may be direct binding with model properties. As a result,\nmodels commonly raise property changed notifications. View\nThe view is responsible for the user interface. It is the part of the application that is visible\nto users. In the MVVM pattern, the view is active. Unlike a passive role where the view is\ncompletely manipulated by a controller or a presenter, and does not have knowledge of the\nmodel, in MVVM views are aware of the model and ViewModel. While views handle their own events, they do not maintain state. They must relay user\nactions to the ViewModel, which can be done through a mechanism such as data binding or\ncommands. A goal with the MVVM pattern is to minimize the amount of code in views. ViewModel\nThe ViewModel in the MVVM pattern is similar to the controller and presenter objects that\nwe covered with the MVC and MVP patterns in that they coordinate between the view and\nthe model. ViewModels provide data to views for display and manipulation, and also contain\ninteraction logic to communicate with views and models. ViewModels must be capable of\nhandling user actions and data input sent from views. It is the ViewModel that contains\nnavigation logic to handle moving to a different view.", "domains": ["Design Patterns", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 266", "position": 266, "chunk_type": "semantic", "token_estimate": 337}
{"text": "Software Architecture Patterns: Chapter 7\n[ 238 ]\nViews and ViewModels communicate through multiple methods, such as data binding,\ncommands, method calls, properties, and events. The Command Query Responsibility\nSegregation pattern\nCommand Query Responsibility Segregation (CQRS) is a pattern in which the model that\nis used to read information is separated from the model that is used to update information. In a more traditional architecture, a single object model is used for both reading and\nupdating data:\nCompromises become necessary in order to use a single object model as domain classes are\nrequired to serve all purposes. The same representation of an entity must support all of\nthe create, read, update, and delete (CRUD) operations, making them larger than they\nneed to be in all circumstances. They contain all of the properties the object will need for various scenarios. If the class is\nmore than just a data transfer object (DTO), it may also contain methods for behavior. With this approach, classes are not ideal for all of the situations in which they need to be\nused as there is often a mismatch between what is required by the read and write\nrepresentations of the data. This can also make managing security and authorization more\ncomplex as each class is used for both read and write operations. In a collaborative domain, multiple operations may be taking place in parallel on the same\nset of data. There is a risk of data contention if records are locked, or update conflicts due to\nconcurrent updates. Workloads between read and write tasks differ, which means that they\nalso have different performance and scalability requirements.", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Software Quality Attributes", "Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 267", "position": 267, "chunk_type": "semantic", "token_estimate": 271}
{"text": "Software Architecture Patterns: Chapter 7\n[ 239 ]\nThe query model and the command model\nOne way to overcome the challenges of using a single object model for both queries and\ncommands is to separate the two. This pattern, known as CQRS, results in two separate\nmodels. The query model is responsible for reads and the command model is responsible\nfor updates:\nQuery objects only return data and do not alter state, while command objects alter state and\ndo not return data. Another way of looking at this concept is that asking a question (a\nquery) should not alter the answer. In order to perform an action that will mutate state, a\ncommand is used. When the system needs to read data, it goes through the query model, and when the\nsystem needs to update data, it goes through the command model. As part of processing a\ncommand, the system may need to read data, but beyond what is necessary for completing\na command, the reading of data should go through the query model. Although not required, if CQRS is taken to the next level, the query and command models\ncan be made to utilize separate databases. This gives each of the two models its own\nschema, which can be optimized for its specific usage. If separate databases are used, the\ntwo databases must be kept in sync. One way to do that is through the use of events.", "domains": ["Architectural Patterns and Styles", "Design Patterns"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 268", "position": 268, "chunk_type": "semantic", "token_estimate": 239}
{"text": "Software Architecture Patterns: Chapter 7\n[ 240 ]\nUsing event-sourcing with CQRS\nAlthough CQRS can be used without events, they do complement each other so it is\ncommon for systems that use CQRS to leverage the use of events. Events are one way to\neffectively communicate state changes so that the query model can stay up to date as the\ncommand model updates data. As we saw in the Event-sourcing section earlier in this chapter, event-sourcing involves\npersisting events that take place in a system such that the event store can serve as the\nrecord of truth. When the command model alters the state of the system, events can be\nraised so that the query model can be kept in sync. Keeping the query and command models in sync is necessary when the query and\ncommand models use separate data stores. In addition, a data store for the query model\nmay contain denormalized data that has been optimized for particular queries. Having an\nevent store allows us to replay past events to recreate the system's current state, which can\nbe useful for updating the denormalized data in the query model's data store. Advantages of CQRS\nCQRS is well suited to complex domains and it provides a separation of concerns that helps\nto minimize and manage the complexity. Separating a system into query and command\nmodels makes it more maintainable, extensible, and flexible. Development teams can also\nbe organized so that one team focuses on the query model while another focuses on the\ncommand model. Segregating the responsibility between commands and queries can help to improve\nperformance, scalability, and security. Performance can be improved by optimizing the\nschema specifically for each model. The schema for the query model can be optimized for\nqueries, while the schema for the command model can be optimized for updates. Data in\nthe query model's data store can be denormalized in order to increase the performance of\nqueries that the application needs to execute. Workloads between read and write operations will differ and using CQRS allows you to\nscale each of them independently of the others. Security is improved with CQRS because it\nmakes it easier than with a single object model to ensure that only the right classes can\nupdate data. Security can be easier to implement and test when using CQRS because each class is either\nused for reads or writes, but not both.", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Design Principles", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 269", "position": 269, "chunk_type": "semantic", "token_estimate": 400}
{"text": "Software Architecture Patterns: Security is improved with CQRS because it\nmakes it easier than with a single object model to ensure that only the right classes can\nupdate data. Security can be easier to implement and test when using CQRS because each class is either\nused for reads or writes, but not both. This reduces the chance of inadvertently exposing\ndata and operations that should not be available to a particular user in a certain context.", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 269", "position": 269, "chunk_type": "semantic", "token_estimate": 76}
{"text": "Software Architecture Patterns: Chapter 7\n[ 241 ]\nDisadvantages of CQRS\nFor systems that simply need basic CRUD operations, implementing a CQRS system may\nintroduce unnecessary complexity. CQRS systems have a higher degree of complexity,\nespecially when combined with event-sourcing. For this reason, it is important to\nunderstand that CQRS is not applicable to all situations. Software architects should be\naware that CQRS does not have to be applied to the entirety of a software system. It can be\napplied to just some subsystems of a larger architecture where it will be of the greatest\nbenefit. While using different data stores for query and command models can improve performance\nand increase security, you do have to consider that when you perform read operations, you\nmay be reading data that is stale. If separate databases are used in a CQRS system, the database used for reads must be kept\nup to date with the database used for writes. The software system will follow an eventual\nconsistency model where if no new updates are made to a given item, eventually all access\nto that item will acquire the latest data. Whether the system is using event-sourcing or some other mechanism to keep the two in\nsync, there will be some time delay, even if it is a small one, before they are made\nconsistent. This means that any data that is read could potentially be stale if the latest\nupdates to it have not been applied yet. This is in contrast with a strong consistency model,\nin which all data changes are atomic and a transaction is not allowed to complete until all of\nthe changes have been completed successfully or, in the case of a failure, everything has\nbeen undone. Service-oriented architecture\nService-oriented architecture (SOA) is an architectural pattern for developing software\nsystems by creating loosely coupled, interoperable services that work together to automate\nbusiness processes. A service is a part of a software application that performs a specific\ntask, providing functionality to other parts of the same software application or to other\nsoftware applications. Some examples of service consumers include web applications,\nmobile applications, desktop applications, and other services.", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 270", "position": 270, "chunk_type": "semantic", "token_estimate": 359}
{"text": "Software Architecture Patterns: Chapter 7\n[ 242 ]\nSOA achieves a SoC, which is a design principle that separates a software system into parts,\nwith each part addressing a distinct concern. We discussed SoC in Chapter 6, Software\nDevelopment Principles and Practices. A key aspect of SOA is that it decomposes application\nlogic into smaller units that can be reused and distributed. By decomposing a large problem\ninto smaller, more manageable concerns satisfied by services, complexity is reduced and\nthe quality of the software is improved. Each service in a SOA encapsulates a certain piece of logic. This logic may be responsible\nfor a very specific task, a business process, or a subprocess. Services can vary in size and\none service can be composed of multiple other services to accomplish its task. What makes SOA different from other distributed\nsolutions? Distributing application logic and separating it into smaller, more manageable units is not\nwhat makes SOA different from previous approaches to distributed computing. Naturally,\nyou might think the biggest difference is the use of web services, but keep in mind that\nSOA does not require web services, although they happen to be a perfect technology to\nimplement with SOA. What really sets SOA apart from a traditional distributed\narchitecture is not the use of web services, but how its core components are designed. Although SOA shares similarities with earlier distributed solutions, it is much more than\njust another attempt to create reusable software. The differences undoubtedly provide\nsignificant new value to organizations when implemented properly. Many benefits can be\nrealized from a properly designed SOA. Benefits of using a SOA\nThere are a number of benefits from using a SOA, including:\nIncreases alignment between business and technology\nPromotes federation within an organization\nAllows for vendor diversity\nIncreases intrinsic interoperability\nWorks well with agile development methodologies", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 271", "position": 271, "chunk_type": "semantic", "token_estimate": 305}
{"text": "Software Architecture Patterns: Chapter 7\n[ 244 ]\nIncreases intrinsic interoperability\nSOA provides increased intrinsic interoperability for an organization. It allows for the\nsharing of data and the reuse of logic. Different services can be assembled together to help\nautomate a variety of business processes. It can allow an existing software system to\nintegrate with others through web services. Greater interoperability can lead to the\nrealization of other strategic goals. Works well with agile development methodologies\nAnother benefit of SOA is that it lends itself well to agile software development\nmethodologies. The fact that complex software systems are broken down into services with\nsmall, manageable units of logic fits well with an iterative process and how tasks are\nallocated to resources. You may also find that having developers take on tasks is easier with SOA because each\ntask can be made to be manageable in size and more easily understood. Although this is\nbeneficial for any developer, it is particularly helpful for junior developers or those who are\nnew to a project and may not have as much experience with the functionality and business\ndomain. Cost-benefit analysis of SOA\nAs a software architect, if you are considering SOA for an application, you will need to\nexplain the reasons why you are considering it. Adopting an SOA comes at some cost, but\nthere are points you can make to justify the costs and ways that you can alleviate them. The cost of implementing a SOA may outweigh the benefits for some organizations, so each\ncase must be considered separately. While it may not be appropriate for some organizations\nto implement an SOA, for others a properly designed SOA will bring many benefits,\nincluding a positive return on investment. Adopting SOA can be a gradual, evolutionary process. Because a contemporary SOA\npromotes federation, creating an SOA does not have to be an all-or-nothing process. An\norganization does not have to replace all existing systems at once. Legacy logic can be\nencapsulated and can work with new application logic. As a result, the adoption of SOA\nand its related costs can be spread out over time. Adopting SOA leads to reduced integration expenses. A loosely coupled SOA should\nreduce complexity and therefore will reduce the cost to integrate and manage such systems. Loosely coupled services are more flexible and can be used in more situations.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 273", "position": 273, "chunk_type": "semantic", "token_estimate": 392}
{"text": "Software Architecture Patterns: Chapter 7\n[ 245 ]\nSOA can increase asset reuse. It is common for each application to be built in isolation,\nleading to higher development costs and greater maintenance costs over time. However, by\ncreating business processes by reusing existing services, costs and time to market can be\nreduced. SOA increases business agility. Change is something that all organizations must face. Regardless of the cause of the change, by using loosely coupled services, organizational\nagility is increased and both the time and cost to adapt to change are reduced. Adopting SOA reduces business risk and exposure. A properly designed SOA facilitates the\ncontrol of business processes, allows for the implementation of security and privacy\npolicies, and provides audit trails for data, which can all reduce risk. It can also help with\nregulatory compliance. The penalties for non-compliance can be significant, and SOA can\nprovide organizations with increased business visibility that reduces the risk of changing\nregulations. Challenges with SOA\nAlthough adopting an SOA does provide many benefits, it also introduces some new\ncomplexities and challenges. SOA solutions may allow organizations to do more, including\nautomating more of their business processes. This can cause enterprise architectures to\ngrow larger in scope and functionality as compared to legacy systems. Taking on a larger\nscope of functionality will add complexity to a software system. In an SOA, new layers may be added to software architectures, providing more areas where\nfailure can occur and making it more difficult to pinpoint those failures. In addition, as\nincreasing numbers of services are created, deploying new services and new versions of\nexisting services must be managed carefully so that troubleshooting can be effective when\nan error occurs with a specific transaction. Another challenge with successful SOA adoption is related to people and not technology. SOA is a mature architectural style that has been around a long time. The technology exists\nto allow organizations to automate a variety of complex business processes. However,\npeople can be a challenge to SOA-adoption because there are still technical and business\nprofessionals who are not familiar with SOA and really do not know what it means. People\ncan also be naturally resistant to change, and if your organization is not already using SOA,\nchange will be necessary. In order for a SOA to be successful, there has to be buy-in from\nthe people in the organization.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 274", "position": 274, "chunk_type": "semantic", "token_estimate": 397}
{"text": "Software Architecture Patterns: Chapter 7\n[ 246 ]\nKey principles for service orientation\nService-oriented solutions are designed so that they adhere to certain key principles. They\nare:\nStandardized service contract\nService loose coupling\nService abstraction\nService reusability\nService autonomy\nService statelessness\nService discoverability\nService composability\nThese principles are detailed in Thomas Erl's book, Service-Oriented Architecture, Second\nEdition. Service-orientation principles are applied to the service-oriented analysis and\ndesign phases of the SOA delivery life cycle. Standardized service contract\nEach service should have a standardized service contract, consisting of a technical interface\nand service description. Even though we want services to be independent, they have to\nadhere to a common agreement so that units of logic can maintain a certain level of\nstandardization. In order to have standardized service contracts, all service contracts within a particular\nservice inventory should follow a set of design standards. Standardization enables\ninteroperability and allows the purpose of services to be more easily understood. Service loose coupling\nServices should be loosely coupled and independent of each other. Service contracts should\nbe designed to have independence from service consumers and from their\nimplementations. Loosely coupled services can be modified faster and easier. Decoupling service contracts\nfrom their implementations allows service contracts to be modified with minimal impact to\nservice consumers and service implementations. By minimizing the dependencies between\nservices, each service can change and evolve independently while minimizing the effects of\nthose changes on other services.", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 275", "position": 275, "chunk_type": "semantic", "token_estimate": 238}
{"text": "Software Architecture Patterns: Chapter 7\n[ 247 ]\nService abstraction\nService contracts should only contain information that it is necessary to reveal, and service\nimplementations should also hide their details. Any information that is not essential to\neffectively use the service can be abstracted out. Design decisions, such as the technology used for a service, can be abstracted away. This\nfollows the information hiding principle that was covered in Chapter 6, Software\nDevelopment Principles and Practices. If a design decision needs to be changed later, the goal\nis that it can be made with minimal impact. Service reusability\nServices should be designed with reusability in mind, with their service logic being\nindependent of a particular technology or business process. When services can be reused\nfor different purposes, software development teams experience increased productivity,\nleading to savings in both costs and time. Service reusability increases organizational agility because organizations can use existing\nservices to respond to new business automation needs. Existing services can be composed\ntogether to create solutions for new problems or to take advantage of a new opportunity. Reusable services can accelerate development and may allow a feature or product to reach\nthe market faster. In some cases, this can be critical for a project. Decomposing tasks into more services for reusability requires more analysis and potentially\nintroduces more complexity. However, when reusable services are designed correctly, there\ncan be significant long-term cost savings. If a need arises that is satisfied by an existing\nservice, resources do not have to be devoted to working on it. Service reuse leads to higher quality software because existing services have already been\ntested. They may already be in production and, if there were any defects with the service,\nthey may have already been exposed and corrected. Service autonomy\nServices should be designed to be autonomous, with more independence from their\nruntime environments. The design should seek to provide services with increased control\nover their runtime environments. When services can operate with less dependence on resources in their runtime\nenvironments that they cannot control, this leads to better performance and increased\nreliability of those services at runtime.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 276", "position": 276, "chunk_type": "semantic", "token_estimate": 354}
{"text": "Software Architecture Patterns: Chapter 7\n[ 248 ]\nService statelessness\nService designs should strive to minimize the amount of state management that takes place,\nand separate state data from services. Services can reduce their resource consumption if they do not manage state when it is\nunnecessary, which will allow them to handle more requests reliably. Having statelessness\nin services improves service scalability and improves the reusability of services. Service discoverability\nServices need to be discoverable. By including consistent and meaningful metadata with a\nservice, the purpose of the service and the functionality it provides can be communicated. Service developers are required to provide this metadata. Services should be discoverable by humans who are searching manually as well as software\napplications searching programmatically. Services must be aware of each other for them to\ninteract. Service composability\nServices should be designed so that they are composable. This is the ability to use a service\nin any number of other services, and those services may themselves be composed of other\nservices. Some other service-orientation principles facilitate service composability. Service\ncomposition is heavily related to service reusability. The ability to create solutions by\ncomposing existing services provides organizations with one of the most important SOA\nbenefits: organizational agility. SOA delivery strategies\nThere are three main SOA delivery strategies: top-down, bottom-up, and agile. A delivery\nstrategy is needed to coordinate the delivery of application, business, and process services. The three SOA delivery strategies mirror the main approaches to software architecture\ndesign that were covered in Chapter 5, Designing Software Architectures.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 277", "position": 277, "chunk_type": "semantic", "token_estimate": 255}
{"text": "Software Architecture Patterns: Chapter 7\n[ 249 ]\nThe top-down strategy\nThe top-down strategy begins with analysis. It centers on the organization's business logic\nand requires that business processes become service-oriented. The top-down approach,\nwhen done properly, results in a high quality SOA. Each service is thoroughly analyzed,\nand as a result, reusability is maximized. The downside is that this approach requires many resources, in terms of time and money. There is substantial pre-work that must take place with the top-down strategy. If an\norganization has the time and money to invest in a project, then this may be an effective\napproach. It should be noted that, because analysis occurs at the beginning, it could be quite some\ntime before any results are realized. This may or may not be acceptable for a given project. In order to meaningfully perform the service-oriented analysis and service-oriented design\nstages of the SOA life cycle, the top-down strategy has to be used at least to some extent. The bottom-up strategy\nThe bottom-up approach, in contrast, begins with the web services themselves. They are\ncreated on an as-needed basis. Web services are designed and deployed based on immediate\nneeds. Integration with an existing system is a common motivation for using the bottom-up\nstrategy. Organizations want to add web services to an existing application environment to\nallow for integration with a legacy system. A wrapper service is created to expose logic in\nan existing system. Although this approach is common in the industry, it is not a valid approach to achieving\nSOA. In order to create a valid SOA later, a lot of effort and refactoring will probably be\nrequired. Web services created with this approach may not be enterprise ready. They are\ncreated to serve some need, so if you are not careful they will not take into consideration\nthe enterprise as a whole.", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 278", "position": 278, "chunk_type": "semantic", "token_estimate": 310}
{"text": "Software Architecture Patterns: Chapter 7\n[ 250 ]\nThe agile strategy\nThe third approach is an agile strategy, which is sometimes referred to as a meet-in-the-\nmiddle approach. It is a compromise between the top-down and bottom-up approaches. In\nthis approach, analysis can occur concurrently with design and development. As soon as\nenough analysis has been completed, design and development can begin. While such\nefforts are underway, analysis continues with other functionality. This approach pairs well\nwith an iterative, agile software development methodology. This is sort of a best-of-both-worlds approach in that a proper design can be completed that\nwill yield all of the service-oriented qualities. This approach can fulfill immediate needs\nwhile maintaining service-oriented qualities of the architecture. However, as more analysis is finished, this approach may require completed services to be\nrevisited. Services can become misaligned after ongoing analysis, requiring them to be\nrefactored. Service-oriented analysis\nService-oriented analysis is a stage in the SOA project life cycle and is used to decide what\nservices should be built and what logic should be encapsulated by each service. The\nanalysis is an iterative process that takes place once for each business process. When a team is committed to building a SOA, it should perform some form of analysis\nspecific to service-orientation and beyond standard analysis. One way that organizations\ncan improve service modeling is how they go about incorporating service-oriented analysis\nand design into their software development process. Each organization has its own\nsoftware development methodology and should determine how best to include service\nmodeling into their own process. Service-Oriented Architecture, Second Edition, by Thomas Erl, details three steps to service-\noriented analysis: defining business automation requirements, identifying existing\nautomation systems, and modeling candidate services. Defining business automation requirements\nThe first step in service-oriented analysis is to define the business automation requirements\nfor the business process being analyzed in the current iteration. Requirements can be\ngathered using the organization's normal method of eliciting and capturing requirements.", "domains": ["Architectural Patterns and Styles", "Design Patterns"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 279", "position": 279, "chunk_type": "semantic", "token_estimate": 326}
{"text": "Software Architecture Patterns: Chapter 7\n[ 251 ]\nWith those requirements, the business process we want to automate can be documented at\na high level. The details of the business process are used when we model candidate\nservices. Identifying existing automation systems\nOnce the requirements have been established for the current iteration, the next step in\nservice-oriented analysis involves identifying what parts, if any, of the business process\nlogic are already automated. Taking into consideration existing systems that may already automate all or part of any of\nthe business processes allows us to determine what parts of the business processes still\nneed to be automated. This information serves as an input when we model candidate\nservices. Modeling candidate services\nThe final step, modeling candidate services, consists of identifying service operation\ncandidates and grouping them into candidate services. It is important to note that these\ncandidate operations and services are abstract and a logical model. During design, other\nfactors, such as constraints and limitations, will be considered. The final concrete design\nmay differ from the service candidates. Modeling candidate services should be a collaborative process between technical and\nbusiness resources. Business analysts and domain experts can use their business knowledge\nto help the technical team define service candidates. Service layers and service models\nEnterprise logic consists of both business and application logic. Business logic is an\nimplementation of the business requirements and includes an organization's business\nprocesses. These requirements include things such as constraints, dependencies, pre-\nconditions, and post-conditions. Application logic is the implementation of business logic in a technology solution. Application logic might be implemented in a purchased solution, a custom developed\nsolution, or some combination of the two. The development team works to design and\ndevelop the application logic. Topics such as performance requirements, security\nconstraints, and vendor dependencies are considered in the technical solution.", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 280", "position": 280, "chunk_type": "semantic", "token_estimate": 306}
{"text": "Software Architecture Patterns: Chapter 7\n[ 252 ]\nService-orientation is related to business and application logic because a SOA is a way to\nrepresent, execute, and share that logic. Service-orientation principles can be applied to\nboth business and application logic. The role of services is to realize the concepts and principles introduced by service-\norientation. A service layer in a software architecture is typically placed between the\nbusiness and application layers. This allows services to represent business logic and\nabstract application logic. Just as different applications within an organization's application\nlayer can be implemented in different technologies, services within the service layers can\nalso be implemented in different technologies. Abstraction is one of the important characteristics of an SOA and it enables other key\ncharacteristics, such as organizational agility. Abstraction is critical because abstracting\nbusiness and application logic allows for a service-oriented solution with loosely coupled\nservices. Achieving the appropriate level of abstraction is not a trivial task, but it can be\naccomplished with a dedicated team. By creating layers of abstraction, or service layers, the\nteam can figure out how services should represent application and business logic, and how\nto best promote agility. During service modeling, it becomes apparent that there are some common types of\nservices. These types are service models, which can be used to classify candidate services. Those candidate services can then be grouped together based on their service model into a\nservice layer. The three common service models (and layers) are task service, entity service, and utility\nservice:", "domains": ["Architectural Patterns and Styles", "Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 281", "position": 281, "chunk_type": "semantic", "token_estimate": 253}
{"text": "Software Architecture Patterns: Chapter 7\n[ 253 ]\nTask service\nThis type of service has a non-agnostic functional context, which means that it contains \nbusiness process logic and was created for a specific business task or process. Task services\ndo not have a great deal of reuse potential. Task services typically compose multiple services in their logic and are sometimes referred\nto as task-centric business services or business process services. If a task service has extensive orchestration logic or is hosted within an orchestration\nplatform, it is sometimes referred to as an orchestrated task service. Orchestration logic\nautomates the arrangement, coordination, and management of multiple services to\naccomplish a task. Entity service\nThis service model has an agnostic functional context, meaning its logic is not bound to a\nsingle business process and is reusable. Entity services are business-centric services that are\nassociated with one or more business entities. They are sometimes referred to as entity-\ncentric business services or business entity services. Business entities come from the business domain, and involving people on the team who\nthoroughly understand the business domain and business processes will be beneficial in\nperforming the analysis needed to determine the appropriate entity services. Entity services\nmay compose utility services in their logic. Utility service\nUtility services, like entity services, have an agnostic functional context. They contain multi-\npurpose logic and are highly reusable. The difference between entity services and utility\nservices is that utility services are not associated with a business entity or business logic. Utility services are concerned with technology-related functions, such as a software\nsystem's cross-cutting concerns. Examples include logging, caching, notifications,\nauthentication, and authorization. Service-oriented design\nThe service-oriented design phase begins once the analysis is complete. Having a thorough\nunderstanding of the requirements and using the service models from the analysis stage\nwill allow for the creation of a proper service design.", "domains": ["Architectural Patterns and Styles", "Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 282", "position": 282, "chunk_type": "semantic", "token_estimate": 310}
{"text": "Software Architecture Patterns: Chapter 7\n[ 254 ]\nThe service-oriented design phase uses the logical candidate services that were derived\nduring service-oriented analysis and creates the physical service designs. Before designing\nthe implementation of the services, the first step is to design the physical service interfaces. We need to make decisions regarding the service interfaces based on the service candidates,\nthe requirements that need to be met, as well as the organization and industry standards\nthat will be required to have a standardized service contract. Once the service contracts\nhave been established, the logic and implementation of the service can be designed. Designing the service interfaces and implementing them are two distinct steps. We should\nfully focus on the service contracts first, independent of their implementations. Some teams\ndesign the two concurrently or skip to the development stage and just let the service\ninterface emerge from the implemented service. However, software architects should devote time to considering the service contracts before\nthe implementations. Not only do we need to ensure that the service contracts satisfy the\nrequirements, but they need to follow the key principles of service orientation, which\ninclude the fact that they should be loosely coupled from their implementations. Only after\nthe service contracts have been established do we need to consider the design of the\nimplementations. Service interface design\nOne of the main goals of service-oriented design is to derive the physical service interface\ndefinitions based on the service candidates that were determined during service-oriented\nanalysis. Service interface design is significant because the design phase is the first time that\nreal technology is identified. If you recall the key principles for service orientation, they need to be applied to the design\nof the service interfaces. Service contracts need to be standardized with each other and\nwithin a service inventory. They should be loosely coupled from their implementations,\nwith design decisions abstracted out so that the interfaces only contain what is essential for\nservice consumers. Service interface design identifies internal and external exposure of the services. For\nexample, an order posting service may need to be used externally as well as internally. This\nis an example of a service that may need more than one interface published for the same\nservice. Each interface to the service may expose different operations and will require\ndifferent levels of security and authentication.", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 283", "position": 283, "chunk_type": "semantic", "token_estimate": 391}
{"text": "Software Architecture Patterns: This\nis an example of a service that may need more than one interface published for the same\nservice. Each interface to the service may expose different operations and will require\ndifferent levels of security and authentication. The differences must be determined and\neach interface must be designed prior to the design of the implementations.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 283", "position": 283, "chunk_type": "semantic", "token_estimate": 58}
{"text": "Software Architecture Patterns: Chapter 7\n[ 255 ]\nIn addition to developers, a service interface plays an important role in testing and quality\nassurance. Testers need the service interface to design their tests. Once the service interface\nis known, a test harness can be created that is capable of invoking the service that needs to\nbe tested. Services need to be tested independently of other services as well as within the\nservices that will be consuming it. Service interface granularity\nMaking decisions about interface granularity can be very important in service-oriented\ndesign. Granularity can have a significant impact on performance and other concerns. Usually, a service interface contains more than one operation, and the operations of a\nservice should be semantically related. Fine-grained service operations offer greater flexibility to service consumers but lead to\nmore network overhead, which could reduce performance. The more coarse-grained\nservice operations we have, the less flexible they are, although they do reduce network\noverhead and could therefore improve performance. Software architects should seek to find the right balance between the number of services\nand the number of operations in each service. You do not want to group too many\noperations into a single service, even if they are semantically related, because it makes the\nservice too bulky and hard to understand. It may also increase the number of service\nversions that will need to be released going forward, as parts of the service need to be\nmodified. However, if your service interface is too fine-grained, you may end up with an\nunnecessarily large number of service interfaces. Service registries\nA service registry contains information about the available services and is a key component\nof SOA governance. It helps to make systems interoperable and facilitates the discovery of\nweb services. Although some organizations may find that a service registry is not needed,\nmany SOA implementations can benefit from having a service registry. As an organization\nbegins to publish and use more and more web services, some of which may be outside the\norganization, the need for a centralized registry becomes more apparent.", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 284", "position": 284, "chunk_type": "semantic", "token_estimate": 346}
{"text": "Software Architecture Patterns: Chapter 7\n[ 257 ]\nAny service that wants to act as an ultimate receiver must have service description\ndocuments. Service descriptions typically have both abstract and concrete information. The\nabstract part details the service interface without getting into the details of the specific\ntechnologies being used. The beauty of the abstraction is that the integrity of the service\ndescription is maintained even if the details of the technical implementation are changed in\nthe future. The abstract description typically includes a high-level overview of the service\ninterface, including what operations it can perform. Input and output messages of the\noperations are also detailed. The concrete part of the service description provides details about the physical transport\nprotocol that is connected to the web service interface. This specific transport and location\ninformation includes the binding (requirements for the service to establish a connection or\nfor a connection to be established with a service), port (physical address of the web service),\nand service (a group of related endpoints) so that the web service can be used. Possible challenges to developing service descriptions include the following:\nDecomposing web services properly based on business needs\nDetermining the exact purpose and responsibilities of a particular service\nDeciding on the operations that a web service will need to provide\nProperly communicating a service's interface in the abstract part of the service\ndescription so that potential service consumers can make an informed decision\nbased on their needs\nStructuring namespaces\nA namespace is a unique Uniform Resource Locator (URI). Namespaces are used to group\nrelated services and elements together and to differentiate between different ones that share\nthe same name. It is important for software architects to put thought into namespaces. By\nproviding a unique namespace, even if your organization uses services from another one,\nyour elements will be guaranteed to be unique. Even if two organizations have a service\nwith the same name, they will be differentiated by their namespace. In addition to providing unique names, namespaces are used to logically organize various\nservices and elements. An appropriate namespace should provide meaning to the service or\nelement so that someone who is looking at it can gain an understanding of the service. Namespaces make it easier to name new services as well as to find existing ones.", "domains": ["Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 286", "position": 286, "chunk_type": "semantic", "token_estimate": 383}
{"text": "Software Architecture Patterns: Chapter 7\n[ 258 ]\nIn order to select good namespaces, we have to consider how namespaces are structured. A\ncompany's domain name is an important part of a namespace and because domain names\nare unique, they are commonly part of a namespace. Typically, the role follows the domain\nname in a namespace. This will allow differentiation between schema (for example,\nmessage types) and interfaces (for example, web services). A business area typically follows the role in the structure of a namespace. This is where\ndomain experts and business analysts can assist software architects in coming up with a\nbusiness structure that makes sense. Another part of a namespace that is common is some\nform of version or date. This allows differentiation between multiple versions of the same\nservice or element. Versioning is another important use of namespaces. Orchestration and choreography\nService orchestration and service choreography serve important roles in SOAs, as they are\napproaches to assembling multiple services so that they can work together. Orchestration represents business process logic in a standardized way using services. It\nautomates the execution of a workflow by coordinating and managing different services. In\nservice orchestration, there is a centralized process containing fixed logic. An orchestrator\ncontrols the process by deciding which services to invoke and when to invoke them. Orchestration is analogous to the conductor of an orchestra, who unifies and directs\nindividual performers to create an overall performance. Interoperability for an organization is promoted in solutions using orchestration because of\nthe integration endpoints that are introduced in processes. In a SOA, orchestrations\nthemselves are services. This promotes federation because multiple business processes,\npotentially from different applications, can be merged together. Choreography is another form of service composition. Choreographies define message\nexchanges and can involve multiple participants, each of which may assume multiple roles. In contrast with orchestration, there is no centralized process, or orchestrator, that is\ncontrolling it. With choreography, there is an agreed upon set of coordinated interactions\nthat specify the conditions in which data will be exchanged. Each service in a choreography\nacts autonomously to execute its part based on the conditions that were established and the\nactions of the other participants.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 287", "position": 287, "chunk_type": "semantic", "token_estimate": 365}
{"text": "Software Architecture Patterns: Chapter 7\n[ 259 ]\nBoth orchestration and choreography can be used for business-process logic owned by a\nsingle organization (intra-organization) and collaboration between multiple organizations\n(inter-organization). However, orchestration is less likely to be used when there are\nmultiple organizations involved because you would need to own and operate the\norchestration. Choreography allows for collaboration without having a single organization\ncontrol the whole process. Summary\nSoftware architects should be familiar with software architecture patterns, as they are a\npowerful tool when designing a software architecture. Architecture patterns provide a\nproven solution to recurring problems for a given context. Leveraging architecture patterns gives the software architect a high-level structure of the\nsoftware system, and provides a grouping of design decisions that have been repeated and\nused successfully. Using them reduces complexity by placing constraints on the design and\nallows us to anticipate the qualities that the software system will exhibit once it is\nimplemented. In this chapter, you learned about some of the common software architecture patterns\navailable, including layered architecture, EDA, MVC, MVP, MVVM, CQRS, and SOA. The focus of the next chapter is on some of the relatively newer software architecture\npatterns and paradigms. These include microservice architecture, serverless architecture,\nand cloud-native applications. As cloud deployment of software applications becomes the\ndominant trend, these concepts become crucial for any software architect to understand.", "domains": ["Architectural Patterns and Styles"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 288", "position": 288, "chunk_type": "semantic", "token_estimate": 227}
{"text": "Architecting Modern Applications: Chapter 8\n[ 261 ]\nThe following diagram shows an application with a monolithic architecture: \nThe different concerns of an application, such as user interface, business logic,\nauthorization, logging, and database access, are not kept separate in a monolithic\narchitecture. These different pieces of functionality are intertwined in a monolithic\napplication. Benefits of a monolithic architecture\nDespite some obvious disadvantages to using a monolithic architecture, if an application is\nrelatively small, then there are benefits to using one. Applications with a monolithic\narchitecture typically have better performance. With the interaction between the machine\nrunning the application and other machines minimized, better levels of performance are\nrealized. Small applications that have this type of architecture are easier to deploy because of the\nsimplicity of the high-level architecture. In spite of the tightly coupled logic, monolithic\napplications can be easier to test and debug because they are simpler, with fewer separate\ncomponents to consider. Monolithic applications are typically easy to scale because all it takes is to run multiple\ninstances of the same application. However, different application components have\ndifferent scaling needs and we cannot scale the components independently with a\nmonolithic architecture. We are limited to adding more instances of the entire application in\norder to scale.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 290", "position": 290, "chunk_type": "semantic", "token_estimate": 209}
{"text": "Architecting Modern Applications: Chapter 8\n[ 262 ]\nDrawbacks of a monolithic architecture\nAlthough a monolithic architecture may work for some applications, as applications grow\nin size and complexity, there are serious drawbacks. Monolithic applications greatly inhibit\nthe agility of the organization as it becomes difficult to make changes to the software. One\naspect of this is the fact that continuous deployment is difficult to achieve. Even if a change\nis made to only one component of a monolithic application, the entire software system will\nneed to be deployed. Organizations are required to devote more resources, such as time\nand testers, to deploy a new version of a monolithic application. If the application is small, it can be easy to maintain due to the simplicity of the\narchitecture. However, larger and more complex monolithic applications start to suffer in\nterms of maintainability. Tightly coupled components make it more difficult to make\nchanges because a change in one part of the application is more likely to affect other parts\nof the application. The large codebase of a monolithic application can make it difficult for team members to\nunderstand it. This is especially true for new team members as they attempt to become\nfamiliar with it and begin working with it. Even loading a large, monolithic application into an integrated development environment\n(IDE) and working with it can be frustrating due to slower performance of the IDE. It also\ntakes longer for such applications to start up, lowering the productivity of the team during\ndevelopment. Monolithic applications require a commitment to a particular programming language and\ntechnology stack. Since the application is written as a single unit, it makes it more difficult\nto introduce different types of technology. In some cases, it can even make it difficult to\nmove to a newer version of the same technology! If a migration to a different technology is\nneeded, it requires the organization to commit to rewriting the entire application. Larger and more complex applications benefit from dividing responsibilities up among\nmultiple development teams, such as having each team focus on a particular functional\narea. However, this becomes difficult to do with a monolithic application as changes made\nby one development team may affect another development team. Due to these drawbacks, software applications that are large and complex should move\naway from being a monolithic application. MSA and serverless architecture are alternatives\nto monolithic architectures.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 291", "position": 291, "chunk_type": "semantic", "token_estimate": 400}
{"text": "Architecting Modern Applications: Chapter 8\n[ 263 ]\nMicroservice architecture\nThe microservice architecture (MSA) pattern builds software applications using small,\nautonomous, independently versioned, self-contained services. These services use well-\ndefined interfaces and communicate with each other over standard, lightweight protocols. Interaction with a microservice takes place through a well-defined interface. A microservice\nshould be a black box to the consumers of the service, hiding its implementation and\ncomplexity. Each microservice focuses on doing one thing well and they can work together\nwith other microservices in order to accomplish tasks that are more complex. A microservice architecture is particularly well-suited for large and/or complex software\nsystems. In contrast with the monolithic architecture, applications built using a\nmicroservice architecture handle complexity by splitting the application into smaller\nservices that are easier to manage. The following diagram illustrates a system that has a\nmicroservices architecture:\nIncoming requests are commonly handled by an API gateway, which serves as the entry\npoint to the system. It is an HTTP server that takes requests from clients and routes them to\nthe appropriate microservice through its routing configuration. In the Service\ndiscovery section later in this chapter, we will take a closer look at service discovery to\nexplain how API gateways can get the locations of available service instances. SOA done right\nMicroservice architecture emerged, not as a solution looking for a problem, but as the result\nof the shortcomings and drawbacks of the traditional service-oriented architecture (SOA)\nand monolithic architecture.", "domains": ["Architectural Patterns and Styles", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 292", "position": 292, "chunk_type": "semantic", "token_estimate": 244}
{"text": "Architecting Modern Applications: Chapter 8\n[ 264 ]\nSOA provides a number of benefits, which were covered in Chapter 7, Software Architecture\nPatterns, such as increasing the alignment between business and technology, promoting\nfederation within an organization, allowing for vendor diversity, and increasing\ninteroperability. Although microservice architecture is a variation of the service-oriented architecture, there\nare key differences between the two. Some people even refer to the microservice\narchitecture as SOA done right. In addition to moving us away from monolithic architectures, microservice architecture\nimproves on traditional SOA. Implementing SOA can be quite expensive, difficult to\nimplement, and may be overkill for many applications. The microservice architecture\npattern addresses some of these drawbacks. Rather than using an enterprise service bus\n(ESB), as is common with SOA, microservice architecture favors implementing ESB-like\nfunctionality in the services themselves. Characteristics of microservice architecture\nThere are a number of characteristics related to the microservice architecture. In this\nsection, we will examine the following characteristics:\nSmall, focused services\nWell-defined service interfaces\nAutonomous and independently deployable services\nIndependent data storage\nCommunicating with lightweight protocols\nBetter fault isolation\nSmall, focused services\nThe functionality of a microservice should be small in scope. Each microservice should\nfollow the tenet of the Unix philosophy that advocates for doing one thing and doing it\nwell. Keeping the scope small for each service allows the complexity of the software to be\nbetter managed. Applications with a microservice architecture are easier to understand and modify because\nevery service has a focused responsibility. This allows the services to be developed faster. Smaller solutions are easier to load up and use in an IDE, which increases productivity. In\naddition, new team members are able to become productive in a shorter amount of time.", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 293", "position": 293, "chunk_type": "semantic", "token_estimate": 288}
{"text": "Architecting Modern Applications: Chapter 8\n[ 265 ]\nA microservice can be developed by a small team. This allows organizations to easily split\nup work among multiple development teams. Each microservice can be worked on by a\nsingle team, independent of other microservices. Tasks for different microservices can be\nworked on in parallel. If a software system is also using domain-driven design (DDD), the concept of bounded\ncontexts works well with microservices as it helps with the partitioning of services. In some\nsystems, each microservice is implemented as its own bounded context. Well-defined service interfaces\nMicroservices are treated like black boxes, hiding their complexity and implementation\ndetails from service consumers. This makes a well-defined interface, with clear entry and\nexit points, which is important to help facilitate microservices working together. Services\ninteract with each other through their interfaces. Autonomous and independently deployable services\nAn application using a microservice architecture consists of a system of autonomous\nservices. The services should be loosely coupled, interacting through their well-defined\ninterfaces and not dependent on the implementation of the service. This allows the implementation of services to change and evolve independently of the other\nservices. As long as the service interface is unchanged, modifying a microservice is less\nlikely to require changes to other parts of the application. Autonomous services are independently deployable, making it easier to deploy them to\nproduction. A microservice architecture enables continuous deployment because it is easier\nto release updates to the services. If a change is made to a microservice, it can be deployed\nindependently of the other microservices. The autonomy of the services increases organizational agility. It allows organizations to\nquickly adapt to changing business requirements and take advantage of new business\nopportunities. Independent data storage\nOne of the characteristics that support service autonomy is that each microservice can have\nits own data store. This helps services to be independent and loosely coupled to other\nservices. A service's data store can be altered without those changes affecting other services.", "domains": ["Architectural Patterns and Styles", "Domain-Driven Design", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 294", "position": 294, "chunk_type": "semantic", "token_estimate": 331}
{"text": "Architecting Modern Applications: Chapter 8\n[ 266 ]\nDesigning each microservice to have its own data store may come in the form of a\nmicroservice having its own database, but that is not the only way to keep a microservice's\ndata storage private. If the data storage technology is a relational database management\nsystem (RDBMS), then in addition to the option of having a separate database server, data\ncan be kept separate by designating certain tables to be owned by a particular service. Another option is to designate a schema that is to only be used by a single microservice. In the Polyglot persistence section later in this chapter, you will learn about some additional\nbenefits to using a separate database for each microservice. Better fault isolation\nA system built on a microservice architecture improves fault isolation. When one\nmicroservice goes down, other services can still operate normally, allowing other parts of\nthe system to remain operational. This is in contrast to a monolithic application, where a\nfault can potentially bring down the entire system. Communicating with lightweight message protocols\nMicroservices should communicate using well-known, lightweight message protocols. There is no rule dictating a particular protocol, and microservices can communicate\nsynchronously or asynchronously. A common implementation for microservices is to have\nthem expose HTTP endpoints that are invoked through REST API calls. For synchronous\ncommunication, REST is one of the preferred protocols. It is common for REST to be used\nwith JavaScript Object Notation (JSON). Service operations can accept and return data in\nthe JSON format as it is a popular and lightweight data-interchange format. Some applications may require asynchronous communication. A common messaging\nprotocol used for asynchronous communication with microservices is Advanced Message\nQueueing Protocol (AMQP). It is an open standard that can connect a variety of services,\nincluding those on different platforms and across organizations. It was designed for\nsecurity, reliability, and interoperability. AMQP can support the following types of message-delivery guarantees:\nAt least once: A message is guaranteed to be delivered but it may be delivered\nmultiple times\nAt most once: A message is guaranteed to be delivered once or never\nExactly once: A message is guaranteed to be delivered once and only once", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 295", "position": 295, "chunk_type": "semantic", "token_estimate": 367}
{"text": "Architecting Modern Applications: Chapter 8\n[ 267 ]\nAnother protocol that is popular with microservices is gRPC. It was designed by Google as\nan alternative to REST and other protocols. It is an open source protocol that was designed\nto be faster and more compact than other protocols used for distributed systems. gRPC is built on protocol buffers, also known as protobufs, which is a way of serializing\ndata that is language and platform neutral. This allows gRPC to efficiently connect polyglot\nservices because it supports a variety of programming languages. The increased prevalence\nof containerized applications and microservices has made gRPC popular. Given modern\nworkloads, gRPC is an attractive choice because it is a high-performance and lightweight\nprotocol. gRPC is inherently efficient, but it is also based on HTTP/2. This yields additional\nbenefits such as decreased latency and higher data compression. The communication protocol that you select for your microservice architecture really\ndepends on your requirements. There is no silver bullet answer that will apply to all\nsituations. The context of your design concerns will drive the selection, such as whether\nyour communication needs are synchronous or asynchronous. Designing polyglot microservices\nOne of the many advantages of using a microservice architecture is that it affords you the\noption of using multiple programming languages, runtimes, frameworks, and data storage\ntechnologies. Monolithic applications focus on using a particular programming language and technology\nstack. Because that type of application is written as a single unit, it is more difficult to take\nadvantage of different types of technology. However, complex applications need to solve a\nvariety of problems. Being able to select different technologies for different problems can be\nuseful rather than trying to solve all of the problems with a single technology. A development team can select best-of-breed solutions depending on the task they need to\ncomplete. A microservice architecture allows teams to experiment and try out new\ntechnologies with, having to commit to them for the entire system. Having polyglot microservices is certainly not required when using a microservice\narchitecture. In many cases, an organization will focus on a limited number of technologies,\nand the skillsets of the development team will reflect that. However, software architects\nshould be aware of the option and recognize opportunities where it can be used effectively. Two of the concepts related to polyglot microservices are polyglot programming and\npolyglot persistence.", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 296", "position": 296, "chunk_type": "semantic", "token_estimate": 394}
{"text": "Architecting Modern Applications: Chapter 8\n[ 268 ]\nPolyglot programming\nWith polyglot programming, a single application uses multiple programming languages in\nits implementation. It can be useful to take advantage of the strengths of different\nprogramming languages to handle different tasks within an application. A microservice architecture allows each microservice to be developed using the\nprogramming language that best fits the problem at hand. When a new technology\nbecomes available, an existing application can take advantage of it for new microservices\nthat are developed or for new versions of an existing microservice. Polyglot persistence\nSimilar to polyglot programming, there is the concept of polyglot persistence, in which\nmultiple persistence options are used within a single application. Different data storage\ntechnologies are better suited to different tasks, and microservices allows you to take\nadvantage of that. Each microservice is in charge of its own data storage, so it can choose the best data storage\ntechnology based on what it is trying to achieve. The following diagram depicts two\nmicroservices, each with its own database:\nA microservice dedicated to providing product recommendations based on friends, ratings,\nand prior purchases would be ideal for a graph database. A product catalog with details\nabout a company's products, requiring frequent reads and infrequent writes, could be\nimplemented with a document database. When processing an order, the transactional\ncapabilities of a relational database might be ideal.", "domains": ["Architectural Patterns and Styles"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 297", "position": 297, "chunk_type": "semantic", "token_estimate": 230}
{"text": "Architecting Modern Applications: Chapter 8\n[ 269 ]\nA single microservice can even use more than one data storage technology, although that is\nfairly unusual in practice. It introduces extra complexity, and if you find yourself wanting\nto do that, you have to consider whether your microservice is too large in scope. In that\ncase, perhaps the microservice needs to be divided into more than one. Using too many technologies\nAs with many things, polyglot programming and polyglot persistence can be overdone. There is a cost to introducing too many disparate technologies within a single organization. Mastering a technology is difficult and you will need staff members who are well-versed in\nthe technologies that you are using. The use of different technologies may create a need for\nmore developer training, which costs the organization money as well as its employees'\ntime. Using different programming languages and data storage technologies introduces\ncomplexity into the build, deployment, and test processes. The software will need to be\ncontinually maintained after it is released to production, requiring a diverse development\nteam capable of handling all of the different technologies. Having the ability to take advantage of different technologies can be a valuable asset, but\nconsideration must be made each time using a new one is being considered to ensure that it\nis the best decision for the project. Considering service granularity\nThe granularity of a service refers to the scope of its business functionality, which varies for\neach service. With microservices, the goal is to have fine-grained services so that each one\nfocuses on a single business capability. It is important to get the granularity of microservices correct to ensure that the system\nconsists of appropriately sized services. One of the goals of a microservice architecture is to\ndecompose a domain into small, focused, and reusable services. Smaller services carry with\nthem less context, increasing their level of reusability. Nanoservices\nSoftware architects and developers who are designing microservices should be careful not\nto make the granularity of their services too fine-grained, though. Services whose\ngranularity is too fine-grained are referred to as nanoservices and this is considered an\nanti-pattern.", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 298", "position": 298, "chunk_type": "semantic", "token_estimate": 355}
{"text": "Architecting Modern Applications: Chapter 8\n[ 270 ]\nA system with very small services tends to have more services and, as the number of\nservices in a system increases, so does the amount of communication that must take place. Services use up network resources that are not infinite, and services are bound by these\nlimitations. Having too many services can lead to the reduced performance of your services\nand of your overall application. When a system has many nanoservices, there is also an increase in the overall overhead for\nthe services. Each service requires some management, including things such as\nconfiguration and entry into a service registry. Larger numbers of services lead to\nincreasing amounts of overhead. Nanoservices can lead to fragmented logic. If a single business task that fits well into a\nsingle, cohesive service is decomposed further into multiple, smaller services, the logic\nbecomes separated. Making services unnecessarily small is part of the nanoservice, anti-\npattern. When the overhead of a service outweighs its utility, then it is a nanoservice and\nrefactoring should be considered. Nanoservices can be refactored by either combining\nmultiple nanoservices into a new, larger service or moving the functionality of each\nnanoservice into an appropriate existing service. There may be exceptions in which a nanoservice is appropriately sized and does not require\nrefactoring, so it is always prudent to use your best judgment. Although you do not want\nyour system to consist of too many nanoservices, sometimes, the functionality of a\nparticular nanoservice simply does not belong anywhere else. Sharing dependencies between microservices\nDevelopment teams should avoid sharing dependencies, such as frameworks and third-\nparty libraries, between microservices. You may have multiple microservices that share the\nsame dependency, so it is natural to think about sharing them on the host to make them\ncentrally available. However, each microservice should remain independent of other microservices. If we want\nto update the dependencies, we don't want to affect any other services. Doing so would\nincrease the risk of introducing defects and will broaden the scope of the testing that will\nneed to be conducted related to the change. Sharing dependencies also introduces host affinity, which we want to avoid. Microservices\nshould function independently of the host they are deployed on because assumptions\ncannot be made regarding the host on which a service will execute.", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 299", "position": 299, "chunk_type": "semantic", "token_estimate": 388}
{"text": "Architecting Modern Applications: Chapter 8\n[ 271 ]\nStateless versus stateful microservices\nEach microservice can either be stateless or stateful. A system that uses microservices\ntypically has a stateless web and/or mobile application that uses stateless and/or stateful\nservices. Stateless microservices do not maintain any state within the services across calls. They take\nin a request, process it, and send a response back without persisting any state information. A stateful microservice persists state in some form in order for it to function. Rather than store this state internally, a microservice should store state information\nexternally, in some type of data store. Examples of a data store to persist state include a\nrelational database management system (RDBMS), a NoSQL database, or some type of\ncloud storage. Persisting the state externally provides availability, reliability, scalability,\nand consistency for the state information. Service discovery\nThe client of a service, whether it is an API gateway or another service, needs the ability to\ndiscover the location of a service instance. In a traditional distributed environment, service\nlocations (IP address and port) are generally static, and a service instance can be found\neasily. For example, service locations could be read from a configuration file. However, service discovery is more complex with a cloud-based application using\nmicroservices. The number and location of service instances changes dynamically in the\ncloud. A service registry can be used to keep track of service instances and their locations. Using a service registry\nA service registry plays a key role in service discovery. It is a database containing service\ninstances and their locations. The service registry must be highly available and kept up to\ndate. In order to be accurate, service instances must be registered and deregistered with the\nservice registry. This can be accomplished through either self-registration or third-party\nregistration.", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 300", "position": 300, "chunk_type": "semantic", "token_estimate": 300}
{"text": "Architecting Modern Applications: Chapter 8\n[ 272 ]\nSelf-registration pattern\nUsing the self-registration pattern, service instances are responsible for registering with,\nand deregistering from, the service registry:\nWhen a service instance starts up, it must register itself with the service registry. Similarly,\nwhen a service instance shuts down, it must unregister itself from the service registry. This approach is relatively straightforward, although it does couple the service instances to\nthe service registry. If you have a polyglot system in which different microservices are\nusing different programming languages and/or frameworks, service registration logic must\nexist for each programming language and framework that is used for the development of\nyour microservices. It is a common requirement to have registered service instances periodically renew their\nregistration or send a heartbeat request to indicate they are still alive and responsive. If a\nservice instance does not do this, then they can be deregistered automatically. Doing this\nwill handle situations in which a service instance is running but is unavailable for some\nreason. Such service instances may not be able to unregister themselves from the service\nregistry. For small applications, the self-registration pattern may be sufficient, but large applications\nwill want to use the third-party registration pattern. Third-party registration pattern\nWhen using the third-party registration pattern, a dedicated component, sometimes\nreferred to as the service registrar, handles registering, deregistering, and checking the health\nof service instances. Like the service registry itself, the service registrar is an important\ncomponent and therefore must be highly available.", "domains": ["Architectural Patterns and Styles", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 301", "position": 301, "chunk_type": "semantic", "token_estimate": 249}
{"text": "Architecting Modern Applications: Chapter 8\n[ 273 ]\nThe following diagram illustrates the third-party registration pattern:\nBy either polling the available service instances or subscribing to relevant events, the\nservice registrar can register new service instances and deregister service instances that no\nlonger exist. It can perform health checks on service instances and take appropriate actions\nbased on the result. Unlike the self-registration pattern, the service instances are decoupled from the service\nregistry. Services can focus on their single responsibility and not have to be concerned with\nservice registration. If different microservices in your system are using different\nprogramming languages and/or frameworks, you do not need to implement service\nregistration logic for each one that is used by the development team(s). One drawback with this pattern is that unless the service registrar is a built-in component\nof your deployment environment, it is another component that must be set up and\nmanaged. Types of service discovery\nThere are two main patterns for service discovery:\nClient-side discovery pattern \nServer-side discovery pattern\nWe will now look at both types in detail.", "domains": ["Architectural Patterns and Styles", "Design Principles", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 302", "position": 302, "chunk_type": "semantic", "token_estimate": 178}
{"text": "Architecting Modern Applications: Chapter 8\n[ 274 ]\nClient-side discovery pattern\nWith the client-side discovery pattern, the service client, whether it is an API gateway or\nanother service, queries a service registry for the locations of available service instances:\nOnce the locations of available service instances are obtained from the service registry, the\nservice client uses a load balancing algorithm to select one of them. At that point, the\nservice client can interact with a specific service instance. This pattern is straightforward, although it does couple the service client with the service\nregistry. For organizations that are taking advantage of using multiple programming\nlanguages and/or frameworks for their microservice development, service-discovery logic\nwill need to be written for each programming language and framework that is used for the\ndevelopment of microservices. Server-side discovery pattern\nThe other main pattern for service discovery is the server-side discovery pattern. The\nservice client, such as an API gateway or another service, makes a request to a router. The\nrouter is typically a load balancer.", "domains": ["Architectural Patterns and Styles"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 303", "position": 303, "chunk_type": "semantic", "token_estimate": 170}
{"text": "Architecting Modern Applications: Chapter 8\n[ 275 ]\nThe following diagram illustrates the server-side discovery pattern:\nIn this pattern, it is the load balancer that queries the service registry for the locations of\navailable service instances. The service registry may be built into the load balancer or it may\nbe a separate component. The load balancer is then responsible for forwarding the request\nto one of the available service instances. Unlike the client-side discovery pattern, service clients are decoupled from the service\nregistry. Service client code is simpler as it does not have to be concerned with interacting\nwith the service registry or implementing a load balancing algorithm. Service clients can\nsimply make a request to the load balancer. If different programming\nlanguages/frameworks are being used for microservice development, there is no need to\nimplement service discovery logic for each one. A drawback to this pattern is that unless the cloud provider is providing the load balancer\nand associated functionality (service registry), it is another component that must be\ninstalled and managed. Also, there are more network hops involved with this pattern due\nto the load balancer compared with the client-side discovery pattern. Using microservices is not for everyone\nAs is the case with other types of architecture patterns, there are disadvantages to using a\nmicroservice architecture. The benefits of using a microservice architecture must outweigh\nthe added complexity, so it is not the ideal solution for all types of applications.", "domains": ["Architectural Patterns and Styles", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 304", "position": 304, "chunk_type": "semantic", "token_estimate": 241}
{"text": "Architecting Modern Applications: Chapter 8\n[ 276 ]\nAs a distributed system, a microservice architecture introduces complexity simply not\nfound in a monolithic application. When multiple services are working together in a\ndistributed system and something goes wrong, there is added complexity in figuring out\nwhat and where something failed. A service may not respond downstream and the system\nmust be able to handle the disruption. Decomposing a complex system into the right set of microservices can be difficult. It\nrequires a knowledge of the domain and can be somewhat of an art. You don't want a\nsystem with services that are too fine-grained, resulting in a large number of services. As\nthe number of services increase, the management of those services becomes increasingly\ncomplex. At the same time, you do not want the services of a system to be too coarse-grained, so that\nthey are responsible for too much functionality. The last thing you want is a bunch of\nservices that are tightly coupled, making it so that they have to be deployed together. If you\nare not careful, you will end up with a microservice architecture that is a monolith in\ndisguise. The use of multiple databases is another challenge when using a microservice architecture. It is common for a business transaction to update multiple entities, which will require the\nuse of multiple microservices. With each one having its own database, this means that\nupdates must take place in multiple databases. One way to handle this is through event-\nsourcing and having eventual consistency. Even if that is acceptable, implementation of\nevent-sourcing is another added complexity. Service clients, such as an API gateway and other services, will need a way to know the\nlocation of available service instances. Unless a cloud provider is providing a service\nregistry and associated functionality, these additional components will need to be\nconfigured and managed. Serverless architecture\nServerless architecture allows the rapid development of software applications that can\nhandle various levels of traffic in production environments. The term serverless refers to the\nfact that compute services are provided without requiring you to manage or administer\nservers. Your code is executed on demand, as it is needed. Utilizing compute services in this way is similar to how cloud storage is used. With cloud\nstorage, you do not need to manage physical hardware and you do not need to know where\nthe data is stored.", "domains": ["Architectural Patterns and Styles", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 305", "position": 305, "chunk_type": "semantic", "token_estimate": 400}
{"text": "Architecting Modern Applications: Chapter 8\n[ 277 ]\nSimilarly, with serverless architecture, you do not need to deal with physical servers, and\nthe complexity of how compute resources are provided is hidden from you. Software\napplications use as much or as little compute capacity as they need. Serverless architecture is maturing and its use is increasingly common. It can be the ideal\narchitectural choice for some software applications. A number of cloud vendors, including\nAmazon, Microsoft, Google, and IBM, provide compute services. Serverless architecture can be used for many types of software applications and tasks. Some\nof the common types of applications that are suited to a serverless architecture include web\napplications, event-driven data processing, event workflows, scheduled tasks (CRON jobs),\nmobile applications, chatbots, and Internet of Things (IoT) applications. It is also good for\ndata transformation tasks related to things such as images/video (for example, for the\npurposes of compression and/or optimization), voice packets, and PDF generation. The following diagram shows an example of a system that has a serverless architecture:\nServerless architectures use function as a service (FaaS) and backend as a service (BaaS)\nmodels to provide backend logic for software applications. The two are used together to\nprovide the functionality of the application. Function as a Service (FaaS)\nIn a serverless architecture, a small piece of code, like a function, can be executed using an\nephemeral compute service to produce a result. This is known as Function as a Service\n(FaaS). By ephemeral, we mean that it will only last for a limited amount of time. The code\nexecutes in a container that is spun up on invocation and then brought back down when it\nis complete.", "domains": ["Architectural Patterns and Styles"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 306", "position": 306, "chunk_type": "semantic", "token_estimate": 280}
{"text": "Architecting Modern Applications: Chapter 8\n[ 278 ]\nFunctions are typically invoked due to events or HTTP requests. When a function\ncompletes its processing, it can either return a value to the caller or pass the result to\nanother function that will be invoked as part of a workflow. The output can either be\nstructured, such as an HTTP response object, or unstructured, such as a string or an integer. Each function should follow the single responsibility principle (SRP) and serve a single,\nwell-defined purpose. Development teams can code functions to perform server-side logic\nand then upload the code to their service provider for on-demand execution. FaaS makes it\neasy to go from writing code on your laptop to having it execute in the cloud. Functions should be designed to be idempotent, so that multiple executions of the same\nrequest yield the same result, and if the same request is processed more than once, there\nshould not be any adverse effects. Copies of messages may exist on multiple servers to provide redundancy and high\navailability. In order to ensure at-least-once delivery, it is possible that a function may be\ninvoked more than once. For example, a server with a copy of a message may become\nunavailable when a message is received or deleted, leading to the same message being sent\nto a function again. The functions in FaaS can be either synchronous or asynchronous, depending on the needs\nof the task. One of the ways that asynchronous functions work in FaaS is that the platform\nwill return a unique identifier that can then be used to poll the status of the asynchronous\noperation. An important part of a serverless architecture is its API gateway. An API gateway is the\nentry point to a system. It is an HTTP server that takes requests from clients and uses its\nrouting configuration to route them to the relevant function container. The functions run in\ncompute containers and should be stateless. The result of the FaaS function is then sent\nback to the API gateway and ultimately returned to the client as an HTTP response. Implementations of FaaS that are available from providers include Amazon Web Services\nLambda, Microsoft Azure Functions, Google Cloud Functions, and IBM Cloud Functions.", "domains": ["Design Patterns", "Design Principles", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 307", "position": 307, "chunk_type": "semantic", "token_estimate": 374}
{"text": "Architecting Modern Applications: Chapter 8\n[ 279 ]\nExamples of the functionality provided by service applications include a database, push\nnotifications, file storage, and authentication services. In the serverless architecture\ndiagram, the authentication service and the storage service for the static resources are\nexamples of BaaS. Advantages of serverless architectures\nUsing a serverless architecture provides many important benefits, which explains why it is\nbecoming increasingly popular. Cost savings\nWith a serverless architecture, your code is only executed when it is needed. You get utility\nbilling, in that you are only charged for the actual compute resources that are used. Organizations moving to a serverless architecture will reduce hardware costs, as there are\nno longer servers and network infrastructure to support. In addition, organizations do not\nhave to hire staff to support all of that infrastructure. These cost savings are similar to what\norganizations might save from using Infrastructure as a Service (IaaS) or Platform as a\nService (PaaS), except that there are also reduced development costs (taking advantage of\nBaaS) and scaling costs (taking advantage of FaaS). Scalable and flexible\nYou can't underprovision or overprovision your compute capacity with a serverless\narchitecture. You avoid being in a situation where you do not have enough servers during\nperiods of peak capacity or have too many servers sitting idly during off-peak periods. The scalability and flexibility of serverless architectures allows for compute capacity to\nscale up and down as demand changes. You only use as much compute capacity as you\nneed and you are charged for the amount that you use. Wasting compute capacity is kept to\na minimum and organizations will experience lowered costs as a result. Focus on building your core products\nAnother one of the main advantages of using a serverless architecture is that there are no\nservers to manage. This allows organizations to focus on creating solutions and shipping\nmore features.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 308", "position": 308, "chunk_type": "semantic", "token_estimate": 313}
{"text": "Architecting Modern Applications: Chapter 8\n[ 280 ]\nNot having to administer an infrastructure increases productivity and reduces the time to\nmarket. Even a small development team can start building an application and deploy it to\nproduction relatively quickly because they won't need to provision an infrastructure up\nfront and they won't have as much to manage after deployment. A development team that\nwants to build an application quickly while worrying less about operational concerns will\nfind serverless architecture attractive. Polyglot development\nA serverless architecture allows for polyglot development. Development teams are\nprovided with the ability to select best-of-breed languages and runtimes based on the\nrequired functionality. It also gives teams a chance to easily try out and experiment with\ndifferent technologies. Although there are limitations as to which languages are available to\nyou, cloud providers are expanding their selection so that different languages can be used\nfor the different functions being developed. Disadvantages of serverless architectures\nAlthough there are many benefits to using a serverless architecture, you should be aware\nthat there are some drawbacks. Difficulties with debugging and monitoring\nThere is complexity in debugging distributed systems using serverless architecture. When\nmultiple functions integrate with each other to perform a task and something goes wrong,\nit can be difficult to understand when and why the problem occurred. Vendors provide\ntools for debugging and monitoring, but there is still a level of immaturity with the\nserverless architecture pattern. Serverless architecture applications will become more\nprevalent, but it is still relatively new compared to other types of software architecture. Multitenancy issues\nMultitenancy issues, while not unique to serverless systems, deserve consideration. Any\ntime software for different customers are executed on the same machine, there is the\npossibility of one customer affecting a different customer. Examples of this include security\nissues, such as one customer being able to see another customer's data, or performance\nissues when one customer experiences a heavy load that affects performance for another\ncustomer.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 309", "position": 309, "chunk_type": "semantic", "token_estimate": 326}
{"text": "Architecting Modern Applications: Chapter 8\n[ 282 ]\nTaking a hybrid approach to serverless\nUsing a serverless architecture for a software system does not have to be an all or nothing\napproach. In addition to new applications fully leveraging serverless architecture, you may\nchoose to design a part of your system with a serverless architecture and use a different\narchitecture pattern for the other parts of your system. For example, an organization may elect to take a hybrid approach and build some new\nfeatures for an existing application in a serverless environment and use them with other\narchitecture environments. Function deployment\nWhen functions are deployed in a serverless system, they go through a deployment\npipeline. While the steps in the pipeline can vary depending on the cloud provider, there\nare some fundamental steps that typically take place. Developers must first upload the function definition, which contains specifications about\nthe function as well as the code. The specifications and metadata include things such as a\nunique identifier, name, description, version identifier, runtime language, resource\nrequirements, execution timeout (the maximum time a function call can execute until it is\nterminated), created date/time, and last modified date/time. When a function is invoked, it\nis invoked for a specific function version. The version identifier is used to select an\nappropriate function instance. The following illustration shows a typical function deployment pipeline:\nAlong with the specification, the code itself and its dependencies must be provided (for\nexample, as a ZIP file or a Dockerfile). If the code is stored in an external repository, the\npath of the code location along with the credentials required for access must be provided.", "domains": ["Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 311", "position": 311, "chunk_type": "semantic", "token_estimate": 274}
{"text": "Architecting Modern Applications: Chapter 8\n[ 283 ]\nOnce the functional definition has been uploaded to the cloud provider, the build process\nuses it for compilation to produce an artifact. The resultant artifact may be a binary file, a\npackage, or a container image. The starting of an instance function can be the result of cold start or  warm start. With a \nwarm start, one or more function instances have already been deployed and are ready to be\nexecuted when needed. A cold start takes longer since the function starts from an\nundeployed state. The function must be deployed and then executed when it is needed. Function invocation\nWhen functions are invoked, there are four main invocation methods that can be used to\ninvoke a function:\nSynchronous request\nAsynchronous request\nMessage stream\nBatch job\nSynchronous request\nWhen a client makes a synchronous request, it waits for a response. A request-reply pattern\nis used to handle synchronous requests. Examples of requests include an HTTP request or a\ngRPC call. The following diagram shows the flow of a synchronous request, which first\ngoes through an API gateway:", "domains": ["Design Patterns", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 312", "position": 312, "chunk_type": "semantic", "token_estimate": 185}
{"text": "Architecting Modern Applications: Chapter 8\n[ 284 ]\nIn order to locate a function instance, the API gateway will either use client-side service\ndiscovery or pass it on to a router (load balancer) for server-side service discovery. Once a\nservice instance is located, the request is passed to it for processing. Once execution of the\nfunction is complete, the response is sent back to the client. Asynchronous request (message queue)\nWhen you want to process requests asynchronously, the publish-subscribe pattern can be\nused. Incoming requests are published to an exchange. Exchanges then distribute messages\nto one or more queues using rules called bindings. From there, function instances are\ninvoked to process published messages:\nExamples of message queues include RabbitMQ, AWS Simple Notification Service,\nMessage Queuing Telemetry Transport, and scheduled CRON jobs. Message stream\nWhen there is a need for real-time processing of messages, a message stream can be used. Message streams can ingest, buffer, and process large amounts of streaming data. When a\nnew stream is created, it is typically partitioned into shards, with each shard going to a\nsingle worker (function instance) for processing:", "domains": ["Architectural Patterns and Styles"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 313", "position": 313, "chunk_type": "semantic", "token_estimate": 185}
{"text": "Architecting Modern Applications: Chapter 8\n[ 285 ]\nExamples of technologies for serverless message streams include AWS Kinesis, AWS\nDynamoDB Streams, and Apache Kafka. Batch job\nBatch jobs are placed in a job queue, either on-demand or based on a schedule. For the\npurpose of speeding up the execution of the jobs, the master/worker pattern can be used\nwith jobs. The master/worker pattern, which is sometimes referred to as the master/slave\npattern, speeds up jobs by splitting them up into smaller tasks so that the tasks can be\nprocessed in parallel. The following diagram shows the processing of a job:\nA component playing the role of Master splits the job (working set) up into tasks and\nlaunches workers. In this case, the workers are function instances and they can process\ntasks in parallel. Workers continue to pull from the working set until there are no more\ntasks to complete. The master gathers the results once the workers have all finished. Cloud-native applications\nDeploying a software application to the cloud does not make it cloud-native. It is about\nhow the software is designed and implemented, not just where it is run. Cloud-native\napplications have been designed and developed from the ground up to be deployed in the\ncloud. In doing so, applications can take full advantage of their deployment environment. In modern application development, the development team needs to have more knowledge\nabout, and a vested interest in, how their application runs in production. Similarly, the\noperations team must be able to work with the development team to improve upon, over\ntime, how the application is deployed and executes in a production environment.", "domains": ["Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 314", "position": 314, "chunk_type": "semantic", "token_estimate": 272}
{"text": "Architecting Modern Applications: Chapter 8\n[ 286 ]\nReasons to move to the cloud\nIn the early years of cloud computing, enterprises were hesitant to adopt cloud\ntechnologies within their IT organization. They had concerns such as losing control over\ntheir infrastructure, security, data risks, and reliability. Since then, enterprises have increasingly relocated their IT workloads to the cloud. Many\norganizations have already done so or have plans to move their applications to the cloud. Major cloud providers, such as Amazon, Microsoft, and Google, are experiencing explosive\ngrowth and the competition between them is intensifying as more applications are\ndeployed to the cloud. There are a number of reasons that enterprises are moving their applications and data to\nthe cloud. Reducing costs\nCloud-based hosting reduces capital expenditure by eliminating the need to spend money\non fixed assets such as hardware and software. It also reduces operational expenditure by\nlowering costs for such things as IT support staff and round-the-clock electricity needs for\npower and cooling. Greater flexibility and scalability\nTaking advantage of the cloud offers enterprises greater levels of flexibility. Workloads can\nscale up and down based on demand very quickly, even when large amounts of computing\nresources are suddenly needed. Large cloud providers have global scale. In addition to\nbeing able to deliver the right amount of compute resources at the right time, they can do\nso from the right geographic location. Cloud computing allows businesses to grow relatively easily without having to make major\nadjustments to things such as the amount of hardware. Automatic updates\nThe task of keeping infrastructure software up to date with the latest software and security\nupdates is moved to the provider. Updates to hardware are also taken care of by the cloud provider, including upgrades to\nservers, memory, processing power, and disk storage. Data centers are regularly updated\nwith the latest generation of hardware, ensuring greater efficiency and faster performance.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 315", "position": 315, "chunk_type": "semantic", "token_estimate": 317}
{"text": "Architecting Modern Applications: Chapter 8\n[ 287 ]\nDisaster recovery\nBackup, disaster recovery, and business continuity are important concerns for software\napplications. Cloud computing provides these services easily and with less expense than\ndoing it on your own. For smaller businesses, the additional expense for disaster recovery can be burdensome. The option of deploying applications to the cloud has made it easier for even small\nbusinesses to have full backup and recovery capabilities in the event of a disaster. What are cloud-native applications? Cloud-native applications are specifically designed for the cloud model. Applications\ndesigned for the cloud are able to take advantage of it as their target platform, providing\norganizations with a competitive advantage, greater agility, easier deployment, on-demand\nscalability, reduced costs, increased resilience, and other benefits. The expectations of today's modern applications are different from those of the past. Cloud-\nnative applications have some distinct characteristics that allow us to meet those \nexpectations and requirements. The Cloud Native Computing Foundation (CNCF)\ncurrently defines cloud-native as using an open source software stack to make applications\nthat are containerized, dynamically orchestrated, and microservices-oriented. Containerized\nContainers are a way of packaging a software application. They are lightweight, stand-\nalone packages of software. The application, along with all of its libraries and\ndependencies, is bundled together in an immutable package. The concept of containers for software is similar to the concept of using physical containers\nin the shipping industry. Prior to standardizing shipping containers, shipping could be an\ninefficient and complicated process. A wide variety of things in all shapes and sizes can be\ntransported on the same cargo ship. Now that there are standardized shipping container\nsizes, we know what and how much will fit on a container ship, no matter what we are\nshipping.", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 316", "position": 316, "chunk_type": "semantic", "token_estimate": 293}
{"text": "Architecting Modern Applications: Chapter 8\n[ 288 ]\nSimilarly, packaging your software with its dependencies in a container based on open\nstandards allows us to know that it can be run anywhere that supports containers. It\nprovides us with predictability in that we know the software will work as expected because\nthe container is the same, no matter where it is executed. It eliminates the experience of\nhaving an application not execute properly on one machine only to have someone else on a\ndifferent machine claim that it runs on my machine. Unexpected errors because of differences\nbetween machines and environments can be drastically reduced or even eliminated. In a cloud-native application, each part of the system is packaged in its own container. This\nallows each part to be reproducible and allows resources to be isolated for each container. Dynamically orchestrated\nMaking your application containerized is not enough. A cloud-native application will need\nthe ability to run multiple containers across multiple machines. That is what will allow you\nto use microservices and to provide the fault tolerance capabilities. Once you have multiple containers running on different machines, you will need to\ndynamically orchestrate them. The system must start the correct container at the right time,\nbe able to scale containers by adding and removing them based on demand, and launch\ncontainers on different machines in the event of a failure. There are a number of container clustering and orchestration tools available. Currently, the\nmost popular is Kubernetes, which is sometimes referred to as K8S because there are eight\nletters in between the K and the S. It is an open source orchestrator that was originally\ndeveloped by Google. Other container orchestration tools besides Kubernetes include\nDocker Swarm and Apache Mesos. Cloud providers also have cloud-based container services. There is Amazon Elastic\nContainer Service (Amazon ECS), which is Amazon's own container-orchestration service\nfor Amazon Web Services (AWS). In addition to ECS, Amazon offers Amazon Elastic\nContainer Service for Kubernetes (Amazon EKS), which is a managed service for\nKubernetes on AWS. Amazon also offers AWS Fargate, which is a technology that can be\nused for Amazon ECS and EKS that lets you run containers without having to manage\nservers or clusters. Microsoft's Azure Container Service (AKS) allows you to use a fully managed Kubernetes\ncontainer orchestration service or select an alternative orchestrator such as unmanaged\nKubernetes, Docker, or Mesosphere DC/OS.", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 317", "position": 317, "chunk_type": "semantic", "token_estimate": 398}
{"text": "Architecting Modern Applications: Chapter 8\n[ 289 ]\nMicroservices-oriented\nA cloud-native application should be partitioned into microservices. Dividing an\napplication into small, autonomous, independently versioned, and self-contained services\nincreases organizational agility and the maintainability of the application. No downtime\nApplications today are expected to be available at all times, with no downtime. It has\nalways been a goal to minimize downtime, but the days when it was acceptable to have a\nsmall maintenance window in which an application was not available are over. Complex software systems will have failures and should be designed to expect failures to\noccur. Cloud-native applications are designed for failure and keep fault tolerance in mind so\nthat they can recover rapidly and minimize downtime. If a physical server fails\nunexpectedly or is taken down as part of planned maintenance, a failover system will\nredirect traffic to a different server. Software components should be designed so that they\nare loosely coupled, such that if one fails, a redundant component can take over. A single failure should not have the effect of bringing down the entire software system. For\nexample, if an instance of a microservice fails, other instances can take incoming requests. If\nall instances of a particular microservice are not operational, the fault is isolated to only a\nportion of the system, so that the entire system does not fail. Continuous delivery\nIncreasing competition and user expectations mean modern applications have shorter\nrelease cycles. Rather than having major releases that are months (or even years) apart, the\nability to have application updates on a more regular basis (think weeks or days rather than\nmonths) is a necessity. Cloud-native applications should release software updates rapidly. Shorter release cycles\nprovide an opportunity to get feedback from users more quickly. Rather than having to\nwait a long period of time to receive feedback, continuous delivery gives us a much tighter\nfeedback loop. The development team can respond more rapidly to users by using the\nfeedback received to make adjustments and improve the software. Cloud-native applications increase organizational agility. By releasing the software and\nreceiving feedback regularly, an organization can respond to the market, competitors, and\nthe needs of their customers quickly. This can give an organization a competitive\nadvantage.", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 318", "position": 318, "chunk_type": "semantic", "token_estimate": 371}
{"text": "Architecting Modern Applications: Chapter 8\n[ 290 ]\nSupport for a variety of devices\nCloud-native applications must be able to support a variety of devices. Users of modern\napplications use mobile devices, desktop machines, tablets, and other devices. They expect\na unified experience across devices as well as the ability to seamlessly switch between\ndevices. In order to provide this type of support, cloud-native applications ensure that\nbackend services are able to provide the functionality that a variety of frontend devices\nneed. With the IoT, many other devices are now connected, which means that some applications\nwill be required to support them. To handle a potentially large number of devices and the\nvolume of data that they produce, highly distributed systems designed with a cloud-native\napproach are required. Twelve-factor apps\nThe twelve-factor app methodology is a set of principles that can be followed when\ndeveloping applications to be deployed to the cloud. It was originally written by the\ncreators of Heroku, which is a popular cloud platform. The principles of the twelve-factor app methodology can be used to design and develop\ncloud-native applications. An application that follows this methodology adheres to certain\nconstraints and conforms to a contract. This gives it a level of predictability that facilitates\ndeployment to the cloud. These factors make scaling easier, maximize portability, and\nensure that the application can be continuously deployed. In his book The Twelve-Factor App, Adam Wiggins describes the twelve factors as follows:\nCodebase: One codebase tracked in revision control, many deployments\nDependencies: Explicitly declare and isolate dependencies\nConfiguration: Store configuration in the environment\nBacking Services: Treat backing services as attached resources\nBuild, release, run: Strictly separate the build and run stages\nProcesses: Execute the app as one or more stateless processes\nPort binding: Export services via port binding\nConcurrency: Scale out via the process model\nDisposability: Maximize robustness with fast startup and graceful shutdown\nDevelopment/production parity: Keep development, staging, and production as\nsimilar as possible\nLogs: Treat logs as event streams", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 319", "position": 319, "chunk_type": "semantic", "token_estimate": 330}
{"text": "Architecting Modern Applications: Chapter 8\n[ 291 ]\nAdministrative processes: Run admin/management tasks as one-off processes\nIn this section, let's explore the twelve tenets of a twelve-factor app in further detail. Codebase\nA cloud-native application should have one, and only one, codebase. Larger software\nsystems may need to be decomposed into multiple applications, each of which should be\ntreated as a separate application with their own codebase. Each codebase should be tracked in a version control system and can be used for multiple\ndeployments of the application. For example, a single codebase could be deployed to the\ndevelopment, QA, staging, and production environments:\nDependencies\nDependencies for the application should be explicitly declared and isolated. A cloud-native\napplication should not rely on the implicit existence of dependencies. No assumptions \nshould be made as to what will be available in the execution environment, so applications\nshould bring their dependencies with them and declare their dependencies both precisely\nand completely. The details of the dependencies should also include the specific version\nthat is needed for each dependency. Most modern programming languages and frameworks provide a way for an application to\nmanage its dependencies. Sometimes these dependencies come in the form of a package,\nwhich is a distribution of software consisting of files typically bundled together in some\ntype of archive file. A package manager is sometimes available, which is a tool that can help\nto install, upgrade, configure, and remove packages.", "domains": ["Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 320", "position": 320, "chunk_type": "semantic", "token_estimate": 238}
{"text": "Architecting Modern Applications: Chapter 8\n[ 294 ]\nPort binding\nCloud-native applications are completely self-contained. Services should make themselves\navailable to other services by specified ports. Similar to any backing services your\napplication may be using, the API you expose may be the backing service for another\napplication. This allows one app to be the backing service of another app. Just as accessing a backing service should be achievable through a simple binding, such as a\nURL, other applications should be able to interface with your applications through a simple\nbinding. Concurrency\nA software application consists of one or more processes and a cloud-native application\ntreats processes as first-class citizens. Examples include a web process to handle HTTP\nrequests and a worker process to handle an asynchronous background task. The idea is that by running multiple processes for an application, it can run independently\nand concurrently. This allows a cloud-native application to scale out horizontally as\nneeded. Disposability\nThe processes of a cloud-native application should be disposable so that they can be started\nor stopped at any time. Processes should be designed so that they start up as quickly as\npossible and are able to shut down gracefully. Cloud-native applications should be robust against crashing. In the event that an\napplication does crash, it should have the ability to start back up without requiring extra\neffort. Applications with these qualities related to disposability allow for elastic scaling by\nthe cloud provider as well as quick deployment of code and/or configuration changes. If the\nstartup times for the processes of a cloud-native application are too slow, it can lead to\nlower levels of availability during periods of high-volume traffic. Development/production parity\nCloud-native applications should minimize differences between development and\nproduction as much as possible. Gaps between different environments include differences\nin the tools and backing services used (including any differences in the versions), the time\nbetween when something is coded and when it is released, and the fact that different\npeople may code the application versus deploy it.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 323", "position": 323, "chunk_type": "semantic", "token_estimate": 337}
{"text": "Architecting Modern Applications: Chapter 8\n[ 295 ]\nDifferences between development and production environments may cause issues to go\nundetected until the software is in production. One way to eliminate these differences and\nattain development/production parity is through the use of containers. Earlier in this\nchapter, we discussed containers and how they are used to bundle an application, along\nwith all of its dependencies, into an immutable package. This container can then be\nexecuted anywhere, including development, staging, and production environments. Containers give us predictability because we know that the application will work in an\nidentical way across the different environments. Logs\nLogs give development and operations staff visibility into the application, its behavior, and\nits exceptions. Logs are an important part of an application and will be discussed further in\nChapter 9, Cross-Cutting Concerns. Cloud-native applications should not be responsible for the routing and storage of its\noutput stream. Rather than writing to or managing log files, each process should treat logs\nas event streams and simply write its event stream to standard output (stdout) and\nstandard error (stderr). In a development environment, developers can view the stream to gain insight into the\napplication's behavior. In a production environment, the stream should be captured by the\nexecution environment and then routed to its final destination. For instance, the stream\ncould be handled by a log management tool and persisted to a data store. By following this guideline, cloud-native applications can dynamically scale to any number\nof machines and not be concerned with aggregating all of the log information. In addition,\nthe method by which log information is stored and processed can be changed without any\nmodification to the application itself. Administrative processes\nPeriodically, there will be reasons that administrative tasks will need to be executed. Examples include a database migration, cleaning up bad data, or executing analytics for a\nreport. When these needs arise, they should be executed as one-off processes in an identical\nenvironment as production. This means the scripts need to run against the same code and\nthe same configuration as any other process executed using that release. Administrative/management scripts for one-off tasks should be committed to the same\ncode repository as the rest of the application and should ship with the rest of the code in\norder to maintain consistency and avoid synchronization issues between environments.", "domains": ["Domain-Driven Design", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 324", "position": 324, "chunk_type": "semantic", "token_estimate": 390}
{"text": "Architecting Modern Applications: Chapter 8\n[ 296 ]\nSummary\nSoftware applications today have expectations and requirements that are different from the\npast. There are demands for greater availability, flexibility, fault tolerance, scalability, and\nreliability. Continuous delivery and ease of deployment may be requirements for\norganizations that want to increase their agility in order to keep their software applications\nclosely aligned with their business goals and market opportunities. We examined how\nMSA, serverless architecture, and cloud-native applications can meet these types of\ndemands and requirements. In the next chapter, we will take a look at cross-cutting concerns. Most software\napplications have common functionality that is needed throughout the application,\nincluding in different layers of the application. This functionality is called cross-cutting\nconcerns, and we will take a look at different types of crosscutting concerns and how to\ntake them into consideration during design and development.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 325", "position": 325, "chunk_type": "semantic", "token_estimate": 144}
{"text": "9: Cross-Cutting Concerns\nAll software applications have various concerns, which are groupings of logic and\nfunctionality. Some of that functionality, known as cross-cutting concerns, is used in\nmultiple areas of the application. In this chapter, we will explore cross-cutting concerns and some general guidelines for\nhandling them. We will take a look at different ways to implement them, including using\ndependency injection (DI), the decorator pattern, and aspect-oriented programming\n(AOP). We'll go over some examples of cross-cutting concerns as well as some of the special\nconsiderations that should be made when dealing with cross-cutting concerns within\nmicroservices. In this chapter, we will cover the following topics:\nCross-cutting concerns\nGeneral guidelines for cross-cutting concerns\nImplementing cross-cutting concerns with the use of DI and the decorator\npattern\nAspect-oriented programming\nTaking a look at various types of cross-cutting concerns\nCross-cutting concerns for microservices, including the use of a microservice\nchassis and the sidecar pattern", "domains": ["Architectural Patterns and Styles", "Design Patterns"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 326", "position": 326, "chunk_type": "semantic", "token_estimate": 151}
{"text": "Cross-Cutting Concerns: Chapter 9\n[ 298 ]\nCross-cutting concerns\nIn a software system, a concern is a grouping of logic or functionality that the application is\nproviding. The concerns of the system reflect the requirements. When designing a system,\nsoftware architects should seek to follow the Separation of Concerns (SoC) principle,\nwhich was described in Chapter 6, Software Development Principles and Practices. It is a \ndesign principle that seeks to reduce complexity by dividing the software system so that\nconcerns are kept separate. There are two main types of concerns in a software system:\nCore concern: It represents functionality that is fundamental to the system and is\na primary reason as to why the software is being written. For example, the logic\nrelated to the calculation of employee salaries and bonuses would be core\nconcerns of a human resource management system. The logic for each core\nconcern is typically localized to particular components. Cross-cutting concern: It is an aspect of the application that relies on and affects\nother concerns. It is functionality that is used in multiple areas, possibly\nspanning multiple layers of the application. Examples of cross-cutting concerns\ninclude security, logging, caching, and error handling. The logic for each cross-\ncutting concern is needed among multiple components. In the following diagram, various core concerns are depicted, represented by Module A,\nModule B, and Module C. You can also see the cross-cutting concerns, which intersect the\ncore concerns:", "domains": ["Design Principles", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 327", "position": 327, "chunk_type": "semantic", "token_estimate": 238}
{"text": "Cross-Cutting Concerns: Chapter 9\n[ 300 ]\nAvoiding scattered solutions\nWhen implementing cross-cutting concerns, we want to avoid simply adding the\nfunctionality to each consuming class that needs it. This approach is called scattering since\nthe implementation is scattered throughout the application. As a software architect, you\nwill want to ensure that developers are not simply copying and pasting logic that they need\nin multiple places. When an implementation for a cross-cutting concern is scattered because it is spread out\namong multiple modules, it violates the Don't Repeat Yourself (DRY) principle. Code\nmust be duplicated in order to provide the functionality of the concern in multiple places. Code duplication is wasteful, makes it more difficult to maintain consistency, increases\ncomplexity, and needlessly makes the codebase larger. These qualities all make\nmaintenance more difficult. If the logic that is duplicated needs to be modified, changes\nwill need to be made in multiple places. Avoiding tangled solutions\nWhen logic for a cross-cutting concern is mixed with logic for a different concern (either a\ncore concern or another cross-cutting concern), it is known as tangling because the logic for\ndisparate concerns is tangled together. An implementation that is tangled is likely in violation of the separation of concerns\nprinciple and tends to suffer from low cohesion. Mixing concerns increases the complexity\nof the software and reduces its quality. It lowers maintainability because changes to both\ncore and cross-cutting concerns are made more difficult. When designing the solution for a cross-cutting concern, we want to avoid tangling. Part of\naccomplishing that is to make the logic for the cross-cutting concern loosely coupled with\nthe code that needs it. Cross-cutting concerns should not be tightly coupled with other\nconcerns so that all concerns can be easily maintained and modified. Another principle that we should follow in order to avoid a tangled solution is the Single\nResponsibility Principle (SRP). A class should be responsible for one thing and do that\none thing well. If it is responsible for a core concern, such as some piece of business\nfunctionality, it should not also be responsible for implementing a cross-cutting concern. A\nresponsibility is a reason to change and a class should have only one reason to change.", "domains": ["Design Principles", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 329", "position": 329, "chunk_type": "semantic", "token_estimate": 370}
{"text": "Cross-Cutting Concerns: Chapter 9\n[ 301 ]\nA class responsible for a core concern should not need to be modified if we want to change\nthe implementation of a cross-cutting concern that it uses. Similarly, the implementation of\na cross-cutting concern should not need to be modified if we need to change the\nimplementation of a class that uses it. Avoiding a tangled solution also allows us to adhere to the Open/Closed Principle (OCP),\nwhich states that software components should be open for extension but closed for\nmodification. When we want to add cross-cutting concerns to business functionality, we\nshould have the ability to do so by extending the component with new code without being\nrequired to modify the existing business logic. Implementing cross-cutting concerns\nImplementations should follow the design goals of cross-cutting concerns by maintaining\nconsistency, not being scattered, and not being tangled. There are several different \napproaches that can be taken when implementing cross-cutting concerns. These include DI,\nthe decorator pattern, and AOP. Using dependency injection (DI)\nOne approach to handling cross-cutting concerns is to use the DI pattern, which we covered\nin Chapter 6, Software Development Principles and Practices. This pattern can be used to inject\ncross-cutting dependencies into classes that need them. This allows us to write loosely\ncoupled code and avoid scattering. The logic for the cross-cutting concern will not be\nduplicated in multiple places. For example, if we had an Order class that had logging and caching cross-cutting concerns,\nwe could inject them like this:\npublic class Order\n{\n    private readonly ILogger _logger;\n    private readonly ICache _cache;\n    public Order(ILogger logger, ICache cache)\n    {\n        if (logger == null)\n            throw new ArgumentNullException(nameof(logger));\n        if (cache == null)\n            throw new ArgumentNullException(nameof(cache));\n        _logger = logger;", "domains": ["Design Patterns", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 330", "position": 330, "chunk_type": "semantic", "token_estimate": 286}
{"text": "Cross-Cutting Concerns: Chapter 9\n[ 302 ]\n        _cache = cache;\n    }\n}\nBy eliminating hard-coded dependencies of cross-cutting concerns through the use of DI, it\nalso provides us with the ability to change the implementation of a cross-cutting concern,\neither at runtime or compile-time. We may want to use a different implementation for a\ncross-cutting concern at runtime based on something such as a configuration setting. As\nlong as we have a common interface that is used by each implementation, this approach\nwould allow us to change the implementation without having to recompile and redeploy\nthe application. Using DI for cross-cutting concerns also increases the testability of the application. Any\ncode that has a cross-cutting concern is dependent on the abstraction of that concern, not\nthe concrete implementation, which allows us to mock the dependencies in our unit tests. However, there are some disadvantages to this approach. It does require you to inject\ndependencies for cross-cutting concerns everywhere that you need them. While this makes\nit consistent with how you may be approaching other dependencies, some cross-cutting\nconcerns are needed in many places (for example, logging), and it may become tedious to\ninject it in all of the places that need it. Although this approach eliminates scattering, it does not eliminate tangling. If you take this\napproach, you will have cross-cutting logic mixed in with your other logic. In the preceding\norder class, code that uses the logging and cache objects that were injected in the\nconstructor will be located throughout the Order class, mixed with logic for other concerns. Using the decorator pattern\nAnother approach to implementing cross-cutting concerns is to use the decorator pattern. The decorator pattern can add behaviors dynamically to an object, including behaviors for\ncross-cutting concerns. It is essentially like creating a wrapper to handle a cross-cutting\nconcern around some other object. The following diagram shows the decorator pattern:", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 331", "position": 331, "chunk_type": "semantic", "token_estimate": 314}
{"text": "Cross-Cutting Concerns: Chapter 9\n[ 303 ]\nIn the preceding diagram, the ConcreteComponent class implements the IComponent\ninterface and is a class that can either be used on its own or wrapped in one or more\ndecorators. The Decorator class is an abstract class that also implements the IComponent\ninterface and wraps (has a reference to) the component. Each concrete decorator class\ninherits from the Decorator class and adds its behavior. For example, let's say we have an IAccountService interface with a Save method and an\nAccountService concrete component that implements that interface:\npublic interface IAccountService\n{\n    void Save(IAccount account);\n}\npublic class AccountService : IAccountService\n{\n    public void Save(IAccount account)\n    {\n        // Save logic\n    }\n}", "domains": ["Design Patterns", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 332", "position": 332, "chunk_type": "semantic", "token_estimate": 117}
{"text": "Cross-Cutting Concerns: Chapter 9\n[ 304 ]\nThe Save method in the AccountService class only contains logic related to the save\n(represented by the Save logic code comment) and does not contain logic for any cross-\ncutting concern. An abstract decorator class can be created that also implements the IAccountService\ninterface and will wrap the AccountService class. You may notice that we are also using\nDI in conjunction with the decorator pattern:\npublic abstract class AccountServiceDecorator : IAccountService\n{\n    protected readonly IAccountService _accountService;\n    public AccountServiceDecorator(IAccountService accountService)\n    {\n        _accountService = accountService;\n    }\n    public virtual void Save(IAccount account)\n    {\n        _accountService.Save(account);\n    }\n}\nWe can now create concrete decorators for the account service. The following is one for\nlogging:\npublic class LoggingAccountService : AccountServiceDecorator\n{\n    private readonly ILogger _logger;\n    public LoggingAccountService(IAccountService accountService,\n        ILogger logger)\n        : base (accountService)\n    {\n        _logger = logger;\n    }\n    public override void Save(IAccount account)\n    {\n        _accountService.Save(account);\n        _logger.LogInfo($\"Saved account: {account.Number}\");\n    }\n}", "domains": ["Design Patterns", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 333", "position": 333, "chunk_type": "semantic", "token_estimate": 151}
{"text": "Cross-Cutting Concerns: Chapter 9\n[ 305 ]\nWe can also make one for caching:\npublic class CachingAccountService : AccountServiceDecorator\n{\n    private readonly ICache _cache;\n    public CachingAccountService(IAccountService accountService,\n        ICache cache)\n        : base(accountService)\n    {\n        _cache = cache;\n    }\n    public override void Save(IAccount account)\n    {\n        _accountService.Save(account);\n        _cache.Put(account.Number.ToString(), account.Name);\n    }\n}\nThe concrete decorator classes, LoggingAccountService and CachingAccountService,\ncontain logic for the cross-cutting concern for which they are responsible. This logic can\noccur either before or after the logic of the core concern. One challenge with this approach\nis the fact that you may want to execute cross-cutting logic, not just before or after core\nlogic, but also in the middle of the core logic. One way around this is to make your methods\nsmaller so that there are additional points where cross-cutting logic can be executed. The fact that the concrete decorator classes all implement the IAccountService interface\n(through their decorator parent class), means that we can have an instance of the account\nservice that contains logic for multiple cross-cutting concerns. For example:\nIAccountService accountService = new AccountService();\nIAccountService loggingAccountService =\n    new LoggingAccountService(accountService, logger);\nIAccountService cachingAndLoggingAccountService =\n    new CachingAccountService(loggingAccountService, cache);", "domains": ["Design Patterns", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 334", "position": 334, "chunk_type": "semantic", "token_estimate": 187}
{"text": "Cross-Cutting Concerns: Chapter 9\n[ 306 ]\nThe instance of the account service being held by the\ncachingAndLoggingAccountService variable has been decorated with both logging and\ncaching functionality. The account service has been wrapped by multiple decorators:\nAs with other types of dependencies, a DI container can handle dependency chains for you\nso that when you want a concrete instance of IAccountService, you will receive one that\nhas been decorated with all of the cross-cutting concerns, without having to do the wiring\nup yourself. Using the decorator pattern in conjunction with DI will allow you to write logic for the\ncross-cutting concerns that are neither scattered nor tangled with other logic. However, it\ndoes require you to create the decorator classes. For any cross-cutting concerns that are\nneeded in many classes, large software systems will require a large number of decorators to\nbe created. This work can be tedious and repetitive, although some of the code could be\ngenerated automatically through code generation. In addition to DI and the decorator pattern, another approach to handling cross-cutting\nconcerns that you can consider is to use AOP in your project. Aspect-oriented programming\nAspect-oriented programming (AOP) is a paradigm that was created to handle the\nscattering and tangling of boilerplate code in object-oriented programming (OOP), such as\nthe code necessary for cross-cutting concerns. Gregor Kiczales and other researchers at\nXerox PARC, which is now known as PARC, a Xerox company, did the original research on\nthe topic. They eventually wrote a paper, AOP, in which they described a solution for\nhandling cross-cutting concerns.", "domains": ["Design Patterns"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 335", "position": 335, "chunk_type": "semantic", "token_estimate": 260}
{"text": "Cross-Cutting Concerns: Chapter 9\n[ 309 ]\nBefore a class can be used, weaving must take place to combine its logic with the logic for\nall of the different cross-cutting concerns that the class uses. AOP tools either perform\nweaving at compile-time or at runtime. Compile-time weaving\nAOP tools that employ compile-time weaving perform an additional step to attach aspects\nafter a program is compiled. A program is first compiled as it normally is, resulting in a\nDLL or EXE. Once the program has been compiled, it is run through a post-compilation\nprocess. This process is handled by a post-processor that is provided with an AOP tool. The post-processor takes the DLL or EXE and adds the aspects to it. Using configuration,\nthe post-processor knows where to apply the advice, such as before a method is executed or\nwhen an exception is thrown. The resulting DLL/EXE has both the logic for the core\nconcern and all of the advice:\nOne of the benefits of compile-time weaving is that there is no overhead during runtime to\nperform the weaving. However, because the aspects are weaved at compile-time, you will\nnot be able to change them at runtime through configuration. Runtime weaving\nRuntime weaving does not take place until after the application starts executing. The advice\nfor an aspect and the code that it will be applied to are both instantiated at runtime. Unlike\nthe compile-time weaving approach, no post-compilation changes are made to binaries. The way that runtime weaving works is similar to the decorator pattern that was described\nin the Using the decorator pattern section that appeared earlier in this chapter. The main\ndifference is the fact that an AOP tool can generate the decorator classes at runtime without\nrequiring developers to manually create them beforehand. The AOP tool generates a proxy\nobject dynamically, which implements the same interface as the target object and can\ndelegate to the target object while weaving the advice with the core logic.", "domains": ["Design Patterns", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 338", "position": 338, "chunk_type": "semantic", "token_estimate": 328}
{"text": "Cross-Cutting Concerns: Chapter 9\n[ 310 ]\nAn advantage of runtime weaving is that there is no post-compilation process involved. The machine that is compiling the application, whether it is a development machine or a\nbuild server, will not require extra software (a post-processor from an AOP tool) to\ncomplete the compilation. Runtime weaving also affords you the flexibility of configuring\naspects so that they can be changed at runtime. The disadvantage of runtime weaving is\nthat, unlike compile-time weaving, there is some amount of overhead involved at runtime\nto perform the weaving. Types of cross-cutting concerns\nThere are a variety of different types of cross-cutting concerns that may be part of a\nsoftware application. This is by no means an exhaustive list, but some common examples of\ncross-cutting concerns include the following:\nCaching\nConfiguration management\nAuditing\nSecurity\nException management\nLogging\nCaching\nLeveraging the use of caching in a software application is a common way to improve\nperformance. It may be used in a variety of places where data is being read, making it a\ncross-cutting concern for the application. A reusable caching service should provide the ability to perform operations, such as\nputting data in a cache, getting data out of a cache, and setting policies on how and when\ncached data will expire. When designing a software system, software architects must make a decision on the type of\ncaching that the system will use. The two main types of server-side caching are an in-\nprocess cache and a distributed cache. With an in-process cache, the cache is local to the\napplication instance. Load balanced applications will have multiple instances of the\napplication, each of which will have their own in-process cache. A distributed cache offers a\nsingle logical view of the cache even when there are application instances on multiple\nservers.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 339", "position": 339, "chunk_type": "semantic", "token_estimate": 303}
{"text": "Cross-Cutting Concerns: Chapter 9\n[ 311 ]\nWe will be examining the use of caching further in Chapter 10, Performance Considerations. Configuration management\nConfiguration management involves deciding what options for a software application\nshould be made configurable and how that configuration will be stored, protected, and\nmodified. In order to provide flexibility in how your application behaves and give it the\nability to run in a variety of environments, configuration options should be made available. These options should be made external to the application so that they can be modified\nwithout requiring recompilation of the application. A software application needs to be deployed and used in multiple environments, such as\ndevelopment, testing, staging, and production. In addition, it may use a number of\ndifferent infrastructures and third-party services. Examples include a database, a service\nregistry, a message broker, an email (SMTP) server, a payment processing service, and a\nmessage broker. Different environments may require different configuration values for\nthese various services. Externalizing the configuration settings allows us to use our application in different ways\nand in different environments without having to change application code and recompile\nthe application. It also makes deploying the application to a cloud environment easier. If you recall from\nChapter 8, Architecting Modern Applications, one of the factors in the twelve-factor app\nmethodology is to ensure that there is a strict separation between an application's code and\nits configuration. As a software architect, you should consider which settings need to be configurable in your\napplication. Only the settings that need to be configurable should be made available for\nmodification. Including unnecessary configuration options can lead to an application\nhaving an excessive amount of settings, which will only make it more difficult to use and\nunderstand. Software applications with overly complex configuration increase the\nlikelihood of incorrect configuration, which can lead to the application not working\nproperly or can make the application vulnerable to security breaches. A release of a software application is an immutable package, whether that software is being\ndeployed on its own server, on a virtual machine (VM), or on a container image. However,\nthat immutable package needs to have the ability to be deployed in different environments. Externalizing the configuration will give you that ability.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 340", "position": 340, "chunk_type": "semantic", "token_estimate": 372}
{"text": "Cross-Cutting Concerns: Chapter 9\n[ 312 ]\nAuditing\nA cross-cutting concern of many software applications is the auditing of data changing\noperations. There may be requirements to maintain an audit trail that includes information\nabout a data change, such as the date/time that it occurred and the identity of the\nindividual who made the change. There may be a requirement to record information about the nature of a particular data\nchange, such as the old and new values. In event-driven systems, persisting events and\ntheir details can serve as the audit trail. Security\nSecurity is a significant cross-cutting concern. It includes the authentication of users and the\nauthorization of the operations the user can perform with the software application. Once a\nuser is authenticated and their identity is known, their authorization must be checked in\norder to determine which operations they are capable of executing within the application. We will be examining security-related topics, including authentication and authorization,\nin Chapter 11, Security Considerations. Exception management\nAn exception is a type of error that occurs during program execution that we expect may\nhappen. They are issues that are known to occur and an application can be designed to \nrecognize and handle them. Exceptions can occur for a variety of reasons. Some examples include attempting to use a\nnull object reference, attempting to access an array using an index that is out of range,\nexceeding a specified timeout, being unable to write to a file, or the inability to connect to a\ndatabase. Many programming languages provide ways to handle exceptions and transfer\nthe flow of execution to a different part of the logic. For example, in the C# language, we\ncan use try/catch/finally statements to handle exceptions. Ineffective exception management makes it more difficult to diagnose and resolve issues\nwith the application. A failure to handle exceptions properly can also lead to security\nissues. Exception management should be treated as a cross-cutting concern and a\ncentralized exception management approach should be designed for the application. A\nsoftware application should have consistency in terms of how exceptions and errors are\nhandled.", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Software Quality Attributes", "Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 341", "position": 341, "chunk_type": "semantic", "token_estimate": 349}
{"text": "Cross-Cutting Concerns: Chapter 9\n[ 313 ]\nCommon boilerplate code to perform operations such as the logging of exceptions (logging\nis its own cross-cutting concern) and communicating the fact that an exception occurred\nback to the user can be handled in a centralized and consistent way. When logging and\ncommunicating exception details, sensitive information should not be revealed. All\nexceptions should be logged as the information may be helpful in resolving an issue. When\nexceptions are logged, any additional details, such as contextual information, should be\nadded to make the information more useful. A good exception management strategy should also take into consideration unhandled\nexceptions and design a way to deal with them. Failures in the application should not leave\nit in an unstable state or corrupt data. Logging\nLogging is an important part of software applications, allowing you to know what your\ncode did when it was executed. It provides you with the ability to see when things execute\nas expected and, perhaps more importantly, assists you in diagnosing problems when they\ndo not. The availability of logs can really help you to troubleshoot application issues. Common characteristics of log entries include the following:\nDate/time: It is imperative to know when the event took place\nSource: We want to know the source/location of the event\nLog Level/Severity: It is helpful to know the level/severity of the log entry\nMessage: Log entries should have some sort of description or detail explaining\nthe log entry\nUnderstanding log levels\nMost logging frameworks support the ability to specify a log level or the severity of each\nlog entry that is created. Although the log levels vary depending on the framework used,\nsome common log levels include the following:\nTRACE: Use this level for tracing the code, such as being able to see when\nexecution flow enters and exits specific methods. DEBUG: This level records diagnostic details that can be helpful during\ndebugging. It can be used to make note of when certain aspects of the logic are\ncompleted successfully. It can also provide details such as executed queries and\nsession information, which can be used in determining the cause of an issue.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 342", "position": 342, "chunk_type": "semantic", "token_estimate": 359}
{"text": "Cross-Cutting Concerns: Chapter 9\n[ 314 ]\nINFO: This level is for logging details about normal operations during the\nexecution of logic. It is common for INFO to be a default log level. It is for useful\ninformation that you want to have but typically won't spend much time\nexamining under normal circumstances. WARN: When you have a situation in which incorrect behavior takes place, but\nthe application can continue, it can be logged at the Warn level. ERROR: Use this level for exceptions and problems that caused an operation to\nfail. FATAL: This level is reserved for the most severe errors, such as those that may\ncause the shutdown of the system or data corruption. Logging frameworks typically allow you to configure the level at which logging will take\nplace, such as being able to specify a minimum log level. For example, if the minimum log\nlevel is configured as Info, then logging will take place for the log levels of INFO, WARN,\nERROR, and FATAL. A detailed log level, such as TRACE, is not typically used for sustained periods,\nparticularly for a production environment. This is due to the high volume of detailed\nentries that will be produced, which can degrade performance and excessively use up disk\nand bandwidth resources. However, when diagnosing an issue, being able to change the\nlog level to DEBUG or TRACE temporarily can provide valuable information. Routing log entries\nMany logging frameworks provide functionality that allows you to configure routing rules\nfor log entries. These may be based on the log level, the source, or some combination of\ncriteria. Log entries can then be set up to target different destinations. Some examples of\ndestinations for log entries include the console, text files, databases, email, and the\nWindows Event Log. Writing logs to text files on local disks has been a common practice, but when your\napplication is running on many different servers, it becomes difficult to search through all\nof the files without the use of tools. This issue is further exacerbated when your application\nis running in the cloud. Given the elasticity provided by hosting your application in the\ncloud, the number of servers running your application at any given time and their location\nare dynamic.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 343", "position": 343, "chunk_type": "semantic", "token_estimate": 374}
{"text": "Cross-Cutting Concerns: Chapter 9\n[ 315 ]\nAs was mentioned in Chapter 8, Architecting Modern Applications, cloud-native applications\nshould simply treat their log as event streams and should not be responsible for the routing\nand storage of those streams. Instead of writing to a log file, it should write event streams\nto standard output (stdout) and standard error (stderr). Your application may scale out to\nany number of machines, and no assumptions should be made as to where your application\nwill run or where log information will be stored. You can either leverage services that your\ncloud provider makes available to aggregate and store log information, or you can provide\nyour own implementation. Using Elastic Stack\nOne of the solutions to centralizing your logging functionality so that log information can\nbe consolidated and managed is the Elastic Stack. The Elastic Stack is an integrated solution\nof open-source products that offer a highly scalable end-to-end solution for aggregating,\nsearching, analyzing, and visualizing logging data. It was formerly known as (and you may still hear it referred to as) the ELK stack, an\nacronym that represents the solution's use of the Elasticsearch, Logstash, and Kibana\nproducts. A data shipper product named Beats, which is part of Elastic Stack, was not part\nof the original ELK stack. One type of data shipper within Beats is Filebeat, which can be\nused for text log files. Elasticsearch\nElasticsearch is an open-source, distributed search engine and document database that can\nstore, search, and analyze data. Among its many uses, it can store all of the log data that\nyour application produces. Elasticsearch allows you to quickly search through data, and its capabilities include full-text\nquerying. As your system grows, it can horizontally scale to handle large amounts of data,\neven if there are billions of log lines. It is easily extendable by adding nodes and resilient to\nfailing nodes.", "domains": ["Design Principles", "Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 344", "position": 344, "chunk_type": "semantic", "token_estimate": 313}
{"text": "Cross-Cutting Concerns: Chapter 9\n[ 317 ]\nBeats\nBeats is a platform for small, lightweight data shippers. These data shippers gather data\nfrom potentially large numbers of different sources and send it to Logstash or Elasticsearch. Beats has shippers for all kinds of data:\nFilebeat: Text log files\nMetricbeat: Metrics for systems and services\nPacketbeat: Network monitoring\nWinlogbeat: Windows event logs\nAuditbeat: Audit data\nHeartbeat: Uptime monitoring\nThe open-source community has created beats for many other sources. Libbeat is the \ncommon library for forwarding data, which can be leveraged to create custom beats. For logging, Filebeat can be used to aggregate log data. It is also container-ready and can be\ndeployed in its own container on the same host. Once deployed, it can collect logs from all\nof the containers running on that host. Filebeat works seamlessly with Logstash. During periods where there is a higher volume of\nlog data being processed, Logstash can let Filebeat know to slow down the reading of the\nlog data until the volume lowers so that processing can return to the regular pace. Cross-cutting concerns for microservices\nProviding cross-cutting concerns for microservices requires some additional consideration. In this section, we will take a look at using a microservice chassis and the sidecar pattern to\nhandle some of the difficulties with implementing cross-cutting concerns for microservices. Leveraging a microservice chassis\nIn a monolithic application, cross-cutting concerns can be designed and developed once. After their development is complete and they are available for use, they can be leveraged\nthroughout the application.", "domains": ["Architectural Patterns and Styles", "Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 346", "position": 346, "chunk_type": "semantic", "token_estimate": 255}
{"text": "Cross-Cutting Concerns: Chapter 9\n[ 318 ]\nMicroservices are independently deployable, self-contained services. As a result, \nimplementing a cross-cutting concern has to be done repeatedly for each microservice. This\ncan make it prohibitively expensive for an organization to develop microservices when\nconsidering the development resources that would be needed. A system may consist of\nhundreds of microservices, and your team may be creating new ones throughout the life of\nthe application. The process of creating a new microservice should be made as fast and easy\nas possible, and you are not going to want to implement cross-cutting concerns for each\none. To overcome this challenge, a microservice chassis can be used. A microservice chassis is a\nframework that can take care of many of the cross-cutting concerns for microservices and\ndo so in a way that allows all of your microservices to utilize the functionality. Some examples of a microservice chassis include Spring Cloud, Microdot, Gizmo, Go kit,\nand Dropwizard. Each microservice framework is different, but some of the cross-cutting\nconcerns that they handle include the following:\nLogging\nExternalized configuration\nMetric reporting and instrumentation\nService registration and discovery\nHealth checks\nTracing\nIt should be noted that it is not required that you use an open-source or third-party\nmicroservice chassis. It could be a framework that you develop within your organization,\nspecifically tailored to your needs and using the technologies that you choose. The\nimportant point is to have a microservice framework that you can reuse so that you are not\nimplementing the same cross-cutting concerns multiple times. Once a microservice\nframework is in place to handle cross-cutting concerns, the development team can focus on\nthe core concerns of the microservices themselves. Using the sidecar pattern\nOne of the options available to you when developing microservices is polyglot\ndevelopment. You have the flexibility to use multiple programming languages, runtimes,\nframeworks, and data storage technologies. Microservices are independently developed\nand deployed, allowing you to select best-of-breed technologies to accomplish the given\ntask.", "domains": ["Architectural Patterns and Styles"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 347", "position": 347, "chunk_type": "semantic", "token_estimate": 330}
{"text": "Cross-Cutting Concerns: Chapter 9\n[ 319 ]\nIf your software system is taking advantage of polyglot microservices, it can make it\ndifficult to maintain libraries for cross-cutting concerns. You would need one for each\nprogramming language that you are using, resulting in duplication of effort and a lower\nlevel of maintainability. One solution to this problem is to use the sidecar pattern. The logic for cross-cutting\nconcerns are placed in their own process or container (known as a sidecar container or a\nsidekick container) and then attached to the primary application. Similar to how a\nmotorcycle sidecar is attached to a motorcycle, the sidecar application is attached to the\nprimary application and runs alongside it:\nUse of the sidecar pattern allows the programming language and framework of the primary\napplication to be different from that of the sidecar application. When used for cross-cutting\nconcerns in a heterogeneous environment, it means that you will not need to develop logic\nfor a cross-cutting concern in each programming language that is being used. Primary and sidecar applications have access to the same resources. This makes it easy, for\nexample, for a sidecar application that has the purpose of monitoring the health of the\nprimary application to monitor its system resources. This pattern is also useful when the sidecar application is owned by a different organization\nor a different team. Even if you do not have control over the implementation of the sidecar\napplication, you will be able to use it with your primary application. For inter-process communication (IPC) between the primary application and the sidecar\napplication, it is a best practice is to use a communication mechanism that is language and\nframework agnostic. Although IPC between the primary and sidecar applications is\ngenerally fast because they are located on the same host, there will be some overhead\ninvolved due to the communication as compared with in-process communication. If the\ninterface is chatty (more fine-grained operations requiring an increased amount of\ncommunication between the two processes) and performance needs to be optimized, use of\nthe sidecar pattern may not be ideal.", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 348", "position": 348, "chunk_type": "semantic", "token_estimate": 347}
{"text": "Cross-Cutting Concerns: Chapter 9\n[ 320 ]\nSummary\nCross-cutting concerns are an important part of a software application. In this chapter, you\nlearned what cross-cutting concerns are and how to avoid scattering and tangling the\nsolutions for cross-cutting concerns. In this chapter, we learned how we can use DI, the decorator pattern, and AOP to\nimplement cross-cutting concerns. This chapter provided some examples of common cross-\ncutting concerns, such as caching, configuration management, auditing, security, exception\nmanagement, and logging. We also explored using a microservice chassis and/or the\nsidecar pattern when implementing cross-cutting concerns for microservices. In the next chapter, we will take a look at the performance considerations that software\narchitects make. We will explore the importance and fundamentals of performance in a\nsoftware application. Various performance improvement techniques will be detailed. The\nchapter will also cover the topic of concurrency and how parallel execution can speed up\nthe performance of multi-processor or multi-core systems.", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 349", "position": 349, "chunk_type": "semantic", "token_estimate": 156}
{"text": "10: Performance Considerations\nUsers have high expectations when it comes to the performance of the applications they\nuse. Performance needs are requirements that must be met and their importance should be\nreflected in the fact that the entire team must take ownership of performance. Performance\nis a quality attribute and should be considered throughout the development of an\napplication. After taking a look at the importance of performance and some of the common terminology\nrelated to it, this chapter will describe a systematic approach to improving performance. We will also cover server-side caching, web application performance, and techniques to\nimprove database performance. In this chapter, we will cover the following topics:\nThe importance of performance\nPerformance terminology\nTaking a systematic approach to performance improvement\nServer-side caching, including different caching strategies and usage patterns\nImproving web application performance, including HTTP caching, compression,\nminifying resources, bundling resources, using HTTP/2, using content delivery\nnetworks, and optimizing web fonts\nDatabase performance, including designing an efficient database schema, using\ndatabase indexes, scaling up/out, and concurrency\nThe importance of performance\nThe performance of a software application indicates the responsiveness of the operations\nthat it can perform. Users have greater expectations today in terms of the responsiveness of\nthe applications that they use. They demand fast response times regardless of their location\nor the device that they are using.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 350", "position": 350, "chunk_type": "semantic", "token_estimate": 222}
{"text": "Performance Considerations: Chapter 10\n[ 322 ]\nMost importantly, software must serve its functional purpose, be reliable in its operation,\nand provides its functionality in a usable way. If it does not, then the speed of the\napplication will not matter. However, once those needs are met, performance is of high\nimportance. Performance affects user experience\nThe speed of your application plays a major role in the overall user experience (UX). A\nuser's satisfaction with the application is influenced by the speed of the application. The\nperformance of your application affects the organization's bottom line, whether it is because\ncustomers are being gained/lost for a customer-facing site or because productivity is being\ngained/lost for an enterprise application. For web and mobile applications, the loading time for a page is a major factor in page\nabandonment. If a page takes too long to load, many users will simply leave. This is evident\nwhen we look at things such as a site's bounce and conversion rates. Bounce rate\nA bounce occurs when a user has just a single-page session on a site and leaves without\nvisiting any of the other pages. The bounce rate, which is sometimes referred to as the exit\nrate, is the percentage of users who bounce:\nAs you would expect, as page load times increase, so too does the bounce rate. Examples of\nactions that result in a bounce include the user closing the browser window/tab, clicking on\na link to visit a different site, clicking the back button to leave the site, navigating to a\ndifferent site by typing in a new URL or using a voice command, or having a session\ntimeout occur. Conversion rate\nThe conversion rate is the percentage of site visitors who ultimately take the desired\nconversion action. The desired conversion action depends on the purpose of the site, but a\nfew examples of common ones include placing an order, registering for membership,\ndownloading a software product, or subscribing to a newsletter.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 351", "position": 351, "chunk_type": "semantic", "token_estimate": 329}
{"text": "Performance Considerations: Chapter 10\n[ 323 ]\nThe conversion rate is represented by the following formula:\nWebsites that have poor performance will have a lower conversion rate. If a site is slow,\nusers will simply leave the site and go somewhere else. Performance is a requirement\nSpeed is a feature of your application and if it is not fast enough, then it is not good enough. Performance is a quality attribute of software systems and cannot be considered as just an\nafterthought. It should play an integral part throughout the life cycle of a software\napplication. It is a requirement of the system and, like other requirements, it must be\nunambiguous, measurable, and testable. When we discussed requirements in Chapter 3, Understanding the Domain, it was stated that\nrequirements must be specified clearly, measurable with specific values/limits when\nappropriate, and testable so that it can be determined whether the requirement has been\nsatisfied. For example, it is not sufficient to simply state that the web page must load in a\ntimely manner. In order to make it unambiguous, measurable, and testable, it would have to\nbe written to state that the web page must load within two seconds. Treating performance as a requirement also means that we should have tests for it. We can\nmeasure how long it takes to execute tests and assert that they can be completed within a\ntime limit. While performance tests are not executed as often as unit tests, it should be easy\nto execute performance tests regularly. Page speed affects search rankings\nPage speed is a consideration in a site's mobile search ranking in Google search results. Currently, this criterion only affects pages with the slowest performance, but it shows the\nimportance Google places on web page performance. For customer-facing websites, you do\nnot want performance to negatively affect your site's search ranking. Defining performance terminology\nBefore we explore the topic of performance further, let's define some of the common terms\nrelated to performance.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 352", "position": 352, "chunk_type": "semantic", "token_estimate": 330}
{"text": "Performance Considerations: Chapter 10\n[ 324 ]\nLatency\nLatency is the amount of time (or delay) it takes to send information from a source to a\ndestination. A phrase you may hear regarding latency is that it is the time spent on the wire,\nsince it represents the amount of time a message spends traveling on a network. Something\nis latent if it is dormant and we must wait to perform any further processing while a\nmessage is traveling across a network. Latency is usually measured in milliseconds. Factors such as the type of network hardware\nbeing utilized, the connection type, the distance that must be traveled, and the amount of\ncongestion on the network all affect latency. In many instances, a significant portion of the total latency takes place between your office\nor home and the internet service provider (ISP). This is known as last-mile latency\nbecause even if data travels across the country or even the world, it can be the first or last\nfew hops that contribute most to the total latency. Throughput\nThroughput is a measure of a number of work items per a particular time unit. In the\ncontext of a network, it is the amount of data that can be transferred from one location to\nanother in a given amount of time. It is typically measured in bits per second (bps),\nmegabits per second (Mbps), or gigabits per second (Gbps). In the context of application logic, throughput is how much processing can be done in a\ngiven amount of time. An example of throughput in this context would be the number of\ntransactions that can be processed per second. Bandwidth\nBandwidth is the maximum possible throughput for a particular logical or physical\ncommunication path. Like throughput, it is typically measured in terms of a bit rate, or the\nmaximum number of bits that could be transferred in a given unit of time.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 353", "position": 353, "chunk_type": "semantic", "token_estimate": 318}
{"text": "Performance Considerations: Chapter 10\n[ 325 ]\nProcessing time\nProcessing time is the length of time that it takes for a software system to process a\nparticular request, without including any time where messages are traveling across the\nnetwork (latency). Sometimes a distinction is made between server processing time and\nclient processing time. A variety of things can affect processing time, such as how the application code is written,\nthe external software that works in conjunction with the application, and the characteristics\nof the hardware that is performing the processing. Response time\nResponse time is the total amount of time between the user making a particular request\nand the user receiving a response to that request. Although some people use the terms\nlatency and response time interchangeably, they are not synonymous. For a given request,\nresponse time is a combination of both the network latency and the processing time. Workload\nWorkload represents the amount of computational processing a machine has been given to\ndo at a particular time. A workload uses up processor capacity, leaving less of it available\nfor other tasks. Some common types of workload that may be evaluated are CPU, memory,\nI/O, and database workloads. Taking regular measurements of workload levels will allow you to predict when peak loads\nfor your application take place and also allow you to compare the performance of your\napplication at different load levels. Utilization\nUtilization is the percentage of time that a resource is used when compared with the total\ntime that the resource is available for use. For example, if a CPU is busy processing\ntransactions for 45 seconds out of a one-minute timespan, the utilization for that interval is\n75%. Resources such as CPU, memory, and disk should be measured for utilization in order\nto obtain a complete picture of an application's performance. As utilization approaches the\nmaximum throughput, response times will rise.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 354", "position": 354, "chunk_type": "semantic", "token_estimate": 314}
{"text": "Performance Considerations: Chapter 10\n[ 326 ]\nTaking a systematic approach to\nperformance improvement\nWhen looking to improve the performance of an application, the entire development team\nshould be involved. Teams will have greater success at optimizing performance when the\nentire team, and not just certain individuals, take ownership of the performance of the\nsoftware application. When engaging in an effort to improve performance, it can be helpful to follow a systematic\napproach. An iterative process that consists of the following steps can be used for\nperformance improvement:\nProfiling the application\nAnalyzing the results\nImplementing changes\nMonitoring changes\nThe following diagram illustrates this process:\nLet's now take a closer look at each of the steps in the process. Profiling an application\nThe first step is to profile the application. Profiling is an analysis of a software system that\nresults in measurements of the system's execution. Development teams should not be \nguessing where performance issues exist, as they will not always be where you expect. Rather than guessing, development teams should be acquiring precise measurements. These measurements can then be used to make decisions. Examples include how much time\na particular method takes to execute, how often a method is called, the time spent in\ncommunication, the amount of I/O that is taking place, and how much of the CPU and\nmemory is being used.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 355", "position": 355, "chunk_type": "semantic", "token_estimate": 224}
{"text": "Performance Considerations: Chapter 10\n[ 327 ]\nThere are profiling tools, called profilers, available that can be leveraged to profile an\napplication. Two broad categories of how profilers collect information are through \ninstrumentation and by sampling. An effective profiling strategy might use both types of\nprofilers to understand where performance issues may exist in a software system. Instrumentation\nWhen instrumentation is used, code is added to the software system being profiled in\norder to collect information. For example, to collect data on the time spent in a method and\nto get a count of how many times the method is used, instrumentation code is executed at\nthe beginning and end of the method. Instrumentation code can be manually added by the development team. However, profilers\ncan add instrumentation automatically. Some profilers that use instrumentation modify\nsource code, while others work at runtime. Either way, instrumentation can provide a great\nlevel of detail. However, a disadvantage of instrumentation is that the instrumentation code\ncan affect the measurements. The degree of the effect really depends on what data is being\ncollected and the extent of the instrumentation. For example, the instrumentation code itself takes some time to execute. Profilers can take\nthis into consideration by calculating the overhead they incur and subtracting that amount\nfrom their measurements. However, adding code to a method can change CPU\noptimizations and change the way that it executes the method. Consequently, very short\nmethods can sometimes yield inaccurate results. Statistical profilers\nProfilers that work by sampling, which are sometimes known as statistical profilers, let\napplications execute without any runtime modifications. This type of profiling is conducted\noutside of the application's process and overcomes some of the disadvantages of\ninstrumentation. An operating system (OS) interrupts the CPU at regular intervals, giving it an opportunity\nfor process switching. Sampling works by collecting information during these\ninterruptions. Sampling is less intrusive than instrumentation, as it allows the software\nsystem to execute close to its normal speed. The downside is the fact that the data collected\nare is often an approximation and is not as numerically accurate as data that can be\ncollected through instrumentation.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 356", "position": 356, "chunk_type": "semantic", "token_estimate": 355}
{"text": "Performance Considerations: Chapter 10\n[ 328 ]\nAnalyzing the results\nOnce performance data is collected through profiling, it can be used to identify\nperformance issues and areas of the application that are bottlenecks. Bottlenecks are parts\nof the software system that limit performance. These parts in the software are unable to\nkeep pace given their capacity and a particular amount of work, which in turn slows down\nthe overall performance of the application. The focus of the performance improvement effort should be on optimizing the bottlenecks\nof the application. Software architects should not focus their attention on optimizing non-\nbottlenecks unless all of the identified bottlenecks have been addressed and there is time to\noptimize non-bottlenecks. Some common bottlenecks include CPU, memory, network, database, and disk utilization. Different problems will lead you to different solutions. For example, if the network is too\nslow, you can look into ways to send less data across, such as compressing or caching data. If the database is too slow, you can work with the database administrator (DBA) to add\nindexes, optimize queries, make use of stored procedures, and possibly denormalize some\nof the data. If the CPU is the bottleneck, you can look into getting a faster processor, adding\nprocessors, storing/caching data so that it doesn't need to be calculated, or making\nimprovements to the algorithms being used. Some bottlenecks will lead you to conclude that you need to either scale horizontally or\nscale vertically. Vertical scaling involves increasing the capacity of existing servers by\nadding resources, such as adding memory and processors, replacing existing processors\nwith faster ones, and increasing the size of available disk space. Horizontal scaling involves\nadding servers to your pool of resources in order to scale wider and handle more traffic. Implementing changes\nProfiling the application and analyzing the results are necessary steps prior to\nimplementing any changes to improve performance because we do not want to make\nchanges unless we know that it will be worth it. Once those steps are complete, though, we\nare ready for the development team to actually implement the changes based on the results\nof the previous steps.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 357", "position": 357, "chunk_type": "semantic", "token_estimate": 355}
{"text": "Performance Considerations: Chapter 10\n[ 329 ]\nThe analysis may identify multiple areas that need improvement. Software architects\nshould consider implementing one set of changes at a time, so as not to mix results and\nmake it more difficult to recognize new performance issues that may have been introduced. When selecting which set of changes to implement for a particular iteration, the most\nimportant bottleneck and the one that is expected to provide the biggest payoff should be\nprioritized. Monitoring results\nEven once changes have been implemented to improve performance, the process is not\ncomplete. We must monitor the results to determine whether the changes that were\nimplemented resolved the performance issues that were identified. When changes are implemented to fix a bottleneck, either the performance issue will\nremain unresolved, it will be fixed, or the bottleneck will be transferred to another part of\nthe system. If the issue is not fixed, one should consider whether it is appropriate to undo\nthe changes that were made. Software architects need to be aware that eliminating one\nbottleneck may reveal another one. We must monitor the results because we may need to\nconduct additional iterations of the performance improvement process. Even if our application is now performing in a satisfactory way, it must be monitored\nbecause things can change over time. As the source code changes with the introduction of\nnew features and bug fixes, new performance issues and bottlenecks may be created. Other\nchanges may also occur over time, such as a change in how many users the application has\nand how much traffic the application is generating. Server-side caching\nSoftware architects should take advantage of caching in order to improve performance and\nscalability. Caching involves copying data that may be needed again to fast storage so that\nit can be accessed quicker in subsequent uses. We will discuss HTTP caching and the use of\ncontent delivery networks in the Improving web application performance section later in this\nchapter. In this section, we will focus on server-side caching strategies. Server-side caches can be used to avoid making expensive data retrievals from the original\ndata store (for example, a relational database) repeatedly. The server-side cache should be\nplaced as close to the application as possible to minimize latency and improve response\ntimes.", "domains": ["Design Patterns", "Software Quality Attributes", "Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 358", "position": 358, "chunk_type": "semantic", "token_estimate": 380}
{"text": "Performance Considerations: Chapter 10\n[ 330 ]\nThe type of storage used for a server-side cache is designed to be fast, such as an in-\nmemory database. The more data and users that an application has to handle, the greater\nthe benefits of caching. Caching data in distributed applications\nIn distributed applications, there are two main types of data caching strategies that you can\nuse. One is the use of a private cache and the other is a shared cache. Keep in mind that you\ncan use both strategies in a single application. Some data can be stored in a private cache,\nwhile other data can be stored in a shared cache. Using a private caching strategy\nA private cache is held on the machine that is running the application that is using it. If\nmultiple instances of an application are running on the same machine, then each\napplication instance can have its own cache. One of the ways that data is stored in a private cache is in-memory, which makes it\nextremely fast. If there is a need to cache more data than can fit in the amount of memory\navailable on the machine, then cached data can be stored on the local file system. In a distributed system using the private caching strategy, each application instance will\nhave its own cache. This means that it is possible for the same query to yield different\nresults depending on the application instance. Using a shared caching strategy\nA shared cache is located in a separate location, possibly accessible through a cache service,\nand all application instances use the shared cache. This resolves the issue of different\napplication instances potentially having different views of cached data. It also improves\nscalability because a cluster of servers can be used for the cache. Application instances\nsimply interact with the cache service, which is responsible for locating the cached data in\nthe cluster. A shared cache is slower than a private cache because rather than being available on the\nsame machine as the application instance, it is located somewhere else; there will be some\nlatency involved in interacting with the cache. However, if a greater level of consistency\nwith data is important, the extra latency may be worth it.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 359", "position": 359, "chunk_type": "semantic", "token_estimate": 375}
{"text": "Performance Considerations: Chapter 10\n[ 331 ]\nPriming the cache\nSoftware architects should consider priming the cache. This means that an application pre-\npopulates the cache at application startup with data that will either be needed at startup, or\nis widely used enough that it makes sense to make the data available in the cache right\nfrom the start. This can help to improve performance as soon as initial requests are received\nby the server. Invalidating cached data\nPhil Karlton, while working at Netscape, once said:\n\"There are only two hard things in Computer Science: cache invalidation and naming\nthings.\" The joke is funny because there is truth to it. Data in a cache may become stale if it is\nchanged after it was placed in the cache. Cache invalidation is the process of replacing or\nremoving cached items. We must ensure that we are handling cached data properly so that\nstale data is replaced or removed. It may also be necessary to remove cached items if the\ncache becomes full. Expiring data\nWhen data is cached, we can configure the data to expire from the cache after a specified\namount of time. Some caching systems allow you to configure a system-wide expiration\npolicy in addition to an expiration policy for an individual cached item. The expiration is\ntypically specified as an absolute value (for example, 1 day). Evicting data\nA cache may become full, in which case the caching system must know which items it can\ndiscard in order to make room for new data. The following are some of the policies that can\nbe used to evict data:\nLeast recently used (LRU): Based on the assumption that cached items that have\nrecently been used are the most likely to be used again soon, this discards items\nthat were least recently used first. Most recently used (MRU): Based on the assumption that cached items that have\nbeen recently used will not be needed again, this discards items that were most\nrecently used first.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 360", "position": 360, "chunk_type": "semantic", "token_estimate": 334}
{"text": "Performance Considerations: Chapter 10\n[ 332 ]\nFirst-in, first-out (FIFO): Like a FIFO queue, this discards the item that was\nplaced in the cache first (oldest data). It does not take into consideration when\nthe cached data was last used. Last-in, first-out (LIFO): This approach is the opposite of FIFO in that it discards\nthe item that was placed in the cache most recently (newest data). It does not take\ninto consideration when the cached data was last used. Explicitly evicting data: There are times when we want to explicitly evict data\nfrom a cache, such as after existing data is deleted or updated. Cache usage patterns\nThere are two main ways that an application works with a cache. The application can either\nmaintain the cache data itself (known as a cache-aside pattern), including reading/writing\nto the database, or it can treat the cache as the system of record and the cache system can\nhandle reading/writing to the database (including read-through, write-through, and write-\nbehind patterns). Cache-aside pattern\nIn the cache-aside pattern, the application is responsible for maintaining the data in the\ncache. The cache is kept aside and it doesn't interact with the database directly. When values\nfrom the cache are requested by the application, the cache is checked first. If it exists in the\ncache, it is returned from there and the system-of-record is bypassed. If it does not exist in\nthe cache, the data is retrieved from the system-of-record, stored in the cache, and\nreturned. When data is written to the database, the application must handle potentially invalidated\ncached data and ensure that the cache is consistent with the system-of-record. Read-through pattern\nWith the read-through pattern, the cache is treated as the system-of-record and has a\ncomponent that is able to load data from the actual system-of-record (the database). When\nan application requests data, the cache system attempts to get it from the cache. If it does\nnot exist in the cache, it retrieves the data from the system-of-record, stores it in the cache,\nand returns it.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 361", "position": 361, "chunk_type": "semantic", "token_estimate": 340}
{"text": "Performance Considerations: Chapter 10\n[ 333 ]\nWrite-through pattern\nA caching system that uses the write-through pattern has a component that has the ability\nto write data to the system-of-record. The application treats the cache as the system-of-\nrecord and when it asks the caching system to write data, it writes the data to the system-\nof-record (the database) and updates the cache. Write-behind pattern\nThe write-behind pattern is sometimes used instead of the write-through pattern. They both\ntreat the cache as the system-of-record but the timing of the write to the system-of-record is\nslightly different. Unlike the write-through pattern, in which the thread waits for the write\nto the database to complete, the write-behind pattern queues the writing of the data to the\nsystem-of-record. The advantage of this approach is that the thread can move on quicker,\nbut it does mean that there is a short time when the data between the cache and the system-\nof-record will be inconsistent. Improving web application performance\nIn this section, we will look at techniques that can be used to improve the performance of\nweb applications. These techniques include, but are not limited, to:\nHTTP caching\nCompression\nMinification\nBundling\nHTML optimization\nHTTP/2\nContent delivery networks (CDNs)\nWeb font optimization\nLeveraging HTTP caching\nMany roundtrips between a client and a server may be necessary to load a page, and\nretrieving resources for that page from the server can take up significant amounts of time. The ability to cache resources that might be needed again so that they do not need to be\ntransferred over the network on subsequent trips is an important part of improving web\napplication performance.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 362", "position": 362, "chunk_type": "semantic", "token_estimate": 275}
{"text": "Performance Considerations: Chapter 10\n[ 334 ]\nBrowsers are capable of caching data so that it doesn't need to be fetched from a server\nagain. Resources such as CSS or JavaScript files might be shared across multiple pages of\nyour web application. As a user navigates to various pages, they will need these resources\nmultiple times. In addition, users may return to your web application at some point in the\nfuture. In both cases, taking advantage of HTTP caching will improve performance and\nbenefit the user. To take advantage of HTTP caching, each response from your web server must include the\nappropriate HTTP header directives. The cache policy you decide to implement is\nultimately dependent on the application's requirements and the type of data being served. Each resource may have different requirements related to caching that should be\nconsidered. Using the various header directives available to you will provide you with the\nflexibility to meet your requirements. Let's look at some of the header directives you can\nuse to control HTTP caching. Using a validation token\nA common scenario with HTTP caching occurs when a response has expired from the cache\nbut has not changed in any way. The client would be required to download the response\nagain, which is wasteful since the resource has not changed. A validation token in the ETag header of a response can be used to check whether an\nexpired resource has changed. Clients can send the validation token along with a request. If\na response has expired from the cache but the resource has not changed, there is no reason\nto download it again. The server will return a 304 Not Modified response and the browser\nwill then know that it can renew the response in the cache and use it. Specifying cache-control directives\nCache-control directives in a response can control whether the response should be cached,\nunder what conditions it can be cached, and for how long it should be cached. If a response\ncontains sensitive information that you do not want cached, a no-store cache-control\ndirective can be used, which will prevent browsers as well as any intermediate caches (for\nexample, a content delivery network), from caching the response. Alternatively, a response\ncan be marked as private, which will allow caching in a user's browser but not in any\nintermediate caches.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 363", "position": 363, "chunk_type": "semantic", "token_estimate": 389}
{"text": "Performance Considerations: Chapter 10\n[ 335 ]\nA cache-control directive that is slightly different than no-store is the no-cache directive. It is\nused to specify that a response should not be used from the cache until a check is\nperformed with the server first to see whether the response has changed. The validation\ntoken must be used to make this determination, and only if the resource has not changed\ncan the cache be used. The max-age directive is used to specify the maximum amount of time, in seconds, that a\nresponse can be reused from the cache. The value is relative to the time of the request. You may find yourself in a situation where you want to invalidate a cached response even\nthough it has not expired yet. Once a response is cached, it will continue to be used unless it\nexpires or the browser's cache has been cleared in some way. However, there may be times\nwhen you want to change a response before it has expired. This can be accomplished by\nchanging the URL of the resource, which will force it to be downloaded. A version number,\nor some other identifier such as a fingerprint of the file, can be included as part of the\nfilename. Using this technique provides differentiation between different versions of the\nsame resource. Taking advantage of compression\nCompression is an important technique for improving performance. It is the use of an\nalgorithm to remove redundancy in a file in order to make it smaller. This improves\ntransfer speed and bandwidth utilization. Software developers do not need to programmatically compress data that is to be\ntransmitted. Servers and browsers have compression implemented already. As long as both\nthe server and the browser understand the compression algorithm, it can be used. It is just a\nmatter of ensuring that the server is configured properly. The two main types of compression that are used to improve web performance are file\ncompression and content-encoding (end-to-end) compression. File compression\nFiles that you transmit, such as images, video, or audio, can have high rates of redundancy. When a web page is downloaded, images might account for the majority of the bytes being\ndownloaded. These types of files should be compressed to save storage space and increase\ntransfer speed.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 364", "position": 364, "chunk_type": "semantic", "token_estimate": 380}
{"text": "Performance Considerations: Chapter 10\n[ 336 ]\nThere are various tools and algorithms that can be used to compress different file formats. Among the choices you can make, depending on your needs, is whether to use a lossless or\na lossy compression algorithm. Lossless compression\nWith lossless compression, all of the bytes from the original file can be recovered when the\nfile is decompressed. This type of compression algorithm may be necessary if the file you\nare compressing cannot afford to lose any information. For example, if you are compressing a text file of data, source code for a program, or an\nexecutable file, you cannot afford to lose any of the contents and would want to use a\nlossless compression algorithm. For image, video, and audio files, you may or may not\nrequire lossless compression, depending on your needs for file size and quality. Graphics Interchange File (GIF) and Portable Network Graphics (PNG) are examples of\nimage file formats that provide lossless compression. If animation is required, you will\nwant to use the GIF format. If you want to preserve high-quality images and not lose any\nfine detail, the PNG image format should be used. Lossy compression\nIf a lossy compression algorithm is used, some of the bytes from the original file will be lost\nwhen the file is decompressed. For a file in which you can afford to lose some bytes, you\ncan achieve a smaller file size as compared to using a lossless compression algorithm. This type of compression works well with images, video, and audio because the loss of\nredundant information may be acceptable. There are different degrees of lossy\ncompression, and how aggressive you are with the optimization depends on the trade-off\nyou are willing to make between file size and quality. In some cases, you can use lossy\ncompression and there will be no perceptible difference to the user. Joint Photographic Experts Group (JPEG) is an example of an image file format that\nprovides lossy compression. If you do not need the highest quality image and can afford to\nlose some fine detail in the image, JPEG can be used.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 365", "position": 365, "chunk_type": "semantic", "token_estimate": 356}
{"text": "Performance Considerations: Chapter 10\n[ 337 ]\nContent-encoding (end-to-end) compression\nSignificant performance improvements can be made when using content-encoding. The\nserver compresses the body of an HTTP message prior to sending it to the client. It will\nremain compressed (end-to-end compression) until it reaches the client. Any intermediate\nnodes that it may pass through while traveling to the client do not decompress the\nmessage. Once the message reaches the client, the client decompresses the body of the\nHTTP message. In order to use content-encoding, the browser and server must agree on the compression\nalgorithm to use via content negotiation. Content negotiation is the process of selecting the\nbest representation of particular content. There are a number of different types of compression, but gzip is the most common. It is a\nlossless type of compression. Although it can be used for any stream of bytes, it works\nparticularly well on text. Brotli (content-encoding type of br) is an open source, lossless\ndata compression library. It is newer than gzip, but it is gaining support and popularity. You should take advantage of content-encoding as much as possible, except when\ntransferring files that have already been compressed with the aforementioned file\ncompression, such as image, video, and audio files. This is because you will typically not\ngain anything by compressing something twice, and it could even lead to a file size that is\nslightly larger than if it is just compressed once. Minifying resources\nMinification is the process of removing all unnecessary or redundant data from a resource. It can be used to remove characters from source code that are not needed without changing\nany of the functionality. Files such as JavaScript, HTML, and CSS are great candidates for minification. Although\nthe minified files that result from the process are not as human-readable as their original\ncounterparts, the file size will be smaller, resulting in faster load times. For example, let's take the following JavaScript code:\n// Class representing a rectangle\nclass Rectangle {\n    constructor(height, width) {\n        this.height = height;\n        this.width = width;\n    }", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 366", "position": 366, "chunk_type": "semantic", "token_estimate": 341}
{"text": "Performance Considerations: Chapter 10\n[ 338 ]\n    // Method to calculate area\n    calculateArea() {\n        return this.width * this.height;\n    }\n}\nAfter minifying it, it appears as follows:\nclass\nRectangle{constructor(t,h){this.height=t;this.width=h}calculateArea(){retur\nn this.width*this.height}}\nYou can see that unnecessary characters for formatting and code comments have been\nremoved and the names of the constructor parameters have been shortened. There are a\nnumber of tools available to you that can minify files. Some of the tools focus on a\nparticular type of file (for example, JavaScript, HTML, or CSS). It is best to minify files prior\nto using the compression technique discussed in the Taking advantage of compression section. It is a good practice to keep two versions of code files that are minified: a version that has\nnot been minified for debugging purposes and a minified version for deployment. They can\nbe given different filenames so that it is clear which one is the minified version. For\nexample, invoice.min.js could be used as the name for the minified version of\ninvoice.js. Bundling resources\nThe first step in reducing the number of HTTP requests that need to be made in order to\nload a page is to remove all unnecessary resources. Once that is done, most web pages will\nstill require multiple files of the same type, such as JavaScript or CSS files, in order to load. During development, it makes sense to separate this type of code into multiple files. However, the use of more individual files translates into more HTTP requests. Bundling is the process of combining multiple files of the same type into a single file,\nwhich can then be transferred in a single request. The technique of bundling files is\nsometimes referred to as concatenating files. Fewer HTTP requests lead to faster page load\nperformance. Bundling is a technique that is complementary with minifying files and the\ntwo are often used in conjunction with each other. Bundling is an effective technique when we are using HTTP/1.1. In order to understand the\nreasons behind that, let's examine how assets are sent. In order to load a web page, the\nbrowser has to load each of the files that it needs, one at a time, over a connection, as shown\nin  the following diagram:", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 367", "position": 367, "chunk_type": "semantic", "token_estimate": 372}
{"text": "Performance Considerations: Chapter 10\n[ 339 ]\nThis process is too slow, so in order to get around this issue, browsers open up multiple\nconnections per host, shown as follows:\nThe maximum number of simultaneous connections that can be made per host varies by\nbrowser but a common number is six. The browser will handle these connections so\napplication developers do not need to make any modifications to their application to take\nadvantage of that feature. There is some overhead involved with setting up each connection\nbut it is worth it in order to have multiple connections available for communication.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 368", "position": 368, "chunk_type": "semantic", "token_estimate": 100}
{"text": "Performance Considerations: Chapter 10\n[ 340 ]\nWe can improve this approach by bundling the files so that fewer HTTP requests, and\npossibly fewer connections, will be necessary to get all of the required assets. In the\nfollowing diagram, all of the CSS files are bundled together in styles.css, and all of the\nJavaScript files are bundled together in scripts.js:\nA downside of caching is that it can cause the cache to be invalidated more frequently. Without bundling, we can control the caching of each individual file. Once we start\nbundling, if any of the files within the bundle have changed, the entire bundle will need to\nbe downloaded to clients again. If a file that is changed is contained within more than one\nbundle, it could cause multiple bundles to be downloaded again. Even with the use of bundling, the number of assets that a web page needs may be higher\nthan the maximum number of connections. This means that additional requests for assets\nfrom the same host are queued by the browser and will have to wait until a connection\nbecomes available. To work around this limitation, the technique of domain sharding was\nintroduced. If there is a limit to the number of connections per domain, a workaround is to introduce\nadditional domains. Domain sharding is a technique in which resources are split among\nmultiple domains, allowing more to be downloaded in parallel. Instead of using the same\ndomain (for example, www.example.com), we can use multiple subdomains\n(shard1.example.com, shard2.example.com, and so on). Each shard is allowed the\nmaximum number of connections, increasing overall parallelism and allowing more assets\nto be transferred at the same time. In the following diagram, we are only returning three\nassets, which doesn't exceed the maximum number of connections per host. However, if we\ndid need more assets, we could retrieve more before queuing by the browser would\nbecome necessary:", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 369", "position": 369, "chunk_type": "semantic", "token_estimate": 316}
{"text": "Performance Considerations: Chapter 10\n[ 341 ]\nThere is overhead involved with adding shards, though, such as additional DNS lookups,\nthe additional resources required on both ends, and the fact that application developers\nwill need to manage how to split up their resources. Let's take a look at HTTP/2 now and learn how it can improve performance. Differences\nbetween HTTP/1.x and HTTP/2 affect how we want to approach techniques such as\nbundling and domain sharding. Using HTTP/2\nHTTP/2 is the latest version of the application layer protocol for data communication. It is\nnot a complete rewrite of the protocol. The HTTP status codes, verbs, methods, and most of\nthe headers that you are already familiar with from using HTTP/1.1 will continue to be the\nsame. One difference between HTTP/2 and HTTP/1.1 is the fact that HTTP/2 is binary, whereas\nHTTP/1.1 is textual. HTTP/2 communication consists of binary-encoded messages and\nframes, making it more compact, efficient to parse, and less error-prone. The fact that\nHTTP/2 is binary is what enables some of the other HTTP/2 features that improve\nperformance.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 370", "position": 370, "chunk_type": "semantic", "token_estimate": 178}
{"text": "Performance Considerations: Chapter 10\n[ 342 ]\nMultiplexing\nOne of the most important features of HTTP/2 is multiplexing. Multiplexing is the ability to\nsend multiple HTTP requests and receive multiple HTTP responses asynchronously\nthrough a single TCP connection, as shown in the following diagram:\nHTTP/1.1 does not support multiplexing, which is what led to the various workarounds to\nimprove performance. With HTTP/2, we should no longer be concatenating files into a\nsmall number of large bundles. The expensive cache invalidation of a bundle that is\nnecessary when any of the files in the bundle are changed can now be avoided or\nminimized. We can now transfer more granular assets, either by not bundling at all or\nhaving an increased number of bundles, where each one contains a small number of related\nfiles rather than just a few bundles containing many. A more granular approach allows us\nto provide an optimal cache policy for each individual file or bundle, maximizing the\namount of content that is coming from our cache. Another technique that is no longer necessary with HTTP/2 is domain sharding. Through\nthe use of multiplexing, we can download multiple assets at the same time using a single\nconnection. The overhead for each shard, which may have been worth it with HTTP/1.1, is\nno longer necessary. Server push\nHTTP/2 provides a feature in which the server can push responses that it thinks a client will\nneed. When a resource is requested from a client, it may contain references to other\nresources that are needed. Rather than wait for the client to send additional requests for\nthese required resources, the server already knows what resources will be needed and can\nproactively send them.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 371", "position": 371, "chunk_type": "semantic", "token_estimate": 282}
{"text": "Performance Considerations: Chapter 10\n[ 343 ]\nThis feature is similar to inlining a resource, which is a technique that is sometimes used to\nimprove performance by reducing the number of requests necessary. Inlining is\naccomplished by embedding a resource, such as JavaScript, CSS, or images, into an HTML\npage. With server push, there is no longer a need to inline resources. We get the same\nadvantages of inlining but with the added benefit of keeping the assets in separate files,\neach with their own caching policies. There are a few caveats with the server push feature. You should be careful not to push too\nmany assets at once; you do not want to delay rendering of the page and negatively affect\nperceived performance. You should put thought into what assets you are pushing and be\nselective about it. If server push is not used properly, resources that a client already has could be transferred\nto it unnecessarily, which would actually hurt performance. As of today, use of the server\npush feature may require some experimentation in order for it to be used in an optimal\nway. Some web servers have the functionality to mitigate the problem of pushing assets\nthat the client does not need, and some browsers may introduce a cache digest so that a\nclient can let the server know what assets it already has in its local cache. Header compression\nHTTP/2 performs header compression to improve performance. With HTTP/1.x, headers\nare always sent as plain text, but HTTP/2 uses the HPACK compression format to reduce\nthe size. HPACK is used for compression with HTTP/2 because it is resilient to some of the\nsecurity attacks that target compression, such as Compression Ratio Info-leak Made\nEasy (CRIME). HPACK uses Huffman coding, a lossless data compression algorithm that can significantly\nreduce the size of the headers, reducing latency and improving performance. Implementing HTTP/2\nIn order for a client to use HTTP/2, all that is needed is that the browser must support it. The latest versions of most browsers support HTTP/2. On the server side, the steps required\nto configure HTTP/2 support varies. A number of web servers provide support for HTTP/2\nand the list continues to increase. In most cases, servers will need to support both HTTP/1.1\nand HTTP/2, so typically a server needs to be configured to fall back to HTTP/1.1 if a client\ndoes not support HTTP/2.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 372", "position": 372, "chunk_type": "semantic", "token_estimate": 400}
{"text": "Performance Considerations: Chapter 10\n[ 344 ]\nUsing content delivery networks (CDNs)\nUsers visiting a website may potentially be located anywhere in the world. Round trips\nbetween clients and servers will take longer if the distance between them is greater. The\nadditional latency may be just milliseconds but it all contributes to the overall time it takes\nto receive a response. Content delivery networks (CDNs) are a geographically distributed group of servers that\ncan deliver content to users quickly. The nodes of a CDN are deployed in multiple locations\nso that they are distributed spatially. This provides us with the ability to reduce network\nlatency and serve up content closer to the end users, which improves load times. CDNs are great for transferring website content, such as JavaScript, HTML, CSS, image,\nvideo, and audio files. In addition to reducing the physical distance between users and\ncontent, CDNs improve load times through efficient load balancing, caching, minification,\nand file compression. The reliability and redundancy of web applications are increased when a CDN is used\nbecause when traffic increases, it can be load balanced among multiple servers. If a server,\nor even an entire data center, is having technical issues, traffic can be routed to servers that\nare operational. CDNs can also help to improve security by mitigating distributed denial-\nof-service (DDoS) attacks and maintaining up-to-date TLS/SSL certificates. Optimizing web fonts\nGood typography is an important part of designing a good user interface, readability,\naccessibility, and branding. There was a time when web designers were limited in the fonts\nthey could use because there were only so many fonts that were guaranteed to be available\non all systems. These were known as web safe fonts. It is possible to use fonts other than just the web safe fonts. For instance, in CSS, you can use\nthe font-family property to specify a list of fonts that can be used for an element, as\nfollows:\np {\n  font-family: Helvetica, Arial, Verdana, sans-serif;\n}\nWith this approach, the browser will use the first one that it finds available on the\nsystem. A disadvantage of using this approach is that during testing you have to ensure\nthat all of the fonts will work properly with your application.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 373", "position": 373, "chunk_type": "semantic", "token_estimate": 371}
{"text": "Performance Considerations: Chapter 10\n[ 345 ]\nA CSS feature called web fonts was introduced to overcome some of the challenges. It\nprovides you with the ability to download font files so that any browser that supports web\nfonts can make the fonts that you want to use for your page available. Text using web fonts\nis selectable, searchable, zoomable, and will look good in a variety of screen sizes and\nresolutions. However, the use of web fonts means that additional resources must be loaded. If a website\nor web application is using web fonts, it is important to consider them as part of your\noverall web performance strategy. Optimization of web fonts can reduce the overall size of\na page and decrease rendering times. One thing that you should do is minimize the number\nof fonts (and their variants) that you use on your pages to minimize the number of\nresources that are needed. To use web fonts, you must first select the one or more fonts that you want to use and\nconsider the character sets that you need to support based on any localization\nrequirements. The size of a font file is dependent on the complexity of the shapes in the\nvarious characters that make up the font. Unfortunately, there is no standard on font formats, which means that different browsers\nsupport different font formats. This lack of a standard means that as of right now, you will\nneed to support four different font formats for each font, and they are as follows:\nWeb Open Font Format version 2 (WOFF 2.0)\nWeb Open Font Format version 1 (WOFF)\nTrueType font (TTF)\nEmbedded Open Type (EOT)\nOnce the web fonts have been selected, the @font-face CSS rule allows you to use a web\nfont by allowing you to specify the font and the URL location where the font data can be\nfound. Regardless of which web font you select and which of the four font formats is being\nused by a particular user, compression is effective in reducing the font size and should be\nused to improve performance. WOFF 2.0 and WOFF have built-in compression, but the TTF\nand EOT formats are not compressed by default, so servers should use compression when\ndelivering these formats. Web fonts can be large Unicode fonts with support for a variety of characters, not all of\nwhich will be needed at a given time.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 374", "position": 374, "chunk_type": "semantic", "token_estimate": 402}
{"text": "Performance Considerations: WOFF 2.0 and WOFF have built-in compression, but the TTF\nand EOT formats are not compressed by default, so servers should use compression when\ndelivering these formats. Web fonts can be large Unicode fonts with support for a variety of characters, not all of\nwhich will be needed at a given time. The unicode-range property in @font-face can\nbe used to split up a font into multiple subsets so that only the characters that are actually\nneeded will be downloaded.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 374", "position": 374, "chunk_type": "semantic", "token_estimate": 82}
{"text": "Performance Considerations: Chapter 10\n[ 346 ]\nOne more note about optimizing fonts is to keep in mind the fact that font resources are not\nupdated frequently. You should ensure that this type of resource is cached with a caching\npolicy that will allow them to live in the cache for a long period of time, with a validation\ntoken so that even once it expires, it can be renewed in the cache and not downloaded again\nas long as it has not changed. Optimizing the critical rendering path\nAn important step in reducing the amount of time that it takes for a page to render is to\noptimize the critical rendering path (CRP). The critical rendering path is the set of steps in \nbetween a browser receiving bytes from a server (for example, HTML, CSS, and JavaScript\nfiles) and the processing involved to render pixels on the device's screen. Before a page can be rendered by a browser, it must construct both the Document Object\nModel (DOM) and the CSS Object Model (CSSOM). The HTML and CSS markup for the\npage are needed for this process. The DOM and the CSSOM are then combined to form a\nrender tree, which has both the content as well as the style information for what will be\nvisible on the screen. Once the render tree has been constructed, the browser moves to the layout stage where it\ncalculates the size and position of the various visible elements. Finally, the paint stage is\nreached, where the browser uses the results of the layout to paint pixels to the screen. Optimizing the critical rendering path is the process of minimizing the time it takes to\nperform these various steps. We are mostly concerned with the portion of the page that is\nabove the fold, which refers to the part of the page that is visible without scrolling. The term\nis a reference to the upper half, or visible portion, of a folded newspaper. Until the user\nscrolls down a page, which may not even occur, they will not see what is below the fold. We want to prevent render blocking by reducing, as much as possible, the resources that\nwill prevent the content above the fold from rendering. The initial step in this process is to\ndetermine what resources are truly necessary for the initial rendering of a page.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 375", "position": 375, "chunk_type": "semantic", "token_estimate": 395}
{"text": "Performance Considerations: We want to prevent render blocking by reducing, as much as possible, the resources that\nwill prevent the content above the fold from rendering. The initial step in this process is to\ndetermine what resources are truly necessary for the initial rendering of a page. We want to\nget these critical resources to the client as quickly as possible to speed up the initial\nrendering. For example, the HTML and CSS that are necessary to create the DOM and the\nCSSOM are render-blocking resources, so we want to get them to the client quickly. Using\nsome of the techniques described previously for web application performance, such as\ncompression and caching, can help to load critical resources faster.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 375", "position": 375, "chunk_type": "semantic", "token_estimate": 119}
{"text": "Performance Considerations: Chapter 10\n[ 347 ]\nResources that are not required for the above-the-fold content or are otherwise not critical\nfor the initial rendering can either be eliminated, their download could be deferred, or they \ncan be loaded asynchronously. For example, if there are image files that are not needed for\ninitial rendering, they can be deferred, and in order to prevent DOM construction any\nblocking JavaScript files can be loaded asynchronously. Understanding what is involved in rendering a page, taking the time to think about what\nthe critical resources are for the initial rendering, and optimizing the critical rendering path\nwill allow a page to be constructed faster. Making a web page almost immediately visible\nand usable greatly improves a user's overall experience, which reduces the bounce rate and\nincreases the conversion rate. Database performance\nA key part of a software system is the database. So, when looking to improve the\nperformance of a system, improving database performance must be part of that effort. In\nthis section, we will take a look at some of the things that can be done to improve database\nperformance. Designing an efficient database schema\nThe foundation of achieving peak database performance is an efficient and properly\ndesigned database schema. As a software architect, you may be working with a DBA who\nwill be responsible for database design. However, it is good to be familiar with the different\naspects of achieving a good database design. Normalizing a database\nNormalization is the process of designing tables (relations) and columns (attributes) so that\nthey not only meet data requirements, but minimize data redundancy and increase data\nintegrity. In order to meet these goals, a database should contain the minimal number of attributes\nnecessary to meet the requirements. Attributes with a close logical relationship should be\nplaced together in the same relation. Redundancy of attributes should be kept to a\nminimum, which makes it easier to maintain data consistency and will minimize the size of\nthe database.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 376", "position": 376, "chunk_type": "semantic", "token_estimate": 331}
{"text": "Performance Considerations: Chapter 10\n[ 348 ]\nDenormalizing a database\nFor performance and scalability reasons, there may be cases where it makes sense to\ndenormalize part of the database. It is important to differentiate between a database that\nhas not been normalized, and one that has been normalized and then is denormalized later. A database should be normalized first and then if there are cases where it makes sense to\ndenormalize, it should be done after careful consideration. Denormalization is a strategy used to improve performance, typically by shortening the\nexecution time of certain queries. This can be accomplished by storing redundant copies of\nsome data or grouping data together in order to minimize joins and improve query\nperformance. While denormalization might improve read performance, it will negatively affect write\nperformance. Some mechanisms, such as a compensating action, will be required to keep\nredundant data consistent. Using database constraints can help to enforce rules that will\nkeep the data consistent even when it is denormalized. Redundant data will also make the\ndatabase larger and therefore take up more disk space. Another reason to introduce denormalization is to keep historical data. For example, let's\nsay that we have an address table and an orders table, and that each order is associated\nwith an address. An order is created, and then in the future, that address is updated. Now\nwhen you look at the old order, you see the new address and not the address given at the\ntime the order was created. By storing the address field values with each order record, you\ncan maintain this historical data. It should be noted, however, that there are ways to\naccomplish this without denormalization. For instance, you could treat an address like a\nvalue object (immutable), and simply create a new address record when one is modified,\nleaving the one associated with the old address intact. Also, in an event-driven system, or\none in which a data audit is being kept that stores the modifications that have been made to\na record, you could reconstruct what the address was at the time the order was created. Identifying primary and foreign keys\nAll of the primary and foreign keys for all of the tables in the database should be identified. The primary key of a table is the column, or combination of columns, that uniquely identify a\nrow in the table.", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Software Quality Attributes", "Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 377", "position": 377, "chunk_type": "semantic", "token_estimate": 397}
{"text": "Performance Considerations: Identifying primary and foreign keys\nAll of the primary and foreign keys for all of the tables in the database should be identified. The primary key of a table is the column, or combination of columns, that uniquely identify a\nrow in the table. Sometimes a row in a table must reference a row from another table. A foreign key is a\ncolumn or combination of columns that hold the primary key value for a row in another\ntable so that it can be referenced.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 377", "position": 377, "chunk_type": "semantic", "token_estimate": 87}
{"text": "Performance Considerations: Chapter 10\n[ 349 ]\nDatabase constraints based on the primary and foreign keys should be created to enforce\ndata integrity. A primary key constraint for a table consists of the one or more columns that\nmake up the primary key and a foreign key constraint consists of the one or more columns\nthat make up the foreign key. Selecting the most appropriate data types\nWhen designing a database table, we should select the most appropriate data type for each\ncolumn. In addition to the data type, the size and nullability of the column should also be\nconsidered. We want to choose a data type that will sufficiently hold all possible values but\nalso be the smallest data type that is necessary. This will maximize efficiency not just for\nperformance but also for storage size. Using database indexes\nDatabase indexes can be used to improve performance and provide more efficient data\naccess and storage. They are stored on disk and associated with a database table or view to\nspeed up data retrieval. The two main types of indexes are the primary/clustered index and\nthe secondary/non-clustered index. Primary/clustered indexes\nWhen designing a table, one approach is to keep the rows unordered and create as many\nsecondary indexes as necessary. Such an unordered structure is known as a heap. Another \napproach is to create a primary index, also known as a clustered index, to order the rows by\nthe primary key. It is common to have a clustered index on a table which is known as a\nclustered table. The only time you might not want one is if the table is very small (and one\nthat you know will remain small over time), such that the overhead of storing and\nmaintaining the index is not worth it when compared to simply searching the table. The one or more columns that make up the primary key of a table are the columns that\nmake up the index definition. A clustered index sorts the rows in a table based on their key\nvalues and physically stores them on disk based on that order. Each table can only have one\nclustered index because the rows in the table can only be sorted and stored in one order.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 378", "position": 378, "chunk_type": "semantic", "token_estimate": 375}
{"text": "Performance Considerations: Chapter 10\n[ 350 ]\nSecondary/non-clustered indexes\nIn addition to specifying a primary index, many database systems provide the ability to\ncreate secondary indexes, also known as non-clustered indexes. The performance of a database\ncan benefit from having secondary keys available for data access. Non-clustered indexes are defined by one or more columns that are ordered logically and\nserve as pointers to find the rest of the data for a given record. The order of a non-clustered\nindex does not match the physical order of how the records are stored on disk. Non-clustered indexes provide a way to specify an alternate key other than the primary key\nfor accessing records in a table. The key could be a foreign key or any column that will be\nfrequently used in joins, where clauses, ordering, or grouping. The advantage of using non-\nclustered indexes is to improve performance for the common ways that data might be\naccessed for a particular table beyond the primary key. Sometimes the primary key is not\nthe only way, and might not even be the most widely-used way, that records are retrieved\nfrom a table. For example, let's say we have an Order table and a Customer table, with OrderId and\nCustomerId being the primary keys of those two tables, respectively. The Order table also\nhas a CustomerId column as a foreign key to the Customer table in order to associate\norders with customers. In addition to retrieving orders by OrderId, the system may need\nto retrieve orders by CustomerId on a frequent basis. Adding a non-clustered index on the\nCustomerId column of the Order table allows for more efficient data retrieval when it is\naccessed with that column. Although non-clustered indexes are an important part of performance tuning a database,\neach table should be analyzed carefully when deciding which non-clustered indexes, if any,\nto add to a table and which column or columns should make up the index. As we will learn\nin a moment, there is overhead related to adding indexes to a table, so we do not want to\nadd any unnecessary ones. Having too many indexes\nThere is a cost associated with adding an index to a table, so when it comes to indexes, you\ncan have too much of a good thing.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 379", "position": 379, "chunk_type": "semantic", "token_estimate": 382}
{"text": "Performance Considerations: As we will learn\nin a moment, there is overhead related to adding indexes to a table, so we do not want to\nadd any unnecessary ones. Having too many indexes\nThere is a cost associated with adding an index to a table, so when it comes to indexes, you\ncan have too much of a good thing. Every time that a record is added or updated in a table,\nan index record also has to be added or updated, incurring some additional overhead to\nthose transactions. In terms of storage, indexes take up additional disk space, increasing the\noverall size of your database.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 379", "position": 379, "chunk_type": "semantic", "token_estimate": 106}
{"text": "Performance Considerations: Chapter 10\n[ 351 ]\nThe more indexes you have on a table, the more the query optimizer of the database\nmanagement system (DBMS) will have to take into consideration for a particular query. The query optimizer is a component of the DBMS that analyzes queries to determine the\nmost efficient execution plans for them. For example, the query optimizer decides whether a particular join in a query should do a\nfull table scan versus using an index. It must take into account, among other things, all of\nthe indexes on that table. As a result, an increased number of indexes on a table could\nadversely affect performance. For these reasons, one should be selective when considering what indexes to add to a table. Properly selected indexes will speed up data access performance, but you do not want to\ncreate unnecessary ones as they can slow down data access and operations, such as inserts\nand updates. Scaling up and out\nScaling your database server vertically (up) or horizontally (out) to improve performance is\nnot something that should be done without an understanding that it is necessary. Prior to\nscaling up or scaling out, software architects and DBAs should ensure that the database\nschema has been designed properly and that indexes have been applied properly. In\naddition, the application using the database should be optimized to improve performance\nand remove bottlenecks. Once those measures have been taken, if the database server is experiencing high levels of\nresource use, it's time to consider scaling the server up or scaling it out. For database\nservers, it is best to scale up first by performing actions such as replacing the server with a\nbetter machine or by adding processors and/or memory. Scaling up should be done first because there are additional complications with scaling a\ndatabase server out. When you have multiple servers, you may need to horizontally\npartition some of the tables and consider data replication. Plans for disaster recovery and\nfailover are also more complex when there are multiple database servers. However, if\ndatabase performance is still not at the level you need it to be after scaling up, scaling out\nmay be necessary.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 380", "position": 380, "chunk_type": "semantic", "token_estimate": 364}
{"text": "Performance Considerations: Chapter 10\n[ 352 ]\nDatabase concurrency\nA relational database system can handle many simultaneous connections. Having a\ndatabase that performs well is only useful if it can handle multiple processes accessing and\nchanging data at the same time. This is what database concurrency is all about. Concurrency control ensures that database transactions that are performed concurrently\nmaintain data integrity. We'll now begin looking at concurrency by learning about database\ntransactions. Database transactions\nA database transaction is a sequence of operations that are performed as a single unit of\nwork. They play an important role in maintaining data integrity and consistency even when\ndata is being accessed and changed at the same time. They provide units of work that either\ncomplete in their entirety or will not be committed at all. Transactions can recover from failures and keep the database in a consistent state. Transactions also provide isolation so that a record that is in the process of being modified\nby one transaction is not affected by a concurrent transaction that must update the same\nrecord. Transactions, once they are complete, are written to durable storage. Optimistic versus pessimistic concurrency control\nConcurrency control ensures that databases transactions that are performed concurrently\nmaintain data integrity. Many databases offer two main types of concurrency control:\noptimistic and pessimistic. Optimistic concurrency control (or optimistic locking) works under the assumption that\nresource conflicts between multiple users, while possible, are not common. Therefore, it\nallows transactions to execute without locking resources. If data is being changed, resources\nare checked for conflicts. If there is a conflict, only one transaction is successful while the\nothers fail.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 381", "position": 381, "chunk_type": "semantic", "token_estimate": 272}
{"text": "Performance Considerations: Chapter 10\n[ 353 ]\nIn contrast, pessimistic concurrency (or pessimistic locking) assumes the worst, such as\nassuming that more than one user will want to update the same record at the same time. In\norder to prevent that, it locks the appropriate resources as they are required for the\nduration of the transaction. Unless a deadlock takes place, pessimistic concurrency ensures\nthat a transaction will be completed successfully. It should be noted that most database\nsystems have different types of locks. For example, one type of lock might specify that a\nrecord that is locked can still be read by another user, while another type of lock might\nprevent that type of read. CAP theorem\nThe Consistency, Availability, and Partition tolerance (CAP) theorem, also known as\nBrewer's Theorem after Eric Brewer who published it, states that a distributed system can\nonly achieve two of the following three guarantees, but not all three:\nConsistency: Every read either returns the latest data or an error. Every\ntransaction either completes successfully and is committed or is rolled back due\nto a failure. Availability: A system always provides a response to every request. Partition tolerance: In a distributed system (data is partitioned to different\nservers), if one of the nodes fails, the system should still be able to function. Databases will stress some of these guarantees over others. A traditional relational database\nmanagement system will focus on consistency and availability. They will favor strong\nconsistency, which is also known as immediate consistency, so that any read of the data\nwill reflect any changes that have been made to that data. These types of databases will\nfollow an ACID consistency model. Some databases, such as some NoSQL databases, will value availability and partition\ntolerance over consistency. For such databases, eventual consistency, rather than strong\nconsistency, is acceptable. Eventually, the data will reflect all of the changes made to it, but\nat any given point in time it is possible to read data that may not reflect the latest changes. These types of databases follow a BASE consistency model.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 382", "position": 382, "chunk_type": "semantic", "token_estimate": 346}
{"text": "Performance Considerations: Chapter 10\n[ 354 ]\nACID model\nDatabases that want to ensure consistency and availability will follow the ACID\nconsistency model. Traditional relational databases follow the ACID model. Database \ntransactions adhere to the ACID properties in that they must be atomic, consistent, isolated,\nand durable. These properties guarantee the validity of the data even when errors or\nfailures occur. Strong consistency will place limits on performance and scalability. Atomicity\nA transaction must be an atomic unit of work, meaning that either all of its data\nmodifications are performed or none at all. This provides reliability because if there is\nfailure in the middle of a transaction, none of the changes in that transaction will be\ncommitted. For example, in a financial transaction, you may insert one record to represent\nthe credit part of the transaction and another to represent the debit part of the transaction. You don't want one of those inserts to take place without the other, so you place them both\nas part of one transaction. Either they will both be committed or neither of them will be\ncommitted. Consistency\nAfter a transaction takes place, all of the data must be in a consistent state. This property\nensures that all transactions maintain data integrity constraints, leaving the data consistent. If a transaction leaves data in an invalid state, the transaction is aborted and an error is\nreported. For example, if you had a column with a check constraint that states a column\nvalue must be greater than or equal to zero (so as not to allow negative numbers), the\ntransaction would fail if it attempted to insert or update a record with a value less than zero\nfor that particular column. Isolation\nChanges made by concurrent transactions must be isolated from changes made by any\nother concurrent transactions. Many DBMSs have different isolation levels that control the\ndegree to which locking occurs on data being accessed. For example, a DBMS may place a\nlock on a record being updated so that another transaction cannot update that same record\nat the same time.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 383", "position": 383, "chunk_type": "semantic", "token_estimate": 346}
{"text": "Performance Considerations: Chapter 10\n[ 355 ]\nDurability\nOnce a transaction completes and is committed, its changes are persisted permanently in\nthe database. For example, a DBMS may implement durability by writing all transactions to\na transaction log. The transaction log can be used to recreate the system state at any point,\nsuch as right before failure. BASE model\nSome databases, such as some distributed NoSQL databases, focus on availability and\npartition tolerance. In some situations, it may be an acceptable tradeoff to have eventual\nconsistency, rather than strong consistency, in order to focus on partition tolerance,\nperformance, and scalability. This approach enables a higher degree of scalability and can\nyield faster performance. These databases use the BASE consistency model instead of the\nACID model. Basic availability\nMost of the time, conflicts do not take place. The database is available most of the time and\na response will be sent for every request. However, conflicts can occur and the response\nmay indicate that a failure occurred when trying to access or change data. Soft state\nRather than following consistency requirements such as those in the ACID model, the\nconcept here is that the state of the system could change over time. Even if no additional\ntransactions are being created, changes could take place due to eventual consistency. Eventual consistency\nA data change will eventually propagate to everywhere that it needs to go. If there are no\nfurther changes to a piece of data, eventually the data will be in a consistent state. This\nmeans it is possible to read stale data if the latest updates to it have not been applied yet. Unlike a strong consistency model in which all data changes are atomic and the transaction\nis not allowed to complete until either the change finishes successfully or is rolled back due\nto a failure, the system will not check the consistency of every transaction.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 384", "position": 384, "chunk_type": "semantic", "token_estimate": 316}
{"text": "Performance Considerations: Chapter 10\n[ 356 ]\nSummary\nIt is more important to ensure that your code is correct than fast. Fast performance is not of\nany use if the application does not yield the correct results. Having said that, performance\nis an important part of designing and developing a successful software application. It plays\na large part in the overall user experience for people who use the application. Regardless of\nthe device they are using or their location, users expect a high level of responsiveness from\ntheir applications. Performance is a quality attribute of the software system and performance requirements\nshould be documented. Like all requirements, they need to be measurable and testable. The\nentire team should take ownership of performance. A systematic, iterative approach to\nperformance improvement can help a development team reach their performance goals. Some problems will only be discovered later, so development teams should be prepared to\nanalyze and optimize in an iterative way. In this chapter, you learned how to use server-\nside caching, about different techniques to improve web application performance, and how\nto improve database performance. In the next chapter, we will explore the various security considerations that a software\narchitect must make. We will examine the goals of security and the design principles and\npractices that will help us to achieve them. The chapter will cover techniques such as threat\nmodeling, and topics such as cryptography, identity, and access management, and how to\nhandle common web application security risks.", "domains": ["Software Quality Attributes", "Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 385", "position": 385, "chunk_type": "semantic", "token_estimate": 248}
{"text": "11: Security Considerations\nDesigning and developing software systems that are secure is of vital importance. A\nsoftware application that does not follow secure practices creates vulnerabilities that can be\nexploited by attackers. The result of an attack can lead to unauthorized access to\nconfidential data, financial losses, and ruining an organization's reputation. We will explore the three states that information can be in and the main goals of\ninformation security, represented by the confidentiality, integrity, and\navailability (CIA) triad. We will take a look at how threat modeling can help to identify\nand prioritize threats. We will learn principles and practices that will help create secure\napplications by design. The chapter will cover tools such as encryption and hashing along with the best ways to\nimplement identity and access management. The chapter concludes by taking a look at\nsome of the most common web application security risks and ways to mitigate them. In this chapter, we will cover the following topics:\nThree states of information\nThe CIA triad\nThreat modeling\nPrinciples and practices of creating software that is secure by design\nCryptography (encryption and hashing)\nIdentity and access management, including authentication and authorization\nMost common web application security risks", "domains": ["Design Patterns", "Software Quality Attributes", "Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 386", "position": 386, "chunk_type": "semantic", "token_estimate": 199}
{"text": "Security Considerations: Chapter 11\n[ 358 ]\nSecuring software systems\nSecurity is the ability of a software application to prevent and protect against malicious\nattacks and the unauthorized use of the application and its data. It involves protecting one\nof the most important assets that an organization can possess, which is information. Information assets include not just data but also things such as logs and source code. It is\nthe responsibility of software architects and developers to protect software applications and\ndata. When we are designing and developing software systems, this is why we must focus\nthe proper amount of attention on security considerations. Security is a quality attribute and, as is the case with other quality attributes that we have\ncovered in this book, we must think about and document the requirements for quality\nattributes. Requirements for security must be specified and they must be precise,\nmeasurable, and testable. We need to be able to determine if we are meeting security\nrequirements. Different software systems have different security requirements so it is important to\nunderstand the security needs of the system. A website for a blog has very different\nsecurity needs than a payroll application. There is a variety of different ways that an\nattacker can compromise a system, so threats must be considered carefully. It is difficult to\njust add on security later. Security is architectural and must be considered during\nrequirements, design, development, and testing. The three states of information\nThe information that we strive to protect can be in one of three states. It can either be at rest,\nin use, or in transit. Information in all of these states is vulnerable to attack and needs to be\nconsidered in the context of security. Information that is at rest is currently not being accessed. It is stored in some form of\npersistent storage, such as a database or a file. It can eventually be accessed either through\nan application or directly if someone had access to the persistent storage. Information that is in use refers to information that is currently being used by some process\nor application. It is data that is currently in a non-persisted state. Data currently in memory\nor in a CPU cache are examples of information that is in use. Information that is in transit is in the process of being moved, perhaps over a network.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 387", "position": 387, "chunk_type": "semantic", "token_estimate": 394}
{"text": "Security Considerations: Data currently in memory\nor in a CPU cache are examples of information that is in use. Information that is in transit is in the process of being moved, perhaps over a network. It is\npotentially accessible as it is being transferred. Secure transmission of the data, using a\nsecure channel, is necessary to provide secure communication of information.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 387", "position": 387, "chunk_type": "semantic", "token_estimate": 61}
{"text": "Security Considerations: Chapter 11\n[ 359 ]\nThe CIA triad\nThe CIA triad represents some primary goals for information security and the protection of\ninformation assets. The CIA triad summarizes the attributes that we want our software\nsystems to exhibit. CIA stands for confidentiality, integrity, and availability:\nYou may see the CIA triad referred to as the AIC triad in order to avoid confusion with the\nCentral Intelligence Agency. Software architects should strive for a balance between the confidentiality, integrity, and\navailability of information. Let's now take a closer look at the three fundamental objectives\nof the CIA triad. Confidentiality\nSoftware applications should protect confidentiality. The information that a software\napplication manages has value to its users and to the organization that created the\napplication. Confidentiality involves preventing unauthorized individuals from accessing\ninformation. The application must protect its data, particularly when the data is private and personal. This includes when data is in any of the three states of information. Information can be\nvulnerable to eavesdropping while it is being transmitted between services or authorized\nparties. Integrity\nSoftware applications should ensure integrity. The goal is to prevent unauthorized\nindividuals from modifying or destroying information. No matter which of the three states\nof information the data is currently in, the application has a responsibility to ensure that it\nhas not been tampered with by an unauthorized party.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 388", "position": 388, "chunk_type": "semantic", "token_estimate": 226}
{"text": "Security Considerations: Chapter 11\n[ 360 ]\nAvailability\nSoftware applications need to maintain availability. When implementing security,\navailability is among the desired qualities. If the security mechanisms employed are overly\nextensive given the requirements, then the usability and availability of the system will be\nlessened. Software applications must allow authorized individuals access to information in a timely\nand reliable way. Securing data serves no purpose if authorized users cannot get to it. The\ninformation that a software application manages only has value when the right people can\naccess it when they need it. Chapter 4, Software Quality Attributes, covered quality attributes, including availability. Part of what this involves is being able to detect, recover from, and prevent faults to ensure\nthat the application and its data are available to users when they want it. Threat modeling\nThreat modeling is a structured approach to analyzing security for an application. A threat\nis a possible danger to a software system and may cause serious harm to it. Threat\nmodeling is a process that identifies and prioritizes potential security threats so that a\ndevelopment team can understand where their application is most vulnerable. Threat\nmodeling evaluates threats with the goal of reducing an application's overall security risks. Once the analysis is complete, a plan can be formulated to mitigate identified security risks. Traditional approaches to software security may have focused on security from the\ndefender's point of view. However, modern approaches use threat modeling to focus on\nsecurity from the attacker's viewpoint. A threat agent is any individual or group that may\nattack a software system and exploit its vulnerabilities. While threat modeling can be applied to an existing software system, the inclusion of threat\nmodeling while designing and developing a software system is an effective way to ensure\nthat security is an integral part of the system. At a high level, software architects should seek to decompose the software system, identify\nand categorize threats, prioritize threats, and create ways to mitigate them.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 389", "position": 389, "chunk_type": "semantic", "token_estimate": 330}
{"text": "Security Considerations: Chapter 11\n[ 361 ]\nDecomposing an application\nWhile a new software system is being designed, or during the analysis of an existing\nsoftware system, we want to begin threat modeling with an understanding of the software\nsystem. Decomposing an application helps us to understand our software application better\nand uncover security vulnerabilities. Decomposing an application includes knowing the\nassets that an attacker may be interested in, the potential attackers of the system, the\ninteractions with external entities, and the entry points into the system. Organizations and their software systems have assets, which are the things of value to\nattackers. Assets might be physical, such as obtaining login credentials or a software\nsystem's data. They can also be abstract, such as an organization's reputation. Part of considering the various threats to your software system includes gaining an\nunderstanding as to who may be potential attackers. Attackers may be external or internal\nto an organization. You must consider both types of attacker, and in doing so will realize\nthere may be different entry points into your application and different ways of attacking it. Attackers will be motivated by the assets you have identified, so taking assets into\nconsideration will help you to identify potential attackers. You need to understand how the software system interacts with different external entities. External entities include users as well as external systems. These interactions will allow you\nto identify entry points into your system, which are locations where potential attackers can\ninteract with the software system. Attackers will focus their efforts on entry points as they\nprovide opportunities for them to carry out their attacks. Identifying and categorizing potential threats\nOnce we have a good understanding of the software system, we need to identify and\ncategorize potential threats. In order to categorize threats, we have to agree on a threat\nclassification model. A threat classification model provides a set of threat categories with\ndefinitions so that each identified threat can be categorized in a systematic and repeatable\nway. STRIDE is one type of threat classification model.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 390", "position": 390, "chunk_type": "semantic", "token_estimate": 342}
{"text": "Security Considerations: Chapter 11\n[ 362 ]\nSTRIDE threat model\nSTRIDE is a security threat model that was originally created by Microsoft. The name is an\nacronym that represents six threat categories:\nSpoofing identity\nTampering with data\nRepudiation\nInformation disclosure\nDenial-of-service\nElevation of Privilege\nSpoo\ufb01ng identity\nSpoofing identity is the act of representing yourself as someone else. For example, if an\nattacker gained access to someone's authentication information, such as their username and\npassword, they could use it to spoof an identity. Other examples of spoofing identity are\nforging an email address or the modification of header information in a request with the\npurpose of gaining unauthorized access to a software system. Tampering with data\nTampering with data involves an attacker who modifies data. Examples of tampering with\ndata include modifying persisted data in a database, changing data as it travels over a\nnetwork, and modifying data in files. Repudiation\nRepudiation threats can occur if a software system does not properly track and log actions\nthat take place. This allows users, legitimate or otherwise, to be able to deny that they\nperformed a particular action. For example, an attacker could manipulate data and then\ndeny responsibility. Without the system being able to trace the operations properly, there\nwould be no way to prove otherwise. Such an attack could involve sending inaccurate\ninformation to log files, making the entries in the log files misleading and unusable. In software systems, we seek non-repudiation, which is the assurance that a person cannot\ndeny an action that they performed. Strong authentication, accurate and thorough logging,\nand the use of digital certificates can be used to counter repudiation threats.", "domains": ["Software Quality Attributes", "Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 391", "position": 391, "chunk_type": "semantic", "token_estimate": 274}
{"text": "Security Considerations: Chapter 11\n[ 363 ]\nInformation disclosure\nInformation disclosure is a category of threat that involves a software system failing to\nprotect information from individuals who are not supposed to have access to the\ninformation, for example, allowing an attacker to read data from a database or while it is in\ntransit over a network. The information that an attacker obtains could potentially be used for other types of attack. For example, an attacker can obtain system information (server OS version, application\nframework version, and so on), source code details, information from error messages,\naccount credentials, or API keys. The information taken by an attacker can then be used as\nthe basis for further, more damaging attacks. Denial-of-service\nA denial-of-service (DoS) attack takes place whenever an attacker is able to deny service to\nvalid users. An attacker can flood servers with packets to the point that the servers become\nunavailable or unusable. If a large number of bogus requests are sent to servers, they can be\noverloaded such that they cannot fulfill legitimate requests. While a DoS attack may be conducted by a single computer, one type of DoS attack, known\nas a distributed denial-of-service (DDoS) attack, floods a victim from many different\nsources. This makes it much more difficult to block the source of the attack as well as to\ndifferentiate between legitimate traffic and traffic that is part of the attack. Elevation of Privilege\nElevation of Privilege (EoP) takes places when an attacker is able to gain authorization for\noperations beyond what was originally granted. For example, an attacker obtains read and\nwrite privileges for an application when they were initially only granted read privileges. The dangerous aspect of this threat is the fact that the attacker is part of the trusted system\nitself. Depending on the privileges the attacker is able to obtain, they may be able to inflict a\nhigh level of damage. Prioritizing potential threats\nOnce threats have been identified and categorized, we can prioritize them based on their\npotential impact on the software system, the likelihood that they may occur, and the ease\nwith which they can be exploited. These qualities can be used to give a qualitative ranking\n(for example, High, Medium, and Low) to prioritize threats.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 392", "position": 392, "chunk_type": "semantic", "token_estimate": 376}
{"text": "Security Considerations: Chapter 11\n[ 364 ]\nAnother approach to prioritizing threats is to utilize a threat-risk ranking model. The\nDREAD risk assessment model is one example of this type of ranking model. DREAD risk assessment model\nDREAD is a risk assessment model that can be used to prioritize security threats. Like the\nSTRIDE model, it was created by Microsoft. DREAD is an acronym that represents the\nfollowing risk factors:\nDamage potential\nReproducibility\nExploitability\nAffected users\nDiscoverability\nEach risk factor for a given threat can be given a score (for example, 1 to 10). The sum of all\nthe factors divided by the number of factors represents the overall level of risk for the\nthreat. A higher score signifies a higher level of risk and would typically be given a higher\npriority when determining which threats should be focused on first. Damage potential\nDamage potential represents the level of damage that could be done to users and the\norganization if an attack were to succeed. For example, damage to an individual user's data\nwould be rated lower than an attack that could bring down the entire system. Depending\non the type of attack and the asset(s) being targeted, damage could be something concrete,\nsuch as financial liability, or abstract, such as damage to an organization's reputation. Reproducibility\nReproducibility is a measure of how easy it is to reproduce a particular attack. An attack\nthat can be reproduced reliably would be rated higher than one that is statistically unlikely\nto be exploited or one that cannot be reproduced consistently.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 393", "position": 393, "chunk_type": "semantic", "token_estimate": 258}
{"text": "Security Considerations: Chapter 11\n[ 365 ]\nExploitability\nThe exploitability of a threat describes how difficult it is to exploit a vulnerability. While\nsome exploits are easily understood and could be done by anyone (even perhaps\nunauthenticated users), some require advanced techniques, tools, or scripts. A threat with a\nvery low level of exploitability would be difficult to execute even with knowledge of the\nvulnerability. A\ufb00ected users\nThe affected users risk factor represents percentage of users that will be affected by a\nparticular threat. While some attacks may only affect a small number of users, some can\naffect almost all users. The greater the number of users who may potentially be affected, the\nhigher this risk factor should be rated. Discoverability\nDiscoverability signifies how easy it is to learn about the vulnerability. A threat that is very\ndifficult to uncover would be rated lower than one that has already been disseminated in\nthe public domain. Many security professionals believe that discoverability should not be part of the model\nbecause the overall threat ranking should not be affected by this factor. Security by\nobscurity is a weak security control and it is not wise to consider a security risk less of a\nthreat simply because it is difficult to discover. Some practitioners use a DREAD-D (DREAD minus D) model and eliminate discoverability\naltogether. Alternatively, a development team can assign the maximum rating for\ndiscoverability for each threat, which effectively removes it as a factor. Responses to threats\nOnce we have identified, categorized, and prioritized the threats to our software system,\nwe can produce approaches that document how we want to respond to the threats. In\nthe Software risk management section of Chapter 2, Software Architecture in an Organization,\nwe discussed software risk management in the context of project management and\ndelivering a software application. The different risk management options can also be\napplied to the context of security threats. As a response to a security risk, we can avoid the\nrisk, transfer the risk to another party, accept the risk, or mitigate the risk.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 394", "position": 394, "chunk_type": "semantic", "token_estimate": 343}
{"text": "Security Considerations: Chapter 11\n[ 366 ]\nAvoiding the risk\nRisk avoidance requires us to make changes so that the risk no longer exists or is reduced. It\nshould be noted that not all security risks can be avoided, and avoiding a security risk can\nlead to other risks. Transferring the risk\nTransferring the risk to another party can be a viable strategy for some security threats. Some common ways in which risk can be shifted to another party is through an insurance\npolicy and through contracts. If an insurance policy is purchased, an insurance company will assume the financial risks\nthat might result from a security threat. Keep in mind though that it may not be possible for\ninsurance to make up for the damage that a security attack can inflict on an organization. If\nan organization's reputation is ruined and the trust of customers is lost, the consequences to\na business can be catastrophic. We can also transfer a security risk to another party by contracting out the work. We will\ndiscuss the different types of security control shortly but whether it is physical or technical\nsecurity, we can opt to have another organization handle it for us through a contract. The\ncontract can contain provisions that protect the organization should requirements not be\nmet. An example of this approach is hosting your application with a cloud provider. Some\naspects of security can be transferred to the cloud provider, such as the physical security of\nfacilities and servers, environmental security (for example, patching the servers for security\nthreats and providing anti-virus/malware protection), and handling the security of your\ndata. Accepting the risk\nAnother approach would be to simply accept the risk. If the prioritization of a security\nthreat is low based on factors such as the potential damage, reproducibility, exploitability,\nthe potential number of affected users, and the effort it would take to mitigate the risk, it is\nsometimes decided to accept the risk.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 395", "position": 395, "chunk_type": "semantic", "token_estimate": 326}
{"text": "Security Considerations: Chapter 11\n[ 367 ]\nMitigating the risk\nRisk mitigation of a security threat involves implementing some type of security control to\nmitigate the risk. The goal is to reduce or eliminate the possibility of the threat occurring\nand/or reduce the amount of damage that can result from the threat. There are different\ntypes of security controls that can be put in place to mitigate security risks depending on\nthe nature of the threat. Types of security control\nSecurity controls are countermeasures or safeguards that are used to handle security risks. When taking a holistic approach to the security of your software application, a software\narchitect must look at the different types of security controls that can be used, including\nphysical, administrative, and technical security. In addition to categorizing security controls by the manner in which they work, we can\ncategorize security controls by their overall goal and purpose. A security goal can be used\nfor prevention, detection, or a response to a threat. A security control can be described by a\ncombination of these two different types of categorization. For example, a security control\ncan be both technical and for the purpose of detection. Physical security controls\nPhysical security consists of security measures that are put in place to prevent\nunauthorized access to facilities, equipment (for example, servers), personnel, and other\nresources. Physical security is a combination of multiple systems working together and\nmay include gates, locks, key cards, video surveillance, lighting, alarm systems, and\nsecurity personnel. When you think about securing a software application, you may tend to focus on other\naspects of security, but physical security should not be overlooked. Some security threats\nare only made possible when an attacker gains physical access to one or more resources. Attackers may be internal or external to the organization, so the use of multiple techniques\nis the most effective way of enforcing physical security.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 396", "position": 396, "chunk_type": "semantic", "token_estimate": 317}
{"text": "Security Considerations: Chapter 11\n[ 368 ]\nFor some organizations, this is yet another advantage of hosting their software applications\nin the cloud. Major cloud providers have a tremendous amount of experience operating\ndata centers and provide a high level of physical security. It can be cost-effective,\nparticularly for smaller organizations, to use a cloud provider and leverage their security as\nopposed to operating their own data center. Administrative controls\nAdministrative controls include organizational policies and procedures that are put in place\nfor security. An organization must consider security concerns in their overall organizational\npolicies and procedures. Some examples of this include:\nSecurity awareness training for employees\nEscalation plan in the event of a security attack\nEmployees being required to carry and display a photo identification card\nPolicies regarding acceptable use of company hardware and networks\nPolicies related to what type of software can be installed on company hardware\nRules on how company and customer data is handled\nImplementation of company password policies, including the required level of\ncomplexity and requiring employees to change them periodically\nRequired use of anti-virus software\nProcedures regarding required software updates, including operating system\npatches\nRules related to opening and sending emails, including their attachments\nPolicies related to remote access of company hardware and networks\nProcedures regarding the monitoring of servers\nWireless network communication procedures\nA service continuity plan that can be used to keep a software system operational\nafter an event such as a security attack or natural disaster\nAn organization's hiring practices for all of their employees should also consider security. They must include steps to minimize the potential of hiring someone who is or can become\na security concern. A pre-employment screening should be conducted prior to hiring a new employee in order\nto investigate and confirm the backgrounds of candidates. Background checks can include\nidentity and address verification, a criminal history check, speaking with references, and\nconfirming academic accomplishments.", "domains": ["Software Quality Attributes", "Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 397", "position": 397, "chunk_type": "semantic", "token_estimate": 318}
{"text": "Security Considerations: Chapter 11\n[ 369 ]\nPre-employment screenings also typically involve a credit check. These types of check do\nnot reveal a candidate's credit score or account numbers. An excessive number of late\npayments and the mishandling of personal finances may indicate that a candidate is not\norganized or responsible. If a candidate has excessive debt, is using a high percentage of\ntheir available credit, or is having other financial difficulties, there may be an increased\nlikelihood of theft. Employers need to notify the candidate and get their permission in\norder to conduct a credit check. Laws regarding credit checks vary by state so the human\nresources (HR) department of an organization should be aware of any laws that it must\nfollow. Screening employees should not be considered a one and done type of event but rather a\ncontinuous process. Many employees do not have any intention of attacking the\norganization when they are hired, but things can change over time that cause an employee\nto change their motivations later. Organizations should also have procedures for the\ntermination of an employee. A set of steps should be taken each time an employee quits or\nis otherwise terminated so that the different types of access they have been granted (for\nexample, physical access to a facility, access to their email account, and network access) are\nrevoked in a timely manner. Technical security controls\nTechnical security controls utilize technology to provide security for a software system. These controls are implemented through technical solutions such as software, firmware, or\nhardware. We will discuss some of the technical security controls available to us later in\nthis chapter, but examples include the use of encryption, hashing, authentication,\nauthorization, logging, monitoring, utilizing proper communication protocols,\nhardware/network protection, and database security. Prevention\nThe purpose of a prevention security control is to avert a security threat before it occurs. Preventing a security threat requires analysis and planning. Some physical, administrative,\nand technical security controls are used for prevention. For example, locks and key cards are physical control measures intended to prevent a\nsecurity attack. Security awareness training and company password policies are\nadministrative examples of preventive controls. Technical controls that are preventive\ninclude encryption, hashing, authentication, authorization, installing operating system\nsecurity patches, using anti-virus and malware detection software, and the use of firewalls.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 398", "position": 398, "chunk_type": "semantic", "token_estimate": 386}
{"text": "Security Considerations: Chapter 11\n[ 370 ]\nDetection\nSecurity controls designed to detect a security threat are an important part of an overall\nsecurity strategy. Regardless of the preventive measures that are put into place, you should\nexpect that they will fail and assume that a security attack will eventually happen. This will\nput you in the proper mindset to earnestly consider how to detect attacks. It is critical for\nyour software to have the ability to detect and properly notify individuals when an attack\noccurs. Security cameras, motion detectors, system monitoring, logging, auditing, and the use of\nanti-virus and malware detection software are all examples of detection security controls. Response\nThe detection of threats has value only if there are available responses. Plans for the\nresponses to various attacks should be made in advance. You do not want to be making\nimportant decisions in the middle of an attack. Examples of a physical security response include the sounding of an alarm and the locking\nof doors. An administrative security control with the purpose of responding to a security\nthreat is an escalation plan that dictates the actions to be taken in the event of an attack. Another example is a service continuity plan that ensures the software application will\ncontinue to be operational even in the face of an unexpected event such as a security attack,\na natural disaster, or some other event. Technical security controls used in the response of a security attack include taking servers\noffline, using anti-virus software to remove and quarantine a virus, rolling back to a backup\nversion of the application, restoring data from a backup, and revoking user\npermissions/disabling a user account. Secure by design\nSoftware architects should strive to create software systems that are secure by design. By\nfollowing proven security principles and practices, we can make our software applications\nmore secure.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 399", "position": 399, "chunk_type": "semantic", "token_estimate": 310}
{"text": "Security Considerations: Chapter 11\n[ 371 ]\nMinimizing the attack surface\nThe attack surface consists of all of the points that an attacker can use to get into a system. The design of a software system should attempt to minimize the total attack surface area as\nmuch as possible. Using different types of security control and following security principles\nand practices that are known to improve security can reduce the attack surface. One should\nconsider the attack surface when choosing from among multiple approaches that will\nsatisfy a particular functional requirement. Defense in depth\nSecurity tends to be more effective when a variety of techniques is used together. No\nsecurity control is perfect, and defense in depth is the concept of using multiple techniques\nin conjunction and the belief that in doing so a software system will be made more secure. If one security control fails, a threat may be prevented by another security control. Layering\ndefenses using several independent methods will make it much more difficult to exploit a\nvulnerability. Principle of least privilege (PoLP)\nOne of the security principle that can be applied to software systems is the principle of\nleast privilege (PoLP). The principle of least privilege, which is sometimes referred to as\nthe principle of least authority, informs us that the least amount of privileges that are\nnecessary should be granted to a user or process in order to reduce security risks. Following this principle is one way to minimize the attack surface. In addition to each user being granted as few privileges as necessary, each component of a\nsystem should only be granted the privileges that are necessary. Components that are\ncomplex should not have a large number of privileges. If necessary, complex components\nmay need to be split up into simpler components. It may take some effort to determine what the least amount of privileges actually is, but\neven though it may be easier to assign more privileges than are necessary, being as precise\nas possible with the level of privileges that are necessary will minimize the attacker surface.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 400", "position": 400, "chunk_type": "semantic", "token_estimate": 345}
{"text": "Security Considerations: Chapter 11\n[ 372 ]\nAvoiding security by obscurity\nSecurity by obscurity, also known as security through obscurity, is the belief that a software\nsystem is secure as long as internal details are kept hidden and vulnerabilities are not\nknown or are difficult to detect. Software architects should not promote or encourage security through obscurity because it\nis a practice we would like to avoid. While it can help to provide some level of security, it is\na weak security control. If it is used, it should be used in conjunction with other, stronger\nsecurity controls. Keep software designs simple\nSimplicity in the overall design of the system is important for security because a system\nthat software architects and developers have a difficult time understanding is one that may\nnot be secure. A more complex system makes it more difficult to reason about all of the\ndifferent threat possibilities. There is a greater likelihood of a mistake being made during\nimplementation, configuration, or the use of a software system when it is complex. Software systems that are more complicated tend to have a larger attack surface. As long as\nrequirements are being met, simple and elegant designs should be favored over complex\nones for a variety of reasons, one of which is security. As was mentioned in discussing the\nprinciple of least privilege, complex components may need to be re-designed in such a way\nto minimize complexity. Secure by default\nSecure by default, also known as security by default, is the concept of delivering your\nsoftware in a state that maximizes security out of the box, without requiring any changes. If\nthe software is configurable in a way that allows security to be reduced, that should be up\nto the user to change rather than be the default behavior. For example, if a software\napplication allows two-factor authentication (2FA) to be configured, it should be turned on\nby default.", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 401", "position": 401, "chunk_type": "semantic", "token_estimate": 321}
{"text": "Security Considerations: Chapter 11\n[ 373 ]\nDefault deny\nAuthorization within the application should follow a default deny approach whereby\npermissions must be granted as opposed to being denied. In other words, access should be\ndenied by default unless it has specifically been granted, as opposed to a user being\nprovided all access except for operations that have been specifically denied. This concept is\nrelated to, and complementary with, the idea of being secure by default. Validating input\nA number of software vulnerabilities can be avoided by being diligent about validating\ninput from any untrusted sources. Whether it is user input from a user interface, command-\nline arguments being passed into a program, environmental variables, or data from third\nparties, the software application should be wary of it and validate it accordingly. In the case of data from a third party, that party may have security policies and standards\nthat differ from your own, so a software application should check the data that it receives\nfrom an external entity to ensure that it is valid. Secure the weakest link\nAs the old saying goes, a chain is only as strong as its weakest link. This concept can be applied\nto software systems as well. Security of a software system is only as secure as its weakest\ncomponent. Attackers will focus on the weakest component, so be sure that the weakest\npoint in the system is secure enough. Security must be usable\nLegitimate users who are using the software in the way that it was intended to be used\nshould only be impacted to the point that is required to make the system secure. The\nsecurity controls that are used for a software application cannot be so intrusive that they\ninterfere greatly with the usability of the application. If the security controls that are used\nare too annoying, users will seek to circumvent them.", "domains": ["Design Patterns", "Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 402", "position": 402, "chunk_type": "semantic", "token_estimate": 313}
{"text": "Security Considerations: Chapter 11\n[ 374 ]\nA software application must be as secure as it needs to be but it should not be overly secure\nto the point where usability is ruined. If usability is affected too much, users will not want\nto use the software. When designing security controls, we should strive to do so in a user-friendly way. This\nincludes ensuring that the security mechanisms are easy to understand. Without \ncompromising security, the amount that users have to remember and the amount of effort\n(for example, the number of mouse clicks) that a user has to perform to abide by security\ncontrols should be minimized where it is possible to do so. Fail securely\nFailures are bound to happen and a software application that handles security properly will\nfail securely. Software architects should design solutions that consider what should\nhappen when something fails and ensure that the software system and its data remain in a\nsecure state after the failure. Application code should be written in such a way that there is\nproper exception handling and the default behavior is to deny access when a failure occurs. Cryptography\nCryptography is the study and practice of keeping information secret. It is used in\ninformation security to maintain the confidentiality and integrity of data as well as to\nenable non-repudiation. It allows for secure communication of information from\nunauthorized parties. In our look at cryptography, we will be examining encryption and\ncryptographic hash functions, which are tools that can be used to secure data. Encryption\nEncryption is the process of transforming ordinary data, which is referred to as plaintext,\ninto a format that is unreadable, which is referred to as ciphertext. This prevents\nunauthorized parties from accessing it. Data is encrypted using an encryption algorithm in\nconjunction with an encryption key. Larger key sizes result in greater encryption strength\nbut make the process of encryption/decryption slower.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 403", "position": 403, "chunk_type": "semantic", "token_estimate": 317}
{"text": "Security Considerations: Chapter 11\n[ 375 ]\nData that is encrypted can be reverted to its original value. Encryption is used over hashing\nwhen there is a need to know the decrypted value. For example, the encryption is used to\nsend a secure message to someone, it must be possible for the recipient to decrypt the\nmessage or it will be useless. Decryption is the process by which data is decrypted so that\nan authorized party can read it again. The pair of algorithms used for\nencryption/decryption is called a cipher. There are two types of encryption:\nSymmetric (secret key)\nAsymmetric (public key)\nSymmetric (secret key) encryption\nSymmetric encryption, also known as secret key encryption, uses a single key for both\nencryption and decryption. Although it is generally faster than asymmetric\nencryption/decryption, the main drawback is that both parties must have access to the\nsecret key. Asymmetric (public key) encryption\nAsymmetric encryption, also known as public key encryption, uses two keys to encrypt and\ndecrypt data. One of the keys, called the public key, can be shared with everyone, while the\nother key, called the private key, is kept secret. Both keys can be used to encrypt a message and the opposite key from the one used to\nencrypt a message can be used to decrypt it. For example, a public key can be used for\nencryption and the private key can be used for decryption. It is generally slower than\nsymmetric encryption/decryption. Cryptographic hash functions\nA hash function is a function that returns a fixed output for a given input. The input can be\nany size but the output is of fixed size. The output of a hash function is commonly called\na hash, but it can also be referred to as a message digest, digest, hash value, or hash code. If\nthere will be no need to know the original value prior to hashing, then hashing should be\nfavored over encryption", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 404", "position": 404, "chunk_type": "semantic", "token_estimate": 323}
{"text": "Security Considerations: Chapter 11\n[ 376 ]\nSome examples of hash functions include MD5, SHA-256, and SHA-512. For example, the\nfollowing is the Secure Hashing Algorithm 256 (SHA-256) hash of the string This is a\nmessage:\na826c7e389ec9f379cafdc544d7e9a4395ff7bfb58917bbebee51b3d0b1c996a\nIn the case of SHA-256, no matter how long the input is, the hash will be a 256-bit (32-byte)\nhash value. This is useful because even if the input is very long (for example, the contents\nof a file), we know that the hash will be a fixed length. Unlike encryption, where the\noriginal value can be determined through decryption, hash functions are not reversible. Hashes can be used for purposes such as comparing two files for equality without having\nto read all of the contents of both files, as a checksum for detecting errors during\ntransmission of data, finding similar records or substrings, and in data structures such as a\nhash table or a Bloom filter. A cryptographic hash function is a type of hash function that guarantees certain\nproperties, making it secure and suitable for cryptography. The combination of these\nproperties makes a hash function useful for cryptography. We can use cryptographic hash\nfunctions for things such digital signatures, HTTPS certificates, and in protocols such as\nSSL/TLS and SSH. Non-cryptographic hash functions are faster but provide weaker\nguarantees. The following are the main properties of a cryptographic hash function:\nQuick: Cryptographic hash functions are quick to generate a hash value for a\ngiven message. If a hash function is not fast, the performance of the processes\nthat use it may reach unacceptable levels for the given use cases. Deterministic: It is deterministic in that the same message will always produce\nthe same hash. It is this property that allows us to compare two hashes in order\nto determine if they represent the same original value, without knowing the\noriginal value. One-way function: It is a one-way function in that it is infeasible to generate a\nmessage from a hash without trying out all possible messages (brute-force\nsearch). Please note that, by infeasible, we mean that although it is not\nimpossible, it is impracticable.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 405", "position": 405, "chunk_type": "semantic", "token_estimate": 352}
{"text": "Security Considerations: Chapter 11\n[ 377 ]\nCollision resistant: It is collision resistant in that it is infeasible to find two\ndifferent messages with the same hash value. A collision takes place when two\ndifferent inputs result in the same hash. There should not be any collisions with a\nsecure hash function. Some hash functions, like MD5 and SHA-1, can result in\ncollisions and should not be used for cryptographic purposes. Small changes result in vastly different hashes: A small change to a message\nshould yield a new hash that is significantly different from the old one, such that\nit is not possible to correlate the two hashes. For example, the first of the two\nhashes below is from the string Hello World while the second one is from the\nstring Hello Worlds. As you can see, even though the original strings are\nalmost identical, the hashes are very different:\na591a6d40bf420404a011733cfb7b190d62c65bf0bcda32b57b277d9ad9f146e\nb0f3fe9cdc1beeb7944d90e9b2e77b416fd097b5cc2c58838f8741e8129a1a52\nIdentity and access management (IAM)\nIdentity and access management (IAM) comprises policies and tools for managing digital\nidentities and controlling access to information and functionality. Two of the fundamental\nconcepts of IAM are authentication and authorization. Authentication\nAuthentication is the process of determining if someone (or something) is who (or what)\nthey claim to be. It deals primarily with validating the identity of a subject. Examples of a\nsubject that may need to be authenticated include a user, a service, a computer, or an\napplication. In the early days of software development, applications would commonly maintain their\nown user profiles for authentication, which would include some type of unique identifier\n(for example, a username or email address) and a password. Users provide their identifier\nand password and, if they match with the values the application has for a user profile, then\nthe user is considered to be authenticated.", "domains": ["Software Quality Attributes", "Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 406", "position": 406, "chunk_type": "semantic", "token_estimate": 300}
{"text": "Security Considerations: Chapter 11\n[ 378 ]\nWhat is multi-factor authentication (MFA)? Multi-factor authentication (MFA) adds an extra level of security. In multi-factor\nauthentication, a person has to present two or more authentication factors. The variation of\nmulti-factor authentication in which only two authentication factors must be presented is\ncalled two-factor authentication (2FA). The following are different types of authentication\nfactor:\nKnowledge factor: Something the person knows, such as a password or PIN\nPossession factor: Something the person has, such as a cell phone that can\nreceive a code or a company identification card that can be swiped\nInherence factor: Something the person is, using such as using a fingerprint\nscanner, palm reader, retina scanner, or some other type of biometric\nauthentication\nFor example, as part of authentication, a software system may require that a user not only\nsupply a password (a knowledge factor) but also enter a numeric code that is sent to the\nuser's cell phone. The cell phone has to be in the user's possession in order for them to\nreceive the code (a possession factor). In order for a hacker to break into an account, they\nwould need to steal not only a user's password but they must also get possession of that\nuser's phone. Authorization\nAuthorization is the process of determining what a subject is permitted to do and what\nresources that subject is allowed to access. It involves granting rights to allow users or\nprograms to have access to a system or parts of a system. The user or program must first be\nauthenticated in order to determine whether they are who they claim to be. Once they are\nauthenticated, they can be authorized to access parts of a system. Software architects should consider the granularity of privileges. If privileges are too\ncoarse-grained, they may be too large and encompass too many rights. This may require\nprivileges to be granted more frequently and give recipients more access than is necessary. In these types of cases, consider splitting up privileges into more fine-grained privileges to\nprovide greater access control.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 407", "position": 407, "chunk_type": "semantic", "token_estimate": 344}
{"text": "Security Considerations: Chapter 11\n[ 379 ]\nStoring plaintext passwords\nAlthough rare today, some applications persist passwords in plaintext in a data store. Obviously, storing passwords in plaintext is an anti-pattern because either an internal or\nexternal attacker could gain access to the database and all of the passwords would be\ncompromised. Storing encrypted passwords\nTo provide protection of passwords, some software applications encrypt the passwords. During registration, the password is encrypted prior to being stored. In order to\nauthenticate, the encrypted password is decrypted using the appropriate algorithm and a\nkey. The plaintext password entered by the user is then compared with the decrypted\npassword. However, because an encrypted value can be decrypted back to its original value, if an\nattacker can either intercept a decrypted password or obtain the details necessary to\ndecrypt a password, security will be compromised. If you need to store passwords,\nencryption is not the method that we want to use. Storing hashed passwords\nAs we learned in the Cryptographic hash functions section earlier in this chapter,\ncryptographic hash functions are one-way functions with no practical way of reversing the\nhash back to the original value. This characteristic makes them useful for password storage. It is imperative though that you select a cryptographic hash function that is not broken\n(does not have any known collisions). As part of user registration, the password is hashed. When a user logs on, they enter their\npassword in plaintext, which is hashed and compared with the stored hash value. However, hashing alone is not sufficient for storing passwords. A dictionary attack can be\nexecuted to guess a password by comparing it with a pre-compiled list. A table of pre-\ncalculated hashes and their original values, called a rainbow table, can be used for\ncomparison with a hash to determine the password.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 408", "position": 408, "chunk_type": "semantic", "token_estimate": 303}
{"text": "Security Considerations: Chapter 11\n[ 380 ]\nIn order to slow down dictionary attacks and the use of rainbow tables to the point where\nthey are impractical, software applications should hash a combination of the password with\nsome piece of random data, known as a salt. A new salt should be randomly generated for\neach password and be of sufficient length (for example, a 64-bit salt). It is the combination\nof the salt and the password that is hashed. When a salted hash is used in conjunction with\na hashing function such as Argon2, scrypt, brcrypt, or PBKDF2, it makes it necessary for a\nrainbow table to be large enough that it is prohibitively difficult for attackers. When a new user is registered, the plaintext password is combined with the salt, hashed,\nand the hashed value is persisted. When a user logs in, the entered password is hashed\nwith the salt and the value is then compared with the persisted hashed value. This\napproach to managing identity and storing passwords as salted hashes is still in common\nuse today. However, many modern applications have taken the responsibility of\nauthentication and the storage of passwords away from applications and given them to a\ncentral identity provider. Using domain authentication\nOnce enterprises started developing applications that lived in their own local networks, it\nmade sense to leverage domain authentication. Rather than have each application\nimplement authentication independently, the functionality was centralized. On Windows\nservers, the domain controller (DC), along with a directory service such as Active\nDirectory (AD), can manage resources and users for the entire domain. When users log on\nto a company network, they are authenticated in that domain and authorization can be\naccomplished using the attributes of the user profile. For intranet applications, this\napproach works well and is still popular. Implementing a centralized identity provider (IdP)\nMany modern applications have to interact with APIs that are not in the same domain and\nmay not be under its control. A web application, mobile application, or API may need to \ncommunicate with other applications and APIs outside its domain, requiring them to be\npublic. Domain authentication is not sufficient for this case. The ability to grant access to\nresources across applications without sharing login credentials is a common requirement\nand can be accomplished by implementing a centralized identity provider (IdP).", "domains": ["Software Quality Attributes", "Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 409", "position": 409, "chunk_type": "semantic", "token_estimate": 390}
{"text": "Security Considerations: Chapter 11\n[ 381 ]\nAnother advantage of an identity provider is that the applications that we build do not\nhave to be responsible for authentication. Instead, that task becomes the responsibility of\nthe identity provider. Functionality such as user registration, password policies, password\nchanges, and handling locked out accounts can be handled by an identity provider. Once\nimplemented, all of this functionality can then be reused across multiple applications. In\naddition to reusability, maintainability is improved because, if a change is required in any\npart of this functionality, it can be modified in a single location. For instance, we have\nalready seen how best practices for authentication and password storage have changed\nover time. These could continue to change, and by using a central identity provider we\nwould only be required to make modifications in one place. OAuth 2/OpenID Connect (OIDC)\nOAuth 2 is an open standard for authorization. It allows an application to be granted access\nto resources from another application and share its own resources with other applications. OpenID Connect (OICD) is an identity layer that sits on top of OAuth 2. It can be used to\nverify the identity of an end-user. Let us take a look at how OAuth 2 and OpenID Connect\nwork together to enable us to implement a centralized identity provider/authorization\nserver for the purpose of handling authentication and authorization. OAuth 2 roles\nOAuth 2 defines four roles:\nResource owner: Represents the person or application who owns the resource for\nwhich we need to control access\nResource server: The server that hosts the resources; for example, a resource\nserver may be an API that stores data that an application needs to access\nClient: The application that is requesting the resource\nAuthorization server: Server that authorizes the client application to have access\nto a resource\nIt should be noted that the resource server and the authorization server can be the same\nserver, but for larger applications they are commonly separate servers.", "domains": ["Software Quality Attributes", "Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 410", "position": 410, "chunk_type": "semantic", "token_estimate": 330}
{"text": "Security Considerations: Chapter 11\n[ 382 ]\nAuthenticating with the identity provider\nAuthentication performed by an authorization server along with OpenID Connect allows\nclients to verify the identities of users. The client application, which is referred to as the\nrelying party because it relies on the identity provider, requires a user's identity. A flow determines how identity and access tokens are returned to the client. There are\nvarious flows depending on the types of application that are communicating and how we\nwant that interaction to work. In one example, the client application (relying party)\nredirects to the authorization server, which serves as the identity provider. It sends an\nauthentication request to the authorization endpoint, as it is this endpoint that the client\napplication uses to obtain authentication and grant authorization to the client application. If the user is authenticated, the identity provider redirects back to the client application\nusing a redirection endpoint to return an authorization code and an identity token. The\nidentity token can then be stored in either web storage (local storage) or a cookie. In line\nwith the OpenID Connect specification, an identity token is a JSON web token (JWT). JSON web token (JWT)\nA JWT is an open standard for representing claims between two parties. It is lightweight,\nmaking it efficient to transport. The following is an example of a JWT:\neyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJleGFtcGxlLmNvbSIsImp0aSI6Im\nMxZDA2YWQxLTRkMTUtNGY1Mi04YmMzLWMwZmVlODI1NDA5OSIsIm5hbWUiOiJKb2huIFNtaXRoI\niwiaWF0IjoxNTU1OTk4NjEwLCJleHAiOjE1NTg1OTA2MTB9.29WdHTGR5egA5_Q4N9WXtQHO-\nhJydVJou-YiQYQpkq8\nEach JWT has three parts:\nHeader\nPayload\nSignature\nThe three parts are concatenated together with each part separated by a dot (period). If you\nlook closely at the JWT above, you will find the three periods and therefore will be able to\ndetermine the three parts.", "domains": ["Software Quality Attributes", "Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 411", "position": 411, "chunk_type": "semantic", "token_estimate": 275}
{"text": "Security Considerations: Chapter 11\n[ 383 ]\nHeader\nThe header of a JSON web token typically has two pieces of information: the type of token\n(\"JWT\"), and the hashing algorithm being used (for example, HMAC SHA256). A sample\nheader looks like the following:\n{\n    \"typ\": \"JWT\",\n    \"alg\": \"HS256\"\n}\nThe header is Base64Url encoded prior to concatenating it with the other parts of the JWT. Payload\nThe payload of a JSON web token contains the claims. Claims are statements about the\nentity being authenticated (for example, the entity may be a user). There are three types of\nclaims:\nRegistered\nPublic\nPrivate\nRegistered claims are claims that have been predefined. They are not required but represent\ncommon claims that may be useful. Some examples of registered claims include:\nIssuer (iss): Issuer of the token\nSubject (sub): Subject of the token; claims are typically statements about the\nsubject\nAudience (aud): Intended recipients of the token\nJWT ID (jti): Unique identifier for the token\nIssued at (iat): Timestamp representing when the token was issued; it can be\nused to determine the age of the token\nExpiration time (exp): Timestamp representing the time on or after which the\ntoken should no longer be accepted\nPublic claims are those that we can create ourselves. They should be defined as a URI with a\nnamespace to avoid collisions. Private claims are custom claims that two parties agree to use\nin order to share information. Private claims are neither registered nor public.", "domains": ["Design Patterns", "Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 412", "position": 412, "chunk_type": "semantic", "token_estimate": 245}
{"text": "Security Considerations: Chapter 11\n[ 384 ]\nThe following is a sample payload:\n{\n    \"iss\": \"example.com\",\n    \"jti\": \"c1d06ad1-4d15-4f52-8bc3-c0fee8254099\",\n    \"name\": \"John Smith\",\n    \"iat\": 1555998610,\n    \"exp\": 1558590610\n}\nLike the header, the payload is Base64Url encoded. Signature\nThe signature of a JSON web token ensures that the token was not altered at any point. If\nthe token is signed with a secret key, then the signature also verifies the sender of the token. The signature is a hash that consists of the encoded header, the encoded payload, and the\nsecret key using the hashing algorithm specified in the header. The following is an example:\nHMACSHA256(\n  base64UrlEncode(header) + \".\" +\n  base64UrlEncode(payload),\n  secretKey\n)\nAuthorizing with the authorization server\nOnce a user is authenticated and the identity token and authorization code are returned, the\nclient application can send a token request to the token endpoint in order to receive an access\ntoken. The token request should include the client ID, the client secret, and the\nauthorization code. An access token is then returned from the authorization server. Access tokens are not\nrequired to be JWTs but this standard is commonly used. Access tokens can be revoked,\nscoped, and time-limited, providing flexibility for authorization. The application can then use the access token to request the resource from the resource\nserver on behalf of the user. The resource server validates the access token and responds\nwith the data.", "domains": ["Software Quality Attributes", "Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 413", "position": 413, "chunk_type": "semantic", "token_estimate": 232}
{"text": "Security Considerations: Chapter 11\n[ 385 ]\nMost common web application security risks\nThe Open Web Application Security Project (OWASP) is an online community focused on\nweb application security. They offer useful information, including documentation,\nmethodologies, and tools. I invite you to visit their website: https:/\u200b/\u200bwww.\u200bowasp.\u200borg. One of the things that they produce every year is a document listing the top web\napplication security risks. In this section, we will take a look at some more recent risks. Injection\nThis security risk occurs when untrusted data is sent to an interpreter and unintended\ncommands are executed. This can cause unauthorized data to be accessed or manipulated. Anyone who can send untrusted data, including external and internal users, are possible\nthreat agents. A common form of injection is SQL injection (SQLi), where SQL statements are included\nin data (such as user input) and are then unknowingly executed against the database. Among other things, a SQL injection attack could be used to retrieve, alter, or delete data. A web application firewall (WAF), which sits between users and the web application, can\nprotect software systems from some of the more common SQL injection attacks by using\ncommon signatures to identify SQL injection code. However, using a WAF is not sufficient,\nas it is impossible to identify all possible attacks. Validating untrusted data, using SQL\nparameters, and using the principle of least privilege are some techniques that can be used\nin conjunction with a WAF to prevent or lessen the effect of SQL injection attacks. Broken authentication\nA flaw in authentication and/or session management can compromise the security of a\nsoftware system. Attackers can find a flaw in authentication or session management\nmanually and then use automated tools to exploit it. Some of the topics we have explored in this chapter, such as hashing passwords with a salt,\nusing multi-factor authentication, and being secure by default by not deploying with\ndefault credentials, can help secure your system from authentication related attacks.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 414", "position": 414, "chunk_type": "semantic", "token_estimate": 327}
{"text": "Security Considerations: Chapter 11\n[ 386 ]\nPassword policies should be put in place to enforce minimum password length and\ncomplexity requirements, as well as to ensure passwords are rotated periodically. Applications should always provide a logout feature and session timeouts should be short\nenough to prevent an attacker from gaining access simply by using a computer where the\nuser did not previously log out. Session IDs should not be exposed in the URL (for\nexample, URL rewriting), and session IDs should be rotated after a successful login. Information such as passwords, tokens, session IDs, and other credentials should be sent\nover secure connections. Sensitive data exposure\nThis security risk involves not properly protecting sensitive data such as social security\nnumbers, credit card numbers, credentials, and other important data. The first step is to\nidentify what data elements (or combinations of data elements) are sensitive. Only store sensitive data if it is necessary and discard it as soon as possible. Data that isn't\nretained in any way cannot be stolen. Earlier in this chapter, we discussed the different\nstates of information. When sensitive data is at rest, it should be encrypted everywhere it is\nstored long-term, including backups of the data. When sensitive data is in transit, it should\nbe encrypted with secure protocols. Strong and up-to-date encryption algorithms should be\nused along with proper key management. Appropriate browser directives and headers should be set to protect sensitive data\nprovided by or sent to the browser. You should consider disabling the caching of responses\nthat contain sensitive data. XML external entity (XXE) attack\nAn XML external entity (XXE) attack is one that can take place against an application that\nparses XML input. When XML input contains a reference to an external entity and is then\nprocessed by an XML parser that has not been configured appropriately, the application is\nvulnerable to this attack. Denial of service (DoS), the disclosure of sensitive data, and Server-Side Request Forgery\n(SSRF) are all possible with an XXE attack. One type of DoS attack that is made possible\nwith XXE is called a billion laughs attack. Sometimes this type of attack is referred to as an\nXML bomb or an exponential entity expansion attack.", "domains": ["Design Patterns", "Software Quality Attributes", "Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 415", "position": 415, "chunk_type": "semantic", "token_estimate": 369}
{"text": "Security Considerations: Chapter 11\n[ 387 ]\nRegardless of the name, it works by defining ten entities, the first of which is simply\ndefined with a string. It is called a billion laughs attack because, in one common variation,\nthe string \"lol\" is used. The following is an example of an XML document for this type of\nattack:\n<?xml version=\"1.0\"?>\n<!DOCTYPE lolz [\n <!ENTITY lol \"lol\">\n <!ELEMENT lolz (#PCDATA)>\n <!ENTITY lol1 \"&lol;&lol;&lol;&lol;&lol;&lol;&lol;&lol;&lol;&lol;\">\n <!ENTITY lol2\n\"&lol1;&lol1;&lol1;&lol1;&lol1;&lol1;&lol1;&lol1;&lol1;&lol1;\">\n <!ENTITY lol3\n\"&lol2;&lol2;&lol2;&lol2;&lol2;&lol2;&lol2;&lol2;&lol2;&lol2;\">\n <!ENTITY lol4\n\"&lol3;&lol3;&lol3;&lol3;&lol3;&lol3;&lol3;&lol3;&lol3;&lol3;\">\n <!ENTITY lol5\n\"&lol4;&lol4;&lol4;&lol4;&lol4;&lol4;&lol4;&lol4;&lol4;&lol4;\">\n <!ENTITY lol6\n\"&lol5;&lol5;&lol5;&lol5;&lol5;&lol5;&lol5;&lol5;&lol5;&lol5;\">\n <!ENTITY lol7\n\"&lol6;&lol6;&lol6;&lol6;&lol6;&lol6;&lol6;&lol6;&lol6;&lol6;\">\n <!ENTITY lol8\n\"&lol7;&lol7;&lol7;&lol7;&lol7;&lol7;&lol7;&lol7;&lol7;&lol7;\">\n <!ENTITY lol9\n\"&lol8;&lol8;&lol8;&lol8;&lol8;&lol8;&lol8;&lol8;&lol8;&lol8;\">\n]>\n<lolz>&lol9;</lolz>\nEach of the other entities is defined as consisting of ten of the previous entity. The XML\ndocument then consists of a single instance of the largest of these entities. When an XML\nparser loads the document, it expands that entity, which results in all of the other entities\nbeing expanded, to the point where there are a billion copies of the first entity. As you can\nsee, the XML document is very small in size but the expansion takes up so much memory\nand time to parse that it causes a DoS. One of the most effective ways of preventing this type of attack is to simply use a different\ndata format, such as JSON. If XML must be used, disabling document type\ndefinitions (DTDs) completely is also effective. If that is not possible, then external entities\nor entity expansion must be disabled. Each parser is different so you will have to research\nhow to go about making these configurations for the particular language/framework that\nyou are using in your application. Make sure to upgrade all XML processors and libraries\nbeing used by the application so as to include the latest security fixes.", "domains": ["Software Quality Attributes", "Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 416", "position": 416, "chunk_type": "semantic", "token_estimate": 292}
{"text": "Security Considerations: Chapter 11\n[ 388 ]\nBroken access control\nExploitation of missing or broken access control is a common security threat. Lack of access\ncontrol can be detected manually or, in some cases, by using automated tools. This can\nallow attackers to act with elevated privileges, which may allow them to retrieve, add,\nupdate, or delete data. Applications must verify security rights not just on the UI side but also on the server side. Even if the functionality is hidden in the UI from users who do not have proper access\nrights, attackers may attempt to alter the URL, application state, identity tokens, or access\ntokens, or forge requests, to gain access to unauthorized functionality. From the client side, development teams should ensure that the UI prevents the use of\nfunctionality by users who have not been granted access. On the server side, checks should\nbe in place to prevent unauthorized access. Access tokens should be invalidated at the\nappropriate times, such as when a user logs out of the system. The deny by default approach that was discussed earlier should be used to remove access\ncontrol vulnerabilities or to minimize their effect. Access control failures should be logged\nand system administrators should be automatically notified when there are too many\nrepeated failures. Software development testing must include the testing of access control. Security misconfiguration\nMisconfiguration of a software application is a major threat to security. Software\napplications that are more complex have a greater chance of being misconfigured. The\napplication must be configured to be secure prior to deployment. This includes checking all\nsettings before going into production, as many default values are not secure. Everything that is unnecessary in a production environment should be disabled, removed,\nor simply not installed. Examples include accounts, privileges, ports, services, and\naccounts. Any default account passwords should be changed or the accounts should be\ndisabled. Some software applications use a number of tools and frameworks and they may not all be\nfully understood. It is critical that all components used within an application are configured\nproperly. The software's error handling should not divulge too much information to users. For example, a detailed stack trace could be used by attackers for malicious purposes.", "domains": ["Design Patterns", "Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 417", "position": 417, "chunk_type": "semantic", "token_estimate": 371}
{"text": "Security Considerations: Chapter 11\n[ 389 ]\nDevelopment teams need to be aware of the vulnerabilities of the components they are\nusing and keep up with the latest versions and patches to correct issues. A process must\nexist that will allow for keeping all software up to date, such as the operating system, the\nweb/app server, database management system, and development frameworks. All too\noften, a vulnerability in a component has been corrected by a third party in a patch, but the\npatch is not installed. Cross-site scripting (XSS)\nCross-site scripting (XSS) vulnerabilities allow attackers to execute scripts in the browser. The scripts might be designed to hijack a user's session, replace web site content, or redirect\nusers. It is a highly prevalent security flaw. There are three major types of XSS attack:\nReflected XSS\nStored XSS", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 418", "position": 418, "chunk_type": "semantic", "token_estimate": 137}
{"text": "Security Considerations: Chapter 11\n[ 390 ]\nTo be completely safe from this vulnerability, an application could simply not accept any\nserialized objects from untrusted sources. When that is not possible, integrity checks should\nbe made on serialized objects (for example, using a digital signature). Code that deserializes\nobjects should be isolated and only be granted low-level privileges as much as possible. Deserialization exceptions should be logged. Using components with known vulnerable\ncomponents\nApplications may consist of a variety of components, including third-party libraries and\nframeworks. A vulnerability in any of these components can compromise the overall\nsecurity of the application. Development teams must be aware of existing vulnerabilities in the components they are\nusing, keep up with the latest announcements of new vulnerabilities, and apply patches\nand/or new versions that will fix vulnerabilities. If there are any unused dependencies or\ncomponents, then they should be removed to reduce the possibility of exploits. When obtaining components and libraries, be sure to get them from official sources and\nover secure links, or you will be inviting the possibility that such code has already been\ncompromised. If your development team is using a component or library that is no longer\nbeing maintained, then any security flaws that exist are not being patched. You may need\nto consider moving to a different component or library. If that is not possible, then such\ncomponents and libraries should be monitored for possible issues. If one is found and the\nsource code is available, your development team may need to provide a patch. Insufficient logging and monitoring\nInsufficient logging and monitoring is an important security risk because it can help to\nenable so many other types of vulnerabilities. In order for attackers to be successful, they\nneed to go undetected for as long as possible. When logging and monitoring are\ninsufficient, then an attack can go unnoticed for a period of time. A software system needs the ability to answer some fundamental who/what/when\nquestions. Being able to associate user accounts with an event, reconstruct what happened\nbefore, during, and after an event, and know when different events occurred can all help\nyou become aware of a security vulnerability.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 419", "position": 419, "chunk_type": "semantic", "token_estimate": 362}
{"text": "Security Considerations: Chapter 11\n[ 391 ]\nCentralized log management is crucial because log data must be easy to consume. A system\nshould be implemented that will send automatic alerts to the appropriate individuals based\non logging and monitoring data. Do not neglect the fact that the logs themselves need to be\nsecured. Unauthorized access to log data, wherever it is persisted, must be prevented in\norder to maintain the integrity of the log. Unvalidated redirects and forwards\nWeb applications may redirect users to other pages and websites. Attackers can use\nredirects to send users to malicious sites or use forwards to access unauthorized pages. If\npossible, try to avoid redirects and forwards. If your application uses redirects and\nforwards, testing of them should include:\nA review of the code should be conducted for all uses of redirect or forward. For\neach use, identify if the target URL is included in any parameter values. If so,\nverify that the parameter(s) are validated to contain only an allowed destination\nor element of a destination. Someone on the team should spider the site to see if it generates any redirects\n(HTTP response codes 300-307, typically 302). Look at the parameters supplied\nprior to the redirect to see if they appear to be a target URL or a portion of such a\nURL. If so, change the URL target and observe whether the site redirects to the\nnew target. All parameters in the code should be analyzed to see if they look like part of a\nredirect or forward URL so that those that are can be tested. You should consider forcing all redirects to go through a page that notifies users that they\nare about to leave your site, with a link users can click on to confirm. Summary\nWe learned about the CIA triad and its goals of providing confidentiality, integrity, and\navailability. Security is about tradeoffs, so you should try to maintain a balance between\nthese goals. Software applications should be designed to be as secure as necessary, but\nrequirements for quality attributes such as usability and availability must be met as well. There are no silver bullets when it comes to implementing security. However, there are\nproven principles and practices we can use to secure our applications and data. This\nchapter examined threat modeling and different techniques to create applications that are\nsecure by design.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 420", "position": 420, "chunk_type": "semantic", "token_estimate": 396}
{"text": "Security Considerations: Chapter 11\n[ 392 ]\nWe learned about cryptography, including encryption and hashing, and IAM. Software\narchitects who work on web applications should keep up with the latest web application\nsecurity risks so that they can be aware of them and learn how to mitigate them. In the next chapter, we will learn about documenting and reviewing software architectures. Documentation of a software architecture is important so that the solution can be\ncommunicated to, and used by, others. The chapter will cover using the Unified Modeling\nLanguage (UML) to model the architecture and create architecture views. We will also\nexplore the process of reviewing software architectures, including several proven review\nmethods. Reviews of architectural decisions are important to ensure that the architecture\nwill satisfy its requirements, including its quality attribute scenarios.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 421", "position": 421, "chunk_type": "semantic", "token_estimate": 133}
{"text": "Documenting and Reviewing Software Architectures: Chapter 12\n[ 396 ]\nAllowing for the reuse of architectural knowledge\nSoftware architecture documentation allows architectural knowledge to be reused for other\nprojects. The design decisions made, the design rationale that formed the decisions, and\nany lessons learned can be leveraged when other software systems need to be created or\nmaintained. Reuse allows organizations to be more efficient and productive with their software\ndevelopment. If an organization is developing a software product line, which consists of\nmultiple products from the same company to address a particular market, the software\nproducts may have some similar functional and non-functional requirements and may\nshare a similar look and feel in terms of the user interface. Parts of an architecture made for\none software product may be useful for one or more of the other ones. Software architecture\ndocumentation can facilitate the reuse of an architecture. Help the software architect\nDocumenting a software architecture helps the software architect. Software architects will\nbe asked plenty of questions about the architecture by a variety of stakeholders and the \ndocumentation can help to answer them. It supports software architects to fulfill some of\ntheir responsibilities by providing artifacts with which to communicate the architecture to\nothers, assist the development team, educate team members, review the software\narchitecture, and pass on architectural knowledge for reuse. Some projects are rather complex and it can be difficult to remember all of the structures\nthat make up the architecture and how they interact with each other, even for the software\narchitect who was directly involved. The documentation can help to remind a software\narchitect of this information if it has to be revisited months or even years later. A software architect may be trying to get a project off the ground or to gain funding for\ntheir project. Solid documentation can help the software architect achieve these goals by\nproviding information to stakeholders. Software architects can use some of the completed\ndocumentation when they have presentations to give about the software. If a software architect leaves the project or the organization, the documentation that is left\nbehind can answer questions when the software architect is no longer available.", "domains": ["Design Principles", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 425", "position": 425, "chunk_type": "semantic", "token_estimate": 363}
{"text": "Documenting and Reviewing Software Architectures: Chapter 12\n[ 399 ]\nNatural language is used with the notation, so these types of artifacts cannot be formally\nanalyzed and tools will not be able to automatically generate code from the artifacts. Semiformal software architecture notations\nA semiformal notation in views is a standardized notation that can be used in diagrams. However, fully-defined semantics are not part of a semiformal notation. Unlike informal\nnotations, this type of notation allows for some level of analysis and can potentially be used\nto automatically generate code from the models. UML is an example of a semiformal notation that is very popular for modeling. There are\nsome notations that can be used with UML that extend it to provide more robust semantics,\nmaking UML more formalized. Formal software architecture notations\nViews of the architecture that use formal notations have precise semantics, usually\nmathematically based. This allows for formal analysis of both the syntax and the semantics. Tools can use artifacts created with a formal notation for automated code generation. Some software projects choose to not use a formal notation because they require more effort\nto use, they require certain skills on the part of those creating the architecture views, and\nstakeholders may find them difficult to understand. Formal notations are not as useful for\ncommunication with non-technical stakeholders. A number of formal notations exist, including various architecture description languages\n(ADLs). An ADL is a formal type of expression that is used to represent and model a \nsoftware architecture. Architecture Analysis and Design Language (AADL) and Systems\nModeling Language (SysML) are two examples of ADLs. AADL was originally created to\nmodel both hardware and software. It can be used to describe a software architecture,\ncreate documentation, and generate code. The book Documenting Software Architectures \u2013 Views and Beyond describes AADL as follows:\n\"The AADL standard defines a textual and graphical language to represent the runtime\narchitecture of software systems as a component-based model in terms of tasks and their\ninteractions, the hardware platform the system executes on, possibly in a distributed\nfashion, and the physical environment it interfaces with, such as a plane, car, medical\ndevice, robot, satellite, or collections of such systems. This core language includes\nproperties concerning timing, resource consumption in terms of processors, memory,\nnetwork, deployment alternatives of software on different hardware platforms, and\ntraceability to the application source code.\"", "domains": ["Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 428", "position": 428, "chunk_type": "semantic", "token_estimate": 395}
{"text": "Documenting and Reviewing Software Architectures: Chapter 12\n[ 400 ]\nSysML is a general-purpose, graphical modeling language for systems. It is a standard that\nis maintained by the Object Management Group (OMG). It can be used for a number of\nactivities, including specification, analysis, design, and verification. SysML is a subset of UML and reuses some of the same diagram types. We will discuss\nUML in more detail shortly but here is a list of the SysML diagram types that are reused\nfrom UML without any modification:\nUse case diagrams\nSequence diagrams\nState diagrams\nPackage diagrams\nThe following diagrams have been modified in SysML from their UML counterparts:\nActivity diagrams\nBlock definition diagrams\nInternal block diagrams\nSysML also introduces some new diagram types that do not exist in UML:\nRequirement diagrams\nParametric diagrams\nIncluding design rationales\nAn architecture description should include the design rationale behind the design\ndecisions being documented. In Chapter 5, Designing Software Architectures, we discussed\nthe design rationale, which is an explanation that contains the reasons and justification for\ndesign decisions related to the architecture. Without documenting the design rationale, the\nreasons that a design decision was made will not be known. Recording the design rationale\nis beneficial even for those who are involved in a design decision, as the details of a\ndecision can be forgotten over time. It is not necessary (or practical) to record every design decision that is made but any\ndecisions that are important to the architecture are candidates to be documented. When\ndocumenting a design rationale, keep in mind that in addition to including design\ndecisions, it is sometimes useful to include details on why alternative approaches were not\ntaken and why certain design decisions were not made at all.", "domains": ["Design Patterns", "Domain-Driven Design", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 429", "position": 429, "chunk_type": "semantic", "token_estimate": 289}
{"text": "Documenting and Reviewing Software Architectures: Chapter 12\n[ 401 ]\nOverview of the Unified Modeling Language\n(UML)\nThe Unified Modeling Language (UML) is a general-purpose, standardized modeling\nlanguage. It is widely used and understood, making it a popular choice for modeling a \nsoftware architecture. While this section is not intended as an exhaustive tutorial on UML,\nwe will cover some of the most popular UML diagrams and their purpose. If you are\nalready familiar with UML or prefer to use a different modeling language, feel free to skip\nthis section. Types of modeling\nIn UML, there are two main types of modeling: structural modeling and behavioral modeling. Structural modeling focuses on the static structure of the system, its parts, and how they are\nrelated to each other. They do not show details about the dynamic behavior of a system. Some of the structure diagrams in UML include:\nClass diagrams\nComponent diagrams\nPackage diagrams\nDeployment diagrams\nBehavioral modeling shows the dynamic behavior of the components in a system. Unlike\nthe static nature of structure diagrams, behavior diagrams describe changes to the system\nover time. Some of the behavior diagrams in UML include:\nUse case diagrams\nSequence diagrams\nActivity diagrams\nClass diagrams\nClasses are templates (blueprints) for creating objects in a software system. They include\nattributes (member variables) that hold the state of an object and operations (methods) that\nrepresent behavior. Class diagrams, which are among the most popular of UML diagrams,\nshow us the structure of a software system by allowing us to see the classes and their\nrelationships. A number of team members may find class diagrams useful, including the\nsoftware architect, developers, QA personnel, operations engineers, product owners, and\nbusiness analysts.", "domains": ["Design Patterns", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 430", "position": 430, "chunk_type": "semantic", "token_estimate": 282}
{"text": "Documenting and Reviewing Software Architectures: Chapter 12\n[ 402 ]\nA rectangle is used in a class diagram to graphically represent a class and each one can\nhave up to three sections in it. The upper section shows the name of the class, the middle\nsection contains the attributes of the class, and the bottom section details the operations. Visibility\nVisibility dictates the accessibility of a member (attribute or operation) and can be\ndesignated by placing a notation before the member's name. In general, you want to give\nonly as much accessibility as is needed. The following table details the most common\nvisibility notations:\nNotation Visibility\nDescription\n+\nPublic\nMember is accessible by other types. #\nProtected Member is accessible within the same type as well as types that inherit from it. ~\nPackage\nMember is accessible from any type within the same package. It is not accessible from\noutside the package, even if it is an inheriting class. -\nPrivate\nMember is accessible only within the type that declares it. For example, the following diagram shows the Order class, which has two private\nattributes, OrderId and OrderDate, as well as two public operations, CalculateTax and\nCalculateTotal:\nAssociation\nAn association is a broad term that refers to a semantic relationship between classes. If one\nclass uses another class (unidirectional), or two classes use each other (bidirectional), they\nhave a relationship. Relationships between classes are represented by an association, which\nis shown on a class diagram as a solid line:", "domains": ["Design Principles", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 431", "position": 431, "chunk_type": "semantic", "token_estimate": 249}
{"text": "Documenting and Reviewing Software Architectures: Chapter 12\n[ 406 ]\nFor example, our domain may have different types of account, such as a checking account\nand a savings account. These classes may share some of the same properties and behaviors. Rather than repeating what is shared in each of these account classes, our model may have\nan Account base class, which contains the generalized attributes and operations that are\ncommon to all account classes:\nThe CheckingAccount and SavingsAccount classes, which are subclasses, inherit from the\nAccount class and demonstrate an is a relationship. CheckingAccount is an Account, just as\nSavingsAccount is an Account. CheckingAccount and SavingsAccount are specializations\nof Account. Depending on the programming language used for implementation, it may be possible to\nallow some attributes or operations to be overridden in subclasses. Subclasses can also\nintroduce their own specialized attributes, operations, and associations that are specific to\ntheir class. Realization\nRealization denotes a relationship in which one element realizes or implements the behavior\nthat another element specifies. A common example of this is when a class implements an\ninterface. Realization is graphically represented with a hollow triangle at the end of a\ndashed line, with the hollow triangle appearing closest to the element that is specifying the\nbehavior.", "domains": ["Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 435", "position": 435, "chunk_type": "semantic", "token_estimate": 209}
{"text": "Documenting and Reviewing Software Architectures: Chapter 12\n[ 407 ]\nIn the preceding diagram, you may have noticed that ILogger is designated as an interface. This designation is done through a stereotype. Stereotypes are one of the extensibility\nmechanisms available in UML, which allow you to extend vocabulary and introduce new\nelements. In this case, a stereotype has been used to indicate that ILogger is an interface. Stereotypes are graphically represented by enclosing the name in guillemets (angle quotes). They are similar to the symbols for less than and greater than, which can be used if\nguillemets are unavailable. Component diagrams\nComponent diagrams detail the structural relationship between components of a system. These diagrams are typically needed with complex software systems that consist of many\ncomponents, as it is helpful to view the components and their relationships. They\nessentially depict how the components of a software system are wired together, which is\nwhy they are sometimes referred to as wiring diagrams. Component diagrams help us to identify the interfaces between different components of\nour software system. Components communicate with each other through interfaces, and\ncomponent diagrams allow us to see system behavior as it relates to an interface. Interfaces\ndefine a contract by defining the methods and properties that are required for\nimplementations. Implementations can be changed as long as the classes that are dependent\non them are coded for interfaces and not for specific implementations.", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 436", "position": 436, "chunk_type": "semantic", "token_estimate": 236}
{"text": "Documenting and Reviewing Software Architectures: Chapter 12\n[ 408 ]\nBy identifying interfaces, we are able to identify the replaceable parts of our software\nsystem. Having this knowledge gives us the ability to know where we can potentially reuse\na component that the organization has already created or where a third-party component\ncould be used. Components that we create for a software system may also be leveraged in\nother software applications that the organization has in development or will develop in the\nfuture. Knowing the components of a software system also makes it easier for project decision\nmakers to divide up the work. Once the interface is agreed upon, one or more developers\non the team, or even a separate team, can work independently of others in developing a\ncomponent. In a component diagram, components are graphically represented as a rectangle with a\ncomponent symbol (a rectangular block with two smaller rectangles on the left side). For\nexample, an Order component would look like the following:\nAlternatively, the component stereotype can be used, either in addition to the component\nsymbol or in place of it, to designate that an object on the diagram is a component. Interfaces that a component provides are graphically represented by a small circle at the end\nof a line, which is also sometimes referred to as the lollipop symbol. Interfaces that a\ncomponent requires are represented by a half circle at the end of a line, which is also\nreferred to as a socket. For example, let's say that our Order component implements the IOrder interface and\nrequires an implementation of ICustomer. The Customer component implements the\nICustomer interface:\nKeep in mind that components can contain other components. For example, we could\nmodel an Order system component that contains, within it, the Order, Customer, and other\ncomponents, along with all of their relationships.", "domains": ["Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 437", "position": 437, "chunk_type": "semantic", "token_estimate": 308}
{"text": "Documenting and Reviewing Software Architectures: Chapter 12\n[ 409 ]\nPackage diagrams\nIn UML, packages logically group elements together and provide a namespace for the\ngroups. Many UML elements, such as classes, use cases, and components, can be grouped\ntogether in packages. Packages can also contain other packages. Package diagrams are used\nto show the dependencies between packages in a software system. The more complex a software system is, the more difficult it can be to understand all of the\nmodels. Package diagrams make it easier for people to reason about large, complex systems\nby grouping elements together and allowing us to see the dependencies. In addition to modeling standard dependencies, we can model package import and package\nmerge types of dependencies. A package import is a relationship between an importing\nnamespace and an imported package. This can allow us to reference package members\nfrom other namespaces without fully qualifying them. A package merge is a relationship\nbetween two packages in which one package is extended by the contents of another\npackage. The two packages are essentially combined. A package is graphically represented in UML by a symbol that looks like a file folder with a\nname. The following is an example of a high-level package diagram for a layered\napplication:\nBy looking at it, we can see that the Presentation Layer depends on the Business Layer, the\nBusiness Layer depends on the Data Layer, and all three layers depend on the Cross-\nCutting package.", "domains": ["Architectural Patterns and Styles", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 438", "position": 438, "chunk_type": "semantic", "token_estimate": 245}
{"text": "Documenting and Reviewing Software Architectures: Chapter 12\n[ 415 ]\nSynchronous messages are shown as a solid line with a solid arrowhead. A synchronous\nmessage is one in which the sender must wait for a response before it can continue. Asynchronous messages are represented by a solid line with a lined arrowhead. With an\nasynchronous message, the sender does not have to wait for a response before continuing. Reply/Return messages and asynchronous return messages are both represented by a\ndashed line with a lined arrowhead. Loops\nIn order to model a loop in a sequence diagram, a box is placed over the part of the diagram\nthat is iterating through a loop. An inverted tab at the top-left corner of the box is labeled\nwith the word loop to signify that the structured control flow is a loop. The subject that is\nbeing iterated over is commonly labeled with a guard message that is placed below the\ninverted tab. In the following example, the logic is iterating over each line item in an order:", "domains": ["Design Principles"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 444", "position": 444, "chunk_type": "semantic", "token_estimate": 174}
{"text": "Documenting and Reviewing Software Architectures: Chapter 12\n[ 418 ]\nThe following diagram shows the workflow for creating a new membership card:\nThe activities in an activity diagram can either be sequential or concurrent. An activity is\nshown as a rectangle with rounded corners. The rectangle encloses all of the elements of an\nactivity, such as its actions and control flows. Start/end nodes\nSome of the nodes that can appear in an activity diagram represent the different ways in\nwhich flows can begin and end:\nAn activity diagram begins with an initial state, or start point, which is graphically\nrepresented by a small, solid circle (Start/Initial Node). The activity diagram ends with a\nfinal state that is graphically represented by a small, filled circle inside another circle\n(End/Final Node).", "domains": ["Design Patterns", "Design Principles"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 447", "position": 447, "chunk_type": "semantic", "token_estimate": 129}
{"text": "Documenting and Reviewing Software Architectures: Chapter 12\n[ 419 ]\nA Flow Final Node, which is a circle with an X inside, can be used to represent the end of a\nspecific process flow. Unlike the end node, which denotes the end of all control flows\nwithin an activity, a flow final node represents the end of a single control flow. Actions/Control \ufb02ow\nActions are single steps within an activity. Like activities, they are also represented as a\nrectangle with rounded corners. A solid line with an open arrowhead is used to show\ncontrol flow:\nDecision/merge nodes\nA decision occurs in a flow when there is some condition and there are at least two paths\nthat branch from that decision. A label can be placed on each of the different branches to\nindicate the guard condition that would allow control to flow down the branch. When you want to bring multiple alternate flows back to a single outgoing flow, a merge\nnode is used. Both decision and merge nodes are graphically represented with a diamond\nsymbol:", "domains": ["Design Principles"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 448", "position": 448, "chunk_type": "semantic", "token_estimate": 176}
{"text": "Documenting and Reviewing Software Architectures: Chapter 12\n[ 421 ]\nSoftware architecture analysis method (SAAM)\nThe software architecture analysis method (SAAM) is one of the first documented\nmethods for evaluating software architectures. The original purpose of SAAM was to assess\nthe modifiability of a software system, although some have extended it to review a software\narchitecture for a variety of quality attributes, including reliability, portability, extensibility,\nand performance. Scenario-based analysis of software architecture\nSAAM is a scenario-based review method, and can be an effective way to review a software\narchitecture. A scenario is a description of the interaction between some source, such as a\nstakeholder, and a software system. It represents some use or expected quality of the\nsoftware system and may consist of a sequence of steps detailing the use or modification of\nit. Scenarios can be used to test software quality attributes, which is one of the purposes\nbehind software quality attribute scenarios:\nA software quality attribute scenario consists of the following parts:\nSource of stimulus: The source is some entity, such as a stakeholder or another\nsoftware system, which generates a particular stimulus. Stimulus: The stimulus is some condition that requires a response from the\nsoftware system. Artifact: The artifact is the software that is stimulated. It can be a part of the\nsoftware system, the entire software system, or a collection of multiple systems. Environment: The environment is the set of conditions under which the stimulus\noccurs. For example, a particular configuration of the software or specific values\nin the data may be necessary for the stimulus to exist.", "domains": ["Software Quality Attributes", "Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 450", "position": 450, "chunk_type": "semantic", "token_estimate": 263}
{"text": "Documenting and Reviewing Software Architectures: Chapter 12\n[ 423 ]\nOnce scenarios have been classified, they should be prioritized based on importance. This\ncan be accomplished by using some type of voting procedure. The scenarios that are\ndetermined to be of highest priority to the review team as a whole can be used for the\nevaluation. Step 4 \u2013 Evaluate scenarios\nIn this SAAM step, the scenarios are evaluated. For each of the direct scenarios, the\nsoftware architect demonstrates how the architecture can execute it. For any indirect\nscenarios, the team should identify what has to be changed (for example,\nmodification/addition/deletion of components) in order to execute each one. The team\nshould estimate the level of effort necessary to change the system so that it can execute\nindirect scenarios. Step 5 \u2013 Assess scenario interaction\nIn this step, the reviewers analyze the interaction of the scenarios. If multiple related\nscenarios interact with the same component, this may be acceptable. However, if multiple\nunrelated scenarios interact with the same component, it could be an indication of a poor\ndesign. Further analysis should be conducted to determine whether the component is\nlacking a clear separation of responsibilities. Refactoring may be necessary to avoid different scenarios interacting with the same\ncomponent. The component may have low cohesion, indicating that its elements are not\nclosely related. It may also exhibit tight coupling, signifying that the component is highly\ndependent on another component. Low cohesion and tight coupling increase complexity\nand reduce the maintainability of the system. If such a situation exists, a component may\nneed to be separated into multiple components. Step 6 \u2013 Create an overall evaluation\nWith the prior steps completed, the review team should have a list of scenarios that have\nbeen classified, prioritized, and evaluated. The interaction of the scenarios may reveal\npotential issues with the design. Ultimately, the review team must make a decision as to\nwhether the architecture is viable and can be accepted as is or if it has to be modified in\nsome way.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 452", "position": 452, "chunk_type": "semantic", "token_estimate": 338}
{"text": "Documenting and Reviewing Software Architectures: Chapter 12\n[ 426 ]\nStep 2 \u2013 Present the business drivers\nThis step is used to present the software system from a business perspective to the various\nparticipants. The business goals, functionality, architectural drivers, and any constraints\nwill help everyone to understand the overall context of the software system. This\ninformation is presented by one of the project decision makers. Step 3 \u2013 Present the architecture\nThe software architect presents the architecture to the participants in this step. The software\narchitect should provide sufficient detail about the architecture so that the participants can\nunderstand it. The level of detail needed in the presentation can vary from project to project. It really\ndepends on the quality attribute scenarios of the system, how much of the architecture\ndesign is complete/documented, and how much time is available for the presentation. In\norder to be clear what level of detail is expected, the software architect should use phase\nzero, when expectations are set, as an opportunity to ask for clarification. Step 4 \u2013 Identify architectural approaches\nBy the time this step takes place, the participants should be familiar with the design\nconcepts used in the architecture. This includes software architecture patterns, reference\narchitectures, tactics, and any externally developed software. This information was\navailable in Phase 0 when the architecture documentation was reviewed, as well as in the\nprior step (Step 3 \u2013 Present the architecture) when the architecture was presented. This step is\nto simply record the design concepts used so that the list can be used in a subsequent step\nfor analysis. Step 5 \u2013 Generate the quality attribute utility tree\nQuality attribute scenarios can be represented in a utility tree, which represents the\nusefulness (utility) of the system. Utility trees help participants understand the quality\nattribute scenarios. The utility tree is a set of detailed statements about the quality attributes and scenarios that\nare important to the software system. Each entry in the tree begins with the quality\nattribute itself (for example, maintainability, usability, availability, performance, or\nsecurity), followed by a subcategory that breaks it down with more detail, followed by a\nquality attribute scenario.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 455", "position": 455, "chunk_type": "semantic", "token_estimate": 359}
{"text": "Documenting and Reviewing Software Architectures: Chapter 12\n[ 427 ]\nFor example, under a software quality attribute such as Security, we may have multiple\nsubcategories (\"Authentication\" and \"Confidentiality\"). Each subcategory will have one or\nmore quality attribute scenarios:\nQuality\nattribute\nSubcategory\nScenario\nSecurity\nAuthentication\nUser passwords will be hashed using the bcrypt hashing function. Confidentiality\nA user playing the role of a customer-service representative will only be\nable to view the last four digits of a customer's social security number. A user playing the role of a customer-service manager will be able to view a\ncustomer's entire social security number. Performance\nEtc. In addition to identifying the quality attribute scenarios, the project decision makers should\nprioritize them. As with the SAAM, a voting scheme can be used to allow participants to\nprioritize the scenarios. Step 6 \u2013 Analyze architectural approaches\nThe quality attributes that were determined to be of the highest priority in Step 5 \u2013 Generate\nthe quality attribute utility tree, are analyzed, one by one, by the evaluation team in this step. The software architect should be able to explain how the architecture can satisfy each one. The evaluation team looks to identify, document, and ask about the architectural decisions\nthat were made to support the scenario. Any issues, risks, or tradeoffs with the architectural\ndecisions are raised and documented. The goal of the team is to match architectural\ndecisions with quality attribute scenarios and determine whether the architecture and those\narchitectural decisions can support the scenarios. By the completion of this step, the team should have a good understanding of the overall\narchitecture, the design decisions that were made, the rationale behind the decisions, and\nhow the architecture supports the main goals of the system. The team should also now be\naware of any risks, issues, and tradeoffs that may exist. The completion of this step signifies\nthe end of Phase 1.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 456", "position": 456, "chunk_type": "semantic", "token_estimate": 315}
{"text": "Documenting and Reviewing Software Architectures: Chapter 12\n[ 430 ]\nOne of the main premises of ADR is that there is too much information involved with\nreviewing an entire architecture at once and not enough time to do it properly. Many \nreviewers may not be familiar with the goals and details of every part of the design. As a\nresult, no single part of the design ends up getting a complete evaluation. In addition, with\nmore conventional review processes, there may not be enough one-on-one interaction\nbetween the reviewer and the designer. ADR attempts to address these deficiencies by changing the focus from a more general\nreview of the entire architecture to a series of more focused reviews. Questionnaires are\nused to provide more opportunities for interaction between reviewers and designers and to\nkeep reviewers engaged. ADR steps\nThere are five steps to the ADR process:\nPrepare the documentation for review\n1. Identify the specialized reviews\n2. Identify the reviewers needed\n3. Design the questionnaires\n4. Conduct the review\n5. We will now take a look at each of these steps in detail. Step 1 \u2013 Prepare the documentation for review\nIn this step of ADR, preparation takes place for the review. This includes preparing the\ndocumentation for review and listing assumptions that are being made about the portion of\nthe architecture being reviewed. The assumptions need to be made clear so that reviewers\nwill be aware of them. These assumptions include any items that the software architect (or\nother designers) think will never change or are highly unlikely to change. In addition, any\nincorrect usage assumptions should also be provided. These are assumptions that the\nsoftware architect (or other designers) deem to be an incorrect usage of the module and\ntherefore should not take place.", "domains": ["Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 459", "position": 459, "chunk_type": "semantic", "token_estimate": 296}
{"text": "Documenting and Reviewing Software Architectures: Chapter 12\n[ 431 ]\nStep 2 \u2013 Identify the specialized reviews\nIn this step, we identify the specific properties of the design that we want the focus to be\nplaced on by the reviewers. Doing this gives reviewers a clear focus and responsibility for\nthe specialized review that they will be conducting. For example, we may want an\nindividual reviewer to be focused on one or more specific quality attributes. Step 3 \u2013 Identify the reviewers needed\nIn this ADR step, the reviewers for the part of the design being reviewed are identified. We\nwant reviewers to focus on the areas they are most suited to review. The goal is to get\npeople with different perspectives and sets of knowledge to participate as reviewers. Examples of reviewers include development team members who did not work on the part\nof the architecture being reviewed, technical staff from other projects, users of the system,\nnon-technical reviewers who are specialists or have knowledge related to the software\nsystem, reviewers who are external to the organization, and anyone else who may be adept\nat identifying potential issues with a design. Step 4 \u2013 Design the questionnaires\nQuestionnaires are designed in this step, which the reviewers will use while evaluating the\narchitecture in the next step. Use of the questionnaires is intended to encourage reviewers\nto take on an active role and to get them to use the architecture documentation during the\nreview. In addition to questions, questionnaires can contain exercises or other instructions\nfor the reviewers to perform. Questions/instructions should be phrased in an open, active way to encourage further\nthought and a more detailed response. For example, rather than asking if the part of the\narchitecture being reviewed is sufficient, the instructions could guide the reviewer to\nprovide an implementation in pseudocode that uses the portion of the architecture to\naccomplish some task. Step 5 \u2013 Conduct the review\nThis step of the process is when the review takes place. Reviewers are assigned to the\nreview and then a presentation of the module being reviewed is made. The reviewers then\nconduct their reviews, including the completion of the questionnaires. Sufficient time must\nbe allotted so that the review can be completed properly.", "domains": ["Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 460", "position": 460, "chunk_type": "semantic", "token_estimate": 375}
{"text": "Documenting and Reviewing Software Architectures: Chapter 12\n[ 435 ]\nStep 8 \u2013 Perform the review\nThe reviewers use the scenarios to determine whether the architecture solves the problem\nthat is presented. Real code or pseudocode can be written to test the scenario. When the\ngroup feels that a conclusion can be reached (or time runs out), the review ends. Step 9 \u2013 Present conclusions\nWith the review complete, the group should be able to draw conclusions as to whether or\nnot the architecture is suitable for the key scenarios. Any issues with the architecture can be\nreviewed so that the architecture can be refactored to correct any problems. Summary\nDocumenting a software architecture is an important step in delivering an architecture. The\ndocumentation communicates the architecture to others, assists the development team,\neducates team members, provides input for architecture reviews, and allows the reuse of\narchitectural knowledge. Architecture views, which are representations of an architecture, allow a software architect\nto communicate their architecture in a manageable and understandable way. There is a cost\nassociated with creating and maintaining views though, so while we never want the\ndocumentation to be insufficient, we do not want to spend time working on views that are\nnot needed. In this chapter, you were provided with an overview of the UML, one of the\nmore widely used modeling languages. You learned about structural and behavioral\nmodeling. Reviewing a software architecture is important to determine whether the architecture will\nmeet the needs of the system. This chapter provided details on several different architecture\nreview methods. In the next chapter, we will gain an understanding of what software architects need to\nknow about DevOps, including its values and practices. We will learn how continuous\nintegration, continuous delivery, and continuous deployment allow an organization to\nrelease software changes quickly and reliably.", "domains": ["Design Patterns"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 464", "position": 464, "chunk_type": "semantic", "token_estimate": 304}
{"text": "DevOps and Software Architecture: Chapter 13\n[ 442 ]\nWhen asking for resources, a development team may ask for more than they need (for\nexample, asking for a larger than necessary physical server or VM), simply because the\nprocess is too troublesome and they do not want to go through the effort of upgrading it\nlater. QA teams may be waiting for development teams to finish writing code so that they\ncan test it. All of this leads to the organization wasting both time and money. Software architects\nshould be striving for greater efficiency in software development processes, with less\nmanual work and automating more of the effort. In today's competitive environment, organizations need the ability to deliver quickly. In\norder to gain an advantage, they need to add incremental value for their customers by\ndelivering frequent changes that contain new features, bug fixes, and other improvements. DevOps allows the organization to deliver value faster. Through continuous delivery, new\nversions of the software can be deployed quickly and with high quality. Customers have a lower tolerance for system outages now, so it is imperative to have\nprocesses in place that detect faults quickly and allow fixes to be deployed easily. The goal\nis to minimize the number of system outages and their duration. There is an increased\nchance of errors with manual processes. Automating processes reduces the chance for\nerrors. When failures do occur, you will want to recover from them quickly. DevOps can\nhelp you to achieve these goals. Software applications today tend to be more complex and have more complicated\ndeployments. A DevOps approach will lead organizations to automate their builds,\neliminating the possibility of errors caused by mistakes in a manual process. Practicing DevOps properly will lead to a greater level of cooperation between\ndevelopment and operations teams, leading to a decrease in deployment time and an\nincrease in reliability. Encouraging information within the organization to be shared will\nlead to less dependence on tribal knowledge. Collaboration across teams increases the\namount of information that is shared. DevOps toolchain\nToolchains are a set of programming tools that are used in combination to perform a task. A\nDevOps toolchain focuses on the development and delivery of software systems. Software\narchitects responsible for selecting tools should aim to achieve consistency with the\nDevOps tools.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 471", "position": 471, "chunk_type": "semantic", "token_estimate": 385}
{"text": "DevOps and Software Architecture: Chapter 13\n[ 443 ]\nIn an organization that embraces DevOps, it is not just the development team that uses\nsoftware tools to perform their jobs. Operations staff should use many of the same\ntechniques as developers. For example, the assets related to operations should be checked\ninto source control and have automated tests. DevOps tools are generally categorized as supporting one or more of the following\nactivities:\nPlan: The plan category of tools help you to define the requirements of the\napplication and plan the work. The activities that take place around planning\nvary depending on the methodology being used, but generally involve product\nowners and business analysts evaluating requirements and other requests. The\nbusiness value and effort are evaluated and work is planned for a release. Create: Create tools are software that help in some way with design, coding, and\nbuilding activities. Examples of these tools include integrated development\nenvironments (IDEs), version control repositories, build automation tools, and\nconfiguration management tools. Verify: Verification tools consist of software that help to ensure the quality of a\nsoftware release. Some verification activities occur during create activities, while\nothers occur once some part of the software is complete. Functional and non-\nfunctional testing must take place, and DevOps tools can help to automate some\nof the testing. Verification tools can help with performance, acceptance,\nregression, and release testing. Static analysis tools can help to analyze the code,\nand security tools can help to identify vulnerabilities. Package: Packaging tools are used for the tasks associated with preparing a\nrelease once it has been verified and is ready for deployment. Packaging\nconfiguration and triggering releases are some of the activities involved in\npackaging. Release: The tools in the release category are used for release-related activities. They assist teams with the scheduling and coordination of deploying software\nsystems into different environments, including production. Configure: Tools are available that assist with the configuration of software\napplications. The configure category includes tools that help with configuration\nmanagement, application provisioning, and the configuration of infrastructure\nresources.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 472", "position": 472, "chunk_type": "semantic", "token_estimate": 343}
{"text": "DevOps and Software Architecture: Chapter 13\n[ 444 ]\nMonitor: It is critical to monitor software applications to identify issues and gain\nan understanding as to their impact. Production metrics give an organization the\ninsight necessary to understand what the problem areas are with a software\napplication. Monitoring the performance of a software application will ensure\nthat end users are not encountering bottlenecks when they use the software. When we covered CALMS, it was mentioned that measurement is one of the core\nvalues of DevOps. Monitoring tools can help teams measure their applications in\norder to determine whether, and how much, automation efforts have improved a\nprocess or product. DevOps practices\nThere is variation depending on the organization and the software development\nmethodology used, but in a DevOps release cycle, the following major activities generally\ntake place:\nDevelopment\nIntegration\nBuild and unit testing\nDelivery to staging\nAcceptance testing\nDeployment to production\nThe following diagram illustrates a typical DevOps release cycle:\nThree of the main DevOps practices are continuous integration, continuous delivery, and\ncontinuous deployment. These practices revolve around the key activities of a DevOps\nrelease cycle, so let's explore them in more detail.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 473", "position": 473, "chunk_type": "semantic", "token_estimate": 193}
{"text": "DevOps and Software Architecture: Chapter 13\n[ 445 ]\nContinuous integration (CI)\nContinuous integration (CI) is the practice of developers merging their changes into a \nshared source control repository as often as possible. Using some type of version control\nsystem is a necessity. Developers should commit changes on a frequent and regular basis. This reduces the number of conflicting changes and makes it easier to resolve conflicts\nwhen any exist. In the DevOps release cycle, the activities covered by continuous integration include\ndevelopment, integration (the checking in of changes into a version control system), and an\nautomated build with automated unit testing:\nSome may consider the delivery to staging phase that occurs after a build is complete as\npart of continuous integration, while others see that as the next step in overall continuous\ndelivery. Automated builds\nTwo of the key aspects of continuous integration is to build all commits and to have a build\nprocess that is automated. A build converts source code, files, and other assets into a\nsoftware project that is in its final, usable form. This may include things such as the\nfollowing:\nDependency checking\nCompiling source files\nPackaging compiled files into a compressed format (for example, JAR or ZIP)\nCreating installers\nCreating/updating database schema\nExecuting data change scripts to modify data in a database\nRunning automated tests", "domains": ["Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 474", "position": 474, "chunk_type": "semantic", "token_estimate": 221}
{"text": "DevOps and Software Architecture: Chapter 13\n[ 446 ]\nAs a software architect, you will want to establish an automated build process if one is not\nalready in place. Some builds require many steps, and performing these manually creates\nmany opportunities for mistakes to take place. Automated builds eliminate the variation\nthat can take place with manual builds, ensuring consistency between builds. Automated builds are what make it possible to execute builds at any time. In the practice of\ncontinuous integration, automated builds are essential, and should take place when\nchanges have been checked in to a version control system. The duration of automated\nbuilds should not be too long (for example, less than 20 minutes) to allow continuous\nintegration to be feasible. Software versioning\nAs part of the build process for your software, you will need to think about how you want\nto version your software. Software versioning is the process of assigning a unique number or\nname to the software you are building. It is beneficial to use a formal convention for software versioning. Once a formal\nconvention has been established, it provides everyone who is interested in the software,\nboth internally and externally, with knowledge about the state of the software. Without the\nuse of a formal versioning convention, version numbers are meaningless to users and\nuseless for dependency management. A meaningful version number communicates to\ninterested parties information regarding the intention of the version and the extent of the\nchanges in the version. One of the software versioning conventions that can be used is\nsemantic versioning. Semantic versioning, also referred to as SemVer, is a popular convention that can be used\nfor software versioning. Semantic versioning uses a three-part version number that follows\nthe MAJOR.MINOR.PATCH format. For example, a version number might be 1.5.2. A new MAJOR version indicates that breaking changes are included with the version. A\nnew MINOR version means that additions and changes have been made, but the software\nis backward-compatible. A new PATCH version indicates that bug fixes are included with\nthe version and that the software remains backward-compatible. Following this versioning\nscheme makes, it easy to tell what type of changes are included in a new version, and\nprovides the ability to anticipate breaking changes. When your software is in its initial\ndevelopment phase, the version number typically starts with 0.1.0. From there, the minor\nversion can be incremented with each subsequent release.", "domains": ["Design Patterns"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 475", "position": 475, "chunk_type": "semantic", "token_estimate": 402}
{"text": "DevOps and Software Architecture: Chapter 13\n[ 448 ]\nContinuous delivery (CD)\nContinuous delivery (CD) is the ability of an organization to release changes to users\nquickly and in a sustainable, repeatable way. The software is released in short cycles, which\nreduces the risk, cost, and effort of delivering changes to customers. Organizations that\npractice continuous delivery have automated their testing as well as their build process. The aim of continuous delivery is to have your software application in a state where it can\nbe deployed to production at any time. This is an important part of DevOps. Continuous\ndelivery includes all of the major activities that take place in a DevOps release cycle. In\naddition to development, integration, and an automated build with automated unit testing,\nwhich are all part of continuous integration, continuous delivery also includes automated\ndelivery to staging, automated acceptance testing, and deployment to production:\nWith continuous delivery, as opposed to continuous deployment, which we will cover next,\ndeployment to production is a manual process. The organization can decide to release at\nwhatever time interval they want (for example, daily, weekly, or monthly). However, the\nbenefits of continuous delivery are increased if deployments take place as early as possible. Changes will be made available to users sooner and feedback can be received sooner. Each\nset of changes is smaller, making it easier to troubleshoot an issue if there is a problem. After deployment to production, post-deployment tests are typically executed. The extent\nof the post-deployment testing varies, but at the very least it should consist of smoke\ntesting. A smoke test ensures that all crucial functionality is working properly in the target\nenvironment. A smoke test is not as exhaustive as full software testing, but it can be\nperformed quickly to determine whether the deployment in the target environment is\nstable. Continuous deployment\nContinuous deployment essentially takes continuous delivery one step further. Like \ncontinuous delivery, continuous deployment aims to enable organizations to release\nchanges to software systems quickly and reliably. It differs from continuous delivery in that\nit automates the deployment to production.", "domains": ["Design Patterns"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 477", "position": 477, "chunk_type": "semantic", "token_estimate": 347}
{"text": "DevOps and Software Architecture: Chapter 13\n[ 449 ]\nContinuous delivery ensures that the software application can be delivered to staging,\nalong with the execution of rigorous tests, using automation. When done properly, the\nsoftware application should be in a state that it can be deployed to production at any time. Continuous deployment merely automates the final step so that all changes are\nautomatically deployed to the production environment. When the organization, the business, and the nature of the software application allow for it,\ncontinuous deployment is an ideal DevOps goal in that it automates the process from\nbeginning to end, and is the quickest way to move application changes into production. However, in some cases, continuous deployment is not practical. There may be business\nreasons that an organization does not want to automatically deploy all changes to\nproduction. Some organizations prefer that the final step, the deployment to production, be\na manual process. Software architects, along with other key decision makers within the\norganization, will have to decide whether continuous deployment is appropriate for the\norganization. Architecting for DevOps\nAs a software architect, you should consider DevOps when making architectural decisions. Some of the DevOps practices that we have discussed are independent of architecture and\ndo not require specific architectural decisions to be made. However, certain architectural\napproaches may be required in order to realize the full benefit of some DevOps practices. Important quality attributes for DevOps\nSoftware systems that have been architected for DevOps should place importance on\nquality attributes, such as testability, deployability, and maintainability. Testability is a\nvalued quality attribute because the ability to test the system is crucial, particularly since\nthere will be automated tests. In order to architect a system to be testable, components need\nto be isolated from each other so that they can be tested independently. Each component\nmust be controllable so that we can dictate input and how that input will exercise the\ncapabilities of the component. It must be possible to observe the input and output of\ncomponents so that it is possible to determine whether it is working properly.", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 478", "position": 478, "chunk_type": "semantic", "token_estimate": 350}
{"text": "DevOps and Software Architecture: Intrusive architectural changes are typically not\nnecessary to add these types of capabilities, but they are important when changes to a\nsoftware system are being made quickly and often. Software architects should design for\nfailure. In other words, we should expect that failures will occur, but fault tolerance should\nbe part of the system so that recovery can be completed quickly in order to minimize\ndowntime.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 479", "position": 479, "chunk_type": "semantic", "token_estimate": 70}
{"text": "DevOps and Software Architecture: Chapter 13\n[ 451 ]\nSome architecture patterns complement DevOps\nSome software architecture approaches, such as the microservice architecture pattern, lend\nthemselves well to the types of requirements we have discussed for DevOps. It is not\nuncommon for an organization that is moving to either DevOps or microservices to then\nadopt the other. A microservice architecture works well with continuous delivery. You may recall from Chapter 8, Architecting Modern Applications, that a microservice\narchitecture consists of small, focused services with well-defined interfaces. The fact that\neach microservice focuses on a small piece of functionality makes it easier and less risky to\nmake changes. Well-defined interfaces facilitate the swapping out of one microservice\nimplementation for another one. As long as the interface remains the same, the\nimplementation can change. Each microservice should be autonomous and independently deployable. These qualities\nallow an organization to make changes to one microservice and deploy it without affecting\nother microservices. Each microservice can have their own independent data storage,\nfurther making them independent from other ones. The downtime to deploy a single\nmicroservice can be a quick and low-risk process. Microservices provide better fault isolation than other architecture patterns, such as a\nmonolithic architecture. If one microservice fails, it does not mean the entire software\nsystem will go down. Proper monitoring gives operations the ability to notice faults\nquickly, and if a recently deployed microservice is causing an issue, the team can revert\nback to the previous version. DevOps does not require microservices, but the point is that certain architectural patterns\nwork better than others with DevOps practices. Therefore, software architects must\nconsider DevOps when making design decisions and creating architecturally significant\nrequirements. Deploying to the cloud\nTransitioning to DevOps is commonly aligned with the use of the cloud. Many of the core\nvalues we have discussed, such as quick delivery and automated processes, are enhanced\nby deploying applications to the cloud. In order to use the cloud in conjunction with DevOps, software architects should\nunderstand the different cloud types that are available, as well as the main cloud models. In\nthis section, we will be exploring both of these topics in detail.", "domains": ["Architectural Patterns and Styles", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 480", "position": 480, "chunk_type": "semantic", "token_estimate": 361}
{"text": "DevOps and Software Architecture: Chapter 13\n[ 452 ]\nCloud types\nThe three main types of deployment for cloud resources are a public, private, and hybrid\ncloud. Although they provide similar benefits, an organization must select the one that\nmost matches its business needs. Public cloud\nA public cloud consists of cloud resources that are owned and operated by a third-party\ncloud provider. The resources and services are provided over the Internet and shared with\nother organizations. Public clouds are often multi-tenant, meaning that an organization's\napplications and data are hosted on the same hardware and networks with other\norganizations. An organization will experience high reliability and practically unlimited scalability from\nthe large number of resources that cloud providers own and manage. Organizations only\nhave to pay for the services that are used and don't have to worry about the maintenance of\nthe resources. Although public clouds provide the best economies of scale among the different cloud\ntypes, it may not be the appropriate choice for some organizations. For data that is overly\nsensitive and/or subject to regulations, a public cloud may not meet all of a software\napplication's requirements. With a public cloud, you lose some control over your data,\nwhich raises regulatory and compliance concerns regarding data storage and privacy. One\nexample of this is the Health Insurance Portability and Accountability Act (HIPAA) of\n1996, which protects patient information. You must be aware of regulations that may affect\nyour software, and ensure that either your team or the cloud provider can meet those\nrequirements. Private cloud\nA private cloud is made up of resources that are used exclusively by one organization. The\nphysical resources can either be located at an organization's own data center or that of a\nthird-party provider. Either way, all of the infrastructure is dedicated to a single\norganization. In contrast with a public cloud, it is a single tenant implementation.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 481", "position": 481, "chunk_type": "semantic", "token_estimate": 316}
{"text": "DevOps and Software Architecture: Chapter 13\n[ 453 ]\nPrivate clouds are costlier with more modest economies of scale. However, an organization\nwill have greater flexibility in customizing the environment to meet its needs. One of the\nmain reasons to opt for a private cloud is the greater level of control and the stronger\nsecurity that is possible. Organizations can take advantage of cloud services while\nmaintaining more control over their enterprise data, which is sometimes a requirement for\nsecurity and privacy reasons. However, a private cloud can be implemented by the organization or through a third-party\ncloud provider. When using a cloud provider, an organization can still benefit from high\nlevels of reliability, efficiency, and scalability, just as they would with a public cloud. The\nmain benefit of an on-premises approach is that an organization has complete control over\nthe processes, data management policies, and physical resources. A software architect should be mindful though, particularly if a private cloud is being\nchosen for security reasons, that a public cloud can be just as secure or even more secure\nthan a private one if the private one is not implemented properly. A security hole turns a\nprivate cloud into a public vulnerability. If an organization implements their own private\ncloud, it is fully responsible for all aspects of it. Many of the steps required to implement a\nprivate cloud raise security issues that must be considered and addressed. Cloud providers know that a secure cloud environment is essential to their business, so\nthey devote resources to ensure that their environments are secure. Among the top\npriorities and core competencies of a cloud provider is security, which is typically not the\ncase for organizations attempting to implement their own private cloud. For small and\nmedium-sized businesses, it is difficult to achieve the same levels of reliability, efficiency,\nand scalability as a major cloud provider. Ultimately, though, deciding between a public\ncloud, on-premises infrastructure for a private cloud, and having a third party host a\nprivate cloud really depends on the needs and long-term strategy of your organization. Hybrid cloud\nA hybrid cloud is a combination of the public and private cloud types. For some\norganizations, it is a best of both worlds approach. With a hybrid cloud, an organization can\nhave the benefits of a private cloud, but can still leverage the public cloud when it is\nneeded.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 482", "position": 482, "chunk_type": "semantic", "token_estimate": 397}
{"text": "DevOps and Software Architecture: Chapter 13\n[ 454 ]\nFunctionality that has higher volume and lower security needs could be hosted in a public\ncloud, while more mission-critical functionality and sensitive data can be served from a\nprivate cloud. Using a hybrid cloud can give an organization the control it needs from a\nprivate cloud with the flexibility to leverage the public cloud where it makes sense to do so. For organizations with on-premises infrastructure, a hybrid approach allows it to transition\nto the cloud gradually. Legacy applications that have particular environment needs could\nbe hosted on-premises while other applications are hosted in a public cloud. A hybrid cloud approach allows for cloud bursting, also known as the burst compute\npattern. The concept is that an application will run in a private cloud until it experiences a \nspike in demand that is large enough that it bursts into the public cloud. The burst compute pattern is used when an application needs additional compute\ncapability on an as-needed basis. Keeping hardware in-house to handle periodically\nincreased needs can be costly, and the burst compute pattern is more cost-effective. It\navoids having idle, over-provisioned resources. The public cloud handles the extra\ndemand, and you are only charged for extra computing power when you need it. A hybrid cloud can also come in handy for predictable outages, such as scheduled\nmaintenance, rolling brown/blackouts, and natural disasters, such as hurricanes. Traffic can\nbe handled by the public cloud during these periods. Cloud models\nThe services that are provided for cloud computing can be categorized into different cloud\nmodels. They are as follows:\nInfrastructure as a Service (IaaS)\nContainers as a Service (CaaS)\nPlatform as a Service (PaaS)\nServerless/Function as a Service (FaaS)\nSoftware as a Service (SaaS)", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 483", "position": 483, "chunk_type": "semantic", "token_estimate": 293}
{"text": "DevOps and Software Architecture: Chapter 13\n[ 455 ]\nThe models differ in their level of abstraction and the amount of control and responsibility\nthat you have:\nAs you move to the right in the diagram, the level of abstraction is higher, and you have\nless control and fewer responsibilities in regards to the infrastructure. Having fewer \nresponsibilities in terms of the infrastructure means that you can then devote more time to\nwriting the actual application logic and adding more business value for your customers. Infrastructure as a service (IaaS)\nOn the far left of the spectrum is Infrastructure as a Service (IaaS), in which you rent the\nhardware that you need. You are not responsible for maintaining the hardware that you\nuse as it is taken care of for you by the provider. Things such as the storage drives, network\ncomponents, and cooling systems are maintained for you. However, you are responsible for just about everything else. In addition to your own\napplication, you have to purchase, install, configure, and maintain things such as the\noperating system (OS), patches and security updates, anti-virus software, and middleware. You are also responsible for server and logical network configuration. Although you have\nquite a bit of control, you need to devote time to the management of these resources.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 484", "position": 484, "chunk_type": "semantic", "token_estimate": 216}
{"text": "DevOps and Software Architecture: Chapter 13\n[ 456 ]\nCommon reasons to use IaaS include website hosting, web apps, test/development\nenvironments, storage/backup/recovery needs, and big data analysis. An important\nadvantage of IaaS is that it eliminates the capital expenditure on hardware. The cost of\nongoing maintenance for that hardware is also reduced. Not having to devote resources to\nthe purchase and maintenance of hardware allows organizations to focus on their core\nbusiness. It also increases organizational agility as teams can respond quickly to new\nmarket opportunities and release applications faster. When organizations first adopt cloud technologies, IaaS is a common starting point as it\nmakes a migration to the cloud fast and easy. IaaS is typically similar to how organizations\nalready run their information technology (IT) departments, further easing the transition. Containers as a Service (CaaS)\nOne of the application development and deployment models used for cloud-native\napplications is Containers as a Service (CaaS). It builds upon IaaS by adding a container\norchestration platform, such as Kubernetes, Docker Swarm, or Apache Mesos. With CaaS,\ndevelopers and IT staff work together to build, ship, and run applications. Cloud-native applications are containerized and dynamically orchestrated. A CaaS\napproach gives development teams control over how their application and dependencies\nare packaged, making them fully portable. Containerization allows applications to run\nanywhere. Teams can deploy the application to different environments without needing to\nreconfigure them. Containers can run on virtual machines (VMs), on a developer's\nmachine, in a private cloud with on-premises equipment, or in a public cloud. Having the ability to dictate the dependencies of the application, along with their specific\nversions, and have them deployed along with their configuration provides a consistent\nenvironment in which your application will execute. It gives you predictability as to how\nthe application will behave, and increases reusability. It's a great way for a development\nteam to deploy their cloud-native, containerized application to the cloud. While this model gives quite a bit of control to development teams, it does mean that there\nare greater responsibilities. In a similar way to IaaS, you are still responsible for the\noperating system, patches, and security updates for the OS, logging/monitoring, capacity\nmanagement, and the scaling of the application.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 485", "position": 485, "chunk_type": "semantic", "token_estimate": 368}
{"text": "DevOps and Software Architecture: Chapter 13\n[ 458 ]\nContainerization is much more lightweight than using a VM because the OS kernel is\nshared. In contrast with VMs, which each have a copy of the OS, there is only one full copy\nof the operating system. As a result, containers have much lower overhead. They can start\nup very quickly, use less space on the disk, and use up far less memory. It is possible to put\nmany more containers on a single server compared with virtual machines. Virtual machines can be slow to start up and shut down since they come with a large\noverhead that includes a full copy of the operating system. This can lead to slow recovery\nshould one fail. With a container engine and an orchestrator, the placement of containers\ncan be dynamic and spread out, allowing for rapid recovery if there is a failure of the\ninfrastructure or the application. Platform as a Service (PaaS)\nPlatform as a Service (PaaS) provides development teams with a complete platform for\ndeveloping and deploying applications in the cloud. This cloud computing model provides\na higher level of abstraction than IaaS and CaaS. The hardware is provided and maintained\nas it is with IaaS, but there isn't as much interaction with the infrastructure. The platform\nabstracts away this interaction with everything that it provides. Unlike IaaS, the operating system is provided and maintained for you, including the\ninstallation of patches and security updates. With PaaS, the development team does not\nhave to manage the OS. In addition to the OS, providers make available a number of\nsupporting software, tools, services, database management systems, and middleware. The deployment and management of an application are easier, as an increased number of\nservices are provided to assist with the common needs of application development and\ndeployment. The availability of supporting services can greatly assist a DevOps\norganization. Having hardware, the OS, and other supporting software maintained for\nthem saves organizations time and allows them to focus their resources on more important\nthings, such as using their domain knowledge to build application code. It reduces\ndevelopment time and enables capabilities without adding any additional staff or time to\nthe project. One of the disadvantages with PaaS to be aware of is the fact that you lose control over the\nOS.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 487", "position": 487, "chunk_type": "semantic", "token_estimate": 388}
{"text": "DevOps and Software Architecture: Chapter 13\n[ 460 ]\nSummary\nOrganizations can gain a competitive advantage by delivering software applications at high\nvelocity, adding incremental value for their customers frequently and quickly. Having the\nability to reliably deliver new improvements of software products to customers on a\nregular basis can differentiate an organization from its competitors. The culture, practices, and technologies of DevOps can enable organizations to achieve\nthese goals. Continuous integration, with automated builds and testing, allows changes to\nbe validated quickly. Frequent check-ins make it easier to detect and resolve any problems. The practice of continuous delivery keeps software systems in a state where they can be\ndeployed to production at any time. This type of organizational agility provides the option\nof releasing changes to users quickly and in a repeatable way. Some DevOps practices do not require any architectural changes, but the needs of DevOps\nshould be considered during requirements, as well as when making architectural design\ndecisions. Consideration must be given to quality attributes, such as testability,\ndeployability, and maintainability, when architecting a solution. In the next chapter, we will learn what it takes to work on and with legacy applications. Refactoring and integrating legacy applications bring unique challenges that a software\narchitect must be prepared to face.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 489", "position": 489, "chunk_type": "semantic", "token_estimate": 212}
{"text": "Architecting Legacy Applications: Some legacy applications rely on older versions of operating systems, other software,\nand/or hardware. These environments can become increasingly difficult to maintain over\ntime. It also increases the possibility of vulnerabilities if security patches are either not\navailable or not applied.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 491", "position": 491, "chunk_type": "semantic", "token_estimate": 44}
{"text": "Architecting Legacy Applications: Chapter 14\n[ 463 ]\nA legacy application may be one that has been inherited from another development team,\nwho may no longer be available to answer questions about it. This may lead to a general\nlack of knowledge about the system. Documentation and unit tests for a legacy application\nmay be lacking, making it more difficult to gain an understanding of the application and to\nmake changes to it. Why are legacy applications used? While there may be challenges to using legacy applications, enterprises continue to use\nthem for a variety of reasons. If they remain useful and still work as intended, an\norganization may see no reason to stop using them. In addition, the cost of replacing a\nlegacy system may not outweigh the benefits. For example, the difficulties and cost of\nreplacing a system that requires almost constant availability may be prohibitively high. Replacement of a large and complicated system can be a long and difficult process. Rewriting a legacy system may be desirable for a development team, as it allows them to\nwork on a greenfield system, but it may not be a good decision from a business perspective. Replacing a legacy system with a new system that performs the same functionality will cost\nan organization money while potentially not adding much new business value. A\ndevelopment team can certainly point out that a rewritten application will have increased\nmaintainability and alleviate the other types of issue with legacy applications that we just\ncovered. These benefits can result in cost savings over time, depending on how long the\nsystem will be in use. However, rewriting a legacy application, particularly a complex one\nthat currently works, can be a tough sell to Management when it may not make financial\nsense. Modernization of a legacy application, during which it is refactored with the purpose\nof improving it, can be a less expensive alternative that poses fewer risks. Another reason why a legacy application might continue to be used is if it is not well\nunderstood. If the original development team is no longer with the organization, the system\nis complex, or there is a lack of documentation, it will take more effort to replace the\nsystem. This type of barrier can be yet another reason why rewriting an application is put\noff.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 492", "position": 492, "chunk_type": "semantic", "token_estimate": 387}
{"text": "Architecting Legacy Applications: Chapter 14\n[ 464 ]\nMore than just code\nSoftware architects should understand that a legacy application does not just consist of the\ncode. A legacy software system includes requirements related to the environment needed\nfor it to run, such as any dependencies with particular versions of an operating system. It\nmay require particular tools or certain versions of a tool, such as source code editing, build,\nversion control, code coverage, code review, debugging, integration, documentation, static\ncode analysis, and unit testing tools. Some of the tools required by a legacy application may\nnot be supported any longer. A legacy software system is also made up of its dependencies, such as those for other\nsystems, third-party software, frameworks, and packages. Similar to tools, these\ndependencies can become outdated or may no longer be supported. Finally, a legacy\nsystem also includes its internal and external documentation. The documentation for some\nlegacy systems is either sorely lacking or outdated. A combination of all of these factors\nmakes up a legacy system and its overall level of quality. Each one, to some degree, affects\nquality attributes such as the maintainability of the overall system. Where legacy systems exist, software architects overseeing them may need to be involved\nwith refactoring, replacing, or integrating with them. Refactoring legacy applications\nWhen you begin working on a legacy application, you will want to refactor it in order to\nmake it more maintainable. You may need to implement new features, fix defects, improve\nthe design, increase quality, or optimize the application. In order to perform these types of\ntask, the legacy system must be in a state where changes can easily be made and without\nmuch risk. In the classic book Refactoring: Improving the Design of Existing Code, author Martin Fowler\ndefines refactoring as \u2026the process of changing a software system in such a way that it does not\nalter the external behavior of the code yet improves its internal structure. Any refactoring that is\nperformed should safely improve the code in some way, without affecting the business\nlogic and expected functionality. Before making any changes, it is helpful to have the right attitude when approaching a\nlegacy codebase. All too often, a software architect or developer will be highly critical of a\nlegacy application before even fully understanding the codebase.", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 493", "position": 493, "chunk_type": "semantic", "token_estimate": 386}
{"text": "Architecting Legacy Applications: Chapter 14\n[ 465 ]\nAs a software architect, you want to modernize and improve the legacy application that\nyou have been tasked with overseeing. You do not want to focus on making unnecessary\nmodifications, particularly if you do not yet fully understand the impact of the changes. For\nexample, rather than making changes because you do not like how something was coded\nstylistically, you should focus your attention on making substantive changes that will have\na positive impact on the codebase. Refactoring a legacy application with the purpose of modernizing and improving it\nincludes performing the following tasks:\nMaking legacy code testable\nRemoving redundant code\nUse tools to refactor the code\nMaking small, incremental changes\nTransforming monoliths to microservices\nMaking legacy code testable\nMany legacy software systems lack automated unit tests and only some of them have\nadequate code coverage. What may be even more problematic is that some legacy systems\nhave not been developed with unit testing in mind, making it difficult to add tests later. Adding unit tests to a legacy system that does not have them should be given a high\npriority. There are a number of benefits to making legacy systems (and all software\nsystems, for that matter) unit-testable. Benefits of unit testing\nOne of the biggest benefits of using unit tests on a legacy system is that it facilitates making\nmodifications to the system, particularly for individuals who may not be familiar with the\nsystem. As you make changes to a legacy application, unit tests will ensure that the changes\ndid not introduce new defects and that the functionality still works properly. Regularly executing unit tests, such as after a change is made or as part of the build process,\nwill make debugging any issues that are found easier. A developer will be able to narrow\ndown the source of the problem to one of the recent changes.", "domains": ["Architectural Patterns and Styles"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 494", "position": 494, "chunk_type": "semantic", "token_estimate": 317}
{"text": "Architecting Legacy Applications: Chapter 14\n[ 467 ]\nWhen using the complexity level as a factor when deciding where to start, it doesn't have to\nbe one way or the other. Some members of the team could begin with highly complex\ncomponents while others start with less complex ones. Removing redundant code\nAny software application may contain redundant code, but legacy applications, which are \nolder and are more likely to have been maintained by a variety of people, tend to have\nincreased instances of code that is either duplicated or no longer needed. When taking over a legacy application, a software architect should look to remove\nredundant code. Reducing the total lines of code minimizes complexity and makes the\nsoftware system easier to understand. Code analysis tools can help to identify some types\nof code that are unnecessary. Refactoring unreachable, dead, commented-out, and\nduplicate code will improve the maintainability of the system. Let's look at each of these\ntypes of code in more detail. Unreachable code\nUnreachable code is code that can never be executed, regardless of the conditions at\nruntime. There is simply no control flow path that will lead to the code being executed. Code can become unreachable for a variety of reasons. Some examples include developers\nforgetting to delete obsolete code, code that was intentionally made unreachable so that it\ncould potentially be used later, code strictly for debugging/test purposes that was not\nremoved, modifications to other code that unknowingly made code unreachable, business\nlogic/data changes that made code unreachable, and a programming error. Static analysis tools can help you to find this type of code. If the unreachable code is the\nresult of a bug and it is determined after analysis that the code is actually needed, then the\ndefect should be corrected so that the code is no longer unreachable. However, if after\nanalysis it is confirmed that the code is unnecessary, then it should be removed. The\nremoval of unreachable code increases the overall maintainability of the software system. If\nunreachable logic was being kept in the code in case it would eventually be needed, version\ncontrol software can be used to obtain it again rather than leaving it in the code.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 496", "position": 496, "chunk_type": "semantic", "token_estimate": 368}
{"text": "Architecting Legacy Applications: Chapter 14\n[ 468 ]\nDead code\nSome people use the terms unreachable code and dead code interchangeably but there is a\nsubtle difference. While unreachable code can never be executed, dead code can be\nexecuted. However, when dead code is executed it has no effect on the output. For\nexample, dead code might perform some logic that produces a result but then the result is\nnot used anywhere. Commented-out code\nCommented-out code is code that is currently not in use and has been commented out by a\ndeveloper rather than deleted. Code that is commented out is among the easiest types of\ncode to remove. Sometimes a developer may comment out code with the intention of it only\nbeing temporary but then does not follow up later to either remove or uncomment the\ncode. Lines of code are sometimes commented out to serve as a record of previous logic. However, having a change history is one of the purposes of a version control system. Leaving code in the codebase that has been commented out only increases the size of the\ncode and makes it less readable. Duplicate code\nDuplicate code is identical (or very similar) code that exists in multiple places. The parts of \nyour codebase that are duplicated violate the Don't Repeat Yourself (DRY) principle. As\nwe learned in Chapter 6, Software Development Principles and Practices, following the DRY\nprinciple means eliminating duplication in our codebase. Duplication of code is wasteful\nand makes the code more difficult to maintain. Some of you may have had the experience\nof needing to make the same code change in multiple places because of duplicate code. Any instances of duplicate code should be abstracted out and placed in a single location. All of the places in the codebase that need the logic can then be routed through the\nabstraction. Eliminating code duplication will increase maintainability and the quality of\nthe software. When the logic must be modified, it can be done in one place, eliminating the\nrisk of missing one of the locations or not making a consistent change when the code is\nduplicated.", "domains": ["Design Principles", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 497", "position": 497, "chunk_type": "semantic", "token_estimate": 355}
{"text": "Architecting Legacy Applications: Chapter 14\n[ 469 ]\nUsing tools to refactor\nWhen possible, take advantage of development tools available to you that will assist with\nrefactoring a legacy application. Your organization may already have licenses for an\nintegrated development environment (IDE) or other tools that can identify areas of the\ncodebase that can be refactored. Some of them will perform some types of refactoring for\nyou and provide you with a preview of the changes before it actually makes them. If your\norganization doesn't have a particular tool, it may be possible to purchase licenses for it\nbased on your recommendations. If such purchases are not going to be possible (or simply\nbecause there are better options which happen to be free), consider using an open-source\ntool. Making small, incremental changes\nWhen refactoring a legacy application, some of the changes that you want to make may be\nlarge ones. However, keep in mind that in order to improve a legacy application, the\nchanges do not have to be large and they do not have to be made all at once. Refactoring a\nlegacy application can take time. Small, incremental changes can sometimes be the best\napproach to improving a legacy codebase. Write and execute unit tests to go along with\nyour changes to ensure that your modifications do not have unintended consequences. With each refactoring, we want to leave the code better than it was before, without\nchanging any of the functionality. You and your development team may not even be granted time to make improvements. If\none of you is tasked with some work, such as fixing a bug in the legacy application, take the\nopportunity to improve the area of the code that is being changed. Over time, more and\nmore parts of the code will be improved. Transforming monoliths to microservices\nYour legacy application may have a monolithic architecture. In Chapter 8, Architecting\nModern Applications, we learned that a monolithic application is designed to work as a\nsingle, self-contained unit. The code may have components that are tightly coupled and\nhighly interdependent. If the application is large and complicated, it can be difficult to\nmake changes to a monolithic application.", "domains": ["Architectural Patterns and Styles", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 498", "position": 498, "chunk_type": "semantic", "token_estimate": 364}
{"text": "Architecting Legacy Applications: Chapter 14\n[ 470 ]\nOne approach to modernizing a legacy application is to begin introducing microservices. Microservices are small, focused, autonomous services that are independently deployable. Unlike a monolithic application, the microservice approach reduces the level of complexity\nby partitioning an application into small services that are easier to maintain and manage. In\na legacy application, we can take pieces of logic and place them in microservices. Places in the legacy application that need the functionality contained in a microservice can\ninteract with it through its interface. Implementations of each service can be changed as\nlong as the interface remains the same. It also provides the ability for a development team\nto modify a microservice independently of other services. In contrast with modifications\nmade to a monolithic application, changes to individual microservices are less likely to\nrequire modifications in other places in the application. There is better fault tolerance with microservices. If a legacy application has a monolithic\narchitecture, a fault could bring down a large part of the application or even the entire\napplication. When a single microservice fails, it will not cause the entire application to\ncrash. Combined with proper monitoring, which will alert administrators to a fault quickly,\na microservice can be restarted quickly. If there is an issue with the currently deployed\nversion of a microservice, a new version that contains a fix can be deployed without having\nto deploy the entire application. It also provides the option of reverting to a prior version of\nthe microservice if it does not contain the issue. Migrating to the cloud\nMigrating a legacy application to the cloud is another way that it can be modernized. Although not every legacy application will be compatible with every cloud service, there\nare migration paths that will allow you to take most legacy applications to the cloud. Certain cloud services will be more suited to a particular application. There are a number of reasons why a legacy application will benefit from a move to the\ncloud. It can reduce costs while at the same time providing greater levels of availability and\nscalability. A cloud provider will be responsible for the hardware and infrastructure. Depending on the cloud model (for example, IaaS, PaaS, or FaaS), it may also be\nresponsible for the operating system and other services. Some legacy applications have security vulnerabilities because they rely on older hardware\nand software.", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 499", "position": 499, "chunk_type": "semantic", "token_estimate": 402}
{"text": "Architecting Legacy Applications: Depending on the cloud model (for example, IaaS, PaaS, or FaaS), it may also be\nresponsible for the operating system and other services. Some legacy applications have security vulnerabilities because they rely on older hardware\nand software. If they are migrated to the cloud, the cloud provider can potentially handle\ntasks such as operating system updates, including security patches.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 499", "position": 499, "chunk_type": "semantic", "token_estimate": 62}
{"text": "Architecting Legacy Applications: Chapter 14\n[ 472 ]\nReplatform\nSome legacy applications cannot be migrated to a cloud platform but an organization does\nnot want to simply retain them in their current environment. For this type of situation, the\napplication can be run on cloud-based IaaS servers using emulators. The application is\nemulated through a virtual machine to make it compatible with cloud technologies. This\napproach allows you to migrate to a newer platform/operating system and take advantage\nof its features. There are a number of tools and services available to assist with\nreplatforming. Rehost\nLift and shift is a migration strategy that involves moving a software application from one\nenvironment to another without redesigning the application. Rehosting is a lift and shift\napproach in which an application and its data are copied from physical or virtual servers to\nan IaaS solution. The application does not need to be changed and the cloud provider hosts\nthe infrastructure. This is a quick, low-risk, and easy way to migrate an application to the\ncloud. Rehosting can be the first step in a cloud migration for an application. Once the legacy\napplication is running in the cloud, it may be refactored later in order to optimize it for its\nnew environment. The fact that it is already hosted in the cloud may make that process\neasier. Repurchase\nRather than move a legacy application to the cloud, an organization might decide to\npurchase a newer product and start using that one instead. This option is commonly\nassociated with a move to a SaaS cloud model. Everything is provided for you with SaaS,\nincluding the hosting and the software. The organization only has to configure the software\nfor use. Refactor (or re-architect)\nWhen refactoring an application for a cloud migration, you are making modifications to the\napplication, possibly including its architecture. You may be refactoring the application to\ntake advantage of the cloud-native features available in its new environment.", "domains": ["Design Patterns"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 501", "position": 501, "chunk_type": "semantic", "token_estimate": 324}
{"text": "Architecting Legacy Applications: Chapter 14\n[ 473 ]\nLegacy applications that are refactored properly will have greater availability, improved\nscalability, and faster performance. In addition to these advantages, an organization can\nrealize cost benefits after the application has been migrated. The refactoring/re-architecting\napproach can potentially be quite involved though. Depending on what needs to be done,\nthe migration process itself may take longer and cost more to execute. Moving to an agile approach\nA legacy application does not just entail the technology that comes with it. It was\ndeveloped using a particular software development methodology. In some cases, that\nmethodology might still be in use. If the methodology used is not a modern one, such as the\nuse of a Waterfall methodology, part of modernizing a legacy application might include\nchanging the development approach. The agile methodology overcomes some of the limitations of older methodologies. While\ntraditional software development methodologies focus on a lot of up-front planning and\ndesign, an agile methodology expects and embraces change. Agile methodologies are\nadaptive rather than predictive. Rather than focusing on predicting the outcome, it places\nemphasis on adapting. It is more responsive to change, which is important in today's\ncompetitive landscape. An agile methodology will enable your team to accomplish some of the things we have\ndiscussed to improve a legacy application, such as refactoring it by making small,\nincremental changes. Each sprint can be focused on particular goals to improve the legacy\napplication. If you are introducing microservices, an agile methodology complements that approach\nwell. An agile approach provides a structure that will allow a development team to work\ntogether effectively. It is also particularly suited to modern build and deployment\nprocesses. Modernizing build and deployment\nprocesses\nAnother aspect of modernizing a legacy application is to update the build and deployment\nprocesses. If outdated, these processes cannot be completed as easily or quickly as modern\nprocesses. A legacy application might even need to be built and deployed manually, which\ncan make it error-prone.", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 502", "position": 502, "chunk_type": "semantic", "token_estimate": 331}
{"text": "Architecting Legacy Applications: Chapter 14\n[ 475 ]\nPracticing continuous delivery (CD)\nA legacy application may not have been designed for CD. In Chapter 13, DevOps and\nSoftware Architecture, we learned that continuous delivery is the ability to release software\nchanges to users quickly and in a reliable way. The build and deployment processes of a\nlegacy application can be slow and tedious, particularly if they are performed manually. If it is feasible to do so, a goal should be made to practice continuous delivery, so that the\nlegacy application will be in such a state that it can be deployed to production at any time. Automating the build process and executing automated unit tests as part of that process is\nan important step in enabling continuous delivery. A continuous delivery process for a legacy application should build and deploy the\napplication to staging automatically. If the organization desires to do so, continuous\ndelivery of the software system can be taken a step further by practicing continuous\ndeployment. If continuous deployment is enabled, then the last step of the process is\nautomated: deployment to production. Updating the build tools\nA legacy application may be using older build tools. Similar to other development tools,\nbuild tools are constantly evolving. If the legacy application you are working on is using an\noutdated build tool, there are probably better options available to you (or possibly newer\nversions of the tools already being used). Software architects should look to update outdated build tools to improve the build and\ndeployment processes. Good build and deployment tools can help you achieve your goals\nof practicing continuous integration and continuous delivery. Integrating with legacy applications\nSoftware architects may need to integrate a legacy application with another software\napplication. If you are responsible for the legacy application, refactoring it may be\nnecessary in order to modernize it to the point that it can be integrated with newer\napplications. When integrating a legacy application with another application, some considerations\nshould be made by the software architect.", "domains": ["Design Patterns"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 504", "position": 504, "chunk_type": "semantic", "token_estimate": 337}
{"text": "Architecting Legacy Applications: Chapter 14\n[ 477 ]\nSharing functionality between systems\nWhen a newer software application is integrated with a legacy one, some of the\nfunctionality may be duplicated across the two systems. Software architects should be\naware of any overlaps because decisions will need to be made as to which system will be\nresponsible for functionality that is shared. Any differences in shared business logic\nbetween the two systems should be identified, as those differences may play a factor in\nsuch a decision. Any logic that is redundant between the two systems is a candidate to be exposed and\nshared so that it is no longer duplicated. This will ensure consistency of the logic. Quality\nand maintainability will be increased as the logic will only need to be maintained and\ntested in one location. Performing data integration\nWhen integrating and sharing data between two systems, we have to ensure that data can\nbe combined from disparate sources in a meaningful way. There should be an\nunderstanding of the data in both systems and what they represent. We need the ability to\nidentify whether or not a piece of data represents the same thing in both systems. Data mapping between systems may be required to transform data from one system to\nanother. When possible, redundancy in data should be removed and decisions need to be\nmade as to which system will be responsible for certain pieces of data. Summary\nDespite the challenges of maintaining a legacy application, enterprises continue to use them\nfor a variety of reasons. Working on a legacy application is a common task for a software\ndevelopment professional. Most of us have worked on a legacy application, and if you have\nnot already, it is likely that you will at some point in your career. Software architects should\nbe knowledgeable about how to effectively oversee one.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 506", "position": 506, "chunk_type": "semantic", "token_estimate": 310}
{"text": "The Soft Skills of Software Architects: Chapter 15\n[ 481 ]\nA software architecture imposes some constraints on implementations and prevents\ndevelopers from making incorrect design decisions. The details of a software architecture\nmust be communicated to developers so that they can complete their tasks and understand\nthe constraints, to avoid developing components that do not conform to the architecture. Communicating about quality attributes\nA software architecture can both enable and inhibit software quality attributes. We learned\nabout quality attributes in Chapter 4, Software Quality Attributes. Examples of software\nquality attributes include maintainability, availability, performance, and security. A\nsoftware architect has to communicate details of how design decisions will affect the\nquality attributes of a software system. Communication with product owners, business\nanalysts, and other stakeholders might be necessary to make design decisions based on\nquality attribute tradeoffs. Communicating expectations\nSoftware architects will need to communicate with project management to assist with\nproject scheduling and resource planning. A software architect's input from a technical\nperspective is necessary for effective project planning. Once the project begins, a software architect will need to continue to communicate with\nmanagement on the status of the project. In some cases, a software architect may be\ninvolved with communicating expectations to one or more customers. If a project does not\nmeet the expectations of the client, it cannot be considered a success. An organization must\nensure that it is on the same page as its clients, and one way to do this is to keep them in the\nloop regarding the current state of the project. The goal is to eliminate surprises to ensure\nthat customers are satisfied. The 7 Cs of communication\nA set of tips regarding effective communication is known as the 7 Cs of communication. There is some variation among these tips, both in the number (for example, some list only\nfive Cs) as well as the terms used. The following section details some of the most common\ntips.", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 510", "position": 510, "chunk_type": "semantic", "token_estimate": 326}
{"text": "The Soft Skills of Software Architects: Chapter 15\n[ 489 ]\nPresent\nPrior to actually presenting, you should have already planned, prepared, and practiced\nyour presentation. When it actually comes time to present, your audience deserves a\nspeaker who is prepared, punctual, and keeps to the allotted time (if any). Dress\nappropriately based on the type of presentation and the audience. Start your presentation off right by making a good first impression. Establish eye contact,\nbe as relaxed as you can, and try to be enthusiastic. If you do not have a microphone, be\nsure to speak loudly enough so that you will be easily heard. During your presentation, use\nthe tips previously given for the 7 Cs of communication, such as being clear, concise,\nconcrete, courteous, considerate, correct, and complete. If something goes wrong during the presentation, do not panic. Do your best to recover and\nmove forward. Try to conclude your presentation on a positive note so that the audience\ncan leave with an upbeat impression. Leadership\nLeadership is a key skill for software architects. There is no single approach to leadership\nthat will fit every leader. You will have to discover for yourself the type of leadership style\nthat goes with your personality and skills. Similarly, no single leadership style works for everyone or every situation. The qualities\npeople look for in a leader will vary from person to person. A single approach to leadership\nmay not work for everyone you are leading. Try to be aware of the differences among the\npeople you are working with and tailor your leadership accordingly. Attempt to provide\nwhat each person needs in a leader while at the same time remaining yourself. Let's take a\ndeeper look at what it means to be a leader. Getting others to follow you\nA major component of being a good leader is how well you influence others in a positive\nway. Your actions, words, and overall attitude will influence others and it is your job as a\nleader to inspire your team to take positive and productive action. How do you get others to follow you? Leaders have certain qualities that inspire others to\nfollow them. You should work to earn respect and gain credibility among your colleagues. If you consistently deliver high-quality work, can be depended on, and are always there to\nhelp others, you will begin to earn respect and credibility.", "domains": ["Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 518", "position": 518, "chunk_type": "semantic", "token_estimate": 400}
{"text": "The Soft Skills of Software Architects: Chapter 15\n[ 493 ]\nMentoring others\nServing as a mentor to others on the team is part of being an effective leader. Being a\nmentor can mean different things but it is essentially making yourself available to support,\nadvise, and teach someone else. If you are open and available to help others, that is a\ncomponent of being a great leader. You should develop a relationship with anyone who you mentor because it will increase\nthe chances that a mentorship will succeed. In addition to imparting professional advice\nabout technical topics, do not forget to provide mentorship for soft skills, too! When you are in the process of mentoring someone, put more weight on their importance\nthan that of the organization. Encourage your mentee to discover and follow their goals\nand passions. Sometimes it might mean that an employee and job are not an ideal fit or that\nit might be time for the employee to go somewhere else to continue to grow. You should\ngive the best advice you can to your mentee, without worrying about how it will affect the\norganization. Mentorship does not just benefit the mentee. Most people enjoy the feeling they get from\nhelping others. You may have been mentored by someone in the past and it's a good feeling\nto be in a position to pay it forward. Being a mentor can also help you become better at\nyour own job, improve your leadership skills, and help you to gain credibility with your\ncolleagues. Leading by example\nLeading by example is one way to lead others. If you say one thing but do another, you lose\ntrust and credibility with those you are leading. As you go about your tasks, whether it is\nrequirement gathering, architecture design, technical guidance, coding, or interacting with\nstakeholders, approach them in the right way and with a good attitude. Set an example as\nyou perform your job, which will provide an opportunity for those around you to observe\nhow you perform your tasks. How you deal with different tasks, challenges, people, and situations will be observed by\nothers. Your employees will learn from you and will ultimately follow your lead. Taking\nresponsibility, delivering on your promises, and producing high-quality work will set an\nexample that others will want to follow.", "domains": ["Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 522", "position": 522, "chunk_type": "semantic", "token_estimate": 389}
{"text": "The Soft Skills of Software Architects: Chapter 15\n[ 494 ]\nDepending on others\nGood leaders are humble and are not afraid to admit when they don't know something. They should ask plenty of questions and shouldn't hesitate to lean on, and learn from,\nothers. If you become a software architect, you may feel the pressure of people expecting\nyou to have a wide range of technical knowledge. Don't avoid saying that you don't know\nabout a particular topic or be reluctant to ask questions when you are unclear about\nsomething or need further information. Sometimes people don't ask enough questions, either because they lack the time due to\ndeadlines, are hesitant to reveal that they don't know something, don't want to bother\nothers, or are simply lazy. Avoid being one of those people, because if you do not have a\ngood enough understanding of the work at hand, you may make incorrect decisions that\ncan be difficult to fix later. Everyone has different strengths and weaknesses, and you will not be an expert on every\ntopic. Your team will consist of a variety of individuals with their own strengths and\nweaknesses. Recognize the person who has more knowledge or experience on the topic in\nquestion and reach out to them. Collaboration with others will foster trust, which is a\nnecessary component of leadership. Listening to the suggestions of others may provide you with ideas you would not have\nthought of otherwise. It will give you different ways of looking at a problem and it will\nallow you to see things from another point of view. Keep an open mind and look forward\nto learning from those you lead. Negotiation\nAs a software architect, you will be involved in obtaining buy-in and making decisions\nwith your development team, customers, management, and other stakeholders. All of the\nindividuals involved in making a decision may not be in agreement initially, so having\nnegotiation skills is useful. Negotiating is a way to settle differences, and a successful one ends with an agreement that\nis acceptable to all. In some cases, a compromise is reached to settle a negotiation. A\nsuccessful negotiation is not just about achieving the best outcome from your point of view\nbut also about seeking a result that is fair to everyone.", "domains": ["Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 523", "position": 523, "chunk_type": "semantic", "token_estimate": 382}
{"text": "The Soft Skills of Software Architects: Chapter 15\n[ 499 ]\nAnother method of transferring knowledge to a remote employee is to have them on-site\nfor a small amount of time. It will give them an opportunity to learn from the team directly\nand they will be able to absorb some of the company's culture, norms, processes, and\nstandards. Once acclimated, remote resources can then potentially train other employees. For example, if a trained employee is an outsourced one, they can return to their own\ncompany's office and train fellow employees who are working on the same project. Work quality\nOne possible difficulty with outsourced resources is that the quality of work from these\nemployees can vary. This can be true of any resource, but an organization may have less\ncontrol over outsourced resources because they may not be directly involved in\ninterviewing, hiring, training, and managing the resources. A software architect should try to mitigate this risk. One way is to ensure that there are\nestablished code review and quality assurance processes. This can uncover defects, which\ncan then be corrected, and provide an opportunity to give employees feedback. If a\nparticular resource's work is not up to scratch, you can perhaps request that another\nresource replace that person. Confidential company data\nWhen you are working with outsourced resources, be careful about exposing confidential\ndata to another company. One way that this risk can be mitigated is through legal\nagreements. Another is to somehow mask, redact, or remove confidential data. Some data is sensitive\nand should be modified so that personally identifiable information (PII), such as names,\nsocial security numbers, address/phone/email information, and other data elements, is\nsomehow protected. Protecting sensitive data may even be applicable when the data will\nonly be seen by internal resources, but more careful consideration should be given when\nany data will be seen and used outside the company.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 528", "position": 528, "chunk_type": "semantic", "token_estimate": 315}
{"text": "Evolutionary Architecture: Chapter 16\n[ 503 ]\nSoftware architects should expect change and design their software systems so that they are\ncapable of withstanding and adapting to it. Designing a software system to have an\nevolutionary architecture is how we can handle inevitable change. In the Designing\nevolutionary architectures section later in the chapter, we will cover how to design software\narchitectures that are capable of adapting to change, but first let's look at the origins of\nsoftware evolution. Lehman's laws of software evolution\nSoftware evolution refers to the process of initially developing a software system and then\niteratively making changes to it. Beginning in the 1970s, Manny Lehman and his colleagues\nstudied the evolution of software systems. They identified a set of behaviors that became\nknown as Lehman's laws of software evolution. Lehman's dedication to the study of\ncontinuous software modifications and their long-term effects earned him the nickname the\nfather of software evolution. Lehman's software categories\nLehman's laws take into consideration the fact that there are different types of software\nsystems. In his paper Programs, Life Cycles, and Laws of Software Evolution, Lehman\ndistinguished between three different types of system:\nS-type systems\nP-type systems\nE-type systems\nIn this section, let's look at these three types of system and learn which one is applicable to\nLehman's laws of software evolution. S-type systems\nAn S-type system is specifiable in that it has a well-known, exact specification and can be\ndeveloped to that specification. It can be described formally and the solutions to such\nsystems are well understood. It is not only possible to definitively determine whether the\nprogram is correct, it is also possible to provide a completely correct solution. The\nrequirements of S-type systems are unlikely to change and they do not evolve.", "domains": ["Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 532", "position": 532, "chunk_type": "semantic", "token_estimate": 294}
{"text": "Evolutionary Architecture: Chapter 16\n[ 504 ]\nAn example of such a system is a calculator program or a program that performs very\nspecific mathematical computations. These types of program have logic that will not\nchange. Systems of this type are the simplest of the three types and are rare. As there is\nlittle chance of change with these systems, Lehman's laws do not apply to them. P-type systems\nA P-type system is one in which the problem may be precisely stated. The end result may be\nwell known and it may even be possible to create an exact specification for the system. However, unlike an S-type system, either the solution is not well understood or it is simply\nimpractical to implement a solution. A common example of P-type systems is a program that can play chess by always making\nthe best possible move with each turn. While it is theoretically possible to develop all of the\nsets of logic to determine what the program can do, in practice, it will not be feasible. The\ncomplexity of the logic is so high that the system will take too much time to calculate each\nmove. If we were not allowed to apply heuristics to reduce the computational effort and\ntake logical shortcuts, the solution would not be practical. Lehman's laws also do not apply\nto P-type systems. E-type systems\nAn E-type, or embedded type, system is modeled after real-world processes and people. The\nmajority of software systems are E-type systems. The term embedded does not mean the\nsoftware is embedded in some device but rather that the system is embedded in the real\nworld. An E-type system affects the world it is in, which may create an evolutionary pressure for\nchange. In addition, the world that is being modeled may undergo change. The needs of the\nbusiness or its users may change, which will require the system to change. E-type systems\nmust evolve in order to remain useful. For these reasons, Lehman's laws of software\nevolution apply to E-type systems. The laws\nObservations and behaviors related to the evolution of software that Lehman and his\ncolleagues identified are known as Lehman's laws of software evolution. There are eight laws:\nLaw I: Continuing change\nLaw II: Increasing complexity", "domains": ["Design Patterns", "Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 533", "position": 533, "chunk_type": "semantic", "token_estimate": 376}
{"text": "Evolutionary Architecture: Chapter 16\n[ 505 ]\nLaw III: Self-regulation\nLaw IV: Conservation of organizational stability\nLaw V: Conservation of familiarity\nLaw VI: Continuing growth\nLaw VII: Declining quality\nLaw VIII: Feedback system\nLet's look at each law in more detail. Law I \u2013 Continuing change\nSoftware systems must go through continuous change or they will progressively become\nless useful. If a software system does not adapt to the changing needs of the business and\nusers, satisfaction with it will progressively decrease. Law II \u2013 Increasing complexity\nOver time, as a software system evolves and the number of changes performed on it\nincreases, the complexity of the software system will increase as well unless effort is taken\nto reduce that complexity. The concept of software entropy was discussed in Chapter 14,\nArchitecting Legacy Applications. Disorder in a software system, known as software entropy,\nincreases as the number of modifications to the system increases. Law III \u2013 Self-regulation\nThe evolution of a software system is self-regulating. Particularly with large systems, there\nare structural and organizational factors that affect and constrain changes to a software\nsystem. Structural factors include the size and complexity of the software system. As a\nsoftware system grows, it becomes larger and more complex, making it increasingly more\ndifficult to make more changes. For this reason, as a typical software system grows older,\nits growth will inevitably slow down. Organizational factors such as gaining consensus on decisions and getting approvals to\nmove forward with proposed changes influence the amount of change that can actually\ntake place in a software system.", "domains": ["Design Patterns"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 534", "position": 534, "chunk_type": "semantic", "token_estimate": 262}
{"text": "Evolutionary Architecture: Chapter 16\n[ 509 ]\nCategories of \ufb01tness functions\nThere are different categories of fitness function. They are not mutually exclusive. For\nexample, a fitness function could be both atomic and temporal. The different fitness\nfunction categories include:\nAtomic versus holistic\nAtomic fitness functions focus on a single context and on one architectural characteristic. For example, a unit test that is designed to test one architectural characteristic is atomic. A\nholistic fitness function takes multiple architectural characteristics into consideration at the\nsame time. Having both atomic and holistic fitness functions is useful because a feature that works fine\nwhen tested atomically could fail when it is combined with other features. It isn't feasible to\ntest every combination of all the architecture's characteristics, but a software architect can\nchoose which important combination of characteristics should be tested. Triggered versus continuous\nTriggered fitness functions are executed based on some event. For example, they could be\ntriggered as part of a build or unit test. A continuous fitness function runs constantly and\nits execution is not based on the occurrence of some event. An example of a continuous\nfitness function is a test, which monitoring tool may be executing constantly, that will\nproduce an alert when a certain condition is met. Static versus dynamic\nA static fitness function is one in which the value for the condition that we are testing for is\nconstant. A test may be looking to ensure that the result is less than some static numeric\nvalue or that a test that returns true or false returns the value that we expect. In contrast,\nthe acceptable values of a dynamic fitness function may change based on a different\ncontext. For example, the desired result of a performance test might be different depending\non the current level of scalability. At a much higher level of scalability, a lower level of\nperformance might be acceptable. Automated versus manual\nAutomated fitness functions are triggered automatically. They could be part of automated\nunit testing or an automated build process. When possible, automated fitness functions are\nideal. However, there may be times when you may either need or want to execute a fitness\nfunction manually.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 538", "position": 538, "chunk_type": "semantic", "token_estimate": 363}
{"text": "Evolutionary Architecture: Chapter 16\n[ 510 ]\nTemporal\nTemporal fitness functions are based on a designated amount of time. While other fitness\nfunctions may focus on a change in the system, temporal ones are triggered based on time. For example, a fitness function may be created to ensure that, if a patch becomes available\nfor a framework that is being used, it is applied within a certain number of days. Intentional versus emergent\nMany fitness functions can be defined early on in a project, once some of the characteristics\nof the architecture are known. These are known as intentional fitness functions. However,\nsome characteristics of the architecture are not known right from the beginning, but emerge\nas the system continues its development. These fitness functions are known as emergent\nones. Domain-specific\nDomain-specific fitness functions are based on specific concerns related to the business\ndomain. Some examples include regulatory, security, and personally identifiable\ninformation (PII) requirements. A domain-specific fitness function can ensure that the\narchitecture continues to conform to these requirements. Examples of \ufb01tness functions\nFitness functions may come in the form of tests (automated or manual), monitoring, and the\ncollection of metrics. Not all tests are fitness functions. Only those that actually assess a\nparticular architectural characteristic are fitness functions. For example, fitness functions can be created to calculate and use various software metrics\nto determine whether a software architecture continues to meet maintainability\nrequirements. In Chapter 4, Software Quality Attributes, we discussed software metrics, such\nas cyclomatic complexity, lines of code (LOC), and the depth of inheritance tree (DIT) as\nmeasurements for maintainability. Fitness functions can allow you to know when a\nsoftware system has exceeded its predefined acceptable levels for these metrics, providing\nyou with an opportunity to analyze recent changes and determine whether refactoring is\nnecessary. Performance tests can be executed to ensure that the architecture continues to meet its\nrequirements and that any recent changes to the software system have not negatively\nimpacted its performance. Security tests can focus on the security dimension of a software\nsystem to ensure that changes have not introduced a security vulnerability.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 539", "position": 539, "chunk_type": "semantic", "token_estimate": 351}
{"text": "Evolutionary Architecture: Chapter 16\n[ 512 ]\nIn Chapter 13, DevOps and Software Architecture, we discussed CI and CD. An aspect of\nmaking incremental changes is having developers continuously integrate their changes into\nthe system. By committing changes to a source control repository frequently, there is a\nreduced chance of merge conflicts, making it easier to resolve any that do occur. Automated\nbuilds that include automated testing give us feedback on our changes quickly. If there is a\nproblem with a set of changes, they will be easier to fix because there will only have been so\nmany changes committed since the last build. CD is the practice of being able to release changes into production in a safe, repeatable, and\nsustainable way. A system that is highly adaptable has the ability to quickly react to change\nand release new versions quickly. Releasing incremental changes to users quickly makes\nyour system highly evolvable. CD is about being able to produce low-risk, high-quality\nreleases with a faster time-to-market. Architectural changes across multiple\ndimensions\nThe final part of the definition for evolutionary architecture put forth by Rebecca Parsons,\nNeal Ford, and Patrick Kua is that an evolutionary architecture is one that can support\nchange across multiple dimensions. A software system consists of different dimensions and\na software architect must consider all of them to build a system that can evolve. Software architects may naturally focus on the technical aspects of an architecture, such as\nits programming language, frameworks, and third-party libraries. However, software\narchitects must also concern themselves with other aspects of the software system. The\ndatabase (and its data), security, performance, and its deployment environment are all\nexamples of different dimensions of the software system. To successfully maintain an\nevolvable architecture, the software architect must consider all of these dimensions as\nchanges are made to it. Loosely coupled architectures\nWhen designing an evolutionary architecture, its components should be loosely coupled. In\nChapter 6, Software Development Principles and Practices, we covered the importance of\nloosely coupled code. Coupling refers to the degree of dependency between components.", "domains": ["Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 541", "position": 541, "chunk_type": "semantic", "token_estimate": 344}
{"text": "Evolutionary Architecture: Chapter 16\n[ 513 ]\nWhen modules are tightly coupled, it is more difficult to make changes. Changes to one\ncomponent have a greater chance of affecting other components, increasing the total\nnumber of changes that have to be made. When you need to modify a tightly coupled\nsystem, more time and effort will be required for development and testing. To ease the effort of making changes to the software system, dependencies between\ncomponents should be minimized and all components should be designed so that they are\nas independent of each other as possible. Loosely coupled components reduce complexity\n(and typically increase cohesion), making your architecture more maintainable. One example of loose coupling for an evolutionary architecture is to loosely couple the\nsoftware system's cross-cutting concerns from the logic of the application's other concerns. By decoupling this type of logic, each cross-cutting concern can evolve separately from any\nlogic that uses it. As we learned in Chapter 9, Cross-Cutting Concerns, logic for cross-cutting\nconcerns may be needed throughout an application, so you want it to be loosely coupled to\nthe other parts of your application. This will reduce code duplication and increase\nmaintainability. One way to do this is to make each cross-cutting concern its own service. Logic that needs\none of those services will be dependent on its interface, allowing the implementation to\nchange without affecting other parts of the application. This will allow cross-cutting\nconcerns, such as caching and logging, to evolve over time. Designing evolvable APIs\nAnother aspect of designing an evolutionary architecture is the proper design of the API. We cannot anticipate what changes may become necessary for an API. Message contracts\nthat are used to communicate with an API may require modifications as a software system\nadapts to change. Some examples of the changes that can be made to an API include the\namount of information that is sent or received, the names or data types of individual pieces\nof data, and the introduction of new representations of data to support different types of\nclient. Maintaining a system that can evolve requires that changes made to an API, particularly\nonce it has been published, are given proper consideration. Your APIs should be designed\nto support evolution, allowing them to adapt to changes without breaking clients that\nalready depend on them.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 542", "position": 542, "chunk_type": "semantic", "token_estimate": 387}
{"text": "Evolutionary Architecture: Chapter 16\n[ 514 ]\nApplying Postel's Law to APIs\nA helpful design guideline with regards to message contracts is the Robustness Principle,\nwhich is also known as Postel's Law. The idea behind Postel's Law is that you should be\nconservative in what you do and liberal in what you accept from others. This principle was\noriginally proposed when designing the TCP protocol, but it is applicable to message\ncontracts. We should be conservative in what we send but liberal in what we accept. In\nother words, the data that is sent out of our system should be kept to the minimum that is\nnecessary. A message sender must conform to the message contract, and reducing the\namount of data that an API exposes lessens the chances that a breaking change may be\nnecessary for the contract. In terms of being liberal in what is accepted, when an API is consumed by a software\nsystem, it should only extract what is needed from a message and ignore the parts that are\nnot needed. Taking this approach minimizes what a software system depends on from a\nparticular message and increases its resilience to change. If the parts that are not needed are\nchanged in the future, the software system will not be affected. Using standards in your software system\nUsing standards in your software system helps to create an evolutionary architecture. When making technology choices and design decisions, leveraging standards can make it\neasier to adapt the software system over time. Using a standard, such as a programming\nlanguage, framework, third-party library, communication protocol, database, data\ninterchange format, development tool, or some other design choice, can increase\nmaintainability and allow your system to evolve more easily. The use of standards generally makes integration with other systems easier to accomplish. In addition, finding resources to work on your software system who are familiar with the\ntechnologies that you have selected, not just during the initial development but during a\npotentially long maintenance life will be easier. When faced with different alternatives, electing to use a standard approach may not always\nbe the best choice. However, it is one factor that a software architect should consider given\nthe possible benefits.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 543", "position": 543, "chunk_type": "semantic", "token_estimate": 369}
{"text": "Evolutionary Architecture: Chapter 16\n[ 515 ]\nLast responsible moment (LRM)\nThe last responsible moment (LRM) is the strategy of delaying a decision until the \nmoment when the cost of not making the decision is greater than the cost of making it. Design decisions made for a software architecture can be among the most important for a\nsoftware system and they can be among the most difficult to change later. With traditional software architectures, decisions were made very early in the project. In\norder to design an evolutionary architecture, it is beneficial to delay a decision until the\nLRM. This lessens the possibility that a premature decision will be made. Decisions that are\nmade too early are very risky because they may end up being incorrect and then they will\nresult in costly rework. As long as the cost of delaying the decision is not greater than the cost of making one,\nwaiting as late as you can to commit to a decision ensures that you will have the most\ninformation available to you. This will allow you to make an informed decision. The challenge with the LRM strategy is that it can be difficult to determine when the LRM\nis going to take place. Costs and benefits are subject to change and it is not always clear\nwhen the optimal time is to make a decision until after that moment has passed. It is\ngenerally not as beneficial, if at all, to delay decisions that are not so important to the\narchitecture. However, for important decisions, you will want to gather as much\ninformation as needed so that you can make a good decision when the appropriate time\narrives. Summary\nChanges to a software system are inevitable as there is a variety of reasons why an\napplication needs to be changed. Software architects should expect change and design their\nsoftware architecture with that in mind. To create an evolutionary architecture that is capable of adapting to change, software\narchitects should guide architecture modifications to ensure that the characteristics of the\narchitecture and its level of quality remain the same. Fitness functions can be used to help\ndetermine whether the architecture continues to achieve the required architectural\ncharacteristics.", "domains": ["Design Patterns"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 544", "position": 544, "chunk_type": "semantic", "token_estimate": 368}
{"text": "Becoming a Better Software Architect: Chapter 17\n[ 526 ]\nMentoring involves advising, supporting, and teaching someone else. While mentoring is\nbeneficial to your mentees, it also provides advantages to you as the mentor. Not only will\nit make you feel good, but it improves your leadership skills and gives you more credibility\nwith your colleagues. Be sure to listen to your mentees so that you understand what they want to learn from you. Software architects can teach and mentor others not just by focusing on technical skills but\nalso by giving advice about the organization, office politics, and soft skills. Using your own\npersonal experiences, you can assist others by instructing them on topics such as their\ncareers. Raise the expectations of your mentees. The expectations that you have for them (and that\nthey have for themselves) will have an impact on their performance. Raised expectations\nshow that you have confidence in them and will encourage them to move outside their\ncomfort zones, which is where true growth can happen. Trying new technologies\nGood software architects care about their craft and enjoy trying out new technologies and\ntechniques. This does not necessarily mean that you need to be an early adopter of new\ntechnologies in your applications but it is important to have an understanding of trending\ntechnologies. Software architects should learn a new technology, framework, language, tool, or other\ntechnology so that they understand the situations in which these should be applied. The\nmore technologies you are familiar with, the greater the number of tools you have at your\ndisposal. As you encounter different problems or want to take advantage of certain\nopportunities, having knowledge of a diverse array of tools will give you the power to\nselect the most ideal one for a particular job. It will give you the ability to speak\nintelligently about different alternatives and allow you to articulate why one solution is\nbetter than another. We have discussed ways in which you might go about learning new things, and these\ntechniques can be applied to gaining an understanding of new technologies. We want to go\nbeyond just being aware of these technologies. In order to understand them more deeply, a\nsoftware architect should try them out. Either as part of your job or in your own time, try out new technologies. Many of them will\nbe free for you to try.", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 555", "position": 555, "chunk_type": "semantic", "token_estimate": 398}
{"text": "Becoming a Better Software Architect: Chapter 17\n[ 527 ]\nSupplement your other types of learning by getting hands-on experience with the\ntechnology. Use a real or contrived problem and attempt to use the technology to solve it. You can create a proof of concept (POC), which is a prototype of a solution to determine its\nfeasibility for a real application. As you get to know it better, you will form opinions about its applicability, ease of use, and\nother characteristics. You will begin to understand the advantages and disadvantages of the\ntechnology. When there comes an opportunity to use it, you will be able to give an\ninformed opinion on whether or not it should be used. Continuing to write code\nIn order for software architects to continuously improve, they should continue to write\ncode. In Chapter 1, The Meaning of Software Architecture, we discussed ivory-tower software\narchitects. They are architects who are isolated in some way from the rest of the team. Rather than having a hands-on approach, they design solutions at a high level but do not\nget involved with the actual coding of solutions. If you do not exercise a particular skill, such as coding, over time you begin to lose it. You\nmay also grow out of touch with what is involved in the implementation of different types\nof solution. You will lose your understanding of the challenges and issues that are facing\nyour developers. Assigning yourself coding tasks\nOne way to stay involved with coding is to have some of the development tasks on your\nproject assigned to you. Having coding tasks assigned to you will allow you to stay close to\nthe rest of the development team. If there are no plans to have the software architect code, speak with your project manager\nto explain what you want to do. To accommodate your other responsibilities, you can agree\nto keep your coding assignments limited so that they do not take up all of your time. Working on your own project\nWe have already discussed working on open source projects and trying out new\ntechnologies. Both of these activities will allow you to keep your programming skills sharp. In addition to those, working on your own side project is another way you can grow as a\nsoftware architect.", "domains": ["Design Patterns"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 556", "position": 556, "chunk_type": "semantic", "token_estimate": 384}
{"text": "Becoming a Better Software Architect: Chapter 17\n[ 530 ]\nMeeting new people\nMeeting new people in your industry is another benefit of attending user groups and\nconferences. Discussing your profession with people outside your current professional and\nsocial circles exposes you to different viewpoints and ideas. If the only people you interact\nwith regarding your craft are on your development team, this limits the range of insights\nthat you can gain. Networking before and after user groups and conference sessions will increase your\npersonal network and can lead to additional opportunities. Discussing your experiences,\nincluding some of the challenges that you have faced, with other industry veterans can give\nyou new ideas and insights. Taking responsibility for your work\nSoftware architects who excel in their role take responsibility for their work. As a leader of\nthe team, it is important to take responsibility for your work and not make excuses. It is\neasy to take responsibility when things go well, but being a leader means accepting\nresponsibility when things do not go well. When things go wrong, rather than making\nexcuses or blaming others, it is more productive to use that time and energy to come up\nwith options for how to alleviate the issue. I think that most people, including myself, really\nrespect colleagues who take ownership and responsibility. If everyone on the team takes responsibility, that type of collective attitude prevents\nsoftware rot or disorder within a software system. Degradation of software can occur for a\nvariety of reasons. One typical cause is the overall culture of the work environment. If a\nknown issue is left unfixed, it becomes easier to leave other issues unresolved. Even if there\nis not sufficient time to resolve an issue immediately, take steps to protect the rest of the\nsoftware from the offending code, such as commenting it out. As a software architect, you\ncan create a culture that makes degradation unacceptable. Sometimes problems are small and happen slowly over time. It can be easy not to notice\nthem at first but, eventually, problems can get out of hand. The team should be\ncontinuously aware of the project's current state and of the big picture. Software architects\nshould take the lead to ensure degradation does not take the team by surprise.", "domains": ["Design Patterns"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 559", "position": 559, "chunk_type": "semantic", "token_estimate": 380}
{"text": "Other Books You May Enjoy: [ 534 ]\nArchitectural Patterns\nPethuru Raj, Anupama Raman, Harihara Subramanian\nISBN: 978-1-78728-749-5\nUnderstand how several architectural and design patterns work to systematically\ndevelop multitier web, mobile, embedded, and cloud applications\nLearn object-oriented and component-based software engineering principles and\npatterns\nExplore the frameworks corresponding to various architectural patterns\nImplement domain-driven, test-driven, and behavior-driven methodologies\nDeploy key platforms and tools effectively to enable EA design and solutioning\nImplement various patterns designed for the cloud paradigm", "domains": ["Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 563", "position": 563, "chunk_type": "semantic", "token_estimate": 79}
{"text": "Index: 6\n6 R's\n   about  471\n   refactor  472\n   rehost  472\n   remove  471\n   replatform  472\n   repurchase  472\n   retain  471\n7\n7 Cs of communication\n   about  481\n   clarity  482\n   completeness  484\n   conciseness  482\n   concreteness  483\n   consideration  483\n   correctness  484\n   courteousness  483\nA\nAAA pattern\n   about  204\n   Act  204\n   Arrange  204\n   Assert  204\naccessibility\n   designing for  88\nAccount base class  406\nACID consistency model\n   about  354\n   atomicity  354\n   consistency  354\n   durability  355\n   isolation  354\nactive design review (ADR)\n   about  429, 430\n   documentation for review, preparing  430\n   questionnaires, designing  431\n   review, conducting  431, 432\n   reviewers, identifying  431\n   specialized reviews, identifying  431\n   steps  430\nactive reviews of intermediate designs (ARID)\n   about  432\n   participant roles  432\n   phases  432\nactivity diagrams, UML\n   about  417, 418\n   actions/control flow  419\n   decision/merge nodes  419\n   fork/join nodes  420\n   start/end nodes  418\nadaptability  99\nAdvanced Message Queueing Protocol (AMQP) \n266\nadvantages, cloud\n   automatic updates  286\n   cost reduction  286\n   disaster recovery  287\n   greater flexibility and scalability  286\nadvantages, serverless architectures\n   core products, building  279\n   cost savings  279\n   polyglot development  280\n   scalable and flexible  279\nAgile approach  473\nAgile Manifesto  33\nAgile software development methodologies\n   about  32\n   adaptive  34\n   Agile principles  33", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 565", "position": 565, "chunk_type": "semantic", "token_estimate": 188}
{"text": "[ 538 ]: project decision makers  424\n   questioner  424\n   scenario scribe  424\n   stakeholders  424\n   team leader  424\nATAM phases\n   evaluation  425, 428\n   follow up  429\n   partnership and preparation  425\nattribute-driven design (ADD)\n    current design analysis, performing  149\n   about  111, 145, 146\n   architectural elements, instantiating  148\n   design concepts, selecting  148\n   design decisions, recording  149\n   input, reviewing  147\n   input, selecting  147\n   interfaces, defining  148\n   iterating process  149\n   iteration goal, establishing  147\n   iteration goal, reviewing  149\n   responsibilities, allocating  148\n   system elements, selecting  148\n   views, sketching  149\nauthentication, IAM\n   about  377\n   multi-factor authentication (MFA)  378\n   two-factor authentication (2FA)  378\nautomated builds\n   about  445, 446\n   software versioning  446, 447\navailability\n   about  91\n   based on request success rate, calculating  93\n   based on time, calculating  92\n   errors  93\n   failures  93\n   faults  93\nAzure Container Service (AKS)  288\nB\nbacklog, prioritizing\n   DIVE criteria  164\nBASE consistency model\n   about  355\n   basic availability  355\n   eventual consistency  355\n   soft state  355\nbeats\n   about  317\n   Auditbeat  317\n   Filebeat  317\n   Heartbeat  317\n   Libbeat  317\n   Metricbeat  317\n   Packetbeat  317\n   Winlogbeat  317\nbehavioral modeling  401\nbest alternative to a negotiated agreement\n(BATNA)  496\nbest practices, for team success\n   about  201\n   deliverables, reviewing  209\n   development environments, setting up  206, 207\n   pair programming  208\n   unit testing  201\nBig Design Up Front (BDUF)  118, 501\nbits per second (bps)  324\nblack box  263\nblockers  80\nblog\n   starting  523, 524\n   visibility, increasing  523\n   writing  522\nbottlenecks  328\nbottom-up approach\n   about  119\n   advantages  120, 121\n   disadvantages  120\n   using  121, 122\nbounce rate  322\nbounded contexts  61, 63\nbrownfield software system  124\nbuild and deployment processes\n   automating  474\n   modernizing  473, 474\nbuild tools\n   updating  475\nbuilt-in self-test (BIST)  95\nbuilt-in test (BIT)  95", "domains": ["Design Patterns", "Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 567", "position": 567, "chunk_type": "semantic", "token_estimate": 272}
{"text": "[ 539 ]: bundling  338\nburst compute pattern  454\nbusiness acumen\n   developing  53\nbusiness entity services  253\nBusiness Process Execution Language (BPEL) \n227\nBusiness Process Manager (BPM)  227\nBusiness Process Model and Notation (BPMN) \n227\nBusiness Tier  223\nbusiness topics  54\nC\ncache usage patterns\n   about  332\n   read-through pattern  332\n   write-behind pattern  333\n   write-through pattern  333\ncached data, invalidating\n   data, evicting  331\n   data, expiring  331\ncaching  310", "domains": ["Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 568", "position": 568, "chunk_type": "semantic", "token_estimate": 67}
{"text": "about  437: 437\n   culture  438, 439\n   Lean  440\n   measurement  441\n   sharing  441\nCAP theorem\n   about  353\n   availability  353\n   consistency  353\n   partition tolerance  353\nCascading Style Sheets (CSS)  183\ncategories, fitness functions\n   atomic, versus holistic  509\n   automated, versus manual  509\n   domain-specific  510\n   intentional, versus emergent  510\n   static, versus dynamic  509\n   temporal  510\n   triggered, versus continuous  509\ncentralized identity provider (IdP)\n   implementing  380\nchallenges, remote resources\n   communication  498\n   confidential company data  499\n   cultural differences  498\n   impromptu meetings  498\n   new employee onboarding  498, 499\n   work quality  499\nchange control board (CCB)  47\ncharacteristics, MSA\n   autonomous and independently deployable\nservices  265\n   communicating, with lightweight message\nprotocols  266\n   fault isolation  266\n   focused services  264\n   independent data storage  265\n   well-defined service interfaces  265\nCIA triad\n   about  357, 359\n   availability  360\n   confidentiality  359\n   integrity  359\ncipher  375\nciphertext  374\nclass diagrams, UML\n   about  401\n   aggregation  403\n   association  402, 403\n   composition  404\n   dependency  405\n   generalization  405\n   multiplicity  404\n   realization  406, 407\n   visibility  402\nclient-server architecture\n   about  220\n   stored procedures, using for application logic \n221, 222\ncloud architect  28\ncloud bursting  454\ncloud models\n   about  454, 455\n   Containers as a Service (CaaS)  456, 458\n   Function as a Service (FaaS)  459\n   Infrastructure as a Service (IaaS)  455, 456", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 568", "position": 568, "chunk_type": "semantic", "token_estimate": 198}
{"text": "[ 540 ]: Platform as a Service (PaaS)  458\n   Software as a Service (SaaS)  459\nCloud Native Computing Foundation (CNCF)  287\ncloud types, DevOps\n   hybrid cloud  453, 454\n   private cloud  452, 453\n   public cloud  452\ncloud-native applications\n   about  285, 287\n   containerized  287, 288\n   continuous delivery  289\n   dynamically orchestrated  288\n   microservices-oriented  289\n   migrating, to cloud  286\n   no downtime  289\n   twelve-factor apps  290\n   variety of devices support  290\nclustered table  349\ncohesion\n   about  172\n   coincidental cohesion  173\n   communicational cohesion  174\n   designing  175\n   functional cohesion  174\n   logical cohesion  173\n   procedural cohesion  173, 174\n   sequential cohesion  174\n   temporal cohesion  173\n   types  172\ncold start  283\nCommand Model  239\nCommand Query Responsibility Segregation\n(CQRS)\n   about  213, 238\n   advantages  240\n   command model  239\n   disadvantages  241\n   event-sourcing, using  240\n   query model  239\ncommented-out code  468\ncommercial off-the-shelf (COTS)  73\ncommon web application security risks\n   about  385\n   broken access control  388\n   broken authentication  385\n   components, using with known vulnerable\ncomponents  390\n   cross-site scripting (XSS)  389\n   injection  385\n   insecure deserialization  389, 390\n   insufficient logging and monitoring  390\n   security misconfiguration  388\n   sensitive data exposure  386\n   unvalidated redirects and forwards  391\n   XML external entity (XXE) attack  386\ncomplex event processing (CEP)  228, 229\ncomplexity\n   minimizing  175\ncomponent under test (CUT)  105\ncompression\n   about  335\n   content-encoding (end-to-end) compression  337\n   file compression  335\nConcreteComponent  303\nconfiguration management\n   about  46, 311\n   changing  47, 48\nContainers as a Service (CaaS)  456, 458\ncontent delivery networks (CDNs)\n   about  344\n   using  344\ncontinuous delivery (CD)\n   about  448, 474, 511\n   practicing  475\ncontinuous deployment  448, 449\ncontinuous integration (CI)\n   about  47, 445, 474, 511\n   automated builds  445, 446\n   automated testing  447\n   practicing  474\ncore assets  50\ncore concern  298\ncreate, read, update, and delete (CRUD)  238\ncritical rendering path (CRP)\n   optimizing  346, 347\ncross-cutting concerns implementation\n   about  301\n   decorator pattern, using  302, 304, 306\n   dependency injection (DI), using  301\ncross-cutting concerns, types", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 569", "position": 569, "chunk_type": "semantic", "token_estimate": 304}
{"text": "[ 541 ]: auditing  312\n   caching  310\n   configuration management  311\n   exception management  312\n   logging  313\n   security  312\ncross-cutting concerns\n   about  298\n   for microservices  317\n   guidelines  299\n   types  310\ncross-site scripting (XSS)\n   about  389\n   DOM XSS  389\n   reflected XSS  389\n   stored XSS  389\ncryptographic hash function\n   about  376\n   collision resistant  377\n   deterministic  376\n   one-way function  376\n   quick  376\ncryptography\n   about  374\n   encryption  374\n   hash functions  375\nCrystal  33\nCSS Object Model (CSSOM)  346\ncustomer relationship management (CRM)  459\ncyclomatic complexity  84, 85, 510\nD\ndaily scrum  35\ndata architect  26\ndata caching, in distributed applications\n   about  330\n   private caching strategy, using  330\n   shared caching strategy, using  330\ndata Tier  224\ndata-transfer object (DTO)  238\ndatabase administrator (DBA)  328\ndatabase concurrency\n   about  352\n   ACID model  354\n   BASE model  355\n   CAP theorem  353\n   database transactions  352\n   optimistic, versus pessimistic concurrency\ncontrol  352, 353\ndatabase indexes\n   many indexes  350\n   primary/clustered indexes  349\n   secondary/non-clustered indexes  350\n   using  349\ndatabase management system (DBMS)  81, 351\ndatabase performance\n   about  347\n   database concurrency  352\n   database indexes, using  349\n   efficient database schema, designing  347\n   scaling out  351\n   scaling up  351\ndead code  468\nDecorator class  303\ndefault deny approach  373\ndeliverables\n   code reviews  209, 210\n   formal inspections  210\n   reviewing  209\n   walkthrough  212\ndelivery strategies, SOA\n   agile strategy  250\n   bottom-up strategy  249\n   top-down strategy  249\ndenial-of-service (DoS) attack  363\nDependency Injection (DI)\n   about  196, 297\n   benefits  196\n   containers  201\n   patterns  197\nDependency Inversion Principle (DIP)\n   about  167, 194\n   Dependency Injection (DI)  196\n   Inversion of Control (IoC)  195\ndepth of inheritance tree (DIT)  86, 510\ndesign principles\n   design concept, selecting  129\n   externally-developed software  132\n   externally-developed software, buy or build\nscenario  133", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Design Principles", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 570", "position": 570, "chunk_type": "semantic", "token_estimate": 270}
{"text": "domain sharding  340: domain-driven design (DDD)\n   about  56, 265\n   benefits  56\n   bounded contexts  61, 63\n   domain, separating, into subdomains  60\n   fundamentals  59\n   ubiquitous language  57, 59\ndomain-specific language (DSL)  227\nDon't Repeat Yourself (DRY) principle\n   about  176, 178, 300, 468\n   copy-and-paste programming  178\n   duplication, avoiding  180\n   magic strings  178\n   overly DRY things  180\nDREAD risk assessment model\n   about  364\n   affected users  365\n   damage potential  364\n   discoverability  365\n   exploitability  365\n   reproducibility  364\nduplicate code  468\nduties, software architect\n   technical leaders  17\nE\nefficient database schema, designing\n   appropriate data types, selecting  349\n   database, denormalizing  348\n   database, normalizing  347\n   primary and foreign keys, identifying  348\nElastic Stack\n   about  315\n   beats  317\n   Elasticsearch  315\n   Kibana  316", "domains": ["Design Principles", "Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 571", "position": 571, "chunk_type": "semantic", "token_estimate": 112}
{"text": "[ 543 ]: Logstash  316\n   using  315\nElevation of Privilege (EoP)  363\nencryption\n   about  374\n   asymmetric (public key) encryption  375\n   symmetric (secret key) encryption  375\nenterprise architect  24\nEnterprise Service Bus (ESB)  264\nentity-centric business services  253\nevaluation, ATAM phases\n   about  425\n   architectural approaches, analyzing  427, 429\n   architectural approaches, identifying  426\n   architecture, presenting  426\n   ATAM, presenting  425\n   business drivers, presenting  426\n   quality attribute utility tree, generating  426\n   results, presenting  429\n   scenarios, brainstorming  428\n   scenarios, prioritizing  428\nevent channels\n   about  225\n   message queues  225\n   message topics  225\n   point-to-point channel pattern  225\n   publish-subscribe pattern  225\nevent processing styles\n   about  228\n   complex event processing (CEP)  229\n   event stream processing (ESP)  229\n   simple event processing (SEP)  229\nevent stream processing (ESP)  228, 229\nevent-driven architecture (EDA)\n   about  213, 224\n   broker topology  227\n   event channels  224\n   event processing styles  228\n   event-driven functionality, types  230\n   mediator topology  226, 227\n   topologies  226\nevent-driven functionality\n   event-carried state transfer  230, 231\n   event-sourcing  231, 232\nevolutionary architectures\n   changes across multiple dimensions  512\n   changes, making  508\n   designing  507\n   evolvable APIs, designing  513\n   fitness functions  508\n   incremental changes, making  511, 512\n   last responsible moment (LRM)  515\n   loosely coupled  512, 513\n   standards, using in software system  514\nevolvable APIs\n   designing  513\n   Postel's Law, applying  514\nexisting solutions\n   leveraging  129\nextensibility  82\nexternally-developed software\n   building, advantages/disadvantages  133\n   buying, advantages/disadvantages  134\n   external software, researching  134\n   open source software (OSS), using  135\nExtreme Programming (XP)  33, 182\nF\nfacilitator  35\nfault detection detection\n   condition monitoring  95\n   self-tests  95\nfault detection\n   heartbeat  94\n   ping/echo reply  94\n   sanity test/sanity checking  95\n   timestamp  94\n   voting  94\nfaults recovery, ways\n   about  95\n   exception handling  95\n   faulty behavior, ignoring  97\n   graceful degradation  97\n   retry strategy  96\n   rollback  97\n   varying levels of redundancy  96\nfaults\n   detecting  94\nfile compression\n   about  336", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Software Quality Attributes", "Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 572", "position": 572, "chunk_type": "semantic", "token_estimate": 289}
{"text": "[ 544 ]: lossless compression  336\n   lossy compression  336\nFilebeat  317\nFileImport class  405\nfitness functions\n   about  508\n   categories  509\n   examples  510\nflexibility  82\nFlow Final Node  419\nformal inspection\n   about  210\n   author  211\n   leader/moderator  211\n   recorder/scribe  211\n   reviewer  211\nformal inspections\n   meeting and follow-up  211\n   roles  211\nformal negotiations  495, 496\nformats, web fonts\n   EOT  345\n   TTF  345\n   WOFF  345\n   WOFF 2.0  345\nfrequently asked questions (FAQs)  91\nFunction as a Service (FaaS)  459\nfunction invocation\n   asynchronous request (message queue)  284\n   batch job  285\n   message stream  284\n   synchronous request  283\nfundamental concepts, AOP\n   advice  307\n   aspect  307\n   join point  307\n   pointcut  307\nfundamentals, domain-driven design (DDD)\n   aggregates entities  60\n   entities  59\n   root entities  60\n   value objects  59\nG\ng11n  100\ngigabits per second (Gbps)  324\ngood help system\n   providing  91\nGraphics Interchange File (GIF)  336\ngreenfield systems\n   about  123\n   versus brownfield software systems  123\nguidelines, cross-cutting concerns\n   about  299\n   consistency, maintaining  299\n   cross-cutting concerns, identifying  299\n   open-source, using  299\n   scattered solutions, avoiding  300\n   tangled solutions, avoiding  300, 301\n   third-party solutions, using  299\ngzip compression  337\nH\nhash function  375\nheader compression  343\nHealth Insurance Portability and Accountability Act\n(HIPAA)  452\nheap  349\nHTTP caching\n   cache-control directives, specifying  334, 335\n   leveraging  333, 334\n   validation token, using  334\nHTTP/2\n   about  341\n   header compression  343\n   implementing  343\n   multiplexing  342\n   server push  342, 343\n   using  341\nhuman resources (HR) department  369\nHypertext Markup Language (HTML)  183\nHypertext Transfer Protocol Secure (HTTPS)  104\nI\ni18n  100\nIComponent interface  303\nidentity and access management (IAM)", "domains": ["Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 573", "position": 573, "chunk_type": "semantic", "token_estimate": 250}
{"text": "[ 545 ]: about  377\n   authentication  377\n   authorization  378\n   centralized identity provider (IdP), implementing \n380, 381\n   domain authentication, using  380\n   encrypted passwords, storing  379\n   hashed passwords, storing  379, 380\n   OAuth 2  381\n   OpenID Connect (OIDC)  381\n   plaintext passwords, storing  379\ninformal negotiations  495, 496\ninformation architect  26\ninformation technology (IT)  456\ninformation-hiding\n   about  180\n   exposed/hidde itemsn  181\n   reasons  181\ninfrastructure architect\n   about  27\n   facilities  27\n   network elements  27\n   servers  27\n   storage systems  27\nInfrastructure as a Service (IaaS)  279, 455\ninstallability  99\nInstitute of Electrical and Electronics Engineers\n(IEEE)  9\nintegrated development environment (IDE)  84,\n176, 262, 443, 469\nIntegrated Information Infrastructure Reference\nModel (III-RM)  158\ninter-process communication (IPC)  319\nInterface Segregation Principle (ISP)  167, 191\nInternational Organization for Standardization\n(ISO)  9\ninternationalization  100, 101\nInternet of Things (IoT)  277\ninternet service provider (ISP)  324\ninteroperability\n   about  102\n   challenges  102, 103\n   information, exchanging with another system \n103\n   information, locating  103\n   standards  104\n   testing  104\nivory tower architect  18, 19\nJ\nJavaScript  183\nJavaScript Object Notation (JSON)  104, 266, 316\nJoint Photographic Experts Group (JPEG)  336\nJSON web token (JWT)\n   about  382\n   header  383\n   payload  383\n   signature  384\nK\nKanban  33\nKeep It Simple, Stupid (KISS)  176\nKibana  316\nKISS principle\n   about  176\n   applying, to software  177\n   origin  176\n   over simplicity, avoiding  177\nL\nlast responsible moment (LRM)  515\nlast-mile latency  324\nlaw of the instrument  519\nlayer  217\nlayered architecture\n   about  216\n   advantages  218, 219\n   client-server architecture  220\n   disadvantages  219, 220\n   N-tier architecture  222\n   open layers, versus closed layers  216, 217\n   tiers, versus layers  217, 218\nleadership  489, 490, 491, 492, 494\nlean software development (LSD)  440\nlearnability  87\nlegacy application\n   migrating, to cloud  470\nlegacy applications\n   about  461, 462, 464\n   changes, making  469", "domains": ["Architectural Patterns and Styles", "Design Principles", "Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 574", "position": 574, "chunk_type": "semantic", "token_estimate": 284}
{"text": "[ 546 ]: data integration, performing  477\n   functionality, sharing between systems  477\n   integrating with  475\n   issues  462, 463\n   monoliths, transforming to microservices  469,\n470\n   need for  463\n   need, confirming  476\n   redundant code, removing  467\n   refactoring  464, 465\n   tools, using to refactor  469\n   type, determining  476\nlegacy code\n   testable, making  465\n   tests, writing  466\n   unit testing, benefits  465, 466\n   unit tests, refactoring  466\nLehman's laws of software evolution\n   about  503, 504\n   change, continuing  505\n   complexity, increasing  505\n   conservation of familiarity  506\n   conservation of organizational stability  506\n   feedback system  507\n   growth, continuing  506\n   quality, declining  506\n   self-regulation  505\nLehman's software categories\n   about  503\n   E-type systems  504\n   P-type systems  504\n   S-type systems  503\nLibbeat  317\nlines of code (LOC)  77, 84, 210, 510\nLiskov Substitution Principle (LSP)  167, 189\nlocalization  100, 101\nlog entries\n   characteristics  313\nlog levels\n   DEBUG  313\n   ERROR  314\n   FATAL  314\n   INFO  314\n   TRACE  313\n   WARN  314\nlogging  313\nlogging, cross-cutting concerns\n   Elastic Stack, using  315\n   log entries  313\n   log entries, routing  314\n   log levels  313\nLogstash  316\nloose coupling\n   about  169\n   common coupling  169\n   content coupling  169\n   control coupling  170\n   data coupling  170, 171\n   designing for  171\n   external coupling  170\n   Law of Demeter (LoD) / principle of least\nknowledge  171\n   message coupling  171\n   no coupling  171\n   stamp coupling (data-structured coupling)  170\n   types  169\nlossless compression  336\nlossy compression  336\nM\nmaintainability\n   about  79\n   cohesion, increasing  83\n   coupling, reducing  83\n   cyclomatic complexity  84, 85\n   depth of inheritance tree (DIT)  86\n   designing  82\n   Lines of code (LOC)  84\n   measuring  83\n   size, reducing  83\nmediator topology\n   about  226, 227\n   implementations  227\nmegabits per second (Mbps)  324\nmessage-delivery guarantees, AMQP\n   at least once  266\n   at most once  266\n   exactly once  266\nmicroservice chassis\n   leveraging  317, 318", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Design Principles", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 575", "position": 575, "chunk_type": "semantic", "token_estimate": 283}
{"text": "[ 547 ]: microservices architecture (MSA)\n   about  260, 263, 451\n   audience, selecting  275, 276\n   characteristics  264\n   dependencies, sharing  270\n   granularity  269\n   nanoservices  269, 270\n   polyglot microservices, designing  267\n   service discovery  271\n   service-oriented architecture (SOA)  263\n   stateless, versus stateful microservices  271\nmicroservices, cross-cutting concerns\n   microservice chassis, leveraging  317, 318\n   sidecar pattern, using  318, 319\nminification  337\nModel-View-Controller (MVC) pattern\n   about  213, 232\n   advantages  234\n   controller  234\n   disadvantages  234\n   model  233\n   view  233\nModel-View-Presenter (MVP) pattern\n   about  213, 235\n   model  235\n   presenter  236\n   view  235\nModel-View-ViewModel (MVVM) pattern\n   about  213, 236\n   model  237\n   view  237\n   ViewModel  237\nmodeling, UML\n   behavioral modeling  401\n   structural modeling  401\nmodifiability  81\nmodifications\n   scope  82\nmonolithic architecture  260\nmulti-factor authentication (MFA)  378\nN\nn-tier architecture\n   about  222\n   Business Tier  223\n   data Tier  224\n   Presentation Tier  223\nnaming conventions\n   for unit tests  204\n   unit test class names  205\n   unit test method names  205\nnegotiation\n   about  494\n   using  495\nnetwork-attached storage (NAS)  27\nno design up front (NDUF)  120\nnodes\n   about  410\n   device nodes  410\n   execution environment nodes (EENs)  410\nnon-agnostic functional context  253\nnon-clustered indexes  350\nO\nOAuth 2\n   about  381\n   roles  381\nobject-oriented programming (OOP)  306\nobject-relational mapping (ORM) tool  224\noffice politics\n   about  40\n   cultural differences  43\n   goals  42\n   negotiating  43\n   organization's goals  42\n   stakeholders, addressing  42\nonolithic architecture\n   benefits  261\n   drawbacks  262\nOpen Source Initiative (OSI)  135, 522\nopen source projects\n   creating  521\n   participating  521\nopen source software (OSS)\n   advantages  136\n   disadvantages  136, 137\nOpen Web Application Security Project (OWASP)\n   about  385\n   reference link  385\nOpen/Closed Principle (OCP)  167, 187, 301", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 576", "position": 576, "chunk_type": "semantic", "token_estimate": 256}
{"text": "[ 548 ]: OpenID Connect (OICD)\n   about  381\n   authenticating, with identity provider  382\n   authorizing, with authorization server  384\noperating system (OS)\n   about  327\n   adaptive maintenance  81\norchestrated task service  253\norganization's business  55\northogonal software systems\n   designing  168\n   high cohesion  172\n   loose coupling  168\nP\npair programming\n   about  208\n   benefits  208, 209\n   using  209\nperformance improvement\n   application, profiling  326\n   changes, implementing  328\n   results, analyzing  328\n   results, monitoring  329\n   systematic approach  326\nperformance terminology\n   bandwidth  324\n   defining  323\n   latency  324\n   processing time  325\n   response time  325\n   throughput  324\n   utilization  325\n   workload  325\nperformance\n   about  321, 322\n   bounce rate  322\n   conversion rate  322\n   need for  323\n   page speed  323\npersonally identifiable information (PII)  499, 510\nplaintext  374\nPlatform as a Service (PaaS)  279, 458\npolicies, data eviction\n   explicitly evicting data  332\n   first-in, first-out (FIFO)  332\n   last-in, first-out (LIFO)  332\n   least recently used (LRU)  331\n   most recently used (MRU)  331\npolyglot microservices\n   designing  267\n   polyglot persistence  268\n   polyglot programming  268\n   technologies, using  269\nportability\n   about  99\n   maintaining  102\nPortable Network Graphics (PNG)  336\npre-meeting phase, ARID\n   design presentation, preparing  433\n   review meeting, preparing for  433\n   reviewers, identifying  433\n   seed scenarios, preparing  433\nPresentation Tier  223\npresentations\n   about  486\n   plan  487\n   practice  488\n   prepare  487, 488\n   present  489\nprimary index  349\nprinciple of least privilege (PoLP)  371\nproduct line engineering (PLE)  49\nprofilers  327\nprofiling  326\nproject back\n   acting  40\n   obtaining, on schedule  38\n   overtime, working  38\n   problem areas, identifying  40\n   resources, adding  39\n   resources, reallocating  39\n   scope, reducing  38\nproject management\n   about  36\n   estimates  36\n   project back, obtaining on schedule  38\n   project schedule changes  38\n   realist  37\n   software project estimation, importance  36", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 577", "position": 577, "chunk_type": "semantic", "token_estimate": 269}
{"text": "[ 549 ]: team and situational factors  37\nproof of concept (POC)  125, 527\nQ\nquality attributes\n   about  76, 77\n   and software development life cycle (SDLC)  78\n   external  77\n   internal  77\n   testing  78\nquality-attribute scenarios\n   about  126\n   prioritizing  127\nQuery Model  239\nR\nredundant legacy code\n   commented-out code  468\n   dead code  468\n   duplicate code  468\n   removing  467\n   unreachable code  467\nreference architectures\n   benefits  130\n   creating  131\n   refactoring  131\nreflected XSS  389\nregistered claims  383\nrelational database management system (RDBMS) \n220, 266, 271\nremote resources\n   benefits  497\n   challenges  497\n   working with  497\nreplaceability  100\nreply/return messages  415\nrequirements elicitation\n   about  68\n   stakeholders access, getting  74\n   techniques  69\nrequirements engineering\n   about  63\n   need for  66\n   software requirements, types  63\nresilience engineering  511\nreturn on investment (ROI)  44, 55\nreview meeting phase, ARID\n   about  434\n   ARID method, presenting  434\n   design, presenting  434\n   scenarios, brainstorming  434\n   scenarios, prioritizing  434\nrisk acceptance  46\nrisk avoidance  45\nrisk management  43\nrisk management options, threat modeling\n   risk, accepting  366\n   risk, avoiding  366\n   risk, mitigating  367\n   risk, transferring  366\n   security controls  367\nrisk mitigation  46\nRobustness Principle  514\nroles, OAuth 2\n   authorization server  381\n   client  381\n   resource owner  381\n   resource server  381\nS\nsampling  327\nscripts.js  340\nScrum  33\nsecondary indexes  350\nsecure by default  372\nsecure by design\n   about  370\n   attack surface, minimizing  371\n   default deny  373\n   defense in depth  371\n   fail securely  374\n   input validation  373\n   principle of least privilege (PoLP)  371\n   secure by default  372\n   security by obscurity, avoiding  372\n   security, must be usable  373, 374\n   simple software designs  372\n   weakest link, securing  373\nSecure Hashing Algorithm 256 (SHA-256)  376\nsecurity architect  28", "domains": ["Software Quality Attributes"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 578", "position": 578, "chunk_type": "semantic", "token_estimate": 268}
{"text": "[ 550 ]: security controls\n   about  367\n   administrative controls  368, 369\n   detection  370\n   physical security controls  367\n   prevention  369\n   response  370\n   technical security controls  369\nsemantic versioning  446\nSeparation of Concerns (SoC)  167, 176, 183,\n218, 298\nsequence diagrams, UML\n   activation boxes  414\n   alternative flows  416, 417\n   lifeline  414\n   loops  415\n   messages  414\n   optional flows  416\nserver push  342, 343\nserver-side caching\n   about  329\n   cache usage patterns  332\n   cache, priming  331\n   cached data, invalidating  331\n   data, caching in distributed applications  330\nServer-Side Request Forgery (SSRF)  386\nserverless architecture\n   about  276\n   advantages  279\n   Backend as a service (BaaS)  278\n   disadvantages  280\n   Function as a service (FaaS)  277\n   function deployment  282\n   function invocation  283\n   hybrid approach  282\nservice discovery\n    service registry, using  271\n   about  271\n   client-side discovery pattern  274\n   server-side discovery pattern  274, 275\n   types  273\nservice registry\n   self-registration pattern  272\n   third-party registration pattern  272\n   using  271\nservice-level agreements (SLAs)  29\nservice-oriented architecture (SOA)\n   about  213, 241, 242, 263\n   analysis  250\n   benefits  242, 243, 244\n   business automation requirements, defining  250\n   candidate services, modeling  251\n   challenges  245\n   cost-benefit analysis  244\n   delivery strategies  248\n   design  253, 254\n   entity service  253\n   existing automation systems, identifying  251\n   key principles  246\n   namespaces, structuring  257\n   service abstraction  247\n   service autonomy  247\n   service choreography  258\n   service composability  248\n   service descriptions  256\n   service discoverability  248\n   service layers  251, 252\n   service loose coupling  246\n   service models  251, 252\n   service orchestration  258\n   service registries  255, 256\n   service reusability  247\n   service statelessness  248\n   service-interface design  254\n   service-interface granularity  255\n   standardized service contract  246\n   task service  253\n   utility service  253\nsidecar pattern\n   using  318, 319\nsimple event processing (SEP)  228, 229\nSingle Responsibility Principle (SRP)  63, 167,\n184, 186, 278, 300\nsoftware architects\n   7 Cs of communication  481\n   about  20, 21, 518, 519, 520\n   application architect  25\n   cloud architect  28\n   communicating  480, 481\n   communication  480", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Design Principles", "Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 579", "position": 579, "chunk_type": "semantic", "token_estimate": 301}
{"text": "[ 551 ]: conferences, attending  528, 529, 530\n   continuous involvement, with coding  527, 528\n   data architect  26\n   duties  18\n   enterprise architect  24\n   expectations, communicating  481\n   information architect  26\n   information security architect  28\n   infrastructure architect  27\n   life outside software development  531\n   listening skills  485, 486\n   preparations, for teaching  524, 525, 526\n   presentations  486\n   quality attributes, communicating  481\n   responsibility  530\n   soft skills  479, 480\n   solution architect  25\n   technical qualities  19, 20\n   technologies, using  526\n   types  24\n   user groups, attending  528, 529, 530\n   working  17\nsoftware architecture analysis method (SAAM)\n   about  421\n   architecture, developing  422\n   overall evaluation, creating  423\n   scenario interaction, assessing  423\n   scenario-based analysis  421\n   scenarios, classifying  422\n   scenarios, developing  422\n   scenarios, evaluating  423\n   scenarios, prioritizing  422\n   steps  422\nsoftware architecture design\n   about  112\n   active backlogs  165\n   architectural analysis  142\n   architectural evaluation  143\n   architectural synthesis  143\n   as iterative process  143\n   backlog, prioritizing  163\n   communicating, to others  116\n   component  115\n   design decisions, avoiding  116\n   design decisions, making  112, 113\n   design terms  113\n   documenting  137\n   dynamic backlogs  164\n   element  114\n   guidance, providing to developers  117\n   importance  115\n   key decisions, creating  115\n   model  142\n   module  114\n   non-technical parts, influencing  117\n   progress, tracking  162\n   progress, tracking with backlog  163\n   selecting  144, 145\n   sketching  137, 138\n   structure  113\n   subsystem  114\n   system  114\n   systematic approach, using  141\nsoftware architecture documentation, uses\n   about  394\n   architectural knowledge reuse, allowing  396\n   architecture, communicating to others  394\n   development team, assisting  395\n   input, providing for reviews  395\n   software architect, helping  396\n   team members, educating  395\nsoftware architecture notations\n   about  398\n   formal software architecture notations  399\n   informal software architecture notations  398\n   semiformal software architecture notations  399\n   types  398\nsoftware architecture patterns\n   about  214, 216\n   overusing  215\n   using  214, 215\n   versus, architecture styles  215, 216\nsoftware architecture views\n   about  397, 398\n   design rationales, including  400\n   notations  398\nsoftware architecture\n   about  8, 10, 16\n   active design review (ADR)  429\n   active reviews of intermediate designs (ARID)", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 580", "position": 580, "chunk_type": "semantic", "token_estimate": 309}
{"text": "[ 552 ]: 432\n   architecture tradeoff analysis method (ATAM) \n424\n   change, managing  13\n   communication ease  13\n   consumers  16\n   contents  10\n   cost estimates, improving  14, 15\n   effort estimates, improving  14, 15\n   implementation constraints, imposing  14\n   need for  10, 11\n   quality attributes, enabling  12\n   quality attributes, inhibiting  12\n   rationale, documenting  138\n   requirements  11\n   reusable model, providing  14\n   reviewing  420\n   serving, as training for team members  15\n   software architecture analysis method (SAAM) \n421\n   system qualities, predicting  12\nsoftware architectures design, Microsoft\n   application overview, creating  151\n   architecture objectives, identifying  150\n   candidate solutions, defining  153\n   key issues, identifying  153\n   key scenarios, identifying  151\n   techniques  150\nSoftware as a Service (SaaS)  459\nsoftware configuration management (SCM)  46\nsoftware development life cycle (SDLC)  78\nsoftware development methodologies\n   about  29\n   Agile software development methodologies  32\n   Waterfall model  30\nsoftware maintenance\n   adaptive maintenance  81\n   corrective maintenance  80\n   perfective maintenance  80\n   preventive maintenance  81\n   types  80\nsoftware product line\n   benefits  50\nsoftware product lines\n   about  49\n   core assets, of organization  50\n   product line engineering, risks  51\nsoftware quality attribute scenario\n   artifact  421\n   environment  421\n   response  422\n   response measure  422\n   source of stimulus  421\n   stimulus  421\nsoftware requirements\n   impact on architecture  67, 68\n   measurable  67\n   testable  67\n   types  63\nsoftware risk management\n   about  43\n   financial risks  44\n   functional risks  44\n   legal risks  44\n   management risks  44\n   personnel risks  44\n   risk acceptance  46\n   risk avoidance  45\n   risk mitigation  46\n   risk, transferring  45\n   technical risks  44\nsoftware systems security\n   about  358\n   CIA triad  359\n   three states of information  358\nsoftware systems\n   change  501, 502\nsoftware\n   uses  91\nSOLID design principles\n   about  184\n   Dependency Inversion Principle (DIP)  194\n   Interface Segregation Principle (ISP)  191\n   Liskov Substitution Principle (LSP)  189\n   Open/Closed Principle (OCP)  187\n   Single Responsibility Principle (SRP)  184, 186\nsolution architect  25\nsource lines of code (SLOC)  84\nSQL injection (SQLi)  385\nstandard error (stderr)  315\nstandard output (stdout)  315", "domains": ["Design Patterns", "Design Principles", "Software Quality Attributes", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 581", "position": 581, "chunk_type": "semantic", "token_estimate": 305}
{"text": "[ 553 ]: storage area networks (SAN)  27\nstored XSS  389\nstrategies, for fault prevention\n   about  98\n   competence set, increasing  98\n   exception prevention  98\n   removal from service  98\n   transactions  98\nStreamReader class  405\nSTRIDE threat model\n   about  362\n   data, tampering with  362\n   denial-of-service (DoS)  363\n   Elevation of Privilege (EoP)  363\n   information disclosure  363\n   repudiation  362\n   spoofing identity  362\nStructured Query Language (SQL)  221\nstyles.css  340\nsynchronous messages  415\nSystem Under Test (SUT)  105, 204, 205\nSystems Modeling Language (SysML)\n   about  399, 400\n   diagram types  400\nT\nTCP Connection  342\ntechniques, requirements elicitation\n   brainstorming sessions  70, 71\n   document analysis  73\n   focus groups  72\n   interviews  69\n   observation  71\n   prototyping  73, 74\n   requirements workshops  69, 70\n   reverse engineering  74\n   surveys  72\ntestability\n   about  105\n   automatability  106\n   controllability  105\n   isolability  106\n   observability  106\n   software complexity  107\n   test documentation, importance  108\ntester\n   qualities  109\nThe Open Group Architecture Framework (TOGAF)\n   architecture domains  158\n   documentation sections  158\nthreat  360\nthreat classification model\n   about  361\n   STRIDE threat model  362\nthreat modeling\n   about  360\n   application, decomposing  361\n   potential threats, categorizing  361\n   potential threats, identifying  361\n   potential threats, prioritizing  363\n   responses, to threats  365\n   risk management  365\ntier  217\ntop-down approach\n   about  117, 118\n   advantages  118, 121, 122\n   disadvantages  118, 119\n   versus bottom-up design approach  117\ntriple modular redundancy (TMR)  94\ntwelve-factor apps\n   about  290\n   administrative processes  295\n   backing services  292\n   Build/Release/Run  292, 293\n   codebase  291\n   concurrency  294\n   configuration  292\n   dependencies  291\n   development/production parity  294\n   disposability  294\n   logs  295\n   port binding  294\n   processes  293\ntwo-factor authentication (2FA)  372, 378\ntypes of advice, AOP\n   after advice (finally)  307\n   after returning advice (on success)  307\n   after throwing advice (on error)  307\n   around advice  307\n   before advice (on start)  307", "domains": ["Software Quality Attributes", "Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 582", "position": 582, "chunk_type": "semantic", "token_estimate": 278}
{"text": "[ 554 ]: types, event-driven functionality\n   event notification  230\ntypes, software requirements\n   business requirements  64\n   constraints  65, 66\n   functional requirements  64\n   non-functional requirements  65\nU\nubiquitous language  57, 59\nUnified Modeling Language (UML)\n   about  393\n   activity diagrams  417\n   class diagram  401\n   component diagrams  407\n   deployment diagrams  410\n   modeling, types  401\n   overview  401\n   package diagrams  409\n   sequence diagrams  413\n   structure diagrams  401\n   use case diagrams  411, 412\nUniform Resource Locator (URI)  257\nunit testing, properties\n   atomic  202\n   automated and repeatable  203\n   deterministic  203\n   easy implementation  203\n   fast  203\n   isolated and independent  203\nunit testing\n   AAA pattern  204\n   about  201\n   benefits  202\n   code coverage  205\n   naming conventions  204\n   properties  202\n   updating  206\nusability\n   about  87\n   need for  89\n   testing  89\nuseful feedback\n   providing  88\nuser experience (UX)\n   conversion rate  323\n   performance affects  322\nusers\n   task completion, allowing  87\nutility tree  426\nV\nvariation points  50\nvirtual machines (VMs)  456\nvisual design\n   appealing  89\nW\nwarm start  283\nWaste Everyone's Time  178\nWaterfall model\n   about  30\n   issues  32\n   phases  31\nWe Enjoy Typing)  178\nweaving\n   about  308\n   compile-time weaving  309\n   runtime weaving  309\nweb application firewall (WAF)  385\nweb application performance, improving\n   compression, using  335\n   content delivery networks (CDNs)  344\n   critical rendering path, optimizing  346, 347\n   HTTP caching, leveraging  333, 334\n   HTTP/2, using  341\n   resources, bundling  338, 339, 340, 341\n   resources, minifying  337\n   techniques  333\n   web fonts, optimizing  344, 345\nweb applications\n   designing  89\nweb fonts\n   about  345\n   font formats  345\n   optimizing  345\nWindows desktop applications\n   designing  90\nWindows Presentation Foundation (WPF)  237\nWrite Everything Twice (WET)  178", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 583", "position": 583, "chunk_type": "semantic", "token_estimate": 255}
{"text": "X: XML external entity (XXE) attack  386\nY\nYou Aren't Gonna Need It (YAGNI)\n   about  176, 182\n   non applicable situations  182\n   over-engineering a solution, avoiding  182", "domains": ["Design Principles", "Domain-Driven Design"], "source": "Joseph Ingeno - Software Architect\u2019s Handbook_ Become a successful software architect by implementing effective architecture concepts-Packt Publishing (2018).pdf", "section": "Page 584", "position": 584, "chunk_type": "semantic", "token_estimate": 26}
