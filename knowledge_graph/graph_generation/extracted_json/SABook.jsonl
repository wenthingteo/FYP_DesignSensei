{"text": "Software Architect's Handbook\nBecome a successful software architect by implementing\ne\ufb00ective architecture concepts\nJoseph Ingeno\nBIRMINGHAM - MUMBAI\n", "page": 2, "type": "text", "section": "Page 2"}
{"text": "Software Architect's Handbook\nCopyright \u00a9 2018 Packt Publishing\nAll rights reserved. No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form\nor by any means, without the prior written permission of the publisher, except in the case of brief quotations\nembedded in critical articles or reviews.\nEvery effort has been made in the preparation of this book to ensure the accuracy of the information presented.\nHowever, the information contained in this book is sold without warranty, either express or implied. Neither the\nauthors, nor Packt Publishing or its dealers and distributors, will be held liable for any damages caused or alleged to\nhave been caused directly or indirectly by this book.\nPackt Publishing has endeavored to provide trademark information about all of the companies and products\nmentioned in this book by the appropriate use of capitals. However, Packt Publishing cannot guarantee the accuracy\nof this information.\nCommissioning Editor: Merint Mathew\nAcquisition Editor: Denim Pinto\nContent Development Editor: Priyanka Sawant\nTechnical Editor: Ketan Kamble\nCopy Editor: Safis Editing\nProject Coordinator: Vaidehi Sawant\nProofreader: Safis Editing\nIndexer: Priyanka Dhadke\nGraphics: Jason Monteiro\nProduction Coordinator: Aparna Bhagat\nFirst published: August 2018\nProduction reference: 1290818\nPublished by Packt Publishing Ltd.\nLivery Place\n35 Livery Street\nBirmingham\nB3 2PB, UK.\nISBN 978-1-78862-406-0\nwww.packtpub.com\n", "page": 3, "type": "text", "section": "Page 3"}
{"text": "To my children, Adriana and Alexander,\nwho make the world a better place.\n", "page": 4, "type": "text", "section": "Page 4"}
{"text": " \nmapt.io\nMapt is an online digital library that gives you full access to over 5,000 books and videos, as\nwell as industry leading tools to help you plan your personal development and advance\nyour career. For more information, please visit our website.\nWhy subscribe?\nSpend less time learning and more time coding with practical eBooks and Videos\nfrom over 4,000 industry professionals\nImprove your learning with Skill Plans built especially for you\nGet a free eBook or video every month\nMapt is fully searchable\nCopy and paste, print, and bookmark content\nPacktPub.com\nDid you know that Packt offers eBook versions of every book published, with PDF and\nePub files available? You can upgrade to the eBook version at www.PacktPub.com and as a\nprint book customer, you are entitled to a discount on the eBook copy. Get in touch with us\nat service@packtpub.com for more details.\nAt www.PacktPub.com, you can also read a collection of free technical articles, sign up for a\nrange of free newsletters, and receive exclusive discounts and offers on Packt books and\neBooks. \n", "page": 5, "type": "text", "section": "Page 5"}
{"text": "Contributors\nAbout the author\nJoseph Ingeno is a software architect who oversees a number of enterprise software\napplications. During his career, he has designed and developed software for a variety of\ndifferent industries. He has extensive experience working on web, mobile, and desktop\napplications using different technologies and frameworks.\nJoseph graduated from the University of Miami a with Master of Science and a Bachelor of\nBusiness Administration degrees in Computer Information Systems, and followed that with\na Master of Software Engineering degree from Brandeis University.\nHe holds several certifications, including the Microsoft Certified Solutions Developer and\nthe Professional Software Engineering Master Certification from the IEEE Computer\nSociety.\nA special thanks to my wife, Sally, and the rest of my family for their understanding and\npatience while I devoted the time necessary to write this book.\nI would like to thank Priyanka Sawant, Ketan Kamble, Ruvika Rao, Gaurav Aroraa,\nAnand Pillai, Denim Pinto, and everyone at Packt Publishing who provided input and\nsupport during the writing of this book.\n", "page": 6, "type": "text", "section": "Page 6"}
{"text": "About the reviewers\nGaurav Aroraa has done an MPhil in computer science. He is a Microsoft MVP, a lifetime\nmember of Computer Society of India (CSI),  an advisory member of IndiaMentor,\ncertified as a Scrum trainer/coach, XEN for ITIL-F, and APMG for PRINCE-F and PRINCE-\nP. He is an open source developer, a contributor to TechNet Wiki, and the founder of\nOvatic Systems Private Limited. In his career of over 20 years, he has mentored thousands\nof students and industry professionals. Apart from that, he's written over 100 white papers\nfor research scholars and various universities across the globe.\nI'd like to thank my wife, Shuby Arora, and my angel daughter, Aarchi Arora, as well as\nthe team at PACKT.\nAnand B Pillai is a technophile by profession with 20 years' of experience in software\ndevelopment, design, and architecture. Over the years, he has worked with numerous\ncompanies in fields ranging from security, search engines, large-scale web portals and big\ndata. He is a founder of the Bangalore Python Users' Group and is the author of Software\nArchitecture with Python (PacktPub, April 2017). Anand is currently a VP of an engineering\nat the early-stage legal technology startup, Klarity Law. He happily resides with his family\nin Bangalore, India.\nPackt is searching for authors like you\nIf you're interested in becoming an author for Packt, please visit authors.packtpub.com\nand apply today. We have worked with thousands of developers and tech professionals,\njust like you, to help them share their insight with the global tech community. You can\nmake a general application, apply for a specific hot topic that we are recruiting an author\nfor, or submit your own idea.\n", "page": 7, "type": "text", "section": "Page 7"}
{"text": "Table of Contents\nPreface\n1\nChapter 1: The Meaning of Software Architecture\n8\nWhat is software architecture?\n8\nISO/IEC/IEEE 42010 standard definition\n9\nWhat makes up a software architecture?\n10\nSoftware architecture is an abstraction\n10\nSoftware architecture is about the important stuff\n10\nWhy is software architecture important?\n11\nDefining a solution to meet requirements\n11\nEnabling and inhibiting quality attributes \n12\nGiving you the ability to predict software system qualities\n12\nEasing communication among stakeholders\n13\nManaging change\n13\nProviding a reusable model\n14\nImposing implementation constraints\n14\nImproving cost and effort estimates\n14\nServes as training for team members\n15\nSoftware architecture is not a silver bullet\n16\nWho are the consumers of software architectures?\n16\nWhat is the software architect role?\n17\nSoftware architects are technical leaders\n17\nSoftware architects perform a number of duties\n18\nIvory tower software architects\n18\nWhat are software architects expected to know?\n19\nDon't be overwhelmed\n20\nIs the software architect role right for you?\n21\nSummary\n22\nChapter 2: Software Architecture in an Organization\n23\nTypes of software architects\n24\nEnterprise architect\n24\nSolution architect\n25\nApplication architect\n25\nData architect/information architect\n26\nInfrastructure architect\n27\nInformation security architect\n28\nCloud architect\n28\nSoftware development methodologies\n29\nThe Waterfall model\n30\n", "page": 8, "type": "text", "section": "Page 8"}
{"text": "Table of Contents\n[ ii ]\nPhases of the Waterfall methodology\n31\nIssues with the Waterfall methodology\n32\nAgile software development methodologies\n32\nAgile values and principles\n33\nAn iterative methodology\n34\nAdaptive rather than predictive\n34\nDaily stand-up meetings\n35\nProject management\n36\nThe importance of software project estimation\n36\nPutting effort into the estimates\n36\nBeing a realist (or even a pessimist)\n37\nTeam and situational factors to consider\n37\nProject schedule changes\n37\nGetting a project back on schedule\n38\nWorking overtime\n38\nReducing scope\n38\nAdding resources\n39\nReallocating resources\n39\nIdentifying problem areas\n40\nActing as early as possible\n40\nOffice politics\n40\nUnderstanding your organization's goals\n42\nAddressing the concerns of others\n42\nAssisting people with their goals\n42\nKnowing when to compromise\n43\nBeing aware of cultural differences\n43\nSoftware risk management\n43\nRisk avoidance\n45\nTransferring the risk to another party\n45\nRisk mitigation\n46\nRisk acceptance\n46\nConfiguration management\n46\nChanging management\n47\nSoftware product lines\n49\nBenefits of a software product line\n50\nCore assets of an organization\n50\nRisks of product line engineering\n51\nSummary\n51\nChapter 3: Understanding the Domain\n53\nDeveloping business acumen\n53\nFamiliarity with general business topics\n54\nUnderstanding your organization's business\n55\nDomain-driven design\n56\nEncourages and improves communication\n56\nWhat is a ubiquitous language?\n57\n", "page": 9, "type": "text", "section": "Page 9"}
{"text": "Table of Contents\n[ iii ]\nEntities, value objects, and aggregates\n59\nEntities\n59\nValue objects\n59\nAggregates and root entities\n60\nSeparating the domain into subdomains\n60\nWhat are bounded contexts?\n61\nRequirements engineering\n63\nTypes of software requirements\n63\nBusiness requirements\n64\nFunctional requirements\n64\nNon-functional requirements\n65\nConstraints\n65\nThe importance of requirements engineering\n66\nSoftware requirements must be measurable and testable\n67\nSoftware requirements that affect architecture\n67\nRequirements elicitation\n68\nTechniques to elicit requirements\n69\nInterviews\n69\nRequirements workshops\n69\nBrainstorming\n70\nObservation\n71\nFocus groups\n72\nSurveys\n72\nDocument analysis\n73\nPrototyping\n73\nReverse engineering\n74\nGet access to the proper stakeholders\n74\nSummary\n75\nChapter 4: Software Quality Attributes\n76\nQuality attributes\n76\nExternal or internal\n77\nQuality attributes and the SDLC\n78\nTesting quality attributes\n78\nMaintainability\n79\nTypes of software maintenance\n80\nCorrective maintenance\n80\nPerfective maintenance\n80\nAdaptive maintenance\n81\nPreventive maintenance\n81\nModifiability\n81\nExtensibility and flexibility\n82\nScope of modifications\n82\nDesigning for maintainability\n82\nReducing size\n83\nIncreasing cohesion\n83\nReducing coupling\n83\nMeasuring maintainability\n83\n", "page": 10, "type": "text", "section": "Page 10"}
{"text": "Table of Contents\n[ iv ]\nLines of code (LOC)\n84\nCyclomatic complexity\n84\nDepth of inheritance tree (DIT)\n86\nUsability\n87\nAllowing users to complete their tasks efficiently\n87\nLearnability\n87\nProviding useful feedback\n88\nAccessibility   \n88\nUsability needs to be considered during requirements\n89\nUsability testing\n89\nAppealing visual design\n89\nProviding a good help system\n91\nSoftware must be useful, and not just usable\n91\nAvailability\n91\nCalculating availability based on time\n92\nCalculating availability based on request success rate\n93\nFaults, errors, and failures\n93\nDetecting faults\n94\nPing/echo reply\n94\nHeartbeat\n94\nTimestamp\n94\nVoting\n94\nSanity test/sanity checking\n95\nCondition monitoring\n95\nSelf-tests\n95\nRecovering from faults\n95\nException handling\n95\nRetry strategy\n96\nVarying levels of redundancy\n96\nRollback\n97\nGraceful degradation\n97\nIgnoring faulty behavior\n97\nPreventing faults\n98\nRemoval from service\n98\nTransactions\n98\nIncreasing competence sets\n98\nException prevention\n98\nPortability\n99\nAdaptability\n99\nInstallability\n99\nReplaceability\n100\nInternationalization and localization\n100\nMaintaining portability\n102\nInteroperability\n102\nChallenges with interoperability\n102\nLocating and exchanging information with another system\n103\nInteroperability standards\n104\n", "page": 11, "type": "text", "section": "Page 11"}
{"text": "Table of Contents\n[ v ]\nInteroperability testing\n104\nTestability\n105\nControllability\n105\nObservability\n106\nIsolability\n106\nAutomatability\n106\nComplexity of the software\n107\nImportance of test documentation\n108\nWhat makes a good tester?\n109\nSummary\n110\nChapter 5: Designing Software Architectures\n111\nSoftware architecture design\n112\nMaking design decisions\n112\nSoftware architecture design terms\n113\nStructure\n113\nElement\n114\nSystem\n114\nSubsystem\n114\nModule\n114\nComponent\n115\nThe importance of software architecture design\n115\nMaking key decisions\n115\nAvoiding design decisions can incur technical debt\n116\nCommunicating the architecture to others\n116\nProviding guidance to developers\n117\nInfluencing non-technical parts of the project\n117\nTop-down versus bottom-up design approaches\n117\nTop-down approach\n117\nAdvantages of the top-down approach\n118\nDisadvantages of the top-down approach\n118\nBottom-up approach\n119\nAdvantages of the bottom-up approach\n120\nDisadvantages of the bottom-up approach\n120\nWhich approach should I use?\n121\nGreenfield versus brownfield software systems\n123\nGreenfield systems\n123\nBrownfield systems\n124\nArchitectural drivers\n124\nDesign objectives\n125\nPrimary functional requirements\n126\nQuality attribute scenarios\n126\nPrioritizing quality attribute scenarios\n127\nConstraints\n127\nArchitectural concerns\n128\nLeveraging design principles and existing solutions\n129\n", "page": 12, "type": "text", "section": "Page 12"}
{"text": "Table of Contents\n[ vi ]\nSelecting a design concept\n129\nSoftware architecture patterns\n130\nReference architectures\n130\nBenefits of reference architectures\n130\nRefactoring a reference architecture for your needs\n131\nCreating your own reference architecture\n131\nTactics\n132\nExternally developed software\n132\nBuy or build?\n133\nAdvantages/disadvantages of building\n133\nAdvantages/disadvantages of buying\n134\nResearching external software\n134\nShould I use open source software (OSS)?\n135\nAdvantages of using open source software\n136\nDisadvantages of using open source software\n136\nDocumenting the software architecture design\n137\nSketching the architecture design\n137\nDocumenting the design rationale\n138\nDesign rationale for design evaluation\n139\nDesign rationale for design verification\n139\nDesign rationale for design knowledge transfer\n139\nDesign rationale for design communication\n140\nDesign rationale for design maintenance\n140\nDesign rationale for design documentation\n140\nDesign rationale for design reuse\n140\nUsing a systematic approach to software architecture design\n141\nA general model of software architecture design\n142\nArchitectural analysis\n142\nArchitectural synthesis\n143\nArchitectural evaluation\n143\nArchitecture design is an iterative process\n143\nSelecting an architecture design process\n144\nAttribute-driven design (ADD)\n145\nStep 1 \u2013 Reviewing inputs\n147\nStep 2 \u2013 Establishing the iteration goal and selecting inputs to be\nconsidered in the iteration\n147\nStep 3 \u2013 Choosing one or more elements of the system to refine\n148\nStep 4 \u2013 Choosing one or more design concepts that satisfy the inputs\nconsidered in the iteration\n148\nStep 5 \u2013 Instantiating architectural elements, allocating responsibilities, and\ndefining interfaces\n148\nStep 6 \u2013 Sketching views and recording design decisions\n149\nStep 7 \u2013 Performing analysis of current design and reviewing the iteration\ngoal and design objectives\n149\nStep 8 \u2013 Iterating if necessary\n149\nMicrosoft's technique for architecture and design\n150\nStep 1 \u2013 Identifying architecture objectives\n150\n", "page": 13, "type": "text", "section": "Page 13"}
{"text": "Table of Contents\n[ vii ]\nStep 2 \u2013 Identifying key scenarios\n151\nStep 3 \u2013 Creating application overview\n151\nDetermining your application type\n152\nIdentifying your deployment constraints\n152\nIdentifying important architecture design styles\n152\nDetermining relevant technologies\n152\nStep 4 \u2013 Identifying key issues\n153\nStep 5 \u2013 Defining candidate solutions\n153\nArchitecture-centric design method (ACDM)\n153\nStep 1 \u2013 Discovering architectural drivers\n155\nStep 2 \u2013 Establishing project scope\n155\nStep 3 \u2013 Creating notional architecture\n155\nStep 4 \u2013 Architectural review\n155\nStep 5 \u2013 Production go/no-go\n156\nStep 6 \u2013 Experiment planning\n156\nStep 7 \u2013 Experimenting with and refining the architecture\n156\nProduction planning and production\n157\nArchitecture development method (ADM)\n157\nThe Open Group Architecture Framework (TOGAF)\n157\nTOGAF architecture domains\n158\nTOGAF documentation\n158\nPhases of the ADM\n159\nPhase A \u2013 Architecture vision\n160\nPhase B \u2013 Business architecture\n160\nPhase C \u2013 Information systems architectures\n161\nPhase D \u2013 Technology architecture\n161\nPhase E \u2013 Opportunities and solutions\n161\nPhase F \u2013 Migration planning\n162\nPhase G \u2013 Implementation governance\n162\nPhase H \u2013 Architecture change management\n162\nTracking the progress of the software architecture's design\n162\nUsing a backlog to track the architecture design progress\n163\nPrioritizing the backlog\n163\nDIVE criteria\n164\nDependencies\n164\nInsure against risks\n164\nBusiness value\n164\nEstimated effort\n164\nActive and dynamic architecture backlogs\n164\nSummary\n165\nChapter 6: Software Development Principles and Practices\n167\nDesigning orthogonal software systems\n168\nLoose coupling\n168\nTypes of coupling\n169\nContent coupling\n169\nCommon coupling\n169\nExternal coupling\n170\n", "page": 14, "type": "text", "section": "Page 14"}
{"text": "Table of Contents\n[ viii ]\nControl coupling\n170\nStamp coupling (data-structured coupling)\n170\nData coupling\n170\nMessage coupling\n171\nNo coupling\n171\nThe Law of Demeter (LoD) / principle of least knowledge\n171\nDesigning for loose coupling\n171\nHigh cohesion\n172\nTypes of cohesion\n172\nCoincidental cohesion\n172\nLogical cohesion\n173\nTemporal cohesion\n173\nProcedural cohesion\n173\nCommunicational cohesion\n174\nSequential cohesion\n174\nFunctional cohesion\n174\nDesigning for high cohesion\n175\nMinimizing complexity\n175\nKISS principle \u2013 \"Keep It Simple, Stupid\"\n176\nOrigin of KISS\n176\nApplying KISS to software\n177\nDon't make it overly simple\n177\nDRY \u2013 \"Don't Repeat Yourself\"\n178\nCopy-and-paste programming\n178\nMagic strings\n178\nHow to avoid duplication\n180\nDon't make things overly DRY\n180\nInformation hiding\n180\nReasons for information hiding\n181\nWhat needs to be exposed/hidden?\n181\nYAGNI \u2013 \"You Aren't Gonna Need It\"\n182\nAvoid over-engineering a solution\n182\nSituations where YAGNI doesn't apply\n182\nSeparation of Concerns (SoC)\n183\nFollowing SOLID design principles\n184\nSingle Responsibility Principle (SRP)\n184\nOpen/Closed Principle (OCP)\n187\nLiskov Substitution Principle (LSP)\n189\nInterface Segregation Principle (ISP)\n191\nDependency Inversion Principle (DIP)\n194\nInversion of Control (IoC)\n195\nDependency Injection (DI)\n196\nBenefits of DI\n196\nDI patterns\n197\nDI containers\n200\nHelping your team succeed\n201\nUnit testing\n201\nWhat are unit tests?\n201\nBenefits of unit testing\n202\nProperties of a good unit test\n202\n", "page": 15, "type": "text", "section": "Page 15"}
{"text": "Table of Contents\n[ ix ]\nAtomic\n202\nDeterministic\n203\nAutomated and repeatable\n203\nIsolated and independent\n203\nEasy to set up and implement\n203\nFast\n203\nThe AAA pattern\n204\nArrange\n204\nAct\n204\nAssert\n204\nNaming conventions for unit tests\n204\nUnit test class names\n205\nUnit test method names\n205\nCode coverage for unit tests\n205\nKeeping unit tests up to date\n206\nSetting up development environments\n206\nProviding a README file\n207\nPair programming\n208\nBenefits of pair programming\n208\nUsing pair programming when it is needed\n209\nReviewing deliverables\n209\nCode reviews\n209\nFormal inspections\n210\nRoles for a formal inspection\n211\nInspection meeting and follow-up\n211\nWalkthroughs\n212\nSummary\n212\nChapter 7: Software Architecture Patterns\n213\nSoftware architecture patterns\n214\nUsing software architecture patterns\n214\nOverusing architecture patterns\n215\nUnderstanding the difference between architecture styles and architecture\npatterns\n215\nLayered architecture\n216\nOpen versus closed layers\n216\nTiers versus layers\n217\nAdvantages of layered architectures\n218\nDisadvantages of layered architectures\n219\nClient-server architecture (two-tier architecture)\n220\nUsing stored procedures for application logic\n221\nN-tier architecture\n222\nPresentation tier\n223\nBusiness tier\n223\nData tier\n224\nEvent-driven architecture\n224\nEvent channels\n224\nMessage queues\n225\nThe point-to-point channel pattern\n225\nMessage topics\n225\n", "page": 16, "type": "text", "section": "Page 16"}
{"text": "Table of Contents\n[ x ]\nThe publish-subscribe pattern\n225\nEvent-driven architecture topologies\n226\nThe mediator topology\n226\nEvent mediator implementations\n227\nThe broker topology\n227\nEvent processing styles\n228\nSimple event processing (SEP)\n229\nEvent stream processing (ESP)\n229\nComplex event processing (CEP)\n229\nTypes of event-driven functionality\n230\nEvent notification\n230\nEvent-carried state transfer\n230\nEvent-sourcing\n231\nThe Model-View-Controller pattern\n232\nModel\n233\nView\n233\nController\n234\nAdvantages of the MVC pattern\n234\nDisadvantages of the MVC pattern\n234\nThe Model-View-Presenter pattern\n235\nModel\n235\nView\n235\nPresenter\n236\nThe Model-View-ViewModel pattern\n236\nModel\n237\nView\n237\nViewModel\n237\nThe Command Query Responsibility Segregation pattern\n238\nThe query model and the command model\n239\nUsing event-sourcing with CQRS\n240\nAdvantages of CQRS\n240\nDisadvantages of CQRS\n241\nService-oriented architecture\n241\nWhat makes SOA different from other distributed solutions?\n242\nBenefits of using a SOA\n242\nIncreases alignment between business and technology\n243\nPromotes federation within an organization\n243\nAllows for vendor diversity\n243\nIncreases intrinsic interoperability\n244\nWorks well with agile development methodologies\n244\nCost-benefit analysis of SOA\n244\nChallenges with SOA\n245\nKey principles for service orientation\n246\nStandardized service contract\n246\nService loose coupling\n246\nService abstraction\n247\nService reusability\n247\n", "page": 17, "type": "text", "section": "Page 17"}
{"text": "Table of Contents\n[ xi ]\nService autonomy\n247\nService statelessness\n248\nService discoverability\n248\nService composability\n248\nSOA delivery strategies\n248\nThe top-down strategy\n249\nThe bottom-up strategy\n249\nThe agile strategy\n250\nService-oriented analysis\n250\nDefining business automation requirements\n250\nIdentifying existing automation systems\n251\nModeling candidate services\n251\nService layers and service models\n251\nTask service\n253\nEntity service\n253\nUtility service\n253\nService-oriented design\n253\nService interface design\n254\nService interface granularity\n255\nService registries\n255\nService descriptions\n256\nStructuring namespaces\n257\nOrchestration and choreography\n258\nSummary\n259\nChapter 8: Architecting Modern Applications\n260\nMonolithic architecture\n260\nBenefits of a monolithic architecture\n261\nDrawbacks of a monolithic architecture\n262\nMicroservice architecture\n263\nSOA done right\n263\nCharacteristics of microservice architecture\n264\nSmall, focused services\n264\nWell-defined service interfaces\n265\nAutonomous and independently deployable services\n265\nIndependent data storage\n265\nBetter fault isolation\n266\nCommunicating with lightweight message protocols\n266\nDesigning polyglot microservices\n267\nPolyglot programming\n268\nPolyglot persistence\n268\nUsing too many technologies\n269\nConsidering service granularity\n269\nNanoservices\n269\nSharing dependencies between microservices\n270\nStateless versus stateful microservices\n271\nService discovery\n271\nUsing a service registry\n271\nSelf-registration pattern\n272\n", "page": 18, "type": "text", "section": "Page 18"}
{"text": "Table of Contents\n[ xii ]\nThird-party registration pattern\n272\nTypes of service discovery\n273\nClient-side discovery pattern\n274\nServer-side discovery pattern\n274\nUsing microservices is not for everyone\n275\nServerless architecture\n276\nFunction as a Service (FaaS)\n277\nBackend as a Service (BaaS)\n278\nAdvantages of serverless architectures\n279\nCost savings\n279\nScalable and flexible\n279\nFocus on building your core products\n279\nPolyglot development\n280\nDisadvantages of serverless architectures\n280\nDifficulties with debugging and monitoring\n280\nMultitenancy issues\n280\nVendor lock-in\n281\nComplexity of designing many functions\n281\nNot as many runtime optimizations\n281\nStill immature\n281\nTaking a hybrid approach to serverless\n282\nFunction deployment\n282\nFunction invocation\n283\nSynchronous request\n283\nAsynchronous request (message queue)\n284\nMessage stream\n284\nBatch job\n285\nCloud-native applications\n285\nReasons to move to the cloud\n286\nReducing costs\n286\nGreater flexibility and scalability\n286\nAutomatic updates\n286\nDisaster recovery\n287\nWhat are cloud-native applications?\n287\nContainerized\n287\nDynamically orchestrated\n288\nMicroservices-oriented\n289\nNo downtime\n289\nContinuous delivery\n289\nSupport for a variety of devices\n290\nTwelve-factor apps\n290\nCodebase\n291\nDependencies\n291\nConfiguration\n292\nBacking services\n292\nBuild/Release/Run\n292\nProcesses\n293\nPort binding\n293\nConcurrency\n294\n", "page": 19, "type": "text", "section": "Page 19"}
{"text": "Table of Contents\n[ xiii ]\nDisposability\n294\nDevelopment/production parity\n294\nLogs\n295\nAdministrative processes\n295\nSummary\n296\nChapter 9: Cross-Cutting Concerns\n297\nCross-cutting concerns\n298\nGeneral guidelines for cross-cutting concerns\n299\nIdentifying cross-cutting concerns\n299\nUsing open-source and third-party solutions\n299\nMaintaining consistency\n299\nAvoiding scattered solutions\n300\nAvoiding tangled solutions\n300\nImplementing cross-cutting concerns\n301\nUsing dependency injection (DI)\n301\nUsing the decorator pattern\n302\nAspect-oriented programming\n306\nTypes of advice\n307\nWeaving\n308\nCompile-time weaving\n309\nRuntime weaving\n309\nTypes of cross-cutting concerns\n310\nCaching\n310\nConfiguration management\n311\nAuditing\n312\nSecurity\n312\nException management\n312\nLogging\n313\nUnderstanding log levels\n313\nRouting log entries\n314\nUsing Elastic Stack\n315\nElasticsearch\n315\nLogstash\n316\nKibana\n316\nBeats\n317\nCross-cutting concerns for microservices\n317\nLeveraging a microservice chassis\n317\nUsing the sidecar pattern\n318\nSummary\n320\nChapter 10: Performance Considerations\n321\nThe importance of performance\n321\nPerformance affects user experience\n322\nBounce rate\n322\nConversion rate\n322\nPerformance is a requirement\n323\n", "page": 20, "type": "text", "section": "Page 20"}
{"text": "Table of Contents\n[ xiv ]\nPage speed affects search rankings\n323\nDefining performance terminology\n323\nLatency\n324\nThroughput\n324\nBandwidth\n324\nProcessing time\n325\nResponse time\n325\nWorkload\n325\nUtilization\n325\nTaking a systematic approach to performance improvement\n326\nProfiling an application\n326\nInstrumentation\n327\nStatistical profilers\n327\nAnalyzing the results\n328\nImplementing changes\n328\nMonitoring results\n329\nServer-side caching\n329\nCaching data in distributed applications\n330\nUsing a private caching strategy\n330\nUsing a shared caching strategy\n330\nPriming the cache\n331\nInvalidating cached data\n331\nExpiring data\n331\nEvicting data\n331\nCache usage patterns\n332\nCache-aside pattern\n332\nRead-through pattern\n332\nWrite-through pattern\n333\nWrite-behind pattern\n333\nImproving web application performance\n333\nLeveraging HTTP caching\n333\nUsing a validation token\n334\nSpecifying cache-control directives\n334\nTaking advantage of compression\n335\nFile compression\n335\nLossless compression\n336\nLossy compression\n336\nContent-encoding (end-to-end) compression\n337\nMinifying resources\n337\nBundling resources\n338\n Using HTTP/2\n341\nMultiplexing\n342\nServer push\n342\nHeader compression\n343\nImplementing HTTP/2\n343\nUsing content delivery networks (CDNs)\n344\nOptimizing web fonts\n344\n", "page": 21, "type": "text", "section": "Page 21"}
{"text": "Table of Contents\n[ xv ]\nOptimizing the critical rendering path\n346\nDatabase performance\n347\nDesigning an efficient database schema\n347\nNormalizing a database\n347\nDenormalizing a database\n348\nIdentifying primary and foreign keys\n348\nSelecting the most appropriate data types\n349\nUsing database indexes\n349\nPrimary/clustered indexes\n349\nSecondary/non-clustered indexes\n350\nHaving too many indexes\n350\nScaling up and out\n351\nDatabase concurrency\n352\nDatabase transactions\n352\nOptimistic versus pessimistic concurrency control\n352\nCAP theorem\n353\nACID model\n354\nAtomicity\n354\nConsistency\n354\nIsolation\n354\nDurability\n355\nBASE model\n355\nBasic availability\n355\nSoft state\n355\nEventual consistency\n355\nSummary\n356\nChapter 11: Security Considerations\n357\nSecuring software systems\n358\nThe three states of information\n358\nThe CIA triad\n359\nConfidentiality\n359\nIntegrity\n359\nAvailability\n360\nThreat modeling\n360\nDecomposing an application\n361\nIdentifying and categorizing potential threats\n361\nSTRIDE threat model\n362\nSpoofing identity\n362\nTampering with data\n362\nRepudiation\n362\nInformation disclosure\n363\nDenial-of-service\n363\nElevation of Privilege\n363\nPrioritizing potential threats\n363\nDREAD risk assessment model\n364\nDamage potential\n364\nReproducibility\n364\nExploitability\n365\nAffected users\n365\n", "page": 22, "type": "text", "section": "Page 22"}
{"text": "Table of Contents\n[ xvi ]\nDiscoverability\n365\nResponses to threats\n365\nAvoiding the risk\n366\nTransferring the risk\n366\nAccepting the risk\n366\nMitigating the risk\n367\nTypes of security control\n367\nPhysical security controls\n367\nAdministrative controls\n368\nTechnical security controls\n369\nPrevention\n369\nDetection\n370\nResponse\n370\nSecure by design\n370\nMinimizing the attack surface\n371\nDefense in depth\n371\nPrinciple of least privilege (PoLP)\n371\nAvoiding security by obscurity\n372\nKeep software designs simple\n372\nSecure by default\n372\nDefault deny\n373\nValidating input\n373\nSecure the weakest link\n373\nSecurity must be usable\n373\nFail securely\n374\nCryptography\n374\nEncryption\n374\nSymmetric (secret key) encryption\n375\nAsymmetric (public key) encryption\n375\nCryptographic hash functions\n375\nIdentity and access management (IAM)\n377\nAuthentication\n377\nWhat is multi-factor authentication (MFA)?\n378\nAuthorization\n378\nStoring plaintext passwords\n379\nStoring encrypted passwords\n379\nStoring hashed passwords\n379\nUsing domain authentication\n380\nImplementing a centralized identity provider (IdP)\n380\nOAuth 2/OpenID Connect (OIDC)\n381\nOAuth 2 roles\n381\nJSON web token (JWT)\n382\nHeader\n383\nPayload\n383\nSignature\n384\nAuthorizing with the authorization server\n384\nMost common web application security risks\n385\nInjection\n385\n", "page": 23, "type": "text", "section": "Page 23"}
{"text": "Table of Contents\n[ xvii ]\nBroken authentication\n385\nSensitive data exposure\n386\nXML external entity (XXE) attack\n386\nBroken access control\n388\nSecurity misconfiguration\n388\nCross-site scripting (XSS)\n389\nInsecure deserialization\n389\nUsing components with known vulnerable components\n390\nInsufficient logging and monitoring\n390\nUnvalidated redirects and forwards\n391\nSummary\n391\nChapter 12: Documenting and Reviewing Software Architectures\n393\nUses of software architecture documentation\n394\nCommunicating your architecture to others\n394\nAssisting the development team\n395\nEducates team members\n395\nProviding input for software architecture reviews\n395\nAllowing for the reuse of architectural knowledge\n396\nHelp the software architect\n396\nCreating architecture descriptions (ADs)\n397\nSoftware architecture views\n397\nSoftware architecture notations\n398\nInformal software architecture notations\n398\nSemiformal software architecture notations\n399\nFormal software architecture notations\n399\nIncluding design rationales\n400\nOverview of the Unified Modeling Language (UML)\n401\nTypes of modeling\n401\nClass diagrams\n401\nVisibility\n402\nAssociation\n402\nAggregation\n403\nComposition\n404\nMultiplicity\n404\nDependency\n405\nGeneralization/specialization\n405\nRealization\n406\nComponent diagrams\n407\nPackage diagrams\n409\nDeployment diagrams\n410\nUse case diagrams\n411\nSequence diagrams\n413\nLifeline\n414\nActivation boxes\n414\nMessages\n414\nLoops\n415\nOptional flows\n416\nAlternative flows\n416\nActivity diagrams\n417\n", "page": 24, "type": "text", "section": "Page 24"}
{"text": "Table of Contents\n[ xviii ]\nStart/end nodes\n418\nActions/Control flow\n419\nDecision/merge nodes\n419\nFork/join nodes\n420\nReviewing software architectures\n420\nSoftware architecture analysis method (SAAM)\n421\nScenario-based analysis of software architecture\n421\nSAAM steps\n422\nStep 1 \u2013 Develop scenarios\n422\nStep 2 \u2013 Describe the architecture\n422\nStep 3 \u2013 Classify and prioritize scenarios\n422\nStep 4 \u2013 Evaluate scenarios\n423\nStep 5 \u2013 Assess scenario interaction\n423\nStep 6 \u2013 Create an overall evaluation\n423\nArchitecture tradeoff analysis method (ATAM)\n424\nATAM participant roles\n424\nATAM phases\n424\nPhase 0 \u2013 Partnership and preparation\n425\nPhase 1 \u2013 Evaluation\n425\nPhase 2 \u2013 Evaluation (continued)\n428\nPhase 3 \u2013 Follow-up\n429\nActive design review (ADR)\n429\nADR steps\n430\nStep 1 \u2013 Prepare the documentation for review\n430\nStep 2 \u2013 Identify the specialized reviews\n431\nStep 3 \u2013 Identify the reviewers needed\n431\nStep 4 \u2013 Design the questionnaires\n431\nStep 5 \u2013 Conduct the review\n431\nActive reviews of intermediate designs (ARID)\n432\nARID participant roles\n432\nARID phases\n432\nPhase 1 \u2013 Pre-meeting\n433\nPhase 2 \u2013 Review meeting\n434\nSummary\n435\nChapter 13: DevOps and Software Architecture\n436\nDevOps\n437\nCALMS\n437\nCulture\n438\nAutomation\n439\nLean\n440\nMeasurement\n441\nSharing\n441\nWhy DevOps?\n441\nDevOps toolchain\n442\nDevOps practices\n444\nContinuous integration (CI)\n445\nAutomated builds\n445\nSoftware versioning\n446\nAutomated testing\n447\nContinuous delivery (CD)\n448\n", "page": 25, "type": "text", "section": "Page 25"}
{"text": "Table of Contents\n[ xix ]\nContinuous deployment\n448\nArchitecting for DevOps\n449\nImportant quality attributes for DevOps\n449\nSome architecture patterns complement DevOps\n451\nDeploying to the cloud\n451\nCloud types\n452\nPublic cloud\n452\nPrivate cloud\n452\nHybrid cloud\n453\nCloud models\n454\nInfrastructure as a service (IaaS)\n455\nContainers as a Service (CaaS)\n456\nPlatform as a Service (PaaS)\n458\nServerless/Function as a Service (FaaS)\n459\nSoftware as a Service (SaaS)\n459\nSummary\n460\nChapter 14: Architecting Legacy Applications\n461\nLegacy applications\n461\nIssues with legacy applications\n462\nWhy are legacy applications used?\n463\nMore than just code\n464\nRefactoring legacy applications\n464\nMaking legacy code testable\n465\nBenefits of unit testing\n465\nRefactoring for unit tests\n466\nWhere to start writing tests?\n466\nRemoving redundant code\n467\nUnreachable code\n467\nDead code\n468\nCommented-out code\n468\nDuplicate code\n468\nUsing tools to refactor\n469\nMaking small, incremental changes\n469\nTransforming monoliths to microservices\n469\nMigrating to the cloud\n470\nThe 6 R's\n471\nRemove (or retire)\n471\nRetain\n471\nReplatform\n472\nRehost\n472\nRepurchase\n472\nRefactor (or re-architect)\n472\nMoving to an agile approach\n473\nModernizing build and deployment processes\n473\nAutomating the build and deployment processes\n474\nPracticing continuous integration (CI)\n474\n", "page": 26, "type": "text", "section": "Page 26"}
{"text": "Table of Contents\n[ xx ]\nPracticing continuous delivery (CD)\n475\nUpdating the build tools\n475\nIntegrating with legacy applications\n475\nConfirming the need for integration\n476\nDetermining the type of integration\n476\nSharing functionality between systems\n477\nPerforming data integration\n477\nSummary\n477\nChapter 15: The Soft Skills of Software Architects\n479\nSoft skills\n479\nCommunication\n480\nCommunicating the architecture\n480\nCommunicating about quality attributes\n481\nCommunicating expectations\n481\nThe 7 Cs of communication\n481\nClarity\n482\nConciseness\n482\nConcreteness\n483\nCourteousness\n483\nConsideration\n483\nCorrectness\n484\nCompleteness\n484\nListening skills\n485\nHearing is not listening\n485\nShowing empathy\n485\nTips for effective listening\n485\nGiving presentations\n486\nThe 4 Ps of presentations\n487\nPlan\n487\nPrepare\n487\nPractice\n488\nPresent\n489\nLeadership\n489\nGetting others to follow you\n489\nDealing with challenges\n490\nBeing a technical leader\n490\nTaking responsibility\n490\nFocusing on others\n491\nDelegating tasks\n491\nDriving change\n492\nCommunication and leadership   \n492\nMentoring others\n493\nLeading by example\n493\nDepending on others\n494\nNegotiation\n494\nHow negotiation skills may be used\n495\n", "page": 27, "type": "text", "section": "Page 27"}
{"text": "Table of Contents\n[ xxi ]\nInformal/formal negotiations\n495\nWorking with remote resources\n497\nBenefits of using remote resources\n497\nChallenges when using remote resources\n497\nCommunication\n498\nCultural differences\n498\nImpromptu meetings\n498\nNew employee onboarding\n498\nWork quality\n499\nConfidential company data\n499\nSummary\n500\nChapter 16: Evolutionary Architecture\n501\nChange is inevitable\n501\nReasons for change\n502\nExpecting change\n502\nLehman's laws of software evolution\n503\nLehman's software categories\n503\nS-type systems\n503\nP-type systems\n504\nE-type systems\n504\nThe laws\n504\nLaw I \u2013 Continuing change\n505\nLaw II \u2013 Increasing complexity\n505\nLaw III \u2013 Self-regulation\n505\nLaw IV \u2013 Conservation of organizational stability\n506\nLaw V \u2013 Conservation of familiarity\n506\nLaw VI \u2013 Continuing growth\n506\nLaw VII \u2013 Declining quality\n506\nLaw VIII \u2013 Feedback system\n507\nDesigning evolutionary architectures\n507\nMaking guided architectural changes\n508\nFitness functions\n508\nCategories of fitness functions\n509\nExamples of fitness functions\n510\nMaking incremental changes\n511\nArchitectural changes across multiple dimensions\n512\nLoosely coupled architectures\n512\nDesigning evolvable APIs\n513\nApplying Postel's Law to APIs\n514\nUsing standards in your software system\n514\nLast responsible moment (LRM)\n515\nSummary\n515\nChapter 17: Becoming a Better Software Architect\n517\nPracticing continuous learning\n518\nImproving the breadth and depth of your knowledge\n518\nAvoiding the law of the instrument\n519\n", "page": 28, "type": "text", "section": "Page 28"}
{"text": "Table of Contents\n[ xxii ]\nFinding the time for learning\n519\nWays to keep your skills sharp\n520\nParticipating in open source projects\n521\nCreating your own open source project\n521\nWriting your own blog\n522\nIncreasing your visibility\n523\nStarting your own blog\n523\nThings to avoid\n524\nSpending time teaching others\n524\nFinding opportunities to teach\n525\nBeing a mentor\n525\nTrying new technologies\n526\nContinuing to write code\n527\nAssigning yourself coding tasks\n527\nWorking on your own project\n527\nReading code\n528\nAttending user groups and conferences\n528\nPresenting at a user group or conference\n529\nMeeting new people\n530\nTaking responsibility for your work\n530\nAttending to your well-being\n531\nBeing proud of your work\n531\nSummary\n531\nOther Books You May Enjoy\n533\nIndex\n536\n", "page": 29, "type": "text", "section": "Page 29"}
{"text": "Preface\nModern software systems are complex, and the software architect role is a challenging one.\nThis book was written to help software developers transition into the role of a software\narchitect, and to assist existing software architects to be successful in their role. It\nhelps readers understand how being a software architect is different than being a developer\nand what it takes to be an effective software architect.\nThis comprehensive guide to software architecture begins by explaining what software\narchitecture entails, the responsibilities of the software architect position, and what you will\nbe expected to know. Software architects must have technical and non-technical skills, and\nthey must have both breadth and depth of knowledge.\nThe book progress to covering non-technical topics such as the importance of\nunderstanding your organization's business, working in the context of an organization, and\ngathering requirements for software systems. It then takes a deep dive into technical topics\nsuch as software quality attributes, software architecture design, software development\nbest practices, architecture patterns, how to improve performance, and security\nconsiderations.\nAfter reading this book, you should have a familiarity with the many topics related to\nsoftware architecture and understand how to be a software architect. Technologies and\npractices may change over time, but the book lays a strong foundation on which you can\nbuild a successful career as a software architect.\nWho this book is for\nThis book is aimed at senior developers and software architects who want to learn how to\nbe a successful software architect. Readers should be experienced software development\nprofessionals who want to advance in their career and become a software architect. It\ncovers a wide range of topics that will help readers learn what it takes to be effective in the\nsoftware architect role.\n", "page": 30, "type": "text", "section": "Page 30"}
{"text": "Preface\n[ 2 ]\nWhat this book covers\nChapter 1, The Meaning of Software Architecture, begins the book by providing a definition\nof software architecture. The book establishes what makes up a software architecture and\nthe reasons why it is important to a software system. It also details the software architect\nrole, including the responsibilities of software architects and what they are expected to\nknow.\nChapter 2, Software Architecture in an Organization, focuses on software architecture in the\ncontext of an organization. It covers the different types of software architect roles and\nsoftware development methodologies that you may encounter. Non-technical topics such\nas project management, office politics, and risk management are explained. The\ndevelopment of software product lines and the creation of architectural core assets are also\ncovered.\nChapter 3, Understanding the Domain, discusses the business aspects of being a software\narchitect. It covers topics such as familiarizing yourself with your organization's business,\ndomain-driven design (DDD), and how to effectively elicit requirements for the software\nsystem from stakeholders.\nChapter 4, Software Quality Attributes, covers software quality attributes and their\nimportance to a software architecture. Some common software quality attributes are\npresented, including maintainability, usability, availability, portability, interoperability,\nand testability.\nChapter 5, Designing Software Architectures, concentrates on the important topic of software\narchitecture design. It details what is involved with architecture design and its importance\nto a software system. The chapter discusses different approaches to architecture design, the\ndrivers for it, and the design principles that can be leveraged during the process.\nThe chapter presents the use of various systematic approaches to software architecture\ndesign, including attribute-driven design (ADD), Microsoft's technique for architecture\nand design, the architecture-centric design method (ACDM), and the architecture\ndevelopment method (ADM).\nChapter 6, Software Development Principles and Practices, describes proven software\ndevelopment principles and practices that can be used to build high-quality software\nsystems. Concepts such as loose coupling and high cohesion are covered, as well as\nprinciples such as KISS, DRY, information hiding, YAGNI, and the Separation of Concerns\n(SoC).\n", "page": 31, "type": "text", "section": "Page 31"}
{"text": "Preface\n[ 3 ]\nThe chapter includes a discussion of the SOLID principles, which include the single\nresponsibility, open/closed, Liskov substitution, interface segregation, and dependency\ninversion principles. The chapter closes with topics related to helping your team succeed,\nincluding unit testing, setting up development environments, pair programming, and\nreviewing deliverables.\nChapter 7, Software Architecture Patterns, discusses one of the most useful software\narchitecture design concepts. Learning the architecture patterns that are available to you\nand when to properly apply them is a key skill for software architects. The chapter details a\nnumber of software architecture patterns, including layered architecture, event-driven\narchitecture (EDA), Model-View-Controller (MVC), Model-View-Presenter (MVP),\nModel-View-ViewModel (MVVM), Command Query Responsibility Segregation\n(CQRS), and Service-Oriented Architecture (SOA).\nChapter 8, Architecting Modern Applications, explains the software architecture patterns and\nparadigms that are used with modern applications deployed to the cloud. After describing\na monolithic architecture, the chapter details microservices architecture, serverless\narchitecture, and cloud-native applications.\nChapter 9, Cross-Cutting Concerns, places its focus on functionality that is used in multiple\nareas of the system. It explains how to handle cross-cutting concerns in your applications.\nTopics covered include using Dependency Injection (DI), the decorator pattern, and\naspect-oriented programming (AOP) to implement cross-cutting concerns. The chapter\nalso provides a look at different cross-cutting concerns, including caching, configuration\nmanagement, auditing, security, exception management, and logging.\nChapter 10, Performance Considerations, takes a close look at performance. It describes the\nimportance of performance and techniques to improve it. Topics such as server-side\ncaching and database performance are discussed. An examination of web application\nperformance is included in the chapter, including coverage of HTTP caching, compression,\nminimizing and bundling of resources, HTTP/2, content delivery networks (CDNs),\noptimizing web fonts, and the critical rendering path.\nChapter 11, Security Considerations, covers the critical topic of software application security.\nSecurity concepts such as the confidentiality, integrity, and availability (CIA) triad and\nthreat modeling are presented. The chapter provides readers with various principles and\npractices for creating software that is secure by design.\n", "page": 32, "type": "text", "section": "Page 32"}
{"text": "Preface\n[ 4 ]\nChapter 12, Documenting and Reviewing Software Architectures, places its focus on software\narchitecture documentation and reviewing software architectures. It describes the various\nuses for software architecture documentation and explains how to use UML to document a\nsoftware architecture. The chapter discusses various software architecture review methods,\nincluding the software architecture analysis method (SAAM), architecture tradeoff\nanalysis method (ATAM), active design review (ADM), and active reviews of\nintermediate designs (ARID).\nChapter 13, DevOps and Software Architecture, provides coverage of the culture, practices,\ntools, and culture of DevOps. The chapter explains key DevOps practices such as\ncontinuous integration (CI), continuous delivery (CD), and continuous deployment.\nChapter 14, Architecting Legacy Applications, provides readers with an understanding of\nhow to work with legacy applications. The widespread use of legacy applications makes\nthis topic important for software architects. The chapter covers refactoring legacy\napplications and how to migrate them to the cloud. It discusses modernizing build and\ndeployment processes for legacy applications as well as how to integrate with them.\nChapter 15, The Soft Skills of Software Architects, is all about the soft skills that software\narchitects should possess to be an effective software architect. After describing what soft\nskills are, the chapter proceeds to topics such as communication, leadership, negotiation,\nand working with remote resources.\nChapter 16, Evolutionary Architecture, teaches how to design software systems so that they\nhave the ability to adapt to change. It explains that change is inevitable, so software\narchitects should design software architectures that can evolve over time. The chapter\nexplains some of the ways that change can be handled and it introduces the use of fitness\nfunctions to ensure that an architecture continues to meet its desired architectural\ncharacteristics as it undergoes change.\nChapter 17, Becoming a Better Software Architect, stresses to readers that the process of career\ndevelopment is an ongoing one. After becoming a software architect, one must seek to\ncontinuously gain new knowledge and improve their skills. The chapter details ways that a\nsoftware architect can practice self-improvement, including continuous learning,\nparticipating in open source projects, writing your own blog, spending time teaching\nothers, trying new technologies, continuing to write code, attending user groups and\nconferences, taking responsibility for your work, and attending to your general well-being.\n", "page": 33, "type": "text", "section": "Page 33"}
{"text": "Preface\n[ 5 ]\nTo get the most out of this book\nAlthough readers should have experience of software development, no specific\nprerequisites are required to begin reading this book. All of the information that you need\nis contained in the various chapters. The book does not require knowledge of any particular\nprogramming language, framework, or tool. The code snippets in the book that illustrate\nvarious concepts are written in C#, but they are simple enough that prior C# experience is\nnot necessary.\nDownload the color images\nWe also provide a PDF file that has color images of the screenshots/diagrams used in this\nbook. You can download it\nhere: https://www.packtpub.com/sites/default/files/downloads/SoftwareArchitectsH\nandbook_ColorImages.pdf.\nConventions used\nThere are a number of text conventions used throughout this book.\nCodeInText: Indicates code words in text, database table names, folder names, filenames,\nfile extensions, pathnames, dummy URLs, user input, and Twitter handles. Here is an\nexample: \"Now we can use that constant in our GetFilePath method.\"\nA block of code is set as follows:\npublic string GetFilePath()\n{\n    string result = _cache.Get(FilePathCacheKey);\n    if (string.IsNullOrEmpty(result))\n    {\n        _cache.Put(FilePathCacheKey, DetermineFilePath());\n        result = _cache.Get(FilePathCacheKey);\n    }\n    return result;\n}\n", "page": 34, "type": "text", "section": "Page 34"}
{"text": "Preface\n[ 6 ]\nWhen we wish to draw your attention to a particular part of a code block, the relevant lines\nor items are set in bold:\npublic string GetFilePath()\n{\n    string result = _cache.Get(FilePathCacheKey);\n    if (string.IsNullOrEmpty(result))\n    {\n        _cache.Put(FilePathCacheKey, DetermineFilePath());\n        result = _cache.Get(FilePathCacheKey);\n    }\n    return result;\n}\nBold: Indicates a new term, an important word, or words that you see onscreen. For\nexample, words in menus or dialog boxes appear in the text like this. Here is an example:\n\"In a direct dependency graph, at compile-time, Class A references Class B, which\nreferences Class C\"\nWarnings or important notes appear like this.\nTips and tricks appear like this.\nGet in touch\nFeedback from our readers is always welcome.\nGeneral feedback: Email feedback@packtpub.com and mention the book title in the\nsubject of your message. If you have questions about any aspect of this book, please email\nus at questions@packtpub.com.\nErrata: Although we have taken every care to ensure the accuracy of our content, mistakes\ndo happen. If you have found a mistake in this book, we would be grateful if you would\nreport this to us. Please visit www.packtpub.com/submit-errata, selecting your book,\nclicking on the Errata Submission Form link, and entering the details.\n", "page": 35, "type": "text", "section": "Page 35"}
{"text": "Preface\n[ 7 ]\nPiracy: If you come across any illegal copies of our works in any form on the Internet, we\nwould be grateful if you would provide us with the location address or website name.\nPlease contact us at copyright@packtpub.com with a link to the material.\nIf you are interested in becoming an author: If there is a topic that you have expertise in\nand you are interested in either writing or contributing to a book, please visit\nauthors.packtpub.com.\nReviews\nPlease leave a review. Once you have read and used this book, why not leave a review on\nthe site that you purchased it from? Potential readers can then see and use your unbiased\nopinion to make purchase decisions, we at Packt can understand what you think about our\nproducts, and our authors can see your feedback on their book. Thank you!\nFor more information about Packt, please visit packtpub.com.\n", "page": 36, "type": "text", "section": "Page 36"}
{"text": "1\nThe Meaning of Software\nArchitecture\nA comprehensive look at software architecture must first begin with its definition. This\nchapter provides reasons as to why software architecture plays an important role in a\nsoftware project, and the benefits of having a good architectural design.\nIt is also important to understand the stakeholders and team members who are affected by\nthe software architecture of a system. The chapter will go into detail about the software\narchitect's role, what software architects are supposed to know, and whether the role is\nright for you.\nIn this chapter, we will cover the following topics:\nWhat is software architecture?\nWhy is software architecture important?\nWho are the consumers of software architectures?\nWhat is the software architect role?\nWhat is software architecture?\nWhat exactly is software architecture? You probably have your own ideas about what it is,\nbased on your knowledge and experiences. Certainly, there are plenty of definitions out\nthere. If you do an online search or ask various friends and colleagues, you will get varying\nanswers. The definition is somewhat subjective and influenced by the viewpoints and\nperceptions of the individual who is providing the definition. However, there are some core\nconcepts that are essential to software architecture, and before we delve into deeper topics,\nestablishing a common understanding of what software architecture entails is imperative.\n", "page": 37, "type": "text", "section": "Page 37"}
{"text": "The Meaning of Software Architecture\nChapter 1\n[ 9 ]\nUsing the word architecture for software originated from similarities with the construction\nindustry. When the term was first used, the Waterfall software development methodology\nwas common and it dictated that large, up-front designs needed to be completed before any\ncode was written. Similar to the architecture of a building, which necessitates a lot of\nplanning before construction takes place, so it was with software as well.\nIn modern software design, the relationship between the construction and software\nindustries is no longer as close. Software methodologies now focus on developing software\napplications that are highly adaptable and can be changed easily over time, resulting in less\nof a need for rigid, upfront planning. However, software architecture still consists of early\ndesign decisions that can be difficult to change later.\nISO/IEC/IEEE 42010 standard definition\nThere is a standard definition for software architecture, which resulted from a joint effort\nbetween the International Organization for Standardization (ISO) and the Institute of\nElectrical and Electronics Engineers (IEEE). ISO/IEC/IEEE 42010 systems and software\nengineering's  architecture description is an international standard that defines software\narchitecture as:\n\"Fundamental concepts or properties of a system in its environment embodied in its\nelements, relationships, and in the principles of its design and evolution.\"\nThe standard makes the following main points:\nA software architecture is a fundamental part of a software system\nA software system is situated in an environment, and its software architecture\ntakes into consideration the environment in which it must operate\nAn architecture description documents the architecture and communicates to\nstakeholders how the architecture meets the system's needs\nArchitecture views are created from the architecture description, and each view\ncovers one or more architecture concerns of the stakeholders\n", "page": 38, "type": "text", "section": "Page 38"}
{"text": "The Meaning of Software Architecture\nChapter 1\n[ 10 ]\nWhat makes up a software architecture?\nIn the book, Software Architecture in Practice, 2nd Edition, a definition of software architecture\nis given as:\n\"The software architecture of a program or computing system is the structure or\nstructures of the system, which comprise software elements, the externally visible\nproperties of those elements, and the relationships among them.\"\nA software system contains structures, and this definition notes that a software system is\nmade up of one or more of them. It is the combination of these that forms the overall\nsoftware architecture. A large software project may have multiple teams working on it,\neach responsible for a particular structure.\nSoftware architecture is an abstraction\nSoftware architecture is an abstraction of a software system. The structures of a software\nsystem consist of its elements. Software architecture concerns itself with defining and\ndetailing the structures, their elements, and the relationships of those elements with each\nother.\nSoftware architecture focuses on the public aspects of the elements, and how they interact\nwith each other. For elements, this may take the form of their public interfaces. It does not\ndeal with the private implementation details of the elements. While the behavior of the\nelements does not have to be exhaustively documented, care should be taken in\nunderstanding how elements have to be designed and written so that they can properly\ninteract with each other.\nSoftware architecture is about the important stuff\nComputer scientist Ralph Johnson, who co-authored Design Patterns: Elements of Reusable\nObject-Oriented Software, once said:\n\"Architecture is about the important stuff. Whatever that is.\"\nSoftware projects vary, and the amount of design effort, time, focus, and documentation\ndevoted to particular aspects of a software architecture differ. Ultimately, software\narchitecture consists of important design decisions that shape the system. It is made up of\nthe structures and components that are significant to the quality, longevity, and usefulness\nof the system.\n", "page": 39, "type": "text", "section": "Page 39"}
{"text": "The Meaning of Software Architecture\nChapter 1\n[ 11 ]\nSoftware architecture consists of some of the earliest decisions that are made for a software\nsystem and some of the hardest to change. In modern software development, the\narchitecture should anticipate change, and be designed in such a way as to maximize the\npotential of adapting and evolving to this change. We will be discussing evolutionary\narchitecture in Chapter 16, Evolutionary Architecture.\nWhy is software architecture important?\nWhy should we care about software architecture anyway? Sometimes a developer just\nwants to jump right in and start coding.\nSoftware architecture is the foundation of a software system. Like other types of\nengineering, the foundation has a profound effect on the quality of what is built on top of it.\nAs such, it holds a great deal of importance in terms of the successful development, and\neventual maintenance, of the system.\nSoftware architecture is a series of decisions. Some of the earliest decisions come from\ndesigning the architecture, and these carry a high degree of importance because they affect\nthe decisions that come after it.\nAnother reason software architecture is important is because all software systems have an\narchitecture. Even if it comprised just one structure with one element, there is an\narchitecture. There are software systems that don't have a formal design and others that\ndon't formally document the architecture, but even these systems still have an architecture.\nThe greater the size and complexity of a software system, the more you will need a well\nthought-out architecture in order to succeed. Software architecture provides a number of\nbenefits when done properly, which greatly increase the chances that the software system\nwill succeed.\nA proper foundation laid down by a software system's architecture yields a number of\nbenefits. Let's take a deeper look at those benefits.\nDefining a solution to meet requirements\nSoftware strives to meet all functional, non-functional, technical, and operational\nrequirements. Working closely with stakeholders, such as domain experts, business\nanalysts, product owners, and end users, allows requirements to be identified and\nunderstood. A software architecture defines a solution that will meet those requirements.\n", "page": 40, "type": "text", "section": "Page 40"}
{"text": "The Meaning of Software Architecture\nChapter 1\n[ 12 ]\nSoftware architecture is the foundation for software, so software systems that lack a solid\narchitecture make it more difficult to meet all of the requirements. Poor architectures will\nlead to implementations that fail to meet the measurable goals of quality attributes, and\nthey are typically difficult to maintain, deploy, and manage.\nEnabling and inhibiting quality attributes \nSoftware architecture either enables quality attributes or inhibits them. Quality attributes\nare measurable and testable properties of a system. Some examples of quality attributes\ninclude maintainability, interoperability, security, and performance.\nThey are non-functional requirements of a software system as opposed to its features, which\nare functional requirements. Quality attributes and how they satisfy the stakeholders of the\nsystem are critical, and software architecture plays a large role in ensuring that quality\nattributes are satisfied. The design of a software architecture can be made to focus on\ncertain quality attributes at the cost of others. Quality attributes may be in conflict with\neach other. A software architecture, when designed properly, sets out to achieve agreed-\nupon and validated requirements related to quality attributes.\nGiving you the ability to predict software system\nqualities\nWhen you look at a software architecture and its documentation, you can predict the\nsoftware system's qualities. Making architecture decisions based on quality attributes\nmakes it easier to fulfill those requirements. You want to start thinking about quality\nattributes as early as possible in the software development process as it is much more\ndifficult (and costly) to make changes to fulfill them later. By thinking about them up front,\nand using modeling and analysis techniques, we can ensure that the software architecture\ncan meet its non-functional requirements.\nIf you are not able to predict if a software system will fulfill quality attributes until it is\nimplemented and tested, then costly and time-consuming rework may be necessary. A\nsoftware architecture allows you to predict a software system's qualities and avoid costly\nrework.\n", "page": 41, "type": "text", "section": "Page 41"}
{"text": "The Meaning of Software Architecture\nChapter 1\n[ 13 ]\nEasing communication among stakeholders\nSoftware architecture and its documentation allow you to communicate the software\narchitecture and explain it to others. It can form the basis for discussions related to aspects\nof the project, such as costs and duration. We will discuss this topic further when we go\ninto detail about software architecture in an organization.\nA software architecture is abstract enough that many stakeholders, with little or no\nguidance, should be able to reason about the software system. Although different\nstakeholders will have different concerns and priorities in terms of what they want to know\nabout the architecture, providing a common language and architecture design artifacts\nallows them to understand the software system. It is particularly useful for large, complex\nsystems that would otherwise be too difficult to fully understand. As requirements and\nother early decisions are made for the software system, a formal software architecture plays\nan important role and facilitates negotiations and discussions.\nManaging change\nChanges to a software system are inevitable. The catalyst for change can come from the\nmarket, new requirements, changes to business processes, technology advances, and bug\nfixes, among other things.\nSome view software architecture as inhibiting agility and would prefer to just let it emerge\nwithout up-front design. However, a good software architecture helps with both\nimplementing and managing changes. Changes fall into one of the following categories:\nLimited to a single element\nInvolve a combination of elements, but do not require any architectural changes\nRequire an architectural change\nSoftware architecture allows you to manage and understand what it would take to make a\nparticular change. Furthermore, a good architecture reduces complexity so that most of the\nchanges that need to be made can be limited to a single element or just a few elements,\nwithout having to make architectural changes.\n", "page": 42, "type": "text", "section": "Page 42"}
{"text": "The Meaning of Software Architecture\nChapter 1\n[ 14 ]\nProviding a reusable model\nAn established architecture might be used again within an organization for other products\nin a product line, particularly if the products have similar requirements. We'll discuss an\norganization's product lines, reuse of architecture, and the benefits in the next chapter. For\nnow, simply recognize that, once a software architecture is completed, documented,\nunderstood, and used in a successful implementation, it can be reused.\nWhen code is reused, resources, such as time and money, are saved. More importantly, the\nquality of software that takes advantage of reuse is increased because the code has already\nbeen tested and proven. The increase in quality alone translates to savings in resources.\nWhen a software architecture is reused, it is not just code that is reused. All of the early\ndecisions that shaped the original architecture are leveraged as well. The thought and effort\nthat went into the requirements necessary for the architecture, particularly non-functional\nrequirements, may be applicable to other products. The effort that went into making those\ndecisions does not necessarily have to be repeated. The experience gained from the original\narchitectural design can be leveraged for other software systems.\nWhen a software architecture is reused, it is the architecture itself, and not just the software\nproduct, that becomes an asset to the organization.\nImposing implementation constraints\nA software architecture introduces constraints on implementation and restricts design\nchoices. This reduces the complexity of a software system and prevents developers from\nmaking incorrect decisions.\nIf the implementation of an element conforms to the designed architecture, then it is\nabiding by the design decisions made by the architecture. Software architecture, when done\nproperly, enables developers to accomplish their objectives and prevents them from\nimplementing things incorrectly.\nImproving cost and effort estimates\nProject managers ask questions such as: When is it going to be done? How long is it going\nto take? How much is it going to cost? They need this type of information to properly plan\nresources and monitor progress. One of the many duties of a software architect is to assist\nproject management by providing this type of information and assisting with determining\nthe necessary tasks and estimates for those tasks.\n", "page": 43, "type": "text", "section": "Page 43"}
{"text": "The Meaning of Software Architecture\nChapter 1\n[ 15 ]\nThe design of the software architecture itself affects what types of task will be necessary for\nimplementation. As a result, work-breakdown of tasks is dependent on the software\narchitecture and the software architect can assist project management with the creation of\nthe tasks.\nTwo major approaches to project management estimation are as follows:\nTop-down approach: This starts with the final deliverables and goals and breaks\nthem down into smaller packages of work\nBottom-up approach: This starts with specific tasks first, and groups them\ntogether into packages of work\nFor some projects, a project manager may take a more top-down approach, while\ndevelopers who are going to be working on specific tasks may take a bottom-up\nperspective. With the experience and knowledge that most software architects possess, they\ncan potentially assist with either approach. A combination of these approaches, where tasks\nare looked at from both viewpoints, can lead to the best estimates.\nIt can be helpful when project managers, the software architect, and the developers work\ntogether to provide estimates. The most accurate estimates can be obtained by mutual\ndiscussions between team members until a consensus is achieved. Sometimes during the\nconsensus building, someone on the team will provide an insight that others had not\npreviously considered, allowing everyone to rethink their position and possibly revise their\nestimates.\nA software system with accurate requirements that are reflected in the software\narchitecture can avoid costly rework that would be necessary if key requirements were\nmissed. In addition, a well-thought-out architecture reduces complexity, allowing it to be\neasily reasoned about and understood. Reduced complexity can result in more accurate cost\nand effort estimates.\nServes as training for team members\nThe system's architecture and its documentation serve as training for the developers on the\nteam. By learning the various structures and elements of the system, and how they are\nsupposed to interact, they learn the proper way in which the functionality is to be\nimplemented.\nA software development team may experience change, such as having new team members\njoin or existing ones leave. The introduction and orientation of new members to a team\noften takes time. A well-thought-out architecture can make it easier for developers to\ntransition to the team.\n", "page": 44, "type": "text", "section": "Page 44"}
{"text": "The Meaning of Software Architecture\nChapter 1\n[ 16 ]\nThe maintenance phase of a software system can be one of the longest and costliest phases\nof a software project. Like new team members introduced during development, it is\ncommon for different developers to work on the system over time, including those\nintroduced to maintain it. Having a solid architecture available to teach and bring aboard\nnew developers can provide an important advantage.\nSoftware architecture is not a silver bullet\nThe Mythical Man-Month by Frederick P. Brooks is one of the seminal texts in software\nproject management. It contains various essays on software engineering. Although this\nbook was written some time ago, and some of the references are now outdated, it provides\nthought-provoking advice about software development that is timeless and still applicable\ntoday:\n\"There is no single development, in either technology or management technique, which by\nitself promises even one order-of-magnitude improvement within a decade in productivity,\nin reliability, in simplicity.\"\nFred Brooks 1986 essay, No Silver Bullet \u2013 Essence and Accident in Software Engineering, which\nis included in the twentieth anniversary edition of the book, begins with this quote. It\nessentially conveys the idea that there is no silver bullet in software development.\nSoftware architecture, as well, is not a silver bullet. Although we have covered a number of\nreasons why software architecture is important, there is no specific architecture or\ncombination of components that will serve as a silver bullet. It can't be thought of as a\nmagical solution that will solve all problems. As we will learn in more detail later, software\narchitectures are about compromises between different and sometimes conflicting\nrequirements. Each architectural approach has pros and cons that must be weighed and\nevaluated. No one approach should be viewed as a silver bullet.\nWho are the consumers of software\narchitectures?\nWhen we create a software architecture, who is it for? There are a variety of stakeholders in\na software system, such as the end users of the system, business analysts, domain experts,\nquality assurance personnel, managers, those who may integrate with the system, and\noperations staff members. Each of these stakeholders is affected by the software\narchitecture to some degree. While certain stakeholders will have access to, and be\ninterested in, examining the software architecture and its documentation, others will not.\n", "page": 45, "type": "text", "section": "Page 45"}
{"text": "The Meaning of Software Architecture\nChapter 1\n[ 17 ]\nSome of these stakeholders are indirect consumers of the architecture in that they care\nabout the software, and because the software architecture is the foundation of the system,\nthey become indirect consumers of the architecture. As a software architect, you are serving\nthese types of consumers in addition to the direct consumers. For example, end users are\nperhaps one of the most important stakeholders and should be a major focus. The software\narchitecture must allow the implementation to satisfy the requirements of the end users.\nWhen we discuss the consumers of a software architecture, we can't omit the developers\nwho work on that software. As a software architect, you need to be thinking about your\ndevelopers, whose work is directly affected by the software architecture. They are the ones\nwho will be working on the software on a daily basis.\nWhat is the software architect role?\nNow that we know what software architecture is, the importance and benefits of it, and\nhave an understanding that there are a variety of stakeholders who are affected by it, let's\nexamine the software architect role. What makes someone a software architect? What does\nit mean to be a software architect?\nCertainly, software systems can be developed without a software architect. You may have\nworked on a project in which no one was playing the software architect role. In some of\nthose cases, the project may have succeeded despite that, or it may have failed because of it.\nWhen no one is specifically given the software architect title, someone on the team may end\nup making architectural decisions. Such an individual is sometimes called an accidental\narchitect. They haven't been given the title of software architect, but they are performing\nsome of the same duties and making the same types of decision. Occasionally, when there\nis no software architect, the architectural design results from a collaboration between\nmultiple developers.\nThe smaller and less complex the software system is, the more you may be able to succeed\nwithout a software architect. However, if a project is large in size and/or complexity, you\nare more likely to need someone to play the formal role of software architect.\nSoftware architects are technical leaders\nSoftware architects are technical leaders of a software project and should be committed to\nthe project no matter what challenges arise. They provide technical guidance to\nmanagement, customers, and developers. As such, they are often a liaison between\ntechnical and non-technical resources.\n", "page": 46, "type": "text", "section": "Page 46"}
{"text": "The Meaning of Software Architecture\nChapter 1\n[ 18 ]\nAlthough software architects have many responsibilities, foremost among them is being\nresponsible for the technical aspects of software systems. While the software architect\ncollaborates with others, as the technical leader the software architect is ultimately\nresponsible for the software architecture, its design, and the architecture documentation for\na software system.\nSoftware architects perform a number of duties\nSoftware architects are required to undertake different types of duties, not all of which are\ntechnical. Software architects combine their experience, knowledge, and skills, both\ntechnical and non-technical, to fulfill such duties. Software architects will be expected to\nhave a firm grasp of designing software architectures, architecture patterns, and best\npractices.\nSoftware architects should have the ability to foresee possible issues and design\narchitectures to overcome them. They should be able to mitigate risks and evaluate\nsolutions such that they can select the proper one to resolve a particular problem. While\nsome of the skills and duties of a software architect are similar to what a senior developer\nmight do, it is a very different role. Software architects shoulder a greater amount of\nresponsibility, and there is a larger expectation of what a software architect brings to a\nproject.\nSenior developers have a great depth of knowledge regarding the technologies that they use\non a project. They are highly proficient in the languages, tools, frameworks, and databases\nthat are used in their software systems. While software architects are expected to have this\ndepth of knowledge as well, they must also possess a wide breadth of knowledge. They\nneed to be familiar with technologies that are not currently being used in the organization\nso that they can make informed decisions about the design of the architecture.\nIdeally, software architects have the breadth of knowledge to be aware of multiple\nsolutions to a problem and understand the trade-offs between them. It can be just as\nimportant for a software architect to understand why a particular solution will not work as\nit is to understand why one will.\nIvory tower software architects\nIf you find yourself in the role of a software architect, you are going to want to avoid being\nan ivory tower architect. A software architect who is in an ivory tower refers to one who,\neither by how they approach their position or because of how an organization works, is\nisolated from others.\n", "page": 47, "type": "text", "section": "Page 47"}
{"text": "The Meaning of Software Architecture\nChapter 1\n[ 19 ]\nIf a software architect is working from an ivory tower, they may be creating an architecture\nbased on a perfect-world environment that really doesn't reflect real scenarios. In addition,\nthey may not be working closely with the developers who will be creating implementations\nbased on the architecture.\nThe more that a software architect works on their own, isolated from stakeholders and\nother developers, the more likely they are to be out of touch with the needs of those\nindividuals. As a result, they may be designing software architectures that do not meet the\nvarying needs and requirements of a diverse group of stakeholders.\nSoftware architects should take a more hands-on approach. A software architect's duties\nshould already include involvement in a number of phases in a software life cycle, but\nbeing hands-on helps avoid being out of touch. For example, a software architect may do\nsome of the coding with the team in order to stay more involved. Leading by example, such\nas using your own code to serve as references for others, is one way to take a hands-on\napproach while also keeping your skills sharpened.\nAn involved approach will help you keep abreast of what issues and difficulties developers\nmay be facing, and what the architecture may be lacking. Leading from the trenches can be\nmuch more effective than leading from an ivory tower, and you are more likely to gain the\ntrust and respect of your teammates. If a software architect is out of touch or misinformed,\neven if the perception is inaccurate, their effectiveness as a leader will be diminished.\nAn ivory tower architect might be someone who is viewed as commanding from above. A\nsoftware architect should use their experience and knowledge to teach others, and not\npreach. Take opportunities to make your teammates better by teaching, but also look\nforward to learning from others. Teammates can and will provide valuable and insightful\nfeedback regarding your designs.\nAn organization should not have processes and/or an organizational hierarchy in place that\nseparate the architect from stakeholders. They should not be separated from the technical\nimplementation because doing so will take the architect away from the technology and\nskills that made them a good candidate for being a software architect in the first place.\nWhat are software architects expected to know?\nSoftware architects are expected to have skills and knowledge on a variety of topics. This\nbook focuses on many of those topics. They include non-technical duties, such as:\nProviding leadership\nAssisting project management, including cost and effort estimation\nMentoring team members\n", "page": 48, "type": "text", "section": "Page 48"}
{"text": "The Meaning of Software Architecture\nChapter 1\n[ 20 ]\nHelping to select team members\nUnderstanding the business domain\nParticipating in gathering and analyzing requirements\nCommunicating with a variety of technical and non-technical stakeholders\nHaving a vision for future products\nTechnical topics that software architects should be familiar with include:\nUnderstanding non-functional requirements and quality attributes\nBeing able to effectively design software architectures\nUnderstanding patterns and best practices for software development\nHaving a deep knowledge of software architecture patterns, their pros and cons,\nand knowing when to choose one over another\nKnowing how to handle cross-cutting concerns\nEnsuring performance and security requirements are met\nBeing able to document and review software architectures\nHaving an understanding of DevOps and the deployment process\nKnowing how to integrate and work with legacy applications\nBeing able to design software architectures that adapt to change and evolve over\ntime\nDon't be overwhelmed\nIf you find yourself in the software architect role for the first time, or if you are joining a\nteam that has been working on an existing software system for some time, it can be natural\nto feel overwhelmed by all that you do not know. It will take time to wrap your head\naround everything that you will eventually need to know.\nAs your experience grows, you'll feel more comfortable when you start on a new project.\nJust like anything, experience in different situations will make you more comfortable with\ntaking on new challenges. You'll also understand that it will take some time to become\nacquainted with the business domain, people, processes, technology, details, and intricacies\nthat come with each software system.\n", "page": 49, "type": "text", "section": "Page 49"}
{"text": "The Meaning of Software Architecture\nChapter 1\n[ 21 ]\nIs the software architect role right for you?\nIf you care about the software that you are working on and all of its stakeholders, including\nthe software's end users and developers, then you care about the important design\ndecisions that go into building the software. Ultimately, that means you care about its\narchitecture. Concerning yourself with the most important decisions can be challenging,\nbut it can be enjoyable and rewarding for that very reason.\nSoftware architects need to communicate with a variety of stakeholders and sometimes\nserve as a bridge between management, technical staff, and non-technical staff. If this is not\nsomething you want to get involved with, being a software architect may not be the best fit\nfor you.\nSoftware architects are passionate about technology. They have a deep understanding of\nthe technologies they are working with and keep those skills fresh by practicing their craft\nand being involved with projects. They must have a large breadth of knowledge and have a\nfamiliarity with technologies that they may not be currently using on a project. It is\nnecessary to keep up with the fast pace of change in areas such as languages, tools, and\nframeworks. Being aware of a range of technologies will allow you to recommend the best\nsolution to a particular problem.\nSoftware architects should love to learn and play with new technologies because being a\nsoftware architect requires continuous learning. As someone with a lot of wisdom to share,\nand who will be a leader on a team, you should enjoy mentoring and teaching others.\nMaking those who work around you better at their jobs is a part of your job.\nAll software applications have a purpose. Good software architects make every effort to\nensure that the software applications they work on serve their purpose as best that they\ncan. If this is something you care about, the software architect role may be right for you.\n", "page": 50, "type": "text", "section": "Page 50"}
{"text": "The Meaning of Software Architecture\nChapter 1\n[ 22 ]\nSummary\nSoftware architecture is the structure or structures of a system, their elements, and the\nrelationships between those elements. It is an abstraction of a software system. Software\narchitecture is important because all software systems have an architecture, and that\narchitecture is the foundation for the software system.\nSoftware architecture provides a number of benefits, such as enabling and inhibiting\nquality attributes, allowing you to predict software system qualities, easing communication\nwith stakeholders, and allowing you to more easily make changes. It also provides a\nreusable model that could be used in multiple software products, imposes implementation\nconstraints that reduce complexity and minimizes developer errors, improves cost/effort\nestimates, and serves as training for new team members.\nSoftware architects are technical leaders who are ultimately responsible for technical\ndecisions, the architecture, and its documentation. They perform a number of duties and\nare expected to have knowledge of a variety of topics, both technical and non-technical.\nAlthough the role can be challenging, if you care about the software that you are working\non and all of its stakeholders, then the software architect role can be extremely rewarding.\nIn the next chapter, we'll explore software architecture in an organization. Most software\narchitects operate within the context of an organization, so it is important to understand the\ndynamics of developing software within one. The chapter will detail topics such as the\nvarious software architect roles you will typically find in an organization, software\ndevelopment methodologies that are used, working with project and configuration\nmanagement, navigating office politics, and creating software product lines that leverage\narchitectural reuse.\n", "page": 51, "type": "text", "section": "Page 51"}
{"text": "2\nSoftware Architecture in an\nOrganization\nIn the previous chapter, we discussed software architecture and the role of the software\narchitect. In this chapter, we will explore those topics further, but in the context of an\norganization.\nSoftware systems are developed to satisfy the business goals of an organization. Many\nsoftware architects work as part of an organization. As a result, the organization's business\ngoals, objectives, stakeholders, project management, and processes greatly affect the\nsoftware architect and their work.\nThis chapter focuses on topics a software architect should be familiar with when working\nwithin an organization. We will take a look at the various types of software architecture\nroles that are commonly found in organizations, and the software development\nmethodologies they adopt. You'll gain a good understanding of how you might be expected\nto work with project management and the dynamics of office politics.\nRisk management and configuration management are two other aspects of working on\nsoftware projects within an organization. Finally, we'll take a look at software product lines,\nand how architectural reuse can create core assets that can make building software\nproducts faster, more efficient, and of a higher quality.\nIn this chapter, we will cover the following topics:\nTypes of software architects\nSoftware development methodologies\nProject management\nOffice politics\nSoftware risk management\nConfiguration management\nSoftware product lines\n", "page": 52, "type": "text", "section": "Page 52"}
{"text": "Software Architecture in an Organization\nChapter 2\n[ 24 ]\nTypes of software architects\nThe role of a software architect can vary from organization to organization. You may have\nalso heard of a variety of job titles related to software architects, such as the following:\nEnterprise architect\nSolution architect\nApplication architect\nData architect/Information architect\nSolution architect\nSecurity architect\nCloud architect\nSome organizations have one or more architects who perform a combination of these roles.\nThey may go by the title of software architect or by the title of one of these roles. In other\norganizations, different individuals play different architectural roles. Some companies\norganize their software architects so that they are in an architecture team. They collaborate\nwith the team on architecture tasks but also work on other teams to design and implement\nsoftware products.\nThis book does not focus on any one type of software architect. It deals with mostly\ntechnical topics, and so is geared toward a number of technical architect roles. Many of the\ntechnical, non-technical, and soft skills described in this book are required by more than\none type of software architect. Even in organizations that have different types of architects,\nthere is an overlap in their responsibilities and duties. Let's take a closer look at the\ndifferent types of software architect roles and what they typically mean.\nEnterprise architect\nEnterprise architects are responsible for the technical solutions and strategic direction of an\norganization. They must work with a variety of stakeholders to understand an\norganization's market, customers, products, business domain, requirements, and\ntechnology.\nThe enterprise architect ensures that an organization's business and strategic goals are in\nsync with the technical solutions. They need to take a holistic view to ensure that their\narchitecture designs, and the designs of other architects, are in line with the overall\norganization.\n", "page": 53, "type": "text", "section": "Page 53"}
{"text": "Software Architecture in an Organization\nChapter 2\n[ 25 ]\nThey should have both a deep and broad understanding of technology so that they can\nmake the proper recommendations and architecture designs. They must also look to the\nfuture to ensure that solutions are in line with both existing needs as well as anticipated\nones.\nIn addition to high-level architecture design documents, enterprise architects work with\nother architects, such as application architects, to ensure that solutions meet all of the\ndefined requirements. Enterprise architects come up with and maintain best practices for\nthings such as designs, implementations, and policies. For organizations that have multiple\nsoftware products, they will analyze them to identify areas for architectural reuse.\nEnterprise architects provide guidance, mentorship, advice, and technical leadership for\nother architects and developers.\nSolution architect\nA solution architect converts requirements into an architecture for a solution. They work\nclosely with business analysts and product owners to understand the requirements so that\nthey can design a solution that will satisfy those requirements.\nSolution architects select the most appropriate technologies for the problem that needs to be\nsolved. They may work with enterprise architects, or if such a role does not exist in the\norganization, take on the responsibilities of an enterprise architect, to consider an\norganization's overall strategic goals and enterprise architecture principles when designing\ntheir solution.\nThe designs created by solution architects may be reused in multiple projects. It is common\nin an organization to reuse architectural components and to reuse patterns across\narchitectures in different solution areas. In large organizations that have architects playing\ndifferent roles, solution architects bridge a gap between enterprise architects and\napplication architects.\nApplication architect\nApplication architects focus on one or more applications and their architecture. They\nensure that the requirements for their application are satisfied by the design of that\napplication. They may serve as a liaison between the technical and non-technical staff\nworking on an application.\n", "page": 54, "type": "text", "section": "Page 54"}
{"text": "Software Architecture in an Organization\nChapter 2\n[ 26 ]\nMost of the time, application architects are involved in all the steps in the software\ndevelopment process. They may recommend solutions or technologies for an application,\nand evaluate alternative approaches to problems. Individuals in this role need to keep up\nwith trends and new technologies. They know when to use them in order to solve a\nproblem or take advantage of an opportunity. When appropriate, they are involved with\nhow applications within an organization will work and integrate with each other.\nApplication architects ensure that the development team is following best practices and\nstandards during implementation. They provide guidance and leadership for team\nmembers. They may be involved in reviewing designs and code. Application architects\nwork with enterprise architects to ensure that the solutions designed for an individual\napplication align with the overall strategy of the organization.\nData architect/information architect\nData architects are responsible for designing, deploying, and managing an organization's\ndata architecture. They focus on data management systems, and their goal is to ensure that\nthe appropriate consumers of an organization's data have access to the data in the right\nplace at the right time.\nData architects are responsible for all of an organization's data sources, both internal and\nexternal. They ensure that an organization's strategic data requirements are met. They\ncreate designs and models and decide how data will be stored, consumed, and integrated\ninto the organization's various software systems. Data architects also ensure the security of\nthe organization's data, and define processes for data backup, data archiving, and database\nrecovery.\nThey maintain database performance by monitoring environments and may be tasked with\nidentifying and resolving various issues, including problems in production environments.\nData architects may support developers by assisting with their database design and coding\nwork.\nSome organizations have the role of an information architect. Although the data architect\nand information architect roles are related, and may even be fulfilled by the same person,\nthere is a difference between the two roles.\nWhile data architects focus their attention on databases and data structures, information\narchitects place their focus on users. They are concerned with user intent related to data\nand how data affects the user experience. They are primarily interested in how the data is\ngoing to be used and what is going to be done with it.\n", "page": 55, "type": "text", "section": "Page 55"}
{"text": "Software Architecture in an Organization\nChapter 2\n[ 27 ]\nInformation architects want to provide a positive user experience and ensure that users can\neasily interact with the data. They want to design solutions so that users have the ability to\nintuitively find the information that they need. They may conduct usability testing to\ngather feedback so that they can determine what changes, if any, should be made to a\nsystem. They work with UX designers and others to develop strategies that will improve\nthe user experience.\nInfrastructure architect\nInfrastructure architects focus on the design and implementation of an organization's\nenterprise infrastructure. This type of architect is responsible for the infrastructure\nenvironment meeting the organization's business goals, and provide hardware,\nnetworking, operating system, and software solutions to satisfy them.\nThe infrastructure must support the business processes and software applications of the\norganization. These architects are involved with infrastructure components such as the\nfollowing:\nServers: Physical or virtual servers for either cloud or on-premises environments\nNetwork elements: Elements such as routers, switches, firewalls, cabling, and\nload balancers\nStorage systems: Data storage systems such as storage area networks (SAN) and\nnetwork-attached storage (NAS)\nFacilities: The physical location of the infrastructure equipment, and ensuring\npower, cooling, and security needs are met\nInfrastructure architects support the delivery of an enterprise's software applications. This\nincludes designing and implementing infrastructure solutions and integrating new\nsoftware systems with an existing or new infrastructure. Once in production, they also\nensure that existing software systems continue to fulfill requirements affected by\ninfrastructure, and run at optimal levels. Infrastructure architects may make\nrecommendations, such as using new technologies or hardware, which will improve an\norganization's infrastructure.\nTo fulfill the demands of the enterprise, they monitor and analyze characteristics such as\nworkload, throughput, latency, capacity, and redundancy so that a proper balance is\nachieved and desired performance levels are met. They use infrastructure management\ntools and services to assist them with the management of the infrastructure.\n", "page": 56, "type": "text", "section": "Page 56"}
{"text": "Software Architecture in an Organization\nChapter 2\n[ 28 ]\nInformation security architect\nA security architect is responsible for an organization's computer and network security.\nThey build, oversee, and maintain an organization's security implementations. Security\narchitects must have a full understanding of an organization's systems and infrastructure so\nthat they can design secure systems.\nSecurity architects conduct security assessments and vulnerability testing to identify and\nevaluate potential threats. Security architects should be familiar with security standards,\nbest practices, and techniques that can be used to combat any identified threats. They\nrecognize security gaps in existing and proposed software architectures and recommend\nsolutions to close those gaps.   \nOnce security components are put into place, security architects are involved in testing\nthem to ensure that they work as expected. When a security incident does occur, security\narchitects are involved in their resolution and conduct a post-incident analysis. The results\nof the analysis are used to take proper action so that a similar incident will not occur again.\nA security architect may oversee an organization's security awareness program and help to\nimplement an organization's corporate security policies and procedures.\nCloud architect\nNow that cloud deployments are the norm, having someone in an organization dedicated\nto cloud adoption, with the relevant expertise, has become increasingly common and\nnecessary. A cloud architect is someone who is responsible for an organization's cloud\ncomputing strategy and initiatives. They are responsible for the cloud architecture used for\nthe deployment of software systems.  An organization that has someone who is focused on\ncloud architecture leads to increased levels of success with cloud adoption.\nThe responsibilities of cloud architects include selecting a cloud provider and selecting the\nmodel (for example, SaaS, PaaS, or IaaS) that is most appropriate for the organization's\nneeds. They create cloud migration plans for existing applications not already in the cloud,\nincluding the coordination of the adoption process. They may also be involved in designing\nnew cloud-native applications that are built from the ground up for the cloud.\n", "page": 57, "type": "text", "section": "Page 57"}
{"text": "Software Architecture in an Organization\nChapter 2\n[ 29 ]\nCloud architects oversee cloud management, and create policies and procedures for\ngovernance. They use tools and services to monitor and manage cloud deployments. The\nexpertise that cloud architects possess typically means they are involved in negotiating\ncontracts with cloud service providers and ensuring that service-level agreements (SLAs)\nare satisfied.\nCloud architects should have a firm understanding of security concerns, such as protecting\ndata deployed into different types of cloud and cloud/hybrid systems. They work with\nsecurity architects, or if such a role does not exist in the organization, take on the\nresponsibilities of a security architect, to ensure that systems deployed to the cloud are\nsecure.\nFor organizations that have not fully migrated to the cloud, one of the tasks for a cloud\narchitect is to lead a cultural change within the organization for cloud adoption. Cloud\nstrategies can fail if the organization's culture does not fully embrace them. Part of the\ncloud architect's job is to evangelize cloud adoption by communicating the many benefits,\nand influence behavior changes toward cloud adoption that will ultimately lead to cultural\nchanges.\nSoftware development methodologies\nAs a software architect working in an organization, you will typically be required to use the\nsoftware development methodology that has been adopted by the organization. However,\nin some cases, the software architect may play a role in deciding which software\ndevelopment methodology is used. Either way, software architects may be able to provide\ninput to an organization's processes, giving them the ability to make suggestions that may\nimprove these processes.\nFor these reasons, it is good to have an understanding of the more common software\ndevelopment methodologies. There are a variety of different types that can be employed for\na software project, each of which has its own strengths and weaknesses. Today, Agile\nmethodologies are much more widely used than traditional ones, but even among the\nmethodologies that are considered Agile, there are numerous variations.\nUnfortunately, sometimes, a software project moves forward with a software development\nmethodology that is not appropriate for the project. Prior to choosing one, you should\nconsider which one would be the most appropriate to use. In the following sections, we'll\ntake a look at two types of software development methodologies: the Waterfall model and\nAgile.\n", "page": 58, "type": "text", "section": "Page 58"}
{"text": "Software Architecture in an Organization\nChapter 2\n[ 30 ]\nThe Waterfall model\nThe Waterfall software development methodology is a sequential one, in which each stage\nof the life cycle has to be completed in its entirety before moving on to the next stage. Some\nadvantages of this model include the following:\nThe model is simple and easy to understand.\nStakeholders will know what to expect in terms of timeline, functionality, and\ncost.\nDue to the artifacts that are required to be produced out of each phase, different\ntypes of documentation about the system will be available. The documentation is\nbeneficial for those who will be maintaining the software going forward. It can\nalso facilitate bringing new employees onboard and minimize the impact of any\nemployee turnover.\nIn the following diagram of the Waterfall model, you can see how the flow of steps\nresembles a waterfall:\nWaterfall model\n", "page": 59, "type": "text", "section": "Page 59"}
{"text": "Software Architecture in an Organization\nChapter 2\n[ 31 ]\nPhases of the Waterfall methodology\nThe following are the most common steps in the Waterfall model:\nRequirements: During this initial phase, requirements are elicited from\nstakeholders and analyzed. The results are placed in a requirements document.\nDesign: In the design phase, technical design specifications are created. The\nspecifications detail how the requirements will be fulfilled by the technical\nsolution.\nImplementation: This phase represents the actual work of coding. The design\nspecifications are implemented in code.\nVerification: During this phase, testing is conducted to ensure that the\nimplementation works correctly and that the requirements are fulfilled.\nMaintenance: Once the software has been deployed, the life cycle enters this\nphase for bug fixes and other enhancements.\nThere is also a variation in the Waterfall model wherein the names of the individual steps\ndiffer. For example, coding may be used instead of implementation, or testing may be used\ninstead of verification. Some organizations add more steps to the process. There may be a\nseparate step for analysis that occurs before the design phase, or one for deployment that\noccurs after the verification phase.\nThe Waterfall model has gone out of favor, but if the following is true (which is often not\nthe case), the project may be a candidate for this model: \nThe team is already very knowledgeable regarding the project's business domain,\nbusiness rules, and functionality\nThe requirements are well understood and unlikely to change\nThe scope of the project is set and stable\nThe team understands the technology and the architecture well, and those are\nunlikely to change\nThe project is not large in size and does not have a high degree of complexity\nIt is acceptable that a working version of the software will not be available to\ninternal or external stakeholders until later phases\nThe project is constrained by a time frame/deadline that has already been\ndecided\n", "page": 60, "type": "text", "section": "Page 60"}
{"text": "Software Architecture in an Organization\nChapter 2\n[ 32 ]\nIssues with the Waterfall methodology\nThere are a number of issues with the Waterfall approach. Testing does not begin until after\nall implementation is complete, so there is no testing feedback until later in the life cycle.\nUsers do not receive a working version of the software until the very end, so valuable\nfeedback can't be provided until that time. Users can have difficulty visualizing the final\nproduct from just the requirements documentation and may miss important requirements.\nAlthough mock-ups and wireframes can minimize this issue, it's not the same as having a\nworking version of the software throughout an iterative approach.\nIf requirements are missed or incorrect, making changes late in the development process\ncan be a large effort, resulting in cost and/or time overruns. If time is running out on a\nsoftware development project, and testing occurs at the end, testing may be cut short,\nlowering the quality of the software.\nThe rigidity of the Waterfall model can be a major problem if the requirements are subject\nto change or are poorly understood. Even with projects that are thought to be predictable,\nthe nature of software development is that change is almost inevitable in one form or\nanother. As such, a process that is too rigid may put the project at a disadvantage.\nFor projects that are not suitable for a traditional Waterfall model, a more flexible life cycle\nmodel may be needed. Agile development processes can provide this type of flexibility.\nDue to the limitations of traditional models and the benefits that Agile methodologies\nprovide, they have become more popular in modern software development.\nAgile software development methodologies\nAgile software development methodologies were created by leading software professionals\nbased on their real-world experiences developing software. They address many of the\nlimitations of traditional development methodologies, and as a result, have become much\nmore popular.\n", "page": 61, "type": "text", "section": "Page 61"}
{"text": "Software Architecture in an Organization\nChapter 2\n[ 33 ]\nThere are a variety of different types of Agile processes available, such as Scrum, Kanban,\nExtreme Programming (XP), and Crystal. They each have their differences, but in essence,\nthey all focus on being adaptable to change. They attempt to find a balance between not\nhaving enough processes and having too much of them.\nAgile values and principles\nAgile methodologies have certain values and follow certain principles of software\ndevelopment. These were documented in the Agile Manifesto and the 12 principles of\nAgile software, written by the thought leaders who created the Agile software\ndevelopment approach. The following are the four core values of the Agile Manifesto:\nIndividuals and interactions are valued more than processes and tools. Valuing\npeople is more important than processes and tools. The people are the ones who\nunderstand the business and drive the development of the software. Teams will\nbe less responsive to business needs if it is the processes and tools that are more\nimportant than the people.\nWorking software is more important than documentation. Traditional methods\nmay have focused on a large amount of documentation, but software that works\nand meets requirements is more important. There need only be enough\ndocumentation as is necessary, and it shouldn't take precedence over working\nsoftware.\nCustomer interaction and continuous engagement are valued over contract\nnegotiation. Agile stresses the importance of customer interaction and\nengagement throughout the software development process. This increases the\nlikelihood that software will meet the needs of customers.\nResponding to change is more important than following a plan. Traditional\nsoftware development methodologies place a high importance on up-front\nplanning and a desire to avoid deviations from that plan. Agile embraces change,\nincluding requirement changes throughout the development process. Short\niterations provide an opportunity to add and change requirements at any time, as\nlong as it adds value and satisfies customers.\n", "page": 62, "type": "text", "section": "Page 62"}
{"text": "Software Architecture in an Organization\nChapter 2\n[ 34 ]\nAn iterative methodology\nUnlike the Waterfall approach, which is sequential, Agile methodologies are iterative:\nIterative methodology\nSoftware is built incrementally, with each iteration satisfying a portion of the total\nrequirements. At the end of each iteration is a working version of the software, rather than\nattempting to deliver all of the software at once. Testing takes place in the same iteration as\nthe coding. There is continuous feedback, and if something needs to be changed, it becomes\napparent sooner.\nAdaptive rather than predictive\nAgile methodologies are adaptive rather than predictive. While methodologies such as\nWaterfall focus on a large amount of upfront planning to detail and predict outcomes in the\nfuture, Agile methodologies focus on adapting to changes as they occur.\nAlthough it becomes difficult to know what the state of the software will be in the future,\nand increasingly so the further into the future you want to examine, Agile methodologies\nallow you to make changes after the initial planning takes place. Feedback from users and\ntesters can be taken into consideration immediately, and incorporated into the work of\nsubsequent iterations.\n", "page": 63, "type": "text", "section": "Page 63"}
{"text": "Software Architecture in an Organization\nChapter 2\n[ 35 ]\nDaily stand-up meetings\nIf you are going to adopt an Agile software-development methodology, a common practice\nthat you should consider implementing on your team is the daily stand-up meeting. If you\nare following the Scrum framework, this meeting is known as the daily scrum. It is a brief\nmeeting, typically held toward the beginning of the day, in which everyone on the team\nprovides a status update.\nA facilitator typically runs the meeting, and this role should be rotated on a regular basis.\nThere shouldn't be a reliance on any single facilitator. This also keeps the team talking with\neach other, which is the purpose of the meeting, and not just to a single facilitator.\nDuring the stand-up, each person provides their daily status by answering three questions:\nWhat did I work on yesterday?\nWhat am I going to work on today?\nAre there any impediments that I am facing?\nThe meeting should be kept brief, so if anyone's status begins to expand into a larger\ndiscussion, the facilitator should ask that any such discussions be taken offline, to be\ndiscussed at another time.\nThere are several benefits to having this meeting:\nEveryone stays informed as to the current tasks that are being worked on by\nothers on the team.\nWhen a team member states an impediment, it makes the impediment known to\nthe team. It provides an opportunity for team members to help each other.\nSomeone on the team may have ideas as to how to resolve the issue. Also, other\nteam members may have the same issue. Even if they don't, they will become\naware of the problem and the possible solution, and could help themselves or\nsomeone else in the future should they face the same issue.\nIt keeps team members identifying as a team. Rather than working in isolation,\nhaving everyone interact with each other regularly will help everyone to identify\nwith the team.\n", "page": 64, "type": "text", "section": "Page 64"}
{"text": "Software Architecture in an Organization\nChapter 2\n[ 36 ]\nProject management\nSoftware architects typically assist project management throughout the life cycle of a\nsoftware system. One of the ways in which you will be asked to do so is in estimating the\nlevel of effort for technical tasks. How estimates are derived and who participates in\nestimation varies from organization to organization, but typically, software architects, with\ntheir expertise and experience, will be expected to provide some level of input during the\nprocess of estimating tasks.\nThe importance of software project estimation\nThe importance of estimation cannot be understated. Estimates and project planning are\nmajor factors in the success of a project.\nEstimation can be difficult and must take into account a number of factors. However, the\nteam should work together to come up with accurate estimates as project management\nrelies on them. Estimates are used to organize work and plan releases. Inaccurate estimates\nand poor project planning are among the main reasons software projects fail.\nPutting effort into the estimates\nSometimes, estimation is done informally. You may be asked to provide an estimate or an\nopinion on the spot without an opportunity to analyze the work further. You want to\nestablish a culture within the organization that values putting effort into the estimates,\nrather than off-the-cuff estimates that amount to little more than guesswork.\nIf you are asked by someone to provide an estimate for a task extemporaneously, and you\nare not comfortable doing so, ask whether it's acceptable to get back to them. This will slow\nthe process down, and provide you with an opportunity to conduct a proper analysis\nbefore providing an estimate. Sometimes, if you are on the spot to provide an estimate, it is\neasy to be inaccurate and later regret it.\nWhen estimating tasks, some will involve work that is understood, making it easier to\nprovide estimates. It is the work that involves unknowns that can be challenging. If you are\nput in that position, see whether you can afford some time to perform some analysis, or, if\nappropriate, even create a proof of concept (POC). A POC will allow you to prototype a\nsolution to gain a better understanding of the effort involved, or whether the solution is\neven feasible.\n", "page": 65, "type": "text", "section": "Page 65"}
{"text": "Software Architecture in an Organization\nChapter 2\n[ 37 ]\nBeing a realist (or even a pessimist)\nPeople who provide estimates are optimists, realists, or pessimists. If you are generally an\noptimist, or the other individuals who are working with you to come up with the estimates\nare generally optimists, consider making a conscious effort to be more of a realist or\npessimist. Think about what could become challenging about the work, and make sure you\nare not leaving anything out when considering your estimate. Your knowledge and\nexperience as a software architect, along with the proper approach, is what is necessary to\nproduce a useful estimate.\nTeam and situational factors to consider\nIf you are involved with project planning, also consider situational factors such as the\nexisting infrastructure of the organization (facilities, equipment), the organization's culture,\nthe maturity of the organization's processes, and the tools that are made available for the\nproject to use.\nMake sure you also take into consideration the skill level and experience of your team. This\nincludes not just the developers, but everyone on the team, such as the business analysts\nand QA testers. There can be quite a bit of variance depending on this factor. You may not\nknow who will be assigned a particular task yet, but consider the overall skill levels of the\nteam, along with their strengths and weaknesses, during project planning.\nWhatever method your organization chooses to adopt, use it consistently and then analyze\nthe results. Depending on how well it works, you can refine your methods over time based\non the results you collect, which should yield even better results going forward.\nProject schedule changes\nIt would be great if projects always stayed on schedule, but as we all know, a variety of\nfactors can cause a project to fall behind schedule. For these types of situations, you will\nhave to work with project management to discuss what could be done to bring the project\nback on schedule, or if the project timelines can be adjusted.\n", "page": 66, "type": "text", "section": "Page 66"}
{"text": "Software Architecture in an Organization\nChapter 2\n[ 38 ]\nYou may also find yourself in a situation where deadlines have been moved up. This might\nbe initiated by internal management, but external forces may also dictate a change in the\nproject schedule. A new business opportunity may present itself, such as the possibility of a\nnew client, or your organization may be invited to an important trade show and want to\ndemo the software application. When situations such as these arise, depending on the\nimportance and the direction given by management, the team may need to make every\nattempt to conform to the new time frame.\nEven if you have adopted an Agile software development methodology, which anticipates\nchange and helps the team adapt to it, you will need to meet certain project deadlines.\nGetting a project back on schedule\nIn order to get a project back on schedule, or to adjust to a new deadline, there are a\nnumber of approaches. Let's have a look at them.\nWorking overtime\nOne approach is to have the team work longer hours, allowing them to accomplish more in\nthe same amount of time. However, you run the risk of lowering morale and productivity\nas hours increase.\nOne way to motivate your team with this approach is to pay extra for overtime work, or\nissue compensatory time that can be used later. Of course, this requires management's\napproval. If extra pay is involved, the budget of the project must be taken into\nconsideration. Overtime pay, particularly if there are contractors on the project, can add up\nquickly.\nReducing scope\nThe team could choose to revisit the scope of the project. If you can identify anything\nsuperfluous on the project schedule, you could notify project management to have it\nremoved. In addition, any gold plating, which refers to adding extra features that are not\nnecessary, should be eliminated, pushed to a later time, or reduced in scope.\n", "page": 67, "type": "text", "section": "Page 67"}
{"text": "Software Architecture in an Organization\nChapter 2\n[ 39 ]\nIf nothing obvious can be found, you could work with the appropriate stakeholders, such\nas domain experts, business analysts, end users, and product owners, to consider changing\nrequirements. Lower-priority functionality could potentially be dropped or delayed until a\nfuture release.\nAdding resources\nAnother common way organizations may try to get a project back on schedule is to add\nmore resources. However, this approach doesn't necessarily allow a project to be completed\nfaster, and it may cause the project to take even longer. Brooks' Law, from The Mythical\nMan-Month, states that ...adding human resources to a late software project makes it later.\nAs you add more people to a project, the communication paths increase in a multiplicative\nway. In addition to taking more time to communicate, many more opportunities for\nmiscommunication are created. Productivity and the number of defects (both in\nrequirements as well as design and implementation) are known to increase as projects get\nlarger.\nIf the team is going to add more resources, make sure that it is prepared to support them.\nNew team members will require guidance and mentorship in the early stages, but that\neffort is necessary so that they can become as productive as possible in the shortest amount\nof time.\nReallocating resources\nSometimes, it isn't so much about adding resources as it is about making sure that the ones\nyou have are assigned to the right tasks. Some resources may be more knowledgeable or\nefficient at certain tasks. A software architect who is working closely with the team may be\nable to provide valuable input to project management to reallocate resources.\nIf you see resources who are not working on critical items, you can suggest that they be\nreassigned to more important tasks. If a project is behind schedule, everyone needs to be\nworking on the critical path to complete the most important tasks first.\nIf there are tasks being worked on in a serial manner, which may cause a delay, think about\nhow the tasks could potentially be reassigned so that they can be worked on in a more\nparallel fashion.\n", "page": 68, "type": "text", "section": "Page 68"}
{"text": "Software Architecture in an Organization\nChapter 2\n[ 40 ]\nIdentifying problem areas\nTry to understand why the project is late. The first step in coming up with a solution is to\nfigure out the problem. If you are going to make suggestions to project management about\nhow to get back on schedule, make every attempt to identify the main problems.\nConsider getting thoughts from the team as to what improvements could be done. You\ndon't want to turn it into a finger-pointing exercise, but the team can provide good insight.\nThere could be some process that is causing a delay, which could be adjusted or eliminated.\nFor example, maybe there is a regular meeting that is not adding much value but taking up\nvaluable time, and the meeting could be eliminated or made to happen less frequently.\nIf there are any delays caused by external forces, examine whether anything can be done to\nalleviate those types of issues.\nActing as early as possible\nWhatever approach you take, one thing you do not want to do is to cut testing time short.\nNot performing the adequate amount of testing can have serious implications for the\nsuccess of the project. Software that does not meet requirements and/or contains defects can\nput the whole project in jeopardy. The cost of inadequate testing may be far greater than\nwhatever time or cost saving it offers.\nThe absolute best thing you can do is act as early as possible. Communicate with project\nmanagement so that they have a detailed understanding of the current state of the project.\nUse your experience to recognize warning signs and potential pitfalls for the team. Don't\nput off acting on potential problems, because the earlier in the project you realize there is an\nissue, the more options you have to resolve it.\nOffice politics\nAlmost everyone in an organization has to deal with office politics to some degree. As you\ncan see from the following pie chart, the majority of people think it is best to have an\nawareness of the office politics in your own organization:\n", "page": 69, "type": "text", "section": "Page 69"}
{"text": "Software Architecture in an Organization\nChapter 2\n[ 41 ]\nPie chart displaying How Employees View O\ufb03ce Politics\nAs a software architect, you may find yourself engaging in office politics more than you\nhave in the past, such as when you were a senior developer. This is because you will be\ninteracting and communicating with a larger variety of stakeholders, and you may be doing\nso more frequently.\nIn an organization, individuals from diverse backgrounds, with their own unique goals and\nviewpoints, come together to work toward the goals of an organization. A variety of factors\ncan shape people's motivations, which may lead to office politics.\nWhen you want to achieve something, you may need the help of others in the organization.\nDifferent actions you undertake may cause you to spend, receive, or borrow political\ncapital. It can be useful to have political skills when working within an organization.\nI would rather focus on building great software and not deal with office politics, but\nsometimes it is unavoidable. The following are some things that you can do as a software\narchitect to be successful in office politics.\n", "page": 70, "type": "text", "section": "Page 70"}
{"text": "Software Architecture in an Organization\nChapter 2\n[ 42 ]\nUnderstanding your organization's goals\nBe aware of your organization's strategic goals and direction. As much as possible, align\nyour goals with the goals of the organization. There will be less friction with others, and\nthose who are in conflict with such goals will be in conflict with the organization.\nHaving an understanding of how your organization makes money, and how that money is\ntypically invested, can be useful. If you are seeking resources for your project, such as\nadditional team members, equipment, and licenses for tools, it requires an investment by\nthe organization. The organization is going to want a return on that investment, so be\nprepared to explain how it aligns with the organization's goals and objectives.\nAddressing the concerns of others\nIn an organization, many stakeholders may come to the software architect with concerns\nover a software product or process. Although the priority and legitimacy of these concerns\nwill vary, look to address these concerns as promptly as possible.\nIf an issue is left unresolved, it could be escalated to others in the organization. It may\nappear to be a bigger issue than it really is. When someone brings a concern to you, you\nwant to make sure that person knows that you have listened to their concern, understand it,\nand will take appropriate action for it. If no action is going to be taken, explain the reasons\nbehind that decision, and make sure the person knows that at least their concern has been\nnoted.\nAssisting people with their goals\nWhen you have the opportunity, help others in your organization meet their goals or solve\ntheir problems. You are all working for the same organization. As long as what they are\ntrying to accomplish does not conflict with your own values or the goals of the company,\nassist people when you can.\nDo this without any expectation of getting anything in return. You don't want a reputation\nas someone who only does favors expecting some personal benefit. If you help your fellow\nemployees, you will be looked upon kindly. At some point, you may need someone else's\nhelp, and even though you are not owed anything, you will be more likely to receive that\nhelp.\n", "page": 71, "type": "text", "section": "Page 71"}
{"text": "Software Architecture in an Organization\nChapter 2\n[ 43 ]\nKnowing when to compromise\nAs a software architect, you will find yourself in discussions with others over a variety of\ntopics. You may be negotiating something, or in conflict with someone, or you could be\nmaking a request for something. Whatever the reason, when it becomes apparent that you\nmay not get what you want, be willing to compromise and know when to do so.\nOne reason to compromise is when you are simply lacking leverage, and if you fail to\ncompromise, you may not accomplish any part of your request or goal. A compromise can\nmake sense in that situation because you may still get something suitable out of the\nexchange. A compromise also makes sense if you think it will be beneficial in the long run.\nGiving up something now may pay off at some point in the future.\nBe prepared to compromise from the onset by proactively thinking about how important\nthe issue is to you as well as what would and would not make an acceptable compromise.\nListen to the person or group you are dealing with regarding the issue and try to\nunderstand their viewpoints and reasoning. Understanding the other side may provide you\nwith additional points you can make, or at the very least, give you an idea as to what might\nbe an acceptable middle ground.\nBeing aware of cultural differences\nMany organizations have offices and customers around the world, and they may outsource\nwork to resources in other countries. Keep cultural differences in mind when\ncommunicating with individuals from other countries. Different cultures may prefer\ndifferent phrases or approaches. Take the time to learn about the cultures of the people you\nwill be dealing with to maximize the clarity of your communications and to avoid\nmisunderstandings.\nSoftware risk management\nRisks are potential problems, and there are many risks involved in designing and\ndeveloping software. Organizations, and the people working for them, have different levels\nof risk tolerance. Whatever that level is, an organization should have a plan for risk\nmanagement.\nAs a software architect, you will need to assist project managers in managing those risks. If\nleft unmanaged, risks can lead to cost/time overruns, rework, operational failures, and\npossibly total project failure.\n", "page": 72, "type": "text", "section": "Page 72"}
{"text": "Software Architecture in an Organization\nChapter 2\n[ 44 ]\nThe first step in assisting with risk management is being able to identify risks. The team\nshould come up with and document potential risks. Using your knowledge and experience\nas a software architect, you may be able to help identify risks that stakeholders, project\nmanagement, and other team members cannot.\nSome examples of potential types of risks include the following:\nFunctional risks: Incorrect requirements, lack of participation from end users\nand business analysts, conflicting business goals\nTechnical risks: Degree of complexity, size of the project, new\nlanguages/tools/frameworks that the team is not familiar with, dependencies on\nvendors and subcontractors outside of the organization\nPersonnel risks: Team members without the proper experience or skillset,\ninability to staff the project, productivity issues with team members\nFinancial risks: Lack of sufficient funding for the project, return on investment\n(ROI) constraints that will be difficult to meet\nLegal risks: Needing to abide by government regulations, changing legal\nrequirements, contractual changes\nManagement risks: Lack of the appropriate experience and skillset, incorrect\nplanning, lack of communication, organizational issues\nOnce a risk is identified, it should be evaluated for its potential impact and the probability\nthat it will occur. Risks that have both a high impact as well as a high likelihood of\noccurring are the most critical to a project:\nRisk Analysis\n", "page": 73, "type": "text", "section": "Page 73"}
{"text": "Software Architecture in an Organization\nChapter 2\n[ 45 ]\nAfter risks are evaluated, software architects may be asked by the organization to assist\nwith a plan on how to manage the risks. There are four main techniques that can be used to\nhandle risks: risk avoidance, transferring the risk to another party, risk mitigation, and risk\nacceptance.\nRisk avoidance\nOne technique for handling risk is risk avoidance, which involves changing the project in\nsome way so as to eliminate the risk altogether. An example of a risk that could be dealt\nwith in this way would be a project team that is considering using a programming\nlanguage and/or technology that the team is not familiar with. Moving forward with such a\nchoice brings with it some potentially significant risks.\nThe project may take longer than anticipated due to the learning curve, or it may be\ndiscovered that features the team wants to implement are not well suited to the chosen\ntechnology. One way to handle this risk would be to select a technology that the team is\nalready familiar with, as long as it can satisfy the needs of the project. This would allow the\nteam to effectively avoid the risk.\nKeep in mind that not all risks can be avoided, and avoiding a risk could lead to other risks.\nIn addition, taking risks is sometimes necessary in order to take advantage of an\nopportunity. These should be some of your thoughts when considering whether or not to\navoid a risk.\nTransferring the risk to another party\nAnother technique in risk management is to transfer the risk to another party. An example\nof this might be a project team that hires a subcontractor to implement parts of a project,\nsuch as the design and development of a user interface for a web application. There is a risk\nthat the subcontractor may not complete their deliverables on time or that their deliverables\nmay not meet certain quality standards. One way to handle this risk would be to transfer it\nto the subcontractor by building penalties into the contract that will be incurred if the\ndeliverables are late or do not meet the level of quality desired.\n", "page": 74, "type": "text", "section": "Page 74"}
{"text": "Software Architecture in an Organization\nChapter 2\n[ 46 ]\nRisk mitigation\nOne can also employ risk mitigation to handle a risk, which is reducing the likelihood that\na risk will occur. Let's say a software project recently hired a new team member. This\nperson is not as experienced as some of the other team members, so there is a risk that the\nwork of the new team member could be delayed due to a learning curve or the work\nproduced may not be correct because of a lack of knowledge regarding technical standards\nor the business domain. One way to mitigate, or lessen, this risk would be to assign the new\nemployee a mentor. This mentor could devote some time to teaching, answering questions,\nand reviewing the work of the new team member.\nRisk acceptance\nRisk acceptance is another way deal with a risk. This approach is to simply accept the risk\nand any possible consequences. For example, perhaps your company and another company\nare both developing a new software product to serve a new market. Ideally, your product\nwould be the first to the market, but since you started later, there is a risk that you may not\nbe. Not being the first to the market could result in a loss of market shares. After analyzing\nthe risk, you may decide to accept any consequences of not being the first to the market in\nexchange for developing a better product by not rushing it.\nOne point to keep in mind is that actions taken to resolve a risk by mitigating or\ntransferring it may have their own set of risks. This must be taken into consideration as part\nof the analysis of that risk. If, in the last example, the company decided to mitigate the risk\nby attempting to complete the product sooner, additional risks might have been created,\nsuch as a greater probability that requirements could be missed or that the quality might\nsuffer.\nConfiguration management\nAs a software architect in an organization, you will be expected to be involved with\nconfiguration management. In many organizations, there is a software configuration\nmanagement (SCM) team. In addition, there may be an SCM team at the project level. For\nsmall projects, this may be a single person, or it may even be taken on by someone who isn't\neven dedicated to the role, such as a team member who performs other duties.\n", "page": 75, "type": "text", "section": "Page 75"}
{"text": "Software Architecture in an Organization\nChapter 2\n[ 47 ]\nSome of the responsibilities for a software configuration management team include\nidentifying configuration items (software, documents, models, plans), implementing a\nchange control process, and managing the process and tools used for builds. We will\nexamine automated builds and continuous integration (CI) in Chapter 14, Architecting\nLegacy Applications.\nChanging management\nOrganizations will implement a formal change control process to handle changes to a\nsoftware product. This includes all aspects of a software system, such as requirements,\nsource code, and documentation. There are many reasons a change might be proposed,\nincluding correcting a problem (a bug in the software), implementing a functional change\n(a change to a business rule), or adding new functionality to the software.\nThe goal of the process is to ensure that if changes are made, it is appropriate to do so, and\nto minimize the effort, difficulty, and disruption when implementing the change.\nSome change control processes involve a change control board (CCB). This is a group of\nproject stakeholders who are designated to analyze proposed changes and decide whether\nthey should be implemented. As a software architect, you may be part of a formal or\ninformal CCB. Your knowledge and experience might be leveraged by the board to help\nwith:\nEvaluating a proposed change to decide whether the change is something that\nshould be implemented in the project\nPrioritizing the proposed change based on its importance and severity (if it's a\ndefect)\nProviding an estimate as to the effort involved to implement the change\nThere are several approaches to change management that an organization might use. A\nsoftware architect should be familiar with their organization's process. As a software\narchitect, you may offer input to change an existing process or you may need to help\nformulate the process for the first time.\n", "page": 76, "type": "text", "section": "Page 76"}
{"text": "Software Architecture in an Organization\nChapter 2\n[ 48 ]\nOne approach to change management, which is the most formal, requires that a CCB\nreview all changes, regardless of how large or complex the change may be. It does have the\nadvantage of allowing multiple people to consider the change. Having more people aware\nof the proposed change and giving them the opportunity to discuss it will increase the\nchances of making the right decision. Especially with large, complex systems, it can be very\ndifficult or even impossible for a single person to be aware of all of the technical and\nfunctional ramifications of a particular change. Having multiple sets of eyes looking at it\ncan certainly help. Unfortunately, this process is going to take more time. The CCB could\neasily become a bottleneck. Not only will more time be spent making decisions, sometimes,\ntime will pass just setting up the meetings to get everyone together. For situations where\nmany changes or decisions need to be made in a timely manner, having a CCB review all\nchanges may not be practical.\nThe second approach to change management is the opposite of the first method. Rather\nthan have a CCB review every change, in this approach to change control, there is no CCB.\nIndividual developers make their own decisions for all changes. The advantage of this\napproach is that changes can move through the process much more quickly than either of\nthe other two approaches. Also, empowering developers can give them a certain level of\nconfidence and satisfaction. However, only one person is thinking about each change, and\nas mentioned earlier, this can have a detrimental effect. The quality achieved for a given\nchange will solely depend on the developer making the change. Some developers may be\nnew to the software or inexperienced in general, or they may simply not be very good.\nWhatever the reason, the change they decide to make could end up causing even more\nproblems than what it is attempting to solve. As was mentioned earlier, even with an\nexperienced and quality developer, it can be difficult with large systems to be aware of all\nof the ramifications of a particular change. For minor defects, this isn't much of an issue, but\nit can be for large changes.\nThe third approach to change management seeks a balance between the first and second. In\nthis approach, a CCB only reviews some of the changes. Typically, this means reviewing\nthe most important or most complex changes. While changes won't be completed as fast as\nthe second approach, and the changes won't be under as much scrutiny as the first\napproach, at least the changes that could potentially cause the most problems are reviewed,\nand the CCB is not a bottleneck for the other changes. It simply may not be practical to have\na CCB review everything, but you also don't want to sacrifice all reviews. For example, the\nCCB might review the majority of enhancements and high-priority/severity defects, but not\nthe lower-priority/severity defects (although an analyst, developer, and tester will still have\nan opportunity to think about all fixes while they are working on a defect). It really\ndepends more on how large and complex the change is, and how important the\nfunctionality is that will be affected, and not so much on whether it is a defect or an\nenhancement. Sometimes, a defect fix is more complicated than an enhancement, so those\nfactors are always taken into consideration.\n", "page": 77, "type": "text", "section": "Page 77"}
{"text": "Software Architecture in an Organization\nChapter 2\n[ 49 ]\nPerhaps the best approach to change management, and the most pragmatic, is to always\nuse a CCB, but create multiple levels of it. The organization may designate different levels\nof authority, and changes with limited impact and scope can be approved at lower levels of\nauthority, while changes that have a larger impact are escalated to higher levels of\nauthority within a CCB. When there are multiple levels of a CCB, the project's stage (that is,\nhow close the team is to releasing) is sometimes considered when determining what CCB\nlevel is appropriate. A shift typically occurs as a project gets closer to release, with the focus\ngoing from wanting flexibility to wanting more stability and control over the changes.\nSoftware product lines\nGrowing competitive pressures combined with the complexity of developing software\nsystems makes it a necessity for organizations to increase efficiencies wherever possible.\nOne way to accomplish this is to create software product lines. A variety of industries,\nincluding the software industry, have used product line engineering (PLE) successfully.\nProduct lines are multiple products from a single company that address a particular need\nor market. These products could be sold under the same brand, with the idea that\ncustomers who use one product from the brand are more likely to purchase another\nproduct from the same brand.\nOrganizations that develop software may have multiple existing software products and/or\nother software products in development. These software products may work in a similar\nway, have similar functional and/or non-functional requirements, and have a similar\nlook/feel.\nWithout any type of reuse, the same functionality may be written multiple times. When\nworking as a software architect in an organization, you should seek to reuse architecture by\nbuilding software systems from core assets. A software architecture that provides a solution\nfor one software product may also be useful for another one. Architectural components can\nbe reused, allowing a generic solution to address the same problem in multiple products.\nIn addition to building software, organizations may acquire software, or other\norganizations that have existing software products, and this software may become a new\nsoftware line for the acquiring organization. The acquired software may also be merged\nwith an existing product line.\n", "page": 78, "type": "text", "section": "Page 78"}
{"text": "Software Architecture in an Organization\nChapter 2\n[ 50 ]\nBenefits of a software product line\nSome of the benefits of strategic, planned architectural reuse across a software product line\ninclude the following:\nReduced development effort\nLower costs\nIncreased productivity\nIncreased quality\nDecreased time to market\nCore assets of an organization\nThe goal of a software product line is to have reusable components, known as core assets,\nthat share a common architecture. Some of the components that can be reused and\ndesignated as core assets include the following:\nRequirements analysis\nDomain models and analysis\nSoftware architecture design\nTest plans and test cases\nWork plans, schedules, budgets\nProcesses, methods, tools\nThe knowledge, skills, and experience of the employees\nUser guides and technical documentation\nThe core assets need to be built while keeping in mind that they will be used across\nmultiple products in a product line. Each product will have variation, so to account for this,\ncore assets should be built with variation points, places that provide an opportunity to take\ninto consideration this variation and allow the team to tailor the asset for use in a specific\nproduct.\nOnce these assets have been compiled, it will require less effort, time, and cost to reuse the\nassets than to create them each time they are needed. Building a software system then\nrequires the team to reuse the appropriate assets, using the variation points as appropriate,\nand building the system from there.\n", "page": 79, "type": "text", "section": "Page 79"}
{"text": "Software Architecture in an Organization\nChapter 2\n[ 51 ]\nRisks of product line engineering\nAlthough product line engineering for software may result in benefits, be aware that there\nare risks related to its successful implementation. Moving to this type of approach requires\nadopting a new technical strategy for the organization as a whole. It requires coordination\nand managerial support.\nIt needs to be clear to development teams what core assets are available to them, and what\nthe variation points are so that they can utilize them properly. The scope of the product line\nneeds to be set appropriately so that it is neither too broad nor too narrow.\nIf the organization is creating all of the products in the same product line, it requires\nsuccessful execution by the software architects and development teams, as well as the\nproper organizational management. If the organization is acquiring software products to be\nincluded in a software product line, then the proper resources, both technical and\nmanagerial, must be available and prepared to put forth the effort required to identify and\nutilize the common components of those products.\nOrganizations that are not prepared to fully adopt this approach may experience failure.\nSummary\nWhen working as an architect in an organization, you should be aware of the various types\nof software architect roles. The various roles have similarities and differences in terms of\ntheir responsibilities and duties. If you are required to use a software development\nmethodology that has already been selected for your organization, it is important to\nbecome familiar with it and its practices. If you are in a position to select the software\ndevelopment methodology, be sure to be knowledgeable about the choices so that an\nappropriate one can be selected for the project's needs.\nSoftware architects use their expertise and experience to work with project management on\nestimates for tasks, project planning, and controlling and monitoring the project during its\nlife cycle.\nDue to communicating with a larger number of stakeholders and management, office\npolitics is a reality that a software architect will have to deal with more than those in other\ndevelopment roles.\nOrganizations will want involvement from software architects for risk management.\nSoftware architects can identify risks and come up with approaches to handle them.\nSoftware architects work with the SCM team to coordinate builds, development\nenvironments, and to provide input for change management.\n", "page": 80, "type": "text", "section": "Page 80"}
{"text": "Software Architecture in an Organization\nChapter 2\n[ 52 ]\nSoftware architects should consider introducing software products lines for the many\nbenefits that they provide. Reusing architectural components to create software products\nallows them to be created faster and with a higher degree of quality.\nIn the next chapter, we will examine software requirements and how they affect the\nsoftware architecture. Software architects should have an understanding of the business\ndomain and requirements in order to design an appropriate solution. We will learn about\nrequirements engineering, how to extract and summarize requirements from stakeholders,\nand architectural quality attributes.\n", "page": 81, "type": "text", "section": "Page 81"}
{"text": "3\nUnderstanding the Domain\nMoving from other roles, such as a software developer role, to being a software architect\nrequires one to expand and broaden their sphere of knowledge to include a deep\nunderstanding of their domain. Effectively designing a solution for a problem space\nrequires knowledge about the domain and the requirements for the software.\nThis chapter begins by describing the foundation of understanding a domain, which\nincludes general business knowledge and a keen understanding of your organization's\nbusiness. We will go over the important concepts of domain-driven design (DDD), and\nhow it can help your software project team handle complexity and model a solution around\nreal-world concepts. The chapter also details the different types of software requirements\nand the techniques to elicit them from key stakeholders.\nIn this chapter, we will cover the following topics:\nDeveloping business acumen\nDomain-driven design\nRequirements engineering\nRequirements elicitation\nDeveloping business acumen\nWhile being a software architect requires in-depth technical knowledge, to be successful in\nthe role also requires a thorough understanding of your organization's business. In order to\ndesign an appropriate architecture, you need to have knowledge of the business problems\nyou are trying to solve and the business opportunities your organization is seeking to\nexploit. A technically advanced software system is of no use if it does not meet its goals.\n", "page": 82, "type": "text", "section": "Page 82"}
{"text": "Understanding the Domain\nChapter 3\n[ 54 ]\nWhen designing a software architecture, in order to ensure that the solution is an\nappropriate one, you must consider the goals of the business, the users, and the software\nsystem:\nEach of these focus areas has its own goals, which can significantly overlap and impact each\nother. For example, a business goal of a specific, aggressive time to market could mean that\nthere is not enough time for things such as proper requirements analysis or quality\nassurance testing, which could significantly impact user goals. When goals are in conflict\nwith each other, it requires you to find an acceptable balance among them when designing\nyour software architecture.\nFamiliarity with general business topics\nHaving a competent and practical understanding of business and its terminology will be\nuseful to you in becoming a complete architect. While your passion may be for technology,\nas a software architect you will benefit from business knowledge more than other roles,\nsuch as a software developer. Software architects need to interact with a variety of\nstakeholders, and understanding the language of business will ensure that you have a\ncommon understanding with them.\n", "page": 83, "type": "text", "section": "Page 83"}
{"text": "Understanding the Domain\nChapter 3\n[ 55 ]\nUltimately, you are designing a software architecture that will fulfill business goals, and\nyour understanding of business will guide you in that task. Having general business\nknowledge of topics such as finance, operations, management, and marketing will help you\nto understand the value your software is supposed to bring to an organization. Business\ndecisions will be made based on things like return on investment (ROI) calculations for the\nsoftware project and cost-effectiveness analysis of different approaches. A good grasp of\nthese concepts will help you to add value to such discussions.\nOne way to gain this knowledge is through formal education. If that is not feasible, there\nare other ways to get at least a rudimentary understanding of business topics. There are\nonline classes available, some of which you can attend for free. Another option is to obtain\nand read one or more books on the relevant topics.\nUnderstanding your organization's business\nOnce you have some general business knowledge, you will want to gain a good\nunderstanding of your organization's business. It is a crucial aspect of being a successful\nsoftware architect and separates a good architect, who only has technical knowledge, from\na great one.\nA good starting point is to gain an understanding of your organization's products and\nservices, and the value they provide to their customers. How does your organization make\nmoney? If you are the software architect for a specific product, pay particular attention to\nthat product. Invest the time to understand the various business processes of your\norganization.\nYou should learn about the market that your organization operates in and its trends. It is\nprudent to become familiar with your organization's competitors. You should seek out\nanswers to questions such as:\nWhat do your competitors do differently?\nWhat do they do that is similar?\nWhat are the strengths and weaknesses of your competitors?\nMost importantly, spend the time to understand your organization's customers. The\nsoftware products you help design are for your customers, and they are perhaps the most\nimportant aspect of your organization's business. What does their business do? How do\nthey use your products and services? Why did they choose your products and services over\nthose of a competitor?\n", "page": 84, "type": "text", "section": "Page 84"}
{"text": "Understanding the Domain\nChapter 3\n[ 56 ]\nOnce you become familiar with your organization's business, the market in which it\noperates, its products/services, and its customers, you are on the path to fully\nunderstanding your organization's domain.\nDomain-driven design\nUnderstanding the domain of your software application is part of what is necessary to\ndiscover the appropriate architecture for any solutions you need to develop. The domain is\nthe subject and body of knowledge on which the software will be applied.\nThe term domain-driven design (DDD) was coined by Eric Evans in his book, Domain-\nDriven Design: Tackling Complexity in the Heart of Software. DDD is an approach to\ndeveloping software that aims to make the software better by focusing on the domain. It\nhas a set of concepts and patterns that have successfully been used to solve problems and\nbuild great software.\nDDD is particularly useful for large software applications that have complex and sizable\nmodels. DDD helps you to solve complex problems. Even if you decide not to follow DDD\nfully while architecting your applications, some DDD concepts may be helpful to you. It is\nbeneficial to become familiar with these concepts as some of them are referenced in other\nareas of this book, and in the event that you encounter them in your work.\nEncourages and improves communication\nOne of the benefits of DDD is the fact that it encourages and improves communication.\nCommunication among all team members is encouraged. In particular, DDD stresses the\nimportance of interacting with domain experts.\nA domain expert, or subject matter expert (SME), is someone who possesses expertise\nabout, and is an authority in, a particular area. Understanding the domain of your software\napplication is highly beneficial and domain experts will help the entire team gain this\nunderstanding.\n", "page": 85, "type": "text", "section": "Page 85"}
{"text": "Understanding the Domain\nChapter 3\n[ 57 ]\nIn addition to encouraging communication with domain experts, DDD improves\ncommunication among all team members and stakeholders by introducing the concept of a\nubiquitous language.\nWhat is a ubiquitous language?\nThe development team may not have a strong understanding of the domain, and may not\nbe familiar with terms and concepts used by stakeholders, including the domain experts.\nThey may use their own language when discussing the functionality and discuss the\ndomain in terms of their technical design. The stakeholders, including the domain experts,\nwill use their own jargon when discussing their domain, and may not have a good\nunderstanding of technical terms. Because different people may use different language to\ndescribe the same concepts in a particular domain, it can take longer to communicate ideas,\nand it can lead to misunderstandings.\nIn Domain-Driven Design: Tackling Complexity in the Heart of Software, Eric Evans described\nthis problem:\n\"The terminology of day-to-day discussions is disconnected from the terminology\nembedded in the code (ultimately the most important product of a software project). And\neven the same person uses different language in speech and in writing, so that the most\nincisive expressions of the domain often emerge in a transient form that is never captured\nin the code or even in the writing.\nTranslation blunts communication and makes knowledge crunching anemic.\nYet none of these dialects can be a common language because none serves all needs.\"\nSome team members may become familiar with the domain terminology and act as\ntranslators for the rest of the team, but they can become bottlenecks.\n", "page": 86, "type": "text", "section": "Page 86"}
{"text": "Understanding the Domain\nChapter 3\n[ 58 ]\nIn order to mitigate these types of risks, Eric Evans created the concept of a ubiquitous\nlanguage. It is a common language among all team members and stakeholders based on the\ndomain model:\nDeveloping a ubiquitous language can take time, and can evolve and grow as the team's\nunderstanding of the domain changes. Domain experts should use their understanding of\nthe domain to point out terms that don't correctly express an idea, and everyone can look\nfor inconsistencies and ambiguities in an effort to improve the ubiquitous language.\n", "page": 87, "type": "text", "section": "Page 87"}
{"text": "Understanding the Domain\nChapter 3\n[ 59 ]\nAlthough it takes effort, once you have a ubiquitous language, it simplifies communication\nand leads to a greater understanding among everyone involved with the project. No\ntranslation will be needed because everyone has agreed on and understands the various\nterms. The important thing is to use it consistently and throughout the project. The\nubiquitous language should be used during discussions and in all of the project artifacts\nsuch as documentation, diagrams, code, and tests.\nEntities, value objects, and aggregates\nSome of the basic building blocks of DDD are entities, value objects, and aggregates. When\nmodeling them, the ubiquitous language should be used.\nEntities\nEntities are objects that are defined by their identity and not their attributes. They are\nmutable because the values of their attributes can change without changing their identity. If\ntwo objects have the same values for their attributes, other than their unique identifier, they\nare not considered equal.\nFor example, if you had two Person objects with the same first and last name values for\nthose corresponding attributes, they are still two different objects because they have\ndifferent identities. This also means that a value for an attribute such as last name can be\nchanged on a Person object and it still represents the same person.\nValue objects\nUnlike entities, value objects are objects that describe some characteristic or attribute, but\nhave no concept of identity. They are defined by the values of their attributes and are\nimmutable. If two objects have the same values assigned to their properties, they can be\nconsidered equal.\nFor example, if two objects that represent points on a graph using Cartesian coordinates\nhave the same x and y values, they can be considered equal and would be modeled as a\nvalue object.\n", "page": 88, "type": "text", "section": "Page 88"}
{"text": "Understanding the Domain\nChapter 3\n[ 60 ]\nAggregates and root entities\nAggregates are groupings of entities and value objects that are treated as a single unit. A\nboundary is defined that groups them together. Without aggregates, complicated domain\nmodels can become unwieldy to manage, as the many entities and their dependencies grow\nlarge in number. Retrieving and saving an entity and all of its dependent objects can\nbecome difficult and error-prone.\nAn example of an aggregate is an order object that contains an address object and a\ncollection of line item objects. The address object and the collection of line item objects are\nall separate objects, but they are treated as a single unit for data retrieval and changes.\nSeparating the domain into subdomains\nOne practice of DDD is to separate the domain model into multiple subdomains. While a\ndomain is the entire problem space that the software solution is being developed for, a\nsubdomain is a partitioned piece of the overall domain. This is particularly useful for large\ndomains, where it is not feasible to have one large and unwieldy domain model.\nBy focusing on one subdomain at a time, it reduces complexity and makes the overall work\nmore digestible. Rather than attempting to address too many issues at once, dividing your\ndomain into subdomains provides more of a divide and conquer approach.\nFor example, in a student information system, you may have subdomains for contact\nmanagement, admissions, financial aid, student accounts, and academics, among others.\nOne or more of the subdomains may be designated as a core domain, which is typically the\npart of the domain that is fundamental to the organization. If there is a part of the domain\nthat differentiates the organization from competitors, it is probably one of the core\ndomains. Core domains are the reason that the software is worth writing, rather than\nbuying existing software off the shelf or outsourcing the work.\nThe domain experts on the project can help with identifying the core domains, as well as\nthe division of domains into subdomains.\n", "page": 89, "type": "text", "section": "Page 89"}
{"text": "Understanding the Domain\nChapter 3\n[ 61 ]\nWhat are bounded contexts?\nA domain model is a conceptual model based on the domain and includes both behaviors\nand data. It represents a part of the overall solution that fulfills the goals of the business.\nBounded contexts are a pattern in DDD that represent partitions in the domain model.\nSimilar to subdomains, which are partitions in the domain, bounded contexts are partitions\nin the domain model. As is the case with subdomains, creating partitions and boundaries\nreduces the overall complexity.\nA bounded context may map to a single subdomain, but keep in mind that is not always the\ncase. The domain model for a subdomain may require multiple bounded contexts for the\noverall solution of that subdomain.\nFor example, if we were creating a software system for a business that sells clothing online,\nwe might allow customers to sign up for a newsletter that contains deals and discounts.\nAnother part of the application would allow customers to place orders and provide\npayment information.\nWith these two pieces of functionality, some concepts are shared, while some are not. If\ndifferent development teams, or different developers on a single team, are working on\nthese two sets of functionality, it is not clear what overlap, if any, exists. If there is overlap,\nwhat should or should not be shared between these two pieces of functionality? This is\nwhere the concept of bounded contexts is applicable. A domain model applies to a\nparticular context, so we can define the various contexts to clear up some of the ambiguities\nthat exist.\n", "page": 90, "type": "text", "section": "Page 90"}
{"text": "Understanding the Domain\nChapter 3\n[ 62 ]\nIn this example, we could create one bounded context for marketing (Marketing Context),\nand one for order processing (Order Processing Context). Each bounded context may have\nentities that are unique to itself. For example, the Order Processing Context has the concept\nof an order line item, whereas the Contact Management Context does not. However, both\nbounded contexts have the concept of a Customer. Is Customer referring to the same\nconcept in both bounded contexts? By separating them out, we can begin to answer this\nquestion:\nIn the context of marketing, all that may be required for a Customer entity is an identity\n(unique identifier), first name, last name, and email address. However, in the context of\nplacing an order, the Customer entity would require additional information, such as a\nshipping address and payment information.\nYou could create one Customer entity, but using it for different contexts adds complexity\nand can lead to inconsistencies. Validation that requires payment information only applies\nin the Order Processing Context, and not the Marketing Context. The behavior required\nfor a Customer in the Order Processing Context should not prevent a Customer from\nbeing created in the Marketing Context, where only the first name, last name, and email\naddress are required.\n", "page": 91, "type": "text", "section": "Page 91"}
{"text": "Understanding the Domain\nChapter 3\n[ 63 ]\nWe will discuss the single responsibility principle (SRP) later in this book, but the basic\nidea is that each class should be responsible for a single aspect of the functionality. The\nCustomer entity is still small now, but you can begin to see how it could grow quickly. If it\nwere to be used in multiple contexts, it would attempt to fulfill too many disparate\nresponsibilities and break the SRP.\nThe context for each model should be clearly defined and there should be an explicit\nboundary between bounded contexts. They are created so that everyone on the team, or\nacross multiple teams, can have the same understanding of what belongs in each context.\nWhile the example used is a simplistic one, a large domain model will have many entities\nand contexts, and it usually isn't immediately clear what is unique or common across\ndifferent contexts, and how each context should interact with each other.\nDDD and the concept of bounded contexts work well with microservices, which we will be\ndiscussing in further detail later in this book. Now that we have a better understanding of\nDDD concepts, let's go into detail about requirements. Working with domain experts and\nother stakeholders, we need to have an understanding of the requirements prior to design.\nRequirements engineering\nIn order to model your domain and design an appropriate architecture, you will need to\nknow the requirements for the software you are building. Requirements engineering\ninvolves establishing the functionality that is required by the stakeholders, along with the\nconstraints under which the software must be developed and operate. It encompasses all of\nthe tasks involved in eliciting, analyzing, documenting, validating, and maintaining the\nrequirements of the software system. As a software architect, you will be participating in\nthese tasks so it is helpful to be familiar with them.\nTypes of software requirements\nThere are different types of software requirements, and software architects should be\nknowledgeable about them. The main types of software requirements include:\nBusiness requirements\nFunctional requirements\nNon-functional requirements\nConstraints\n", "page": 92, "type": "text", "section": "Page 92"}
{"text": "Understanding the Domain\nChapter 3\n[ 64 ]\nLet's take a closer look at each of these types.\nBusiness requirements\nBusiness requirements represent the high-level business goals of the organization building\nthe software. This type of requirement defines the business problems that the software will\nsolve or the business opportunities that will be addressed by the software.\nBusiness requirements may include requirements that come from the market. An\norganization may want to ensure that they are not excluding some functionality that a\ncompetitor is providing. They may also want to differentiate themselves from a competitor\nby providing functionality that a competitor is not, or provide the same functionality but in\nsome improved form (for example, faster response times). As a result, business\nrequirements often influence the quality attributes of a software system.\nFunctional requirements\nFunctional requirements describe the functionality of the software., In other words, what\nthe software system must do. They detail the capabilities of the software system in terms of\nbehavior. The functionality and capabilities described by the functional requirements\nenable stakeholders to perform their tasks.\nFunctional requirements include the interaction of the software with its environment. They\nconsist of the inputs, outputs, services, and external interfaces that should be included with\nthe software.\nKeep in mind that requirements can come from a variety of sources, such as the following:\nOrganizational requirements: Requirements that are based on organizational\npolicies and procedures\nLegislative requirements: Non-functional requirements that detail any laws and\nregulations that the software must follow\nEthical requirements: Any requirements for the ethical operation of the software,\nsuch as concerns related to privacy or safety\nDelivery requirements: Requirements that are related to the delivery and\ndeployment of the software\nStandards requirements: Requirements for any standards that must be followed\nfor the development of the software or how the software must operate\n", "page": 93, "type": "text", "section": "Page 93"}
{"text": "Understanding the Domain\nChapter 3\n[ 65 ]\nExternal requirements: Requirements that originate externally, such as\nrequirements from external systems that must integrate with the software system\nbeing designed\nNon-functional requirements\nNon-functional requirements are conditions that must be met in order for the solution to\nbe effective, or constraints that must be taken into consideration. Business analysts and\nother stakeholders tend to do a good job at capturing functional requirements, but they\ndon't always focus as much on non-functional requirements. However, non-functional\nrequirements are an important part of requirements engineering. The success of the project\nis dependent on the non-functional requirements and whether or not they are met.\nWhen a software architecture is designed, the software architect must ensure that the non-\nfunctional requirements can be satisfied. Non-functional requirements can have a\nsignificant impact on the design of the architecture. For that reason, they are of great\nimportance to software architects. Software architects need to play an active role in eliciting\nnon-functional requirements from stakeholders and ensuring that they are captured.\nQuality attributes are a subset of non-functional requirements and include the ilities, such\nas maintainability, usability, testability, and interoperability. We'll go into more detail on\nsome of the different quality attributes in the next chapter.\nConstraints\nConstraints are some type of restriction on the solution and may be technical or non-\ntechnical in nature. Some constraints on a project might be captured and classified as a\nfunctional or non-functional requirement, or they might be explicitly categorized as a\nconstraint. Either way, the important thing is that they are decisions that have already been\nmade and must be honored. Typically, a constraint cannot be changed, and the software\narchitect does not have any control over it. However, if you have reasons why you believe a\nconstraint should be changed or removed, there may be situations in which you could\nprovide your input.\nConstraints can pertain to a number of aspects of a software project. The following are\nsome examples of constraints:\nAn organization might have an existing agreement with a particular vendor or\nhas already purchased a certain technology or tool that you will be required to\nuse\nThere may be a law or regulation that the software must follow\n", "page": 94, "type": "text", "section": "Page 94"}
{"text": "Understanding the Domain\nChapter 3\n[ 66 ]\nThere may be a particular deadline for a milestone or the final delivery of the\nsoftware that cannot be altered\nManagement might dictate that a certain number of resources be assigned to the\nproject, or that the project must utilize outsourced resources\nIf the development team already exists and they are skilled in a particular\nprogramming language, the organization may require that programming\nlanguage be used\nConstraints should be considered while designing the solution, just like other types of\nrequirements.\nThe importance of requirements engineering\nThe importance of requirements analysis cannot be overstated. Proper requirements\nanalysis is crucial for a successful project since it affects all of the subsequent phases. If it\nisn't done properly, additional work will be required, resulting in time and cost overruns.\nThe later in the life cycle such problems are encountered, the more it will cost and the\nlonger it will take to correct those mistakes. When a problem with requirements is\ndiscovered later in the life cycle, the deliverables that have already been produced in\nsubsequent phases, such as design and development, may require refactoring. In Code\nComplete (Second Edition), Steve McConnell explains that the principle is to find an error as close\nas possible to the time at which it was introduced. The longer the defect stays in the software food\nchain, the more damage it causes further down the chain.\nSome of the many benefits of proper requirements analysis include:\nReduced rework\nFewer unnecessary features\nLower enhancement costs\nFaster development\nReduced development costs\nBetter communication\nMore accurate system testing estimates\nHigher customer satisfaction levels\nIt is imperative that management understands the importance and benefits of proper\nrequirements engineering. If they do not, an attempt must be made to communicate that to\nthem so that proper time can be scheduled for requirements engineering.\n", "page": 95, "type": "text", "section": "Page 95"}
{"text": "Understanding the Domain\nChapter 3\n[ 67 ]\nSoftware requirements must be measurable and\ntestable\nWhen defining software requirements, they should be complete in that all of them are\ndefined, and consistent in that they are clear and do not contradict each other. Each\nsoftware requirement should be unambiguous, measurable, and testable. Testing should be\nconsidered when requirements are written. Requirements need to be specific enough that\nthey can be verified.\nBusiness analysts and other stakeholders who are defining the requirements must write\nthem in a way so that they are measurable and testable. As a software architect, if you see\nrequirements that do not satisfy these conditions, you need to point them out so that they\ncan be modified.\nIf a requirement is to be considered measurable, it should provide specific values or limits.\nIn order for a requirement to be testable, there must be a practical, cost-effective way to\ndetermine whether the requirement has been satisfied. It must be possible to write a test\ncase that can verify whether or not the requirement has been met.\nFor example, consider a requirement that states that the web page must load in a timely\nmanner. What exactly does that mean? Stakeholders and the development team may have a\ndifferent understanding of what will satisfy such a requirement. It should be written with a\nspecific limit in mind, such as the web page must load within two seconds.\nA common understanding and mutually agreed upon expectations need to be set with\nstakeholders so that there are no surprises when the final product is delivered.\nSoftware requirements that affect architecture\nAs a software architect who is designing an architecture that will satisfy the requirements,\ncomplete and validated requirements are crucial to your job. Requirements, particularly the\nquality attributes, can greatly affect the architectural design.\nHowever, the degree to which a particular requirement has an effect on the architecture\nvaries. Some do not have any effect, while others have a profound one. You must be able to\nrecognize the requirements that may affect decisions you are making architecturally.\n", "page": 96, "type": "text", "section": "Page 96"}
{"text": "Understanding the Domain\nChapter 3\n[ 68 ]\nMany times, the requirements that affect a software architecture's design are quality\nattributes. Therefore, you should pay particular attention to those. Be aware though that it\nis common for the definition of quality attributes to be lacking when defining requirements.\nStakeholders may focus on functional requirements, and may not define quality attributes,\nor if they do, they may do so in a way that is not measurable and testable.\nSoftware architects may need to make an extra effort to understand the quality attributes\nthat are important to the stakeholders, and the values expected to make them testable to get\nthe quality attributes defined and documented. In the next section, let's examine ways in\nwhich you might elicit requirements, including quality attributes, from stakeholders.\nRequirements elicitation\nPerhaps you have heard of the terms known knowns, known unknowns, and unknown\nunknowns. They are used to describe that which we know about, that which we are aware of\nbut do not know about, and that which we are not even considering because we do not\nknow about them.\nIdeally, the requirements and business domain of a software project are well understood.\nHowever, the development team may not have such an understanding from the onset of\nthe project. Even for those who do have the knowledge, such as some of the stakeholders,\nthey may not know exactly what they want from the software.\nAs a result, you will be dealing with both knowns and unknowns. Part of requirements\nengineering involves gaining as much knowledge as possible regarding the requirements of\nthe system we want to build. We seek to eliminate the unknown unknowns and consider as\nmany of the requirements as possible when designing the software.\nThe start of that process is to elicit requirements from stakeholders, which is known as\nrequirements gathering or requirements elicitation. Requirements gathering seems to\nimply simply collecting requirements that are easy to discover, although it typically\ninvolves much more than that. Often, it is necessary to elicit the requirements from\nstakeholders because not all of them are at the forefront of the thoughts of stakeholders. It\nis more of a proactive, and not a reactive, process.\nAs Andrew Hunt and David Thomas point out in The Pragmatic Programmer:\n\"Requirements rarely lie on the surface. Normally, they are buried deep beneath layers of\nassumptions, misconceptions, and politics.\"\n", "page": 97, "type": "text", "section": "Page 97"}
{"text": "Understanding the Domain\nChapter 3\n[ 69 ]\nTechniques to elicit requirements\nObtaining information from stakeholders takes effort, but there are proven techniques that\ncan assist you to draw them out. Each technique has its own advantages and\ndisadvantages, so select the ones that are most likely to work given your situation. Don't\nforget that you can use more than one of these techniques. Using multiple techniques in\nconjunction with each other may yield the best results.\nInterviews\nOne way to elicit requirements is to conduct interviews with stakeholders. Interviews for\nthis purpose can either be conducted formally or informally. Each interview session should\neither be with a single person or a small group. If it is with more than one person, you don't\nwant to have too many people in the session or you risk not getting the maximum amount\nof information from each individual stakeholder.\nOne or more people can ask questions, and at least one person should be designated to take\nnotes. Ask open-ended questions to spur discussion and get information, and closed-ended\nquestions can be used to confirm facts.\nAs with all of the techniques, the success of interviews depends on the knowledge of the\ninterviewee, as well as their willingness to participate. It is good to interview different\ntypes of stakeholders in order to get different perspectives. You need to take into\nconsideration their knowledge and experience when reviewing the results of the\ninterviews. Interviews are not always a good way to reach consensus because not all of the\nstakeholders may be present, but they could be effective to obtain information.\nRequirements workshops\nRequirements workshops are one of the most common and effective elicitation methods.\nThey are used to collect and prioritize requirements. A group of relevant stakeholders is\ninvited to attend a session in which they will provide their feedback. An inevitable result of\nhaving such discussions will be a higher level of clarity on how the software should work,\nand what it is required to do.\nA clear agenda should be set for each requirements workshop. The scope varies, but you\nmight want to keep each session restricted to a certain part of the business process or\nsoftware application. Someone should be designated as the facilitator who can run the\nmeeting, and a different person should take notes.\n", "page": 98, "type": "text", "section": "Page 98"}
{"text": "Understanding the Domain\nChapter 3\n[ 70 ]\nThe duration of a requirements workshop can vary and is dependent on the scope. It can\nlast anywhere from an hour to several days. The length of the workshop should be\nappropriate for its scope.\nYou can acquire quite a bit of information from a requirements workshop. Just be sure to\nhave an ideal number of people participate. If there are too many, the process could be\nslowed down, and some people may not have an opportunity to share their thoughts. On\nthe other hand, if there are not enough attendees, then you may not gather enough\ninformation.\nSometimes it can be difficult to get all of the stakeholders in one place at the same time. If\nyou can't arrange that, you could consider running multiple workshops on the same topic.\nBrainstorming\nBrainstorming sessions involve getting thoughts from a group spontaneously and\ndocumenting those thoughts. It can be a fun, productive way to get requirements for a\nsystem. If you are going to conduct such a session, make sure you invite the relevant\nstakeholders. If there are many stakeholders, you may want to consider holding multiple\nsessions and keeping the attendance of each one to between five and ten people.\nWhen inviting the stakeholders, make sure you have a variety. Different types of\nstakeholder will have different perspectives and may provide ideas that others would not\nhave considered.\nThe brainstorming session should have clear goals, and each session should not be too\nbroad. You may conduct a brainstorming session to get requirements for a specific piece of\nfunctionality within the software system.\nTry to hold the brainstorming session in a relaxed, comfortable environment so that the\nparticipants feel comfortable sharing their ideas. Someone should be designated as the\nfacilitator. The facilitator may need to encourage participation, especially at the beginning\nof the session, since some participants may hold back their thoughts. As a software\narchitect who wants the group to generate ideas, you can take it upon yourself to come up\nwith the first idea to encourage others.\nCriticism of ideas shouldn't be tolerated, as you don't want to discourage anyone from\nparticipating further. While there may not be any bad ideas, sometimes there will be\nthoughts that are not relevant to the goals of the meeting. If a discussion does get off topic,\nthe facilitator should limit the discussion and steer it in another direction.\n", "page": 99, "type": "text", "section": "Page 99"}
{"text": "Understanding the Domain\nChapter 3\n[ 71 ]\nEither the facilitator or another person should take notes, preferably on a whiteboard so\nthat everyone can see the ideas that have previously been given. For remote meetings\nwhere everyone is not in a room together, someone should share their screen so that the\nideas are visible.\nThere should be a time limit so that everyone is aware when the session will end. If there is\na clear ending where no more new ideas are being generated, the session could be called off\nearly.\nObservation\nObservation is a technique where someone studies a stakeholder in their work\nenvironment, performing tasks related to the software project. It is particularly useful when\nyou are attempting to understand a current process. It is effective because the observer may\nnotice things that aren't mentioned through other elicitation techniques. Stakeholders may\nforget certain requirements or may not even be aware that what they are doing is a\nrequirement that needs to be documented. By observing the actual work performed, you\ncan sometimes gather important information.\nThis technique can either be performed in a passive way or in an active way, depending on\nwhat is agreed upon and what would be most effective. If the observer is passive, then he\nor she makes every attempt not to be disruptive. The observer does not ask many\nquestions, nor do they interrupt the tasks that the stakeholder is performing. If the observer\nis active, then they can have an ongoing dialog with the stakeholder and ask questions\nwhile they are performing their tasks.\nThere are disadvantages to this technique. It can be time-consuming to observe someone\nperforming his or her daily work. The person who is being observed may find it disruptive\nto their work, so you may only get limited time to perform the observation.\nEven if a lot of time is spent observing, you may not witness all of the possible scenarios,\nand being aware of scenarios that do not happen as frequently is still important for the\nrequirements of the software.\nAlthough you shouldn't use this technique as the only one, it can be useful as a supplement\nto other techniques because it may draw out requirements that would not be revealed using\nother elicitation techniques.\n", "page": 100, "type": "text", "section": "Page 100"}
{"text": "Understanding the Domain\nChapter 3\n[ 72 ]\nFocus groups\nFocus groups can be organized to elicit requirements. This technique is more formal than\nbrainstorming and involves inviting a group of participants to provide feedback. This\ntechnique is commonly used for public applications that will have external users. In that\ncase, the invited participants are users or outside experts who are external to the\norganization.\nA moderator runs the session. The selected moderator is often skilled at running focus\ngroups and is hired specifically to perform that role. The moderator asks the questions and\nencourages the participation of all the participants. Moderators should remain neutral\nduring the session.\nThe questions asked during a focus group are typically open-ended and promote\ndiscussion. Responses in a focus group are typically spoken, as opposed to written. In that\ntype of setting, things like nonverbal communication and group interaction can be\nobserved. Focus group participants can foster new ideas from each other. This technique\ncan be faster than conducting interviews individually.\nDespite the advantages, there are some disadvantages to this technique. Focus groups run\nthe risk of individuals following the crowd as they hear the feedback from others in the\ngroup. Some people are hesitant about sharing their ideas in a group setting, and the\nmoderator may need to be paid to conduct the focus group.\nSurveys\nSurveys can be created and given to stakeholders to get information. Surveys should have a\nclear purpose. Rather than create a large survey that covers many topics, it may be more\neffective to create multiple surveys, each covering a portion of the business processes or\nsoftware application. Some people will be averse to filling out extremely long surveys.\nThe questions in the survey should be well thought out, clear, and concise. Although\nsurveys can have open-ended questions, typically the questions in a survey are closed-\nended ones. This makes it easier for participants to provide answers, and, more\nimportantly, the answers will be easier to analyze. If you do want to ask open-ended\nquestions in a survey, keep in mind that it will require more effort to analyze the answers.\n", "page": 101, "type": "text", "section": "Page 101"}
{"text": "Understanding the Domain\nChapter 3\n[ 73 ]\nDocument analysis\nDocument analysis utilizes existing documentation to obtain information and\nrequirements. The documentation may cover the relevant business processes or existing\nsoftware systems. If there is an existing system in use, it can serve as a starting point for the\nrequirements of the new system. The documentation may come in the form of technical\ndocumentation, user manuals, contracts, statements of work, emails, training materials, and\nanything else that may be of use.\nThe documentation may even be commercial off-the-shelf (COTS) software package\nmanuals. There may be existing software that provides part or all of the functionality you\nare seeking to implement, and by analyzing that documentation you can get the\nrequirements for your software system.\nAnalyzing documents is particularly useful if stakeholders are not available for other\nrequirements elicitation techniques.\nPrototyping\nPrototyping is a requirements elicitation technique that involves building a prototype that\nstakeholders can use to some degree, or at least see. Some people are more visually-\noriented than others, and having a prototype can trigger ideas regarding requirements.\nThe disadvantage of prototyping is that it can take time to build a prototype. However,\nwith most modern technologies a prototype can be built quickly. There is also the option of\nsimply creating visual diagrams of the software, rather than a prototype. For web\napplications, this involves creating wireframes, which are visual representations of web\npages that let a person see the layout and structure of web pages.\nThe scope of a prototype can be as broad or narrow as you want it to be. While it can\ndemonstrate an entire application, it could be focused on a specific piece of functionality.\nPrototyping can be useful in conjunction with other techniques so that you can validate\nrequirements and uncover things that had not already been discussed.\nPrototyping can also be taken to a different level in which a working version of the\nsoftware is produced. In a situation where the direction and purpose of the software have\nnot been fully evaluated yet, perhaps because the stakeholders don't know where to begin,\nor they have many ideas but cannot agree among themselves, an initial prototype can be\ndeveloped. If you are using an agile methodology, a few initial iterations can take place,\neach ending with a working version of the software that can be shared with the\nstakeholders.\n", "page": 102, "type": "text", "section": "Page 102"}
{"text": "Understanding the Domain\nChapter 3\n[ 74 ]\nOnce they have something concrete to look at and use, it may inspire them with new ideas\nand approaches. Everyone will be able to see what works, and just as important, what does\nnot work. If done properly, as refactoring occurs and further iterations take place, the\nrequirements will become more apparent as the software takes shape.\nReverse engineering\nReverse engineering is a method in which existing code is analyzed to determine the\nrequirements. It is similar to the document analysis technique in that it assumes that there\nare existing artifacts to analyze. This is not always the case when designing a new software\nsystem. It also requires access to the source code, and someone with the technical skill to\nanalyze the code and extract requirements from it.\nIt is a time-consuming technique but might be used as a last resort if other techniques are\nnot possible. For example, if stakeholders are not available to you, or the ones who are\navailable to you are not knowledgeable, many of the other techniques may not be viable. If\nthere is also a lack of documentation, then document analysis may also not be possible.\nThis method is not just a final course of action when others are not possible though. When\nappropriately used, it can be a powerful technique. Stakeholders may have limited\nperspectives, and may not think of everything that the software is required to do. If there is\nan existing system, looking at the code is a way to determine exactly what needs to happen.\nGet access to the proper stakeholders\nEven armed with techniques to elicit requirements, it can be difficult to get them from the\nproper stakeholders. You may find yourself in situations where certain stakeholders are not\nmade available to you. You may also find yourself in a situation where certain\nstakeholders, for various reasons, are not being helpful or do not want to participate in the\nproject.\nDue to the importance of requirements analysis, you must make the effort to get access to\nthe relevant stakeholders. This may involve speaking with management to get the proper\naccess. Although this may be easier if you work for the same organization, many\nstakeholders will be external to the organization. The success of the project may depend on\nit, so you may need to escalate this need to your own management or to the management of\nthe stakeholder's organization.\n", "page": 103, "type": "text", "section": "Page 103"}
{"text": "Understanding the Domain\nChapter 3\n[ 75 ]\nSummary\nBeing an effective software architect means understanding the domain of the software you\nare building. Gaining knowledge of general business topics and a deep understanding of\nthe organization's business is the foundation for becoming an expert on the problem space\nfor which you will be designing a solution.\nDDD is a proven approach to modeling a domain. Creating a ubiquitous language that will\nsimplify and facilitate communication between everyone involved in the software project,\nand working with domain experts, will facilitate learning a particular domain.\nOther practices, such as separating your domain into subdomains and creating bounded\ncontexts in your domain model, will minimize complexity and allow you and your team to\nfirmly grasp even complex domains.\nOne of the keys to building software successfully is proper requirements engineering,\nincluding knowing how to effectively elicit requirements from stakeholders. Knowing the\nrequirements for the software is crucial to designing an appropriate solution.\nIn the next chapter, we will further explore one of the most important types of software\nrequirements: quality attributes. Building quality software requires the software architect to\nknow and understand the details of the quality attributes that are important to the\nstakeholders.\n", "page": 104, "type": "text", "section": "Page 104"}
{"text": "4\nSoftware Quality Attributes\nQuality attributes are of the utmost importance to software architects because they affect\narchitectural decisions. In this chapter, we will begin by explaining what quality attributes\nare, and why it is important to consider them throughout the software development life\ncycle. Some quality attributes can be difficult to test, so we'll explore what it takes to test\nthem.\nWe will go into detail on some more common software quality attributes, such as\nmaintainability, usability, availability, portability, interoperability, and testability. After\nreading this chapter, you will know the considerations that need to be taken into account\nfor a software system to meet these quality attributes.\nIn this chapter, we will cover the following topics:\nQuality attributes\nMaintainability\nUsability\nAvailability\nPortability\nInteroperability\nTestability\nQuality attributes\nQuality attributes are properties of a software system and a subset of its non-functional\nrequirements. Like other requirements, they should be measurable and testable. Software\nquality attributes are benchmarks that describe the software system's quality and measure\nthe fitness of the system. A software system is made up of a combination of quality\nattributes, and the degree to which they are fulfilled describes the overall quality of the\nsoftware.\n", "page": 105, "type": "text", "section": "Page 105"}
{"text": "Software Quality Attributes\nChapter 4\n[ 77 ]\nQuality attributes can have a significant impact on the design of the architecture, so they\nare of great interest to software architects. They affect multiple facets of a software system,\nsuch as its design, degree of maintainability, runtime behavior, and the overall user\nexperience.\nWhen designing your architecture, it is imperative to understand that software quality\nattributes can affect each other, and the degree to which one is met can affect the degree to\nwhich others can be met. It's important to identify potential conflicts between quality\nattributes. For example, a need for ultra-fast performance may conflict with the ability to\nachieve extreme scalability, and having a high level of security could decrease the level of\nusability.\nSuch trade-offs need to be analyzed to provide a balance so that an acceptable solution can\nbe created. The priority of each quality attribute will be a factor in your overall design.\nExternal or internal\nQuality attributes can be internal or external. Internal quality attributes can be measured by\nthe software system itself and are visible to the development team. Consequently, they are\nmeasurable both during and after development. Examples of internal quality attributes are\naspects of the software system such as lines of code (LOC), level of cohesion, readability of\nthe code, and the degree of coupling between modules.\nThese attributes reflect the complexity of the software system. Although internal quality\nattributes are not visible to users directly, they affect external quality attributes. A greater\nlevel of internal quality often leads to a greater level of external quality.\nExternal quality attributes are properties that are externally visible; hence, they are\nnoticeable to end users. These quality attributes are measured with respect to how the\nsoftware system relates to its environment. Unlike internal quality attributes, a working\nversion of the software must be deployed so that it can be tested. Examples of external\nquality attributes are the performance, reliability, availability, and usability of the system.\n", "page": 106, "type": "text", "section": "Page 106"}
{"text": "Software Quality Attributes\nChapter 4\n[ 78 ]\nQuality attributes and the SDLC\nQuality attributes should be considered throughout the software development life cycle\n(SDLC). The process begins during requirements engineering by ensuring that they are\ncaptured completely and correctly. In the previous chapter, we discussed the importance of\nmeasurable and testable requirements. This should be stressed further in relation to quality\nattributes, as some quality attributes can be difficult to measure. Quality attributes must be\nmeasurable and testable in order to determine if the software system satisfies the needs of\nstakeholders.\nThe software architecture design must ensure that it can meet the quality objectives. During\ntesting, quality attributes must be verified to ensure that the software system satisfies the\nrequirements.\nTesting quality attributes\nWhat kinds of testing techniques should be used to verify quality attributes? Beyond the\nsimplest of software applications, it can be challenging to test software quality attributes\nand all of the possible scenarios that are affected by them. A variety of testing techniques\nmust be employed in order to test different properties. For example, the following are some\ntests you may need to conduct:\nManually testing the software for usability\nCreating benchmarks and using tools for performance testing\nPerforming code reviews and calculating code metrics to test for maintainability\nExecuting automated unit tests to ensure the system behaves as expected\nEach testing technique comes with its own set of strengths and weaknesses. Some software\nquality attributes are difficult to assess, so you may need to combine multiple testing\ntechniques in order to effectively test a quality attribute. Given unlimited resources, we\nmight perform an exhaustive amount of testing, but projects are typically constrained by\nsome combination of cost and time. As a result, a balance sometimes needs to be reached\nbetween the amount of testing and the time available.\nAutomating as much of the testing as possible is key to being able to execute tests in a short\namount of time while maximizing test coverage. Automated tests can be executed on-\ndemand or as part of a continuous delivery process (for example, as part of an automated\nbuild). Development teams can be assured that they still meet quality attribute objectives as\nthey continue to make changes to the code.  \n", "page": 107, "type": "text", "section": "Page 107"}
{"text": "Software Quality Attributes\nChapter 4\n[ 79 ]\nNow that we know more about quality attributes, let's go into detail about some common\nones (performance and security will be covered in Chapter 10, Performance Considerations,\nand Chapter 11, Security Considerations).\nMaintainability\nMaintainability focuses on the ease with which a software system can be maintained.\nMaintenance of a software system takes place as changes are made to it after the software is\nin operation. Maintenance is necessary to preserve the value of the software over time.\nChange is constant in the real world. Sometimes it is expected and can be planned for,\nwhile other times it is not. Either way, it is inevitable that software systems will experience\nchange. With the knowledge that change is unavoidable, it is important to build\nmaintainable systems.\nDecades ago, the greater part of a software project's costs went into software development.\nHowever, over the years there has been a shift in the cost ratio from development to\nmaintenance. Today, the majority of a system's lifetime costs can typically be attributed to\nmaintenance. Doing whatever is possible to keep those costs down can make a significant\ndifference in the total money spent on the software over its lifetime.\nCode that is easy to maintain allows maintenance work to be completed more quickly, and\nin turn will help to keep maintenance costs down. When a developer is writing code, he or\nshe has to take into consideration not just the end user of the software, but also those who\nwill be maintaining it.\nEven if the original developer ends up also being responsible for the maintenance of a\nparticular piece of code, consider the fact that the developer could leave the organization.\nFurther, a developer may have a need to revisit their own code after some time passes, only\nto have forgotten the intricacies of it. In some cases, a developer may not even remember at\nfirst that they were the original developer of the code! Maintainable code benefits whoever\nneeds to maintain it, even if it is the original developer.\nMaintainability also affects how easily a software system can be reverse-engineered. There\nmay be a need for a software system to be reverse-engineered, possibly so that it can be\nmigrated to a newer technology. An architecture that exhibits a high level of modifiability\nwill be easier to understand and reason about, making it easier to reverse-engineer.\n", "page": 108, "type": "text", "section": "Page 108"}
{"text": "Software Quality Attributes\nChapter 4\n[ 80 ]\nTypes of software maintenance\nSoftware maintenance is performed for a variety of reasons, such as correcting defects,\nimproving quality in some way, or meeting new requirements. As a result, software\nmaintenance work can be categorized into the following different types of software\nmaintenance:\nCorrective\nPerfective\nAdaptive\nPreventive maintenance\nCorrective maintenance\nCorrective maintenance is the work involved in analyzing and fixing defects in the\nsoftware. Although it isn't the only kind of maintenance, it is the type that people associate\nmost with maintenance work. Defects might be found internally, or by users in production.\nThe severity and priority of defects vary depending on the nature of the bug.\nSeverity represents the level of impact the bug has on the operation of the software.\nOrganizations have various classification systems for severity, but categories such as\ncritical, high, medium, and low are common examples.\nThe priority of a defect is the order in which it will be fixed. Typically, the higher the\npriority, the quicker it will be fixed. Like severity, organizations may have different\nclassification systems for priority, but categories such as high, medium, and low are common.\nAnother common classification system is P0, P1, P2, P3, and P4, with P0 being the highest\npriority. P0 defects are critical and are considered blockers. A release will be put on hold\nuntil all P0 defects are fixed. \nMaintainability can be measured by the time it takes to analyze and fix a particular defect.\nHigher levels of maintainability allow these tasks to be completed in a shorter amount of\ntime.\nPerfective maintenance\nPerfective maintenance is necessary when the software needs to implement new or\nupdated requirements. These types of changes are mostly focused on the functionality of\nthe software. An example of perfective maintenance is a new enhancement to the software\nsystem.\n", "page": 109, "type": "text", "section": "Page 109"}
{"text": "Software Quality Attributes\nChapter 4\n[ 81 ]\nSoftware that has a higher level of maintainability will allow for perfective changes to be\nmade with less effort, and therefore at a lower total cost.\nAdaptive maintenance\nAdaptive maintenance is defined as the work required to adapt a software system to\nchanges in the software environment. Examples of this may be to adapt the software system\nfor a new operating system (OS), a new version of the same OS, or to use a new database\nmanagement system (DBMS).\nThe duration of time it takes to adapt a software system to changes in the environment is a\nmeasure of the maintainability of the software.\nPreventive maintenance\nThe goal of preventive maintenance tasks is to prevent problems in the future, by\nincreasing quality. This may include improving quality attributes of the software system,\nsuch as increasing maintainability and reliability.\nPreventive maintenance may take place to make maintenance easier in the future. An\nexample of this is refactoring a software component to make it less complex.\nModifiability\nModifiability is one aspect of maintainability. Modifiability is the ease with which changes\ncan be made to the software without introducing defects or reducing quality. It is an\nimportant quality attribute because there are a number of reasons why software needs to be\nchanged.\nSome software can remain useful in production for years, or even decades. Inevitably, that\ncode will need to be modified for the different types of maintenance that were described\npreviously. The time required from when a change is specified until it is deployed is an\nindication of the modifiability of the system.\nIn today's world, agile software development methodologies are the most common. These\nsoftware projects embrace change. In addition to new functionality, each iteration of the\nproject can involve changes to existing code. Improving modifiability is not just beneficial\nfor maintenance but also for the entire development of the software.\n", "page": 110, "type": "text", "section": "Page 110"}
{"text": "Software Quality Attributes\nChapter 4\n[ 82 ]\nExtensibility and flexibility\nExtensibility and flexibility are additional characteristics related to maintainability. The\nlevel of extensibility reflects how easy it is to extend or enhance the software system.\nSoftware systems that are designed to be extensible take future growth into consideration\nby anticipating the need to add new functionality.\nFlexibility is similar but mostly focuses on how easy it is to change a capability so that it can\nbe used in a way that wasn't originally designed. Both extensibility and flexibility are\ncharacteristics that dictate the level of ease with which someone can perform perfective\nmaintenance.\nScope of modifications\nNot all changes to software are equal, and the scope of a particular modification is a factor\nin how difficult the change will be to implement. The larger and the more complex the\nmodification, the greater the effort to complete it.\nIn addition to size, if the changes require architecture level changes, that will increase the\nlevel and scale of effort involved. Some components and their interactions may need to be\nrefactored extensively for large changes.\nDesigning for maintainability\nIn order to design a software architecture that exhibits maintainability, you must reduce the\ndifficulty in implementing changes. Making it easier to implement changes, in large part,\nmeans reducing the complexity of the architecture and its components.\nHigher levels of complexity make software systems harder to understand, test, and\nmaintain. For this reason, the level of complexity is a predictive measure of some quality\nattributes, including maintainability.\nEvidence shows that modules with greater complexity are also more likely to contain\nerrors. What makes this situation worse is that such modules are also more difficult to test,\nmeaning that it may be more likely that an error will go undetected.\nAlthough taking measurements and designing architectures to lower complexity may\nrequire more time (and therefore cost more money) during development, cost savings will\nbe realized in the long term due to higher quality and a greater level of maintainability.\n", "page": 111, "type": "text", "section": "Page 111"}
{"text": "Software Quality Attributes\nChapter 4\n[ 83 ]\nSome techniques used to reduce complexity and increase maintainability are to:\nReduce size\nIncrease cohesion\nReduce coupling\nReducing size\nModules that are large tend to be more complex, and are responsible for a larger share of\nthe logic. Therefore, the larger a module is, the more difficult it can be to change. Your\ndesign should seek to reduce the size of individual modules. One way to accomplish this\nwould be to split up a module into multiple ones.\nIncreasing cohesion\nCohesion represents how interrelated the elements in a particular module are. Designs\nshould seek to increase cohesion by not allowing a particular module to have too many\ndisparate elements. Increasing cohesion reduces complexity and enables a number of\ndifferent quality attributes, including maintainability. High cohesion often correlates to\nloose coupling.\nReducing coupling\nCoupling refers to how dependent different modules are on each other. Designs should\nseek loose coupling, such that different modules are independent of each other or almost\nindependent. If it becomes necessary to make changes to a highly coupled module, it will\nbe more likely that other modules will be affected and therefore also require changes. Loose\ncoupling reduces complexity and enables a number of different quality attributes, including\nmaintainability. Loose coupling often correlates to high cohesion.\nMeasuring maintainability\nA number of software metrics can help to measure the complexity of software, and\ntherefore also the maintainability of software. Although measuring complexity and\nmaintainability can be difficult, there are software metrics that can provide you with\ninsights about the level of complexity and maintainability of your software.\n", "page": 112, "type": "text", "section": "Page 112"}
{"text": "Software Quality Attributes\nChapter 4\n[ 84 ]\nLines of code (LOC)\nOne such software metric is lines of code (LOC), also known as source lines of code\n(SLOC). This measurement simply represents the size of a software system by determining\nthe number of lines in its source code.\nTypically, software systems with more lines of code are greater in complexity and more\ndifficult to maintain than those with fewer lines of code. However, comparisons of LOC\nbetween different software systems are really only useful if they involve an order of\nmagnitude difference in lines of code. For example, if one software system has 50,000 lines\nof code, and another has 48,000 lines of code, you won't be able to make a determination on\nwhich software is more maintainable. However, if you were to compare a software system\nwith 10,000 lines of code with one that has 100,000 lines of code, then you are more likely to\nmake a useful determination.\nDevelopment tools and integrated development environments (IDEs) can count the \nnumber of lines of code for you. However, there are different ways that it can be calculated,\nand it is debatable which is the most useful. There is also a challenge if a software system\nuses multiple languages.\nThe two major ways that it can be calculated are the physical LOC and the logical LOC. The\nphysical LOC typically is just a count of all of the source code lines, excluding comment\nlines. The logical LOC takes into consideration the actual number of program language\nstatements in an attempt to only count effective lines of code. While the physical LOC is\neasier to calculate, it is more sensitive to being affected by such things as line spacing and\nother formatting.\nCyclomatic complexity\nCyclomatic complexity is a quantitative software metric that reflects the complexity of a\nsoftware module. It was developed by Thomas J. McCabe and is sometimes referred to as\nMcCabe's cyclomatic complexity. It measures the number of linearly independent paths\nthrough a module or detailed design element. Higher cyclomatic complexities indicate that\nthe software is more complex.\nCyclomatic complexity can be calculated in a few different ways, but one of the most\ncommon is by using the following formula:\n", "page": 113, "type": "text", "section": "Page 113"}
{"text": "Software Quality Attributes\nChapter 4\n[ 85 ]\nAn explanation for the preceding formula is as follows:\nCC = Cyclomatic complexity\nE = the number of edges of the graph\nN = the number of nodes of the graph\nP = the number of connected parts in the graph\nGiven a control flow graph created by examining the code or simply by eyeballing the code\nwithout creating a control flow graph, you must first count the number of nodes and edges.\nNodes are a single block of code with statements executed in a sequence with no jumps.\nEdges represent control flow between nodes.\nFor example, assume we have the following pseudocode representing a simple\nif/then/else structure:\nif (N1)\n  then N2\n  else N3\nend if\n  N4\nThe control flow graph for it would look like the following:\nThere is only one entry point (N1), and one exit point (N2) to this example, so there is only\none path (P = 1). There are four edges and four nodes, so the cyclomatic complexity is 4 - 4 +\n(2 x 1) = 2.\nCyclomatic complexity values greater than 10 typically indicate modules that are complex,\nmaking them more error-prone and harder to test. Teams should seek to refactor\ncomplicated modules to lower their cyclomatic complexity.\n", "page": 114, "type": "text", "section": "Page 114"}
{"text": "Software Quality Attributes\nChapter 4\n[ 86 ]\nDepth of inheritance tree (DIT)\nThe depth of inheritance tree (DIT) is a code metric that is specific to object-oriented\nprogramming. It measures the maximum length between a node and the root node in a\nclass hierarchy. Take a look at the following diagram of a simple class hierarchy:\nIn this example, Class C inherits from Class B, which inherits from Class A. The DIT of\nClass A is 0, the DIT of Class B is 1, and the DIT of Class C is 2. A higher number for the\nDIT indicates a greater level of complexity. It also means there is a larger number of\nattributes and methods being inherited, which means there is code reuse through\ninheritance but it may make it more difficult to predict behavior. A lower number for the\nDIT indicates less complexity but it also may mean that there is less code reuse through\ninheritance.\nInheritance is a powerful concept in object-oriented programming and designs should\nstrive for code reuse. Therefore, it is not that the DIT should always be kept to a minimum.\nA balance must be reached between code reuse and complexity. As a general rule, if the\nDIT is greater than 5, you may want to analyze the reasons for it and possibly reduce\ncomplexity by refactoring so that the tree is not as deep.\n", "page": 115, "type": "text", "section": "Page 115"}
{"text": "Software Quality Attributes\nChapter 4\n[ 87 ]\nUsability\nUsability describes how easy it is for users to perform the required tasks using the software\nsystem. User satisfaction is directly correlated to its level of usability. Users are much more\nlikely to be satisfied with a software system if it is easy to use and provides a good user\nexperience. The users' perception of the software system's overall quality will also be\nhigher. Increasing usability can be one of the easiest and cheapest ways to improve the\nquality of a software system.\nUsability is an important quality attribute because software that is not sufficiently usable\nwill decrease the productivity of users. Even worse, users may not want to use the software\nat all. If the software is a website, and it is hard to use, difficult to navigate, slow to load, or\ndifficult to read, users will simply start using an alternative. \nAllowing users to complete their tasks efficiently\nSoftware systems that exhibit usability allow users to complete their tasks efficiently. Once\nusers have learned how to use the system, the speed at which they perform actions reflects\nthe usability of the software system.\nLearnability\nLearnability is the degree to which new users can learn how to effectively use the software\nsystem. It is defined in ISO/IEC 25010, which is a series of standards that focus on a number\nof product quality characteristics. It reflects how easy it is for users to achieve their goals of\nlearning how to use the software system. In addition to new users, learnability also\nincludes how easy it is for experienced users to learn new functionality that is added to the\nsystem.\nIf a software system is usable, then its learnability level will be higher. The system should\nbe intuitive, so that it does not take a lot of time or effort to learn a piece of functionality\nthat is part of the software system.\n", "page": 116, "type": "text", "section": "Page 116"}
{"text": "Software Quality Attributes\nChapter 4\n[ 88 ]\nProviding useful feedback\nUsable software systems help users by preventing them from making mistakes, minimizing\nthe impact of mistakes, and providing feedback. Examples of useful feedback include:\nAppropriate validation and helpful messages when validation fails\nFriendly and informative messages to the user\nTooltips\nFeedback for long-running processes, such as a progress-bar or notifications\nwhen asynchronous operations complete successfully or fail for some reason\nAccessibility   \nAccessibility is the aspect of usability which provides features that make it easier for\npeople with disabilities or impairments to effectively use the software. This could\npotentially mean thinking about vision (partial or total blindness, color blindness), hearing\n(deaf or hard of hearing), and physical (not being able to type or use a mouse) impairments.\nDesigning for accessibility may include things such as:\nMaking the software usable when only using a keyboard (no mouse)\nProviding support for assistive technologies, such as screen magnifiers, screen\nreaders, text-to-speech software, alternative keyboards, and pointing devices\nEnsuring non-text content (for example, controls, time-based media, and\nCAPTCHAs) has a text alternative\nDesigning the software so that there are ways to help users navigate the software\nProviding users with enough time to read and use the web page or screen (for\nexample, allowing users to adjust or turn off time limits and provide users with\nthe ability to pause, or stop content, such as auto-updating or scrolling content)\nUsing colors that take into consideration color blindness\nEnsuring there is a logical tab order for controls and input fields\nOne key to success in this area is to include people with impairments in the requirement,\ndesign, and usability testing processes. Their feedback can be tremendously helpful in\ngauging how accessible your software system is for them.\n", "page": 117, "type": "text", "section": "Page 117"}
{"text": "Software Quality Attributes\nChapter 4\n[ 89 ]\nUsability needs to be considered during\nrequirements\nDuring requirements engineering, use the requirement elicitation technique of observation\nthat was previously mentioned to observe how users go about their work. This includes the\nprocess they go through as well as the current software that they use (if that software is\nrelevant to the one being built). Through this technique, you can glean what works for the\nusers, and just as importantly, what does not work.\nOther requirement elicitation techniques, such as interviews where you can ask questions,\ncan provide information regarding usability as well. The results of requirements elicitation\ncan be used to create requirements related to usability.\nUsability testing\nOnce a working version of the software is available for testing, usability testing should\ntake place. Some of the same techniques used for requirements elicitation can also be used\nto get usability feedback. Arrange for users to operate the software and perform tasks with\nit while observing what works well and what pain points they may be having. Use\ninterviews and focus groups to get feedback from users on what they liked and did not like\nabout the software. \nAppealing visual design\nThe aesthetics, or the visual appearance, of the software system, is a component of\nusability. The appearance of the application can have a great effect on users, so it is worth\nspending time ensuring that the application is designed to be visually appealing to users.\nFor web applications, there are a number of design choices that can be made to increase\nusability. When designing web applications, consider the following:\nPlace emphasis on readability, including using headers, proper spacing, readable\nfonts, attractive colors, and appropriately formatted text\nMake sure the page layouts for all pages are well thought out\nKeep content concise by avoiding excessive text\nEnsure navigation and menus are not overly complicated to avoid making it\ndifficult for users to find what they want\nEnsure that any links are not broken\n", "page": 118, "type": "text", "section": "Page 118"}
{"text": "Software Quality Attributes\nChapter 4\n[ 90 ]\nProvide tooltips, such as using the title attribute on hyperlinks so that helpful\npop-up text will let users know more about a link\nBe consistent throughout the application with things such as colors, icons, fonts,\nand captions/terms\nDisplay progress indicators when the user has to wait for a response, such as a\nwait animation or a percentage complete indicator\nThe following are some things to consider when designing user interfaces for Windows\ndesktop applications:\nDesign with proper spacing of controls so that they are not too close or too far\nfrom each other\nSize windows and controls such as buttons, dropdowns, grids, and list views\nappropriately based on their expected content\nIf a window is made to be resizable, and content is truncated, make sure that\nmore content is shown as the window is made larger\nConsider setting a minimum window size if content is no longer usable at a\nparticular size\nLabel every control and every group of controls\nMake sure that the tab order of the controls on a window is correct\nDesignate shortcut keys (a single key or key combination) that will allow users to\nquickly perform frequently executed commands\nAssign access keys (alphanumeric keys used with the Alt key) so that users can\nquickly activate a command\nEnsure proper capitalization in window headers, controls, labels, and content\nProvide helpful tooltips\nBe consistent throughout the application with things such as colors, icons, fonts,\nand captions/terms\nProvide the user with feedback on system status when it is busy with a long-\nrunning process, such as showing a progress bar\nThe application should display informative messages to the user, such as\nconfirmation, warning, and error messages, when appropriate\n", "page": 119, "type": "text", "section": "Page 119"}
{"text": "Software Quality Attributes\nChapter 4\n[ 91 ]\nProviding a good help system\nDesigning a system that is highly usable also means that it should be designed to allow\nusers to easily learn how to use it. A thorough and up-to-date help system makes an\napplication easier to learn. Whether you are developing a desktop application or a web-\nbased one, ensure that you have a help system that is extensive enough for the given\napplication so that it will be of use to people. Depending on the software, who is using it,\nand how it is being used, an overall help system may include tooltips, online help, product\nmanuals, tutorials, frequently asked questions (FAQs), a knowledge base/discussion\nforum, training events, and ways to submit support cases.\nThis is not to say that a help system is the primary way of making an application usable.\nThe application should be intuitive and easy to use based on other factors, but an\nappropriate help system does supplement the overall effort to increase learnability and\nusability.\nSoftware must be useful, and not just usable\nKeep in mind that, regardless of how usable the software is, it must have utility. For a\nsystem to have utility, it must provide the functionality that its users need. Being usable is\ngreat, but in order to be useful, it must also have utility. Users must be able to accomplish\ntheir goals and the software must serve its purpose.\nIs the software system meeting the needs of the users? Is it useful to them? Even if the\nsoftware system provides a nice visual experience, without usefulness it will not be a good\nuser experience.\nAvailability\nAvailability describes the degree to which the software system will work as required when\nit is needed. It is the probability that the software system is operating properly when\nneeded by a user and is not experiencing unplanned downtime due to failure and repair.\nIt is typically measured in terms of nines, as in how many nines represent the availability\nlevel (99.9%, 99.99%, or 99.999%).\n", "page": 120, "type": "text", "section": "Page 120"}
{"text": "Software Quality Attributes\nChapter 4\n[ 92 ]\nCalculating availability based on time\nAvailability can be calculated with the following formula:\nMTBF is the mean time between failures, which is the average time between two failures of\nthe system. MTTR is the mean time to repair, which is the average time to troubleshoot and\nrepair the system back to its operational state. Only unplanned outages are considered\nwhen calculating system downtime.\nTo achieve high availability, which is considered five nines, or 99.999% availability, your\nsoftware system can only be down for 5 minutes and 15 seconds or less during the course of\na whole year! The following table shows some sample availability calculations:\nAvailability\nDowntime per year\nDowntime per month\nDowntime per week\n99.0%\n3.65 days\n7.2 hours\n1.68 hours\n99.9%\n8.76 hours\n43.2 minutes\n10.1 minutes\n99.99%\n52.6 minutes\n4.32 minutes\n60.5 seconds\n99.999%\n5.26 minutes\n25.9 seconds\n6.05 seconds\nExtreme levels of availability are not necessarily ideal because, at a certain point, you will\nexperience diminishing returns. Consider that each additional nine requires a whole\nmagnitude improvement of availability. This comes at a cost, in terms of time and money.\nThis can have a negative effect on the software because it will limit how much time and\neffort can be spent on other things, such as releasing new features.\nAlso, consider the fact that a user won't be able to tell the difference between 99.99% and\n99.999% availability. For example, in the case of a web or mobile application, there are other\nfactors such as the user's device and the network that the user is on, which may not be as\nreliable as the software system.\nThe software architect and the stakeholders need to weigh the benefits of higher availability\nlevels against the increased costs of providing them to determine which level suits them\nbest.\n", "page": 121, "type": "text", "section": "Page 121"}
{"text": "Software Quality Attributes\nChapter 4\n[ 93 ]\nCalculating availability based on request success\nrate\nIf your software is globally distributed, perhaps by taking advantage of cloud services\nprovided by companies such as Amazon, Microsoft, or Google, you may be experiencing\nvery high availability, such that at least some traffic is being served at any given time. As a\nresult, calculating availability based on time as we did previously may not be very\nmeaningful.\nAn alternate calculation would be to base availability on the request success rate. This can\nbe calculated as follows:\nYour software system will have a variety of requests, some of which are more important\nthan others, but looking at all requests should provide a good estimation of unplanned\ndowntime.\nAvailability calculations based on time take into consideration the total failure duration,\nincluding the time to repair the software system. It doesn't distinguish between, for\nexample, two failures of 30 minutes and one failure of an hour, but a calculation based on\nthe request success rate does. Considering both of these metrics provides insights into\ndifferent facets of your software's availability, which gives you a more complete picture.\nFaults, errors, and failures\nA part of availability involves how a software system can handle and overcome faults so\nthat the duration of an unplanned outage does not exceed the specified value over a\nparticular amount of time.\nA system fault is a characteristic of a software system that can lead, either on its own or in\nconjunction with other faults, to a system failure. A system fault exists somewhere in the\ncode. An error is an erroneous state of the software system caused by one or more faults. A\nsystem failure is an event experienced by a user in which the software system does not\nbehave as expected. System failures are caused by one or more faults.\nIn order to handle and overcome faults and prevent failures, we can attempt to detect\nfaults, recover from faults, or prevent faults.\n", "page": 122, "type": "text", "section": "Page 122"}
{"text": "Software Quality Attributes\nChapter 4\n[ 94 ]\nDetecting faults\nBefore a software system can recover from a fault, it must first detect a fault. The following\nare some techniques that can be used to detect faults.\nPing/echo reply\nIn this method of detecting faults, a component, acting as a system monitor, sends an\nInternet Control Message Protocol (ICMP) echo request to another component (it pings\nthe component) and waits for an ICMP echo reply. If the target does not respond to the\nping in a predefined amount of time, the component acting as the system monitor reports\nthat the other component has failed.\nHeartbeat\nThis technique requires one component to periodically send a message (heartbeat) to\nindicate that it is operating normally. If the listening component does not receive the\nheartbeat message within the predefined amount of time, it determines that a system fault\nhas occurred and takes the appropriate action.\nTimestamp\nThis strategy of detecting faults focuses on catching an incorrect sequence of events. By\nusing a timestamp, or even just a sequence of numbers, faults can be reported if the\nsequence is in the incorrect order.\nVoting\nA voting system can be used to report faults. One such approach is triple modular\nredundancy (TMR), sometimes called triple-mode redundancy. It utilizes three\ncomponents to perform the same process.\nVoting logic will compare the results to produce a single output. If all three components\nproduce the same output, then everything worked as expected. If two out of three\ncomponents agree, they can correct the fault by outvoting the third component. A fault is\nthen reported on the third component.\n", "page": 123, "type": "text", "section": "Page 123"}
{"text": "Software Quality Attributes\nChapter 4\n[ 95 ]\nSanity test/sanity checking\nThis fault detection technique uses a test to evaluate if a result from a process is reasonable\nand possible. If the test determines that the result is not, it can report a fault. Such a test is a\nsimple one, and not intended to test for a wide variety of issues. It is used to detect obvious\nfaults in a quick way.\nCondition monitoring\nCondition monitoring is a method whereby conditions are checked in a software system in\norder to detect a fault or a situation in which a fault may develop. When a problem is\ndetected, the fault can be handled.\nOne benefit of condition monitoring is that it may detect a problem before a fault occurs, or\nat least before it becomes an even bigger issue.\nSelf-tests\nSoftware systems can incorporate self-tests, allowing their components the ability to detect\nfaults. Such a test is sometimes referred to as a built-in self-test (BIST) or a built-in test\n(BIT). If this technique is used, software components or combinations of components are \ndeveloped with the logic that gives them the ability to test themselves for correct operation.\nSelf-tests can be initiated by the component they are testing or by a separate component\nacting as a system monitor. If a test fails, the fault is reported.\nRecovering from faults\nOnce faults are detected, a strategy can be undertaken to recover from them. The following\nare some ways to recover from faults.\nException handling\nOnce an exception is detected, the software system can utilize an exception handling\nmechanism. How an exception is handled is dependent on the language, framework, and\ntype of exception, but it can involve anything from simply returning an error code to\nreturning an instance of some error class that contains helpful information about the\nexception, such as an error code, a message, and a stack trace.\n", "page": 124, "type": "text", "section": "Page 124"}
{"text": "Software Quality Attributes\nChapter 4\n[ 96 ]\nThe software can use exception information to potentially recover from the fault, such as\ncorrecting the cause of the exception and retrying the operation, and/or showing the user a\nuser-friendly message about the problem.\nRetry strategy\nTransient faults are errors that occur due to some temporary conditions, such as network\nconnectivity issues, temporary unavailability of a service, timeout of a service, or\ninfrastructure-level faults.\nRetry strategies can be used to attempt to retry an operation when it encounters a transient\nfault. A retry policy can be tailored based on the nature of the component that caused the\nerror, which can dictate things such as how many retry attempts are made, and the length\nof time to wait in between attempts.\nSome common retry interval types include:\nRegular intervals: The software system waits the same amount of time in\nbetween each interval attempt.\nIncremental intervals: The software system waits a short amount of time before\nthe first retry, and then incrementally increases the amount of time before each\nsubsequent retry. For example, retry attempts may occur at 2 seconds, 6 seconds,\n12 seconds, and so on.\nExponential backoff: The software system waits a short amount of time before\nthe first retry, and then exponentially increases the amount of time before each\nsubsequent retry.\nImmediate retry: A retry can take place immediately. However, there should not\nbe multiple immediate retry attempts. If a single immediate retry attempt fails,\nthen any subsequent attempts should use one of the other interval types.\nRandomization: Any of the aforementioned interval types can be used in\nconjunction with randomization to prevent retry attempts from being sent at the\nsame time from multiple instances of a client.\nVarying levels of redundancy\nOne way to recover from faults and achieve availability is to have a failover mechanism.\nThere are varying levels of redundancy that can be provided.\n", "page": 125, "type": "text", "section": "Page 125"}
{"text": "Software Quality Attributes\nChapter 4\n[ 97 ]\nIn active/hot spare environments, each component has another one that performs the same\nprocesses with the same inputs, so that, if one fails, the other component can take over at\nany time. Failover is generally transparent because the recovery time is nearly\ninstantaneous.\nA passive/warm spare environment is one in which only active components perform\nprocesses from inputs, but the active components provide the backup components with\nperiodic state updates. A passive redundancy approach is not as highly available as an\nactive/hot spare, but it is less expensive to operate. Depending on how frequent state updates\nare provided to the backup, recovery time can be in terms of seconds or minutes.\nWith a cold spare approach, redundant components are kept out of service until they are\nneeded. The component that failed will be out of operation until it can be repaired or\nreplaced. Using a cold spare takes more time to bring the redundant component into\noperation as compared with hot or warm spares. Recovery time could be a few hours.\nRollback\nThe rollback technique returns the system to a checkpoint in which the system is known to\nhave been in a good state. This does require that checkpoints be persisted in some manner.\nOnce the system is rolled back, regular operations can continue once again. This approach\ncan be used with active or passive redundancy so that a failed component can be made\nactive again after a rollback.\nGraceful degradation\nGraceful degradation is a fault recovery approach in which some functionality is made\nunavailable in order to prevent the entire system from becoming unusable. If failures are\npreventing the entire software system from being operational, then some functions can be\ndropped in favor of others. For example, if a system is running low on resources, the most\ncritical functions can be kept working while others are shut down.\nIgnoring faulty behavior\nAnother approach to fault handling is to simply ignore the fault. If a particular type of fault\nfrom a certain source is known to be one that can be ignored, then the system may simply\ndisregard the fault and processing can continue.\n", "page": 126, "type": "text", "section": "Page 126"}
{"text": "Software Quality Attributes\nChapter 4\n[ 98 ]\nPreventing faults\nAn alternative to detecting and recovering from faults is to prevent them from occurring in\nthe first place. There are a number of strategies that can be employed to prevent faults.\nRemoval from service\nOne way to prevent faults is to remove the offending component from service. A software\ncomponent can be removed from operation in anticipation of faults. It can then be restored\nwhen it is operational again.\nTransactions\nTransactions can be used to prevent faults. Multiple steps in a process can be bundled\ntogether in a transaction, so that if one step fails then the entire bundle can be undone. This\napproach can prevent data from being saved in an incorrect state, or prevent race\nconditions when more than one process attempts to access or change the same data.\nIncreasing competence sets\nThe competence set for a given component in a software system determines what states and\nconditions it can handle. A component's level of fault tolerance and the cases that it is\ndesigned to handle depend on the collection of logic that make up its competence set. A\ncomponent can be modified so that it handles more cases, reducing the exceptions that are\nthrown.\nFor example, if a method does not handle a null value for a particular parameter, an\nexception may be thrown, leading to a system failure. The code could be modified to\nhandle null values, increasing the competence set of the software and preventing a fault.\nException prevention\nThis strategy involves writing the code in such a way that exceptions can be prevented. For\nexample, methods could perform bounds checking on arguments and handle them\ngracefully, which would prevent exceptions from being thrown.\n", "page": 127, "type": "text", "section": "Page 127"}
{"text": "Software Quality Attributes\nChapter 4\n[ 99 ]\nPortability\nPortability describes how efficiently and effectively a software system can be transferred\nfrom one environment to another. Some factors that influence portability are adaptability,\ninstallability, and replaceability.\nAdaptability\nAdaptability is the degree to which a software system can be adapted for different\nenvironments, such as different hardware, operating systems, or other operational\ncharacteristics.\nIn order to test for adaptability, functional testing must be conducted to ensure that the\nsoftware system can perform all of its tasks in all target environments.\nInstallability\nInstallability is the ease with which a software system can be installed or uninstalled in a\nspecified environment. The installation process should be easy to understand, and may also\nprovide configuration options as part of the installation. For example, the process may\nprompt the user to configure things such as the location for the installation, database\nconnection information, and other configuration options for the software.\nAnother aspect of installability is how the software handles an update/upgrade process.\nThe software system should provide a friendly and usable update process so that a user can\nupgrade the software to a newer version. The update process should clean up the older\nversion, possibly by automatically uninstalling it first. \nInstallability also encompasses any functionality to uninstall the application. When a\nsoftware system is uninstalled, it should remove the software system and any related\ncomponents entirely from the machine. It may also delete any folders that are no longer\nneeded.\nA feature you may consider including that increases installability is providing users with\nthe ability to cancel an install, update, or uninstall process once it has been initiated.\nIdeally, the process will clean-up after itself properly if the user aborts it or if it fails.\n", "page": 128, "type": "text", "section": "Page 128"}
{"text": "Software Quality Attributes\nChapter 4\n[ 100 ]\nInstallability testing should ensure that all of the functionality related to installing,\nupdating, and uninstalling the software works properly and without error. It should test\nthat any configuration options work properly, and check how the process handles\nsituations such as when there is an insufficient amount of disk space available to install the\nsoftware or an update.\nReplaceability\nReplaceability is the capability of a software system to replace another software system for\nthe same purpose, in the same environment. This may entail only replacing one or more\nsoftware components within a software system.\nA good example of replaceability is when a software system is upgraded. Customers desire\na system that is highly replaceable so that an upgrade to a newer version of the software\nfrom an older version goes smoothly.\nTesting for replaceability should confirm that, after any replacement, the software system\nstill works properly. All relevant functionality should be tested to verify that it works as\nexpected.\nInternationalization and localization\nInternationalization and localization are part of portability and consist of adapting a\nsoftware system for use with different languages, taking into consideration cultural\ndifferences, and meeting other requirements for various locales.\nAs a software architect, you will need to know if there are internationalization/localization\nrequirements, so that you may consider them in the design. Even if there are no\nrequirements currently, if your organization has business goals for international business\nexpansion, the software may eventually be used in different areas around the world. If that\nis the case, you will want to consider that fact in your initial design, as it is more difficult to\nprovide this capability later.\nInternationalization is designing software in such a way as to enable localization and\nsupport for different languages, cultures, and regions. Internationalization in this context is\noften abbreviated to i18n, where 18 is the number of letters between i and n in the word\ninternationalization. Sometimes the word globalization is used instead of\ninternationalization. When that is the case, it is sometimes abbreviated as g11n.\n", "page": 129, "type": "text", "section": "Page 129"}
{"text": "Software Quality Attributes\nChapter 4\n[ 101 ]\nAs part of internationalization, the software should be designed so that adapting it to\ndifferent locales later will not require modifications to the code. For example, all\ntranslatable strings, such as captions, messages, tooltips, and other content, should be\nplaced in resource files. The code can then reference those resource files when it needs that\ncontent. Later, while localizing the application, these strings can be translated for different\nlocales without having to change any code.\nUnicode should be used as the character set for applications that require\ninternationalization, as it supports all of the characters from any written language\nworldwide. If a development team were to use different character sets for different\nlanguages, localizing an application would be much more difficult. Unicode will facilitate\ndisplaying characters from any language that needs to be supported, and because there is a\nunique encoding for each character, data can be sorted, searched, and manipulated in the\nsame way.\nOnce an application is ready to be localized, the following are some things which need to be\nconsidered:\nTranslating captions, messages, tooltips, and other content for another language\nEnsuring the user interface can accommodate words properly after they are\ntranslated (for example, spacing and the wrapping of words could be different)\nVariation in the spelling of words, even for countries which use the same\nlanguage, such as aeroplane/airplane, capitalise/capitalize, and\norganisations/organizations\nLanguage and cultural differences may require content to be rewritten to ensure\nthat nothing will be misunderstood or taken the wrong way\nWhether text is read from left to right or right to left\nDifferences in telephone numbers\nDifferences in weights and measures\nDifferent date/time formats, such as whether 6/1/2019 should be interpreted as\nJune 1st or January 6th\nTime zone differences (UTC)\nFormatting numbers, such as decimal mark symbols, digit group separators, and\ndigit groupings\nDifferent currencies, such as currency symbols and whether the currency symbol\nshould appear before or after the monetary value\nAny legal requirements\n", "page": 130, "type": "text", "section": "Page 130"}
{"text": "Software Quality Attributes\nChapter 4\n[ 102 ]\nMaintaining portability\nOnce a software system is made portable for different environments, it is important that\nportability is maintained. As changes are made to the software as part of maintenance, it is\npossible those modifications could affect portability.\nWhile making modifications, consideration must be given as to whether or not it will affect\nportability. During testing, it must be verified that any changes made did not adversely\naffect any requirements for portability.\nInteroperability\nInteroperability is the degree to which a software system can exchange and use\ninformation from another software system. In order for two systems to interoperate in a\nuseful manner, they must be capable of communicating with each other (syntactic\ninteroperability) as well as be able to interpret the information exchanged in a meaningful\nand correct way (semantic interoperability).\nThe term interoperability is related to, and sometimes used interchangeably with, the term\nintegration.  \nChallenges with interoperability\nThere are a number of difficulties in providing interoperability between existing or new\nsoftware systems. When dealing with requirements related to interoperability, keep in\nmind the following potential pitfalls:\nEven when a software system follows a particular standard when developing\ninteroperability with another software system, the desired level of\ninteroperability may not be met due to the two systems interpreting the\nstandards specifications in different ways.\nInteroperability between two software systems is sometimes scaled back or put\non hold because of the involvement of an existing legacy system, either directly\nor indirectly, when there is a desire to maintain compatibility with the legacy\nsystem. An organization may not want to make modifications to the legacy\nsystem.\n", "page": 131, "type": "text", "section": "Page 131"}
{"text": "Software Quality Attributes\nChapter 4\n[ 103 ]\nTests for interoperability are sometimes insufficient and may miss certain\ninteroperability issues.\nSoftware systems are sometimes released even when there are known\ninteroperability issues.\nEven if interoperability is achieved between two software systems, it can be\ndifficult to maintain as new versions of the two software systems get released. It\nis not uncommon for interoperability to be broken, requiring ongoing\nmaintenance to maintain it.\nThere can be legal issues for interoperability, so be aware of any that may affect\nyour system when interoperating with another one. This may be particularly true\nif the two software systems are owned by different organizations, and even more\nso if they operate in different jurisdictions, such as two different countries. One\nexample of this may be privacy and security laws related to healthcare\ninformation being exchanged.\nLocating and exchanging information with\nanother system\nTo achieve interoperability between two software systems, the consumer system must be\nable to locate the other and the two systems must be able to exchange information in a\nsemantically meaningful way.\nThe consumer system may or may not need to discover the other system at runtime. If it\ndoes need to happen at runtime, a known directory service is searched by one or more\nattributes (for example, a URL) in order to locate the system. It should be noted that there\ncan be n levels of indirection during the location process. For example, once one location is\nfound, it may lead to another location, and so on, until the service is discovered.\nOnce the system is located, the response must be handled. The service will send back a\nresponse to the requester, forward a response on to another system, or broadcast a response\nto any interested services that may be listening.\nIn order for the two systems to exchange information, interoperability tactics, such as\norchestration and the management of interfaces, are used. Orchestration involves directing\nand managing the services that are called and ensuring that the necessary steps take place\nin the correct sequence.\n", "page": 132, "type": "text", "section": "Page 132"}
{"text": "Software Quality Attributes\nChapter 4\n[ 104 ]\nIn order to manage interfaces to facilitate information exchange, capabilities may be added\nor removed for the purposes of interoperability. A capability may be added to an interface\nspecifically to enable data exchange. An example of this may be functionality related to the\nbuffering of data. In addition, capabilities may be removed from an interface if we do not\nwant those capabilities to be exposed to other systems. For example, functionality to delete\ncertain data may be removed from the interface if we do not want any external clients to\nhave that capability.\nInteroperability standards\nWhen interoperability involves software systems from different organizations, one of the\nways to achieve interoperability is to follow a common standard. The standard may be\ncreated by the two organizations working together as part of a partnership, or it may be an\nexisting industry, national, international, or open standard.\nIn some cases, an entire industry will work together to come up with a standard. The\nstandard may be one that is newly created by the industry, or some existing standard might\nbe used as a starting point. The interoperability standard that is agreed upon can then be\nfollowed during the course of development and used during testing.\nAnother aspect of standards that may facilitate interoperability is using a common\ntechnology, such as agreeing on the data interchange format and the communication\nprotocol. For example, it may be agreed upon that JavaScript Object Notation (JSON) is\nused as the data interchange format and that Hypertext Transfer Protocol Secure (HTTPS)\nis used as the web transfer protocol.\nInteroperability testing\nIn addition to unit and system testing the two systems that must interoperate, it is critical to\nperform integration testing between the two systems. Tests must be conducted to ensure\nthat the consumer system can locate the other system and that they can exchange\ninformation properly.\nThe testing environment should be as close to what will be experienced in production as\npossible. Keep in mind that, even if both systems follow a particular standard for\ninteroperability, and even if both systems have passed their own conformance tests based\non those standards, it is not uncommon for there to be issues with interoperability. Some\nissues may not be uncovered until both systems are tested together.\n", "page": 133, "type": "text", "section": "Page 133"}
{"text": "Software Quality Attributes\nChapter 4\n[ 105 ]\nTestability\nTestability is the degree to which a software system supports testing in its given context.\nThe higher the level of testability, the easier the software system and its components are to\ntest. If a component is not easy to test, it may indicate that the design is not ideal, leading to\nan implementation that is unnecessarily complex. A significant part of the development\ncosts for software systems is related to testing, so if the software architecture can play a role\nin testability there can be noticeable cost benefits.  \nA software system that has a higher level of testability makes it easier to test, which results\nin an increase in both the efficiency as well as the effectiveness of testing. Testing efficiency\nincreases because it takes less time and effort to create and execute tests. Testing\neffectiveness is increased because you are more likely to find existing defects in the\nsoftware system, and you are more likely to find them sooner.\nFinding defects sooner has a tremendous effect on the software system's overall level of\nquality. Not only is it more likely that defects won't make it into production, but the sooner\na defect is found, the less costly it is to repair.\nSome factors that influence testability are as follows:\nControllability\nObservability\nIsolability\nAutomatability\nComplexity of the software\nControllability\nControllability represents the level to which it is possible to control the state of the\ncomponent being tested. The component being tested is sometimes referred to as the\nsystem under test (SUT) or the component under test (CUT). Controlling the state of a\ncomponent involves being able to dictate the inputs of the component as well as the level to\nwhich those inputs exercise its capabilities.\nWhen designing components, you should seek to increase their controllability, as this\nincreases testability.\n", "page": 134, "type": "text", "section": "Page 134"}
{"text": "Software Quality Attributes\nChapter 4\n[ 106 ]\nObservability\nObservability represents the level to which it is possible to observe the state of the\ncomponent being tested. This includes being able to observe inputs and outputs so that we\ncan determine if the component is working correctly.\nIf it is not possible for the testing framework being used to see the inputs and outputs of a\ncomponent, then the component is not observable, and it won't be possible to confirm if the\nresults are correct.\nWhen designing components, you should seek to increase their observability, as it is\ndirectly correlated to testability.\nIsolability\nIsolability is the degree to which a component can be isolated. The goal is to have tests that\ncan focus on specific pieces of functionality (for example, unit tests) that do not have\ndependencies on other components. This allows us to create and execute tests on a\ncompleted component, even if other components are not complete.\nWe want to avoid a situation where we have to write a great deal of code before any of it\ncan be tested since it is desirable to get feedback as quickly as possible. If a problem is\nfound, it will be easier to determine its source and cause if components exhibit isolability.\nAn increase in isolability increases the testability of a component.\nAutomatability\nAutomatability is the level to which a process or action can be automated. If a system\nexhibits automatability, then automated tests can be created and executed for the system.\nAutomated testing utilizes pre-scripted tests that can then be executed automatically. These\ntests can be run at any time, such as before a code check-in or prior to a build taking place.\nIf automated builds are in place, automated tests can be executed as part of that process.\nAutomated tests give you quick feedback on whether a new defect was introduced into the\nsystem. As we just noted, discovering a defect as close to when it was introduced is highly\nbeneficial. When designing a software system, you should seek to increase automatability,\nas the ability to have automated tests increases testability. Automated builds and\nautomated testing will be covered further in Chapter 13, DevOps and Software Architecture.\n", "page": 135, "type": "text", "section": "Page 135"}
{"text": "Software Quality Attributes\nChapter 4\n[ 107 ]\nComplexity of the software\nAs is the case with other quality attributes, the complexity of the software plays a role in\ntestability. Reducing dependencies and isolating modules (isolability), decreases the\ncomplexity of the software.\nWhen we discussed maintainability, we mentioned that minimizing the complexity of\narchitectural elements can be accomplished by reducing the number of lines of code in\nmodules, increasing cohesion and reducing coupling. Techniques that increase\nmaintainability also increase the controllability of elements. This has a direct and positive\nimpact on testability.\nIn addition to the structural complexity of components, there is behavioral complexity. Non-\ndeterminism is one type of behavioral complexity. If some algorithm we are testing is non-\ndeterministic, then it means that given the same inputs, it can exhibit different behavior\nwith each execution. This is in contrast with a deterministic algorithm, which, if given the\nsame inputs, will behave in the same way each time it is executed.\nNon-deterministic code is more difficult to test, so the first step would be to identify any\nareas in the software system that are non-deterministic. For such areas, the ideal approach,\nif possible, would be to refactor the logic to make it deterministic, or allow the logic to be\nmocked as deterministic.\nA simple example often seen in practice involves tests and the current time. In most\nprogramming languages, there is a way to get the current date and time of day. Doing so\ncan tightly couple your code to the host environment. It is non-deterministic because each\ntime we make a call to get the current date and time, the result will be different. If there is\nlogic that uses this value in a way that would affect a unit test, your test results will vary\ndue to the non-deterministic nature of the result. This logic can be refactored to wrap the\ncall to get the current date and time into some other class. Doing so will allow you to inject\nthis dependency into the class that needs this logic. At that point, a testing framework can\nmock the dependency, specifying what the result will be when the call to get the current\ndate and time is made.\nHowever, there are cases where it will not be possible to refactor logic to make it\ndeterministic, or to make it so that the logic can be mocked as deterministic. An example of\nthis is if you had a multi-threaded system that is required to interact with an external\ncomponent, and that external component raises events in a non-deterministic manner.\n", "page": 136, "type": "text", "section": "Page 136"}
{"text": "Software Quality Attributes\nChapter 4\n[ 108 ]\nImportance of test documentation\nAn aspect of testability is the ease with which tests can be executed, including testers who\ndid not originally write the test cases. The reusability and maintainability of tests, part of\ntestability, are improved with the existence of quality test documentation. When dealing\nwith a large system, it can be difficult to remember all of the business rules and alternative\npaths related to a particular scenario.\nWith agile software development methodologies, excessive documentation is viewed as a\nproject risk rather than something that will reduce risk. Project teams aim to be efficient\nwith documentation and produce only as much as is needed. In addition, automated tests\nshould be fairly self-documenting through things such as the test method names, test class\nnames, and comments.\nHowever, there are many different types of testing beyond automated tests that need to be\nconducted, and some can be quite involved. There can be complexity in the business logic,\nin the inputs and outputs of tests, in the data that is needed to set up particular tests, and to\nensure adequate coverage on a variety of business scenarios. Some form of test\ndocumentation can really be of benefit.\nResources can, and more than likely will, change over time. An employee may leave an\norganization, or testers simply get different assignments over time and a piece of\nfunctionality isn't necessarily owned by a single tester. Testing documentation allows\ndifferent testers to use the test case over time and it facilitates such transitions so that\nquality is not sacrificed.\nTo add to that point, it is increasingly common for development departments to outsource\ndevelopment work, including testing. Not only may you have to deal with outsourced\nresources changing, but testing tasks may need to move from internal resources to\noutsourced resources, and vice versa.\nDocumentation of artifacts such as designs and tests is very useful so that new resources\ncan get up to speed quickly. It's basically the concept of tribal knowledge and being able to\npass on things that have been previously learned to others. Projects with good\ndocumentation typically demonstrate a higher level of organizational maturity and\ncontribute to the project's overall success.\n", "page": 137, "type": "text", "section": "Page 137"}
{"text": "Software Quality Attributes\nChapter 4\n[ 109 ]\nWhat makes a good tester?\nWhile we are on the topic of testability, I wanted to touch upon the characteristics of a good\ntester. As a software architect, you may be providing guidance to testers. Good testers\nperform their tasks with a greater level of efficiency, effectiveness, and thoroughness.\nTesters that operate efficiently do so in an organized way and their efficiency allows them\nto complete test cases faster, and find defects quicker. They are not just using the software\nand happening upon bugs, but using a systematic approach to finding defects.\nTesters who achieve a greater level of effectiveness do so because they focus particular\nattention on problems that users will care about in the released version. Part of the process\nof finding defects involves documenting the problem and giving them to developers so that\nthey can be fixed.\nExceptional testers are thorough in their work. They plan their testing activities and they\ndocument their test cases. Testers attempt to be as thorough as possible, exercising all\naspects of a software's functionality and considering a variety of scenarios. They use a\nvariety of inputs and test things such as edge cases, which are scenarios that involve\nextreme input values, such as those at the minimum or maximum of a range of possibilities.\nAnother characteristic of a good tester is the ability to understand the software's behavior,\nenvironments, and capabilities. Because a tester cannot test all possible inputs and\nscenarios, an effective software tester must understand the software's behavior. This\nincludes what the software is doing, and what things could potentially cause it to fail.\nSoftware operates in an environment that interacts with a variety of inputs and outputs.\nThere is the user interface (UI) that people interact with, but there is also the kernel\ninterface (operating system), software interfaces (for example, in things such as database\nsystems), and file system interfaces (for example, errors related to accessing, reading, and\nwriting to files). A good tester must consider the totality of the environment in which the\nsoftware operates.\nFinally, the tester must understand the software's capabilities. Although there may be a\nlimited number of basic capabilities (for example, accepting input, producing output,\nstoring data, and executing calculations), these capabilities can be combined into complex\nfunctionality. A suitable tester should consider all of these capabilities in order to maximize\nthe probability that any defects that exist will be found.\n", "page": 138, "type": "text", "section": "Page 138"}
{"text": "Software Quality Attributes\nChapter 4\n[ 110 ]\nSummary\nSoftware architects should pay particular attention to quality attributes, as they influence\nthe architecture of software. The software must meet the designated quality attributes, so\nidentifying and specifying them in a way that they can be measured and tested is\nimportant.\nAlthough some stakeholders might focus on functionality, non-functional requirements\nsuch as quality attributes are a principal factor in whether or not a software system will be a\nsuccess. Some of the more important quality attributes include maintainability, usability,\navailability, portability, interoperability, and testability.\nNow that we understand more about quality attributes and the fact that they influence\nsoftware architecture, we are ready to explore software architecture design. Architecture\ndesign involves making decisions to create a solution for functional requirements, quality\nattributes, and constraints. You will learn what is involved with architecture design, about\ndesign principles that you can leverage in your designs, and about architecture design\nprocesses.\n", "page": 139, "type": "text", "section": "Page 139"}
{"text": "5\nDesigning Software\nArchitectures\nSoftware architecture design is a key step in building successful software systems, and this\nchapter begins by exploring what software architecture design is, and why it is important\nin a software project.\nThere are two main approaches to architecture design: the top-down and bottom-up\napproaches. Each approach has advantages and disadvantages, and in this chapter you will\nlearn how to select the best approach for a given project. Designing a software and\narchitecture can be challenging, but we will take a look at design principles and existing\nsolutions that can be leveraged in a design.\nArchitecture design processes provide guidance to software architects to ensure that a\ndesign satisfies requirements, quality attribute scenarios, and constraints. This chapter\ncovers the activities that are typically performed as part of an architecture design process\nand then provides an overview of four processes: attribute-driven design (ADD),\nMicrosoft's technique for architecture and design, the architecture-centric design method\n(ACDM), and the architecture development method (ADM).\nThe chapter will conclude by explaining how to use an architecture backlog to prioritize\nwork and track the progress of architecture designs.\nIn this chapter, we will cover the following topics:\nSoftware architecture design\nThe importance of software architecture design\nTop-down versus bottom-up design approaches\nGreenfield versus brownfield software systems\nArchitectural drivers\nLeveraging design principles and existing solutions\nDocumenting the software architecture design\n", "page": 140, "type": "text", "section": "Page 140"}
{"text": "Designing Software Architectures\nChapter 5\n[ 112 ]\nUsing a systematic approach to software architecture design\nAttribute-driven design (ADD)\nMicrosoft's technique for architecture and design\nArchitecture-centric design method (ACDM)\nArchitecture development method (ADM)\nTracking the progress of the software architecture's design\nSoftware architecture design\nSoftware architecture design involves making decisions in order to satisfy functional\nrequirements, quality attributes, and constraints. It is a problem-solving process that leads\nto the creation of an architecture design.\nSoftware architecture design comprises defining the structures that will make up the\nsolution and documenting them. The structures of a software system are made up of\nelements, and the relationships between the elements. The properties and behaviors of the\nelements that are publicly exposed, through an interface, should be identified as part of the\ndesign. The design allows you to understand how the elements behave and interact with\neach other. Private implementations of the elements are not architecturally significant and\nneed not be considered as part of the design.\nThe software architecture design serves as technical guidance for development and\ntypically occurs iteratively until the initial architecture is at a point where the development\nteam can begin their work. Once an initial architecture is designed, it can continue to evolve\nas development is taking place. For example, additional design iterations may occur to\nrefactor an architecture to fulfill new requirements or quality attributes.\nSoftware architecture design is a creative process. Software architects have the privilege of\ncoming up with solutions to complex problems and can use creativity to do it. It can be one\nof the most fun and rewarding parts of a software project.\nMaking design decisions\nThe set of software requirements consists of a series of design issues that must be solved.\nFor each of these design issues, such as providing certain business functionality, respecting\na particular constraint, meeting performance objectives, or providing a certain level of\navailability, there may be numerous ways to solve the problem. You will need to consider\nthe strengths and weaknesses of these alternatives in order to select the most appropriate\nchoice.\n", "page": 141, "type": "text", "section": "Page 141"}
{"text": "Designing Software Architectures\nChapter 5\n[ 113 ]\nA large part of software architecture design is making design decisions to resolve issues so\nthat a solution can be implemented. As the software architect, you will be leading the\ndecision-making process.\nIt is a collaborative process, and usually the best designs incorporate knowledge and\nfeedback from multiple people, such as other software architects and experienced\ndevelopers. Joint designs and reviewing the architecture with others is beneficial in coming\nup with a solid software architecture design.\nThe result of the design is a set of decisions that shape your software architecture. The\ndesign is documented in artifacts that can be used for the implementation of a solution.\nSoftware architects should keep in mind that a decision that is made for one design issue\nmay affect another one. This is why software architecture design is an iterative process.\nEach decision for a design issue may not be optimal for another issue, but the overall\nsolution must be acceptable by satisfying all of the requirements.\nPerfect is the enemy of good, an aphorism that has its origins in the thoughts of the French\nphilosopher, Voltaire, and others, is applicable to software architecture design. A\ncompleted design may not be perfect, as there will be conflicting requirements that need to\nbe met, and trade-offs made in order to meet them. If the design satisfies all of the\nrequirements, then it is a good one, even if it is not perfect.\nSoftware architecture design terms\nBefore we go any further, let's define some of the terms that we will be using while\ndetailing the process of software architecture design. These terms can vary, depending on\nthe organization and the team. Regardless of the terms used, the important thing is that\nthey are used consistently by the team members and that they are understood by all team\nmembers.\nFor the purposes of this book, we'll be using the terms structure, element, system,\nsubsystem, module, and component.\nStructure\nStructures are groupings of, and relations between, elements. Anything that is complex and\nmade up of elements can be referred to as a structure. We previously defined software\narchitecture, in part, by saying it is made up of the structures, their elements, and the\nrelationships of those elements with each other.\n", "page": 142, "type": "text", "section": "Page 142"}
{"text": "Designing Software Architectures\nChapter 5\n[ 114 ]\nElement\nAn element is a generic term that can be used to represent any of the following terms:\nsystem, subsystem, module, or component. If we want to refer to pieces of a software\napplication in a general way, we can refer to them as elements.\nSystem\nThe software system represents the entire software project, including all of its subsystems.\nA system consists of one or more subsystems. It is the highest level of abstraction in a\nsoftware architecture design.\nSubsystem\nSubsystems are logical groupings of elements that make up a larger system. The\nsubsystems can be created in a variety of ways, including partitioning a system by\nfunctionality.\nAlthough they do not have to be, subsystems can represent standalone software\napplications. An overall software system may be composed of multiple subsystems, and\nany number of them might be a standalone application. These standalone applications can\nbe external applications that were not developed by the organization.\nOrganizing a larger software system into subsystems lowers complexity, and allows for\nsoftware development to be better managed. In some cases, one or more development\nteams may be formed for each subsystem. Each subsystem is made up of one or more\nmodules.\nModule\nModules, like subsystems, are logical groupings of elements. Each module is contained\nwithin a subsystem and consists of other modules and/or components. They are typically\nfocused on a single logical area of responsibility.\nDevelopment teams assigned to a particular subsystem will be responsible for the modules\nthat make up that subsystem.\n", "page": 143, "type": "text", "section": "Page 143"}
{"text": "Designing Software Architectures\nChapter 5\n[ 115 ]\nComponent\nComponents are execution units that represent some well-defined functionality. They\ntypically encapsulate their implementation and expose their properties and behaviors\nthrough an interface.\nComponents are the smallest level of abstraction and typically have a relatively small\nscope. Components can be grouped together to form more complex elements, such as\nmodules.\nThe importance of software architecture\ndesign\nA software architecture is the foundation of a software system. The design of the\narchitecture is significant to the quality and long-term success of the software. A proper\ndesign determines whether the requirements and quality attributes can be satisfied.\nThere are a number of reasons why a good software architecture design is critical to\nbuilding useful software. In this section, we will explore the following reasons:\nSoftware architecture design is when key decisions are made regarding the\narchitecture.\nAvoiding design decisions can incur technical debt.\nA software architecture design communicates the architecture to others.\nThe design provides guidance to the developers.\nThe impact of the software architecture design is not limited to technical\nconcerns. It also influences the non-technical parts of the project.\nMaking key decisions\nIt is during software architecture design that key decisions are made that will determine\nwhether requirements, including quality attributes, can be satisfied. Software architecture\nenables or inhibits quality attributes, so design decisions play a large role in whether or not\nthey can be met.\nSome of the earliest decisions are made during design. If these decisions need to change, it\nis easier and less costly to change architectural decisions early, before coding has even\nbegun, than to make changes later.\n", "page": 144, "type": "text", "section": "Page 144"}
{"text": "Designing Software Architectures\nChapter 5\n[ 116 ]\nAvoiding design decisions can incur technical\ndebt\nCritical decisions are made during the design, and for that reason, there is a cost to either\ndelaying a design decision or not making one at all. Delaying or avoiding certain design\ndecisions can incur technical debt.\nTechnical debt is similar to financial debt. In the context of design, it is the cost and effort\nfor the additional work that will be necessary later due to decisions that are made now, or\nbecause decisions have not been made.\nIn addition to delaying or avoiding decisions, a decision may be made knowing that it will\ncost some amount of technical debt. As a software architect, you may decide to take an\neasier route to a solution, incurring technical debt, even though there is a better solution. As\nis the case with financial debt, technical debt is not always a bad thing. Sometimes, you will\nwant to pay a debt later in order to get something now. For example, designing a better\nlong-term solution may take more time and effort, and you may decide on a solution that\ntakes less time in order to get the software in production to take advantage of a market\nopportunity.\nIt can be difficult to measure the impact of technical debt accurately. Keep in mind that in\naddition to the time and effort that might be required later to make up for a decision that is\nmade or avoided now, technical debt can have other negative repercussions. For example, a\ndesign that is not optimal, leading to lower levels of modifiability and extensibility, can\nhinder the team's ability to deliver other functionality. This is an additional cost that should\nbe added to the technical debt.\nThe software architect needs to take all of these factors into consideration when deciding\nwhether or not to incur a technical debt.\nCommunicating the architecture to others\nThe results of the architecture design allows you to communicate the software architecture\nto others. There will be a variety of people who will potentially be interested in the design\nof the architecture.\nThe design will also improve cost and effort estimates since it influences what tasks will be\nrequired for implementation. Understanding the nature of the work that lies ahead and\nwhat types of tasks will be needed to complete the project will assist project managers with\ntheir planning. Being able to estimate cost, effort, and the quality attributes that will be met\nis also useful for project proposals.\n", "page": 145, "type": "text", "section": "Page 145"}
{"text": "Designing Software Architectures\nChapter 5\n[ 117 ]\nProviding guidance to developers\nA software architecture design provides guidance to the development team, by steering\nimplementation choices as well as providing training on the technical details of the project.\nThe design imposes implementation constraints, making it important for coding tasks.\nKnowing the software architecture design helps developers be aware of the implementation\nchoices available to them, and minimizes the possibility of making an incorrect\nimplementation decision.\nIt can also be used as training for developers. At the start of the project, the development\nteam will need to understand the design decisions that have been made, and the structures\nthat have been designed. Creating detailed designs for components and implementing\nthem requires an understanding of the architecture design. If new developers join the team\nlater, they can also use the architecture design as part of their onboarding.\nInfluencing non-technical parts of the project\nAnother reason that software architecture design is important is the fact that design\ndecisions affect aspects of the software project other than the architecture. For example,\ncertain architecture design decisions could affect the purchasing of tools and licenses, the\nhiring of team members, the organization of the development environment, and how the\nsoftware will eventually be deployed.\nTop-down versus bottom-up design\napproaches\nThere are two fundamental approaches to the design of software architecture. One is a top-\ndown design approach, and the other is a bottom-up approach. These strategies apply to a\nvariety of disciplines, including software architecture design. Let's look at both of them in\nmore detail.\nTop-down approach\nA top-down approach starts with the entire system at the highest level, and then a process\nof decomposition begins to work downward toward more detail. The starting point is the\nhighest level of abstraction. As decomposition progresses, the design becomes more\ndetailed, until the component level is reached.\n", "page": 146, "type": "text", "section": "Page 146"}
{"text": "Designing Software Architectures\nChapter 5\n[ 118 ]\nWhile the detailed design and implementation details of the components are not part of the\narchitecture design, the public interfaces of the components are part of the design. It is the\npublic interfaces that allow us to reason about how components will interact with each\nother.\nA design using the top-down approach is typically performed iteratively, with increasing\nlevels of decomposition. It is particularly effective if the domain is well understood. \nThis systematic approach has been favored by enterprises since it can handle large and\ncomplex projects and because the method of design is planned. A systematic approach to\narchitecture design is attractive to enterprises because it can help with time and budget\nestimates. However, a strict top-down approach, which requires a lot of upfront\narchitecture design, has become less common in modern software architecture.\nAdvantages of the top-down approach\nThere are a number of benefits to using a top-down approach. It is a systematic approach to\ndesign and breaks the system down into smaller parts. As a system is decomposed, it lends\nitself well to the division of work. On larger projects with multiple teams, this work can be\ndivided among the teams.\nAs further decomposition takes place, tasks can be created for individual team members.\nThis supports project management in the assignment of tasks, scheduling, and budgeting.\nThis type of ability to plan is attractive to enterprises. The management teams of\norganizations may prefer, or even insist on, a top-down approach. Earlier in the book, we\ndiscussed how, as a software architect, you may be asked to assist with project estimates,\nand a top-down approach will allow you to do that with more accuracy.\nAlthough this approach works well on both small and large projects, it can be particularly\nuseful for large projects. By decomposing a system into smaller components, a large project\ncan become more manageable as the size and complexity of each component is reduced.\nDisadvantages of the top-down approach\nA strictly top-down approach runs the risk of a big design up front (BDUF), sometimes\nreferred to as a big up-front design (BUFD). Software is complex and it can be difficult to\ncreate the entire architecture up front. Design flaws or missing functionality in the\narchitecture may not be uncovered until later in the process, when components are\ndesigned or implemented. If architecture changes are required in higher levels of the\narchitecture after some work is already completed, it will be more difficult to make the\nmodifications.\n", "page": 147, "type": "text", "section": "Page 147"}
{"text": "Designing Software Architectures\nChapter 5\n[ 119 ]\nA top-down approach works best when the domain is well understood, which is not always\nthe case. Plenty of projects begin without the domain being fully understood. Even when it\nis understood, stakeholders and users can sometimes be unclear as to what the software\nshould do, and how it should work.\nIf multiple teams are working on a project, each responsible for a particular subsystem or\nmodule, knowledge sharing and reuse can be difficult with this approach. Each team can\nwork independently of the others, which has its advantages, but it does not facilitate the\nsharing of code or knowledge. The software architect may have to recognize areas of reuse,\nabstract them out, and communicate them to the teams. Another way to mitigate this issue\nis to provide opportunities and collaboration tools for teams to communicate with each\nother.\nIf you use the top-down approach, be careful not to become an ivory tower architect. If you\ndesign the higher levels of an architecture and then hand them off to developers to handle\nthe lower-level detailed design, it is easy to become disengaged. As much as your\norganization and the project permits, make an effort to stay involved with the team. If\narchitectural changes are required later, you will already be familiar with the ongoing\nimplementation, which will help you to make the correct changes.\nBottom-up approach\nIn contrast with the top-down approach, the bottom-up approach begins with the\ncomponents that are needed for the solution, and then the design works upward into\nhigher levels of abstraction. Various components can then be used together, like building\nblocks, to create other components and eventually larger structures. The process continues\nuntil all the requirements have been met.\nUnlike the top-down approach, which begins with the high-level structure, there is no up-\nfront architecture design with the bottom-up approach. The architecture emerges as more\nwork is completed. Hence, this is sometimes referred to as emergent design or emergent\narchitecture.\nThe bottom-up approach does not require that the domain be well-understood, as the team\nonly focuses on a small piece at a time. The system grows incrementally as the team learns\nmore about the problem domain as well as the solution.\n", "page": 148, "type": "text", "section": "Page 148"}
{"text": "Designing Software Architectures\nChapter 5\n[ 120 ]\nAdvantages of the bottom-up approach\nOne advantage of a bottom-up approach is the greater level of simplicity. The team only\nhas to focus on individual pieces and builds only what it needs for a particular iteration.\nThis approach works well with agile development methodologies. With an iterative\napproach that handles change, refactoring can take place to add new functionality or to\nchange existing functionality. Each iteration ends with a working version of the software\nuntil eventually the entire system is built. Agile practices, such as automated unit testing\nand continuous integration, are encouraged and can lead to higher quality software.\nA bottom-up approach avoids the possibility of a big design up front, which can lead to\noverdesigning a solution. Some in the agile community feel that a lot of design effort up\nfront is wasted time and that an emergent design, or no design up front (NDUF), would be\nmore effective.\nThe bottom-up approach allows the development team to begin coding very early in the\nprocess, which also means testing can occur earlier. This includes automated unit testing as\nwell as manual testing by team members such as QA analysts and other users. Getting\nfeedback earlier in the process allows the team to identify any necessary changes earlier.\nThis approach facilitates code reuse. As the team is focused on a limited number of\ncomponents at any given time, recognizing opportunities for reuse becomes easier.\nDisadvantages of the bottom-up approach\nA bottom-up, or emergent, approach, assumes that change is cheap. Agile methodologies\nand practices provide an approach that anticipates change and can adapt to it. However,\ndepending on the nature of the change, refactoring software architecture design can be very\ncostly.\nA bottom-up approach, with no initial architecture, can lead to lower levels of\nmaintainability. With the refactoring that may be necessary with this approach, issues can\narise. If the team is not diligent, this problem can become worse over time.\nThe entire scope of work may not be known when using this approach. This makes it more\ndifficult to plan and estimate the entire project, which may be unacceptable for enterprise\nsoftware.\n", "page": 149, "type": "text", "section": "Page 149"}
{"text": "Designing Software Architectures\nChapter 5\n[ 121 ]\nOne of the disadvantages of the top-down approach is that design flaws may not be\ndetected until later, leading to costly refactoring. However, just because there is no initial\ndesign with the bottom-up approach does not make it immune to uncovering design flaws\nlater in the project. It may not be until after the architecture emerges that certain design\nflaws become apparent.\nWhich approach should I use?\nThere are certain factors to consider when deciding whether the top-down or bottom-up\napproach is better for a software project. Software architects may find it advantageous to\nuse a top-down approach if more than one of the following is true:\nThe project is large in size\nThe project is complex\nEnterprise software is being designed for an organization\nThe team is large, or there are multiple teams that will be working on the project\nThe domain is well-understood\nIt may be more appropriate to use a bottom-up approach if more than one of the following\nis true:\nThe project is small in size\nThe project is not very complex\nEnterprise software for an organization is not being designed\nThe team is small, or there is only a single team\nThe domain is not well-understood\nTaking an extreme approach, such as doing a big upfront architecture design or no\narchitecture design at all, is typically not ideal. Although some situations will lead you to\nselect a top-down or bottom-up approach, software architects should also consider using a\ncombination of the two approaches. In this way, you may be able to realize some of the\nbenefits of both approaches, while minimizing the drawbacks.\nIn the beginning of the project, rather than starting to code immediately, a top-down\napproach will provide the opportunity to spend at least some time thinking about the\noverall structure of the design. The design of a high-level architecture provides some\nstructure that can then be leveraged for further design and development.\n", "page": 150, "type": "text", "section": "Page 150"}
{"text": "Designing Software Architectures\nChapter 5\n[ 122 ]\nA high-level architecture design can be used to define and organize the teams, and\nprovides details to project management so that they can perform resource allocation,\nscheduling, and budget planning. As a software architect, you may be asked to provide\ninput regarding these types of project management activities, and having at least a high-\nlevel architecture will assist you in such tasks.\nThose who advocate for a strictly bottom-up approach, in which the architecture emerges\nfrom implementation, tend to think that software architecture inhibits agility and the ability\nto make changes to the software. However, as was mentioned in Chapter 1, The Meaning of\nSoftware Architecture, a good software architecture actually facilitates making changes as\nwell as managing them. A good architecture allows you to understand what it would take\nto make a particular change.\nUsing a top-down approach for part of the design does not require a big upfront design.\nYou can focus on architecturally significant design issues, and once a high-level\narchitecture is established, you can employ a bottom-up approach. Components and\nmodules based on the high-level architecture can then be designed and implemented.\nThe quality of the architecture design is not solely dependent on selecting the correct\napproach. The correct design decisions must be made during the design process, as both a\ntop-down as well as a bottom-up approach can lead to poor architecture designs. A design\ncreated with a top-down approach can miss key requirements, which may lead to costly\narchitectural refactoring. A design created with a bottom-up approach may require\nsubstantial refactoring while the team figures out how the software system should be\nstructured.\nNo single approach is applicable to all situations. Each project, organization, and team is\ndifferent, so the decision of which approach to take will vary. Even with a hybrid approach,\nthe amount of upfront architecture design that is necessary will vary, so it is about\ndetermining how much design is needed. That is part of the challenge of being a software\narchitect. Good architects eventually learn how much design is appropriate for a given\nsituation.\n", "page": 151, "type": "text", "section": "Page 151"}
{"text": "Designing Software Architectures\nChapter 5\n[ 123 ]\nGreenfield versus brownfield software\nsystems\nWhen you are starting the design process, one of the first considerations is whether you are\ndesigning a greenfield or a brownfield system. The terms greenfield and brownfield are\nused in a number of disciplines. It is an analogy to a construction project, and whether it\nwill begin on greenfield land, as in land that is undeveloped, or brownfield land, referring to\nland that was previously developed but is not currently in use.\nGreenfield systems\nA greenfield software system is a completely new software application, one in which you\ncan start with a clean slate. There are no constraints based on any prior work. A greenfield\nsystem can be designed for a well-understood domain or for a novel domain.\nA well-understood domain is one that is mature, and the possibilities for innovation are\nvery limited. Examples include Windows desktop applications, standard mobile\napplications, and enterprise web applications. There will be existing frameworks, tools, and\nsample architectures for the software that you need to build. The software architectures of\nexisting applications can be used as a guide.\n It will be more likely that you are developing software for a well-understood domain, and\nthe benefit is that there will be a tremendous amount of knowledge that you can leverage\nfrom the experience of those who have built similar applications.\nA greenfield system for a novel domain is also a new software application that does not\nneed to take into consideration any prior work. The difference between a greenfield system\nfor a mature domain and one for a new domain lies in the fact that a new domain is not as\nwell understood, and requires a lot more innovation.\nUnlike a well-understood domain, you will not find as much supporting information for a\nnew domain. Rather than relying on a plethora of reference architectures or referring to a\nlarge knowledge base, you will find yourself spending time building prototypes to test out\nyour solutions.\nFor novel domains, it may be beneficial to design a throwaway prototype initially. These are\nprototypes of some piece of a software system so that you can test it out, such as getting\nfeedback from users or testing quality attributes. They will help you to gain an\nunderstanding of what will make a viable solution for a novel domain.\n", "page": 152, "type": "text", "section": "Page 152"}
{"text": "Designing Software Architectures\nChapter 5\n[ 124 ]\nThrowaway prototypes are not built for long-term use, hence the term throwaway, so\nqualities such as maintainability and reusability are not the focus of such prototypes. If you\nare using new technologies, or technologies that are not already familiar to you, a prototype\ncan be a good way to try out a solution.\nBrownfield systems\nA brownfield software system is an existing software system. If changes to an existing\nsystem require architectural changes, architecture design will be needed. Modifications\nmay be necessary for purposes such as correcting defects, implementing new functionality,\nor changing existing functionality.\nArchitectural changes may also be performed on existing software to improve it in some\nway without changing any functionality. For example, an architecture for an existing\nsoftware system might be refactored to improve a particular quality attribute. Most of the\ntime, work on brownfield systems does not involve wholesale changes in the overall\narchitecture unless major rework is required.\nOne of the crucial first steps for the software architecture design of brownfield systems is to\ngain an understanding of the existing architecture. You need to understand the overall\nstructure, the elements, and the relationships between those elements. From there, the\ndesign is not so different from a greenfield system that has been through some iterations to\nestablish an initial architecture.\nWe will explore the topic of architecture for legacy systems in Chapter 14, Architecting\nLegacy Applications.\nArchitectural drivers\nArchitectural drivers are considerations that need to be made for the software system that\nare architecturally significant. They drive and guide the design of the software architecture.\nArchitectural drivers describe what you are doing and why you are doing it. Software\narchitecture design satisfies architectural drivers.\nArchitectural drivers are inputs into the design process, and include:\nDesign objectives\nPrimary functional requirements\n", "page": 153, "type": "text", "section": "Page 153"}
{"text": "Designing Software Architectures\nChapter 5\n[ 125 ]\nQuality attribute scenarios\nConstraints\nArchitectural concerns\nDesign objectives\nDesign objectives focus on the purpose of the specific architecture design. For the\nparticular design in question, what are the reasons behind why the software is being\ndesigned?\nThe design objectives influence the design and are therefore one of the architectural drivers.\nA common design objective is to design an architecture for a solution, prior to\ndevelopment. The overall objective is to facilitate the implementation of a solution that will\nsatisfy requirements.\nThis type of design objective might be for a greenfield or a brownfield type of system. As\nwe already explored, the differences between these types of systems might lead you to\nfocus on different design objectives.\nDesigning a software architecture for development is not the only type of design objective.\nAs a software architect, you may find yourself involved with project proposals. For such\npre-sales activity, the design objective may focus on coming up with the software's\ncapabilities, the possible timeframe for delivery, a breakdown of work tasks, and the\nfeasibility of the proposed project. If this is the purpose of the design, then this type of\ninitial design will not be nearly as detailed as one that you are designing for development.\nFor the purposes of a project proposal, it will not be necessary to be as detailed as you\nwould be in preparation for development. You may be required to produce a design for a\nproject proposal in a short amount of time to meet a particular sales deadline. In addition,\nuntil the sale is complete, you will probably not be allocated the funds or time for a full-\nscale design.\nSimilarly, software architects may need to create a prototype. This may be for a project\nproposal, but it could also be to test out a new technology or framework, to create a proof\nof concept (POC) for some solution to a particular problem, or to explore how a certain\nquality attribute might be effectively met. As with project proposals, if the design objective\nis to build a prototype, the focus and scope of the software architecture design will be\ndifferent from one that is being done for development.\nIt is important to keep the design objectives in mind as an architectural driver when\nsoftware architecture design is about to begin.\n", "page": 154, "type": "text", "section": "Page 154"}
{"text": "Designing Software Architectures\nChapter 5\n[ 126 ]\nPrimary functional requirements\nAnother important type of input into the architecture design is the primary functional\nrequirements that need to be satisfied. Primary functional requirements are those that are\ncritical to the organization's business goals. In Chapter 3, Understanding the Domain, we\ndiscussed core domains, which refer to the part of the domain that makes the software\nworth writing. Some of the primary functionality will come from the core domain. It is\nwhat differentiates the organization from competitors.\nAlthough satisfying functional requirements is a goal of software architecture design, keep\nin mind that not all functionality is affected by the architecture. While some functionality\nis highly affected by the architecture, other functionality can be delivered equally as well\nwith different architectures.\nEven in cases where functionality is not influenced by the architecture directly, functional\nrequirements may be an architectural driver for other reasons. One example of this would\nbe the need to make modifications to the functionality later. Maintainability and\nmodifiability of the software are affected by the software architecture.\nQuality attribute scenarios\nQuality attributes are measurable properties of a software system. They are the ilities, such\nas maintainability, usability, testability, and interoperability. We have been stressing the\nimportance of quality attributes since they play such an important part in the success of\nsoftware systems, and because software architecture decisions will affect them. \nThis makes quality attributes one of the main architectural drivers for software architecture\ndesign. The design decisions that are made will determine what quality attributes will be\nmet. As an architectural driver, quality attributes are typically described in the context of a\nparticular scenario.\nA quality attribute scenario is a short description of how the software system should\nrespond to a particular stimulus. Scenarios make quality attributes measurable and testable.\nFor example, a quality attribute of performance or a requirement that states a particular\nfunction should be fast is not measurable or testable. An actual example of a valid quality\nattribute related to performance would be as follows: When the user selects the Login option, a\nLogin page is displayed within two seconds.\n", "page": 155, "type": "text", "section": "Page 155"}
{"text": "Designing Software Architectures\nChapter 5\n[ 127 ]\nPrioritizing quality attribute scenarios\nPrior to the start of the architecture design process, the quality attribute scenarios should be\nprioritized. It is helpful to be aware of the priority of each quality attribute scenario when\ndesigning the architecture. In addition to being able to plan accordingly, such as focusing\non higher priority quality attributes first, there may be trade-offs involved when enabling\ncertain quality attributes. Understanding the priorities will help you make better design\ndecisions regarding the quality attributes and any trade-offs that need to be made.\nQuality attribute scenarios can be prioritized by ranking them based on two criteria: their\nbusiness importance and the technical risk associated with the scenario. A ranking scale of\nHigh (H), Medium (M), and Low (L) can be used. \nStakeholders can help to provide the ranking based on business importance, while the\nsoftware architect typically provides the ranking based on technical risk. Once the rankings\nare complete, each quality attribute should have a combination of the two rankings.\nIf each quality attribute scenario were assigned a unique number, they could be placed in a\ntable such as the following:\nBusiness importance/technical risk\nL\nM\nH\nL\n6, 21\n7, 13\n15\nM\n3, 10, 11\n14, 16, 17\n1, 5\nH\n4, 18, 19, 20\n2, 12\n8, 9\nQuality attribute scenarios located toward the bottom-right side of the table will be of\nhigher importance. The most important ones will be those with an H, H ranking, indicating\nthey were ranked high on both criteria. Initial design iterations can focus on those scenarios\nfirst. Subsequent iterations can consider the most important quality attribute scenarios that\nremain, such as H, M and M, H, until all of the quality attribute scenarios have been\nconsidered.\nConstraints\nConstraints are decisions imposed on a software project that must be satisfied by the\narchitecture. They typically cannot be changed. They can affect the software architecture\ndesign and are therefore an architectural driver.\n", "page": 156, "type": "text", "section": "Page 156"}
{"text": "Designing Software Architectures\nChapter 5\n[ 128 ]\nConstraints are generally fixed from the beginning of the project and might be technical or\nnon-technical. Examples of technical constraints include being required to use a specific\ntechnology, having the ability to deploy to a particular type of target environment, or using\na specific programming language. Examples of non-technical constraints are being required\nto abide by a certain regulation, or that the project must meet a particular deadline.\nConstraints may also be classified by whether they are internal or external. Internal\nconstraints originate from within the organization and you may have some control over\nthem. In contrast, external constraints come from outside of the business and you may not\nhave any control over them.\nLike the other architectural drivers, constraints need to be considered in the design as an\ninput into the design process.\nArchitectural concerns\nArchitectural concerns are interests of the software architect that impact the software\narchitecture. As a result, they are an architectural driver. Just as functional requirements\nand quality attributes are design issues important to stakeholders, architectural concerns\nare design issues important to the software architect.\nArchitectural concerns need to be considered part of the design, but are not captured as\nfunctional requirements. In some cases, they may be captured as quality attributes rather\nthan architectural concerns, or an architectural concern may lead to new quality attribute\nscenarios that need to be met.\nFor example, a software architect may have concerns related to software instrumentation or\nlogging. If not already recorded as part of a quality attribute, such as maintainability, the\narchitectural concern may lead to a new quality attribute.\nGood software architects will be able to recognize possible architectural concerns based on\nthe type of software they are designing. Architectural concerns may also arise from\nprevious architecture design iterations, so be aware that architecture changes may lead to\nnew concerns being created from them.\n", "page": 157, "type": "text", "section": "Page 157"}
{"text": "Designing Software Architectures\nChapter 5\n[ 129 ]\nLeveraging design principles and existing\nsolutions\nDesigning a software architecture from scratch for a project with some level of complexity\ncan be a challenging task. However, software architects have a number of tools at their\ndisposal when designing an architecture.\nThe design issues facing a project may have already been solved by others, and rather than\nreinventing the wheel, those solutions can be leveraged in your architecture design. These\narchitecture design principles and solutions, which are sometimes referred to as design\nconcepts, are building blocks used to design a software architecture.\nSelecting a design concept\nThere are many design concepts, so a software architect needs to know which ones are\nsuitable for a particular problem, and then select the one that is most appropriate among\nthe alternatives. You may also find cases where you need to combine multiple design\nconcepts to create a solution.\nDepending on the stage of the architecture design you are in and the nature of the problem,\ncertain design concepts will make more sense than others. For example, a reference\narchitecture would be useful when creating the initial structure of the architecture, but later\nin the design, when considering a specific quality attribute scenario, a tactic might be used.\nSoftware architects generally determine which design concepts are available by using their\nknowledge and experience, leveraging the knowledge and experience of their teammates,\nand following best practices.\nWhen choosing a specific design concept among multiple alternatives that have been\nidentified, you'll want to weigh the pros and cons, as well as the cost of each alternative.\nKeep any project constraints in mind when selecting design concepts, as a constraint may\nprevent you from using certain alternatives.\nSome of the design concepts available to you include software architecture patterns,\nreference architectures, tactics, and externally developed software.\n", "page": 158, "type": "text", "section": "Page 158"}
{"text": "Designing Software Architectures\nChapter 5\n[ 130 ]\nSoftware architecture patterns\nWhen designing a software architecture, some of the design issues that you will face have\nalready been solved by others. Software architecture patterns provide solutions for\nrecurring architecture design problems. Patterns are discovered while observing what\npeople were doing successfully to solve a particular problem, and then documenting those\npatterns so that they can be reused. They can be leveraged in an architecture design if the\nsoftware application has the same design issue.\nSoftware architects should take advantage of the work and experience of others when they\nhave a problem that can be solved by a pattern. The challenging part is to be aware of what\npatterns are available, and which ones are applicable to the problem you are trying to solve.\nAs with any design pattern though, you shouldn't try to force the use of one. You should\nonly use an architecture pattern if it truly solves the design issue that you have and if it is\nthe best solution given your context.\nWe will be exploring software architecture patterns in more detail in Chapter 7, Software\nArchitecture Patterns.\nReference architectures\nA reference architecture is a template for an architecture that is best suited to a particular\ndomain. It is composed of design artifacts for a software architecture that provides\nrecommended structures, elements, and the relationships between the elements.\nBenefits of reference architectures\nA reference architecture can answer many of the most common questions for systems that\nneed a particular design. They can be very helpful to software architects because they\nprovide a tested solution to a problem domain, and reduce some of the complexities\ninvolved in designing a software architecture. Reference architectures are proven, in both\ntechnical as well as business contexts, as viable solutions for certain problems.\nUsing a reference architecture allows the team to deliver a solution quicker, and with fewer\nerrors. Re-using an architecture provides advantages such as quicker delivery of a solution,\nreduced design effort, reduced costs, and increased quality.\nLeveraging the experiences of past software applications and learning from them can be of\ngreat value to software architects. They help us to avoid making certain mistakes and can\nprevent costly delays that may result from not using a previously proven approach.\n", "page": 159, "type": "text", "section": "Page 159"}
{"text": "Designing Software Architectures\nChapter 5\n[ 131 ]\nRefactoring a reference architecture for your needs\nJust as a design without using a reference architecture may require multiple iterations to\nachieve the final result, it is also the case when using a reference architecture. Design\ndecisions will need to be made regarding the reference architecture.\nDuring iterations for the architecture design, refactoring can take place on a reference\narchitecture to meet the specific needs of the software application being designed. The\namount of refactoring necessary depends on how closely the reference architecture meets\nthe functional and quality attribute requirements.\nReference architectures may be created at different levels of abstraction. If you want to use\none and it is not at the level of abstraction you need, you might still be able to learn from it,\nand use it as a guide when designing your own architecture.\nFor well-understood domains, there may be a number of reference architectures available\nto you. In contrast, if you are designing a solution for a greenfield system that is in a novel\ndomain, there may be few, if any, available for you to use. Even for those types of projects\nthough, you may find a reference architecture you can leverage, even if it is just for a\nportion of the design. It might just require more refinement and refactoring than when a\nmore fitting reference architecture is available.\nWhen you use a reference architecture, you adopt issues from that reference architecture\nthat you will need to address. If a reference architecture deals with a particular design\nissue, you will need to make design decisions about that issue, even if you don't have a\nspecific requirement related to it. The decision might very well be to exclude something\nfrom your architecture that is in the reference architecture.\nFor example, if a reference architecture includes instrumentation as a cross-cutting concern,\nyou will need to make design decisions about instrumentation during your design.\nCreating your own reference architecture\nOnce an organization has a completed software architecture, which may or may not have\nused a reference architecture, it can then become a reference architecture itself. When an\norganization needs to create new software applications, perhaps as part of a software\nproduct line, it can use the architecture of an existing product as a reference architecture.\nUsing a reference architecture from your own organization is just like using a reference\narchitecture from somewhere else. You will reap benefits by doing so, but some amount of\nrefactoring may be required to use it in a particular application. The added advantage is\nthat the reference architecture would be likely to already be suited to your particular\ndomain.\n", "page": 160, "type": "text", "section": "Page 160"}
{"text": "Designing Software Architectures\nChapter 5\n[ 132 ]\nTactics\nTactics are proven techniques to influence quality attribute scenarios. They focus on a\nsingle quality attribute, so they are simpler than other design concepts, such as architecture\npatterns and reference architectures, which aim to solve a greater number of design issues.\nTactics provide options to satisfy quality attributes, and the use of other design concepts,\nsuch as architecture patterns or an externally built framework, along with code, are\nrequired to fully complete the tactic.\nWe went over some tactics when we explored quality attributes, such as:\nSatisfying a maintainability quality attribute scenario by reducing complexity in\na component by increasing cohesion and reducing coupling\nIncreasing usability in a scenario by providing friendly and informative\nmessages to the user\nImplementing a retry strategy in a process to handle a possible transient fault in\norder to improve an availability quality attribute scenario\nSatisfying a portability quality attribute scenario by increasing installability by\nensuring that a software update process to a newer version properly cleans up\nthe older version\nExternally developed software\nWhen designing a software architecture, you will be making design decisions for a number\nof design issues. Some of these design issues already have solutions in the form of concrete\nimplementations that have been developed externally. Rather than build a solution in-\nhouse to solve a particular design issue, you can leverage software that has already been\ndeveloped outside of the organization.\nThe externally developed software can come in different forms, such as a component, an\napplication framework, a software product, or a platform. There are many examples of\nexternally developed software, such as a logging library for logging functionality, a UI\nframework for creating user interfaces, or a development platform for server-side logic.\n", "page": 161, "type": "text", "section": "Page 161"}
{"text": "Designing Software Architectures\nChapter 5\n[ 133 ]\nBuy or build?\nOne of the decisions that software architects need to make is the classic buy or\nbuild dilemma. When you are in need of a solution to a particular design issue, you will\nneed to decide whether to buy or build it. When using the term buy, we are referring to\nusing something built externally, and not necessarily the fact that it may have a monetary\ncost. Depending on what type of solution you are looking for, there may be a number of\nfree solutions available to you, including those that are open source.\nWhen deciding whether to use an externally developed solution or to build it in-house, you\nmust first make sure that you understand the problem that you are trying to solve, and the\nscope of that problem. You will need to research whether externally developed software\nexists that will solve the design problem. If the problem is unique to your organization,\nthere may not be any suitable software available.\nYou should also know whether or not the organization has, or can attain, resources to\nbuild, maintain, and support a solution. If the solution is to be built internally by the project\nteam, there must be sufficient resources, including time, to build it.\nAdvantages/disadvantages of building\nAn advantage of building it internally is that the solution will be unique to your\norganization, and tailored to it. The organization will have complete control over the\nsolution, including full ownership of the source code. This will allow the organization to\nmodify it in any way that it wants. If a need arises to make changes or add functionality,\nthe organization will be able to do so with full authority.\nAnother benefit of building it yourself is that there could be a competitive advantage. If the\nsolution provides some feature that competitors do not currently have, building it and\nowning it could provide a strategic advantage to the organization.\nThe disadvantages of building it yourself are that it will require time and resources to do\nso. The end result may not have as robust a set of features as an externally developed\nsolution. For example, if you are in need of a distributed, scalable, enterprise-level, full-text\nsearch engine as part of your application, it is probably impractical to build it yourself\nrather than use a proven solution that already exists.\n", "page": 162, "type": "text", "section": "Page 162"}
{"text": "Designing Software Architectures\nChapter 5\n[ 134 ]\nAdvantages/disadvantages of buying\nUsing an externally developed solution has its own set of advantages. It will save time, as\nno effort will need to be spent developing it. It may be of higher quality, assuming that it\nhas already been tested and used in production. Feedback from other users of the software\nmay have exposed problems that have already been fixed.\nThe external solution might be continually improved to achieve higher levels of quality and\nto introduce new features. Support and training may be available that your team will be\nable to leverage.\nHowever, there are downsides to using an externally developed solution. There may be a\ncost to using such a solution. Depending on the license type, you may not have access to the\nsource code and may be limited in how the solution can be used. If you can't modify the\nsolution, then the solution's functionality will be controlled by someone else, and it may not\nexactly fit your needs. In addition, if there are issues with the solution, or you need it\nchanged in some way, you will need to rely on an external organization.\nResearching external software\nIn order to find out whether external software exists that will be a suitable solution for the\nproblem being solved, or in order to select an external solution from multiple alternatives\nthat might be available, some research will be required.\nThe software architect should consider the following:\nDoes it solve the design problem?\nIs the cost of the software acceptable?\nIs the type of license that comes with the software compatible with the project's\nneeds?\nIs the software easy to use? Does the team have resources that can use it?\nCan the software be integrated with the other technologies that are going to be\nused on the project?\nIs the software mature, providing stable releases?\nDoes it provide the level of support that might be needed, whether that support\nis paid support or through a development community?\nIs the software widely known, such that the organization can easily hire\nresources familiar with it?\nCreating one or more prototypes that use the possible candidate solutions is a good way to\nevaluate and compare them. A POC to ensure that it is a workable solution is a wise idea.\n", "page": 163, "type": "text", "section": "Page 163"}
{"text": "Designing Software Architectures\nChapter 5\n[ 135 ]\nShould I use open source software (OSS)?\nWhen searching for an externally developed solution that will solve a design problem, one\npossibility is to find open source software (OSS) that will fulfill your needs. OSS is written\nby the community and is intended for use by the community.\nGiven the wide availability and range of open source software, there are many solutions\navailable for a variety of problems. It is much more common now to use open source\nsolutions as part of a software application. Some organizations do not permit the use of\nopen source software but if your organization does, then you should give it consideration\nas a viable alternative for a given task.\nOne consideration when selecting open source software is the license that is associated with\nit. The license dictates the terms and conditions under which the software can be used,\nmodified, and shared. One set of open source licenses that are popular is a group of licenses\nthat have been approved by the Open Source Initiative (OSI). Some of the OSI-approved\nlicenses include (in alphabetical order):\nApache License 2.0\nBSD 2-clause Simplified or FreeBSD license\nBSD 3-clause New or Revised license\nCommon Development and Distribution License\nEclipse Public License\nGNU General Public License (GPL)\nGNU Lesser General Public License (LGPL)\nMIT license\nMozilla Public License 2.0\nThere are differences in the terms and conditions of the various licenses. For example, your\napplication can incorporate open source software that uses the MIT license and you will be\nable to distribute your application without making it open source.\nIn contrast, if your application incorporates software that uses the GNU General Public\nLicense and you then distribute your application, your application would need to be made\nopen source. This is true even if your application is free and you do not change the open\nsource software you are using in any way. If your software is for internal use only and it is\nnot distributed, then your application could remain proprietary and closed source. \n", "page": 164, "type": "text", "section": "Page 164"}
{"text": "Designing Software Architectures\nChapter 5\n[ 136 ]\nAdvantages of using open source software\nThere are benefits to using open source software, which explains its popularity. Using an\nopen source solution for a design problem provides many of the same advantages as using\none that has been purchased. You don't have to spend time building the solution, it may\nhave a robust set of features, and it may already be a tested and proven solution with many\nother users.\nUnlike software that must be purchased, open source software is freely available so there\nare cost savings. You just have to keep in mind the license that comes with the software.\nIf the open source software is a popular solution with an active community, it might be\ncontinuously improved with bug fixes and new features. You will be able to take advantage\nof this work. Bugs may be detected and fixed quickly because many people are using and\nworking on the code. This is the idea behind Linus's Law, which is named after Linus\nTorvalds, the creator of the Linux kernel. Linus's Law basically states that given enough\neyeballs, or people looking at the code, all bugs are shallow. In other words, with many\npeople looking at the source code, problems will be detected sooner rather than later, and\nsomeone will be able to provide a fix.\nAlthough some view open source software as less secure due to the availability of the code,\nsome people see it as more secure because there are many eyes that are using, looking at,\nand fixing the code.\nAnother advantage of open source software is the fact that you have access to the source\ncode. If necessary, your development team will be able to modify it just as you would with\nan in-house solution.\nDisadvantages of using open source software\nDespite its advantages, there are some disadvantages to using open source software that\nyou should consider. Even though the software is free, there are still costs related to using\nit. Someone has to spend time integrating the solution into the software system, and there is\nan associated cost for that effort. If the open source software has to be modified in any way\nto suit the needs of the project, there is a cost related to that work as well.\nIf there is no one on the team who knows how to use the software, and it is complex\nenough that some training is required, learning how to use a piece of open source software\nmay also take time.\n", "page": 165, "type": "text", "section": "Page 165"}
{"text": "Designing Software Architectures\nChapter 5\n[ 137 ]\nEven for a popular open source project with an active community, there is no guarantee\nthat the software will continue to experience support. There is always a risk of the software\ngoing out of favor. If the project is abandoned, you won't be able to rely on support for bug\nfixes or new features unless the development team performs that work for themselves.\nOne reason an open source software project may become less secure is if no one is actively\nworking on it. Even if the project has not been abandoned, no one is necessarily reading the\ncode. The average programmer writes much more code than they read. The existence of\nsome prominent security bugs has shown that it is possible for critical security\nvulnerabilities to go undetected for some time.\nDespite Linus's Law, the fact that the source code is readily available introduces a degree of\nsecurity risk. Malicious individuals can analyze the source code to identify security\nvulnerabilities and attempt to take advantage of them.      \nDocumenting the software architecture\ndesign\nAn important part of architecture design is documenting the design, including the many\ndesign decisions that are made during the process. This typically comes in the form of\nsketching architecture views and documenting the design rationale.\nSketching the architecture design\nSoftware architectures are commonly documented through the creation of architecture\nviews. Architecture views are representations of a software architecture that are used to\ndocument it and communicate it to various stakeholders. Multiple views of an architecture\nare typically required, as a software architecture is too complex to be represented in a\nsingle, comprehensive model.\nFormal documentation of a software architecture through views will be covered Chapter\n12, Documenting and Reviewing Software Architectures, and is not typically done as part of the\ndesign process. While that type of documentation comes afterward, informal\ndocumentation, in the form of sketches, should take place during the architecture design.\nSketches can record the structures, elements, relationships between the elements, and the\ndesign concepts used.\n", "page": 166, "type": "text", "section": "Page 166"}
{"text": "Designing Software Architectures\nChapter 5\n[ 138 ]\nThe sketches do not necessarily need to use any formal notation, but they should be clear.\nWhile it's not necessary to sketch everything, at a minimum, you will want to sketch out\nimportant decisions and design elements. These sketches can be done on a whiteboard, on\npaper, or using a modeling tool. \nDocumenting the design by creating sketches during the design process will help you to\ncreate architecture views later. If you already have informal sketches, when the time comes\nto create formal documentation, you will find it to be an easier task.\nDocumenting the design as it occurs will also ensure that you do not forget any design\ndetails when it comes to creating the architecture views. Your architecture will be analyzed\nand validated later to ensure that it satisfies functional requirements and quality attribute\nscenarios, so it is helpful to record details during design that can then be used to explain\nhow requirements and quality attributes were satisfied by the architecture design.\nIf you aren't able to sketch out a part of your design, you'll have to consider the possible\nreasons for that. Perhaps it is not well understood, too complex, you haven't put enough\nthought into how to communicate it, or there may be parts of it that are unclear to you. If\nthat is the case, you should revisit the design until you are able to sketch it. If you can\nsketch the design created in an iteration effortlessly and clearly, your audience will be able\nto understand it.\nDocumenting the design rationale\nSoftware architecture design involves making many design decisions, and software\narchitects should document those decisions along with their design rationale. While design\nsketches may explain what was designed, they don't give any indication as to the design\nrationale.\nA design rationale explains the reasons, and justification, behind the decisions that are\nmade during the design of the software architecture. Design rationale can also include\ndocumentation on what decisions were not made, as well as alternatives that were\nconsidered for decisions that were made. Reasons for rejection can be recorded for each\nalternative that was not selected.\nRecording design rationale can be useful during the design process, as well as once the\ndesign is complete. Software architects who document their design rationale are afforded\nan opportunity to clarify their thoughts and arguments as they capture the design rationale,\nwhich may even expose flaws in their thinking.\n", "page": 167, "type": "text", "section": "Page 167"}
{"text": "Designing Software Architectures\nChapter 5\n[ 139 ]\nOnce the design rationale is documented, anyone who wants to know why a particular\ndesign decision was made, even after some time has passed, can refer to it. Even\nindividuals who were involved with a design decision, including the software architect,\nmay forget the rationale behind a particular decision and will be able to refer to the\ndocumentation.\nThe design rationale should refer to the specific structures that were designed and the\nspecific requirements that they intended to meet. Some software design tools provide\nfunctionality that can assist the software architect in capturing the design rationale.\nA complete design rationale provides a history of the software architecture design process.\nThere are a number of uses for design rationale, such as for design evaluation, verification,\nknowledge transfer, communication, maintenance, documentation, and reuse.\nDesign rationale for design evaluation\nDesign rationale can be used to evaluate different software architecture designs and their\ndesign choices. The various designs can be compared with each other, and an\nunderstanding can be gained as to the situations in which one design would be chosen over\nanother.\nDesign rationale for design verification\nThe purpose of software architecture design verification is to ensure that the software\nsystem, as designed, is the software system that was intended. It verifies that the software\narchitecture meets the requirements, including the quality attributes, and works as\nexpected. The design rationale can be used as part of the verification.\nDesign rationale for design knowledge transfer\nThe design rationale can be used for knowledge transfer to team members, including those\nwho may join the team later, either during development or after the software goes into its\nmaintenance phase.\nTeam members can learn about the design decisions and the reasons behind them by\nreviewing the design rationale. It is particularly useful for knowledge transfer when the\noriginal software architect, and others who collaborated on the design of the software\narchitecture, are no longer available to provide the information in other ways.\n", "page": 168, "type": "text", "section": "Page 168"}
{"text": "Designing Software Architectures\nChapter 5\n[ 140 ]\nDesign rationale for design communication\nIt will be necessary at different times to communicate the design of the software\narchitecture to various stakeholders and other individuals. The information provided by\nthe design rationale adds value to the overall communication.\nIn addition, the design rationale can be used by those who are reviewing the software\narchitecture so that they can learn the reasons behind particular design decisions.\nDesign rationale for design maintenance\nDuring the maintenance phase of a software project, it is helpful to know the design\nrationale for the decisions that went into the software architecture design. When a certain\npiece of the software needs to be changed for maintenance, the design rationale can assist in\ndetermining what areas of the software will require modifications.\nIt can also be used to identify weaknesses in the software, and areas of the software that\ncould be improved. For example, based on certain design decisions, quality attributes may\nbe enabled or inhibited, and if changes are being considered that would alter those\ndecisions, team members could be aware of the reasons behind those decisions.\nThe design rationale will also point out design alternatives that were not chosen, allowing\nthose considering modifications to either avoid previously rejected design alternatives or to\nat least be knowledgeable about the reasons those alternatives were rejected in order to\nmake an educated decision.\nDesign rationale for design documentation\nThe software architecture must be documented, and the design rationale is an important\npart of that documentation. If the documentation only shows the design, those looking at it\nwill know what was designed but won't know why it was designed that way. They also\nwon't be aware of the alternatives that were considered, and why those alternatives were\nrejected.\nDesign rationale for design reuse\nSoftware architecture reuse involves creating multiple software applications using core\nassets, allowing architectural components to be reused across multiple software products.\nOrganizations seek to take advantage of efficiencies that can be gained when reusing\narchitectural components to build multiple software products as part of a software product\nline.\n", "page": 169, "type": "text", "section": "Page 169"}
{"text": "Designing Software Architectures\nChapter 5\n[ 141 ]\nCapturing design rationale can facilitate successful architectural reuse. It can help designers\nunderstand what parts of the application can be reused. It may also provide some insight\ninto where modifications can be made to a component in order to reuse it in the application.\nDue to the variation between software products, reusable components are typically\ndesigned with variation points, or places where modifications can be made in order to adapt\nthe component for use in a particular software product. Understanding the design rationale\nwill help designers use the component properly, and prevent harmful modifications from\nbeing made.\nUsing a systematic approach to software\narchitecture design\nIf you are going to dedicate some time to designing the architecture of a software system,\nand not just let it emerge after implementing features, you should do so in a systematic way.\nSoftware architects need to ensure that the architecture they are designing will satisfy the\narchitectural drivers, and a systematic approach can assist in accomplishing that goal. In\nDesigning Software Architectures, A Practical Approach, the following is said about using an\narchitecture design process:\n\"The question is, how do you actually perform design? Performing design to ensure that\nthe drivers are satisfied requires a principled method. By \"principled\", we refer to a\nmethod that takes into account all of the relevant aspects that are needed to produce an\nadequate design. Such a method provides guidance that is necessary to guarantee that your\ndrivers are satisfied.\"\nUsing an established architecture design process will provide you, as the software architect,\nwith guidance on how to go about designing an architecture that will satisfy functional\nrequirements and quality attribute scenarios. There are a number of design processes that\ncan be used for software architecture. Although they differ from each other, including\ndifferences in terminology, they also have some fundamental commonalities.\n", "page": 170, "type": "text", "section": "Page 170"}
{"text": "Designing Software Architectures\nChapter 5\n[ 142 ]\nA general model of software architecture design\nThe paper, A general model of software architecture design derived from five industrial approaches,\nby Christine Hofmeister, Philippe Kruchten, Robert L. Nord, Henk Obbink, Alexander Ran,\nand Pierre America, compared five different architecture design methods to come up with a\ngeneral model of architecture design based on the similarities. Having a general model\nhelps us to understand the types of activities that are typically performed in a software\narchitecture design process and allows us to compare the strengths and weaknesses of\ndifferent processes.\nIt was found that most architecture design processes involve analyzing architectural\ndrivers, designing candidate solutions that will satisfy the architectural drivers, and then\nevaluating the design decisions and candidate solutions to ensure that they are correct.\nThe three main design activities that were identified in A general model of software\narchitecture design derived from five industrial approaches are Architectural Analysis,\nArchitectural Synthesis, and Architectural Evaluation:\nArchitectural analysis\nDuring architectural analysis, the problems that the architecture is trying to solve are\nidentified. These are sometimes referred to as architecturally significant requirements\n(ASRs) because they will influence the design of the architecture. Not all of the design\nissues that must be considered are requirements though. We must address all of the\narchitectural drivers, which include design objectives, primary functional requirements,\nquality attribute scenarios, constraints, and architectural concerns.\nThe output of this activity is a set of architectural drivers that will serve as the input to\narchitectural synthesis.\n", "page": 171, "type": "text", "section": "Page 171"}
{"text": "Designing Software Architectures\nChapter 5\n[ 143 ]\nArchitectural synthesis\nThe architectural synthesis activity is where solutions are designed on the basis of the set\nof architectural drivers that were identified in the architectural analysis activity. It is during\nthis activity that we leverage design concepts such as architecture patterns, reference\narchitectures, tactics, and externally developed software, and combine them with the\ndesign of structures, elements, and relationships between the elements. This produces\nsolutions for the set of architectural drivers.\nThe output of this activity is one or more candidate solutions for the problems selected.\nArchitectural evaluation\nIn the architectural evaluation, the candidate solutions that were designed during\narchitectural synthesis are evaluated to ensure that they solve the problems that they were\nintended for, and that all of the design decisions that were made are correct.\nAt the conclusion of the architectural evaluation activity, each candidate solution has either\nbeen validated or invalidated. We will cover reviewing software architectures in Chapter\n12, Documenting and Reviewing Software Architectures.\nArchitecture design is an iterative process\nAnother important similarity found in architecture design is the fact that it is an iterative\nprocess. Designing a software architecture is too complex to address all of the architectural\ndrivers simultaneously.\nThe design of the architecture occurs over multiple iterations until all architectural drivers\nhave been addressed. Each iteration starts by selecting the architectural drivers that will be\nconsidered for that iteration. If candidate solutions have been validated after they have\nbeen evaluated, those design decisions are integrated into the overall architecture.\nIf there are no more architectural drivers that need solutions, the validated architecture is\ncomplete. If outstanding architectural drivers exist, a new iteration will begin.\n", "page": 172, "type": "text", "section": "Page 172"}
{"text": "Designing Software Architectures\nChapter 5\n[ 144 ]\nSelecting an architecture design process\nNow that we understand the fundamental activities that occur during software architecture\ndesign, which one do we use? There are many different software architecture design\nprocesses. One way that we can compare design processes is to examine the activities and\nartifacts of the design process:\nWhat are the activities and artifacts of the design process?\nAre there any activities/artifacts that you think are not needed?\nAre there any activities/artifacts that you feel are lacking?\nWhat are the techniques and tools of the design process?\nIf it helps, you can compare the activities and artifacts of the design process with those that\nexist in the general model. Some of the activities and artifacts of the design process may\nhave corresponding ones in the general model, although different names may be used.\nThere may also be activities and artifacts in the design process that do not have any\ncorresponding ones in the general model, as well as ones in the general model that do not\nexist in the design process.\nAfter doing this type of comparison, you should have an understanding as to what each\ndesign process entails, along with their strengths and weaknesses. This knowledge can be\nused to select the one that is most suited to your project.\nThe software architect can modify a design process to make it more suitable for a project's\nneeds, although such changes should only be done thoughtfully. If, after analyzing and\nselecting a design process, you feel that a particular activity or artifact is not needed, you\ncould remove it.\n", "page": 173, "type": "text", "section": "Page 173"}
{"text": "Designing Software Architectures\nChapter 5\n[ 145 ]\nConversely, if you see that a design process lacks an activity and/or artifact, you can change\nthe process to include it. You might be able to draw on a technique, tool, or even another\ndesign process to supplement what you feel is missing from the design process you want to\nuse.\nSo far, we have been discussing architecture design processes in fairly general terms, so\nnow let's  explore several concrete ones at a high level. Three of the architecture design\nprocesses available to you are ADD, Microsoft's technique for architecture and design, and\nthe ACDM.\nAttribute-driven design (ADD)\nAttribute-driven design (ADD) is one of the systematic approaches for designing software\narchitectures. It is an iterative, organized, step-by-step method that can be followed during\narchitectural design iterations.\nThis method pays particular attention to software quality attributes during the design\nprocess. As a result, one of the primary benefits of using ADD is that you begin to consider\nquality attributes early in the design process.\nEnabling a quality attribute in a software architecture design may affect other quality\nattributes. Consequently, trade-offs between quality attributes may be necessary. By\nfocusing on quality attributes using the ADD method, these types of trade-offs can be\nconsidered at an early stage during the process.\nThe ADD process is specifically focused on architecture design and, as such, doesn't cover\nthe entire architectural life cycle. The process doesn't include the gathering of architectural\ndrivers, documenting the architecture, or evaluating the architecture once it is designed.\nHowever, you can combine ADD with other methods to fill in these gaps.\n", "page": 174, "type": "text", "section": "Page 174"}
{"text": "Designing Software Architectures\nChapter 5\n[ 146 ]\nADD is a widely-used method for software architecture design, and has been used\nsuccessfully on a variety of software applications. There are eight steps in the attribute-\ndriven design process:\n", "page": 175, "type": "text", "section": "Page 175"}
{"text": "Designing Software Architectures\nChapter 5\n[ 147 ]\nStep 1 \u2013 Reviewing inputs\nThe first step in the ADD process is to review the inputs into the attribute-driven design.\nBefore the design starts, we want to ensure that we are clear on the overall design problem\nwe are solving.\nThe inputs are the architectural drivers that we reviewed earlier:\nDesign objectives\nPrimary functional requirements\nQuality attribute scenarios\nConstraints\nArchitectural concerns\nIf the software system is either a brownfield system or it is not the initial iteration for the\narchitecture design of a greenfield system, there is at least some part of an architecture\nalready in place. This existing architecture must be considered as part of the input into the\niteration.\nStep 2 \u2013 Establishing the iteration goal and\nselecting inputs to be considered in the iteration\nOnce inputs are reviewed, one or more design iterations will take place, with each iteration\nbeginning with Step 2. If you are using an agile methodology, multiple iterations will take\nplace until the architecture is complete and the design purpose has been accomplished. An\nagile methodology is preferred, and is more common, as attempting to provide solutions\nfor all of the architectural drivers at once can be too difficult.\nAt the start of each iteration, we want to establish the design goal for that iteration. We\nshould be able to answer the question, What design issue are we trying to solve in the iteration?\nEach goal will be associated with one or more inputs. The inputs, or architectural drivers,\nthat are relevant to the goal are identified and will be the focus of the iteration.\n", "page": 176, "type": "text", "section": "Page 176"}
{"text": "Designing Software Architectures\nChapter 5\n[ 148 ]\nStep 3 \u2013 Choosing one or more elements of the\nsystem to refine\nBased on the iteration goal and the architectural drivers that we want to create a solution\nfor, we must select the various elements that we want to decompose.\nIf your project is a greenfield system and this is the first iteration, you begin at the highest\nlevel and start by decomposing the system itself. For any other iteration, the system has\nalready been decomposed to some degree. You would select one or more of the existing\nelements to focus on for this iteration.\nStep 4 \u2013 Choosing one or more design concepts\nthat satisfy the inputs considered in the iteration\nOnce elements have been selected for decomposition, we need to select one or more design\nconcepts that can be used to meet the iteration goal and satisfy the inputs (architectural\ndrivers). Design concepts refer to design principles and solutions such as architecture\npatterns, reference architectures, tactics, and externally developed software.\nStep 5 \u2013 Instantiating architectural elements,\nallocating responsibilities, and defining\ninterfaces\nBased on the design concepts that can be leveraged for this iteration, analysis is performed\nso that details can be provided regarding the responsibilities for the elements being\ndecomposed, along with the public interfaces of those elements that will be exposed.\nEach element being decomposed (parent element) may yield one or more child elements.\nBy considering the responsibilities of the parent element, we can assign responsibilities to\nthe various child elements. All of the responsibilities of the parent element are considered,\nwhether or not they are architecturally significant.\n", "page": 177, "type": "text", "section": "Page 177"}
{"text": "Designing Software Architectures\nChapter 5\n[ 149 ]\nStep 6 \u2013 Sketching views and recording design\ndecisions\nViews should be sketched recording the solution designed so that it can be communicated.\nIn this step, all of the design decisions that were made during this particular iteration are\ndocumented. This documentation should also include the design rationale.\nThe artifacts created in this step can simply be sketches and do not have to be the formal,\ndetailed software architecture views. In the ADD process, the creation of the architecture\nviews comes later, but the design decisions made in this iteration should be reflected in\nsketches that can then be used in the formal architecture views later.\nWe will explore documenting software architectures in Chapter 12, Documenting and\nReviewing Software Architectures.\nStep 7 \u2013 Performing analysis of current design\nand reviewing the iteration goal and design\nobjectives\nIn this final step of a software architecture design iteration, the software architect and other\nteam members should analyze the current design. The design decisions are analyzed to\nensure that they are correct and satisfy the iteration goal and architectural drivers that were\nestablished for the iteration.\nThe result of this analysis should determine whether more architecture design iterations\nwill be necessary.\nStep 8 \u2013 Iterating if necessary\nIf it is decided that more iterations are needed, the process should go back to Step 2 for\nanother iteration. As a software architect, there will be times where you feel more iterations\nare necessary, but something will prevent you from conducting more iterations. For\nexample, the project management team may decide that there is not enough time for more\niterations and that the architecture design process is done.\nIf no further iterations will take place, the software architecture design is complete.\n", "page": 178, "type": "text", "section": "Page 178"}
{"text": "Designing Software Architectures\nChapter 5\n[ 150 ]\nMicrosoft's technique for architecture and\ndesign\nAnother example of a systematic approach for designing software architectures is\nMicrosoft's technique for architecture and design. Like ADD, it is an iterative, step-by-\nstep method. This process can be used to design the initial architecture as well as to refine it\nlater, if necessary. There are five steps in the process:\nStep 1 \u2013 Identifying architecture objectives\nThe design process begins by identifying the objectives you want to achieve for the\narchitecture. The purpose of this step is to ensure that the design process has clear\nobjectives so that the solution focuses on the appropriate problems. Once design iterations\nstart, we want to ensure that we are clear on the overall design problems that we are\nsolving.\n", "page": 179, "type": "text", "section": "Page 179"}
{"text": "Designing Software Architectures\nChapter 5\n[ 151 ]\nThe various architectural drivers, such as design objectives, primary functional\nrequirements, quality attribute scenarios, constraints, and architectural concerns all\ncombine to form the architecture objectives. The software architect should also consider\nwho will consume the architecture. The architecture design might be used by other\narchitects, developers, testers, operations personnel, and management. Both the needs and\nthe experience levels of the various people who will view and use the architecture design\nshould be considered during the design process. As with ADD, if the software system has\nan existing architecture, either because it is a brownfield system or design iterations have\nalready taken place, the existing architecture is another consideration.\nStep 2 \u2013 Identifying key scenarios\nOnce the design objectives are identified and established, one or more design iterations will\nbe necessary. Step 1 will only occur once, and each design iteration will begin with Step 2.\nStep 2 focuses on identifying key scenarios for the software application.\nIn this design process, a scenario is defined as a more encompassing user interaction with\nthe software system, and not just a single use case. Key scenarios are the most important of\nthese scenarios and are required for the success of the application. Scenarios could be an\nissue, an architecturally significant use case (one that is business critical and has a high\nimpact), or involve an intersection between functional requirements and quality attributes.\nOnce again, keep in mind that there may be trade-offs with quality attributes, so the\nscenarios should take those into consideration.\nStep 3 \u2013 Creating application overview\nIn this step, armed with architecture objectives and key scenarios, an application overview\nis created. An application overview is what the architecture will look like when it is\ncomplete. An application overview is intended to connect an architecture design with real-\nworld decisions.\nCreating an application overview consists of determining your application type, identifying\ndeployment constraints, identifying architecture design styles, and determining relevant\ntechnologies.\n", "page": 180, "type": "text", "section": "Page 180"}
{"text": "Designing Software Architectures\nChapter 5\n[ 152 ]\nDetermining your application type\nThe software architect must determine what type of application is appropriate based on the\nobjectives and key scenarios. Examples of application types include web, mobile, service,\nand Windows desktop applications.\nIt is possible that a software application may be a combination of more than one type.\nIdentifying your deployment constraints\nWhen designing a software architecture, among the many constraints that you may have to\nconsider are constraints related to deployment. You may be required to follow particular\npolicies of the organization. The infrastructure and target environment of the software \napplication may be dictated by the organization, and such constraints may be something\nthat you will have to work around.\nThe earlier that any conflicts and issues related to constraints can be identified regarding\nthe software application and the target infrastructure, the easier those issues can be\nresolved.\nIdentifying important architecture design styles\nArchitecture design styles, also known as architecture patterns, are general solutions to\ncommon problems. Using an architecture style promotes reuse by leveraging a known\nsolution to a recurring problem.\nIdentifying the appropriate architecture design style, or a combination of styles, that will be\nused in the software application is an important part of creating an application overview.\nWe will go into detail about various architecture patterns in Chapter 7, Software\nArchitecture Patterns, and Chapter 8, Architecting Modern Applications.\nDetermining relevant technologies\nAt this point, you are ready to select relevant technologies for your project. The decisions\nare based on the type of application, which was determined earlier, the architectural styles,\nand the key quality attributes.\nIn addition to technologies specific to the application type (for example, selection of a web\nserver for a web application), technologies will be needed for categories such as application\ninfrastructure, workflow, data access, database server, development tools, and integration.\n", "page": 181, "type": "text", "section": "Page 181"}
{"text": "Designing Software Architectures\nChapter 5\n[ 153 ]\nStep 4 \u2013 Identifying key issues\nThis step of the process involves identifying the important issues you may be facing in the\narchitecture. These issues may require additional focus because they are areas where\nmistakes are most likely to be made.\nKey issues typically map in one form or another to either quality attributes or cross-cutting\nconcerns. We took a look at quality attributes in Chapter 4, Software Quality Attributes, and\nwill be exploring cross-cutting concerns in Chapter 9, Cross-Cutting Concerns.\nAnalyzing quality attributes and cross-cutting concerns closely based on the issues you\nidentify will allow you to know which areas to give extra attention to in your design. The\ndesign decisions that are made as a result should be documented as part of the architecture.\nStep 5 \u2013 Defining candidate solutions\nOnce key issues are identified, candidate solutions can be created. Depending on whether\nor not this is the first iteration, either an initial architecture is created, or the existing\narchitecture is refined to include the solutions that were designed in the current iteration.\nOnce candidate solutions are integrated into the architecture design for the current\niteration, the architecture can be reviewed and evaluated. We will go into further detail on\nreviewing software architectures in Chapter 12, Documenting and Reviewing Software\nArchitectures.\nIf it is determined that more work is necessary for the architecture design, a new iteration\ncan begin. The process goes back to Step 2 so that key scenarios can be identified for the\nnext sprint.\nArchitecture-centric design method (ACDM)\nThe architecture-centric design method (ACDM) is an iterative process used to design\nsoftware architectures. It is a lightweight method with a product focus and seeks to ensure\nthat the software architecture maintains a balance between business and technical concerns.\nIt attempts to make the software architecture the intersection between requirements and the\nsolution.\n", "page": 182, "type": "text", "section": "Page 182"}
{"text": "Designing Software Architectures\nChapter 5\n[ 154 ]\nLike all architecture design processes, the ACDM provides guidance to software architects\nas they design an architecture. While it covers the complete life cycle of software\narchitecture, it is not a complete development process. It is designed to fit in with existing\nprocess frameworks though so that it can be used in conjunction with other methods to\ncover activities outside of architecture. It does not have to replace an existing process\nframework and can complement it instead.\nThere are some minor variations in the number and naming of the steps involved with the\nACDM, but the process is essentially the same. Let's go over the ACDM, which is a seven-\nstep process:\n", "page": 183, "type": "text", "section": "Page 183"}
{"text": "Designing Software Architectures\nChapter 5\n[ 155 ]\nStep 1 \u2013 Discovering architectural drivers\nThe first step in the ACDM is to meet with stakeholders to determine the architectural\ndrivers, which include design objectives, primary functional requirements, quality attribute\nscenarios, constraints, and architectural concerns. The prioritization of quality attribute\nscenarios also takes place in this step.\nStep 2 \u2013 Establishing project scope\nIn this step, the architectural drivers established in Step 1 are reviewed. First, consolidation\nof the information gathered takes place to remove duplicate architectural drivers.\nNext, if any of the architectural drivers gathered are unclear, missing, or incomplete,\nadditional information will be needed. The same is true of any requirements or quality\nattribute scenarios that are not measurable or testable.\nIf any clarification or additional information is needed, it will be gathered in this step from\nthe relevant stakeholders.\nStep 3 \u2013 Creating notional architecture\nUsing the architectural drivers, a notional architecture is created. It is the first attempt at\ndesigning the architecture. The initial representations of the structures that make up the\narchitecture are created and documented.\nNot a lot of time is typically spent on the notional architecture. The idea is that the\narchitecture will be refined through multiple iterations until it is complete.\nStep 4 \u2013 Architectural review\nDuring this step, a review is conducted on the architecture as it currently exists. Reviews\nmay be conducted internally, externally with stakeholders, or there may even be multiple\nreview sessions so that both internal and external ones can be conducted.\nThe purpose of the review is to ensure that all of the design decisions are correct and to\nuncover any potential issues or problems with the architecture. For a given design decision,\nalternative approaches can be discussed, along with the trade-offs and the rationale behind\nthe decision, in order to determine whether the best alternative was taken.  \n", "page": 184, "type": "text", "section": "Page 184"}
{"text": "Designing Software Architectures\nChapter 5\n[ 156 ]\nStep 5 \u2013 Production go/no-go\nOnce the architectural review is complete, a decision is made as to whether the architecture\nis complete and ready for production, or if further refinement is needed. In the ACDM\ncontext, production refers to implementation, including using the architecture in the detailed\ndesign of elements, coding, integration, and testing.\nAny risks identified during the architectural review are considered in the production\ngo/no-go decision. The decision does not have to be an all-or-nothing one. It is possible that\nonly parts of the design require further refinement, in which case a portion of the design\ncan move on to production.\nIf the production decision is a go, and no further refinements are needed, the process can\nskip ahead to production planning, and eventually on to production. However, if the\nproduction decision is a no-go, then the process moves on to Step 6.\nStep 6 \u2013 Experiment planning\nIn this step, any experiments that the team feels are necessary are planned. The purpose of\nan experiment may be to resolve an issue uncovered during the architectural review, to\ngain a greater understanding of one or more architectural drivers, or to improve elements\nand modules of the design before they are committed to the overall architecture.\nExperiment planning includes solidifying the goals of the experiment, estimating the level\nof effort, and assigning the resources that will be needed.\nStep 7 \u2013 Experimenting with and refining the\narchitecture\nAny experiments that were planned are executed during this step. The results of the\nexperiments are recorded. Based on the results of the experiment, if the architecture needs\nto be refined, it is done during this step.\nAfter the refinement is complete, the process goes back to Step 4 so that another \narchitectural review can take place.\n", "page": 185, "type": "text", "section": "Page 185"}
{"text": "Designing Software Architectures\nChapter 5\n[ 157 ]\nProduction planning and production\nOnce architecture design iterations are complete, and the architecture is ready to move into\nproduction, production planning is conducted. Once again, production in the ACDM context\nrefers to using the architecture in implementation.\nGiven this context, production planning involves planning the design and development of\nelements, scheduling the work, and assigning tasks to resources. The project management\nteam creates plans for the work, and bases them, in part, on the architecture.\nOnce the architecture can be moved to production, it can be used by development teams for\nthe detailed design of elements, coding, integration, and testing.\nArchitecture development method (ADM)\nThe architecture development method (ADM) is a step-by-step software architecture \ndesign approach specifically made for enterprise architectures. The ADM was created from\nthe contributions of many software architecture practitioners.\nLike the other architecture design methods that we have covered, the ADM is an iterative\nprocess. The process as a whole is iterative, but it is also iterative between phases and\nwithin a single phase. Each iteration is an opportunity to revisit scope, the level of detail to\nbe defined, schedules, and milestones.\nThe Open Group Architecture Framework\n(TOGAF)\nThe ADM is a core part of The Open Group Architecture Framework (TOGAF), which is a\nframework for enterprise architecture. TOGAF provides a detailed method, the ADM,\nalong with a set of tools for developing enterprise architectures.\nTOGAF is maintained by The Open Group, which is a global industry consortium that\nfocuses on using open and vendor-neutral technology standards to help organizations\nachieve business objectives.\n", "page": 186, "type": "text", "section": "Page 186"}
{"text": "Designing Software Architectures\nChapter 5\n[ 158 ]\nTOGAF architecture domains\nFour standard architecture areas, or architecture domains, for enterprise architecture are\ndefined by TOGAF. They are business, data, applications, and technology architectures.\nThese domains are sometimes referred to as the BDAT domains.\nAll four of these architecture domains are considered during the ADM and we will explore\nthem in more detail when we go over the various phases of the ADM.\nTOGAF documentation\nThe TOGAF documentation is broken up into the following seven sections:\nPart I \u2013 Introduction: The first part is an introduction to the concepts of \nenterprise architecture, the TOGAF approach, and definitions of relevant terms\nused in TOGAF.\nPart II \u2013 Architecture development method: This section details the architecture\ndevelopment method (ADM), which is the core of TOGAF. We will be focusing\nour attention on the ADM part of the TOGAF.\nPart III \u2013 ADM guidelines and techniques: This part of the documentation\nprovides guidelines and techniques for applying TOGAF and the ADM.\nPart IV \u2013 Architecture content framework: In this part, information is provided\non the TOGAF content framework, including the architectural artifacts and\ndeliverables that are part of the process.\nPart V \u2013 Enterprise continuum and tools: This section covers the architecture\nrepository for an enterprise, including the categorization and storage of\narchitecture artifacts.\nPart VI \u2013 TOGAF reference models: In this section, various architectural\nreference models are provided, including the TOGAF Foundation Architecture\nand the Integrated Information Infrastructure Reference Model (III-RM).\nPart VII \u2013 Architecture capability framework: The final part provides guidelines\non establishing and operating an enterprise architecture capability within an\nenterprise, including processes, skills, roles, and responsibilities.\n", "page": 187, "type": "text", "section": "Page 187"}
{"text": "Designing Software Architectures\nChapter 5\n[ 159 ]\nPhases of the ADM\nThe architecture development method consists of multiple phases. There is a preliminary\nphase in which the organization prepares for a successful software architecture\nimplementation. After this preliminary phase, there are eight phases to the process:\n", "page": 188, "type": "text", "section": "Page 188"}
{"text": "Designing Software Architectures\nChapter 5\n[ 160 ]\nEach phase continuously checks with requirements to ensure that they are being met.\nOrganizations can modify or extend the process to meet their needs, and it is usable with\nthe deliverables of other frameworks if it is decided that those deliverables are more\nsuitable.\nPhase A \u2013 Architecture vision\nIn this step of the ADM, the team defines the overall vision for the enterprise architecture,\nincluding its capabilities and business value. The team agrees on items such as scope,\nbusiness goals, business drivers, constraints, requirements, roles, responsibilities, and\nscheduling. These decisions are documented in the Statement of Architecture Work, which\nis a deliverable for this phase. The document typically contains the following:\nArchitecture project request and background information\nProject description and scope of the architecture\nAn overview of the architecture vision\nChange of scope procedures\nRoles, responsibilities, and deliverables for the project\nDetails on the acceptance criteria and procedures\nProject plan and schedule\nPhase B \u2013 Business architecture\nBusiness architecture is one of the four architecture domains defined in TOGAF. The\nbusiness architecture focuses on the business and/or service strategies of the organization,\nalong with its business environment. An understanding of the business architecture is a\nprerequisite to perform architecture work on the other three domains defined in TOGAF\n(data, application, and technology).\nThe goal of this phase is to determine the target business architecture for how the enterprise\nachieves its business objectives and its strategic drivers. In order to create a roadmap on\nhow to reach the target state, the following four steps are undertaken:\nGain an understanding of the current state of the architecture\n1.\nRefine and validate the target state of the architecture\n2.\nDetermine the gap that exists between the current and target states of the\n3.\narchitecture\nCreate a roadmap to transition between the current and target architecture states\n4.\n", "page": 189, "type": "text", "section": "Page 189"}
{"text": "Designing Software Architectures\nChapter 5\n[ 161 ]\nPhase C \u2013 Information systems architectures\nData and application architecture are two of the other architecture domains defined in\nTOGAF. The data architecture focuses on an organization's data and how it is managed,\nwhile application architecture involves the enterprise's software applications.\nThe results from the architecture vision and business architecture phases are used to\ndetermine the architectural changes that will be necessary to an enterprise's data and\napplication architectures.\nAs was the case with Phase B, the current and target states of the architecture are compared\nso as to determine the gap between the two. This allows for an architecture roadmap to be\ncreated for the candidate application and data components that will be needed to bridge the\ngap.\nPhase D \u2013 Technology architecture\nTechnology architecture, one of the other architecture domains defined in TOGAF, involves\nthe enterprise's infrastructure components. This includes the hardware and software\nnecessary to support the enterprise's business, data, and application architectures.\nThe goal of this phase is to develop the target technology architecture that will support the\nenterprise's solutions. An assessment of the enterprise's current infrastructure capabilities is\ncompleted and compared with the desired target state so that the gap between them can be\nidentified. From there, a roadmap of the target state for the technology architecture can be\ncreated along with the candidate components.\nPhase E \u2013 Opportunities and solutions\nThis phase focuses on how to deliver the target architecture as we move from a conceptual\nview of the target architecture toward implementation. The roadmaps created in Phase B,\nPhase C, and Phase D are consolidated into an overall architecture roadmap. The candidate\nsolutions that were created in the previous phases are organized into high-level candidate\nwork packages.\nThe overall architecture roadmap, which includes all of the gaps between the current and\ntarget states of the architecture, is used to determine the best approach on how to deliver\nthe target architecture.\nIf an incremental approach is to be taken, transition architectures are identified so that\nbusiness value continues to be delivered.\n", "page": 190, "type": "text", "section": "Page 190"}
{"text": "Designing Software Architectures\nChapter 5\n[ 162 ]\nPhase F \u2013 Migration planning\nThe overall architecture roadmap and the candidate work packages are used to plan the\nimplementation of the architecture. The software architect works with the enterprise's\nproject and program management teams to determine existing or new projects that can be\nused for the work.\nAn enterprise's existing processes for change and project management can be used to plan\nthe necessary initiatives.\nPhase G \u2013 Implementation governance\nImplementation governance, along with the next phase, architecture change management,\nruns in parallel with the implementation of the architecture. The development of the\narchitecture takes place using the enterprise's existing software development process.\nThis phase ensures that software architects stay engaged during implementation by\nassisting and reviewing the development work. Software architects must ensure that the\narchitecture being implemented is achieving the architecture vision.\nPhase H \u2013 Architecture change management\nDuring implementation, issues may arise that require decisions to be made. It may be\nfound that changes are necessary to the candidate solutions.\nSoftware architects are involved with the enterprise's change management process to make\ndecisions regarding proposed changes. Changes to the architecture must be managed and\nthere must be a continuous focus on ensuring that the architecture meets requirements and\nstakeholder expectations.\nTracking the progress of the software\narchitecture's design\nDuring the software architecture design process, you will want to keep track of the design's\nprogress. Keeping track of progress enables you to know how much of the design work is\ncomplete, and how much of it remains. The remaining work can be prioritized, assisting\nsoftware architects in determining what should be worked on next. In addition to tracking\nprogress, it serves as a reminder of design issues that are still outstanding, so that nothing\nis forgotten.\n", "page": 191, "type": "text", "section": "Page 191"}
{"text": "Designing Software Architectures\nChapter 5\n[ 163 ]\nThe technique that management will want to use to track progress really depends on your\nproject, software development methodology, and the organization. If you are using an agile\nmethodology such as Scrum, you may be using product and sprint backlogs to track\nprogress.\nUsing a backlog to track the architecture design\nprogress\nA product backlog contains a complete list of the features and bugs of the product. You\nshould consider creating a product backlog that is specific to the software architecture of\nthe project, which is separate from the other product backlog. An architecture product\nbacklog would contain items, design issues, design decisions that need to be made, and\nideas that are specific to the architecture design.\nPrior to each sprint, sprint planning takes place. The team selects items from the product\nbacklog that they will work on and complete during the upcoming sprint. Once tasks can\nbe created for the product backlog items, they can be assigned to a resource and then\ntracked for progress. Items from the product backlog that get selected for a sprint are then\nmoved to the sprint backlog. Once an item is completed, it can be removed from the\nbacklog.\nBefore sprint planning takes place though, the product backlog items should be prioritized,\nas the prioritization may affect what gets selected for a particular sprint.\nPrioritizing the backlog\nThe list of features and bugs in the product backlog should be prioritized by the team,\nwhich assists with project planning so that teams know which items to focus on first.\nBacklog prioritization is not something that occurs just once. As the architecture backlog\nchanges, priorities may need to change as well. You can revisit the prioritization of\narchitecture backlog items as many times as necessary.\nProduct backlog items should be linearly ordered based on criteria. One set of criteria that\nhas been used in practice to prioritize backlog items is called the DIVE criteria.\n", "page": 192, "type": "text", "section": "Page 192"}
{"text": "Designing Software Architectures\nChapter 5\n[ 164 ]\nDIVE criteria\nDIVE is an acronym that stands for the types of criteria that are used to prioritize product\nbacklog items. It focuses on Dependencies, to Insure against risks, business Value, and\nestimated Effort as the factors used to determine priority.\nDependencies\nSome product backlog items will be dependent on others, and therefore those dependencies\nwill need to be completed first. For example, if item A depends on item B, B would be\nprioritized higher than item A.\nInsure against risks\nWhen prioritizing backlog items, you want to insure against risks, which include both\nbusiness and technical risks. Taking potential risks into consideration may lead the team to\nprioritize a backlog item higher or lower when compared to other backlog items.\nBusiness value\nThe business value of a product backlog item is an important criterion for prioritization.\nProduct backlog items with greater levels of business value may be deemed a higher\npriority. The input of relevant stakeholders can help to determine the business value of a\nproduct backlog item.\nEstimated e\ufb00ort\nThe estimated level of effort for a product backlog item may be a factor when prioritizing\nwork. This may be due to factors such as scheduling or resource availability. There may be\ncases where a product backlog item has a large estimated effort, and the team wants to\ntackle the item sooner rather than later to ensure that it will be completed in time.\nActive and dynamic architecture backlogs\nAs with any product backlog, the architecture backlog is not static and will evolve as the\narchitecture design takes place. As architecture design iterations are completed, new\narchitectural drivers may be uncovered, necessitating the need for new items to be added to\nthe backlog.\n", "page": 193, "type": "text", "section": "Page 193"}
{"text": "Designing Software Architectures\nChapter 5\n[ 165 ]\nAnother reason that items may be added to the architecture backlog is when issues are\ndiscovered with the architecture. When the design is reviewed, a problem may become\napparent, requiring further work to be done.\nAs architectural design decisions are made, it may cause the creation of new architecture\nbacklog items. When a design decision is made, new concerns may arise from that decision.\nFor example, if it is decided that the application will be a web application, backlog items\nrelated to security, session management, and performance that are specific to web\napplications may need to be added to the architecture backlog if they did not already exist.\nChanges to the architecture backlog may prompt you to revisit the priorities of the backlog\nitems.\nThe architecture backlog should be made available to anyone who may need to be aware of\nthe design's progress. If you do have separate backlogs for the architecture and the rest of\nthe project, keep in mind that the audience for the two backlogs may be different. It really\ndepends on the project and the level of involvement and transparency that exists between\nthe project team and other stakeholders involved with the project.\nIn some cases, clients may have access to the product backlog to track functionality, but the\nteam may want to keep the architecture backlog private.  \nSummary\nSoftware architecture design plays a critical part in the creation and success of software\narchitectures. At its core, architecture design involves making design decisions to produce\nsolutions to design problems. The result is an architecture design that can be validated,\nformally documented, and eventually used by development teams.\nThere are two main approaches to architecture design, the top-down and bottom-up\napproaches. We examined situations in which one would be used over the other and\nlearned how a combination of the two approaches often works best.\nArchitectural drivers, which are the inputs into the architecture design process, guide the\narchitecture design. They include design objectives, primary functional requirements,\nquality attribute scenarios, constraints, and architectural concerns.\nDesigning a software architecture can be challenging, but we can leverage design concepts,\nsuch as software architecture patterns, reference architectures, tactics, and externally\ndeveloped software, to assist with the design of solutions.\n", "page": 194, "type": "text", "section": "Page 194"}
{"text": "Designing Software Architectures\nChapter 5\n[ 166 ]\nWhile formal documentation of an architecture does not need to occur during the design\nprocess, documenting, such as sketching the design and recording the design rationale,\nshould take place.\nFollowing an architecture design process helps to guide software architects with their\ndesign. There are a number of architecture design processes that are available to use, so\nyou'll have to do some research in order to select a process that will work best for your\nproject. Architecture design processes can be modified and supplemented with other\ntechniques and processes to fill in any gaps with the process that you want to use.\nA way of prioritizing and tracking the progress of architecture work should be put into\nplace, such as having a backlog specific to architecture.\nIn the next chapter, we will explore some of the principles and best practices of software\ndevelopment. Some of them can be applied to software architecture, while others may be\nconcepts that you will want to communicate to your team and encourage them to use in\ntheir implementations.\n", "page": 195, "type": "text", "section": "Page 195"}
{"text": "6\nSoftware Development\nPrinciples and Practices\nOne of the main goals for software architects is to design high-quality software\napplications. There are a number of software design principles and best practices that can\nbe applied to achieve that goal.\nSoftware architects can apply these principles and practices when designing software\narchitectures and encourage developers to use them in their implementations. These\nprinciples and practices are used to improve quality, simplify maintenance, increase\nreusability, find defects, and make software systems easier to test.\nIn this chapter, we will cover the following topics:\nDesigning orthogonal software systems, including a focus on loose coupling and\nhigh cohesion\nMinimizing complexity in a software system by following principles such as\nKISS, DRY, information hiding, YAGNI, and Separation of Concerns (SoC)\nThe SOLID design principles, which include the Single Responsibility Principle\n(SRP), Open/Closed Principle (OCP), Liskov Substitution Principle (LSP),\nInterface Segregation Principle (ISP), and the Dependency Inversion Principle\n(DIP)\nUsing Dependency Injection (DI) to provide dependencies to a class\nUsing unit testing to improve the quality of a software system\nEnsuring that development environments can be set up easily\nPractice of pair programming\nReviewing deliverables, such as code reviews, formal inspections, and\nwalkthroughs\n", "page": 196, "type": "text", "section": "Page 196"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 168 ]\nDesigning orthogonal software systems\nIn geometry, two Euclidean vectors are orthogonal if they are perpendicular (form a right\nangle of 90 degrees). The two vectors meet at the origin point, but do not intersect. The two\nvectors are independent of each other:\nSoftware that is well designed is orthogonal in that its modules are independent of each\nother. Ideally, changes to one module in a software system should not require changes to\nanother module. Software systems will undergo many changes during their lifetime and\ndesigning them with this in mind provides a number of benefits, including increased\nproductivity for those who work on them and lowered risk of introducing defects when\nchanges are made. Designing orthogonal systems may have higher upfront costs, but over\ntime, a highly maintainable and extendable system will be worth it.\nOrthogonal systems are designed so that their elements are loosely coupled and highly\ncohesive. Let's look at the concepts of coupling and cohesion in more detail.\nLoose coupling\nCoupling is the degree to which a software module depends on another software module.\nCoupling between modules is a measure of how closely connected they are, and it can\neither be loose, sometimes described as low or weak, or it can be tight, sometimes referred\nto as high or strong. The degree of coupling between modules reflects the quality of their\ndesign.\nSoftware modules that are tightly coupled are more complex, which decreases their\nmaintainability. Tight coupling makes modifying the code more difficult because a change\nin a tightly coupled module will likely require changes in other modules. This introduces a\nhigher degree of risk as there is a greater likelihood that a new defect could be introduced if\na software module is modified.\n", "page": 197, "type": "text", "section": "Page 197"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 169 ]\nIt is also easy to engage in parallel development if the code is loosely coupled. One\ndeveloper can work on one part of the application independent of another developer who\nis working on a different part of the application.\nModules that are loosely coupled do not have as many dependencies with other modules.\nChanges to tightly coupled modules will take more time and effort due to the\ninterdependencies with other modules. As the number of modules that are affected by a\nchange increases, it will take longer for developers to make the modifications, and for\ntesters to test the changes. Tight coupling also reduces reusability. It is harder to reuse a\nmodule when dependent modules must be included in the reuse.\nTypes of coupling\nThere are different types of coupling. The following are details regarding those types, in\norder of the tightest (least desirable) to loosest (most desirable) coupling. It should be noted\nthat it is possible for two modules to be coupled in more than one way. In those situations,\nthe coupling type is determined by the worst, or tightest, coupling type.\nContent coupling\nContent coupling is the highest type of coupling. It is considered so bad that it is also\nreferred to as pathological coupling. It occurs when one module directly references the\ninternal or private information in another module. For example, it exists when one module\naccesses or changes private data in another module.\nModules should never be designed to have this type of coupling. If modules have content\ncoupling, they should be refactored so that there is a proper level of abstraction. The\nmodules should not directly rely on the internal workings of each other.\nCommon coupling\nCommon coupling, also known as global coupling, is a high level of coupling. This type of\ncoupling is highly undesirable. Although sometimes it is unavoidable, modules should be\ndesigned to minimize the existence of this type of coupling.\nModules exhibit common coupling when they share the same global data, such as a global\nvariable. It is perfectly acceptable to share configuration data throughout an application.\nHowever, as a general rule, if you are going to use other types of global data, it is better to\nuse something that has a fixed value, such as a constant, rather than a variable whose value\ncan vary at runtime.\n", "page": 198, "type": "text", "section": "Page 198"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 170 ]\nExternal coupling\nExternal coupling is another type of high coupling. It exists when multiple modules share\nthe same part of an environment that is external to the software. This could come in the\nform of having to use an external data format, interface, communication format, tool, or\ndevice.\nSometimes, external dependencies are imposed and unavoidable, but we should still seek\nto limit the number of modules that have those dependencies. Doing so will ensure that if\nthe external dependency changes, only a limited number of modules are affected.  \nControl coupling\nControl coupling is a moderate type of coupling. Two modules exhibit control coupling\nwhen one module controls the internal logic of the other by passing it information. An\nexample of this is when a module passes a control flag to another module, which uses it to\ncontrol its flow.\nThis type of coupling may be acceptable, but an effort should be made to make it known\nthat the coupling exists so that the modules can be tested together. It is beneficial to detect\nany problems with either of the modules earlier rather than later.\nStamp coupling (data-structured coupling)\nStamp coupling is a fairly low type of coupling. It is also known as data-structure coupling\nbecause it occurs when modules share a composite data structure. By composite data\nstructure, we mean that it is data that has some internal structure to it, such as a record.\nWhen a composite data structure is shared between two modules, some of the fields in the\ndata structure may not even be used. For example, a module passes a composite data\nstructure to another module, which then just uses one field in it.\nIt is similar to data coupling, except that the data shared is a composite data type rather\nthan primitive data values and that not all of the values shared may be used.\nData coupling\nData coupling occurs when two modules share some data which are just primitive data\nvalues. It is another low type of coupling. A common type of data coupling is when a\nmodule calls a method on another module, and inputs and outputs are shared in the form\nof method parameters and the return value.\n", "page": 199, "type": "text", "section": "Page 199"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 171 ]\nWhen two modules need to interact, this is a common and acceptable type of coupling.\nUnlike stamp collecting, where some of the values in the shared composite data structure\nmay not be used, all of the parameters in data coupling are used. If any parameters are not\nneeded, they should be removed.\nMessage coupling\nModules exhibit message coupling when one module calls a method on another and does\nnot send any parameters. The only coupling is on the name of the method, but nothing else.\nIt is the lowest type of coupling.\nNo coupling\nThere are, of course, situations where there is no coupling between modules. This is when\ntwo modules have no direct communication at all. It is an ideal that allows the two modules\nto be implemented, tested, and maintained independently. \nThe Law of Demeter (LoD) / principle of least knowledge\nThe Law of Demeter (LoD), or principle of least knowledge, is a design principle related\nto loose coupling. In order to minimize coupling between software modules, the principle\ncan be followed when designing software.\nThe principle follows the only talk to your friends idiom, which keeps coupling loose by\nlimiting a module's communication with other modules. Ideally, a method should only call\nother methods in the same object, in objects that were passed into it, in direct component\nobjects, in objects that it created/instantiated, or in objects in a global variable that are\naccessible.\nAnother one of the tenets of LoD is that a software module should know as little as possible\nabout other modules. This will ensure its independence from other modules, allowing\ncoupling to remain loose. See the Information hiding section for a principle that helps to\nachieve this goal.\nDesigning for loose coupling\nDuring designs and implementations, your goal as a software architect should be to\nminimize the amount of coupling that exists between modules. Modules should be\ndesigned to be as independent as possible.\n", "page": 200, "type": "text", "section": "Page 200"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 172 ]\nCoupling can be reduced by eliminating or reducing the number of unnecessary\ndependencies. For any coupling that must exist, it should be the lowest type that is\nnecessary. Loose coupling reduces complexity and increases maintainability and\nreusability.\nCoupling typically affects the level of cohesion, so that loose coupling correlates with high\ncohesion and tight coupling correlates with low cohesion.\nHigh cohesion\nCohesion is the degree to which the elements inside a module belong together. It is the\nstrength of the relationships of elements within a module, and how united they are in their\npurpose. Cohesion is a qualitative measure of the consistency of purpose within a module.\nThere are different types of cohesion, and those that reflect a higher level of cohesion are\npreferable. Highly cohesive modules have a single, well-defined purpose, and reflect a\nbetter quality of design.\nSoftware modules with low cohesion are harder to maintain. If a module contains multiple\nunrelated functions, changes to it are more likely to require changes in other modules. This\nwill require extra time and effort, not just in development, but also testing. The extra\ncomplexity in modules with low cohesion make it more likely that defects may be\nintroduced when they are modified. They may also be harder to understand, making them\nmore difficult to modify.\nReusability is lessened for modules with low cohesion. Modules with low cohesion,\nperforming many disparate functions, are less likely to be reused for other purposes. A\nmodule that works together as a logical unit with a clear purpose is more likely to be\nreused.\nTypes of cohesion\nThe level of cohesion in a module is represented by the type of cohesion. Let's examine the\ndifferent types of cohesion, from lowest (least desirable) to highest (most desirable).\nCoincidental cohesion\nCoincidental cohesion occurs when elements in a module are grouped arbitrarily. There is\nno relationship among the different elements, making it the lowest (worst) type of cohesion.\nSometimes, you will see this type of cohesion in a utilities or helpers class where a number of\nunrelated functions have been placed together.\n", "page": 201, "type": "text", "section": "Page 201"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 173 ]\nCoincidental cohesion should be avoided and, if it is encountered in a module, the module\nshould be refactored. Each part of the module should be moved to an existing or new \nmodule where it would make logical sense for it to exist.\nLogical cohesion\nModules exhibit logical cohesion when elements are grouped together because they are\nrelated in some way logically. Even though the functionality of logically cohesive modules\nmight be of the same general category, they may be different in other ways. For this reason,\nthis type of cohesion is considered low. While better than coincidental cohesion, these types\nof modules are not very cohesive.\nAn example of logical cohesion would be a module that contains a set of functions that\nhandles I/O for the application. While they are related logically, the nature of the various\nfunctions would be quite different. They would be more cohesive if each type of I/O was\nhandled by a separate module.    \nTemporal cohesion\nTemporal cohesion exists when the elements of a module are grouped together based on\nwhen they are processed. This can occur when different elements are grouped together\nsimply because they need to be executed at a single moment in time. This is another type of\nlow cohesion.\nAn example of temporal cohesion is grouping a bunch of elements together because they\nare all related to system startup, system shutdown, or the handling of a system error. Even\nthough the elements are related temporally, they are only weakly related to each other. This\nmakes the module harder to maintain and reuse.\nThe elements should be grouped into different modules, with each module designed for a\nsingle purpose.\nProcedural cohesion\nA module exhibits procedural cohesion when its elements have been grouped together\nbecause they always execute in a particular sequence. For example, payment processing for\na customer placing an order might involve the following steps being executed in a\nparticular sequence:\nGathering payment information\nValidating payment method details\nChecking whether funds are available or whether there is enough available credit\n", "page": 202, "type": "text", "section": "Page 202"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 174 ]\nPersisting the order in a database\nChecking inventory levels\nCreating a back order or canceling an order based on inventory\nSending the order for fulfillment\nSending an email confirmation to the customer\nAlthough the various parts are all related by the order of execution, some of the individual\nactivities are quite distinct from each other.\nThis type of cohesion is considered moderate. Although it is an acceptable level of cohesion,\nit is not ideal. If possible, refactoring can be performed to improve the level of cohesion.\nCommunicational cohesion\nCommunicational cohesion occurs when parts of a module are grouped together because\nthey use the same set of inputs and outputs. If a module has different elements that have\nbeen grouped together because they access and modify the same data structure, it would\ndemonstrate communicational cohesion.\nFor example, a data structure that represents the contents of a customer's shopping basket\nmight be used by a variety of elements in a single module. The elements might calculate\ndiscounts, shipping, and taxes based on the same data structure.\nThis level of cohesion is moderate and usually considered acceptable.\nSequential cohesion\nSequential cohesion exists when the different parts of a module are grouped together\nbecause the output of one part serves as the input for another part. Modules of this type\nhave a moderate level of cohesion.\nAn example of a module that is sequentially cohesive would be one that is responsible for\nformatting and validating a file. The output of an activity that formats a raw record\nbecomes the input for an activity that then validates the fields in that record.\nFunctional cohesion\nFunctional cohesion occurs when elements of a module are grouped together because they\nare united for a single, well-defined purpose. All of the elements in the module work\ntogether to fulfill that purpose. Functional cohesion in a module is ideal and is the highest\ntype of cohesion.\n", "page": 203, "type": "text", "section": "Page 203"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 175 ]\nFunctional cohesion promotes the reusability of a module and makes it easier to\nmaintain. Examples of functionally cohesive modules include one that is responsible for\nreading a particular file and one that is responsible for calculating shipping costs for an\norder.\nDesigning for high cohesion\nSoftware architects should design modules to have high cohesion. Each module should\nhave a single, well-defined purpose. The elements contained in the module should be\nrelated and contribute to that purpose.\nIf there are auxiliary elements contained in a module that are not directly related to the\nmain purpose, consider moving them to either a new module or an existing module that\nhas the same purpose of the element being moved.\nCohesion and coupling are related in that high cohesion correlates with loose coupling and\nlow cohesion correlates with tight coupling.\nMinimizing complexity\nBuilding software is inherently complex and a number of problems result from complexity.\nHigher levels of complexity in software:\nCause delays in schedules\nLead to cost overruns\nMay cause the software to behave in unintended ways or lead to an\nunanticipated application state\nMay create security loopholes or prevent security issues from being discovered\nin a timely fashion\nAre a predictive measure of lower levels of some quality attributes, such as lower\nmaintainability, extendibility, and reusability\nIn The Mythical Man-Month, Fred Brooks divides the problems facing software engineering\ninto two categories, essential and accidental:\n\"All software construction involves essential tasks, the fashioning of the complex\nconceptual structures that compose the abstract software entity, and accidental tasks, the\nrepresentation of these abstract entities in programming languages and the mapping of\nthese onto machine languages within space and speed constraints.\"\n", "page": 204, "type": "text", "section": "Page 204"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 176 ]\nAccidental difficulties are problems that are just inherent to the production of software in\ngeneral. They are problems that software engineers can fix and may not even be directly\nrelated to the problem they are trying to solve. Improvements in programming languages,\nframeworks, design patterns, integrated development environments (IDEs), and software\ndevelopment methodologies are just some examples of progress over the years in\neliminating or reducing accidental difficulties.\nEssential difficulties are the core problems that you are trying to solve and they can't\nsimply be removed to reduce complexity. Software development teams spend more time on\nessential complexities than accidental ones.\nWe try to manage and minimize the complexity, whether it is accidental or essential. As it\nhas probably become apparent by now, a recurring theme in this book is the importance of\nmanaging and minimizing complexity. It has a direct relationship with the quality of the\nsoftware and is therefore a major focus for software architects.\nMinimizing complexity in software helps to eliminate or manage both accidental and\nessential difficulties. Some of the principles related to minimizing complexity include KISS,\nDRY, information hiding, YAGNI, and SoC.\nKISS principle \u2013 \"Keep It Simple, Stupid\"\nThe KISS principle, which is an acronym for Keep It Simple, Stupid, has been used in\nnumerous contexts to convey the idea that systems generally work best if they are kept\nsimple. The principle is applicable to the design of software systems. A development team\nshould strive to not overcomplicate their solutions.\nVariations of the acronym include Keep It Short, Simple, Keep It  Simple, Stupid, Keep It Simple,\nStraightforward, and Keep It Simple, Silly. All of them have the same basic meaning, which is\nto express the value of simplicity in designs.\nOrigin of KISS\nThe creation of the principle is typically credited to the late Kelly Johnson, who was an\naeronautical and systems engineer. Among other accomplishments, he contributed to\naircraft designs for the Lockheed Corporation (now known as Lockheed Martin after its\nmerger with Martin Marietta).\n", "page": 205, "type": "text", "section": "Page 205"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 177 ]\nThough the principle is commonly communicated as Keep It Simple, Stupid,\nKelly's original version didn't have a comma in it. The word Stupid wasn't\nintended to refer to a person.\nKelly introduced the principle by explaining to the engineers that the jet aircraft they were\ndesigning needed to be something that a man in the field could fix with basic training and\ncommon tools. The design needed to satisfy this requirement, which is understandable\ngiven that the aircraft might need to be repaired quickly in a combat situation.\nApplying KISS to software\nSimplicity is a highly desirable quality in software systems, and this includes their designs\nas well as their implementations. Making software more complicated than it needs to be\nlowers its overall quality. Greater complexity reduces maintainability, hinders reusability,\nand may lead to an increase in the number of defects.\nSome ways to follow the KISS principle in software include:\nEliminating duplication as much as possible (see the DRY \u2013 \"Don't Repeat\nYourself\" section)\nEliminating unnecessary features (see the YAGNI \u2013 \"You Aren't Gonna Need It\"\nsection)\nHiding complexity and design decisions (see the Information hiding section)\nFollowing known standards when possible and minimizing deviations and\nsurprises\nEven after a module is implemented, if you see a method or class that could be made\nsimpler, consider refactoring it if you have the opportunity.\nDon't make it overly simple\nIn a quest for simplicity, we cannot oversimplify a design or implementation, though. If we\nreach a point that it negatively affects the ability to deliver on required functionality or \nquality attributes, we have gone too far.\nKeep the following quote, attributed to Albert Einstein, in mind when designing software:\nEverything should be made as simple as possible, but not simpler.\n", "page": 206, "type": "text", "section": "Page 206"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 178 ]\nDRY \u2013 \"Don't Repeat Yourself\"\nThe DRY principle stands for Don't Repeat Yourself and strives to reduce duplication in a\ncodebase. Duplication is wasteful and makes a codebase unnecessarily larger and more\ncomplex. This makes maintenance more difficult. When code that has been duplicated\nneeds to be changed, modifications are required in multiple locations. If the changes\napplied everywhere are not consistent, defects may be introduced. Software architects and\ndevelopers should avoid duplication whenever possible.\nWhen a design violates the DRY principle, it is sometimes referred to as a WET (Write\nEverything Twice) solution (or Waste Everyone's Time or We Enjoy Typing).\nCopy-and-paste programming\nIn poorly written codebases, code duplication often results from copy-and-paste\nprogramming. This happens when a developer needs the exact same or very similar logic,\nwhich exists somewhere else in the system, so they duplicate (copy and paste) the code.\nThis violates the DRY principle and lowers the quality of the code.\nCopy-and-paste programming can sometimes be acceptable and serve a useful purpose.\nCode snippets, which are small blocks of reusable code, may speed up development. Many\nIDEs and text editors provide snippet management to make the use of snippets easier for\ndevelopers. However, beyond the appropriate application of snippets, it is usually not a\ngood idea to copy and paste your application code in multiple places.\nMagic strings\nMagic strings are strings that appear directly in your code. Sometimes, these strings are\nneeded in multiple places and are duplicated, violating the DRY principle. Maintenance of\nthese strings can become a nightmare because if you want to change the value of the string,\nyou have to change it in multiple places. The problem is exacerbated when the string is\nused, not just in multiple places within the same class, but within multiple classes.\nThere are many examples of magic strings, from exception messages, settings in\nconfiguration files, parts of a file path, or a web URL. Let's look at an example where the\nmagic string value represents a cache key. This serves as a good example because this is a\ncase where a magic string might be duplicated multiple times within the same class and\neven within the same method:\npublic string GetFilePath()\n{\n    string result = _cache.Get(\"FilePathCacheKey\");\n", "page": 207, "type": "text", "section": "Page 207"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 179 ]\n    if (string.IsNullOrEmpty(result))\n    {\n        _cache.Put(\"FilePathCacheKey\", DetermineFilePath());\n        result = _cache.Get(\"FilePathCacheKey\");\n    }\n    return result;\n}\nThis key is repeated multiple times, increasing the possibility of a typo resulting in a defect.\nIn addition, if we ever want to change the cache key, we will have to update it in multiple\nplaces.\nTo follow the DRY principle, let's refactor this code so that the cache key is not repeated.\nFirst, let's declare a constant at the class level for the magic string:\nprivate const string FilePathCacheKey = \"FilePathCacheKey\";\nNow, we can use that constant in our GetFilePath method:\npublic string GetFilePath()\n{\n    string result = _cache.Get(FilePathCacheKey);\n    if (string.IsNullOrEmpty(result))\n    {\n        _cache.Put(FilePathCacheKey, DetermineFilePath());\n        result = _cache.Get(FilePathCacheKey);\n    }\n    return result;\n}\nNow, the string is declared in just one location. If you are going to place a magic string in a\nconstant, you should think about where the constant should be declared. One consideration\nis the scope of its use. It may be appropriate to declare it within the scope of a particular\nclass, but in some cases, a broader or narrower scope will make more sense.\nAlthough placing a magic string in a constant is a good technique for a variety of situations,\nit is not always ideal. This decision also depends on the type of string and its purpose. For\nexample, if the string is a validation message, you might want to place it in a resource file.\nIf there are any internationalization requirements, placing translatable strings, such as\nvalidation messages, in a resource file will facilitate translating the messages into different\nlanguages.\n", "page": 208, "type": "text", "section": "Page 208"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 180 ]\nHow to avoid duplication\nDRYness can be achieved by being mindful and taking action when appropriate. If you find\nyourself copying and pasting code, or simply writing code that is identical or similar to\nexisting code, think about what you are trying to accomplish and how it can be made\nreusable.\nDuplication in logic can be eliminated by abstraction. This concept is referred to as the\nabstraction principle (or the principle of abstraction). The principle is consistent with the\nDRY principle and is a way to reduce duplication. The code that is needed in multiple\nplaces should be abstracted out, and the locations that need it can then be routed through\nthe abstraction. Some refactoring may be necessary to make it generic enough to be reused,\nbut it is worth the effort. Once the logic is centralized, if it needs to be modified in the\nfuture, perhaps to fix a defect or to enhance it in some way, you will be able to make the\nchanges in a single location.\nAs we saw in the case of magic strings, duplication with values can be eliminated by\nplacing the value in a central location, such as the declaration of a constant.\nIf there is duplication in a process, it may be possible to reduce it through automation.\nManual unit testing, builds, and integration processes can be eliminated by automating\nthose processes. The automation of tests and builds will be discussed further in Chapter\n13, DevOps and Software Architecture.\nDon't make things overly DRY\nWhen attempting to follow the DRY principle, be careful not to consolidate disparate items\nthat just happen to be duplicates in some way. If two or more things are duplicates, it may\nbe that they are just coincidentally repetitive.\nFor example, if two constants have the same value, that does not mean they should be\ncombined into one constant for the sake of eliminating duplication. If the constants\nrepresent distinct concepts, they should remain separate.\nInformation hiding\nInformation hiding is a principle that advocates for software modules to be designed such\nthat they hide implementation details from the rest of the software system. The idea of\ninformation hiding was introduced by D.L. Parnas in On the Criteria to Be Used in\nDecomposing Systems into Modules, which was published in 1972.\n", "page": 209, "type": "text", "section": "Page 209"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 181 ]\nInformation hiding decouples the internal workings of a module from the places in the\nsystem that call it. The details of a module that do not need to be revealed should be made\ninaccessible. Information hiding defines constraints related to what properties and\nbehaviors can be accessed. Callers interact with the module's public interface and are\nprotected from the implementation details. \nThere are a number of reasons to abide by the principle of information hiding.\nReasons for information hiding\nInformation hiding is useful at all levels of design. Only exposing the details that need to be\nknown reduces complexity, which improves maintainability. Unless you are specifically\ninterested in the internal details, you do not need to concern yourself with them.\nAnother one of the key reasons for information hiding is to hide design decisions from the\nrest of the software system. This is particularly beneficial if the design decision might\nchange. By hiding a design decision, if the decision needs to be changed, it minimizes the\namount and extent of the modifications that will be necessary. It provides the flexibility to\nmake changes later if it is necessary to do so.\nWhether the design decision is to use a particular API, represent data in a certain way, or\nuse a particular algorithm, the modifications necessary to change that design decision\nshould be kept as localized as possible.\nWhat needs to be exposed/hidden?\nYou and your team should really think about the properties and behaviors (methods) that\nneed to be exposed for a module. Everything else can be hidden. Through the use of a\npublic interface, we can define what we want to make available.\nInformation hiding assists with defining public interfaces. Rather than lazily exposing most\nof a class, it forces us to consider what really needs to be made public. The public interface\ndefines a contract that the implementation must follow, and allows others to know what is\navailable. It is up to the implementation to decide how it is accomplished.\n", "page": 210, "type": "text", "section": "Page 210"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 182 ]\nYAGNI \u2013 \"You Aren't Gonna Need It\"\nYAGNI, which stands for You Aren't Gonna Need It, or You Ain't Gonna Need It, is a\nprinciple from the software development methodology of Extreme Programming (XP). XP\nis one of the first agile methods and was the dominant one until the rise of the popularity of\nScrum. YAGNI is similar to the KISS principle in that they both aim for simpler solutions,\nwith YAGNI focusing on a specific aspect, which is the removal of unnecessary\nfunctionality and logic.\nAvoid over-engineering a solution\nThe idea behind YAGNI is that you should only implement functionality when you need it\nand not just because you think you may need it some day. Ron Jeffries, one of the co-\nfounders of XP, once said:\n\"Always implement things when you actually need them, never when you just foresee that\nyou need them.\"\nFollowing the YAGNI principle helps you to avoid over-engineering a solution. You don't\nwant to spend time on future scenarios that are unknown. The problem with implementing\na feature that you think might eventually be needed is that quite often the feature ends up\nnot being needed or the requirements for it change.\nCode that is not written equates to time and money that is saved. Spending time and\nmoney on a feature you don't need takes away from time and money you could have spent\non something that you do need. Resources are finite, and using them on something that is\nunnecessary is a waste. As was the case with code duplication, adding unnecessary logic to\nan application increases its size and complexity, which reduces maintainability.\nSituations where YAGNI doesn't apply\nYAGNI applies to presumptive features, as in functionality that is not currently needed. It\ndoes not apply to code that would make the software system easier to maintain and modify\nlater. In fact, following YAGNI means you may be changing the system later to add a\nfeature, so the system should be well designed for this purpose. If a software system is not\nmaintainable, making changes later may be difficult.\nYou may come across times where, in hindsight, a change made sooner would have\nprevented more expensive changes later. This may be particularly true for software\narchitects if the change is architecture related. Design decisions made for architecture are\namong the earliest decisions made, and having to change them later can be costly.\n", "page": 211, "type": "text", "section": "Page 211"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 183 ]\nIt can sometimes be difficult to foresee which changes should have been made before they\nwere needed. However, for the most part, following YAGNI is beneficial. Even in the case\nof an architecture change, a good architecture design reduces complexity and makes it\neasier to make changes. It also makes it more likely that when a change is needed, it can be\nlimited in scope and may not even require architectural changes.\nAs software architects gain more experience, they become more adept at spotting\nexceptions to the YAGNI principle where a particular change should be made before it is\nneeded.\nSeparation of Concerns (SoC)\nConcerns are the different aspects of functionality that the software system\nprovides. Separation of Concerns (SoC) is a design principle that manages complexity by\npartitioning the software system so that each partition is responsible for a separate concern,\nminimizing the overlap of concerns as much as possible.\nFollowing the principle involves decomposing a larger problem into smaller, more\nmanageable concerns. SoC reduces complexity in a software system, which reduces the\neffort needed to make changes and improves the overall quality of the software.\nWhen the DRY principle is followed, and logic is not repeated, a SoC is usually a natural\nresult as long as the logic is organized properly.\nSoC is a principle that can be applied to multiple levels in a software application. At the\narchitecture level, software applications can follow a SoC by separating different logic such\nas user-interface functionality, business logic, and infrastructure logic. An example of an\narchitecture pattern that separates concerns at this level is the Model-View-Controller\n(MVC) pattern, which we will cover in the next chapter.\nWe can apply SoC at a lower level, such as with classes. If we were providing order\nprocessing functionality in a software system, the concern of validating credit card\ninformation shouldn't exist in the same place as the concern for updating inventory. They\nare distinct concerns that should not be placed together. At this level, it is related to the\nSingle Responsibility Principle, which we will discuss shortly.\nAn example of separating concerns by language in web programming is Hypertext\nMarkup Language (HTML), Cascading Style Sheets (CSS), and JavaScript. They\ncomplement each other with one being focused on the content of web pages, one for the\npresentation, and one for the behavior.\n", "page": 212, "type": "text", "section": "Page 212"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 184 ]\nFollowing SOLID design principles\nSOLID design principles focus on creating code that is more understandable, maintainable,\nreusable, testable, and flexible. SOLID is an acronym that represents five separate software\ndesign principles:\nSingle Responsibility Principle (SRP)\nOpen/Closed Principle (OCP)\nLiskov Substitution Principle (LSP)\nInterface Segregation Principle (ISP)\nDependency Inversion Principle (DIP)\nSoftware architects should be familiar with SOLID principles and apply them in their\ndesigns and implementations. They should realize, though, that the principles are\nguidelines, and while you should strive to follow them, you may not always be able to\naccomplish that fully. Use your judgement as to when, and to what degree, these principles\nshould be followed.\nNow, let's explore the five design principles that make up SOLID in more detail.\nSingle Responsibility Principle (SRP)\nThe Single Responsibility Principle (SRP) states that each class should have only one\nresponsibility, meaning it should do one thing and do that thing well. A responsibility is a\nreason to change, so each class should have only one reason to change. If we group together\nthe functions that need to change for the same reason, and separate out the things that\nchange for other reasons, we can create a class that follows this principle.\nIf a class has multiple responsibilities, there is a likelihood that it is used in a greater\nnumber of places. When one responsibility is changed, not only do we run a higher risk of\nintroducing defects into other responsibilities in the same class, but there is a greater\nnumber of other classes that might be impacted.\nBy following the single responsibility principle, if we need to change a particular\nresponsibility, that change would be located in a single class. This is the way to create an\northogonal software system.\nApplying this principle does not necessarily mean, as some posit, that each class should\nonly have a single public method. Although it does reduce the size of classes, the goal is to\nhave each class have a single responsibility. Fulfilling a single responsibility may require\nmultiple public methods.\n", "page": 213, "type": "text", "section": "Page 213"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 185 ]\nThis principle is related to the SoC principle because, as concerns are separated from each\nother, it facilitates the creation of classes that have a single responsibility. Following the\nDRY principle also helps us to abide by the SRP. By removing duplicate code and placing it\nin a single location so that it can be reused, the classes that need the logic do not have to\nrepeat it and therefore do not need to be responsible for it.\nLet's take a look at an example of the SRP. It is written in C#, although you will get the idea\neven if you do not use that particular programming language. We have an email service\nthat is responsible for sending out emails. There is a requirement to log information to a log\nfile, so it also contains logic to open, write to, and close a log file on a file system:\npublic class EmailService : IEmailService\n{\n    public SendEmailResponse SendEmail(SendEmailRequest request)\n    {\n        if (request == null)\n            throw new ArgumentNullException(nameof(request));\n        SendEmailResponse response = null;\n        try\n        {\n            // Logic to send email\n            // Log info about sent email\n            LogInfo(\"Some info message\");\n        }\n        catch (Exception ex)\n        {\n            // Log details about error\n            LogError(\"Some error message\");\n        }\n        return response;\n    }\n    private void LogInfo(string message)\n    {\n        // Logic to write to file system for logging\n    }\n    private void LogError(string message)\n    {\n        // Logic to write to file system for logging\n    }\n}\n", "page": 214, "type": "text", "section": "Page 214"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 186 ]\nAs you can see in this simple example, this class has more than one responsibility: sending\nout emails as well as handling the logging. This means that it has more than one reason to\nchange. If we wanted to change how emails were sent out, or allow for the logging to target\ncloud file storage instead of a file on a local file system, both would require changes to the\nsame class.\nThis violates the SRP. Let's refactor this class so that it is only responsible for one thing:\npublic class EmailService : IEmailService\n{\n    private readonly ILogger _logger;\n    public EmailService(ILogger logger)\n    {\n        if (_logger == null)\n            throw new ArgumentNullException(nameof(logger));\n        _logger = logger;\n    }\n    public SendEmailResponse SendEmail(SendEmailRequest request)\n    {\n        if (request == null)\n            throw new ArgumentNullException(nameof(request));\n        SendEmailResponse response = null;\n        try\n        {\n            // Logic to send email\n            // Log info about sent email\n            _logger.LogInfo(\"Info message\");\n        }\n        catch (Exception ex)\n        {\n            // Log details about error\n            _logger.LogError($\"Error message: {ex.Message}\");\n        }\n        return response;\n    }\n}\nNow, the EmailService class is only responsible for sending out emails. The logic for\nlogging has been abstracted out to an interface. This dependency is injected in through the\nclass's constructor, and the implementation will be responsible for how logging works.\n", "page": 215, "type": "text", "section": "Page 215"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 187 ]\nThis class is now only responsible for a single thing and therefore only has one reason to\nchange. Only changes related to the sending of emails will require modifications to this\nclass. It no longer violates the SRP.\nOpen/Closed Principle (OCP)\nThe Open/Closed Principle (OCP) states that software components, such as classes, should\nbe open for extension but closed for modification. When requirements change, the design\nshould minimize the amount of changes that need to occur on existing code. We should be\nable to extend a component by adding new code without having to modify existing code\nthat already works.\nWhen Dr. Bertrand Meyer first came up with the principle in his book Object Oriented\nSoftware Construction, it focused on using implementation inheritance as the solution. If new\nfunctionality is needed, a new subtype is created and the base class and any existing\nsubtypes could remain unchanged.\nSoftware engineer Robert C. Martin, popularly known as Uncle Bob, redefined the principle\nin his article The Open-Closed Principle, and later in his book Agile Software Development,\nPrinciples, Patterns, and Practices, by stressing the importance of abstraction and the use of\ninterfaces. Using interfaces, we can change implementations as needed. In this way, we can\nchange behavior without having to modify existing code that relies on the interfaces.\nLet's take a look at an example. In this program, we have a Shape class with Rectangle\nand Circle classes that inherit from it. A Canvas class has methods that allow us to draw\nthe shapes:\npublic class Canvas\n{\n    public void DrawShape(Shape shape)\n    {\n        if (shape is Rectangle)\n            DrawRectangle((Rectangle)shape);\n        if (shape is Circle)\n            DrawCircle((Circle)shape);\n    }\n    public void DrawRectangle(Rectangle r)\n    {\n        // Logic to draw a rectangle\n    }\n    public void DrawCircle(Circle c)\n", "page": 216, "type": "text", "section": "Page 216"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 188 ]\n    {\n        // Logic to draw a circle\n    }\n}\nIf a new request comes in that requires us to be able to draw a new shape, such as a\ntriangle, we will have to modify the Canvas class. This class has not been designed to be\nclosed for modification and violates the OCP.\nDevelopers will need to understand the Canvas class in order to add a new shape.\nModifications to the Canvas class will require unit tests to be revisited and introduces the\npossibility that existing functionality could be broken.\nLet's refactor this poor design so that it no longer violates the OCP:\npublic interface IShape\n{\n    void Draw();\n}\npublic class Rectangle : IShape\n{\n    public void Draw()\n    {\n        // Logic to draw rectangle\n    }\n}\npublic class Circle : IShape\n{\n    public void Draw()\n    {\n        // Logic to draw circle\n    }\n}\npublic class Canvas\n{\n    public void DrawShape(IShape shape)\n    {\n        shape.Draw();\n    }\n}\n", "page": 217, "type": "text", "section": "Page 217"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 189 ]\nThe Canvas class is now much smaller, with each shape now having its own\nimplementation of how to draw itself. If there was a requirement to add a new shape, we\ncan create a new Shape class that implements the IShape interface without having to make\nany changes to the Canvas class or any of the other shapes. It is now open for extension but\nclosed for modification.\nLiskov Substitution Principle (LSP)\nInheritance is one of the four pillars of object-oriented programming (OOP). It allows\nsubclasses to inherit from a base class (sometimes referred to as the parent class), which\nincludes the properties and methods of the base class. When you first learned about\ninheritance, you may have been taught about \"is a\" relationships. For example, \"Car is a\nVehicle,\" if Car was a base class and Vehicle was a subtype of that base class.\nThe Liskov Substitution Principle (LSP) is an object-oriented principle that states that\nsubtypes must be substitutable for their base types without having to alter the base type. If\na subtype is inherited from a base class, we should be able to substitute the subclass for that\nbase class without any issues. Subtypes extending a base class should do so without\nchanging the behavior of the base class. When the LSP is violated, it makes for confusing\ncode that is hard to understand.\nFor a given base class or an interface the base class implements, the subtypes of that base\nclass should be usable through the base class or an interface the base class implements. The\nmethods and properties of the base class should make sense and work as intended for all of\nthe subtypes. If the classes work without an issue and behave as expected, the subtypes are\nsubstitutable for the base class. When the LSP is violated, this is not the case. Although the\ncode may compile, unexpected behavior or runtime errors may be experienced.\nA classic example to illustrate the LSP is that of a Rectangle class and a Square class. In\ngeometry, a square is a type of rectangle, so every square is a rectangle. The only difference\nis that with a square all of the sides have the same length.\nWe can model this by creating a Rectangle class, which is a base class, and a Square class,\nwhich is a subtype that inherits from it:\npublic class Rectangle\n{\n    public virtual int Width { get; set; }\n    public virtual int Height { get; set; }\n    public int CalculateArea()\n    {\n", "page": 218, "type": "text", "section": "Page 218"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 190 ]\n        return Width * Height;\n    }\n}\npublic class Square : Rectangle\n{\n    private int _width;\n    private int _height;\n    public override int Width\n    {\n        get { return _width; }\n        set\n        {\n            _width = value;\n            _height = value;\n        }\n    }\n    public override int Height {\n        get { return _height; }\n        set\n        {\n            _width = value;\n            _height = value;\n        }\n    }\n}\nAs you can see from this code sample, the Square class overrides how the width and\nheight are set to ensure that they remain equal:\nRectangle rect = new Rectangle\n{\n    Width = 5,\n    Height = 4\n};\nConsole.WriteLine(rect.CalculateArea());\nSquare sqr = new Square\n{\n    Width = 4\n};\nConsole.WriteLine(sqr.CalculateArea());\nRectangle sqrSubstitutedForRect = new Square\n{\n    Width = 3,\n", "page": 219, "type": "text", "section": "Page 219"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 191 ]\n    Height = 2\n};\nConsole.WriteLine(sqrSubstitutedForRect.CalculateArea());\nConsole.ReadLine();\nThe area of the rect object will calculate to 20 and the area of the sqr object will calculate\nto 16, as expected. However, the area of the sqrSubstitutedForRect object will calculate\nto 4 and not 6. As these classes are currently designed, the Square subtype is really not\nsubstitutable for the Rectangle base class.\nThis code will compile but it violates the SRP and leads to confusing results. This is a\nsimplistic example, but you start to get the idea. With complex class hierarchies, violations\nof the SRP can lead to defects, some of which can be difficult to solve.\nInterface Segregation Principle (ISP)\nInterfaces define methods and properties, but do not provide any implementation. Classes\nthat implement an interface provide the implementation. Interfaces define a contract, and\nclients can use them without concerning themselves with their implementation details. The\nimplementation can change and as long as a breaking change is not made to the interface,\nthe client does not need to change their logic.\nThe Interface Segregation Principle (ISP) states that clients should not be forced to depend\non properties and methods that they do not use. When designing software, we prefer\nsmaller, more cohesive interfaces. If an interface is too large, we can logically split it up into\nmultiple interfaces so that clients can focus on only the properties and methods that are of\ninterest to them.\nWhen interfaces are too large and attempt to cover too many aspects of functionality, they\nare known as fat interfaces. The ISP is violated when classes are dependent on an interface\nwith methods they do not need. Violation of the ISP increases coupling and makes\nmaintenance more difficult.\nLet's look at an example where we are creating a system for a business that sells books. We\ncreate the following interface and class for the products:\npublic interface IProduct\n{\n    int ProductId { get; set; }\n    string Title { get; set; }\n    int AuthorId { get; set; }\n    decimal Price { get; set; }\n", "page": 220, "type": "text", "section": "Page 220"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 192 ]\n}\npublic class Book : IProduct\n{\n    public int ProductId { get; set; }\n    public string Title { get; set; }\n    public int AuthorId { get; set; }\n    public decimal Price { get; set; }\n}\nNow, let's say that the business owners want to start selling physical disks of movies. The\nproperties needed are very similar to IProduct, so an inexperienced developer might use\nthe IProduct interface for their Movie class:\npublic class Movie : IProduct\n{\n    public int ProductId { get; set; }\n    public string Title { get; set; }\n    public int AuthorId {\n        get => throw new NotSupportedException();\n        set => throw new NotSupportedException();\n    }\n    public decimal Price { get; set; }\n    public int RunningTime { get; set; }\n}\nAuthorId doesn't make sense for movies, but in this example, the developer decides to just\nmark the property as not supported. This is one of the code smells for violation of the ISP. If\na developer of a class that is implementing an interface finds themselves having to mark\nproperties or methods as not supported/not implemented, perhaps the interface needs to be\nsegregated.\nThe Movie class also needs to represent the running time of the movie, which isn't a\nproperty needed by the Book class. If it is added to the IProduct interface, all classes that\nimplement that interface will need to be modified. This is another code smell indicating that\nthere may be an issue with the design of the interface.\nIf we were to refactor this code so that it no longer violates the ISP, we would separate the\nIProduct interface into more than one so that the single \"fat\" interface is separated into\nmultiple smaller and more cohesive ones:\npublic interface IProduct\n{\n    int ProductId { get; set; }\n    string Title { get; set; }\n    decimal Price { get; set; }\n", "page": 221, "type": "text", "section": "Page 221"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 193 ]\n}\npublic interface IBook : IProduct\n{\n    int AuthorId { get; set; }\n}\npublic interface IMovie : IProduct\n{\n    int RunningTime { get; set; }\n}\npublic class Book : IBook\n{\n    public int ProductId { get; set; }\n    public string Title { get; set; }\n    public int AuthorId { get; set; }\n    public decimal Price { get; set; }\n}\npublic class Movie : IMovie\n{\n    public int ProductId { get; set; }\n    public string Title { get; set; }\n    public decimal Price { get; set; }\n    public int RunningTime { get; set; }\n}\nWe can see that the IProduct interface only contains the properties needed by all\nproducts. The IBook interface inherits from IProduct, so that any class implementing\nIBook will need to implement the AuthorId property in addition to everything in\nIProduct. Similarly, IMovie inherits from IProduct and contains the RunningTime\nproperty that is only needed for movie implementations.\nWe are no longer in violation of the ISP, and classes implementing these interfaces do not\nhave to deal with properties and methods that are not of interest to them.\n", "page": 222, "type": "text", "section": "Page 222"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 194 ]\nDependency Inversion Principle (DIP)\nThe Dependency Inversion Principle (DIP) is a principle that describes how to handle\ndependencies and write loosely coupled software. In their book Agile Principles, Patterns,\nand Practices in C#, Robert C. Martin and Micah Martin state the principle as follows:\n\"The high-level modules should not depend on low-level modules. Both should depend on\nabstractions.\nAbstractions should not depend upon details. Details should depend upon abstractions.\"\nFor example, let's say that Class A depends on Class B and Class B depends on Class C:\nIn a direct dependency graph, at compile time, Class A references Class B which references\nClass C. At runtime, the control flow will go from Class A to Class B to Class C. Class A\nand Class B will have to instantiate, or new up, their dependencies. This creates tightly\ncoupled code that is difficult to maintain and test. Changes to one of the dependencies may\nrequire changes to the classes that use those dependencies.\nAnother disadvantage of this approach is that the code is not unit-testable because of its\ndependencies. We will not be able to create mock objects for dependencies because we are\nreferencing concrete types rather than abstractions. There is no way to inject the mock\nobjects so that we can create true unit tests that are not dependent on other classes.\nRather than high-level classes being dependent on lower-level classes, they should depend\non abstractions through an interface. The interfaces do not depend on their\nimplementations. Instead, the implementations depend on the interfaces:\n", "page": 223, "type": "text", "section": "Page 223"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 195 ]\nWith an inverted dependency graph, at compile time, Class A depends on an abstraction\n(Interface B), which in turn depends on an abstraction (Interface C). Class B and Class C\nimplement Interface B and Interface C, respectively:\nAt run time, the flow of control goes through the interfaces, and each interface has an\nimplementation.\nThe DIP is closely related to the inversion of control principle, with the inversion of control\nbeing applied to dependencies.\nInversion of Control (IoC)\nInversion of Control (IoC) is a design principle in which a software system receives the\nflow of control from reusable code, such as a framework. In traditional procedural\nprogramming, a software system would call into a reusable library. The IOC principle\ninverts this flow of control by allowing the reusable code to call into the software system.\n", "page": 224, "type": "text", "section": "Page 224"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 196 ]\nDevelopers are now so familiar with using a variety of frameworks, and even multiple\nframeworks on a single project, that the principle of IoC is no longer a novel one. Although\nthe principle of IoC can be applied to many more things than just dependencies, it has\nbecome closely related to dependencies for its IOC over them.\nThis is the reason why DI containers were originally, and sometimes still are, referred to as\nIoC containers. DI containers are frameworks that provide DI functionality, and we will\ndiscuss them shortly.\nDependency Injection (DI)\nDependency Injection (DI) is a technique that provides dependencies to a class, thereby\nachieving dependency inversion. Dependencies are passed (injected) to a client that needs\nit. There are a number of benefits to using DI in a software application.\nBene\ufb01ts of DI\nDI removes hardcoded dependencies and allows them to be changed, at either runtime or\ncompile-time. If the implementation of a dependency is determined at runtime rather than\ncompile-time, this is known as late binding, or runtime binding. As long as we are\nprogramming to an interface, the implementation can be swapped out.\nDI allows us to write loosely coupled code, making applications easier to maintain, extend,\nand test. As we know, when we need to make a change, loosely coupled code allows us to\nmake a change in one part of our application without it affecting other areas of our\napplication.\nTestability increases in software applications that use DI. Loosely coupled code can be\ntested more easily. Code is written to depend on abstractions and not concrete\nimplementations, so dependencies can be mocked with unit testing frameworks. Following\nthe LSP, a class is not dependent on a concrete type; it is only dependent on the interface.\nAs a result, we can inject mock objects for the dependencies by using the interface and\nwriting unit tests.\nParallel development is made easier with DI. Developers can work on different pieces of\nfunctionality in parallel. Since the implementations are independent of each other, as long\nas the shared interfaces are agreed upon, development can occur at the same time. This is\nparticularly beneficial on larger projects with multiple teams. Each team can work\nindependently and share interfaces for the functionality that must be integrated.  \n", "page": 225, "type": "text", "section": "Page 225"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 197 ]\nDI patterns\nThe following are some of the patterns available that can be used for DI:\nConstructor injection\nProperty injection\nMethod injection\nService Locator\nThe following section provides details on these various patterns.\nConstructor injection\nConstructor injection is a technique in which dependencies are passed through a class's\nconstructor. This is an excellent way to inject dependencies because the dependencies are\nmade explicit. The object cannot be instantiated without its dependencies.\nIf a class can be instantiated and its methods called, but the functionality does not work\nproperly because one or more dependencies have not been provided, classes are being\ndishonest with their clients. It is a better practice to explicitly require dependencies.\nThe following example shows the constructor injection pattern:\npublic class Employee : Person\n{\n    private readonly ILogger _logger;\n    private readonly ICache _cache;\n    // Dependencies are injected via the constructor,\n    // including the base class\n    public Employee(ILogger logger, ICache cache,\n        IOrgService orgService)\n        : base(logger, orgService)\n    {\n        if (logger == null)\n            throw new ArgumentNullException(nameof(logger));\n        if (cache == null)\n            throw new ArgumentNullException(nameof(cache));\n        _logger = logger;\n        _cache = cache;\n    }\n}\npublic class Person\n{\n    private readonly ILogger _logger;\n", "page": 226, "type": "text", "section": "Page 226"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 198 ]\n    private readonly IOrgService _orgService;\n    // Dependencies are injected via the constructor\n    public Person(ILogger logger, IOrgService orgService)\n    {\n        if (logger == null)\n            throw new ArgumentNullException(nameof(logger));\n        if (orgService == null)\n            throw new ArgumentNullException(nameof(orgService));\n        _logger = logger;\n        _orgService = orgService;\n    }\n}\nIn this example, you can see that the Employee class has three dependencies: instances of\nILogger, ICache, and IOrgService. The Person class, which is the base class of\nEmployee, has two dependencies: instances of ILogger and IOrgService.\nConstructor injection is used to provide instances of all of the dependencies for both classes.\nIn this example, notice that the Employee class passes the dependencies that the Person\nclass needs to it. Both the Employee and the Person classes have a dependency to ILogger\nand only the Employee class has a dependency to ICache. The Employee class doesn't\neven use the orgService instance directly, which is why it is not assigned to a class-level\nvariable, but since its base class needs it, it is a dependency that is injected in and then\npassed on to its base class (Person).\nThe dependencies that are injected in are assigned to readonly variables. In the C#\nlanguage, the readonly keyword indicates that the field can only be assigned as part of its\ndeclaration or in the constructor. We will not be assigning an instance of the dependency\nanywhere else other than the constructor with constructor injection, so it can be marked as\nreadonly.\nThere is a guard clause that ensures that the required dependencies are not null. If they are,\nan exception is thrown. If a valid instance is passed in, it is assigned to a private variable so\nthat the instance can be used later in the logic of the class.\nProperty injection\nProperty injection allows clients to supply a dependency through a public property. If you\nneed to provide callers with the ability to provide an instance of a dependency, such as to\noverride the default behavior, you can use this injection pattern.\n", "page": 227, "type": "text", "section": "Page 227"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 199 ]\nThe first time the getter is called, if the dependency has not already been supplied, a default\ninstance should be provided through lazy initialization:\npublic class Person\n{\n    private IOrgService _orgService;\n    public IOrgService OrgService\n    {\n        get\n        {\n            if (_orgService == null)\n            {\n                // Lazy initialization of default\n                _orgService = new OrgService();\n            }\n            return _orgService;\n        }\n        set\n        {\n            if (value == null)\n            {\n                throw new ArgumentNullException(nameof(value));\n            }\n            // Only allow dependency to be set once\n            if (_orgService != null)\n            {\n                throw new InvalidOperationException();\n            }\n            _orgService = value;\n        }\n    }\n}\nIf you want a dependency to only be supplied once through a setter, a check can be\nperformed in the setter, as is done in the example.\nFor the lazy initialization, you will need a way to get the default instance. It is not ideal to\ninstantiate, or new up, the dependency in the class. However, the default value could be\nprovided by another means, such as constructor injection. The property would give you the\nflexibility to provide a different instance at runtime.\nThe property injection pattern provides dependencies in an implicit way. If a default\ninstance is not provided in the getter when the dependency has not been previously set and\nthe logic needs the dependency to work properly, an error or unexpected results could\noccur. If you are going to use this pattern over an explicit one, such as constructor injection,\na default instance should be provided in some way.\n", "page": 228, "type": "text", "section": "Page 228"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 200 ]\nUnlike the example for constructor injection, notice that the private class-level variable to\nhold the dependency (_orgService) does not have the readonly keyword. In order to use\nproperty injection, we need the ability to set the variable outside of the variable's\ndeclaration and the constructor.\nMethod injection\nMethod injection is similar to property injection, except a dependency is provided through\na method rather than a property:\npublic class Person\n{\n    private IOrgService _orgService;\n    public void Initialize(IOrgService orgService)\n    {\n        if (orgService == null)\n            throw new ArgumentNullException(nameof(orgService));\n        _orgService = orgService;\n    }\n}\nService Locator \nThe service locator pattern uses a locator object that encapsulates logic to determine and\nprovide an instance of the dependencies that are needed. Although it will vary depending\non your implementation, a sample call with a service locator might look something like the\nfollowing, resulting in an instance being provided based on the specified interface:\nvar cache = ServiceLocator.GetInstance<ILogger>();\nUsing the service locator pattern to get dependencies is considered to be an anti-pattern by\nsome people because it hides a class's dependencies. As opposed to constructor injection,\nwhere we can see the dependencies in the public constructor, we would have to look at the\ncode to find dependencies being resolved through the service locator. Hiding dependencies\nin this way can lead to runtime or compile-time issues, and make it more difficult to reuse\nthe code. This is particularly true if we do not have access to the source code, which might\nbe the case if we are using code from a third party. It is preferable to use an explicit method\nto acquire an instance of a dependency.\nDI containers\nA Dependency Injection container, sometimes referred as DI container or IoC container, is\na framework that helps with DI. It creates and injects dependencies for us automatically.\n", "page": 229, "type": "text", "section": "Page 229"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 201 ]\nIt is not necessary to use a DI container in order to take advantage of DI. However, using a\nDI container makes handling dependencies much easier. Unless your application is very\nsmall in size, leveraging a DI container will eliminate the repetitive grunt work of doing it\nmanually. If you were to write some generic code to automate some of it, you are\nessentially creating your own container when you could use an existing framework built\nfor that purpose. Dependencies go n levels deep and things can get complicated quickly.\nIf you do opt to use a DI container, there is a variety of containers available. While there are\ndifferences among them, they all come with some similar, basic functionality that will\nfacilitate DI. Many of them will also come with other advanced features and your needs\nwill dictate which features you use. Getting DI containers up and running to use the basic\nfeatures is typically an easy and straightforward process.\nHelping your team succeed\nOne of the many goals of a software architect is to help their team succeed and this can\ncome in many forms. In this section, we will cover some of the practices that a software\narchitect can put into place to help team members succeed. They include unit testing,\nensuring that a development environment is easy to set up, pair programming, and code\nreviews.\nUnit testing\nTesting is one of those things that some developers do not enjoy doing, even though it is\nessential to develop high-quality software applications. Developers should test early and\noften, which is a practice of many agile software development methodologies.\nIf the practice of writing and executing unit tests is not already part of the organization,\nsoftware architects should institute the practice. In this section, we will go over some of the\nessential details of unit testing.\nWhat are unit tests?\nUnit testing is the practice of testing the smallest testable units of a software system, such\nas methods, to ensure that they are working properly. It plays an important role in\nincreasing the quality of your software. Functionality in a software system is decomposed\ninto discrete, testable behaviors that are then tested as units.\n", "page": 230, "type": "text", "section": "Page 230"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 202 ]\nSome of the principles we have discussed, such as creating loosely coupled code, following\nprinciples such as DRY, SoC, and single responsibility, and using techniques such as DI,\ncreate code that is independent and decoupled from dependencies. These qualities make\nour code testable and allow us to focus a unit test on a single unit.\nBenefits of unit testing\nThere are important benefits to unit testing your software, such as improving the quality of\nthe software. With good unit tests, defects can be found before code is checked in or before\na build is attempted. By testing early and often, bugs can be fixed without affecting other\ncode.\nOne way to make it easier to test early and often is to automate the unit testing. Unit testing\nis ideal for automation. While developers will want to manually execute unit tests as they\nchange code or prior to check-ins, unit tests can be automatically executed as part of some\nprocess, such as a build process. We will discuss automation and build processes in more\ndetail in Chapter 13, DevOps and Software Architecture.\nDebugging is made easier with regular unit testing because the source of the bug can be\nnarrowed down to recent changes. Unit tests also serve as a form of documentation. By\nlooking at the unit tests, one can begin to understand what functionality a particular class\nprovides and how it is supposed to work.\nProperties of a good unit test\nThere are various properties that make for a good unit test. Keep them in mind when\nwriting your unit tests. Unit tests should be atomic, deterministic, automated and\nrepeatable, isolated and independent, easy to set up and implement, and fast.\nAtomic\nUnit tests should only test a single assumption about a small piece of functionality. This\nassumption should focus on a behavior of the unit being tested, with the unit typically\nbeing a single method. Therefore, multiple tests are necessary to check all of the\nassumptions for a given unit. If you are testing multiple assumptions in a single test or\ncalling multiple methods in a single test, the scope of your unit test is probably too large.\n", "page": 231, "type": "text", "section": "Page 231"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 203 ]\nDeterministic\nUnit tests should be deterministic. If no code changes are made, unit tests should yield the\nsame result every time they are executed.\nAutomated and repeatable\nThe benefits of unit testing are fully realized when the execution of unit tests are automated\nand repeatable. This will allow unit tests to be executed as part of a build process.\nIsolated and independent\nEach unit test should have the ability to be run independently of other tests and in any\norder. If tests are isolated and independent, it should be possible to execute any unit test at\nany time.\nUnit tests should not depend on anything except the class we are testing. It should not rely\non other classes, nor should it depend on things such as connecting to a database, using a\nhardware device, accessing files on a file system, or communicating across a network.\nWith a testing framework and a DI framework, we can mock dependencies for our unit\ntests. A mock object is a simulated object that is instantiated, perhaps with the help of a\ntesting/mocking framework, that mimics the behaviors of the real object. By mocking\nobjects for the dependencies of the class being tested, we keep the unit test independent of\nother classes. We can control the mock and specify what it will return based on some input.\nEasy to set up and implement\nUnit tests should be easy to set up and implement. If they are not, this is a code smell, which\nis a symptom in the system that may indicate a larger problem. The problem could be in the\nway that the unit being tested is designed or in the way that the test is being written.\nAlthough we want a high amount of unit test coverage, we do not want developers to\nspend inordinate amounts of time writing a single unit test. Unit tests should be easy to\nwrite.\nFast\nUnit tests should execute quickly. A complex software system that has adequate test\ncoverage will have a large amount of unit tests. The execution of unit tests should not slow\ndown development or build processes. Based on the other desirable properties of unit tests,\nsuch as being atomic, isolated, and independent, they should execute quickly.\n", "page": 232, "type": "text", "section": "Page 232"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 204 ]\nThe AAA pattern\nThe AAA pattern is a common unit testing pattern. It is a way to arrange and organize test\ncode to make unit tests clear and understandable, and consists of separating each unit test\nmethod into three sections: Arrange, Act, and Assert.\nArrange\nIn this section of the unit test method, you arrange any preconditions and inputs for the test.\nThis includes initializing values and setting up mock objects.\nDepending on the unit test framework and programming language that you are using,\nsome of them provide a way to specify methods that will execute prior to the execution of\nthe unit tests for that test class. This allows you to centralize arrangement logic that you\nwant to execute prior to the execution of all of your tests.\nHowever, each unit test method should have an arrange section where you are performing\ninitialization for that particular test.\nAct\nThe Act section of a unit test method is where you act on the unit that is being tested,\nreferred to as the System Under Test (SUT). Logic should invoke the method being tested,\npassing in values, such as mock objects, that were previously arranged.\nAssert\nThe Assert part of a unit test method is where you assert that the results are what you\nexpect. It verifies that the method executed and behaved as expected.\nNaming conventions for unit tests\nWhen naming your unit test classes and methods, you should follow a naming convention.\nThis provides not just consistency, but allows your tests to be a form of documentation.\nUnit tests are intention-revealing in that they describe the expected behavior. If meaningful\nnames are provided, everyone can know something about the purpose of the test class and\nits methods just by looking at the names.\n", "page": 233, "type": "text", "section": "Page 233"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 205 ]\nUnit test class names\nThe naming of the test classes themselves and the namespaces they are in depend on the\nnaming conventions that your project follows for classes. However, you should consider\nputting the name of the class being tested (the System Under Test (SUT) in the name of the\nclass. For example, if you are testing a class named OrderService, consider naming the\nunit test class OrderServiceTests.\nUnit test method names\nThe unit test methods should be given meaningful names that provide, at a glance, their\npurpose. Characteristics such as the method being tested, some indication as to the specific\ncondition(s) and input(s) of the test, and the expected result of the test are all useful to\nprovide.\nFor example, if we are testing the CalculateShipping method on the OrderService\nclass, we might have test method names such as:\nCalculateShipping_NullOrder_ThrowsArgumentNullException\nCalculateShipping_ValidOrder_CalculatesCorrectAmount\nCalculateShipping_ExpeditedShipping_CalculatesCorrectAmount\nThe exact naming convention for unit test methods is up to you, but the important point is\nto come up with one and to use it consistently in order to provide meaningful information\nto those who will be looking at them.\nCode coverage for unit tests\nCode coverage is a measure of how much of the source code is being covered by tests.\nMany tools exist that will help you to calculate code coverage. Software architects should\nstress the importance of aiming for exhaustive test coverage to ensure that all code paths\nare tested.\nKeep in mind that the code coverage percentage is just part of the consideration when\ndeciding whether or not you have adequate coverage. The code coverage percentage will\nlet you know what percentage of the paths are covered (as in paths that are executed at\nleast once). However, this does not mean that all of the important scenarios concerning\nyour functionality is covered. Code coverage calculations do not consider the range of\nvalues that are possible for the various inputs, and additional tests may be necessary to\ncover different situations.\n", "page": 234, "type": "text", "section": "Page 234"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 206 ]\nKeeping unit tests up to date\nUnit tests are a form of documentation for your system, describing its functionality and the\nexpected behavior. Software architects should encourage their team to not only execute unit\ntests regularly but to keep them up to date. As requirements change or new functionality is\nadded, unit tests need to be changed or added.\nAfter changes are made, developers should modify any tests that need to be changed and\nthen execute all of the unit tests to ensure there are no unintended consequences.\nOne thing that I have seen slip through the cracks before regarding keeping unit tests up to\ndate are bug fixes. If a bug is found, the unit tests did not cover that particular scenario.\nOne or more unit tests should be created that incorporate the situation into the tests to\nensure it remains fixed. It is a concept expressed by the saying that bugs should only be found\nonce.\nTeams that are diligent about updating unit tests for changed requirements and new\nfunctionality sometimes miss this important point. As a software architect, you can remind\nyour teams of it. The knowledge gained from discovering and resolving that bug should\nnot be lost. Once it is documented in the tests, from that point forward, the tests will check\nfor that type of bug to ensure that it never returns.\nSetting up development environments\nSoftware architects should review the process of setting up a new development\nenvironment in order to minimize the amount of time that it takes. Most teams do not want\nto add any extra time to a schedule that may already be tight. The process should not be\nmore difficult than necessary.\nNew developers may join the team or existing team members may need to start working on\na different machine. All too often, setting up a new development environment to the point\nwhere a developer can start coding takes an inordinate amount of time. There may be\ncomplexities involved with the setup, but it should be made as easy as possible.\nMake sure that the process of granting access to new team members, installing the\nnecessary software (for example, development tools), getting the latest code from version\ncontrol, compiling the code, and running the application is a smooth process. There are\ntools available that can create and deploy images of environments for physical and virtual\ndevelopment machines.\n", "page": 235, "type": "text", "section": "Page 235"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 207 ]\nSometimes there are subtle things that need to be done, such as making certain changes to a\nconfiguration file, in order to get the application working on a development machine. This\ntype of knowledge can sometimes become tribal knowledge, or information that is known by \nsome individuals but is unwritten and not known by everyone.\nThe goal is to get the developer up and running as quickly as possible so that they can focus\non the real complexities of their job. If something is making the process difficult, examine\nthe reasons behind it so that action can be taken to improve it. There may be ways to\nimprove the process from both organizational and technical perspectives.\nProviding a README file\nA good README file should be made available for all projects within an organization. It\nallows developers to review the steps required to set up an environment. The README file\nshould include the following:\nA brief description of the software project\nInstructions indicating what other software, such as development tools, need to\nbe installed; this includes the location of setup files for the software that needs to\nbe installed and any license keys or other information required for the\ninstallation\nAny special configuration that is necessary, such as how to point the application\nto a particular database\nInformation on how to connect to version control in order to perform operations\nsuch as getting the latest codebase and checking in changes\nA creation date for the README file and/or a version number of the software it is\nintended for so that readers can know whether they are looking at the correct\nversion\nAny relevant licensing information\nContact information for more help\n", "page": 236, "type": "text", "section": "Page 236"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 208 ]\nPair programming\nPair programming is an agile software development technique where two developers work\ntogether on the same deliverable, whether a technical design or coding. In the case of\nprogramming, the person who is coding is the driver while the other person, who is\nobserving, is the navigator. The roles of driver and navigator can alternate at a prescribed\ninterval (for example, every hour) or the roles can be switched at any time that the two\npeople feel is appropriate. Regardless of the role, each person should be an active\nparticipant.\nBenefits of pair programming\nAs a software architect, you may want to consider using this technique with your team as it\nyields a number of benefits.\nPair programming can improve code quality. Having an additional set of eyes looking at\nthe work may allow the pair to notice a problem or an opportunity that would not have\nbeen apparent if each person was working alone. Also, the driver will have a tendency to be\nmore careful when coding with someone else watching, which may lead to better code\noverall.\nWorking collaboratively during pair programming to accomplish a goal can be beneficial. If\nthe two individuals have different skillsets, they can bring both to bear on the work, and\nsome of these skills will be transferred. In addition to getting work done, sessions can act\nsimilar to a training session in this regard. Working together also helps to enforce and\nspread knowledge of things such as coding standards.\nPair programming allows developers to become more familiar with the codebase. It\nprovides opportunities for more than one person to be knowledgeable about a particular\npart of the system. Eventually, if something has to be changed with the code, more than one\nperson will be familiar with it. Pair programming tends to create a culture of collective\nownership of the codebase.\nTo fully realize this benefit, it is a good practice to rotate pairs and not just have the same\ntwo people always pair up. More knowledge will be shared when the pairs are rotated.\nPair programming can serve as training for less experienced developers or those who may\nbe new to the project. Using this technique provides an opportunity for a software architect\nor senior developer to teach someone.\n", "page": 237, "type": "text", "section": "Page 237"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 209 ]\nSoftware architects are encouraged to participate in pair programming. Rather than become\nan ivory tower architect, isolated from the rest of the team, software architects should work\nclosely with team members. By staying active in the codebase, software architects will stay \nimmersed in the project. In addition, software architects can share their knowledge and\nexperience while pair programming with the developers on the team.\nUsing pair programming when it is needed\nThe use of pair programming does not have to be an all-or-nothing proposition. Although\nsome teams might choose to do all of their development work in pairs, it certainly does not\nhave to be done that way.\nPair programming sessions can be conducted as often as you like. It may not be beneficial\nto pair program to accomplish an easy task. You may choose to do it when there is a\nparticular reason to do so, such as bringing two resources together that complement each\nother due to different skillsets or pairing up a software architect with a junior developer so\nthat the session can serve as a learning experience.\nReviewing deliverables\nSoftware architects are responsible for following an organization's review process and may\nhave a role in shaping the process. Completed deliverables should be reviewed to ensure\nthat they are correct and to identify any potential problems.\nIt is important for management and software architects to establish a culture within the\norganization so that the review process is viewed in a positive manner. It is important for\nteam members to understand that the focus of reviews is on helping each other find defects,\nto learn, and to foster communication among the team.\nSome of the methods that can be used to review deliverables include code reviews, formal\ninspections, and walkthroughs.\nCode reviews\nCode reviews are evaluations of code, typically conducted by peers. Code reviews involve\none or more people, other than the developer of the code being reviewed, who examine\ncode changes to find any problems.\n", "page": 238, "type": "text", "section": "Page 238"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 210 ]\nThe main focus of code reviews is to find both technical and business logic defects.\nHowever, code is also reviewed for other things, such as ensuring coding standards are\nbeing followed and finding opportunities for improvement.\nAn organization should have a process in place for code reviews. Some IDEs, software\nrepositories, and other tools provide functionally that facilitate collaborative code\nreviewing. The process typically involves the following:\nThe author requests a code review from one or more people\nThe requested reviewers either accept or decline the request for a review\nCommunication takes place between reviewers and the author so that comments\nand feedback can be exchanged\nAny defects that are found are recorded, assigned (typically to the author), and\ncorrected\nFixes to defects are tested\nSoftware architects should have some degree of involvement with code reviews. The extent\nof their involvement may vary depending on the project and the organization, but it is\nbeneficial for the software architect to stay involved at this level.\nReviewers should try not to review too many lines of code (LOC) at any given time. There\nis only so much a typical person can process at a time, and once most people go beyond five\nhundred lines or so, their effectiveness drastically diminishes.\nIt may be helpful to use a checklist during code reviews. The checklist can serve as a\nreminder of issues to look out for that have been found to be problematic in the past.\nDefects that are found during a review need to be recorded so that the issues are not left\nunaddressed.\nFormal inspections\nFormal inspections, as you might imagine from the name, are a more structured way of\nreviewing deliverables. It is a group review method and has been found to be quite\neffective at finding defects. The main goal of formal inspections is to evaluate and improve\nthe quality of the software system.\nFormal inspections are meetings in which deliverables, such as a design or code, are\nreviewed in order to find defects. Formal inspections are scheduled in advance, and\nparticipants are invited to the meeting.\n", "page": 239, "type": "text", "section": "Page 239"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 211 ]\nRoles for a formal inspection\nIn a formal inspection, invited participants are assigned a role that they will play. Roles\ninclude leader/moderator, author, reviewer, and recorder/scribe. The following is the\ndescription for all the roles:\nLeader/moderator: A moderator facilitates the meeting and is responsible for obtaining a\nproductive review. They ensure the meeting progresses at an appropriate pace. They\nshould encourage participation when necessary and follow up on any action items after the\nmeeting is over. They may be required to summarize or provide a report on the inspection.\nAuthor: The author is the person who created the design or wrote the code that is being\nreviewed in the inspection. Their role can be rather limited during the meeting. They may\nbe required to explain anything that is unclear or to provide reasons why something that\nappears to be a defect is actually not one.\nReviewer: One or more individuals, other than the author, serve as reviewers for the\ninspection. They can prepare for inspections by reviewing the deliverables before the\nmeeting takes place. Any notes that were taken during preparation should be brought to\nthe meeting so that those can be communicated at that time.\nAs is the case with other types of reviews, it may be helpful for reviewers to have a\nchecklist of items that they want to focus on for the review. Reviewers should be technically\ncompetent and can give positive as well as negative feedback.\nRecorder/scribe: The recorder, or scribe, is responsible for taking notes during the meeting.\nThey should record any defects found as well as action items. Although a scribe could also\nbe a reviewer, the moderator and the author should not play the role of scribe.\nInspection meeting and follow-up\nDuring the actual inspection meeting, reviewers should focus on identifying defects and\nnot on solutions. After the meeting, it is typical that an inspection report is produced,\nsummarizing the results of the inspection.\nAny defects that were found during the inspection should be placed in a backlog or\notherwise assigned to someone for resolution, such as the author. Follow-up should take\nplace, usually by the moderator, to ensure that any action items were completed.\n", "page": 240, "type": "text", "section": "Page 240"}
{"text": "Software Development Principles and Practices\nChapter 6\n[ 212 ]\nWalkthroughs\nA walkthrough is an informal method of review. In a walkthrough, the author of a design\nor code deliverable hosts a meeting in which they guide reviewers through the deliverable.\nUnlike a formal inspection, participants are not assigned specific roles (other than the\nhost/author). Walkthroughs are flexible and an organization can choose how they want to\norganize a walkthrough based on their needs.\nParticipants can prepare for a walkthrough by looking at the deliverable beforehand. The\nfocus of the walkthrough is to identify potential defects. Although the focus is not to correct\nany defects found, unlike formal inspections, the group can decide to allow suggestions of\nchanges that can be made to the deliverable. Similar to formal inspections, management\nshould not attend a walkthrough so as not to influence the meeting in any way.\nWalkthroughs have been found to not be as effective as other review methods for\nevaluating and improving deliverables. However, they do allow a larger number of\nreviewers to participate at once. This provides an opportunity to get feedback from a more\ndiverse group.\nSummary\nEven though software engineering is a relatively new discipline compared to other types of\nengineering, a number of principles and practices have been established to create high-\nquality software systems.\nWe learned that to design orthogonal software systems that can be extended while\nminimizing the impact to existing functionality, we need to focus on loose coupling and\nhigh cohesion. To minimize complexity in our software applications, a number of principles\ncan be applied, such as KISS, DRY, information hiding, YAGNI, and SoC.\nThe SOLID design principles, which include the SRP, OCP, LSP, ISP, and DIP, can be used\nto create code that is more understandable, maintainable, reusable, testable, and flexible. A\nnumber of practices, such as unit testing, pair programming, and reviewing deliverables\ncan be used to identify defects and improve the quality of software systems.\nSoftware architecture patterns are reusable solutions that can be used to solve recurring\nproblems. In the next chapter, we will go over some of the common software architecture\npatterns so that you will be aware of them and can apply them appropriately to your\nsoftware applications.\n", "page": 241, "type": "text", "section": "Page 241"}
{"text": "7\nSoftware Architecture Patterns\nSoftware architecture patterns are one of the most useful tools that can be leveraged for\ndesigning a software architecture. Some of the design issues we face as software architects\nalready have proven solutions. Experienced software architects are knowledgeable about\navailable architecture patterns and can recognize when one can be applied to a given\ndesign scenario.\nThis chapter begins by explaining what software architecture patterns are and how they\ncan be used. It then goes into detail about some commonly used architecture patterns,\nincluding layered architecture, event-driven architecture (EDA), Model-View-Controller\n(MVC), Model-View-Presenter (MVP), Model-View-ViewModel (MVVM), Command\nQuery Responsibility Segregation (CQRS), and service-oriented architecture (SOA).\nIn this chapter, we will cover the following topics:\nSoftware architecture patterns\nLayered architecture\nEvent-driven architecture , including event notifications, event-carried state\ntransfer, and event-sourcing\nModel-View-Controller pattern\nModel-View-Presenter pattern\nModel-View-ViewModel pattern\nCommand Query Responsibility Segregation\nService-oriented architecture\n", "page": 242, "type": "text", "section": "Page 242"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 214 ]\nSoftware architecture patterns\nA software architecture pattern is a solution to a recurring problem that is well\nunderstood, in a particular context. Each pattern consists of a context, a problem, and a\nsolution. The problem may be to overcome some challenge, take advantage of some\nopportunity, or to satisfy one or more quality attributes. Patterns codify knowledge and\nexperience into a solution that we can reuse.\nUsing patterns simplifies design and allows us to gain the benefits of using a solution that\nis proven to solve a particular design problem. When working with others who are familiar\nwith patterns, referencing one of them provides a shorthand with which to reference a\nsolution, without having to explain all its details. As a result, they are useful during\ndiscussions to communicate ideas.\nSoftware architecture patterns are similar to design patterns, except that they are broader in\nscope and are applied at the architecture level. Architecture patterns tend to be more\ncoarse-grained and focus on architectural problems, while design patterns are more fine-\ngrained and solve problems that occur during implementation.\nA software architecture pattern provides a high-level structure and behavior for software\nsystems. It is a grouping of design decisions that have been repeated and used successfully\nfor a given context. They address and satisfy architectural drivers and as a result, the ones\nthat we decide to use can really shape the characteristics and behavior of the architecture.\nEach pattern has its own characteristics, strengths, and weaknesses.\nSoftware architecture patterns provide the structure and main components of the software\nsystem being built. They introduce design constraints, which reduce complexity and help to\nprevent incorrect decisions. When a software architecture pattern is followed consistently\nduring design, we can anticipate the properties that the software system will exhibit. This\nallows us to consider whether a design will satisfy the requirements and quality attributes\nof the system.\nUsing software architecture patterns\nMuch like design patterns, software architecture patterns come into being after they are\nsuccessfully repeated in practice. As a software architect, you will discover and use an\nexisting pattern when appropriate.\n", "page": 243, "type": "text", "section": "Page 243"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 215 ]\nThe majority of the time, you will not be inventing or creating a new pattern. It is possible\nthat you may find yourself tackling challenges and problems in a novel domain, requiring\nyou to create truly new solutions that do not currently exist. However, even in that case the\nsolution will not be a pattern yet. Only after it is repeated in practice and becomes known\nas a solution for a particular context and problem does it become a pattern.\nSoftware architecture patterns can be applied to the entire software system or to one of the\nsubsystems. Consequently, more than one software architecture pattern can be used in a\nsingle software system. These patterns can be combined to solve problems.\nOverusing architecture patterns\nWhile leveraging architecture patterns is a valuable tool, don't force the use of a particular\npattern. A common mistake with design and architecture patterns is to use one even if it is\nnot appropriate to do so.\nA developer or architect may become aware of a particular pattern and then become\noverzealous in their desire to use it. They may apply a pattern just due to their familiarity\nwith it.\nThe key is to gain knowledge of the patterns that are available and an understanding as to\nthe scenarios in which they should be applied. This knowledge allows a software architect\nto select and use the appropriate pattern. A software architecture pattern should only be\nused if it is the best solution for a given design issue and context.\nUnderstanding the difference between\narchitecture styles and architecture patterns\nYou may come across the term architecture style and compare its meaning with the term\narchitecture pattern. For the most part, these two terms are used interchangeably.\nHowever, some people make a distinction between the two, so let's take a moment to\nexplain the difference. It is not completely clear-cut, as the definitions vary depending on\nwho you ask. One of the ways an architecture style is defined is that it is a set of elements,\nand the vocabulary to be used for those elements, that is available to be used in an\narchitecture. It constrains an architecture design by restricting the available design choices.\nWhen a software system adheres to a particular architecture style, it will be expected to\nexhibit certain properties.\n", "page": 244, "type": "text", "section": "Page 244"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 216 ]\nFor example, if we were to follow the microservice architecture style for an application,\nwhich comes with the constraint that each service should be independent of the others, we\ncan expect such a system to have certain properties. A system that follows a microservice\narchitecture style will be able to deploy services independently, isolate faults for a\nparticular service, and use a technology of the team's choosing for a given service.\nA software architecture pattern is a particular arrangement of the available elements into a\nsolution for a recurring problem given a certain context. Given a particular architecture\nstyle, we can use the vocabulary of that style to express how we want to use the elements\navailable in that style in a certain way. When this arrangement is a known solution to a\ncommon, recurring problem in a particular context, it is a software architecture pattern.\nThis book does not focus on making a distinction between these two terms and, for the\nmost part, uses the term software architecture pattern, as evidenced by the title of this\nchapter.\nNow that we know what software architecture patterns (and styles) are, let's explore some\nof the commonly used ones, beginning with the layered architecture.\nLayered architecture\nWhen partitioning a complicated software system, layering is one of the most common\ntechniques. In a layered architecture, the software application is divided into various\nhorizontal layers, with each layer located on top of a lower layer. Each layer is dependent\non one or more layers below it (depending on whether the layers are open or closed), but is\nindependent of the layers above it.\nOpen versus closed layers\nLayered architectures can have layers that are designed to be open or closed. With a closed\nlayer, requests that are flowing down the stack from the layer above must go through it and\ncannot bypass it. For example, in a three-layer architecture with presentation, business, and\ndata layers, if the business layer is closed, the presentation layer must send all requests to\nthe business layer and cannot bypass it to send a request directly to the data layer.\n", "page": 245, "type": "text", "section": "Page 245"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 217 ]\nClosed layers provide layers of isolation, which makes code easier to change, write, and\nunderstand. This makes the layers independent of each other, such that changes made to\none layer of the application will not affect components in the other layers. If the layers are\nopen, this increases complexity. Maintainability is lowered because multiple layers can now\ncall into another layer, increasing the number of dependencies and making changes more\ndifficult.\nHowever, there may be situations in which it is advantageous to have an open layer. One of\nthem is to solve a common problem with the layered architecture, in which unnecessary\ntraffic can result when each layer must be passed even if one or more of them is just passing\nrequests on to the next layer.\nIn our example of a three-layer architecture with presentation, business, and data layers,\nlet's say that we introduce a shared services layer between the business and data layers.\nThis shared services layer may contain reusable components needed by multiple\ncomponents in the business layer. We may choose to place it below the business layer so\nthat only the business layer has access to it. However, now all requests from the business\nlayer to the data layer must go through the shared services layer even though nothing is\nneeded from that layer. If we make the shared services layer open, requests to the data\nlayer can be made directly from the business layer.\nThe important point for software architects to understand when designing a layered\narchitecture is that there are advantages to closed layers and achieving layers of isolation.\nHowever, experienced software architects understand when it might be appropriate to\nopen a layer. It is not necessary to make all of the layers open or closed. You may\nselectively choose which layers, if any, are open. \nTiers versus layers\nYou may have heard the terms tier and layer in reference to layered architectures. Before\nwe proceed with discussing layered architectures, these terms should be clarified.\nLayers are logical separations of a software application and tiers are physical ones.\nWhen partitioning application logic, layers are a way to organize functionality and\ncomponents. For example, in a three-layered architecture, the logic may be separated into\npresentation, business, and data layers. When a software architecture is organized into\nmore than one layer, it is known as a multi-layer architecture. Different layers do not\nnecessarily have to be located on different physical machines. It is possible to have multiple\nlayers on the same machine.\n", "page": 246, "type": "text", "section": "Page 246"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 218 ]\nTiers concern themselves with the physical location of the functionality and components. A\nthree-tiered architecture with presentation, business, and data tiers implies that those three\ntiers have been physically deployed to three separate machines and are each running on\nthose separate machines. When a software architecture is partitioned into multiple tiers, it\nis known as a multi-tier architecture.\nKeep in mind that some people use the two terms interchangeably. When communicating\nwith others, if the distinction is important, you may want to be precise in your language\nand you may need to confirm with the other person what they mean when they use one of\nthe terms.\nAdvantages of layered architectures\nThere are some key benefits to using a layered architecture. This pattern reduces\ncomplexity by achieving a Separation of Concerns (SoC). Each layer is independent and\nyou can understand it on its own without the other layers. Complexity can be abstracted\naway in a layered application, allowing us to deal with more complex problems.\nDependencies between layers can be minimized in a layered architecture, which further\nreduces complexity. For example, the presentation layer does not need to depend directly\non the data layer and the business layer does not depend on the presentation layer.\nMinimizing dependencies also allows you to substitute implementations for a particular\nlayer without affecting the other layers.\nAnother advantage of layered architectures is the fact that they can make development\neasier. The pattern is pervasive and well known to many developers, which makes using it\neasy for the development team. Due to the way that the architecture separates the\napplication logic, it matches up well with how many organizations hire their resources and\nallocate tasks during a project. Each layer requires a particular skill set and suitable\nresources can be assigned to work on each layer. For example, UI developers for the\npresentation layer, and backend developers for the business and data layers.\nThis architecture pattern increases the testability quality attribute of software applications.\nPartitioning the application into layers and using interfaces for the interaction between\nlayers allows us to isolate a layer for testing and either mock or stub the other layers. For\nexample, you can perform unit testing on classes in your business layer without the\npresentation and data layers. The business layer is not dependent on the presentation layer\nand the data layer can be mocked or stubbed.\nApplications using a layered architecture may have higher levels of reusability if more than\none application can reuse the same layer. For example, if multiple applications target the\nsame business and/or data layers, those layers are reusable.\n", "page": 247, "type": "text", "section": "Page 247"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 219 ]\nWhen an application using a layered architecture is deployed to different tiers, there are\nadditional benefits:\nThere is increased scalability as more hardware can be added to each tier,\nproviding the ability to handle increased workloads.\nA multi-tier application can experience greater levels of availability when\nmultiple machines are used per layer. Uptime is increased because, if a hardware\nfailure takes place in a layer, other machines can take over.\nHaving separate tiers enhances security as firewalls can be placed in between the\nvarious layers.\nIf a layer can be reused for multiple applications, it means that the physical tier\ncan be reused as well.\nDisadvantages of layered architectures\nAlthough layered architectures are commonly used, and for good reasons, there are\ndisadvantages to using them. Although the layers can be designed to be independent, a\nrequirement change may require changes in multiple layers. This type of coupling lowers\nthe overall agility of the software application.\nFor example, adding a new field will require changes to multiple layers: the presentation\nlayer so that it can be displayed, the business layer so that it can be\nvalidated/saved/processed, and the data layer because it will need to be added to the\ndatabase. This can complicate deployment because, even for a change such as this, an\napplication may require multiple parts (or even the entire application) to be deployed.\nAnother minor disadvantage is the fact that more code will be necessary for layered\napplications. This is to provide the interfaces and other logic that are necessary for the\ncommunication between the multiple layers.\nDevelopment teams have to be diligent about placing code in the correct layer so as not to\nleak logic to a layer that belongs in another layer. Examples of this include placing business\nlogic in the presentation layer or putting data-access logic in the business layer.\nAlthough applications with good performance can be designed with layered architectures,\nif you are designing a high-performance application, you should be aware that there can be\ninefficiencies in having a request go through multiple layers. In addition, moving from one\nlayer to another sometimes requires data representations to be transformed. One way to\nmitigate this disadvantage is to allow some layers to be open but this should only be done if\nit is appropriate to open a layer.\n", "page": 248, "type": "text", "section": "Page 248"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 220 ]\nThere are some additional disadvantages to layered architectures when they are deployed\nto multiple tiers:\nThe performance disadvantage of layered architectures has already been\nmentioned, but when those layers are deployed to separate physical tiers, as is\ncommon, there is an additional performance cost. With modern hardware, this\ncost may be small but it still won't be faster than an application that runs on a\nsingle machine.\nThere is a greater monetary cost associated with having a multi-tier architecture.\nThe more machines are used for the application, the greater the overall cost.\nUnless the hosting of the software application is handled by a cloud provider or\nhas otherwise been outsourced, an internal team will be needed to manage the\nphysical hardware of a multi-tier application.\nClient-server architecture (two-tier architecture)\nLayered architectures became very prevalent with the popularity of client-server software\nsystems. In a distributed application that uses a client-server architecture, also known as a\ntwo-tier architecture, clients and servers communicate with each other directly. A client\nrequests some resource or calls some service provided by a server and the server responds\nto the requests of clients. There can be multiple clients connected to a single server:\nThe Client part of the application contains the user interface code and the Server contains\nthe database, which traditionally has been a relational database management system\n(RDBMS). The majority of application logic in a client-server architecture is located on the\nserver, but some of it could also be located in the client. The application logic located on the\nserver might exist in software components, in the database, or both.\nWhen the client contains a significant portion of the logic and is handling a large share of\nthe workload, it is known as a thick, or fat, client. When the server is doing that instead, the\nclient is known as a thin client.\n", "page": 249, "type": "text", "section": "Page 249"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 221 ]\nIn some client-server applications, the business logic is spread out between the client and\nthe server. If consistency isn't applied, it can make it difficult to always know where a\nparticular piece of logic is located.\nIf a team isn't diligent, business logic might be duplicated on the client and the server,\nviolating the DRY principle.\nThere may be instances in which the same piece of logic is needed on both the client and the\nserver. For example, there may be business logic needed by the user interface to validate a\npiece of data prior to submitting the data to the server. The server may need this same\nbusiness logic because it also needs to perform this validation. While centralizing this logic\nmay require additional communication between the client and the server, the alternative\n(duplication) lowers maintainability. If the business logic were to change, it would have to\nbe modified in multiple places.\nUsing stored procedures for application logic\nIf application logic does exist in the database, it is commonly found in stored procedures. A\nstored procedure is a grouping of one or more Structured Query Language (SQL)\nstatements that forms a logical unit to accomplish some task. It can be used to do a\ncombination of retrieving, inserting, updating, and deleting data.\nIt used to be popular to use stored procedures with client-server applications because their\nuse reduced the amount of network traffic between the client and the server. Stored\nprocedures can contain any number of statements within them and can call other stored\nprocedures. A single call from the client to the server is all that is needed to execute a stored\nprocedure. If that logic was not encapsulated inside a stored procedure, multiple calls\nwould need to be made over the network between the client and the server to execute the\nsame logic.\nStored procedures are compiled the first time they are executed, at which time an execution\nplan is created that a database's query engine can use to optimize its use on subsequent\ncalls. In addition to some performance benefits, there are security advantages to using\nstored procedures. Users and applications do not need to be granted permissions to the\nunderlying database objects that a stored procedure uses, such as database tables. A user or\napplication can execute a stored procedure but it is the stored procedure that has control\nover what logic is executed and which database objects are used. Stored procedures\nincrease reusability in that once one is written and compiled, it can be reused in multiple\nplaces.\n", "page": 250, "type": "text", "section": "Page 250"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 222 ]\nAlthough there are benefits to using stored procedures, there are drawbacks. There are\nlimited coding constructs, as compared with high-level programming languages, that are\navailable for use in application logic. Having some of your business logic located in stored\nprocedures also means that your business logic isn't centralized.\nIn modern application development, application logic should not be placed in stored\nprocedures. It belongs outside the data layer, independent and decoupled from the\nmechanism used to store the data. Based on that point, some have relegated stored\nprocedures to simple CRUD operations. However, if that is the case, stored procedures are\nnot providing much of a benefit.\nAlthough application logic should not be placed in stored procedures, they can still be of\nuse in some situations. For complex queries (for example, SQL queries with complex table\njoins and WHERE clauses) and queries requiring multiple statements and large amounts of\ndata, the performance advantages of stored procedures can be useful.\nN-tier architecture\nWith an n-tier architecture, also known as a multitier architecture, there are multiple tiers\nin the architecture. One of the most widely-used variations of this type of layered\narchitecture is the three-tier architecture. The rise of the web coincided with a shift from\ntwo-tier (client-server) architectures to three-tier architectures. This change was not a\ncoincidence. With web applications and the use of web browsers, rich client applications\ncontaining business logic were not ideal.\nThe three-tier architecture separates logic into presentation, business, and data layers:\n", "page": 251, "type": "text", "section": "Page 251"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 223 ]\nPresentation tier\nThe presentation tier provides functionality for the application's UI. It should provide an\nappealing visual design as it is the part of the application that users interact with and see.\nData is presented to the user and input is received from users in this tier. Aspects of the\nusability quality attribute, which were discussed in Chapter 4, Software Quality Attributes,\nshould be the concern of the presentation tier.\nThis tier should contain logic to render the user interface, including the placement of data\nin the appropriate UI components, formatting the data that is presented appropriately, and\nhiding/showing UI components as required.\nIt should also provide some basic validation to help users avoid or minimize mistakes, such\nas ensuring the correct type of data is being entered for a given control and that the data is\nin the correct format. Developers should be careful not to introduce business logic into the\nvalidation, which should be handled by the business tier.\nThe presentation tier should provide users with useful feedback, such as friendly and\ninformative messages, tooltips, visual feedback, such as a progress bar for long-running\nprocesses, and notifications to inform users about the completion or failure of\nasynchronous operations.\nSoftware architects should strive to design thin clients that minimize the amount of logic\nthat exists in the presentation tier. The logic in the presentation tier should focus on user\ninterface concerns. A presentation tier devoid of business logic will be easier to test.\nBusiness tier\nThe business tier, which is sometimes referred to as the application tier, provides the\nimplementation for the business logic of the application, including such things as business\nrules, validations, and calculation logic. Business entities for the application's domain are\nplaced in this tier.\nThe business tier coordinates the application and executes logic. It can perform detailed\nprocesses and makes logical decisions. The business tier is the center of the application and\nserves as an intermediary between the presentation and data tiers. It provides the\npresentation tier with services, commands, and data that it can use, and it interacts with the\ndata tier to retrieve and manipulate data.\n", "page": 252, "type": "text", "section": "Page 252"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 224 ]\nData tier\nThe data tier provides functionality to access and manage data. The data tier contains a\ndata store for persistent storage, such as an RDBMS. It provides services and data for the\nbusiness tier.\nThere are variations of n-tier architectures that go beyond just three tiers. For example, in\nsome systems, there is a data access or persistence layer in addition to a data or database\nlayer. The persistence layer contains components for data access, such as an object-\nrelational mapping (ORM) tool, and the database layer contains the actual data store, such\nas an RDBMS. One reason to separate these into two distinct layers is if you wanted the\nability to switch out your data access or database technology for a different one.\nEvent-driven architecture\nAn event is the occurrence of something deemed significant in a software application, such\nas a state change, that may be of interest to other applications or other components within\nthe same application. An example of an event is the placement of a purchase order or the\nposting of a letter grade for a course that a student is taking.\nAn event-driven architecture (EDA) is a distributed, asynchronous software architecture\npattern that integrates applications and components through the production and handling\nof events. By tracking events, we don't miss anything of significance related to the business\ndomain.\nEDAs are loosely coupled. The producer of an event does not have any knowledge\nregarding the event subscribers or what actions may take place as a result of the event.\nSOA can complement EDA because service operations can be called based on events being\ntriggered. The converse can also be designed, such that service operations raise events.\nEDAs can be relatively complex given their inherent asynchronous, distributed processing.\nAs with any distributed architecture, issues may occur due to a lack of responsiveness,\nperformance issues, or failures with event mediators and event brokers (these components\nwill be described shortly).\nEvent channels\nBefore we cover the two main event topologies for EDAs, let's go over the concept of event\nchannels because both topologies make use of them.\n", "page": 253, "type": "text", "section": "Page 253"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 225 ]\nEvent messages contain data about an event and are created by event producers. These\nevent messages use event channels, which are streams of event messages, to travel to an\nevent processor.\nEvent channels are typically implemented as message queues, which use the point-to-point\nchannel pattern, or message topics, which use the publish-subscribe pattern.\nMessage queues\nMessage queues ensure that there is one, and only one, receiver for a message. In the\ncontext of event channels, this means that only one event processor will receive an event\nfrom an event channel. The point-to-point channel pattern is utilized for message queue\nimplementations.\nThe point-to-point channel pattern\nThe point-to-point channel pattern is a messaging pattern used when we want to ensure\nthat there will be exactly one receiver for a given message. If a channel has multiple\nreceivers so that more than one message can be consumed concurrently, and more than one\nreceiver attempts to consume a message, the event channel will ensure that only one of\nthem succeeds. Since the event channel is handling that, it removes any need for\ncoordination between the event processors.\nMessage topics\nMessage topics allow multiple event consumers to receive an event message. The publish-\nsubscribe pattern is used for the implementation of message topics.\nThe publish-subscribe pattern\nThe publish-subscribe pattern, which is sometimes referred to as pub/sub for short, is a\nmessaging pattern that provides a way for a sender (publisher) to broadcast a message to\ninterested parties (subscribers).\nRather than publishers sending messages directly to specific receivers as in the point-to-\npoint channel pattern, the messages can be sent without any knowledge of the subscribers\nor even if there are no subscribers. Similarly, it allows subscribers to show interest in a\nparticular message without any knowledge of the publishers or even if there are no\npublishers.\n", "page": 254, "type": "text", "section": "Page 254"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 226 ]\nEvent-driven architecture topologies\nThe two main topologies for EDAs are the mediator and broker topologies.\nThe mediator topology\nA mediator topology for EDAs uses a single event queue and an event mediator to route\nevents to the relevant event processors. This topology is commonly used when multiple\nsteps are required to process an event.\nWith the mediator topology, event producers send events into an event queue. There can\nbe many event queues in an EDA. Event queues are responsible for sending event messages\non to the event mediator. All of these events, referred to as initial events, go through an\nevent mediator. The event mediator then performs any necessary orchestration:\nAfter the event mediator orchestrates each event from the event queue, it creates one or\nmore asynchronous processing events based on the orchestration. These processing events\nget sent out to an event channel, which can either be a message queue or a message topic.\n", "page": 255, "type": "text", "section": "Page 255"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 227 ]\nMessage topics are more commonly used with the mediator topology due to the\norchestration involved, which allows multiple event processors, which will perform\ndifferent tasks, to receive an event message. This can be seen in the preceding diagram\nwhere some of the event channels are sending an event message to multiple event\nprocessors.\nEvent processors listen in on event channels to pick up events and process them in line\nwith their design. In the Event processing styles section later in this chapter, we will cover the\ndifferent processing styles typically used by event processors.\nEvent mediator implementations\nAn event mediator can be implemented in a number of different ways. For simple\norchestrations, an integration hub can be used. These typically allow you to define\nmediation rules using a domain-specific language (DSL) for the routing of events.\nDomain-specific languages, unlike a general-purpose language, such as C#, Java, or UML,\nallow expressions to be written for a particular domain.\nFor a more complex orchestration of events, Business Process Execution Language\n(BPEL) can be used in conjunction with a BPEL engine. BPEL is an XML-based language\nthat is used to define business processes and their behavior. It is frequently used with SOA\nand web services.\nLarge software applications with complex orchestration needs, which may even include\nhuman interactions, may opt to implement an event mediator that uses a business process\nmanager (BPM). Business process management involves modeling, automating, and\nexecuting business workflows. Some business process managers use Business Process\nModel and Notation (BPMN) to define business processes. BPMN allows for business\nprocess modeling using a graphical notation. Business process diagrams created with\nBPMN are similar to activity diagrams in UML. \nSoftware architects must understand the needs of the software application in order to select\nan appropriate event mediator implementation.\nThe broker topology\nIn a broker topology, the event messages created by event producers enter an event broker,\nsometimes referred to as an event bus. The event broker contains all of the event channels\nused for the event flow. The event channels may be message queues, message topics, or\nsome combination of the two.\n", "page": 256, "type": "text", "section": "Page 256"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 228 ]\nUnlike the mediator topology, there is no event queue with the broker topology. The event\nprocessors are responsible for picking up events from an event broker:\nThis topology is ideal when the processing flow is fairly simple and there is no need for\ncentralized event orchestration. Events flow to the event processor from the event broker\nand, as part of the processing, new events may be created.\nThis can be seen in the preceding diagram, where in some cases, events are flowing from\nthe event processors back to the event broker. A key to the broker topology is chaining\nevents in order to execute a particular business task.\nEvent processing styles\nEvent processors are components that have a specific task and contain logic to analyze and\ntake action on events. Each event processor should be independent and loosely coupled\nwith other event processors.\nOnce event messages reach event processors, there are three prevalent styles for processing\nevents: simple event processing (SEP), event stream processing (ESP), and complex event\nprocessing (CEP). The type of event processing needed by a particular application depends\non the processing complexity that is required. EDAs may utilize a combination of these\nthree styles.\n", "page": 257, "type": "text", "section": "Page 257"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 229 ]\nSimple event processing (SEP)\nIn SEP, notable events are immediately routed in order to initiate some type of downstream\naction. This type of processing can be used for real-time workflows to reduce the amount of\ntime between the event taking place and the resulting action.\nSimple event processing also includes functionality such as transforming event schemas\nfrom one form to another, generating multiple events based on the payload of a single\nevent, and supplementing an event payload with additional data.\nEvent stream processing (ESP)\nESP involves analyzing streams of event data and then taking any necessary action based\non that analysis. Events are screened based on various conditions and then either an action\nmay be taken for the event or the event may be ignored. This type of processing is ideal for\nreal-time flows in which decisions are also involved.\nAn example is a stock trading system in which an event takes place and enters an event\nstream when a stock ticker reports a change in price. Based on the price, an algorithm\ndetermines whether a buy or sell order should be created, and notifies the appropriate\nsubscribers, if necessary.\nComplex event processing (CEP)\nIn CEP, analysis is performed to find patterns in events to determine whether a more\ncomplex event has occurred. A complex event is an event that summarizes or represents a\nset of other events. Events may be correlated over multiple dimensions, such as causal,\ntemporal, or spatial.\nCEP can take place over a longer period of time as compared to the other types of event\nprocessing. This type of event processing might be used to detect business threats,\nopportunities, or other anomalies.\nAn example of functionality that uses CEP is a credit card fraud engine. Each transaction on\na credit card is an event and the system will look at a grouping of those events for a\nparticular credit card to try to find a pattern that might indicate fraud has taken place. If\nprocessing fraud is detected, downstream action is initiated.\n", "page": 258, "type": "text", "section": "Page 258"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 230 ]\nTypes of event-driven functionality\nEDA can mean different things. There are three main types of functionality that can\ntypically be found in systems that have an EDA: event notification, event-carried state\ntransfer, and event sourcing. Event-driven software systems can provide a combination of\nthe three.\nEvent notification\nAn architecture that provides event notification is one in which the software system sends\na message when an event takes place. This functionality is the most common in software\nsystems that have an EDA. The mediator and broker topologies allow us to implement\nevent notifications.\nThere is a loose coupling between the event producer and any event consumers as well as\nbetween the logic that sends event messages and logic that responds to the events. This\nloose coupling allows us to change the logic in one without affecting the other. Event\nprocessor components are single-purpose and independent of other event processors,\nallowing them to be modified without affecting others.\nThe drawback to the loose coupling between event producers and event consumers is that\nit can be difficult to see the logical flow of event notifications. This added complexity also\nmakes it more difficult to debug and maintain. There aren't specific statements you can look\nat to see what logic will be executed. A variety of event consumers, including ones in\nsoftware systems other than the one that produced the event notification, may react to an\nevent. Sometimes the only way to understand the logical flow is to monitor your systems to\nsee the flow of event messages.\nEvent-carried state transfer\nEvent-carried state transfer is a variation on event notification. Its use is not as common as\nregular event notification. When an event consumer receives an event notification, it may\nneed more information from the event producer in order to take the action that they want\nto take.\n", "page": 259, "type": "text", "section": "Page 259"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 231 ]\nFor example, a sales system may send a new order event notification and a shipping system\nmay subscribe to this type of event messages. However, in order to take appropriate action,\nthe shipping system now needs additional information about the order, such as the\nquantity and type of line items that are in the order. This requires the shipping system to\nquery the sales system in some way, such as through an API, for this information.\nWhile the event publisher may not need to know anything about their subscribers, the\nsubscriber is coupled to the producer in the sense that it needs to be aware of the producer\nand have a way to get more information from the producer.\nCallbacks to the system that produced an event notification for more data in order to\nhandle an event increase network load and traffic. One way to resolve this is to add state\ninformation to the events so that they contain enough information to be useful for potential\nconsumers. For example, an event notification for a new order could contain the line item\ndetails needed by the shipping system so that no callback is required. The shipping system\ncan keep its own copy of only the order details that it needs.\nAlthough more data is being passed around, we gain a greater level of availability and\nresilience. The shipping system can function, at least with orders it has already received,\neven if the order system is temporarily unavailable. The shipping system does not need to\ncall back to the order system after the initial event notification is received, which can be\nparticularly beneficial if contacting and receiving data from the order system is slow.\nHowever, with greater availability comes lower consistency. The replication of some data\nbetween the order and shipping systems lowers the consistency of the data.\nEvent-sourcing\nA system can use a data store to read and update the application's current state, but what if\nthere are requirements to know the details of the state changes that got us to the current\npoint? With event-sourcing, the events that take place in a system, such as state changes,\nare persisted in an event store. Having a complete record of all the events that took place\nallows it to serve as a source of truth. Replaying events from an event log can be used to\nrecreate an application's state.\nEvent-sourcing works in a similar way to a transaction log in a database system. The\ntransaction log records all of the modifications that have been made to a database. This\nallows for rollbacks of transactions and also allows us to recreate the system state up to a\nparticular point, such as right before a failure occurred.\n", "page": 260, "type": "text", "section": "Page 260"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 232 ]\nEvents should be immutable as they represent something that has already taken place.\nActions may take place downstream as the result of an event, so if an event could be\nchanged after the fact, it could put your system in an inconsistent state. If an update or\ncancellation of an event is necessary, a compensating event should be created.\nCompensating logic can be executed based on such events and can apply the necessary\nbusiness rules to apply counter-operations. This will ensure that the event store is still a\nsource of truth and that we can replay all of the events to recreate an application's state.\nThe benefits of event-sourcing include the fact that it can aid in debugging a system. It\nprovides the ability to take events and run them through the system to see how the system\nwill behave. This can be used to determine the cause of a problem. Event-sourcing also\nprovides detailed auditing. The complete record of events allows us to see what happened,\nhow it happened, when it happened, and other details.\nAlthough event-sourcing can be very useful, it does introduce some added complexity into\na software system. Multiple instances of an application and multithreaded applications\nmight be persisting events to an event store. The system must be designed to ensure that\nevents are processed in the correct order.\nThe code that processes events and the event schema can change over time. Consideration\nmust be given to ensure that older events, possibly with different event schemas, can still\nbe replayed with the current logic.\nIf part of the sequence of events includes the use of an external system, consideration has to\nbe given to storing the responses from the external system as events. This will ensure that\nwe can replay events accurately to rebuild the application's state without having to call the\nexternal system again.\nThe Model-View-Controller pattern\nThe Model-View-Controller (MVC) pattern is a software architecture pattern that is\nwidely used for the UI of an application. It is particularly well suited to web applications,\nalthough it can also be used for other types of applications, such as desktop applications.\nThe pattern provides a structure for building user interfaces and provides a separation of\nthe different responsibilities involved. A number of popular web and application\ndevelopment frameworks make use of this pattern. A few examples include Ruby on Rails,\nASP.NET MVC, and Spring MVC.\n", "page": 261, "type": "text", "section": "Page 261"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 233 ]\nThe MVC pattern consists of the Model, View, and Controller:\nThe model, view, and controller all have distinct responsibilities for the user interface. Let's\ntake a look at each of them more closely.\nModel\nThe model manages the application data and the state. Among its responsibilities is the\nprocessing of data to and from a data store, such as a database. A model is independent of\nthe controllers and the views, allowing them to be reused with different user interfaces.\nThis also allows them to be tested independently.\nModels receive directives from controllers to retrieve and update data. Models also provide\napplication state updates. In some variations of MVC, the model is passive and must receive\na request to send out an application state update. In other variations, a view may be active\nand push notifications of model state changes to a view.\nView\nThe view is responsible for the presentation of the application. It is the part of the\napplication that is visible to the user. The view displays data to the user in an appropriate\ninterface based on information received from the controller. If the model is providing\napplication state updates directly to views, the views may also be updated based on these\nnotifications.\nAs users manipulate a view, such as providing input or providing some user action, the\nview will send this information to a controller.\n", "page": 262, "type": "text", "section": "Page 262"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 234 ]\nController\nAs users navigate a web application, requests are routed to the appropriate controller based\non routing configuration. A controller acts as an intermediary between the model and the\nview.\nA controller executes application logic to select the appropriate view and sends it the\ninformation that it needs to render the user interface. Views notify controllers of user\nactions so that the controller can respond to them. Controllers will update the model based\non user actions.\nAdvantages of the MVC pattern\nUsing the MVC pattern allows for a separation of concerns. By separating the presentation\nfrom the data, it makes it easier to change one of them without affecting the other. It also \nmakes each part easier to test. However, it is difficult to achieve a complete separation. For\nexample, adding a new field to the application will require a change in both the data and\nthe presentation.\nThe MVC pattern makes presentation objects more reusable. Separating the user interface\nfrom the data allows UI components to be reused. It also means that a model can be reused\nwith more than one view.\nThe separation of the presentation from the business logic and data allows developers to\nspecialize in either frontend or backend development. This can also speed up the\ndevelopment process as some tasks can take place in parallel. For example, one developer\ncan work on the user interface while another works on the business logic.\nDisadvantages of the MVC pattern\nIf your development team is not set up so that developers are focused on either frontend or\nbackend development, this does require developers to be skilled in both areas (full-stack\ndevelopers). This will require developers who are skilled in multiple technologies.\nIf a model is very active and is providing notifications directly to views, frequent changes\nto a model may result in excessive updates to views.\n", "page": 263, "type": "text", "section": "Page 263"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 235 ]\nThe Model-View-Presenter pattern\nThe Model-View-Presenter (MVP) pattern is a variation on the MVC pattern. Like the\nMVC pattern, it provides a separation between UI logic and business logic. However, the\npresenter takes the place of the controller in the MVP pattern.\nEach view in the MVP pattern typically has a corresponding interface (view interface).\nPresenters are coupled with the view interfaces. As compared with the MVC pattern, the\nview is more loosely coupled to the model because the two do not interact with each other\ndirectly:\nBoth web and desktop applications can use the MVP pattern. The main components of this\npattern are the Model, View, and Presenter.\nModel\nAs was the case with the MVC pattern, the model represents the business model and the\ndata. It interacts with the database to retrieve and update data. The model receives\nmessages from the presenter for updates and reports state changes back to the presenter.\nModels in the MVP pattern do not interact directly with views and only interact with the\npresenter.\nView\nThe view is responsible for displaying the user interface and data. Each view in the MVP\npattern implements an interface (view interface). As the user interacts with the view, the\nview will send messages to the presenter to act on the events and data.\n", "page": 264, "type": "text", "section": "Page 264"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 236 ]\nPresenters are loosely coupled with views through the view interface. Views are more\npassive in the MVP model and rely on the presenter to provide information on what to\ndisplay.\nPresenter\nThe presenter is the intermediary between the model and the view. It interacts with both of\nthem. Each view has a presenter and the view notifies the presenter of user actions. The\npresenter updates the model and receives state changes from the model.\nA presenter will receive data from the model and format it for the view to display, taking\nan active role in presentation logic. Presenters encapsulate presentation logic and views\nplay a more passive role.\nUnlike the MVC pattern, where a controller can interact with multiple views, in the MVP\npattern, each presenter typically handles one, and only one, view.\nThe Model-View-ViewModel pattern\nThe Model-View-ViewModel (MVVM) pattern is another software architecture pattern\nand it shares similarities with MVC and MVP in that they all provide a SoC. Partitioning\nthe various responsibilities makes an application easier to maintain, extend, and test. The\nMVVM pattern separates the UI from the rest of the application:\n", "page": 265, "type": "text", "section": "Page 265"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 237 ]\nThere is typically a significant amount of interaction between views and ViewModels,\nfacilitated by data binding. The MVVM pattern works well for rich desktop applications,\nalthough it can be used for other types of application, such as web and mobile applications.\nAn example of a framework that can be used to build MVVM applications is Windows\nPresentation Foundation (WPF).\nThe main components of MVVM are the Model, View, and ViewModel. Let's take a look at\neach of these in more detail.\nModel\nThe model in the MVVM pattern plays a similar role as in MVC and MVP. It represents the\nbusiness domain object and the data. The model uses the database to retrieve and update\ndata.\nIn MVVM applications, there may be direct binding with model properties. As a result,\nmodels commonly raise property changed notifications.\nView\nThe view is responsible for the user interface. It is the part of the application that is visible\nto users. In the MVVM pattern, the view is active. Unlike a passive role where the view is\ncompletely manipulated by a controller or a presenter, and does not have knowledge of the\nmodel, in MVVM views are aware of the model and ViewModel.\nWhile views handle their own events, they do not maintain state. They must relay user\nactions to the ViewModel, which can be done through a mechanism such as data binding or\ncommands. A goal with the MVVM pattern is to minimize the amount of code in views.\nViewModel\nThe ViewModel in the MVVM pattern is similar to the controller and presenter objects that\nwe covered with the MVC and MVP patterns in that they coordinate between the view and\nthe model.\nViewModels provide data to views for display and manipulation, and also contain\ninteraction logic to communicate with views and models. ViewModels must be capable of\nhandling user actions and data input sent from views. It is the ViewModel that contains\nnavigation logic to handle moving to a different view.\n", "page": 266, "type": "text", "section": "Page 266"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 238 ]\nViews and ViewModels communicate through multiple methods, such as data binding,\ncommands, method calls, properties, and events.\nThe Command Query Responsibility\nSegregation pattern\nCommand Query Responsibility Segregation (CQRS) is a pattern in which the model that\nis used to read information is separated from the model that is used to update information.\nIn a more traditional architecture, a single object model is used for both reading and\nupdating data:\nCompromises become necessary in order to use a single object model as domain classes are\nrequired to serve all purposes. The same representation of an entity must support all of\nthe create, read, update, and delete (CRUD) operations, making them larger than they\nneed to be in all circumstances.\nThey contain all of the properties the object will need for various scenarios. If the class is\nmore than just a data transfer object (DTO), it may also contain methods for behavior.\nWith this approach, classes are not ideal for all of the situations in which they need to be\nused as there is often a mismatch between what is required by the read and write\nrepresentations of the data. This can also make managing security and authorization more\ncomplex as each class is used for both read and write operations.\nIn a collaborative domain, multiple operations may be taking place in parallel on the same\nset of data. There is a risk of data contention if records are locked, or update conflicts due to\nconcurrent updates. Workloads between read and write tasks differ, which means that they\nalso have different performance and scalability requirements.\n", "page": 267, "type": "text", "section": "Page 267"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 239 ]\nThe query model and the command model\nOne way to overcome the challenges of using a single object model for both queries and\ncommands is to separate the two. This pattern, known as CQRS, results in two separate\nmodels. The query model is responsible for reads and the command model is responsible\nfor updates:\nQuery objects only return data and do not alter state, while command objects alter state and\ndo not return data. Another way of looking at this concept is that asking a question (a\nquery) should not alter the answer. In order to perform an action that will mutate state, a\ncommand is used. \nWhen the system needs to read data, it goes through the query model, and when the\nsystem needs to update data, it goes through the command model. As part of processing a\ncommand, the system may need to read data, but beyond what is necessary for completing\na command, the reading of data should go through the query model.\nAlthough not required, if CQRS is taken to the next level, the query and command models\ncan be made to utilize separate databases. This gives each of the two models its own\nschema, which can be optimized for its specific usage. If separate databases are used, the\ntwo databases must be kept in sync. One way to do that is through the use of events.\n", "page": 268, "type": "text", "section": "Page 268"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 240 ]\nUsing event-sourcing with CQRS\nAlthough CQRS can be used without events, they do complement each other so it is\ncommon for systems that use CQRS to leverage the use of events. Events are one way to\neffectively communicate state changes so that the query model can stay up to date as the\ncommand model updates data.\nAs we saw in the Event-sourcing section earlier in this chapter, event-sourcing involves\npersisting events that take place in a system such that the event store can serve as the\nrecord of truth. When the command model alters the state of the system, events can be\nraised so that the query model can be kept in sync.\nKeeping the query and command models in sync is necessary when the query and\ncommand models use separate data stores. In addition, a data store for the query model\nmay contain denormalized data that has been optimized for particular queries. Having an\nevent store allows us to replay past events to recreate the system's current state, which can\nbe useful for updating the denormalized data in the query model's data store.\nAdvantages of CQRS\nCQRS is well suited to complex domains and it provides a separation of concerns that helps\nto minimize and manage the complexity. Separating a system into query and command\nmodels makes it more maintainable, extensible, and flexible. Development teams can also\nbe organized so that one team focuses on the query model while another focuses on the\ncommand model.\nSegregating the responsibility between commands and queries can help to improve\nperformance, scalability, and security. Performance can be improved by optimizing the\nschema specifically for each model. The schema for the query model can be optimized for\nqueries, while the schema for the command model can be optimized for updates. Data in\nthe query model's data store can be denormalized in order to increase the performance of\nqueries that the application needs to execute.\nWorkloads between read and write operations will differ and using CQRS allows you to\nscale each of them independently of the others. Security is improved with CQRS because it\nmakes it easier than with a single object model to ensure that only the right classes can\nupdate data.\nSecurity can be easier to implement and test when using CQRS because each class is either\nused for reads or writes, but not both. This reduces the chance of inadvertently exposing\ndata and operations that should not be available to a particular user in a certain context.  \n", "page": 269, "type": "text", "section": "Page 269"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 241 ]\nDisadvantages of CQRS\nFor systems that simply need basic CRUD operations, implementing a CQRS system may\nintroduce unnecessary complexity. CQRS systems have a higher degree of complexity,\nespecially when combined with event-sourcing. For this reason, it is important to\nunderstand that CQRS is not applicable to all situations. Software architects should be\naware that CQRS does not have to be applied to the entirety of a software system. It can be\napplied to just some subsystems of a larger architecture where it will be of the greatest\nbenefit.\nWhile using different data stores for query and command models can improve performance\nand increase security, you do have to consider that when you perform read operations, you\nmay be reading data that is stale.\nIf separate databases are used in a CQRS system, the database used for reads must be kept\nup to date with the database used for writes. The software system will follow an eventual\nconsistency model where if no new updates are made to a given item, eventually all access\nto that item will acquire the latest data.\nWhether the system is using event-sourcing or some other mechanism to keep the two in\nsync, there will be some time delay, even if it is a small one, before they are made\nconsistent. This means that any data that is read could potentially be stale if the latest\nupdates to it have not been applied yet. This is in contrast with a strong consistency model,\nin which all data changes are atomic and a transaction is not allowed to complete until all of\nthe changes have been completed successfully or, in the case of a failure, everything has\nbeen undone.\nService-oriented architecture\nService-oriented architecture (SOA) is an architectural pattern for developing software\nsystems by creating loosely coupled, interoperable services that work together to automate\nbusiness processes. A service is a part of a software application that performs a specific\ntask, providing functionality to other parts of the same software application or to other\nsoftware applications. Some examples of service consumers include web applications,\nmobile applications, desktop applications, and other services.\n", "page": 270, "type": "text", "section": "Page 270"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 242 ]\nSOA achieves a SoC, which is a design principle that separates a software system into parts,\nwith each part addressing a distinct concern. We discussed SoC in Chapter 6, Software\nDevelopment Principles and Practices. A key aspect of SOA is that it decomposes application\nlogic into smaller units that can be reused and distributed. By decomposing a large problem\ninto smaller, more manageable concerns satisfied by services, complexity is reduced and\nthe quality of the software is improved.\nEach service in a SOA encapsulates a certain piece of logic. This logic may be responsible\nfor a very specific task, a business process, or a subprocess. Services can vary in size and\none service can be composed of multiple other services to accomplish its task.\nWhat makes SOA different from other distributed\nsolutions?\nDistributing application logic and separating it into smaller, more manageable units is not\nwhat makes SOA different from previous approaches to distributed computing. Naturally,\nyou might think the biggest difference is the use of web services, but keep in mind that\nSOA does not require web services, although they happen to be a perfect technology to\nimplement with SOA. What really sets SOA apart from a traditional distributed\narchitecture is not the use of web services, but how its core components are designed.\nAlthough SOA shares similarities with earlier distributed solutions, it is much more than\njust another attempt to create reusable software. The differences undoubtedly provide\nsignificant new value to organizations when implemented properly. Many benefits can be\nrealized from a properly designed SOA.\nBenefits of using a SOA\nThere are a number of benefits from using a SOA, including:\nIncreases alignment between business and technology\nPromotes federation within an organization\nAllows for vendor diversity\nIncreases intrinsic interoperability\nWorks well with agile development methodologies\n", "page": 271, "type": "text", "section": "Page 271"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 243 ]\nIncreases alignment between business and technology\nA SOA leads to increased alignment between business and technology. The fulfillment of\nbusiness requirements needs business logic and business processes to be accurately\nrepresented in technology solutions. Business logic, in the form of business entities and\nbusiness processes, exists in the form of physical services with an SOA.\nThis alignment of business and technology promotes organizational agility. Change is\nsomething that practically all organizations must face, and it exists due to a variety of\nfactors, such as market forces, technology changes, new business opportunities, and\ncorporate mergers. Regardless of the cause of change, SOA provides organizational agility\nthrough service abstraction and loose coupling between business and application logic.\nWhen changes are required, they can more easily be made so that the business remains in\nalignment with the technology.\nPromotes federation within an organization\nA SOA promotes federation. Federation in an organization is an environment in which the\nsoftware applications and resources work together while simultaneously maintaining their\nautonomy. Federation gives organizations the freedom not to be required to replace all of\ntheir existing systems that must work together. As long as there is a common, open, and\nstandardized framework, legacy and non-legacy applications can work together.\nOrganizations have the flexibility to choose whether they want to replace certain systems,\nallowing them to use a phased approach to migration.\nAllows for vendor diversity\nAnother advantage of SOA is vendor diversity. In addition to allowing organizations with\npotentially disparate vendors to work together, an organization can use different vendors\ninternally to achieve best-of-breed solutions.\nWhile it is not a goal of SOA to increase vendor diversity, it provides the option of vendor\ndiversity when there is an advantage to introducing new technologies.\n", "page": 272, "type": "text", "section": "Page 272"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 244 ]\nIncreases intrinsic interoperability\nSOA provides increased intrinsic interoperability for an organization. It allows for the\nsharing of data and the reuse of logic. Different services can be assembled together to help\nautomate a variety of business processes. It can allow an existing software system to\nintegrate with others through web services. Greater interoperability can lead to the\nrealization of other strategic goals.\nWorks well with agile development methodologies\nAnother benefit of SOA is that it lends itself well to agile software development\nmethodologies. The fact that complex software systems are broken down into services with\nsmall, manageable units of logic fits well with an iterative process and how tasks are\nallocated to resources.\nYou may also find that having developers take on tasks is easier with SOA because each\ntask can be made to be manageable in size and more easily understood. Although this is\nbeneficial for any developer, it is particularly helpful for junior developers or those who are\nnew to a project and may not have as much experience with the functionality and business\ndomain.\nCost-benefit analysis of SOA\nAs a software architect, if you are considering SOA for an application, you will need to\nexplain the reasons why you are considering it. Adopting an SOA comes at some cost, but\nthere are points you can make to justify the costs and ways that you can alleviate them.\nThe cost of implementing a SOA may outweigh the benefits for some organizations, so each\ncase must be considered separately. While it may not be appropriate for some organizations\nto implement an SOA, for others a properly designed SOA will bring many benefits,\nincluding a positive return on investment.\nAdopting SOA can be a gradual, evolutionary process. Because a contemporary SOA\npromotes federation, creating an SOA does not have to be an all-or-nothing process. An\norganization does not have to replace all existing systems at once. Legacy logic can be\nencapsulated and can work with new application logic. As a result, the adoption of SOA\nand its related costs can be spread out over time.\nAdopting SOA leads to reduced integration expenses. A loosely coupled SOA should\nreduce complexity and therefore will reduce the cost to integrate and manage such systems.\nLoosely coupled services are more flexible and can be used in more situations.\n", "page": 273, "type": "text", "section": "Page 273"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 245 ]\nSOA can increase asset reuse. It is common for each application to be built in isolation,\nleading to higher development costs and greater maintenance costs over time. However, by\ncreating business processes by reusing existing services, costs and time to market can be\nreduced.\nSOA increases business agility. Change is something that all organizations must face.\nRegardless of the cause of the change, by using loosely coupled services, organizational\nagility is increased and both the time and cost to adapt to change are reduced.\nAdopting SOA reduces business risk and exposure. A properly designed SOA facilitates the\ncontrol of business processes, allows for the implementation of security and privacy\npolicies, and provides audit trails for data, which can all reduce risk. It can also help with\nregulatory compliance. The penalties for non-compliance can be significant, and SOA can\nprovide organizations with increased business visibility that reduces the risk of changing\nregulations.\nChallenges with SOA\nAlthough adopting an SOA does provide many benefits, it also introduces some new\ncomplexities and challenges. SOA solutions may allow organizations to do more, including\nautomating more of their business processes. This can cause enterprise architectures to\ngrow larger in scope and functionality as compared to legacy systems. Taking on a larger\nscope of functionality will add complexity to a software system.\nIn an SOA, new layers may be added to software architectures, providing more areas where\nfailure can occur and making it more difficult to pinpoint those failures. In addition, as\nincreasing numbers of services are created, deploying new services and new versions of\nexisting services must be managed carefully so that troubleshooting can be effective when\nan error occurs with a specific transaction.\nAnother challenge with successful SOA adoption is related to people and not technology.\nSOA is a mature architectural style that has been around a long time. The technology exists\nto allow organizations to automate a variety of complex business processes. However,\npeople can be a challenge to SOA-adoption because there are still technical and business\nprofessionals who are not familiar with SOA and really do not know what it means. People\ncan also be naturally resistant to change, and if your organization is not already using SOA,\nchange will be necessary. In order for a SOA to be successful, there has to be buy-in from\nthe people in the organization. They have to be committed and this comes down to the\nculture of the team and the people on it, including managers, software architects,\ndevelopers, and business analysts.\n", "page": 274, "type": "text", "section": "Page 274"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 246 ]\nKey principles for service orientation\nService-oriented solutions are designed so that they adhere to certain key principles. They\nare:\nStandardized service contract\nService loose coupling\nService abstraction\nService reusability\nService autonomy\nService statelessness\nService discoverability\nService composability\nThese principles are detailed in Thomas Erl's book, Service-Oriented Architecture, Second\nEdition. Service-orientation principles are applied to the service-oriented analysis and\ndesign phases of the SOA delivery life cycle.\nStandardized service contract\nEach service should have a standardized service contract, consisting of a technical interface\nand service description. Even though we want services to be independent, they have to\nadhere to a common agreement so that units of logic can maintain a certain level of\nstandardization.\nIn order to have standardized service contracts, all service contracts within a particular\nservice inventory should follow a set of design standards. Standardization enables\ninteroperability and allows the purpose of services to be more easily understood.\nService loose coupling\nServices should be loosely coupled and independent of each other. Service contracts should\nbe designed to have independence from service consumers and from their\nimplementations.\nLoosely coupled services can be modified faster and easier. Decoupling service contracts\nfrom their implementations allows service contracts to be modified with minimal impact to\nservice consumers and service implementations. By minimizing the dependencies between\nservices, each service can change and evolve independently while minimizing the effects of\nthose changes on other services.\n", "page": 275, "type": "text", "section": "Page 275"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 247 ]\nService abstraction\nService contracts should only contain information that it is necessary to reveal, and service\nimplementations should also hide their details. Any information that is not essential to\neffectively use the service can be abstracted out.\nDesign decisions, such as the technology used for a service, can be abstracted away. This\nfollows the information hiding principle that was covered in Chapter 6, Software\nDevelopment Principles and Practices. If a design decision needs to be changed later, the goal\nis that it can be made with minimal impact.\nService reusability\nServices should be designed with reusability in mind, with their service logic being\nindependent of a particular technology or business process. When services can be reused\nfor different purposes, software development teams experience increased productivity,\nleading to savings in both costs and time.\nService reusability increases organizational agility because organizations can use existing\nservices to respond to new business automation needs. Existing services can be composed\ntogether to create solutions for new problems or to take advantage of a new opportunity.\nReusable services can accelerate development and may allow a feature or product to reach\nthe market faster. In some cases, this can be critical for a project.\nDecomposing tasks into more services for reusability requires more analysis and potentially\nintroduces more complexity. However, when reusable services are designed correctly, there\ncan be significant long-term cost savings. If a need arises that is satisfied by an existing\nservice, resources do not have to be devoted to working on it.\nService reuse leads to higher quality software because existing services have already been\ntested. They may already be in production and, if there were any defects with the service,\nthey may have already been exposed and corrected.\nService autonomy\nServices should be designed to be autonomous, with more independence from their\nruntime environments. The design should seek to provide services with increased control\nover their runtime environments.\nWhen services can operate with less dependence on resources in their runtime\nenvironments that they cannot control, this leads to better performance and increased\nreliability of those services at runtime.\n", "page": 276, "type": "text", "section": "Page 276"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 248 ]\nService statelessness\nService designs should strive to minimize the amount of state management that takes place,\nand separate state data from services.\nServices can reduce their resource consumption if they do not manage state when it is\nunnecessary, which will allow them to handle more requests reliably. Having statelessness\nin services improves service scalability and improves the reusability of services.\nService discoverability\nServices need to be discoverable. By including consistent and meaningful metadata with a\nservice, the purpose of the service and the functionality it provides can be communicated.\nService developers are required to provide this metadata.\nServices should be discoverable by humans who are searching manually as well as software\napplications searching programmatically. Services must be aware of each other for them to\ninteract.\nService composability\nServices should be designed so that they are composable. This is the ability to use a service\nin any number of other services, and those services may themselves be composed of other\nservices.\nSome other service-orientation principles facilitate service composability. Service\ncomposition is heavily related to service reusability. The ability to create solutions by\ncomposing existing services provides organizations with one of the most important SOA\nbenefits: organizational agility.\nSOA delivery strategies\nThere are three main SOA delivery strategies: top-down, bottom-up, and agile. A delivery\nstrategy is needed to coordinate the delivery of application, business, and process services.\nThe three SOA delivery strategies mirror the main approaches to software architecture\ndesign that were covered in Chapter 5, Designing Software Architectures.\n", "page": 277, "type": "text", "section": "Page 277"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 249 ]\nThe top-down strategy\nThe top-down strategy begins with analysis. It centers on the organization's business logic\nand requires that business processes become service-oriented. The top-down approach,\nwhen done properly, results in a high quality SOA. Each service is thoroughly analyzed,\nand as a result, reusability is maximized.\nThe downside is that this approach requires many resources, in terms of time and money.\nThere is substantial pre-work that must take place with the top-down strategy. If an\norganization has the time and money to invest in a project, then this may be an effective\napproach.\nIt should be noted that, because analysis occurs at the beginning, it could be quite some\ntime before any results are realized. This may or may not be acceptable for a given project.\nIn order to meaningfully perform the service-oriented analysis and service-oriented design\nstages of the SOA life cycle, the top-down strategy has to be used at least to some extent.\nThe bottom-up strategy\nThe bottom-up approach, in contrast, begins with the web services themselves. They are\ncreated on an as-needed basis. Web services are designed and deployed based on immediate\nneeds.\nIntegration with an existing system is a common motivation for using the bottom-up\nstrategy. Organizations want to add web services to an existing application environment to\nallow for integration with a legacy system. A wrapper service is created to expose logic in\nan existing system.\nAlthough this approach is common in the industry, it is not a valid approach to achieving\nSOA. In order to create a valid SOA later, a lot of effort and refactoring will probably be\nrequired. Web services created with this approach may not be enterprise ready. They are\ncreated to serve some need, so if you are not careful they will not take into consideration\nthe enterprise as a whole.\n", "page": 278, "type": "text", "section": "Page 278"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 250 ]\nThe agile strategy\nThe third approach is an agile strategy, which is sometimes referred to as a meet-in-the-\nmiddle approach. It is a compromise between the top-down and bottom-up approaches. In\nthis approach, analysis can occur concurrently with design and development. As soon as\nenough analysis has been completed, design and development can begin. While such\nefforts are underway, analysis continues with other functionality. This approach pairs well\nwith an iterative, agile software development methodology.\nThis is sort of a best-of-both-worlds approach in that a proper design can be completed that\nwill yield all of the service-oriented qualities. This approach can fulfill immediate needs\nwhile maintaining service-oriented qualities of the architecture.\nHowever, as more analysis is finished, this approach may require completed services to be\nrevisited. Services can become misaligned after ongoing analysis, requiring them to be\nrefactored. \nService-oriented analysis\nService-oriented analysis is a stage in the SOA project life cycle and is used to decide what\nservices should be built and what logic should be encapsulated by each service. The\nanalysis is an iterative process that takes place once for each business process.\nWhen a team is committed to building a SOA, it should perform some form of analysis\nspecific to service-orientation and beyond standard analysis. One way that organizations\ncan improve service modeling is how they go about incorporating service-oriented analysis\nand design into their software development process. Each organization has its own\nsoftware development methodology and should determine how best to include service\nmodeling into their own process.\nService-Oriented Architecture, Second Edition, by Thomas Erl, details three steps to service-\noriented analysis: defining business automation requirements, identifying existing\nautomation systems, and modeling candidate services.\nDefining business automation requirements\nThe first step in service-oriented analysis is to define the business automation requirements\nfor the business process being analyzed in the current iteration. Requirements can be\ngathered using the organization's normal method of eliciting and capturing requirements.\n", "page": 279, "type": "text", "section": "Page 279"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 251 ]\nWith those requirements, the business process we want to automate can be documented at\na high level. The details of the business process are used when we model candidate\nservices.\nIdentifying existing automation systems\nOnce the requirements have been established for the current iteration, the next step in\nservice-oriented analysis involves identifying what parts, if any, of the business process\nlogic are already automated.\nTaking into consideration existing systems that may already automate all or part of any of\nthe business processes allows us to determine what parts of the business processes still\nneed to be automated. This information serves as an input when we model candidate\nservices.\nModeling candidate services\nThe final step, modeling candidate services, consists of identifying service operation\ncandidates and grouping them into candidate services. It is important to note that these\ncandidate operations and services are abstract and a logical model. During design, other\nfactors, such as constraints and limitations, will be considered. The final concrete design\nmay differ from the service candidates.\nModeling candidate services should be a collaborative process between technical and\nbusiness resources. Business analysts and domain experts can use their business knowledge\nto help the technical team define service candidates.\nService layers and service models\nEnterprise logic consists of both business and application logic. Business logic is an\nimplementation of the business requirements and includes an organization's business\nprocesses. These requirements include things such as constraints, dependencies, pre-\nconditions, and post-conditions.\nApplication logic is the implementation of business logic in a technology solution.\nApplication logic might be implemented in a purchased solution, a custom developed\nsolution, or some combination of the two. The development team works to design and\ndevelop the application logic. Topics such as performance requirements, security\nconstraints, and vendor dependencies are considered in the technical solution.\n", "page": 280, "type": "text", "section": "Page 280"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 252 ]\nService-orientation is related to business and application logic because a SOA is a way to\nrepresent, execute, and share that logic. Service-orientation principles can be applied to\nboth business and application logic.\nThe role of services is to realize the concepts and principles introduced by service-\norientation. A service layer in a software architecture is typically placed between the\nbusiness and application layers. This allows services to represent business logic and\nabstract application logic. Just as different applications within an organization's application\nlayer can be implemented in different technologies, services within the service layers can\nalso be implemented in different technologies.\nAbstraction is one of the important characteristics of an SOA and it enables other key\ncharacteristics, such as organizational agility. Abstraction is critical because abstracting\nbusiness and application logic allows for a service-oriented solution with loosely coupled\nservices. Achieving the appropriate level of abstraction is not a trivial task, but it can be\naccomplished with a dedicated team. By creating layers of abstraction, or service layers, the\nteam can figure out how services should represent application and business logic, and how\nto best promote agility.\nDuring service modeling, it becomes apparent that there are some common types of\nservices. These types are service models, which can be used to classify candidate services.\nThose candidate services can then be grouped together based on their service model into a\nservice layer.\nThe three common service models (and layers) are task service, entity service, and utility\nservice:\n", "page": 281, "type": "text", "section": "Page 281"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 253 ]\nTask service\nThis type of service has a non-agnostic functional context, which means that it contains \nbusiness process logic and was created for a specific business task or process. Task services\ndo not have a great deal of reuse potential.\nTask services typically compose multiple services in their logic and are sometimes referred\nto as task-centric business services or business process services.\nIf a task service has extensive orchestration logic or is hosted within an orchestration\nplatform, it is sometimes referred to as an orchestrated task service. Orchestration logic\nautomates the arrangement, coordination, and management of multiple services to\naccomplish a task.\nEntity service\nThis service model has an agnostic functional context, meaning its logic is not bound to a\nsingle business process and is reusable. Entity services are business-centric services that are\nassociated with one or more business entities. They are sometimes referred to as entity-\ncentric business services or business entity services.\nBusiness entities come from the business domain, and involving people on the team who\nthoroughly understand the business domain and business processes will be beneficial in\nperforming the analysis needed to determine the appropriate entity services. Entity services\nmay compose utility services in their logic.\nUtility service\nUtility services, like entity services, have an agnostic functional context. They contain multi-\npurpose logic and are highly reusable. The difference between entity services and utility\nservices is that utility services are not associated with a business entity or business logic.\nUtility services are concerned with technology-related functions, such as a software\nsystem's cross-cutting concerns. Examples include logging, caching, notifications,\nauthentication, and authorization.\nService-oriented design\nThe service-oriented design phase begins once the analysis is complete. Having a thorough\nunderstanding of the requirements and using the service models from the analysis stage\nwill allow for the creation of a proper service design.\n", "page": 282, "type": "text", "section": "Page 282"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 254 ]\nThe service-oriented design phase uses the logical candidate services that were derived\nduring service-oriented analysis and creates the physical service designs. Before designing\nthe implementation of the services, the first step is to design the physical service interfaces.\nWe need to make decisions regarding the service interfaces based on the service candidates,\nthe requirements that need to be met, as well as the organization and industry standards\nthat will be required to have a standardized service contract. Once the service contracts\nhave been established, the logic and implementation of the service can be designed.\nDesigning the service interfaces and implementing them are two distinct steps. We should\nfully focus on the service contracts first, independent of their implementations. Some teams\ndesign the two concurrently or skip to the development stage and just let the service\ninterface emerge from the implemented service.\nHowever, software architects should devote time to considering the service contracts before\nthe implementations. Not only do we need to ensure that the service contracts satisfy the\nrequirements, but they need to follow the key principles of service orientation, which\ninclude the fact that they should be loosely coupled from their implementations. Only after\nthe service contracts have been established do we need to consider the design of the\nimplementations.\nService interface design\nOne of the main goals of service-oriented design is to derive the physical service interface\ndefinitions based on the service candidates that were determined during service-oriented\nanalysis. Service interface design is significant because the design phase is the first time that\nreal technology is identified.\nIf you recall the key principles for service orientation, they need to be applied to the design\nof the service interfaces. Service contracts need to be standardized with each other and\nwithin a service inventory. They should be loosely coupled from their implementations,\nwith design decisions abstracted out so that the interfaces only contain what is essential for\nservice consumers.\nService interface design identifies internal and external exposure of the services. For\nexample, an order posting service may need to be used externally as well as internally. This\nis an example of a service that may need more than one interface published for the same\nservice. Each interface to the service may expose different operations and will require\ndifferent levels of security and authentication. The differences must be determined and\neach interface must be designed prior to the design of the implementations.\n", "page": 283, "type": "text", "section": "Page 283"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 255 ]\nIn addition to developers, a service interface plays an important role in testing and quality\nassurance. Testers need the service interface to design their tests. Once the service interface\nis known, a test harness can be created that is capable of invoking the service that needs to\nbe tested. Services need to be tested independently of other services as well as within the\nservices that will be consuming it.\nService interface granularity\nMaking decisions about interface granularity can be very important in service-oriented\ndesign. Granularity can have a significant impact on performance and other concerns.\nUsually, a service interface contains more than one operation, and the operations of a\nservice should be semantically related.\nFine-grained service operations offer greater flexibility to service consumers but lead to\nmore network overhead, which could reduce performance. The more coarse-grained\nservice operations we have, the less flexible they are, although they do reduce network\noverhead and could therefore improve performance.\nSoftware architects should seek to find the right balance between the number of services\nand the number of operations in each service. You do not want to group too many\noperations into a single service, even if they are semantically related, because it makes the\nservice too bulky and hard to understand. It may also increase the number of service\nversions that will need to be released going forward, as parts of the service need to be\nmodified. However, if your service interface is too fine-grained, you may end up with an\nunnecessarily large number of service interfaces.\nService registries\nA service registry contains information about the available services and is a key component\nof SOA governance. It helps to make systems interoperable and facilitates the discovery of\nweb services. Although some organizations may find that a service registry is not needed,\nmany SOA implementations can benefit from having a service registry. As an organization\nbegins to publish and use more and more web services, some of which may be outside the\norganization, the need for a centralized registry becomes more apparent.\n", "page": 284, "type": "text", "section": "Page 284"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 256 ]\nThere are numerous benefits to using a service registry. By promoting the discovery of web\nservices, organizations can facilitate reuse and avoid building multiple web services that\nperform similar tasks. Developers can programmatically query a service registry to\ndiscover web services that already exist that can satisfy their needs. Similar to the benefits\nthat are derived from any type of reused code, quality is improved and there is an\nincreased level of dependability from reusing web services. Reused web services have\nalready been tested and are usually already being successfully used in another part of the\nsystem or in another system.\nService registries can either be private or public. As the name implies, public registries can\ninclude any organization. This even includes organizations that do not have any web\nservices to offer. Private registries are restricted to those services that the organization\ndevelops itself or services that it has leased or purchased.\nThe benefits of public service registries include being able to find the right businesses and\nservices for a particular need. It can also lead to new customers or allow more access to\ncurrent customers. It could allow an organization to expand their offerings and extend their\nmarket reach. Service registries can be a useful and powerful tool for finding available web\nservices since they can be searched manually by people or programmatically through a\nstandardized API by an application. However, because of these capabilities, organizations\nshould take the time to decide whether to use a public or private registry and what services\nthey want to register into them.\nOne of the challenges of implementing a truly useful and reliable registry service is the\nadministration of the registry. This includes keeping it up to date by adding new services,\nremoving obsolete services, and updating versions, service descriptions, and web service\nlocations.\nService descriptions\nIn order for services to interact with each other, they must be aware of each other. Service\ndescriptions serve the important purpose of providing this awareness. They provide\ninformation about the available services so that potential consumers can decide whether a\nparticular service will satisfy their needs.\nService descriptions help to foster loose coupling, an important principle of SOAs.\nDependencies between services are minimized as services can work together simply\nthrough the awareness they have of each other through their service descriptions.\n", "page": 285, "type": "text", "section": "Page 285"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 257 ]\nAny service that wants to act as an ultimate receiver must have service description\ndocuments. Service descriptions typically have both abstract and concrete information. The\nabstract part details the service interface without getting into the details of the specific\ntechnologies being used. The beauty of the abstraction is that the integrity of the service\ndescription is maintained even if the details of the technical implementation are changed in\nthe future. The abstract description typically includes a high-level overview of the service\ninterface, including what operations it can perform. Input and output messages of the\noperations are also detailed.\nThe concrete part of the service description provides details about the physical transport\nprotocol that is connected to the web service interface. This specific transport and location\ninformation includes the binding (requirements for the service to establish a connection or\nfor a connection to be established with a service), port (physical address of the web service),\nand service (a group of related endpoints) so that the web service can be used.\nPossible challenges to developing service descriptions include the following:\nDecomposing web services properly based on business needs\nDetermining the exact purpose and responsibilities of a particular service\nDeciding on the operations that a web service will need to provide\nProperly communicating a service's interface in the abstract part of the service\ndescription so that potential service consumers can make an informed decision\nbased on their needs\nStructuring namespaces\nA namespace is a unique Uniform Resource Locator (URI). Namespaces are used to group\nrelated services and elements together and to differentiate between different ones that share\nthe same name. It is important for software architects to put thought into namespaces. By\nproviding a unique namespace, even if your organization uses services from another one,\nyour elements will be guaranteed to be unique. Even if two organizations have a service\nwith the same name, they will be differentiated by their namespace.\nIn addition to providing unique names, namespaces are used to logically organize various\nservices and elements. An appropriate namespace should provide meaning to the service or\nelement so that someone who is looking at it can gain an understanding of the service.\nNamespaces make it easier to name new services as well as to find existing ones.\n", "page": 286, "type": "text", "section": "Page 286"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 258 ]\nIn order to select good namespaces, we have to consider how namespaces are structured. A\ncompany's domain name is an important part of a namespace and because domain names\nare unique, they are commonly part of a namespace. Typically, the role follows the domain\nname in a namespace. This will allow differentiation between schema (for example,\nmessage types) and interfaces (for example, web services).\nA business area typically follows the role in the structure of a namespace. This is where\ndomain experts and business analysts can assist software architects in coming up with a\nbusiness structure that makes sense. Another part of a namespace that is common is some\nform of version or date. This allows differentiation between multiple versions of the same\nservice or element. Versioning is another important use of namespaces.\nOrchestration and choreography\nService orchestration and service choreography serve important roles in SOAs, as they are\napproaches to assembling multiple services so that they can work together.\nOrchestration represents business process logic in a standardized way using services. It\nautomates the execution of a workflow by coordinating and managing different services. In\nservice orchestration, there is a centralized process containing fixed logic. An orchestrator\ncontrols the process by deciding which services to invoke and when to invoke them.\nOrchestration is analogous to the conductor of an orchestra, who unifies and directs\nindividual performers to create an overall performance.\nInteroperability for an organization is promoted in solutions using orchestration because of\nthe integration endpoints that are introduced in processes. In a SOA, orchestrations\nthemselves are services. This promotes federation because multiple business processes,\npotentially from different applications, can be merged together.\nChoreography is another form of service composition. Choreographies define message\nexchanges and can involve multiple participants, each of which may assume multiple roles.\nIn contrast with orchestration, there is no centralized process, or orchestrator, that is\ncontrolling it. With choreography, there is an agreed upon set of coordinated interactions\nthat specify the conditions in which data will be exchanged. Each service in a choreography\nacts autonomously to execute its part based on the conditions that were established and the\nactions of the other participants.\n", "page": 287, "type": "text", "section": "Page 287"}
{"text": "Software Architecture Patterns\nChapter 7\n[ 259 ]\nBoth orchestration and choreography can be used for business-process logic owned by a\nsingle organization (intra-organization) and collaboration between multiple organizations\n(inter-organization). However, orchestration is less likely to be used when there are\nmultiple organizations involved because you would need to own and operate the\norchestration. Choreography allows for collaboration without having a single organization\ncontrol the whole process.\nSummary\nSoftware architects should be familiar with software architecture patterns, as they are a\npowerful tool when designing a software architecture. Architecture patterns provide a\nproven solution to recurring problems for a given context.\nLeveraging architecture patterns gives the software architect a high-level structure of the\nsoftware system, and provides a grouping of design decisions that have been repeated and\nused successfully. Using them reduces complexity by placing constraints on the design and\nallows us to anticipate the qualities that the software system will exhibit once it is\nimplemented.\nIn this chapter, you learned about some of the common software architecture patterns\navailable, including layered architecture, EDA, MVC, MVP, MVVM, CQRS, and SOA.\nThe focus of the next chapter is on some of the relatively newer software architecture\npatterns and paradigms. These include microservice architecture, serverless architecture,\nand cloud-native applications. As cloud deployment of software applications becomes the\ndominant trend, these concepts become crucial for any software architect to understand.\n", "page": 288, "type": "text", "section": "Page 288"}
{"text": "8\nArchitecting Modern\nApplications\nModern applications deployed to the cloud have different expectations and requirements\nfrom applications designed in the past. New software architecture patterns and paradigms\nhave been introduced to meet those expectations.\nIn this chapter, we will explore some of these patterns and approaches to software design\nand development. We will begin with a look at monolithic architecture, along with the\nreasons why it should or should not be used. We will then explore microservice\narchitecture (MSA), serverless architecture, and cloud-native applications.\nIn this chapter, we will cover the following topics:\nMonolithic architecture\nMicroservice architecture\nServerless architecture\nCloud-native applications\nMonolithic architecture\nA monolithic architecture is one in which a software application is designed to work as a\nsingle, self-contained unit. Applications that have this type of architecture are common. The\ncomponents within a monolithic architecture are interconnected and interdependent,\nresulting in tightly coupled code.\n", "page": 289, "type": "text", "section": "Page 289"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 261 ]\nThe following diagram shows an application with a monolithic architecture: \nThe different concerns of an application, such as user interface, business logic,\nauthorization, logging, and database access, are not kept separate in a monolithic\narchitecture. These different pieces of functionality are intertwined in a monolithic\napplication.\nBenefits of a monolithic architecture\nDespite some obvious disadvantages to using a monolithic architecture, if an application is\nrelatively small, then there are benefits to using one. Applications with a monolithic\narchitecture typically have better performance. With the interaction between the machine\nrunning the application and other machines minimized, better levels of performance are\nrealized.\nSmall applications that have this type of architecture are easier to deploy because of the\nsimplicity of the high-level architecture. In spite of the tightly coupled logic, monolithic\napplications can be easier to test and debug because they are simpler, with fewer separate\ncomponents to consider.\nMonolithic applications are typically easy to scale because all it takes is to run multiple\ninstances of the same application. However, different application components have\ndifferent scaling needs and we cannot scale the components independently with a\nmonolithic architecture. We are limited to adding more instances of the entire application in\norder to scale.\n", "page": 290, "type": "text", "section": "Page 290"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 262 ]\nDrawbacks of a monolithic architecture\nAlthough a monolithic architecture may work for some applications, as applications grow\nin size and complexity, there are serious drawbacks. Monolithic applications greatly inhibit\nthe agility of the organization as it becomes difficult to make changes to the software. One\naspect of this is the fact that continuous deployment is difficult to achieve. Even if a change\nis made to only one component of a monolithic application, the entire software system will\nneed to be deployed. Organizations are required to devote more resources, such as time\nand testers, to deploy a new version of a monolithic application.\nIf the application is small, it can be easy to maintain due to the simplicity of the\narchitecture. However, larger and more complex monolithic applications start to suffer in\nterms of maintainability. Tightly coupled components make it more difficult to make\nchanges because a change in one part of the application is more likely to affect other parts\nof the application.\nThe large codebase of a monolithic application can make it difficult for team members to\nunderstand it. This is especially true for new team members as they attempt to become\nfamiliar with it and begin working with it.\nEven loading a large, monolithic application into an integrated development environment\n(IDE) and working with it can be frustrating due to slower performance of the IDE. It also\ntakes longer for such applications to start up, lowering the productivity of the team during\ndevelopment.\nMonolithic applications require a commitment to a particular programming language and\ntechnology stack. Since the application is written as a single unit, it makes it more difficult\nto introduce different types of technology. In some cases, it can even make it difficult to\nmove to a newer version of the same technology! If a migration to a different technology is\nneeded, it requires the organization to commit to rewriting the entire application.\nLarger and more complex applications benefit from dividing responsibilities up among\nmultiple development teams, such as having each team focus on a particular functional\narea. However, this becomes difficult to do with a monolithic application as changes made\nby one development team may affect another development team.\nDue to these drawbacks, software applications that are large and complex should move\naway from being a monolithic application. MSA and serverless architecture are alternatives\nto monolithic architectures. They address some of the concerns and limitations of\nmonolithic applications.\n", "page": 291, "type": "text", "section": "Page 291"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 263 ]\nMicroservice architecture\nThe microservice architecture (MSA) pattern builds software applications using small,\nautonomous, independently versioned, self-contained services. These services use well-\ndefined interfaces and communicate with each other over standard, lightweight protocols.\nInteraction with a microservice takes place through a well-defined interface. A microservice\nshould be a black box to the consumers of the service, hiding its implementation and\ncomplexity. Each microservice focuses on doing one thing well and they can work together\nwith other microservices in order to accomplish tasks that are more complex.\nA microservice architecture is particularly well-suited for large and/or complex software\nsystems. In contrast with the monolithic architecture, applications built using a\nmicroservice architecture handle complexity by splitting the application into smaller\nservices that are easier to manage. The following diagram illustrates a system that has a\nmicroservices architecture:\nIncoming requests are commonly handled by an API gateway, which serves as the entry\npoint to the system. It is an HTTP server that takes requests from clients and routes them to\nthe appropriate microservice through its routing configuration. In the Service\ndiscovery section later in this chapter, we will take a closer look at service discovery to\nexplain how API gateways can get the locations of available service instances.\nSOA done right\nMicroservice architecture emerged, not as a solution looking for a problem, but as the result\nof the shortcomings and drawbacks of the traditional service-oriented architecture (SOA)\nand monolithic architecture.\n", "page": 292, "type": "text", "section": "Page 292"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 264 ]\nSOA provides a number of benefits, which were covered in Chapter 7, Software Architecture\nPatterns, such as increasing the alignment between business and technology, promoting\nfederation within an organization, allowing for vendor diversity, and increasing\ninteroperability.\nAlthough microservice architecture is a variation of the service-oriented architecture, there\nare key differences between the two. Some people even refer to the microservice\narchitecture as SOA done right.\nIn addition to moving us away from monolithic architectures, microservice architecture\nimproves on traditional SOA. Implementing SOA can be quite expensive, difficult to\nimplement, and may be overkill for many applications. The microservice architecture\npattern addresses some of these drawbacks. Rather than using an enterprise service bus\n(ESB), as is common with SOA, microservice architecture favors implementing ESB-like\nfunctionality in the services themselves.\nCharacteristics of microservice architecture\nThere are a number of characteristics related to the microservice architecture. In this\nsection, we will examine the following characteristics:\nSmall, focused services\nWell-defined service interfaces\nAutonomous and independently deployable services\nIndependent data storage\nCommunicating with lightweight protocols\nBetter fault isolation\nSmall, focused services\nThe functionality of a microservice should be small in scope. Each microservice should\nfollow the tenet of the Unix philosophy that advocates for doing one thing and doing it\nwell. Keeping the scope small for each service allows the complexity of the software to be\nbetter managed.\nApplications with a microservice architecture are easier to understand and modify because\nevery service has a focused responsibility. This allows the services to be developed faster.\nSmaller solutions are easier to load up and use in an IDE, which increases productivity. In\naddition, new team members are able to become productive in a shorter amount of time.\n", "page": 293, "type": "text", "section": "Page 293"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 265 ]\nA microservice can be developed by a small team. This allows organizations to easily split\nup work among multiple development teams. Each microservice can be worked on by a\nsingle team, independent of other microservices. Tasks for different microservices can be\nworked on in parallel.\nIf a software system is also using domain-driven design (DDD), the concept of bounded\ncontexts works well with microservices as it helps with the partitioning of services. In some\nsystems, each microservice is implemented as its own bounded context.\nWell-defined service interfaces\nMicroservices are treated like black boxes, hiding their complexity and implementation\ndetails from service consumers. This makes a well-defined interface, with clear entry and\nexit points, which is important to help facilitate microservices working together. Services\ninteract with each other through their interfaces.\nAutonomous and independently deployable services\nAn application using a microservice architecture consists of a system of autonomous\nservices. The services should be loosely coupled, interacting through their well-defined\ninterfaces and not dependent on the implementation of the service.\nThis allows the implementation of services to change and evolve independently of the other\nservices. As long as the service interface is unchanged, modifying a microservice is less\nlikely to require changes to other parts of the application.\nAutonomous services are independently deployable, making it easier to deploy them to\nproduction. A microservice architecture enables continuous deployment because it is easier\nto release updates to the services. If a change is made to a microservice, it can be deployed\nindependently of the other microservices.\nThe autonomy of the services increases organizational agility. It allows organizations to\nquickly adapt to changing business requirements and take advantage of new business\nopportunities.\nIndependent data storage\nOne of the characteristics that support service autonomy is that each microservice can have\nits own data store. This helps services to be independent and loosely coupled to other\nservices. A service's data store can be altered without those changes affecting other services.\n", "page": 294, "type": "text", "section": "Page 294"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 266 ]\nDesigning each microservice to have its own data store may come in the form of a\nmicroservice having its own database, but that is not the only way to keep a microservice's\ndata storage private. If the data storage technology is a relational database management\nsystem (RDBMS), then in addition to the option of having a separate database server, data\ncan be kept separate by designating certain tables to be owned by a particular service.\nAnother option is to designate a schema that is to only be used by a single microservice.\nIn the Polyglot persistence section later in this chapter, you will learn about some additional\nbenefits to using a separate database for each microservice.\nBetter fault isolation\nA system built on a microservice architecture improves fault isolation. When one\nmicroservice goes down, other services can still operate normally, allowing other parts of\nthe system to remain operational. This is in contrast to a monolithic application, where a\nfault can potentially bring down the entire system.\nCommunicating with lightweight message protocols\nMicroservices should communicate using well-known, lightweight message protocols.\nThere is no rule dictating a particular protocol, and microservices can communicate\nsynchronously or asynchronously. A common implementation for microservices is to have\nthem expose HTTP endpoints that are invoked through REST API calls. For synchronous\ncommunication, REST is one of the preferred protocols. It is common for REST to be used\nwith JavaScript Object Notation (JSON). Service operations can accept and return data in\nthe JSON format as it is a popular and lightweight data-interchange format.\nSome applications may require asynchronous communication. A common messaging\nprotocol used for asynchronous communication with microservices is Advanced Message\nQueueing Protocol (AMQP). It is an open standard that can connect a variety of services,\nincluding those on different platforms and across organizations. It was designed for\nsecurity, reliability, and interoperability.\nAMQP can support the following types of message-delivery guarantees:\nAt least once: A message is guaranteed to be delivered but it may be delivered\nmultiple times\nAt most once: A message is guaranteed to be delivered once or never\nExactly once: A message is guaranteed to be delivered once and only once\n", "page": 295, "type": "text", "section": "Page 295"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 267 ]\nAnother protocol that is popular with microservices is gRPC. It was designed by Google as\nan alternative to REST and other protocols. It is an open source protocol that was designed\nto be faster and more compact than other protocols used for distributed systems.\ngRPC is built on protocol buffers, also known as protobufs, which is a way of serializing\ndata that is language and platform neutral. This allows gRPC to efficiently connect polyglot\nservices because it supports a variety of programming languages. The increased prevalence\nof containerized applications and microservices has made gRPC popular. Given modern\nworkloads, gRPC is an attractive choice because it is a high-performance and lightweight\nprotocol. gRPC is inherently efficient, but it is also based on HTTP/2. This yields additional\nbenefits such as decreased latency and higher data compression.\nThe communication protocol that you select for your microservice architecture really\ndepends on your requirements. There is no silver bullet answer that will apply to all\nsituations. The context of your design concerns will drive the selection, such as whether\nyour communication needs are synchronous or asynchronous.\nDesigning polyglot microservices\nOne of the many advantages of using a microservice architecture is that it affords you the\noption of using multiple programming languages, runtimes, frameworks, and data storage\ntechnologies.\nMonolithic applications focus on using a particular programming language and technology\nstack. Because that type of application is written as a single unit, it is more difficult to take\nadvantage of different types of technology. However, complex applications need to solve a\nvariety of problems. Being able to select different technologies for different problems can be\nuseful rather than trying to solve all of the problems with a single technology.\nA development team can select best-of-breed solutions depending on the task they need to\ncomplete. A microservice architecture allows teams to experiment and try out new\ntechnologies with, having to commit to them for the entire system.\nHaving polyglot microservices is certainly not required when using a microservice\narchitecture. In many cases, an organization will focus on a limited number of technologies,\nand the skillsets of the development team will reflect that. However, software architects\nshould be aware of the option and recognize opportunities where it can be used effectively.\nTwo of the concepts related to polyglot microservices are polyglot programming and\npolyglot persistence.\n", "page": 296, "type": "text", "section": "Page 296"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 268 ]\nPolyglot programming\nWith polyglot programming, a single application uses multiple programming languages in\nits implementation. It can be useful to take advantage of the strengths of different\nprogramming languages to handle different tasks within an application.\nA microservice architecture allows each microservice to be developed using the\nprogramming language that best fits the problem at hand. When a new technology\nbecomes available, an existing application can take advantage of it for new microservices\nthat are developed or for new versions of an existing microservice.\nPolyglot persistence\nSimilar to polyglot programming, there is the concept of polyglot persistence, in which\nmultiple persistence options are used within a single application. Different data storage\ntechnologies are better suited to different tasks, and microservices allows you to take\nadvantage of that.\nEach microservice is in charge of its own data storage, so it can choose the best data storage\ntechnology based on what it is trying to achieve. The following diagram depicts two\nmicroservices, each with its own database:\nA microservice dedicated to providing product recommendations based on friends, ratings,\nand prior purchases would be ideal for a graph database. A product catalog with details\nabout a company's products, requiring frequent reads and infrequent writes, could be\nimplemented with a document database. When processing an order, the transactional\ncapabilities of a relational database might be ideal.\n", "page": 297, "type": "text", "section": "Page 297"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 269 ]\nA single microservice can even use more than one data storage technology, although that is\nfairly unusual in practice. It introduces extra complexity, and if you find yourself wanting\nto do that, you have to consider whether your microservice is too large in scope. In that\ncase, perhaps the microservice needs to be divided into more than one.\nUsing too many technologies\nAs with many things, polyglot programming and polyglot persistence can be overdone.\nThere is a cost to introducing too many disparate technologies within a single organization.\nMastering a technology is difficult and you will need staff members who are well-versed in\nthe technologies that you are using. The use of different technologies may create a need for\nmore developer training, which costs the organization money as well as its employees'\ntime.\nUsing different programming languages and data storage technologies introduces\ncomplexity into the build, deployment, and test processes. The software will need to be\ncontinually maintained after it is released to production, requiring a diverse development\nteam capable of handling all of the different technologies.\nHaving the ability to take advantage of different technologies can be a valuable asset, but\nconsideration must be made each time using a new one is being considered to ensure that it\nis the best decision for the project.\nConsidering service granularity\nThe granularity of a service refers to the scope of its business functionality, which varies for\neach service. With microservices, the goal is to have fine-grained services so that each one\nfocuses on a single business capability.\nIt is important to get the granularity of microservices correct to ensure that the system\nconsists of appropriately sized services. One of the goals of a microservice architecture is to\ndecompose a domain into small, focused, and reusable services. Smaller services carry with\nthem less context, increasing their level of reusability.\nNanoservices\nSoftware architects and developers who are designing microservices should be careful not\nto make the granularity of their services too fine-grained, though. Services whose\ngranularity is too fine-grained are referred to as nanoservices and this is considered an\nanti-pattern.\n", "page": 298, "type": "text", "section": "Page 298"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 270 ]\nA system with very small services tends to have more services and, as the number of\nservices in a system increases, so does the amount of communication that must take place.\nServices use up network resources that are not infinite, and services are bound by these\nlimitations. Having too many services can lead to the reduced performance of your services\nand of your overall application.\nWhen a system has many nanoservices, there is also an increase in the overall overhead for\nthe services. Each service requires some management, including things such as\nconfiguration and entry into a service registry. Larger numbers of services lead to\nincreasing amounts of overhead.\nNanoservices can lead to fragmented logic. If a single business task that fits well into a\nsingle, cohesive service is decomposed further into multiple, smaller services, the logic\nbecomes separated. Making services unnecessarily small is part of the nanoservice, anti-\npattern.\nWhen the overhead of a service outweighs its utility, then it is a nanoservice and\nrefactoring should be considered. Nanoservices can be refactored by either combining\nmultiple nanoservices into a new, larger service or moving the functionality of each\nnanoservice into an appropriate existing service.\nThere may be exceptions in which a nanoservice is appropriately sized and does not require\nrefactoring, so it is always prudent to use your best judgment. Although you do not want\nyour system to consist of too many nanoservices, sometimes, the functionality of a\nparticular nanoservice simply does not belong anywhere else.\nSharing dependencies between microservices\nDevelopment teams should avoid sharing dependencies, such as frameworks and third-\nparty libraries, between microservices. You may have multiple microservices that share the\nsame dependency, so it is natural to think about sharing them on the host to make them\ncentrally available.\nHowever, each microservice should remain independent of other microservices. If we want\nto update the dependencies, we don't want to affect any other services. Doing so would\nincrease the risk of introducing defects and will broaden the scope of the testing that will\nneed to be conducted related to the change.\nSharing dependencies also introduces host affinity, which we want to avoid. Microservices\nshould function independently of the host they are deployed on because assumptions\ncannot be made regarding the host on which a service will execute.\n", "page": 299, "type": "text", "section": "Page 299"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 271 ]\nStateless versus stateful microservices\nEach microservice can either be stateless or stateful. A system that uses microservices\ntypically has a stateless web and/or mobile application that uses stateless and/or stateful\nservices.\nStateless microservices do not maintain any state within the services across calls. They take\nin a request, process it, and send a response back without persisting any state information.\nA stateful microservice persists state in some form in order for it to function.\nRather than store this state internally, a microservice should store state information\nexternally, in some type of data store. Examples of a data store to persist state include a\nrelational database management system (RDBMS), a NoSQL database, or some type of\ncloud storage. Persisting the state externally provides availability, reliability, scalability,\nand consistency for the state information.\nService discovery\nThe client of a service, whether it is an API gateway or another service, needs the ability to\ndiscover the location of a service instance. In a traditional distributed environment, service\nlocations (IP address and port) are generally static, and a service instance can be found\neasily. For example, service locations could be read from a configuration file.\nHowever, service discovery is more complex with a cloud-based application using\nmicroservices. The number and location of service instances changes dynamically in the\ncloud. A service registry can be used to keep track of service instances and their locations.\nUsing a service registry\nA service registry plays a key role in service discovery. It is a database containing service\ninstances and their locations. The service registry must be highly available and kept up to\ndate. In order to be accurate, service instances must be registered and deregistered with the\nservice registry. This can be accomplished through either self-registration or third-party\nregistration.\n", "page": 300, "type": "text", "section": "Page 300"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 272 ]\nSelf-registration pattern\nUsing the self-registration pattern, service instances are responsible for registering with,\nand deregistering from, the service registry:\nWhen a service instance starts up, it must register itself with the service registry. Similarly,\nwhen a service instance shuts down, it must unregister itself from the service registry.\nThis approach is relatively straightforward, although it does couple the service instances to\nthe service registry. If you have a polyglot system in which different microservices are\nusing different programming languages and/or frameworks, service registration logic must\nexist for each programming language and framework that is used for the development of\nyour microservices.\nIt is a common requirement to have registered service instances periodically renew their\nregistration or send a heartbeat request to indicate they are still alive and responsive. If a\nservice instance does not do this, then they can be deregistered automatically. Doing this\nwill handle situations in which a service instance is running but is unavailable for some\nreason. Such service instances may not be able to unregister themselves from the service\nregistry.\nFor small applications, the self-registration pattern may be sufficient, but large applications\nwill want to use the third-party registration pattern.\nThird-party registration pattern\nWhen using the third-party registration pattern, a dedicated component, sometimes\nreferred to as the service registrar, handles registering, deregistering, and checking the health\nof service instances. Like the service registry itself, the service registrar is an important\ncomponent and therefore must be highly available.\n", "page": 301, "type": "text", "section": "Page 301"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 273 ]\nThe following diagram illustrates the third-party registration pattern:\nBy either polling the available service instances or subscribing to relevant events, the\nservice registrar can register new service instances and deregister service instances that no\nlonger exist. It can perform health checks on service instances and take appropriate actions\nbased on the result.\nUnlike the self-registration pattern, the service instances are decoupled from the service\nregistry. Services can focus on their single responsibility and not have to be concerned with\nservice registration. If different microservices in your system are using different\nprogramming languages and/or frameworks, you do not need to implement service\nregistration logic for each one that is used by the development team(s).\nOne drawback with this pattern is that unless the service registrar is a built-in component\nof your deployment environment, it is another component that must be set up and\nmanaged.\nTypes of service discovery\nThere are two main patterns for service discovery:\nClient-side discovery pattern \nServer-side discovery pattern\nWe will now look at both types in detail.\n", "page": 302, "type": "text", "section": "Page 302"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 274 ]\nClient-side discovery pattern\nWith the client-side discovery pattern, the service client, whether it is an API gateway or\nanother service, queries a service registry for the locations of available service instances:\nOnce the locations of available service instances are obtained from the service registry, the\nservice client uses a load balancing algorithm to select one of them. At that point, the\nservice client can interact with a specific service instance.\nThis pattern is straightforward, although it does couple the service client with the service\nregistry. For organizations that are taking advantage of using multiple programming\nlanguages and/or frameworks for their microservice development, service-discovery logic\nwill need to be written for each programming language and framework that is used for the\ndevelopment of microservices.\nServer-side discovery pattern\nThe other main pattern for service discovery is the server-side discovery pattern. The\nservice client, such as an API gateway or another service, makes a request to a router. The\nrouter is typically a load balancer.\n", "page": 303, "type": "text", "section": "Page 303"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 275 ]\nThe following diagram illustrates the server-side discovery pattern:\nIn this pattern, it is the load balancer that queries the service registry for the locations of\navailable service instances. The service registry may be built into the load balancer or it may\nbe a separate component. The load balancer is then responsible for forwarding the request\nto one of the available service instances.\nUnlike the client-side discovery pattern, service clients are decoupled from the service\nregistry. Service client code is simpler as it does not have to be concerned with interacting\nwith the service registry or implementing a load balancing algorithm. Service clients can\nsimply make a request to the load balancer. If different programming\nlanguages/frameworks are being used for microservice development, there is no need to\nimplement service discovery logic for each one.\nA drawback to this pattern is that unless the cloud provider is providing the load balancer\nand associated functionality (service registry), it is another component that must be\ninstalled and managed. Also, there are more network hops involved with this pattern due\nto the load balancer compared with the client-side discovery pattern.\nUsing microservices is not for everyone\nAs is the case with other types of architecture patterns, there are disadvantages to using a\nmicroservice architecture. The benefits of using a microservice architecture must outweigh\nthe added complexity, so it is not the ideal solution for all types of applications.\n", "page": 304, "type": "text", "section": "Page 304"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 276 ]\nAs a distributed system, a microservice architecture introduces complexity simply not\nfound in a monolithic application. When multiple services are working together in a\ndistributed system and something goes wrong, there is added complexity in figuring out\nwhat and where something failed. A service may not respond downstream and the system\nmust be able to handle the disruption.\nDecomposing a complex system into the right set of microservices can be difficult. It\nrequires a knowledge of the domain and can be somewhat of an art. You don't want a\nsystem with services that are too fine-grained, resulting in a large number of services. As\nthe number of services increase, the management of those services becomes increasingly\ncomplex.\nAt the same time, you do not want the services of a system to be too coarse-grained, so that\nthey are responsible for too much functionality. The last thing you want is a bunch of\nservices that are tightly coupled, making it so that they have to be deployed together. If you\nare not careful, you will end up with a microservice architecture that is a monolith in\ndisguise.\nThe use of multiple databases is another challenge when using a microservice architecture.\nIt is common for a business transaction to update multiple entities, which will require the\nuse of multiple microservices. With each one having its own database, this means that\nupdates must take place in multiple databases. One way to handle this is through event-\nsourcing and having eventual consistency. Even if that is acceptable, implementation of\nevent-sourcing is another added complexity.\nService clients, such as an API gateway and other services, will need a way to know the\nlocation of available service instances. Unless a cloud provider is providing a service\nregistry and associated functionality, these additional components will need to be\nconfigured and managed.\nServerless architecture\nServerless architecture allows the rapid development of software applications that can\nhandle various levels of traffic in production environments. The term serverless refers to the\nfact that compute services are provided without requiring you to manage or administer\nservers. Your code is executed on demand, as it is needed.\nUtilizing compute services in this way is similar to how cloud storage is used. With cloud\nstorage, you do not need to manage physical hardware and you do not need to know where\nthe data is stored. You use as much or as little storage as you need.\n", "page": 305, "type": "text", "section": "Page 305"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 277 ]\nSimilarly, with serverless architecture, you do not need to deal with physical servers, and\nthe complexity of how compute resources are provided is hidden from you. Software\napplications use as much or as little compute capacity as they need.\nServerless architecture is maturing and its use is increasingly common. It can be the ideal\narchitectural choice for some software applications. A number of cloud vendors, including\nAmazon, Microsoft, Google, and IBM, provide compute services.\nServerless architecture can be used for many types of software applications and tasks. Some\nof the common types of applications that are suited to a serverless architecture include web\napplications, event-driven data processing, event workflows, scheduled tasks (CRON jobs),\nmobile applications, chatbots, and Internet of Things (IoT) applications. It is also good for\ndata transformation tasks related to things such as images/video (for example, for the\npurposes of compression and/or optimization), voice packets, and PDF generation.\nThe following diagram shows an example of a system that has a serverless architecture:\nServerless architectures use function as a service (FaaS) and backend as a service (BaaS)\nmodels to provide backend logic for software applications. The two are used together to\nprovide the functionality of the application.\nFunction as a Service (FaaS)\nIn a serverless architecture, a small piece of code, like a function, can be executed using an\nephemeral compute service to produce a result. This is known as Function as a Service\n(FaaS). By ephemeral, we mean that it will only last for a limited amount of time. The code\nexecutes in a container that is spun up on invocation and then brought back down when it\nis complete.\n", "page": 306, "type": "text", "section": "Page 306"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 278 ]\nFunctions are typically invoked due to events or HTTP requests. When a function\ncompletes its processing, it can either return a value to the caller or pass the result to\nanother function that will be invoked as part of a workflow. The output can either be\nstructured, such as an HTTP response object, or unstructured, such as a string or an integer.\nEach function should follow the single responsibility principle (SRP) and serve a single,\nwell-defined purpose. Development teams can code functions to perform server-side logic\nand then upload the code to their service provider for on-demand execution. FaaS makes it\neasy to go from writing code on your laptop to having it execute in the cloud.\nFunctions should be designed to be idempotent, so that multiple executions of the same\nrequest yield the same result, and if the same request is processed more than once, there\nshould not be any adverse effects.\nCopies of messages may exist on multiple servers to provide redundancy and high\navailability. In order to ensure at-least-once delivery, it is possible that a function may be\ninvoked more than once. For example, a server with a copy of a message may become\nunavailable when a message is received or deleted, leading to the same message being sent\nto a function again.\nThe functions in FaaS can be either synchronous or asynchronous, depending on the needs\nof the task. One of the ways that asynchronous functions work in FaaS is that the platform\nwill return a unique identifier that can then be used to poll the status of the asynchronous\noperation.\nAn important part of a serverless architecture is its API gateway. An API gateway is the\nentry point to a system. It is an HTTP server that takes requests from clients and uses its\nrouting configuration to route them to the relevant function container. The functions run in\ncompute containers and should be stateless. The result of the FaaS function is then sent\nback to the API gateway and ultimately returned to the client as an HTTP response.\nImplementations of FaaS that are available from providers include Amazon Web Services\nLambda, Microsoft Azure Functions, Google Cloud Functions, and IBM Cloud Functions.\nBackend as a Service (BaaS)\nBackend as a Service (BaaS), which has its roots in Mobile Backend as a Service (MBaaS),\nis a model that allows developers to take advantage of service applications provided by\nthird parties. This reduces development time and costs, as teams do not have to write these\nservices in-house. In contrast with FaaS, where development teams write their own code for\nthe various functions, BaaS offers the use of existing services.\n", "page": 307, "type": "text", "section": "Page 307"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 279 ]\nExamples of the functionality provided by service applications include a database, push\nnotifications, file storage, and authentication services. In the serverless architecture\ndiagram, the authentication service and the storage service for the static resources are\nexamples of BaaS.\nAdvantages of serverless architectures\nUsing a serverless architecture provides many important benefits, which explains why it is\nbecoming increasingly popular.\nCost savings\nWith a serverless architecture, your code is only executed when it is needed. You get utility\nbilling, in that you are only charged for the actual compute resources that are used.\nOrganizations moving to a serverless architecture will reduce hardware costs, as there are\nno longer servers and network infrastructure to support. In addition, organizations do not\nhave to hire staff to support all of that infrastructure. These cost savings are similar to what\norganizations might save from using Infrastructure as a Service (IaaS) or Platform as a\nService (PaaS), except that there are also reduced development costs (taking advantage of\nBaaS) and scaling costs (taking advantage of FaaS).\nScalable and flexible\nYou can't underprovision or overprovision your compute capacity with a serverless\narchitecture. You avoid being in a situation where you do not have enough servers during\nperiods of peak capacity or have too many servers sitting idly during off-peak periods.\nThe scalability and flexibility of serverless architectures allows for compute capacity to\nscale up and down as demand changes. You only use as much compute capacity as you\nneed and you are charged for the amount that you use. Wasting compute capacity is kept to\na minimum and organizations will experience lowered costs as a result.\nFocus on building your core products\nAnother one of the main advantages of using a serverless architecture is that there are no\nservers to manage. This allows organizations to focus on creating solutions and shipping\nmore features.\n", "page": 308, "type": "text", "section": "Page 308"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 280 ]\nNot having to administer an infrastructure increases productivity and reduces the time to\nmarket. Even a small development team can start building an application and deploy it to\nproduction relatively quickly because they won't need to provision an infrastructure up\nfront and they won't have as much to manage after deployment. A development team that\nwants to build an application quickly while worrying less about operational concerns will\nfind serverless architecture attractive.\nPolyglot development\nA serverless architecture allows for polyglot development. Development teams are\nprovided with the ability to select best-of-breed languages and runtimes based on the\nrequired functionality. It also gives teams a chance to easily try out and experiment with\ndifferent technologies. Although there are limitations as to which languages are available to\nyou, cloud providers are expanding their selection so that different languages can be used\nfor the different functions being developed.\nDisadvantages of serverless architectures\nAlthough there are many benefits to using a serverless architecture, you should be aware\nthat there are some drawbacks.\nDifficulties with debugging and monitoring\nThere is complexity in debugging distributed systems using serverless architecture. When\nmultiple functions integrate with each other to perform a task and something goes wrong,\nit can be difficult to understand when and why the problem occurred. Vendors provide\ntools for debugging and monitoring, but there is still a level of immaturity with the\nserverless architecture pattern. Serverless architecture applications will become more\nprevalent, but it is still relatively new compared to other types of software architecture.\nMultitenancy issues\nMultitenancy issues, while not unique to serverless systems, deserve consideration. Any\ntime software for different customers are executed on the same machine, there is the\npossibility of one customer affecting a different customer. Examples of this include security\nissues, such as one customer being able to see another customer's data, or performance\nissues when one customer experiences a heavy load that affects performance for another\ncustomer.\n", "page": 309, "type": "text", "section": "Page 309"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 281 ]\nVendor lock-in\nVendor lock-in can be an issue with serverless architectures. You may think that moving\nfrom one serverless environment to another one would be easy, but it can be quite\ninvolved. In addition to your code being moved, each provider has specific formats and\ndeployment methods. In addition, you may be taking advantage of technologies and tools\nspecific to your vendor.\nSome degree of refactoring will be required if you want to switch vendors. Ideally, your\nsoftware application will not be reliant on a particular cloud provider. One way to mitigate\nthis disadvantage is to use a framework that packages your application in a way that allows\nfor deployment to any cloud provider. The Serverless Framework (https:/\u200b/\u200bserverless.\ncom) is an example of such a framework.\nComplexity of designing many functions\nSoftware systems that have a serverless architecture tend to consist of numerous functions\nand there is inherent complexity in designing many functions. It will take time to make\ndecisions on the granularity of functions provided in the serverless architecture. A good\ndesign provides a balance between having too many functions and having functions that\nare too large and difficult to maintain.\nThere is complexity in chaining multiple functions together in order to execute complex\ntransactions. The design must consider how it needs to handle a situation in which one\nfunction in a chain fails. For example, in the event of a failure, the system may need to\nexecute compensating logic to cancel a transaction.\nNot as many runtime optimizations\nServerless architectures do not allow for much in terms of runtime optimizations. In a\ntraditional environment, optimizations might be made regarding memory, processors,\ndisks, and the network. However, cloud providers can assist with optimizations for you.\nStill immature\nStandards and best practices for serverless architectures have not been as thoroughly\nestablished as other types of software architecture. However, some organizations and\ndevelopment teams do not mind using cutting-edge technology if they can use it to their\nadvantage. As time passes, this will become less and less of an issue.\n", "page": 310, "type": "text", "section": "Page 310"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 282 ]\nTaking a hybrid approach to serverless\nUsing a serverless architecture for a software system does not have to be an all or nothing\napproach. In addition to new applications fully leveraging serverless architecture, you may\nchoose to design a part of your system with a serverless architecture and use a different\narchitecture pattern for the other parts of your system.\nFor example, an organization may elect to take a hybrid approach and build some new\nfeatures for an existing application in a serverless environment and use them with other\narchitecture environments.\nFunction deployment\nWhen functions are deployed in a serverless system, they go through a deployment\npipeline. While the steps in the pipeline can vary depending on the cloud provider, there\nare some fundamental steps that typically take place.\nDevelopers must first upload the function definition, which contains specifications about\nthe function as well as the code. The specifications and metadata include things such as a\nunique identifier, name, description, version identifier, runtime language, resource\nrequirements, execution timeout (the maximum time a function call can execute until it is\nterminated), created date/time, and last modified date/time. When a function is invoked, it\nis invoked for a specific function version. The version identifier is used to select an\nappropriate function instance.\nThe following illustration shows a typical function deployment pipeline:\nAlong with the specification, the code itself and its dependencies must be provided (for\nexample, as a ZIP file or a Dockerfile). If the code is stored in an external repository, the\npath of the code location along with the credentials required for access must be provided.\n", "page": 311, "type": "text", "section": "Page 311"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 283 ]\nOnce the functional definition has been uploaded to the cloud provider, the build process\nuses it for compilation to produce an artifact. The resultant artifact may be a binary file, a\npackage, or a container image.\nThe starting of an instance function can be the result of cold start or  warm start. With a \nwarm start, one or more function instances have already been deployed and are ready to be\nexecuted when needed. A cold start takes longer since the function starts from an\nundeployed state. The function must be deployed and then executed when it is needed.\nFunction invocation\nWhen functions are invoked, there are four main invocation methods that can be used to\ninvoke a function:\nSynchronous request\nAsynchronous request\nMessage stream\nBatch job\nSynchronous request\nWhen a client makes a synchronous request, it waits for a response. A request-reply pattern\nis used to handle synchronous requests. Examples of requests include an HTTP request or a\ngRPC call. The following diagram shows the flow of a synchronous request, which first\ngoes through an API gateway:\n", "page": 312, "type": "text", "section": "Page 312"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 284 ]\nIn order to locate a function instance, the API gateway will either use client-side service\ndiscovery or pass it on to a router (load balancer) for server-side service discovery. Once a\nservice instance is located, the request is passed to it for processing. Once execution of the\nfunction is complete, the response is sent back to the client.\nAsynchronous request (message queue)\nWhen you want to process requests asynchronously, the publish-subscribe pattern can be\nused. Incoming requests are published to an exchange. Exchanges then distribute messages\nto one or more queues using rules called bindings. From there, function instances are\ninvoked to process published messages:\nExamples of message queues include RabbitMQ, AWS Simple Notification Service,\nMessage Queuing Telemetry Transport, and scheduled CRON jobs.\nMessage stream\nWhen there is a need for real-time processing of messages, a message stream can be used.\nMessage streams can ingest, buffer, and process large amounts of streaming data. When a\nnew stream is created, it is typically partitioned into shards, with each shard going to a\nsingle worker (function instance) for processing:\n", "page": 313, "type": "text", "section": "Page 313"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 285 ]\nExamples of technologies for serverless message streams include AWS Kinesis, AWS\nDynamoDB Streams, and Apache Kafka.\nBatch job\nBatch jobs are placed in a job queue, either on-demand or based on a schedule. For the\npurpose of speeding up the execution of the jobs, the master/worker pattern can be used\nwith jobs. The master/worker pattern, which is sometimes referred to as the master/slave\npattern, speeds up jobs by splitting them up into smaller tasks so that the tasks can be\nprocessed in parallel.\nThe following diagram shows the processing of a job:\nA component playing the role of Master splits the job (working set) up into tasks and\nlaunches workers. In this case, the workers are function instances and they can process\ntasks in parallel. Workers continue to pull from the working set until there are no more\ntasks to complete. The master gathers the results once the workers have all finished.\nCloud-native applications\nDeploying a software application to the cloud does not make it cloud-native. It is about\nhow the software is designed and implemented, not just where it is run. Cloud-native\napplications have been designed and developed from the ground up to be deployed in the\ncloud. In doing so, applications can take full advantage of their deployment environment.\nIn modern application development, the development team needs to have more knowledge\nabout, and a vested interest in, how their application runs in production. Similarly, the\noperations team must be able to work with the development team to improve upon, over\ntime, how the application is deployed and executes in a production environment.\n", "page": 314, "type": "text", "section": "Page 314"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 286 ]\nReasons to move to the cloud\nIn the early years of cloud computing, enterprises were hesitant to adopt cloud\ntechnologies within their IT organization. They had concerns such as losing control over\ntheir infrastructure, security, data risks, and reliability.\nSince then, enterprises have increasingly relocated their IT workloads to the cloud. Many\norganizations have already done so or have plans to move their applications to the cloud.\nMajor cloud providers, such as Amazon, Microsoft, and Google, are experiencing explosive\ngrowth and the competition between them is intensifying as more applications are\ndeployed to the cloud.\nThere are a number of reasons that enterprises are moving their applications and data to\nthe cloud.\nReducing costs\nCloud-based hosting reduces capital expenditure by eliminating the need to spend money\non fixed assets such as hardware and software. It also reduces operational expenditure by\nlowering costs for such things as IT support staff and round-the-clock electricity needs for\npower and cooling.\nGreater flexibility and scalability\nTaking advantage of the cloud offers enterprises greater levels of flexibility. Workloads can\nscale up and down based on demand very quickly, even when large amounts of computing\nresources are suddenly needed. Large cloud providers have global scale. In addition to\nbeing able to deliver the right amount of compute resources at the right time, they can do\nso from the right geographic location.\nCloud computing allows businesses to grow relatively easily without having to make major\nadjustments to things such as the amount of hardware.\nAutomatic updates\nThe task of keeping infrastructure software up to date with the latest software and security\nupdates is moved to the provider.\nUpdates to hardware are also taken care of by the cloud provider, including upgrades to\nservers, memory, processing power, and disk storage. Data centers are regularly updated\nwith the latest generation of hardware, ensuring greater efficiency and faster performance.\n", "page": 315, "type": "text", "section": "Page 315"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 287 ]\nDisaster recovery\nBackup, disaster recovery, and business continuity are important concerns for software\napplications. Cloud computing provides these services easily and with less expense than\ndoing it on your own.\nFor smaller businesses, the additional expense for disaster recovery can be burdensome.\nThe option of deploying applications to the cloud has made it easier for even small\nbusinesses to have full backup and recovery capabilities in the event of a disaster.\nWhat are cloud-native applications?\nCloud-native applications are specifically designed for the cloud model. Applications\ndesigned for the cloud are able to take advantage of it as their target platform, providing\norganizations with a competitive advantage, greater agility, easier deployment, on-demand\nscalability, reduced costs, increased resilience, and other benefits.\nThe expectations of today's modern applications are different from those of the past. Cloud-\nnative applications have some distinct characteristics that allow us to meet those \nexpectations and requirements. The Cloud Native Computing Foundation (CNCF)\ncurrently defines cloud-native as using an open source software stack to make applications\nthat are containerized, dynamically orchestrated, and microservices-oriented.\nContainerized\nContainers are a way of packaging a software application. They are lightweight, stand-\nalone packages of software. The application, along with all of its libraries and\ndependencies, is bundled together in an immutable package.\nThe concept of containers for software is similar to the concept of using physical containers\nin the shipping industry. Prior to standardizing shipping containers, shipping could be an\ninefficient and complicated process. A wide variety of things in all shapes and sizes can be\ntransported on the same cargo ship. Now that there are standardized shipping container\nsizes, we know what and how much will fit on a container ship, no matter what we are\nshipping.\n", "page": 316, "type": "text", "section": "Page 316"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 288 ]\nSimilarly, packaging your software with its dependencies in a container based on open\nstandards allows us to know that it can be run anywhere that supports containers. It\nprovides us with predictability in that we know the software will work as expected because\nthe container is the same, no matter where it is executed. It eliminates the experience of\nhaving an application not execute properly on one machine only to have someone else on a\ndifferent machine claim that it runs on my machine. Unexpected errors because of differences\nbetween machines and environments can be drastically reduced or even eliminated.\nIn a cloud-native application, each part of the system is packaged in its own container. This\nallows each part to be reproducible and allows resources to be isolated for each container.\nDynamically orchestrated\nMaking your application containerized is not enough. A cloud-native application will need\nthe ability to run multiple containers across multiple machines. That is what will allow you\nto use microservices and to provide the fault tolerance capabilities.\nOnce you have multiple containers running on different machines, you will need to\ndynamically orchestrate them. The system must start the correct container at the right time,\nbe able to scale containers by adding and removing them based on demand, and launch\ncontainers on different machines in the event of a failure.\nThere are a number of container clustering and orchestration tools available. Currently, the\nmost popular is Kubernetes, which is sometimes referred to as K8S because there are eight\nletters in between the K and the S. It is an open source orchestrator that was originally\ndeveloped by Google. Other container orchestration tools besides Kubernetes include\nDocker Swarm and Apache Mesos.\nCloud providers also have cloud-based container services. There is Amazon Elastic\nContainer Service (Amazon ECS), which is Amazon's own container-orchestration service\nfor Amazon Web Services (AWS). In addition to ECS, Amazon offers Amazon Elastic\nContainer Service for Kubernetes (Amazon EKS), which is a managed service for\nKubernetes on AWS. Amazon also offers AWS Fargate, which is a technology that can be\nused for Amazon ECS and EKS that lets you run containers without having to manage\nservers or clusters.\nMicrosoft's Azure Container Service (AKS) allows you to use a fully managed Kubernetes\ncontainer orchestration service or select an alternative orchestrator such as unmanaged\nKubernetes, Docker, or Mesosphere DC/OS. Google offers Google Kubernetes Engine,\nwhich is a managed environment for Kubernetes.\n", "page": 317, "type": "text", "section": "Page 317"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 289 ]\nMicroservices-oriented\nA cloud-native application should be partitioned into microservices. Dividing an\napplication into small, autonomous, independently versioned, and self-contained services\nincreases organizational agility and the maintainability of the application.\nNo downtime\nApplications today are expected to be available at all times, with no downtime. It has\nalways been a goal to minimize downtime, but the days when it was acceptable to have a\nsmall maintenance window in which an application was not available are over.\nComplex software systems will have failures and should be designed to expect failures to\noccur. Cloud-native applications are designed for failure and keep fault tolerance in mind so\nthat they can recover rapidly and minimize downtime. If a physical server fails\nunexpectedly or is taken down as part of planned maintenance, a failover system will\nredirect traffic to a different server. Software components should be designed so that they\nare loosely coupled, such that if one fails, a redundant component can take over.\nA single failure should not have the effect of bringing down the entire software system. For\nexample, if an instance of a microservice fails, other instances can take incoming requests. If\nall instances of a particular microservice are not operational, the fault is isolated to only a\nportion of the system, so that the entire system does not fail. \nContinuous delivery\nIncreasing competition and user expectations mean modern applications have shorter\nrelease cycles. Rather than having major releases that are months (or even years) apart, the\nability to have application updates on a more regular basis (think weeks or days rather than\nmonths) is a necessity.\nCloud-native applications should release software updates rapidly. Shorter release cycles\nprovide an opportunity to get feedback from users more quickly. Rather than having to\nwait a long period of time to receive feedback, continuous delivery gives us a much tighter\nfeedback loop. The development team can respond more rapidly to users by using the\nfeedback received to make adjustments and improve the software.\nCloud-native applications increase organizational agility. By releasing the software and\nreceiving feedback regularly, an organization can respond to the market, competitors, and\nthe needs of their customers quickly. This can give an organization a competitive\nadvantage.\n", "page": 318, "type": "text", "section": "Page 318"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 290 ]\nSupport for a variety of devices\nCloud-native applications must be able to support a variety of devices. Users of modern\napplications use mobile devices, desktop machines, tablets, and other devices. They expect\na unified experience across devices as well as the ability to seamlessly switch between\ndevices. In order to provide this type of support, cloud-native applications ensure that\nbackend services are able to provide the functionality that a variety of frontend devices\nneed.\nWith the IoT, many other devices are now connected, which means that some applications\nwill be required to support them. To handle a potentially large number of devices and the\nvolume of data that they produce, highly distributed systems designed with a cloud-native\napproach are required.\nTwelve-factor apps\nThe twelve-factor app methodology is a set of principles that can be followed when\ndeveloping applications to be deployed to the cloud. It was originally written by the\ncreators of Heroku, which is a popular cloud platform.\nThe principles of the twelve-factor app methodology can be used to design and develop\ncloud-native applications. An application that follows this methodology adheres to certain\nconstraints and conforms to a contract. This gives it a level of predictability that facilitates\ndeployment to the cloud. These factors make scaling easier, maximize portability, and\nensure that the application can be continuously deployed.\nIn his book The Twelve-Factor App, Adam Wiggins describes the twelve factors as follows:\nCodebase: One codebase tracked in revision control, many deployments\nDependencies: Explicitly declare and isolate dependencies\nConfiguration: Store configuration in the environment\nBacking Services: Treat backing services as attached resources\nBuild, release, run: Strictly separate the build and run stages\nProcesses: Execute the app as one or more stateless processes\nPort binding: Export services via port binding\nConcurrency: Scale out via the process model\nDisposability: Maximize robustness with fast startup and graceful shutdown\nDevelopment/production parity: Keep development, staging, and production as\nsimilar as possible\nLogs: Treat logs as event streams\n", "page": 319, "type": "text", "section": "Page 319"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 291 ]\nAdministrative processes: Run admin/management tasks as one-off processes\nIn this section, let's explore the twelve tenets of a twelve-factor app in further detail.\nCodebase\nA cloud-native application should have one, and only one, codebase. Larger software\nsystems may need to be decomposed into multiple applications, each of which should be\ntreated as a separate application with their own codebase.\nEach codebase should be tracked in a version control system and can be used for multiple\ndeployments of the application. For example, a single codebase could be deployed to the\ndevelopment, QA, staging, and production environments:\nDependencies\nDependencies for the application should be explicitly declared and isolated. A cloud-native\napplication should not rely on the implicit existence of dependencies. No assumptions \nshould be made as to what will be available in the execution environment, so applications\nshould bring their dependencies with them and declare their dependencies both precisely\nand completely. The details of the dependencies should also include the specific version\nthat is needed for each dependency.\nMost modern programming languages and frameworks provide a way for an application to\nmanage its dependencies. Sometimes these dependencies come in the form of a package,\nwhich is a distribution of software consisting of files typically bundled together in some\ntype of archive file. A package manager is sometimes available, which is a tool that can help\nto install, upgrade, configure, and remove packages.\n", "page": 320, "type": "text", "section": "Page 320"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 292 ]\nConfiguration\nAn application's configuration consists of values that can vary across deployments, such as\ndatabase connection information, URLs for web services, and SMTP server information for\nemails.\nThe configuration for a cloud-native application should be stored in the environment and\nnot in the application code. The configuration will vary depending on the type of\ndeployment (for example, whether the deployment is to development, staging, or\nproduction), while the code will not. There should be a strict separation between an\napplication's code and its configuration.\nBacking services\nA backing service is any service that the application uses over the network that is separate\nfrom the application itself. Examples of a backing service include a data store, a distributed\ncaching system, an SMTP server, an FTP server, and a messaging/queuing system.\nThe binding for each service should be stored in a configuration that is external to the\napplication. The application should not care where the backing services are running and\nshould not make any distinction between local and third-party services.\nAll backing services should be treated as attached resources, and the application should be\nable to attach and detach backing services without any code changes. For example, if a\ndatabase goes down, it should be possible to detach it and attach a different database from\na backup without making any code changes.\nBuild/Release/Run\nCloud-native applications should strictly separate the build, release, and run stages. The\nbuild stage is where the code is converted into an executable bundle. An application's\ndeclared dependencies are considered during this stage.\n", "page": 321, "type": "text", "section": "Page 321"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 293 ]\nThe result of the build stage is a build:\nIn the release stage, the build is combined with configuration information that is external to\nthe application and it is deployed to a cloud environment. One build can be used for\nmultiple deployments. The result of the release stage is an immutable artifact called\na release. Each release should have a unique identifier, using a versioning scheme or a\ntimestamp.\nOnce there is a release, in the run stage, also known as runtime, the application is executed\nin its environment. By the time the build and release tasks are complete, the run stage\nshould be relatively uneventful and the application should simply work.\nProcesses\nA cloud-native application should consist of one or more stateless processes, with any\npersisted data being stored using a backing service. The processes of an application\nfollowing the twelve-factor app methodology are stateless and do not share anything.\nAdhering to this rule will allow your application to be more robust and to scale more easily.\nFor example, in order for a web application to be a cloud-native application, it should not\nrely on sticky sessions. Sticky sessions are where a router/load balancer can assign a single\nserver to a particular user so that subsequent requests from the same user will be routed to\nthe same server. Among other things, this could allow caching user session data in the\nmemory of the application's process, with the expectation that subsequent requests from\nthe same user will be routed to the same process and have access to the cached data.\nInstead of using sticky sessions, a cloud-native application should cache by some other\nmeans, such as using a distributed caching system like Redis or Memcached.\n", "page": 322, "type": "text", "section": "Page 322"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 294 ]\nPort binding\nCloud-native applications are completely self-contained. Services should make themselves\navailable to other services by specified ports. Similar to any backing services your\napplication may be using, the API you expose may be the backing service for another\napplication. This allows one app to be the backing service of another app.\nJust as accessing a backing service should be achievable through a simple binding, such as a\nURL, other applications should be able to interface with your applications through a simple\nbinding.\nConcurrency\nA software application consists of one or more processes and a cloud-native application\ntreats processes as first-class citizens. Examples include a web process to handle HTTP\nrequests and a worker process to handle an asynchronous background task.\nThe idea is that by running multiple processes for an application, it can run independently\nand concurrently. This allows a cloud-native application to scale out horizontally as\nneeded.\nDisposability\nThe processes of a cloud-native application should be disposable so that they can be started\nor stopped at any time. Processes should be designed so that they start up as quickly as\npossible and are able to shut down gracefully.\nCloud-native applications should be robust against crashing. In the event that an\napplication does crash, it should have the ability to start back up without requiring extra\neffort. Applications with these qualities related to disposability allow for elastic scaling by\nthe cloud provider as well as quick deployment of code and/or configuration changes. If the\nstartup times for the processes of a cloud-native application are too slow, it can lead to\nlower levels of availability during periods of high-volume traffic.\nDevelopment/production parity\nCloud-native applications should minimize differences between development and\nproduction as much as possible. Gaps between different environments include differences\nin the tools and backing services used (including any differences in the versions), the time\nbetween when something is coded and when it is released, and the fact that different\npeople may code the application versus deploy it.\n", "page": 323, "type": "text", "section": "Page 323"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 295 ]\nDifferences between development and production environments may cause issues to go\nundetected until the software is in production. One way to eliminate these differences and\nattain development/production parity is through the use of containers. Earlier in this\nchapter, we discussed containers and how they are used to bundle an application, along\nwith all of its dependencies, into an immutable package. This container can then be\nexecuted anywhere, including development, staging, and production environments.\nContainers give us predictability because we know that the application will work in an\nidentical way across the different environments.\nLogs\nLogs give development and operations staff visibility into the application, its behavior, and\nits exceptions. Logs are an important part of an application and will be discussed further in\nChapter 9, Cross-Cutting Concerns.\nCloud-native applications should not be responsible for the routing and storage of its\noutput stream. Rather than writing to or managing log files, each process should treat logs\nas event streams and simply write its event stream to standard output (stdout) and\nstandard error (stderr).\nIn a development environment, developers can view the stream to gain insight into the\napplication's behavior. In a production environment, the stream should be captured by the\nexecution environment and then routed to its final destination. For instance, the stream\ncould be handled by a log management tool and persisted to a data store.\nBy following this guideline, cloud-native applications can dynamically scale to any number\nof machines and not be concerned with aggregating all of the log information. In addition,\nthe method by which log information is stored and processed can be changed without any\nmodification to the application itself.\nAdministrative processes\nPeriodically, there will be reasons that administrative tasks will need to be executed.\nExamples include a database migration, cleaning up bad data, or executing analytics for a\nreport. When these needs arise, they should be executed as one-off processes in an identical\nenvironment as production. This means the scripts need to run against the same code and\nthe same configuration as any other process executed using that release.\nAdministrative/management scripts for one-off tasks should be committed to the same\ncode repository as the rest of the application and should ship with the rest of the code in\norder to maintain consistency and avoid synchronization issues between environments.\n", "page": 324, "type": "text", "section": "Page 324"}
{"text": "Architecting Modern Applications\nChapter 8\n[ 296 ]\nSummary\nSoftware applications today have expectations and requirements that are different from the\npast. There are demands for greater availability, flexibility, fault tolerance, scalability, and\nreliability. Continuous delivery and ease of deployment may be requirements for\norganizations that want to increase their agility in order to keep their software applications\nclosely aligned with their business goals and market opportunities. We examined how\nMSA, serverless architecture, and cloud-native applications can meet these types of\ndemands and requirements.\nIn the next chapter, we will take a look at cross-cutting concerns. Most software\napplications have common functionality that is needed throughout the application,\nincluding in different layers of the application. This functionality is called cross-cutting\nconcerns, and we will take a look at different types of crosscutting concerns and how to\ntake them into consideration during design and development.\n", "page": 325, "type": "text", "section": "Page 325"}
{"text": "9\nCross-Cutting Concerns\nAll software applications have various concerns, which are groupings of logic and\nfunctionality. Some of that functionality, known as cross-cutting concerns, is used in\nmultiple areas of the application.\nIn this chapter, we will explore cross-cutting concerns and some general guidelines for\nhandling them. We will take a look at different ways to implement them, including using\ndependency injection (DI), the decorator pattern, and aspect-oriented programming\n(AOP).\nWe'll go over some examples of cross-cutting concerns as well as some of the special\nconsiderations that should be made when dealing with cross-cutting concerns within\nmicroservices.\nIn this chapter, we will cover the following topics:\nCross-cutting concerns\nGeneral guidelines for cross-cutting concerns\nImplementing cross-cutting concerns with the use of DI and the decorator\npattern\nAspect-oriented programming\nTaking a look at various types of cross-cutting concerns\nCross-cutting concerns for microservices, including the use of a microservice\nchassis and the sidecar pattern\n", "page": 326, "type": "text", "section": "Page 326"}
{"text": "Cross-Cutting Concerns\nChapter 9\n[ 298 ]\nCross-cutting concerns\nIn a software system, a concern is a grouping of logic or functionality that the application is\nproviding. The concerns of the system reflect the requirements. When designing a system,\nsoftware architects should seek to follow the Separation of Concerns (SoC) principle,\nwhich was described in Chapter 6, Software Development Principles and Practices. It is a \ndesign principle that seeks to reduce complexity by dividing the software system so that\nconcerns are kept separate.\nThere are two main types of concerns in a software system:\nCore concern: It represents functionality that is fundamental to the system and is\na primary reason as to why the software is being written. For example, the logic\nrelated to the calculation of employee salaries and bonuses would be core\nconcerns of a human resource management system. The logic for each core\nconcern is typically localized to particular components.\nCross-cutting concern: It is an aspect of the application that relies on and affects\nother concerns. It is functionality that is used in multiple areas, possibly\nspanning multiple layers of the application. Examples of cross-cutting concerns\ninclude security, logging, caching, and error handling. The logic for each cross-\ncutting concern is needed among multiple components.\nIn the following diagram, various core concerns are depicted, represented by Module A,\nModule B, and Module C. You can also see the cross-cutting concerns, which intersect the\ncore concerns:\n", "page": 327, "type": "text", "section": "Page 327"}
{"text": "Cross-Cutting Concerns\nChapter 9\n[ 299 ]\nGeneral guidelines for cross-cutting\nconcerns\nThere are some general guidelines that we can follow when designing solutions for the\ncross-cutting concerns that we need in our applications.\nIdentifying cross-cutting concerns\nAs an initial step, software architects must be able to identify the cross-cutting concerns. By\nrecognizing common functionality across modules and layers of the system, we can\nconsider how concerns can be abstracted so that they are not duplicated. In some cases, the\ncommon functionality is identical among its different usages, while in others, refactoring\nmay be involved to make the logic generic enough to be reusable.\nUsing open-source and third-party solutions\nOnce cross-cutting concerns have been identified, implementations must be provided for\nthem. Software architects should consider solutions that have already been developed, such\nas open-source or third-party solutions, to satisfy the needs of cross-cutting concerns. Prior\nto spending resources developing an in-house solution, a capable solution may already\nexist. For example, for a logging cross-cutting concern, there is no reason to reinvent the\nwheel when frameworks exist that provide all of the functionality that you need.\nMaintaining consistency\nWhen satisfying the needs of cross-cutting concerns, software architects should ensure that\neach concern is implemented consistently. The cross-cutting concern should behave in a\nconsistent way in each place that it is needed. This is one of the reasons why the\nimplementation of a cross-cutting concern should not be duplicated in more than one place.\n", "page": 328, "type": "text", "section": "Page 328"}
{"text": "Cross-Cutting Concerns\nChapter 9\n[ 300 ]\nAvoiding scattered solutions\nWhen implementing cross-cutting concerns, we want to avoid simply adding the\nfunctionality to each consuming class that needs it. This approach is called scattering since\nthe implementation is scattered throughout the application. As a software architect, you\nwill want to ensure that developers are not simply copying and pasting logic that they need\nin multiple places.\nWhen an implementation for a cross-cutting concern is scattered because it is spread out\namong multiple modules, it violates the Don't Repeat Yourself (DRY) principle. Code\nmust be duplicated in order to provide the functionality of the concern in multiple places.\nCode duplication is wasteful, makes it more difficult to maintain consistency, increases\ncomplexity, and needlessly makes the codebase larger. These qualities all make\nmaintenance more difficult. If the logic that is duplicated needs to be modified, changes\nwill need to be made in multiple places.\nAvoiding tangled solutions\nWhen logic for a cross-cutting concern is mixed with logic for a different concern (either a\ncore concern or another cross-cutting concern), it is known as tangling because the logic for\ndisparate concerns is tangled together.\nAn implementation that is tangled is likely in violation of the separation of concerns\nprinciple and tends to suffer from low cohesion. Mixing concerns increases the complexity\nof the software and reduces its quality. It lowers maintainability because changes to both\ncore and cross-cutting concerns are made more difficult.\nWhen designing the solution for a cross-cutting concern, we want to avoid tangling. Part of\naccomplishing that is to make the logic for the cross-cutting concern loosely coupled with\nthe code that needs it. Cross-cutting concerns should not be tightly coupled with other\nconcerns so that all concerns can be easily maintained and modified.\nAnother principle that we should follow in order to avoid a tangled solution is the Single\nResponsibility Principle (SRP). A class should be responsible for one thing and do that\none thing well. If it is responsible for a core concern, such as some piece of business\nfunctionality, it should not also be responsible for implementing a cross-cutting concern. A\nresponsibility is a reason to change and a class should have only one reason to change.\n", "page": 329, "type": "text", "section": "Page 329"}
{"text": "Cross-Cutting Concerns\nChapter 9\n[ 301 ]\nA class responsible for a core concern should not need to be modified if we want to change\nthe implementation of a cross-cutting concern that it uses. Similarly, the implementation of\na cross-cutting concern should not need to be modified if we need to change the\nimplementation of a class that uses it.\nAvoiding a tangled solution also allows us to adhere to the Open/Closed Principle (OCP),\nwhich states that software components should be open for extension but closed for\nmodification. When we want to add cross-cutting concerns to business functionality, we\nshould have the ability to do so by extending the component with new code without being\nrequired to modify the existing business logic.\nImplementing cross-cutting concerns\nImplementations should follow the design goals of cross-cutting concerns by maintaining\nconsistency, not being scattered, and not being tangled. There are several different \napproaches that can be taken when implementing cross-cutting concerns. These include DI,\nthe decorator pattern, and AOP.\nUsing dependency injection (DI)\nOne approach to handling cross-cutting concerns is to use the DI pattern, which we covered\nin Chapter 6, Software Development Principles and Practices. This pattern can be used to inject\ncross-cutting dependencies into classes that need them. This allows us to write loosely\ncoupled code and avoid scattering. The logic for the cross-cutting concern will not be\nduplicated in multiple places.\nFor example, if we had an Order class that had logging and caching cross-cutting concerns,\nwe could inject them like this:\npublic class Order\n{\n    private readonly ILogger _logger;\n    private readonly ICache _cache;\n    public Order(ILogger logger, ICache cache)\n    {\n        if (logger == null)\n            throw new ArgumentNullException(nameof(logger));\n        if (cache == null)\n            throw new ArgumentNullException(nameof(cache));\n        _logger = logger;\n", "page": 330, "type": "text", "section": "Page 330"}
{"text": "Cross-Cutting Concerns\nChapter 9\n[ 302 ]\n        _cache = cache;\n    }\n}\nBy eliminating hard-coded dependencies of cross-cutting concerns through the use of DI, it\nalso provides us with the ability to change the implementation of a cross-cutting concern,\neither at runtime or compile-time. We may want to use a different implementation for a\ncross-cutting concern at runtime based on something such as a configuration setting. As\nlong as we have a common interface that is used by each implementation, this approach\nwould allow us to change the implementation without having to recompile and redeploy\nthe application.\nUsing DI for cross-cutting concerns also increases the testability of the application. Any\ncode that has a cross-cutting concern is dependent on the abstraction of that concern, not\nthe concrete implementation, which allows us to mock the dependencies in our unit tests.\nHowever, there are some disadvantages to this approach. It does require you to inject\ndependencies for cross-cutting concerns everywhere that you need them. While this makes\nit consistent with how you may be approaching other dependencies, some cross-cutting\nconcerns are needed in many places (for example, logging), and it may become tedious to\ninject it in all of the places that need it.\nAlthough this approach eliminates scattering, it does not eliminate tangling. If you take this\napproach, you will have cross-cutting logic mixed in with your other logic. In the preceding\norder class, code that uses the logging and cache objects that were injected in the\nconstructor will be located throughout the Order class, mixed with logic for other concerns.\nUsing the decorator pattern\nAnother approach to implementing cross-cutting concerns is to use the decorator pattern.\nThe decorator pattern can add behaviors dynamically to an object, including behaviors for\ncross-cutting concerns. It is essentially like creating a wrapper to handle a cross-cutting\nconcern around some other object. The following diagram shows the decorator pattern:  \n", "page": 331, "type": "text", "section": "Page 331"}
{"text": "Cross-Cutting Concerns\nChapter 9\n[ 303 ]\nIn the preceding diagram, the ConcreteComponent class implements the IComponent\ninterface and is a class that can either be used on its own or wrapped in one or more\ndecorators. The Decorator class is an abstract class that also implements the IComponent\ninterface and wraps (has a reference to) the component. Each concrete decorator class\ninherits from the Decorator class and adds its behavior.\nFor example, let's say we have an IAccountService interface with a Save method and an\nAccountService concrete component that implements that interface:\npublic interface IAccountService\n{\n    void Save(IAccount account);\n}\npublic class AccountService : IAccountService\n{\n    public void Save(IAccount account)\n    {\n        // Save logic\n    }\n}\n", "page": 332, "type": "text", "section": "Page 332"}
{"text": "Cross-Cutting Concerns\nChapter 9\n[ 304 ]\nThe Save method in the AccountService class only contains logic related to the save\n(represented by the Save logic code comment) and does not contain logic for any cross-\ncutting concern.\nAn abstract decorator class can be created that also implements the IAccountService\ninterface and will wrap the AccountService class. You may notice that we are also using\nDI in conjunction with the decorator pattern:\npublic abstract class AccountServiceDecorator : IAccountService\n{\n    protected readonly IAccountService _accountService;\n    public AccountServiceDecorator(IAccountService accountService)\n    {\n        _accountService = accountService;\n    }\n    public virtual void Save(IAccount account)\n    {\n        _accountService.Save(account);\n    }\n}\nWe can now create concrete decorators for the account service. The following is one for\nlogging:\npublic class LoggingAccountService : AccountServiceDecorator\n{\n    private readonly ILogger _logger;\n    public LoggingAccountService(IAccountService accountService,\n        ILogger logger)\n        : base (accountService)\n    {\n        _logger = logger;\n    }\n    public override void Save(IAccount account)\n    {\n        _accountService.Save(account);\n        _logger.LogInfo($\"Saved account: {account.Number}\");\n    }\n}\n", "page": 333, "type": "text", "section": "Page 333"}
{"text": "Cross-Cutting Concerns\nChapter 9\n[ 305 ]\nWe can also make one for caching:\npublic class CachingAccountService : AccountServiceDecorator\n{\n    private readonly ICache _cache;\n    public CachingAccountService(IAccountService accountService,\n        ICache cache)\n        : base(accountService)\n    {\n        _cache = cache;\n    }\n    public override void Save(IAccount account)\n    {\n        _accountService.Save(account);\n        _cache.Put(account.Number.ToString(), account.Name);\n    }\n}\nThe concrete decorator classes, LoggingAccountService and CachingAccountService,\ncontain logic for the cross-cutting concern for which they are responsible. This logic can\noccur either before or after the logic of the core concern. One challenge with this approach\nis the fact that you may want to execute cross-cutting logic, not just before or after core\nlogic, but also in the middle of the core logic. One way around this is to make your methods\nsmaller so that there are additional points where cross-cutting logic can be executed.\nThe fact that the concrete decorator classes all implement the IAccountService interface\n(through their decorator parent class), means that we can have an instance of the account\nservice that contains logic for multiple cross-cutting concerns. For example:\nIAccountService accountService = new AccountService();\nIAccountService loggingAccountService =\n    new LoggingAccountService(accountService, logger);\nIAccountService cachingAndLoggingAccountService =\n    new CachingAccountService(loggingAccountService, cache);\n", "page": 334, "type": "text", "section": "Page 334"}
{"text": "Cross-Cutting Concerns\nChapter 9\n[ 306 ]\nThe instance of the account service being held by the\ncachingAndLoggingAccountService variable has been decorated with both logging and\ncaching functionality. The account service has been wrapped by multiple decorators:\nAs with other types of dependencies, a DI container can handle dependency chains for you\nso that when you want a concrete instance of IAccountService, you will receive one that\nhas been decorated with all of the cross-cutting concerns, without having to do the wiring\nup yourself.\nUsing the decorator pattern in conjunction with DI will allow you to write logic for the\ncross-cutting concerns that are neither scattered nor tangled with other logic. However, it\ndoes require you to create the decorator classes. For any cross-cutting concerns that are\nneeded in many classes, large software systems will require a large number of decorators to\nbe created. This work can be tedious and repetitive, although some of the code could be\ngenerated automatically through code generation.\nIn addition to DI and the decorator pattern, another approach to handling cross-cutting\nconcerns that you can consider is to use AOP in your project.\nAspect-oriented programming\nAspect-oriented programming (AOP) is a paradigm that was created to handle the\nscattering and tangling of boilerplate code in object-oriented programming (OOP), such as\nthe code necessary for cross-cutting concerns. Gregor Kiczales and other researchers at\nXerox PARC, which is now known as PARC, a Xerox company, did the original research on\nthe topic. They eventually wrote a paper, AOP, in which they described a solution for\nhandling cross-cutting concerns.\n", "page": 335, "type": "text", "section": "Page 335"}
{"text": "Cross-Cutting Concerns\nChapter 9\n[ 307 ]\nIn order to gain an understanding of AOP, let's take a look at some of its fundamental\nconcepts, as follows:\nAspect: The aspect is the modularization of a concern that cuts across multiple\nareas of the application. It is the logic for the cross-cutting concern itself, such as\nlogging or caching.\nJoin point: A join point is a location in between logical steps of your program,\nsuch as after your program starts, before/after creating an object, before/after\ncalling a method, and before the program ends.\nPointcut: A pointcut is a set of join points. They can be fairly simple or rather\ncomplex. Examples of simple pointcuts include before/after any object is created\nor before/after any method is called within a class. An example of a complex\npointcut is after any public method in a particular class except for the add,\nupdate, and delete methods. As you can imagine, there is quite a bit of flexibility\nin defining pointcuts.\nAdvice: The advice is the code that actually performs the cross-cutting concern.\nIn the case of logging, it would be the statements that perform actions, such as\nwriting to the log.\nTypes of advice\nThere are different types of advice that are typically supported by AOP tools:\nBefore advice (on start): Before advice executes before a join point. It cannot\nprevent the flow of logic proceeding on to the join point.\nAfter returning advice (on success): After-returning advice executes after a join\npoint has completed successfully (no exception was thrown).\nAfter throwing advice (on error): After-throwing advice executes if an exception\noccurred in the method.\nAfter advice (finally): After advice executes after a join point exits, whether the\nmethod completed successfully or an exception occurred.\nAround advice: Around advice surrounds a join point so that it can execute logic\nbefore and after a method is invoked. It also has the ability to prevent a method\nfrom executing by either returning its own value or throwing an exception.\n", "page": 336, "type": "text", "section": "Page 336"}
{"text": "Cross-Cutting Concerns\nChapter 9\n[ 308 ]\nWeaving\nWeaving is the process of applying the advice (logic for the cross-cutting concern) to the\nlogic of the core concern. Code for each cross-cutting concern is placed in a single location,\nmaking it easy to maintain and modify the various aspects. If changes need to be made to\nthe advice of a cross-cutting concern, they can be made in a single place. For example, if we\nneed to change the logic for logging, it can be done in one place rather than in all of the core\nconcerns that use logging.\nOnce the advice for a cross-cutting concern has been established, weaving combines it with\nthe logic of various core concerns wherever it is needed. In the following diagram, the\nEmployee class represents some logic for a core concern. The LoggingAspect class\nrepresents the advice. The logic is combined (weaved) together to form the result:\n", "page": 337, "type": "text", "section": "Page 337"}
{"text": "Cross-Cutting Concerns\nChapter 9\n[ 309 ]\nBefore a class can be used, weaving must take place to combine its logic with the logic for\nall of the different cross-cutting concerns that the class uses. AOP tools either perform\nweaving at compile-time or at runtime.\nCompile-time weaving\nAOP tools that employ compile-time weaving perform an additional step to attach aspects\nafter a program is compiled. A program is first compiled as it normally is, resulting in a\nDLL or EXE. Once the program has been compiled, it is run through a post-compilation\nprocess. This process is handled by a post-processor that is provided with an AOP tool.\nThe post-processor takes the DLL or EXE and adds the aspects to it. Using configuration,\nthe post-processor knows where to apply the advice, such as before a method is executed or\nwhen an exception is thrown. The resulting DLL/EXE has both the logic for the core\nconcern and all of the advice:\nOne of the benefits of compile-time weaving is that there is no overhead during runtime to\nperform the weaving. However, because the aspects are weaved at compile-time, you will\nnot be able to change them at runtime through configuration.\nRuntime weaving\nRuntime weaving does not take place until after the application starts executing. The advice\nfor an aspect and the code that it will be applied to are both instantiated at runtime. Unlike\nthe compile-time weaving approach, no post-compilation changes are made to binaries.\nThe way that runtime weaving works is similar to the decorator pattern that was described\nin the Using the decorator pattern section that appeared earlier in this chapter. The main\ndifference is the fact that an AOP tool can generate the decorator classes at runtime without\nrequiring developers to manually create them beforehand. The AOP tool generates a proxy\nobject dynamically, which implements the same interface as the target object and can\ndelegate to the target object while weaving the advice with the core logic.\n", "page": 338, "type": "text", "section": "Page 338"}
{"text": "Cross-Cutting Concerns\nChapter 9\n[ 310 ]\nAn advantage of runtime weaving is that there is no post-compilation process involved.\nThe machine that is compiling the application, whether it is a development machine or a\nbuild server, will not require extra software (a post-processor from an AOP tool) to\ncomplete the compilation. Runtime weaving also affords you the flexibility of configuring\naspects so that they can be changed at runtime. The disadvantage of runtime weaving is\nthat, unlike compile-time weaving, there is some amount of overhead involved at runtime\nto perform the weaving.\nTypes of cross-cutting concerns\nThere are a variety of different types of cross-cutting concerns that may be part of a\nsoftware application. This is by no means an exhaustive list, but some common examples of\ncross-cutting concerns include the following:\nCaching\nConfiguration management\nAuditing\nSecurity\nException management\nLogging\nCaching\nLeveraging the use of caching in a software application is a common way to improve\nperformance.  It may be used in a variety of places where data is being read, making it a\ncross-cutting concern for the application.\nA reusable caching service should provide the ability to perform operations, such as\nputting data in a cache, getting data out of a cache, and setting policies on how and when\ncached data will expire.\nWhen designing a software system, software architects must make a decision on the type of\ncaching that the system will use. The two main types of server-side caching are an in-\nprocess cache and a distributed cache. With an in-process cache, the cache is local to the\napplication instance. Load balanced applications will have multiple instances of the\napplication, each of which will have their own in-process cache. A distributed cache offers a\nsingle logical view of the cache even when there are application instances on multiple\nservers.\n", "page": 339, "type": "text", "section": "Page 339"}
{"text": "Cross-Cutting Concerns\nChapter 9\n[ 311 ]\nWe will be examining the use of caching further in Chapter 10, Performance Considerations.\nConfiguration management\nConfiguration management involves deciding what options for a software application\nshould be made configurable and how that configuration will be stored, protected, and\nmodified. In order to provide flexibility in how your application behaves and give it the\nability to run in a variety of environments, configuration options should be made available.\nThese options should be made external to the application so that they can be modified\nwithout requiring recompilation of the application.\nA software application needs to be deployed and used in multiple environments, such as\ndevelopment, testing, staging, and production. In addition, it may use a number of\ndifferent infrastructures and third-party services. Examples include a database, a service\nregistry, a message broker, an email (SMTP) server, a payment processing service, and a\nmessage broker. Different environments may require different configuration values for\nthese various services.\nExternalizing the configuration settings allows us to use our application in different ways\nand in different environments without having to change application code and recompile\nthe application.\nIt also makes deploying the application to a cloud environment easier. If you recall from\nChapter 8, Architecting Modern Applications, one of the factors in the twelve-factor app\nmethodology is to ensure that there is a strict separation between an application's code and\nits configuration.\nAs a software architect, you should consider which settings need to be configurable in your\napplication. Only the settings that need to be configurable should be made available for\nmodification. Including unnecessary configuration options can lead to an application\nhaving an excessive amount of settings, which will only make it more difficult to use and\nunderstand. Software applications with overly complex configuration increase the\nlikelihood of incorrect configuration, which can lead to the application not working\nproperly or can make the application vulnerable to security breaches.\nA release of a software application is an immutable package, whether that software is being\ndeployed on its own server, on a virtual machine (VM), or on a container image. However,\nthat immutable package needs to have the ability to be deployed in different environments.\nExternalizing the configuration will give you that ability.\n", "page": 340, "type": "text", "section": "Page 340"}
{"text": "Cross-Cutting Concerns\nChapter 9\n[ 312 ]\nAuditing\nA cross-cutting concern of many software applications is the auditing of data changing\noperations. There may be requirements to maintain an audit trail that includes information\nabout a data change, such as the date/time that it occurred and the identity of the\nindividual who made the change.\nThere may be a requirement to record information about the nature of a particular data\nchange, such as the old and new values. In event-driven systems, persisting events and\ntheir details can serve as the audit trail.\nSecurity\nSecurity is a significant cross-cutting concern. It includes the authentication of users and the\nauthorization of the operations the user can perform with the software application. Once a\nuser is authenticated and their identity is known, their authorization must be checked in\norder to determine which operations they are capable of executing within the application.\nWe will be examining security-related topics, including authentication and authorization,\nin Chapter 11, Security Considerations.\nException management\nAn exception is a type of error that occurs during program execution that we expect may\nhappen. They are issues that are known to occur and an application can be designed to \nrecognize and handle them.\nExceptions can occur for a variety of reasons. Some examples include attempting to use a\nnull object reference, attempting to access an array using an index that is out of range,\nexceeding a specified timeout, being unable to write to a file, or the inability to connect to a\ndatabase. Many programming languages provide ways to handle exceptions and transfer\nthe flow of execution to a different part of the logic. For example, in the C# language, we\ncan use try/catch/finally statements to handle exceptions.\nIneffective exception management makes it more difficult to diagnose and resolve issues\nwith the application. A failure to handle exceptions properly can also lead to security\nissues. Exception management should be treated as a cross-cutting concern and a\ncentralized exception management approach should be designed for the application. A\nsoftware application should have consistency in terms of how exceptions and errors are\nhandled.\n", "page": 341, "type": "text", "section": "Page 341"}
{"text": "Cross-Cutting Concerns\nChapter 9\n[ 313 ]\nCommon boilerplate code to perform operations such as the logging of exceptions (logging\nis its own cross-cutting concern) and communicating the fact that an exception occurred\nback to the user can be handled in a centralized and consistent way. When logging and\ncommunicating exception details, sensitive information should not be revealed. All\nexceptions should be logged as the information may be helpful in resolving an issue. When\nexceptions are logged, any additional details, such as contextual information, should be\nadded to make the information more useful.\nA good exception management strategy should also take into consideration unhandled\nexceptions and design a way to deal with them. Failures in the application should not leave\nit in an unstable state or corrupt data.\nLogging\nLogging is an important part of software applications, allowing you to know what your\ncode did when it was executed. It provides you with the ability to see when things execute\nas expected and, perhaps more importantly, assists you in diagnosing problems when they\ndo not. The availability of logs can really help you to troubleshoot application issues.\nCommon characteristics of log entries include the following:\nDate/time: It is imperative to know when the event took place\nSource: We want to know the source/location of the event\nLog Level/Severity: It is helpful to know the level/severity of the log entry\nMessage: Log entries should have some sort of description or detail explaining\nthe log entry\nUnderstanding log levels\nMost logging frameworks support the ability to specify a log level or the severity of each\nlog entry that is created. Although the log levels vary depending on the framework used,\nsome common log levels include the following:\nTRACE: Use this level for tracing the code, such as being able to see when\nexecution flow enters and exits specific methods.\nDEBUG: This level records diagnostic details that can be helpful during\ndebugging. It can be used to make note of when certain aspects of the logic are\ncompleted successfully. It can also provide details such as executed queries and\nsession information, which can be used in determining the cause of an issue.\n", "page": 342, "type": "text", "section": "Page 342"}
{"text": "Cross-Cutting Concerns\nChapter 9\n[ 314 ]\nINFO: This level is for logging details about normal operations during the\nexecution of logic. It is common for INFO to be a default log level. It is for useful\ninformation that you want to have but typically won't spend much time\nexamining under normal circumstances.\nWARN: When you have a situation in which incorrect behavior takes place, but\nthe application can continue, it can be logged at the Warn level.\nERROR: Use this level for exceptions and problems that caused an operation to\nfail.\nFATAL: This level is reserved for the most severe errors, such as those that may\ncause the shutdown of the system or data corruption.\nLogging frameworks typically allow you to configure the level at which logging will take\nplace, such as being able to specify a minimum log level. For example, if the minimum log\nlevel is configured as Info, then logging will take place for the log levels of INFO, WARN,\nERROR, and FATAL.\nA detailed log level, such as TRACE, is not typically used for sustained periods,\nparticularly for a production environment. This is due to the high volume of detailed\nentries that will be produced, which can degrade performance and excessively use up disk\nand bandwidth resources. However, when diagnosing an issue, being able to change the\nlog level to DEBUG or TRACE temporarily can provide valuable information.\nRouting log entries\nMany logging frameworks provide functionality that allows you to configure routing rules\nfor log entries. These may be based on the log level, the source, or some combination of\ncriteria. Log entries can then be set up to target different destinations. Some examples of\ndestinations for log entries include the console, text files, databases, email, and the\nWindows Event Log.\nWriting logs to text files on local disks has been a common practice, but when your\napplication is running on many different servers, it becomes difficult to search through all\nof the files without the use of tools. This issue is further exacerbated when your application\nis running in the cloud. Given the elasticity provided by hosting your application in the\ncloud, the number of servers running your application at any given time and their location\nare dynamic.\n", "page": 343, "type": "text", "section": "Page 343"}
{"text": "Cross-Cutting Concerns\nChapter 9\n[ 315 ]\nAs was mentioned in Chapter 8, Architecting Modern Applications, cloud-native applications\nshould simply treat their log as event streams and should not be responsible for the routing\nand storage of those streams. Instead of writing to a log file, it should write event streams\nto standard output (stdout) and standard error (stderr). Your application may scale out to\nany number of machines, and no assumptions should be made as to where your application\nwill run or where log information will be stored. You can either leverage services that your\ncloud provider makes available to aggregate and store log information, or you can provide\nyour own implementation.\nUsing Elastic Stack\nOne of the solutions to centralizing your logging functionality so that log information can\nbe consolidated and managed is the Elastic Stack. The Elastic Stack is an integrated solution\nof open-source products that offer a highly scalable end-to-end solution for aggregating,\nsearching, analyzing, and visualizing logging data.\nIt was formerly known as (and you may still hear it referred to as) the ELK stack, an\nacronym that represents the solution's use of the Elasticsearch, Logstash, and Kibana\nproducts. A data shipper product named Beats, which is part of Elastic Stack, was not part\nof the original ELK stack. One type of data shipper within Beats is Filebeat, which can be\nused for text log files.\nElasticsearch\nElasticsearch is an open-source, distributed search engine and document database that can\nstore, search, and analyze data. Among its many uses, it can store all of the log data that\nyour application produces.\nElasticsearch allows you to quickly search through data, and its capabilities include full-text\nquerying. As your system grows, it can horizontally scale to handle large amounts of data,\neven if there are billions of log lines. It is easily extendable by adding nodes and resilient to\nfailing nodes.\n", "page": 344, "type": "text", "section": "Page 344"}
{"text": "Cross-Cutting Concerns\nChapter 9\n[ 316 ]\nElasticsearch can be configured so that it will send notifications based on certain conditions.\nLog data is continually monitored and, if the conditions are met, notifications can be sent to\ntargets such as email or other incident management tools.\nElasticsearch makes use of RESTful APIs and JavaScript Object Notation (JSON). It\nsupplies clients in a number of programming languages, including Java, C#, Python,\nJavaScript, PHP, Curl, Perl, and Ruby. In addition, the Elasticsearch community has\ncontributed support for a number of other languages. There is a large community behind\nElasticsearch and it provides good support and an extensive knowledgebase for questions\nand issues.\nLogstash\nLogstash is an open-source log-parsing engine that provides functionality to parse,\ntransform, and transport data. It aggregates, filters, and supplements data from a variety of\nsources. One of the many uses to which it is particularly suited is the ingestion and\nprocessing of log data.\nLogstash can perform tasks such as transforming unstructured data into structured data,\nfiltering out certain types of data, and adding to the data. Once Logstash is done processing\ndata, it can forward it to a destination. For example, altered log data can be transported to\nElasticsearch. If a Logstash node fails, it can guarantee at-least-once delivery through the\nuse of its persistent queue.\nKibana\nKibana is a free tool that allows you to explore and visualize your Elasticsearch data. It is\nwritten in Node.js and has a web-based frontend. It provides a visualization of\nElasticsearch data, allowing you to create useful dashboards that includes charts, graphs,\nhistograms, and other visualizations. It is extensible and gives you the ability to create your\nown visualizations.\nVisualizations created by Kibana can be easily distributed and shared with others. Kibana\ndashboards can be integrated with your application or you can simply share a URL to a\ndashboard with others. Reports can be exported to various formats, such as Portable\nDocument Format (PDF) and comma-separated values (CSV).\n", "page": 345, "type": "text", "section": "Page 345"}
{"text": "Cross-Cutting Concerns\nChapter 9\n[ 317 ]\nBeats\nBeats is a platform for small, lightweight data shippers. These data shippers gather data\nfrom potentially large numbers of different sources and send it to Logstash or Elasticsearch.\nBeats has shippers for all kinds of data:\nFilebeat: Text log files\nMetricbeat: Metrics for systems and services\nPacketbeat: Network monitoring\nWinlogbeat: Windows event logs\nAuditbeat: Audit data\nHeartbeat: Uptime monitoring\nThe open-source community has created beats for many other sources. Libbeat is the \ncommon library for forwarding data, which can be leveraged to create custom beats.\nFor logging, Filebeat can be used to aggregate log data. It is also container-ready and can be\ndeployed in its own container on the same host. Once deployed, it can collect logs from all\nof the containers running on that host.\nFilebeat works seamlessly with Logstash. During periods where there is a higher volume of\nlog data being processed, Logstash can let Filebeat know to slow down the reading of the\nlog data until the volume lowers so that processing can return to the regular pace.\nCross-cutting concerns for microservices\nProviding cross-cutting concerns for microservices requires some additional consideration.\nIn this section, we will take a look at using a microservice chassis and the sidecar pattern to\nhandle some of the difficulties with implementing cross-cutting concerns for microservices.\nLeveraging a microservice chassis\nIn a monolithic application, cross-cutting concerns can be designed and developed once.\nAfter their development is complete and they are available for use, they can be leveraged\nthroughout the application.\n", "page": 346, "type": "text", "section": "Page 346"}
{"text": "Cross-Cutting Concerns\nChapter 9\n[ 318 ]\nMicroservices are independently deployable, self-contained services. As a result, \nimplementing a cross-cutting concern has to be done repeatedly for each microservice. This\ncan make it prohibitively expensive for an organization to develop microservices when\nconsidering the development resources that would be needed. A system may consist of\nhundreds of microservices, and your team may be creating new ones throughout the life of\nthe application. The process of creating a new microservice should be made as fast and easy\nas possible, and you are not going to want to implement cross-cutting concerns for each\none.\nTo overcome this challenge, a microservice chassis can be used. A microservice chassis is a\nframework that can take care of many of the cross-cutting concerns for microservices and\ndo so in a way that allows all of your microservices to utilize the functionality.\nSome examples of a microservice chassis include Spring Cloud, Microdot, Gizmo, Go kit,\nand Dropwizard. Each microservice framework is different, but some of the cross-cutting\nconcerns that they handle include the following:\nLogging\nExternalized configuration\nMetric reporting and instrumentation\nService registration and discovery\nHealth checks\nTracing\nIt should be noted that it is not required that you use an open-source or third-party\nmicroservice chassis. It could be a framework that you develop within your organization,\nspecifically tailored to your needs and using the technologies that you choose. The\nimportant point is to have a microservice framework that you can reuse so that you are not\nimplementing the same cross-cutting concerns multiple times. Once a microservice\nframework is in place to handle cross-cutting concerns, the development team can focus on\nthe core concerns of the microservices themselves.\nUsing the sidecar pattern\nOne of the options available to you when developing microservices is polyglot\ndevelopment. You have the flexibility to use multiple programming languages, runtimes,\nframeworks, and data storage technologies. Microservices are independently developed\nand deployed, allowing you to select best-of-breed technologies to accomplish the given\ntask.\n", "page": 347, "type": "text", "section": "Page 347"}
{"text": "Cross-Cutting Concerns\nChapter 9\n[ 319 ]\nIf your software system is taking advantage of polyglot microservices, it can make it\ndifficult to maintain libraries for cross-cutting concerns. You would need one for each\nprogramming language that you are using, resulting in duplication of effort and a lower\nlevel of maintainability.\nOne solution to this problem is to use the sidecar pattern. The logic for cross-cutting\nconcerns are placed in their own process or container (known as a sidecar container or a\nsidekick container) and then attached to the primary application. Similar to how a\nmotorcycle sidecar is attached to a motorcycle, the sidecar application is attached to the\nprimary application and runs alongside it:\nUse of the sidecar pattern allows the programming language and framework of the primary\napplication to be different from that of the sidecar application. When used for cross-cutting\nconcerns in a heterogeneous environment, it means that you will not need to develop logic\nfor a cross-cutting concern in each programming language that is being used.\nPrimary and sidecar applications have access to the same resources. This makes it easy, for\nexample, for a sidecar application that has the purpose of monitoring the health of the\nprimary application to monitor its system resources.\nThis pattern is also useful when the sidecar application is owned by a different organization\nor a different team. Even if you do not have control over the implementation of the sidecar\napplication, you will be able to use it with your primary application.\nFor inter-process communication (IPC) between the primary application and the sidecar\napplication, it is a best practice is to use a communication mechanism that is language and\nframework agnostic. Although IPC between the primary and sidecar applications is\ngenerally fast because they are located on the same host, there will be some overhead\ninvolved due to the communication as compared with in-process communication. If the\ninterface is chatty (more fine-grained operations requiring an increased amount of\ncommunication between the two processes) and performance needs to be optimized, use of\nthe sidecar pattern may not be ideal.\n", "page": 348, "type": "text", "section": "Page 348"}
{"text": "Cross-Cutting Concerns\nChapter 9\n[ 320 ]\nSummary\nCross-cutting concerns are an important part of a software application. In this chapter, you\nlearned what cross-cutting concerns are and how to avoid scattering and tangling the\nsolutions for cross-cutting concerns.\nIn this chapter, we learned how we can use DI, the decorator pattern, and AOP to\nimplement cross-cutting concerns. This chapter provided some examples of common cross-\ncutting concerns, such as caching, configuration management, auditing, security, exception\nmanagement, and logging. We also explored using a microservice chassis and/or the\nsidecar pattern when implementing cross-cutting concerns for microservices.\nIn the next chapter, we will take a look at the performance considerations that software\narchitects make. We will explore the importance and fundamentals of performance in a\nsoftware application. Various performance improvement techniques will be detailed. The\nchapter will also cover the topic of concurrency and how parallel execution can speed up\nthe performance of multi-processor or multi-core systems.\n", "page": 349, "type": "text", "section": "Page 349"}
{"text": "10\nPerformance Considerations\nUsers have high expectations when it comes to the performance of the applications they\nuse. Performance needs are requirements that must be met and their importance should be\nreflected in the fact that the entire team must take ownership of performance. Performance\nis a quality attribute and should be considered throughout the development of an\napplication.\nAfter taking a look at the importance of performance and some of the common terminology\nrelated to it, this chapter will describe a systematic approach to improving performance.\nWe will also cover server-side caching, web application performance, and techniques to\nimprove database performance.\nIn this chapter, we will cover the following topics:\nThe importance of performance\nPerformance terminology\nTaking a systematic approach to performance improvement\nServer-side caching, including different caching strategies and usage patterns\nImproving web application performance, including HTTP caching, compression,\nminifying resources, bundling resources, using HTTP/2, using content delivery\nnetworks, and optimizing web fonts\nDatabase performance, including designing an efficient database schema, using\ndatabase indexes, scaling up/out, and concurrency\nThe importance of performance\nThe performance of a software application indicates the responsiveness of the operations\nthat it can perform. Users have greater expectations today in terms of the responsiveness of\nthe applications that they use. They demand fast response times regardless of their location\nor the device that they are using.\n", "page": 350, "type": "text", "section": "Page 350"}
{"text": "Performance Considerations\nChapter 10\n[ 322 ]\nMost importantly, software must serve its functional purpose, be reliable in its operation,\nand provides its functionality in a usable way. If it does not, then the speed of the\napplication will not matter. However, once those needs are met, performance is of high\nimportance.\nPerformance affects user experience\nThe speed of your application plays a major role in the overall user experience (UX). A\nuser's satisfaction with the application is influenced by the speed of the application. The\nperformance of your application affects the organization's bottom line, whether it is because\ncustomers are being gained/lost for a customer-facing site or because productivity is being\ngained/lost for an enterprise application.\nFor web and mobile applications, the loading time for a page is a major factor in page\nabandonment. If a page takes too long to load, many users will simply leave. This is evident\nwhen we look at things such as a site's bounce and conversion rates.\nBounce rate\nA bounce occurs when a user has just a single-page session on a site and leaves without\nvisiting any of the other pages. The bounce rate, which is sometimes referred to as the exit\nrate, is the percentage of users who bounce:\nAs you would expect, as page load times increase, so too does the bounce rate. Examples of\nactions that result in a bounce include the user closing the browser window/tab, clicking on\na link to visit a different site, clicking the back button to leave the site, navigating to a\ndifferent site by typing in a new URL or using a voice command, or having a session\ntimeout occur.\nConversion rate\nThe conversion rate is the percentage of site visitors who ultimately take the desired\nconversion action. The desired conversion action depends on the purpose of the site, but a\nfew examples of common ones include placing an order, registering for membership,\ndownloading a software product, or subscribing to a newsletter.\n", "page": 351, "type": "text", "section": "Page 351"}
{"text": "Performance Considerations\nChapter 10\n[ 323 ]\nThe conversion rate is represented by the following formula:\nWebsites that have poor performance will have a lower conversion rate. If a site is slow,\nusers will simply leave the site and go somewhere else.\nPerformance is a requirement\nSpeed is a feature of your application and if it is not fast enough, then it is not good enough.\nPerformance is a quality attribute of software systems and cannot be considered as just an\nafterthought. It should play an integral part throughout the life cycle of a software\napplication. It is a requirement of the system and, like other requirements, it must be\nunambiguous, measurable, and testable.\nWhen we discussed requirements in Chapter 3, Understanding the Domain, it was stated that\nrequirements must be specified clearly, measurable with specific values/limits when\nappropriate, and testable so that it can be determined whether the requirement has been\nsatisfied. For example, it is not sufficient to simply state that the web page must load in a\ntimely manner. In order to make it unambiguous, measurable, and testable, it would have to\nbe written to state that the web page must load within two seconds.\nTreating performance as a requirement also means that we should have tests for it. We can\nmeasure how long it takes to execute tests and assert that they can be completed within a\ntime limit. While performance tests are not executed as often as unit tests, it should be easy\nto execute performance tests regularly.\nPage speed affects search rankings\nPage speed is a consideration in a site's mobile search ranking in Google search results.\nCurrently, this criterion only affects pages with the slowest performance, but it shows the\nimportance Google places on web page performance. For customer-facing websites, you do\nnot want performance to negatively affect your site's search ranking.\nDefining performance terminology\nBefore we explore the topic of performance further, let's define some of the common terms\nrelated to performance.\n", "page": 352, "type": "text", "section": "Page 352"}
{"text": "Performance Considerations\nChapter 10\n[ 324 ]\nLatency\nLatency is the amount of time (or delay) it takes to send information from a source to a\ndestination. A phrase you may hear regarding latency is that it is the time spent on the wire,\nsince it represents the amount of time a message spends traveling on a network. Something\nis latent if it is dormant and we must wait to perform any further processing while a\nmessage is traveling across a network.   \nLatency is usually measured in milliseconds. Factors such as the type of network hardware\nbeing utilized, the connection type, the distance that must be traveled, and the amount of\ncongestion on the network all affect latency.\nIn many instances, a significant portion of the total latency takes place between your office\nor home and the internet service provider (ISP). This is known as last-mile latency\nbecause even if data travels across the country or even the world, it can be the first or last\nfew hops that contribute most to the total latency.\nThroughput\nThroughput is a measure of a number of work items per a particular time unit. In the\ncontext of a network, it is the amount of data that can be transferred from one location to\nanother in a given amount of time. It is typically measured in bits per second (bps),\nmegabits per second (Mbps), or gigabits per second (Gbps).\nIn the context of application logic, throughput is how much processing can be done in a\ngiven amount of time. An example of throughput in this context would be the number of\ntransactions that can be processed per second.\nBandwidth\nBandwidth is the maximum possible throughput for a particular logical or physical\ncommunication path. Like throughput, it is typically measured in terms of a bit rate, or the\nmaximum number of bits that could be transferred in a given unit of time.\n", "page": 353, "type": "text", "section": "Page 353"}
{"text": "Performance Considerations\nChapter 10\n[ 325 ]\nProcessing time\nProcessing time is the length of time that it takes for a software system to process a\nparticular request, without including any time where messages are traveling across the\nnetwork (latency). Sometimes a distinction is made between server processing time and\nclient processing time.\nA variety of things can affect processing time, such as how the application code is written,\nthe external software that works in conjunction with the application, and the characteristics\nof the hardware that is performing the processing.\nResponse time\nResponse time is the total amount of time between the user making a particular request\nand the user receiving a response to that request. Although some people use the terms\nlatency and response time interchangeably, they are not synonymous. For a given request,\nresponse time is a combination of both the network latency and the processing time.\nWorkload\nWorkload represents the amount of computational processing a machine has been given to\ndo at a particular time. A workload uses up processor capacity, leaving less of it available\nfor other tasks. Some common types of workload that may be evaluated are CPU, memory,\nI/O, and database workloads.\nTaking regular measurements of workload levels will allow you to predict when peak loads\nfor your application take place and also allow you to compare the performance of your\napplication at different load levels.\nUtilization\nUtilization is the percentage of time that a resource is used when compared with the total\ntime that the resource is available for use. For example, if a CPU is busy processing\ntransactions for 45 seconds out of a one-minute timespan, the utilization for that interval is\n75%. Resources such as CPU, memory, and disk should be measured for utilization in order\nto obtain a complete picture of an application's performance. As utilization approaches the\nmaximum throughput, response times will rise.\n", "page": 354, "type": "text", "section": "Page 354"}
{"text": "Performance Considerations\nChapter 10\n[ 326 ]\nTaking a systematic approach to\nperformance improvement\nWhen looking to improve the performance of an application, the entire development team\nshould be involved. Teams will have greater success at optimizing performance when the\nentire team, and not just certain individuals, take ownership of the performance of the\nsoftware application.\nWhen engaging in an effort to improve performance, it can be helpful to follow a systematic\napproach. An iterative process that consists of the following steps can be used for\nperformance improvement:\nProfiling the application\nAnalyzing the results\nImplementing changes\nMonitoring changes\nThe following diagram illustrates this process:\nLet's now take a closer look at each of the steps in the process.\nProfiling an application\nThe first step is to profile the application. Profiling is an analysis of a software system that\nresults in measurements of the system's execution. Development teams should not be \nguessing where performance issues exist, as they will not always be where you expect.\nRather than guessing, development teams should be acquiring precise measurements.\nThese measurements can then be used to make decisions. Examples include how much time\na particular method takes to execute, how often a method is called, the time spent in\ncommunication, the amount of I/O that is taking place, and how much of the CPU and\nmemory is being used.\n", "page": 355, "type": "text", "section": "Page 355"}
{"text": "Performance Considerations\nChapter 10\n[ 327 ]\nThere are profiling tools, called profilers, available that can be leveraged to profile an\napplication. Two broad categories of how profilers collect information are through \ninstrumentation and by sampling. An effective profiling strategy might use both types of\nprofilers to understand where performance issues may exist in a software system.\nInstrumentation\nWhen instrumentation is used, code is added to the software system being profiled in\norder to collect information. For example, to collect data on the time spent in a method and\nto get a count of how many times the method is used, instrumentation code is executed at\nthe beginning and end of the method.\nInstrumentation code can be manually added by the development team. However, profilers\ncan add instrumentation automatically. Some profilers that use instrumentation modify\nsource code, while others work at runtime. Either way, instrumentation can provide a great\nlevel of detail. However, a disadvantage of instrumentation is that the instrumentation code\ncan affect the measurements. The degree of the effect really depends on what data is being\ncollected and the extent of the instrumentation.\nFor example, the instrumentation code itself takes some time to execute. Profilers can take\nthis into consideration by calculating the overhead they incur and subtracting that amount\nfrom their measurements. However, adding code to a method can change CPU\noptimizations and change the way that it executes the method. Consequently, very short\nmethods can sometimes yield inaccurate results.\nStatistical profilers\nProfilers that work by sampling, which are sometimes known as statistical profilers, let\napplications execute without any runtime modifications. This type of profiling is conducted\noutside of the application's process and overcomes some of the disadvantages of\ninstrumentation.\nAn operating system (OS) interrupts the CPU at regular intervals, giving it an opportunity\nfor process switching. Sampling works by collecting information during these\ninterruptions. Sampling is less intrusive than instrumentation, as it allows the software\nsystem to execute close to its normal speed. The downside is the fact that the data collected\nare is often an approximation and is not as numerically accurate as data that can be\ncollected through instrumentation.\n", "page": 356, "type": "text", "section": "Page 356"}
{"text": "Performance Considerations\nChapter 10\n[ 328 ]\nAnalyzing the results\nOnce performance data is collected through profiling, it can be used to identify\nperformance issues and areas of the application that are bottlenecks. Bottlenecks are parts\nof the software system that limit performance. These parts in the software are unable to\nkeep pace given their capacity and a particular amount of work, which in turn slows down\nthe overall performance of the application.\nThe focus of the performance improvement effort should be on optimizing the bottlenecks\nof the application. Software architects should not focus their attention on optimizing non-\nbottlenecks unless all of the identified bottlenecks have been addressed and there is time to\noptimize non-bottlenecks.\nSome common bottlenecks include CPU, memory, network, database, and disk utilization.\nDifferent problems will lead you to different solutions. For example, if the network is too\nslow, you can look into ways to send less data across, such as compressing or caching data.\nIf the database is too slow, you can work with the database administrator (DBA) to add\nindexes, optimize queries, make use of stored procedures, and possibly denormalize some\nof the data. If the CPU is the bottleneck, you can look into getting a faster processor, adding\nprocessors, storing/caching data so that it doesn't need to be calculated, or making\nimprovements to the algorithms being used.\nSome bottlenecks will lead you to conclude that you need to either scale horizontally or\nscale vertically. Vertical scaling involves increasing the capacity of existing servers by\nadding resources, such as adding memory and processors, replacing existing processors\nwith faster ones, and increasing the size of available disk space. Horizontal scaling involves\nadding servers to your pool of resources in order to scale wider and handle more traffic.\nImplementing changes\nProfiling the application and analyzing the results are necessary steps prior to\nimplementing any changes to improve performance because we do not want to make\nchanges unless we know that it will be worth it. Once those steps are complete, though, we\nare ready for the development team to actually implement the changes based on the results\nof the previous steps.\n", "page": 357, "type": "text", "section": "Page 357"}
{"text": "Performance Considerations\nChapter 10\n[ 329 ]\nThe analysis may identify multiple areas that need improvement. Software architects\nshould consider implementing one set of changes at a time, so as not to mix results and\nmake it more difficult to recognize new performance issues that may have been introduced.\nWhen selecting which set of changes to implement for a particular iteration, the most\nimportant bottleneck and the one that is expected to provide the biggest payoff should be\nprioritized.\nMonitoring results\nEven once changes have been implemented to improve performance, the process is not\ncomplete. We must monitor the results to determine whether the changes that were\nimplemented resolved the performance issues that were identified.\nWhen changes are implemented to fix a bottleneck, either the performance issue will\nremain unresolved, it will be fixed, or the bottleneck will be transferred to another part of\nthe system. If the issue is not fixed, one should consider whether it is appropriate to undo\nthe changes that were made. Software architects need to be aware that eliminating one\nbottleneck may reveal another one. We must monitor the results because we may need to\nconduct additional iterations of the performance improvement process.\nEven if our application is now performing in a satisfactory way, it must be monitored\nbecause things can change over time. As the source code changes with the introduction of\nnew features and bug fixes, new performance issues and bottlenecks may be created. Other\nchanges may also occur over time, such as a change in how many users the application has\nand how much traffic the application is generating.\nServer-side caching\nSoftware architects should take advantage of caching in order to improve performance and\nscalability. Caching involves copying data that may be needed again to fast storage so that\nit can be accessed quicker in subsequent uses. We will discuss HTTP caching and the use of\ncontent delivery networks in the Improving web application performance section later in this\nchapter. In this section, we will focus on server-side caching strategies.\nServer-side caches can be used to avoid making expensive data retrievals from the original\ndata store (for example, a relational database) repeatedly. The server-side cache should be\nplaced as close to the application as possible to minimize latency and improve response\ntimes.\n", "page": 358, "type": "text", "section": "Page 358"}
{"text": "Performance Considerations\nChapter 10\n[ 330 ]\nThe type of storage used for a server-side cache is designed to be fast, such as an in-\nmemory database. The more data and users that an application has to handle, the greater\nthe benefits of caching.\nCaching data in distributed applications\nIn distributed applications, there are two main types of data caching strategies that you can\nuse. One is the use of a private cache and the other is a shared cache. Keep in mind that you\ncan use both strategies in a single application. Some data can be stored in a private cache,\nwhile other data can be stored in a shared cache.\nUsing a private caching strategy\nA private cache is held on the machine that is running the application that is using it. If\nmultiple instances of an application are running on the same machine, then each\napplication instance can have its own cache.\nOne of the ways that data is stored in a private cache is in-memory, which makes it\nextremely fast. If there is a need to cache more data than can fit in the amount of memory\navailable on the machine, then cached data can be stored on the local file system.\nIn a distributed system using the private caching strategy, each application instance will\nhave its own cache. This means that it is possible for the same query to yield different\nresults depending on the application instance.\nUsing a shared caching strategy\nA shared cache is located in a separate location, possibly accessible through a cache service,\nand all application instances use the shared cache. This resolves the issue of different\napplication instances potentially having different views of cached data. It also improves\nscalability because a cluster of servers can be used for the cache. Application instances\nsimply interact with the cache service, which is responsible for locating the cached data in\nthe cluster.\nA shared cache is slower than a private cache because rather than being available on the\nsame machine as the application instance, it is located somewhere else; there will be some\nlatency involved in interacting with the cache. However, if a greater level of consistency\nwith data is important, the extra latency may be worth it.\n", "page": 359, "type": "text", "section": "Page 359"}
{"text": "Performance Considerations\nChapter 10\n[ 331 ]\nPriming the cache\nSoftware architects should consider priming the cache. This means that an application pre-\npopulates the cache at application startup with data that will either be needed at startup, or\nis widely used enough that it makes sense to make the data available in the cache right\nfrom the start. This can help to improve performance as soon as initial requests are received\nby the server.\nInvalidating cached data\nPhil Karlton, while working at Netscape, once said:\n\"There are only two hard things in Computer Science: cache invalidation and naming\nthings.\"\nThe joke is funny because there is truth to it. Data in a cache may become stale if it is\nchanged after it was placed in the cache. Cache invalidation is the process of replacing or\nremoving cached items. We must ensure that we are handling cached data properly so that\nstale data is replaced or removed. It may also be necessary to remove cached items if the\ncache becomes full.\nExpiring data\nWhen data is cached, we can configure the data to expire from the cache after a specified\namount of time. Some caching systems allow you to configure a system-wide expiration\npolicy in addition to an expiration policy for an individual cached item. The expiration is\ntypically specified as an absolute value (for example, 1 day).\nEvicting data\nA cache may become full, in which case the caching system must know which items it can\ndiscard in order to make room for new data. The following are some of the policies that can\nbe used to evict data:\nLeast recently used (LRU): Based on the assumption that cached items that have\nrecently been used are the most likely to be used again soon, this discards items\nthat were least recently used first.\nMost recently used (MRU): Based on the assumption that cached items that have\nbeen recently used will not be needed again, this discards items that were most\nrecently used first.\n", "page": 360, "type": "text", "section": "Page 360"}
{"text": "Performance Considerations\nChapter 10\n[ 332 ]\nFirst-in, first-out (FIFO): Like a FIFO queue, this discards the item that was\nplaced in the cache first (oldest data). It does not take into consideration when\nthe cached data was last used.\nLast-in, first-out (LIFO): This approach is the opposite of FIFO in that it discards\nthe item that was placed in the cache most recently (newest data). It does not take\ninto consideration when the cached data was last used.\nExplicitly evicting data: There are times when we want to explicitly evict data\nfrom a cache, such as after existing data is deleted or updated.\nCache usage patterns\nThere are two main ways that an application works with a cache. The application can either\nmaintain the cache data itself (known as a cache-aside pattern), including reading/writing\nto the database, or it can treat the cache as the system of record and the cache system can\nhandle reading/writing to the database (including read-through, write-through, and write-\nbehind patterns).\nCache-aside pattern\nIn the cache-aside pattern, the application is responsible for maintaining the data in the\ncache. The cache is kept aside and it doesn't interact with the database directly. When values\nfrom the cache are requested by the application, the cache is checked first. If it exists in the\ncache, it is returned from there and the system-of-record is bypassed. If it does not exist in\nthe cache, the data is retrieved from the system-of-record, stored in the cache, and\nreturned. \nWhen data is written to the database, the application must handle potentially invalidated\ncached data and ensure that the cache is consistent with the system-of-record.\nRead-through pattern\nWith the read-through pattern, the cache is treated as the system-of-record and has a\ncomponent that is able to load data from the actual system-of-record (the database). When\nan application requests data, the cache system attempts to get it from the cache. If it does\nnot exist in the cache, it retrieves the data from the system-of-record, stores it in the cache,\nand returns it.\n", "page": 361, "type": "text", "section": "Page 361"}
{"text": "Performance Considerations\nChapter 10\n[ 333 ]\nWrite-through pattern\nA caching system that uses the write-through pattern has a component that has the ability\nto write data to the system-of-record. The application treats the cache as the system-of-\nrecord and when it asks the caching system to write data, it writes the data to the system-\nof-record (the database) and updates the cache.\nWrite-behind pattern\nThe write-behind pattern is sometimes used instead of the write-through pattern. They both\ntreat the cache as the system-of-record but the timing of the write to the system-of-record is\nslightly different. Unlike the write-through pattern, in which the thread waits for the write\nto the database to complete, the write-behind pattern queues the writing of the data to the\nsystem-of-record. The advantage of this approach is that the thread can move on quicker,\nbut it does mean that there is a short time when the data between the cache and the system-\nof-record will be inconsistent.\nImproving web application performance\nIn this section, we will look at techniques that can be used to improve the performance of\nweb applications. These techniques include, but are not limited, to:\nHTTP caching\nCompression\nMinification\nBundling\nHTML optimization\nHTTP/2\nContent delivery networks (CDNs)\nWeb font optimization\nLeveraging HTTP caching\nMany roundtrips between a client and a server may be necessary to load a page, and\nretrieving resources for that page from the server can take up significant amounts of time.\nThe ability to cache resources that might be needed again so that they do not need to be\ntransferred over the network on subsequent trips is an important part of improving web\napplication performance.\n", "page": 362, "type": "text", "section": "Page 362"}
{"text": "Performance Considerations\nChapter 10\n[ 334 ]\nBrowsers are capable of caching data so that it doesn't need to be fetched from a server\nagain. Resources such as CSS or JavaScript files might be shared across multiple pages of\nyour web application. As a user navigates to various pages, they will need these resources\nmultiple times. In addition, users may return to your web application at some point in the\nfuture. In both cases, taking advantage of HTTP caching will improve performance and\nbenefit the user.\nTo take advantage of HTTP caching, each response from your web server must include the\nappropriate HTTP header directives. The cache policy you decide to implement is\nultimately dependent on the application's requirements and the type of data being served.\nEach resource may have different requirements related to caching that should be\nconsidered. Using the various header directives available to you will provide you with the\nflexibility to meet your requirements. Let's look at some of the header directives you can\nuse to control HTTP caching.\nUsing a validation token\nA common scenario with HTTP caching occurs when a response has expired from the cache\nbut has not changed in any way. The client would be required to download the response\nagain, which is wasteful since the resource has not changed.\nA validation token in the ETag header of a response can be used to check whether an\nexpired resource has changed. Clients can send the validation token along with a request. If\na response has expired from the cache but the resource has not changed, there is no reason\nto download it again. The server will return a 304 Not Modified response and the browser\nwill then know that it can renew the response in the cache and use it.\nSpecifying cache-control directives\nCache-control directives in a response can control whether the response should be cached,\nunder what conditions it can be cached, and for how long it should be cached. If a response\ncontains sensitive information that you do not want cached, a no-store cache-control\ndirective can be used, which will prevent browsers as well as any intermediate caches (for\nexample, a content delivery network), from caching the response. Alternatively, a response\ncan be marked as private, which will allow caching in a user's browser but not in any\nintermediate caches.\n", "page": 363, "type": "text", "section": "Page 363"}
{"text": "Performance Considerations\nChapter 10\n[ 335 ]\nA cache-control directive that is slightly different than no-store is the no-cache directive. It is\nused to specify that a response should not be used from the cache until a check is\nperformed with the server first to see whether the response has changed. The validation\ntoken must be used to make this determination, and only if the resource has not changed\ncan the cache be used.\nThe max-age directive is used to specify the maximum amount of time, in seconds, that a\nresponse can be reused from the cache. The value is relative to the time of the request.\nYou may find yourself in a situation where you want to invalidate a cached response even\nthough it has not expired yet. Once a response is cached, it will continue to be used unless it\nexpires or the browser's cache has been cleared in some way. However, there may be times\nwhen you want to change a response before it has expired. This can be accomplished by\nchanging the URL of the resource, which will force it to be downloaded. A version number,\nor some other identifier such as a fingerprint of the file, can be included as part of the\nfilename. Using this technique provides differentiation between different versions of the\nsame resource.\nTaking advantage of compression\nCompression is an important technique for improving performance. It is the use of an\nalgorithm to remove redundancy in a file in order to make it smaller. This improves\ntransfer speed and bandwidth utilization.\nSoftware developers do not need to programmatically compress data that is to be\ntransmitted. Servers and browsers have compression implemented already. As long as both\nthe server and the browser understand the compression algorithm, it can be used. It is just a\nmatter of ensuring that the server is configured properly.\nThe two main types of compression that are used to improve web performance are file\ncompression and content-encoding (end-to-end) compression.\nFile compression\nFiles that you transmit, such as images, video, or audio, can have high rates of redundancy.\nWhen a web page is downloaded, images might account for the majority of the bytes being\ndownloaded. These types of files should be compressed to save storage space and increase\ntransfer speed.\n", "page": 364, "type": "text", "section": "Page 364"}
{"text": "Performance Considerations\nChapter 10\n[ 336 ]\nThere are various tools and algorithms that can be used to compress different file formats.\nAmong the choices you can make, depending on your needs, is whether to use a lossless or\na lossy compression algorithm.\nLossless compression\nWith lossless compression, all of the bytes from the original file can be recovered when the\nfile is decompressed. This type of compression algorithm may be necessary if the file you\nare compressing cannot afford to lose any information.\nFor example, if you are compressing a text file of data, source code for a program, or an\nexecutable file, you cannot afford to lose any of the contents and would want to use a\nlossless compression algorithm. For image, video, and audio files, you may or may not\nrequire lossless compression, depending on your needs for file size and quality.\nGraphics Interchange File (GIF) and Portable Network Graphics (PNG) are examples of\nimage file formats that provide lossless compression. If animation is required, you will\nwant to use the GIF format. If you want to preserve high-quality images and not lose any\nfine detail, the PNG image format should be used.\nLossy compression\nIf a lossy compression algorithm is used, some of the bytes from the original file will be lost\nwhen the file is decompressed. For a file in which you can afford to lose some bytes, you\ncan achieve a smaller file size as compared to using a lossless compression algorithm.\nThis type of compression works well with images, video, and audio because the loss of\nredundant information may be acceptable. There are different degrees of lossy\ncompression, and how aggressive you are with the optimization depends on the trade-off\nyou are willing to make between file size and quality. In some cases, you can use lossy\ncompression and there will be no perceptible difference to the user.\nJoint Photographic Experts Group (JPEG) is an example of an image file format that\nprovides lossy compression. If you do not need the highest quality image and can afford to\nlose some fine detail in the image, JPEG can be used.\n", "page": 365, "type": "text", "section": "Page 365"}
{"text": "Performance Considerations\nChapter 10\n[ 337 ]\nContent-encoding (end-to-end) compression\nSignificant performance improvements can be made when using content-encoding. The\nserver compresses the body of an HTTP message prior to sending it to the client. It will\nremain compressed (end-to-end compression) until it reaches the client. Any intermediate\nnodes that it may pass through while traveling to the client do not decompress the\nmessage. Once the message reaches the client, the client decompresses the body of the\nHTTP message.\nIn order to use content-encoding, the browser and server must agree on the compression\nalgorithm to use via content negotiation. Content negotiation is the process of selecting the\nbest representation of particular content.\nThere are a number of different types of compression, but gzip is the most common. It is a\nlossless type of compression. Although it can be used for any stream of bytes, it works\nparticularly well on text. Brotli (content-encoding type of br) is an open source, lossless\ndata compression library. It is newer than gzip, but it is gaining support and popularity.\nYou should take advantage of content-encoding as much as possible, except when\ntransferring files that have already been compressed with the aforementioned file\ncompression, such as image, video, and audio files. This is because you will typically not\ngain anything by compressing something twice, and it could even lead to a file size that is\nslightly larger than if it is just compressed once.\nMinifying resources\nMinification is the process of removing all unnecessary or redundant data from a resource.\nIt can be used to remove characters from source code that are not needed without changing\nany of the functionality.\nFiles such as JavaScript, HTML, and CSS are great candidates for minification. Although\nthe minified files that result from the process are not as human-readable as their original\ncounterparts, the file size will be smaller, resulting in faster load times.\nFor example, let's take the following JavaScript code:\n// Class representing a rectangle\nclass Rectangle {\n    constructor(height, width) {\n        this.height = height;\n        this.width = width;\n    }\n", "page": 366, "type": "text", "section": "Page 366"}
{"text": "Performance Considerations\nChapter 10\n[ 338 ]\n    // Method to calculate area\n    calculateArea() {\n        return this.width * this.height;\n    }\n}\nAfter minifying it, it appears as follows:\nclass\nRectangle{constructor(t,h){this.height=t;this.width=h}calculateArea(){retur\nn this.width*this.height}}\nYou can see that unnecessary characters for formatting and code comments have been\nremoved and the names of the constructor parameters have been shortened. There are a\nnumber of tools available to you that can minify files. Some of the tools focus on a\nparticular type of file (for example, JavaScript, HTML, or CSS). It is best to minify files prior\nto using the compression technique discussed in the Taking advantage of compression section.\nIt is a good practice to keep two versions of code files that are minified: a version that has\nnot been minified for debugging purposes and a minified version for deployment. They can\nbe given different filenames so that it is clear which one is the minified version. For\nexample, invoice.min.js could be used as the name for the minified version of\ninvoice.js.\nBundling resources\nThe first step in reducing the number of HTTP requests that need to be made in order to\nload a page is to remove all unnecessary resources. Once that is done, most web pages will\nstill require multiple files of the same type, such as JavaScript or CSS files, in order to load.\nDuring development, it makes sense to separate this type of code into multiple files.\nHowever, the use of more individual files translates into more HTTP requests.\nBundling is the process of combining multiple files of the same type into a single file,\nwhich can then be transferred in a single request. The technique of bundling files is\nsometimes referred to as concatenating files. Fewer HTTP requests lead to faster page load\nperformance. Bundling is a technique that is complementary with minifying files and the\ntwo are often used in conjunction with each other.\nBundling is an effective technique when we are using HTTP/1.1. In order to understand the\nreasons behind that, let's examine how assets are sent. In order to load a web page, the\nbrowser has to load each of the files that it needs, one at a time, over a connection, as shown\nin  the following diagram:\n", "page": 367, "type": "text", "section": "Page 367"}
{"text": "Performance Considerations\nChapter 10\n[ 339 ]\nThis process is too slow, so in order to get around this issue, browsers open up multiple\nconnections per host, shown as follows:\nThe maximum number of simultaneous connections that can be made per host varies by\nbrowser but a common number is six. The browser will handle these connections so\napplication developers do not need to make any modifications to their application to take\nadvantage of that feature. There is some overhead involved with setting up each connection\nbut it is worth it in order to have multiple connections available for communication.\n", "page": 368, "type": "text", "section": "Page 368"}
{"text": "Performance Considerations\nChapter 10\n[ 340 ]\nWe can improve this approach by bundling the files so that fewer HTTP requests, and\npossibly fewer connections, will be necessary to get all of the required assets. In the\nfollowing diagram, all of the CSS files are bundled together in styles.css, and all of the\nJavaScript files are bundled together in scripts.js:\nA downside of caching is that it can cause the cache to be invalidated more frequently.\nWithout bundling, we can control the caching of each individual file. Once we start\nbundling, if any of the files within the bundle have changed, the entire bundle will need to\nbe downloaded to clients again. If a file that is changed is contained within more than one\nbundle, it could cause multiple bundles to be downloaded again.\nEven with the use of bundling, the number of assets that a web page needs may be higher\nthan the maximum number of connections. This means that additional requests for assets\nfrom the same host are queued by the browser and will have to wait until a connection\nbecomes available. To work around this limitation, the technique of domain sharding was\nintroduced.\nIf there is a limit to the number of connections per domain, a workaround is to introduce\nadditional domains. Domain sharding is a technique in which resources are split among\nmultiple domains, allowing more to be downloaded in parallel. Instead of using the same\ndomain (for example, www.example.com), we can use multiple subdomains\n(shard1.example.com, shard2.example.com, and so on). Each shard is allowed the\nmaximum number of connections, increasing overall parallelism and allowing more assets\nto be transferred at the same time. In the following diagram, we are only returning three\nassets, which doesn't exceed the maximum number of connections per host. However, if we\ndid need more assets, we could retrieve more before queuing by the browser would\nbecome necessary:\n", "page": 369, "type": "text", "section": "Page 369"}
{"text": "Performance Considerations\nChapter 10\n[ 341 ]\nThere is overhead involved with adding shards, though, such as additional DNS lookups,\nthe additional resources required on both ends, and the fact that application developers\nwill need to manage how to split up their resources.\nLet's take a look at HTTP/2 now and learn how it can improve performance. Differences\nbetween HTTP/1.x and HTTP/2 affect how we want to approach techniques such as\nbundling and domain sharding.\n Using HTTP/2\nHTTP/2 is the latest version of the application layer protocol for data communication. It is\nnot a complete rewrite of the protocol. The HTTP status codes, verbs, methods, and most of\nthe headers that you are already familiar with from using HTTP/1.1 will continue to be the\nsame.\nOne difference between HTTP/2 and HTTP/1.1 is the fact that HTTP/2 is binary, whereas\nHTTP/1.1 is textual. HTTP/2 communication consists of binary-encoded messages and\nframes, making it more compact, efficient to parse, and less error-prone. The fact that\nHTTP/2 is binary is what enables some of the other HTTP/2 features that improve\nperformance.  \n", "page": 370, "type": "text", "section": "Page 370"}
{"text": "Performance Considerations\nChapter 10\n[ 342 ]\nMultiplexing\nOne of the most important features of HTTP/2 is multiplexing. Multiplexing is the ability to\nsend multiple HTTP requests and receive multiple HTTP responses asynchronously\nthrough a single TCP connection, as shown in the following diagram:\nHTTP/1.1 does not support multiplexing, which is what led to the various workarounds to\nimprove performance. With HTTP/2, we should no longer be concatenating files into a\nsmall number of large bundles. The expensive cache invalidation of a bundle that is\nnecessary when any of the files in the bundle are changed can now be avoided or\nminimized. We can now transfer more granular assets, either by not bundling at all or\nhaving an increased number of bundles, where each one contains a small number of related\nfiles rather than just a few bundles containing many. A more granular approach allows us\nto provide an optimal cache policy for each individual file or bundle, maximizing the\namount of content that is coming from our cache.\nAnother technique that is no longer necessary with HTTP/2 is domain sharding. Through\nthe use of multiplexing, we can download multiple assets at the same time using a single\nconnection. The overhead for each shard, which may have been worth it with HTTP/1.1, is\nno longer necessary.\nServer push\nHTTP/2 provides a feature in which the server can push responses that it thinks a client will\nneed. When a resource is requested from a client, it may contain references to other\nresources that are needed. Rather than wait for the client to send additional requests for\nthese required resources, the server already knows what resources will be needed and can\nproactively send them.\n", "page": 371, "type": "text", "section": "Page 371"}
{"text": "Performance Considerations\nChapter 10\n[ 343 ]\nThis feature is similar to inlining a resource, which is a technique that is sometimes used to\nimprove performance by reducing the number of requests necessary. Inlining is\naccomplished by embedding a resource, such as JavaScript, CSS, or images, into an HTML\npage. With server push, there is no longer a need to inline resources. We get the same\nadvantages of inlining but with the added benefit of keeping the assets in separate files,\neach with their own caching policies.\nThere are a few caveats with the server push feature. You should be careful not to push too\nmany assets at once; you do not want to delay rendering of the page and negatively affect\nperceived performance. You should put thought into what assets you are pushing and be\nselective about it.\nIf server push is not used properly, resources that a client already has could be transferred\nto it unnecessarily, which would actually hurt performance. As of today, use of the server\npush feature may require some experimentation in order for it to be used in an optimal\nway. Some web servers have the functionality to mitigate the problem of pushing assets\nthat the client does not need, and some browsers may introduce a cache digest so that a\nclient can let the server know what assets it already has in its local cache.\nHeader compression\nHTTP/2 performs header compression to improve performance. With HTTP/1.x, headers\nare always sent as plain text, but HTTP/2 uses the HPACK compression format to reduce\nthe size. HPACK is used for compression with HTTP/2 because it is resilient to some of the\nsecurity attacks that target compression, such as Compression Ratio Info-leak Made\nEasy (CRIME).\nHPACK uses Huffman coding, a lossless data compression algorithm that can significantly\nreduce the size of the headers, reducing latency and improving performance.\nImplementing HTTP/2\nIn order for a client to use HTTP/2, all that is needed is that the browser must support it.\nThe latest versions of most browsers support HTTP/2. On the server side, the steps required\nto configure HTTP/2 support varies. A number of web servers provide support for HTTP/2\nand the list continues to increase. In most cases, servers will need to support both HTTP/1.1\nand HTTP/2, so typically a server needs to be configured to fall back to HTTP/1.1 if a client\ndoes not support HTTP/2.\n", "page": 372, "type": "text", "section": "Page 372"}
{"text": "Performance Considerations\nChapter 10\n[ 344 ]\nUsing content delivery networks (CDNs)\nUsers visiting a website may potentially be located anywhere in the world. Round trips\nbetween clients and servers will take longer if the distance between them is greater. The\nadditional latency may be just milliseconds but it all contributes to the overall time it takes\nto receive a response.\nContent delivery networks (CDNs) are a geographically distributed group of servers that\ncan deliver content to users quickly. The nodes of a CDN are deployed in multiple locations\nso that they are distributed spatially. This provides us with the ability to reduce network\nlatency and serve up content closer to the end users, which improves load times.\nCDNs are great for transferring website content, such as JavaScript, HTML, CSS, image,\nvideo, and audio files. In addition to reducing the physical distance between users and\ncontent, CDNs improve load times through efficient load balancing, caching, minification,\nand file compression.\nThe reliability and redundancy of web applications are increased when a CDN is used\nbecause when traffic increases, it can be load balanced among multiple servers. If a server,\nor even an entire data center, is having technical issues, traffic can be routed to servers that\nare operational. CDNs can also help to improve security by mitigating distributed denial-\nof-service (DDoS) attacks and maintaining up-to-date TLS/SSL certificates.\nOptimizing web fonts\nGood typography is an important part of designing a good user interface, readability,\naccessibility, and branding. There was a time when web designers were limited in the fonts\nthey could use because there were only so many fonts that were guaranteed to be available\non all systems. These were known as web safe fonts.\nIt is possible to use fonts other than just the web safe fonts. For instance, in CSS, you can use\nthe font-family property to specify a list of fonts that can be used for an element, as\nfollows:\np {\n  font-family: Helvetica, Arial, Verdana, sans-serif;\n}\nWith this approach, the browser will use the first one that it finds available on the\nsystem. A disadvantage of using this approach is that during testing you have to ensure\nthat all of the fonts will work properly with your application.\n", "page": 373, "type": "text", "section": "Page 373"}
{"text": "Performance Considerations\nChapter 10\n[ 345 ]\nA CSS feature called web fonts was introduced to overcome some of the challenges. It\nprovides you with the ability to download font files so that any browser that supports web\nfonts can make the fonts that you want to use for your page available. Text using web fonts\nis selectable, searchable, zoomable, and will look good in a variety of screen sizes and\nresolutions.\nHowever, the use of web fonts means that additional resources must be loaded. If a website\nor web application is using web fonts, it is important to consider them as part of your\noverall web performance strategy. Optimization of web fonts can reduce the overall size of\na page and decrease rendering times. One thing that you should do is minimize the number\nof fonts (and their variants) that you use on your pages to minimize the number of\nresources that are needed.\nTo use web fonts, you must first select the one or more fonts that you want to use and\nconsider the character sets that you need to support based on any localization\nrequirements. The size of a font file is dependent on the complexity of the shapes in the\nvarious characters that make up the font.\nUnfortunately, there is no standard on font formats, which means that different browsers\nsupport different font formats. This lack of a standard means that as of right now, you will\nneed to support four different font formats for each font, and they are as follows:\nWeb Open Font Format version 2 (WOFF 2.0)\nWeb Open Font Format version 1 (WOFF)\nTrueType font (TTF)\nEmbedded Open Type (EOT)\nOnce the web fonts have been selected, the @font-face CSS rule allows you to use a web\nfont by allowing you to specify the font and the URL location where the font data can be\nfound. Regardless of which web font you select and which of the four font formats is being\nused by a particular user, compression is effective in reducing the font size and should be\nused to improve performance. WOFF 2.0 and WOFF have built-in compression, but the TTF\nand EOT formats are not compressed by default, so servers should use compression when\ndelivering these formats.\nWeb fonts can be large Unicode fonts with support for a variety of characters, not all of\nwhich will be needed at a given time.  The unicode-range property in @font-face can\nbe used to split up a font into multiple subsets so that only the characters that are actually\nneeded will be downloaded.\n", "page": 374, "type": "text", "section": "Page 374"}
{"text": "Performance Considerations\nChapter 10\n[ 346 ]\nOne more note about optimizing fonts is to keep in mind the fact that font resources are not\nupdated frequently. You should ensure that this type of resource is cached with a caching\npolicy that will allow them to live in the cache for a long period of time, with a validation\ntoken so that even once it expires, it can be renewed in the cache and not downloaded again\nas long as it has not changed.\nOptimizing the critical rendering path\nAn important step in reducing the amount of time that it takes for a page to render is to\noptimize the critical rendering path (CRP). The critical rendering path is the set of steps in \nbetween a browser receiving bytes from a server (for example, HTML, CSS, and JavaScript\nfiles) and the processing involved to render pixels on the device's screen.\nBefore a page can be rendered by a browser, it must construct both the Document Object\nModel (DOM) and the CSS Object Model (CSSOM). The HTML and CSS markup for the\npage are needed for this process. The DOM and the CSSOM are then combined to form a\nrender tree, which has both the content as well as the style information for what will be\nvisible on the screen.\nOnce the render tree has been constructed, the browser moves to the layout stage where it\ncalculates the size and position of the various visible elements. Finally, the paint stage is\nreached, where the browser uses the results of the layout to paint pixels to the screen.\nOptimizing the critical rendering path is the process of minimizing the time it takes to\nperform these various steps. We are mostly concerned with the portion of the page that is\nabove the fold, which refers to the part of the page that is visible without scrolling. The term\nis a reference to the upper half, or visible portion, of a folded newspaper. Until the user\nscrolls down a page, which may not even occur, they will not see what is below the fold.\nWe want to prevent render blocking by reducing, as much as possible, the resources that\nwill prevent the content above the fold from rendering. The initial step in this process is to\ndetermine what resources are truly necessary for the initial rendering of a page. We want to\nget these critical resources to the client as quickly as possible to speed up the initial\nrendering. For example, the HTML and CSS that are necessary to create the DOM and the\nCSSOM are render-blocking resources, so we want to get them to the client quickly. Using\nsome of the techniques described previously for web application performance, such as\ncompression and caching, can help to load critical resources faster.\n", "page": 375, "type": "text", "section": "Page 375"}
{"text": "Performance Considerations\nChapter 10\n[ 347 ]\nResources that are not required for the above-the-fold content or are otherwise not critical\nfor the initial rendering can either be eliminated, their download could be deferred, or they \ncan be loaded asynchronously. For example, if there are image files that are not needed for\ninitial rendering, they can be deferred, and in order to prevent DOM construction any\nblocking JavaScript files can be loaded asynchronously.\nUnderstanding what is involved in rendering a page, taking the time to think about what\nthe critical resources are for the initial rendering, and optimizing the critical rendering path\nwill allow a page to be constructed faster. Making a web page almost immediately visible\nand usable greatly improves a user's overall experience, which reduces the bounce rate and\nincreases the conversion rate.\nDatabase performance\nA key part of a software system is the database. So, when looking to improve the\nperformance of a system, improving database performance must be part of that effort. In\nthis section, we will take a look at some of the things that can be done to improve database\nperformance.\nDesigning an efficient database schema\nThe foundation of achieving peak database performance is an efficient and properly\ndesigned database schema. As a software architect, you may be working with a DBA who\nwill be responsible for database design. However, it is good to be familiar with the different\naspects of achieving a good database design.\nNormalizing a database\nNormalization is the process of designing tables (relations) and columns (attributes) so that\nthey not only meet data requirements, but minimize data redundancy and increase data\nintegrity.\nIn order to meet these goals, a database should contain the minimal number of attributes\nnecessary to meet the requirements. Attributes with a close logical relationship should be\nplaced together in the same relation. Redundancy of attributes should be kept to a\nminimum, which makes it easier to maintain data consistency and will minimize the size of\nthe database.\n", "page": 376, "type": "text", "section": "Page 376"}
{"text": "Performance Considerations\nChapter 10\n[ 348 ]\nDenormalizing a database\nFor performance and scalability reasons, there may be cases where it makes sense to\ndenormalize part of the database. It is important to differentiate between a database that\nhas not been normalized, and one that has been normalized and then is denormalized later.\nA database should be normalized first and then if there are cases where it makes sense to\ndenormalize, it should be done after careful consideration.\nDenormalization is a strategy used to improve performance, typically by shortening the\nexecution time of certain queries. This can be accomplished by storing redundant copies of\nsome data or grouping data together in order to minimize joins and improve query\nperformance.\nWhile denormalization might improve read performance, it will negatively affect write\nperformance. Some mechanisms, such as a compensating action, will be required to keep\nredundant data consistent. Using database constraints can help to enforce rules that will\nkeep the data consistent even when it is denormalized. Redundant data will also make the\ndatabase larger and therefore take up more disk space.\nAnother reason to introduce denormalization is to keep historical data. For example, let's\nsay that we have an address table and an orders table, and that each order is associated\nwith an address. An order is created, and then in the future, that address is updated. Now\nwhen you look at the old order, you see the new address and not the address given at the\ntime the order was created. By storing the address field values with each order record, you\ncan maintain this historical data. It should be noted, however, that there are ways to\naccomplish this without denormalization. For instance, you could treat an address like a\nvalue object (immutable), and simply create a new address record when one is modified,\nleaving the one associated with the old address intact. Also, in an event-driven system, or\none in which a data audit is being kept that stores the modifications that have been made to\na record, you could reconstruct what the address was at the time the order was created.\nIdentifying primary and foreign keys\nAll of the primary and foreign keys for all of the tables in the database should be identified.\nThe primary key of a table is the column, or combination of columns, that uniquely identify a\nrow in the table.\nSometimes a row in a table must reference a row from another table. A foreign key is a\ncolumn or combination of columns that hold the primary key value for a row in another\ntable so that it can be referenced.\n", "page": 377, "type": "text", "section": "Page 377"}
{"text": "Performance Considerations\nChapter 10\n[ 349 ]\nDatabase constraints based on the primary and foreign keys should be created to enforce\ndata integrity. A primary key constraint for a table consists of the one or more columns that\nmake up the primary key and a foreign key constraint consists of the one or more columns\nthat make up the foreign key.\nSelecting the most appropriate data types\nWhen designing a database table, we should select the most appropriate data type for each\ncolumn. In addition to the data type, the size and nullability of the column should also be\nconsidered. We want to choose a data type that will sufficiently hold all possible values but\nalso be the smallest data type that is necessary. This will maximize efficiency not just for\nperformance but also for storage size.\nUsing database indexes\nDatabase indexes can be used to improve performance and provide more efficient data\naccess and storage. They are stored on disk and associated with a database table or view to\nspeed up data retrieval. The two main types of indexes are the primary/clustered index and\nthe secondary/non-clustered index.\nPrimary/clustered indexes\nWhen designing a table, one approach is to keep the rows unordered and create as many\nsecondary indexes as necessary. Such an unordered structure is known as a heap. Another \napproach is to create a primary index, also known as a clustered index, to order the rows by\nthe primary key. It is common to have a clustered index on a table which is known as a\nclustered table. The only time you might not want one is if the table is very small (and one\nthat you know will remain small over time), such that the overhead of storing and\nmaintaining the index is not worth it when compared to simply searching the table.\nThe one or more columns that make up the primary key of a table are the columns that\nmake up the index definition. A clustered index sorts the rows in a table based on their key\nvalues and physically stores them on disk based on that order. Each table can only have one\nclustered index because the rows in the table can only be sorted and stored in one order.\n", "page": 378, "type": "text", "section": "Page 378"}
{"text": "Performance Considerations\nChapter 10\n[ 350 ]\nSecondary/non-clustered indexes\nIn addition to specifying a primary index, many database systems provide the ability to\ncreate secondary indexes, also known as non-clustered indexes. The performance of a database\ncan benefit from having secondary keys available for data access.\nNon-clustered indexes are defined by one or more columns that are ordered logically and\nserve as pointers to find the rest of the data for a given record. The order of a non-clustered\nindex does not match the physical order of how the records are stored on disk.\nNon-clustered indexes provide a way to specify an alternate key other than the primary key\nfor accessing records in a table. The key could be a foreign key or any column that will be\nfrequently used in joins, where clauses, ordering, or grouping. The advantage of using non-\nclustered indexes is to improve performance for the common ways that data might be\naccessed for a particular table beyond the primary key. Sometimes the primary key is not\nthe only way, and might not even be the most widely-used way, that records are retrieved\nfrom a table.\nFor example, let's say we have an Order table and a Customer table, with OrderId and\nCustomerId being the primary keys of those two tables, respectively. The Order table also\nhas a CustomerId column as a foreign key to the Customer table in order to associate\norders with customers. In addition to retrieving orders by OrderId, the system may need\nto retrieve orders by CustomerId on a frequent basis. Adding a non-clustered index on the\nCustomerId column of the Order table allows for more efficient data retrieval when it is\naccessed with that column.\nAlthough non-clustered indexes are an important part of performance tuning a database,\neach table should be analyzed carefully when deciding which non-clustered indexes, if any,\nto add to a table and which column or columns should make up the index. As we will learn\nin a moment, there is overhead related to adding indexes to a table, so we do not want to\nadd any unnecessary ones.\nHaving too many indexes\nThere is a cost associated with adding an index to a table, so when it comes to indexes, you\ncan have too much of a good thing. Every time that a record is added or updated in a table,\nan index record also has to be added or updated, incurring some additional overhead to\nthose transactions. In terms of storage, indexes take up additional disk space, increasing the\noverall size of your database.\n", "page": 379, "type": "text", "section": "Page 379"}
{"text": "Performance Considerations\nChapter 10\n[ 351 ]\nThe more indexes you have on a table, the more the query optimizer of the database\nmanagement system (DBMS) will have to take into consideration for a particular query.\nThe query optimizer is a component of the DBMS that analyzes queries to determine the\nmost efficient execution plans for them.\nFor example, the query optimizer decides whether a particular join in a query should do a\nfull table scan versus using an index. It must take into account, among other things, all of\nthe indexes on that table. As a result, an increased number of indexes on a table could\nadversely affect performance.\nFor these reasons, one should be selective when considering what indexes to add to a table.\nProperly selected indexes will speed up data access performance, but you do not want to\ncreate unnecessary ones as they can slow down data access and operations, such as inserts\nand updates.\nScaling up and out\nScaling your database server vertically (up) or horizontally (out) to improve performance is\nnot something that should be done without an understanding that it is necessary. Prior to\nscaling up or scaling out, software architects and DBAs should ensure that the database\nschema has been designed properly and that indexes have been applied properly. In\naddition, the application using the database should be optimized to improve performance\nand remove bottlenecks.\nOnce those measures have been taken, if the database server is experiencing high levels of\nresource use, it's time to consider scaling the server up or scaling it out. For database\nservers, it is best to scale up first by performing actions such as replacing the server with a\nbetter machine or by adding processors and/or memory.\nScaling up should be done first because there are additional complications with scaling a\ndatabase server out. When you have multiple servers, you may need to horizontally\npartition some of the tables and consider data replication. Plans for disaster recovery and\nfailover are also more complex when there are multiple database servers. However, if\ndatabase performance is still not at the level you need it to be after scaling up, scaling out\nmay be necessary.\n", "page": 380, "type": "text", "section": "Page 380"}
{"text": "Performance Considerations\nChapter 10\n[ 352 ]\nDatabase concurrency\nA relational database system can handle many simultaneous connections. Having a\ndatabase that performs well is only useful if it can handle multiple processes accessing and\nchanging data at the same time. This is what database concurrency is all about.\nConcurrency control ensures that database transactions that are performed concurrently\nmaintain data integrity. We'll now begin looking at concurrency by learning about database\ntransactions.\nDatabase transactions\nA database transaction is a sequence of operations that are performed as a single unit of\nwork. They play an important role in maintaining data integrity and consistency even when\ndata is being accessed and changed at the same time. They provide units of work that either\ncomplete in their entirety or will not be committed at all.\nTransactions can recover from failures and keep the database in a consistent state.\nTransactions also provide isolation so that a record that is in the process of being modified\nby one transaction is not affected by a concurrent transaction that must update the same\nrecord. Transactions, once they are complete, are written to durable storage.\nOptimistic versus pessimistic concurrency control\nConcurrency control ensures that databases transactions that are performed concurrently\nmaintain data integrity. Many databases offer two main types of concurrency control:\noptimistic and pessimistic.\nOptimistic concurrency control (or optimistic locking) works under the assumption that\nresource conflicts between multiple users, while possible, are not common. Therefore, it\nallows transactions to execute without locking resources. If data is being changed, resources\nare checked for conflicts. If there is a conflict, only one transaction is successful while the\nothers fail.\n", "page": 381, "type": "text", "section": "Page 381"}
{"text": "Performance Considerations\nChapter 10\n[ 353 ]\nIn contrast, pessimistic concurrency (or pessimistic locking) assumes the worst, such as\nassuming that more than one user will want to update the same record at the same time. In\norder to prevent that, it locks the appropriate resources as they are required for the\nduration of the transaction. Unless a deadlock takes place, pessimistic concurrency ensures\nthat a transaction will be completed successfully. It should be noted that most database\nsystems have different types of locks. For example, one type of lock might specify that a\nrecord that is locked can still be read by another user, while another type of lock might\nprevent that type of read.\nCAP theorem\nThe Consistency, Availability, and Partition tolerance (CAP) theorem, also known as\nBrewer's Theorem after Eric Brewer who published it, states that a distributed system can\nonly achieve two of the following three guarantees, but not all three:\nConsistency: Every read either returns the latest data or an error. Every\ntransaction either completes successfully and is committed or is rolled back due\nto a failure.\nAvailability: A system always provides a response to every request.\nPartition tolerance: In a distributed system (data is partitioned to different\nservers), if one of the nodes fails, the system should still be able to function.\nDatabases will stress some of these guarantees over others. A traditional relational database\nmanagement system will focus on consistency and availability. They will favor strong\nconsistency, which is also known as immediate consistency, so that any read of the data\nwill reflect any changes that have been made to that data. These types of databases will\nfollow an ACID consistency model.\nSome databases, such as some NoSQL databases, will value availability and partition\ntolerance over consistency. For such databases, eventual consistency, rather than strong\nconsistency, is acceptable. Eventually, the data will reflect all of the changes made to it, but\nat any given point in time it is possible to read data that may not reflect the latest changes.\nThese types of databases follow a BASE consistency model.\n", "page": 382, "type": "text", "section": "Page 382"}
{"text": "Performance Considerations\nChapter 10\n[ 354 ]\nACID model\nDatabases that want to ensure consistency and availability will follow the ACID\nconsistency model. Traditional relational databases follow the ACID model. Database \ntransactions adhere to the ACID properties in that they must be atomic, consistent, isolated,\nand durable. These properties guarantee the validity of the data even when errors or\nfailures occur. Strong consistency will place limits on performance and scalability.\nAtomicity\nA transaction must be an atomic unit of work, meaning that either all of its data\nmodifications are performed or none at all. This provides reliability because if there is\nfailure in the middle of a transaction, none of the changes in that transaction will be\ncommitted. For example, in a financial transaction, you may insert one record to represent\nthe credit part of the transaction and another to represent the debit part of the transaction.\nYou don't want one of those inserts to take place without the other, so you place them both\nas part of one transaction. Either they will both be committed or neither of them will be\ncommitted.\nConsistency\nAfter a transaction takes place, all of the data must be in a consistent state. This property\nensures that all transactions maintain data integrity constraints, leaving the data consistent.\nIf a transaction leaves data in an invalid state, the transaction is aborted and an error is\nreported. For example, if you had a column with a check constraint that states a column\nvalue must be greater than or equal to zero (so as not to allow negative numbers), the\ntransaction would fail if it attempted to insert or update a record with a value less than zero\nfor that particular column.\nIsolation\nChanges made by concurrent transactions must be isolated from changes made by any\nother concurrent transactions. Many DBMSs have different isolation levels that control the\ndegree to which locking occurs on data being accessed. For example, a DBMS may place a\nlock on a record being updated so that another transaction cannot update that same record\nat the same time.\n", "page": 383, "type": "text", "section": "Page 383"}
{"text": "Performance Considerations\nChapter 10\n[ 355 ]\nDurability\nOnce a transaction completes and is committed, its changes are persisted permanently in\nthe database. For example, a DBMS may implement durability by writing all transactions to\na transaction log. The transaction log can be used to recreate the system state at any point,\nsuch as right before failure.\nBASE model\nSome databases, such as some distributed NoSQL databases, focus on availability and\npartition tolerance. In some situations, it may be an acceptable tradeoff to have eventual\nconsistency, rather than strong consistency, in order to focus on partition tolerance,\nperformance, and scalability. This approach enables a higher degree of scalability and can\nyield faster performance. These databases use the BASE consistency model instead of the\nACID model.\nBasic availability\nMost of the time, conflicts do not take place. The database is available most of the time and\na response will be sent for every request. However, conflicts can occur and the response\nmay indicate that a failure occurred when trying to access or change data.\nSoft state\nRather than following consistency requirements such as those in the ACID model, the\nconcept here is that the state of the system could change over time. Even if no additional\ntransactions are being created, changes could take place due to eventual consistency.\nEventual consistency\nA data change will eventually propagate to everywhere that it needs to go. If there are no\nfurther changes to a piece of data, eventually the data will be in a consistent state. This\nmeans it is possible to read stale data if the latest updates to it have not been applied yet.\nUnlike a strong consistency model in which all data changes are atomic and the transaction\nis not allowed to complete until either the change finishes successfully or is rolled back due\nto a failure, the system will not check the consistency of every transaction.\n", "page": 384, "type": "text", "section": "Page 384"}
{"text": "Performance Considerations\nChapter 10\n[ 356 ]\nSummary\nIt is more important to ensure that your code is correct than fast. Fast performance is not of\nany use if the application does not yield the correct results. Having said that, performance\nis an important part of designing and developing a successful software application. It plays\na large part in the overall user experience for people who use the application. Regardless of\nthe device they are using or their location, users expect a high level of responsiveness from\ntheir applications.\nPerformance is a quality attribute of the software system and performance requirements\nshould be documented. Like all requirements, they need to be measurable and testable. The\nentire team should take ownership of performance. A systematic, iterative approach to\nperformance improvement can help a development team reach their performance goals.\nSome problems will only be discovered later, so development teams should be prepared to\nanalyze and optimize in an iterative way. In this chapter, you learned how to use server-\nside caching, about different techniques to improve web application performance, and how\nto improve database performance.\nIn the next chapter, we will explore the various security considerations that a software\narchitect must make. We will examine the goals of security and the design principles and\npractices that will help us to achieve them. The chapter will cover techniques such as threat\nmodeling, and topics such as cryptography, identity, and access management, and how to\nhandle common web application security risks.\n", "page": 385, "type": "text", "section": "Page 385"}
{"text": "11\nSecurity Considerations\nDesigning and developing software systems that are secure is of vital importance. A\nsoftware application that does not follow secure practices creates vulnerabilities that can be\nexploited by attackers. The result of an attack can lead to unauthorized access to\nconfidential data, financial losses, and ruining an organization's reputation.\nWe will explore the three states that information can be in and the main goals of\ninformation security, represented by the confidentiality, integrity, and\navailability (CIA) triad. We will take a look at how threat modeling can help to identify\nand prioritize threats. We will learn principles and practices that will help create secure\napplications by design.\nThe chapter will cover tools such as encryption and hashing along with the best ways to\nimplement identity and access management. The chapter concludes by taking a look at\nsome of the most common web application security risks and ways to mitigate them.\nIn this chapter, we will cover the following topics:\nThree states of information\nThe CIA triad\nThreat modeling\nPrinciples and practices of creating software that is secure by design\nCryptography (encryption and hashing)\nIdentity and access management, including authentication and authorization\nMost common web application security risks\n", "page": 386, "type": "text", "section": "Page 386"}
{"text": "Security Considerations\nChapter 11\n[ 358 ]\nSecuring software systems\nSecurity is the ability of a software application to prevent and protect against malicious\nattacks and the unauthorized use of the application and its data. It involves protecting one\nof the most important assets that an organization can possess, which is information.\nInformation assets include not just data but also things such as logs and source code. It is\nthe responsibility of software architects and developers to protect software applications and\ndata. When we are designing and developing software systems, this is why we must focus\nthe proper amount of attention on security considerations.\nSecurity is a quality attribute and, as is the case with other quality attributes that we have\ncovered in this book, we must think about and document the requirements for quality\nattributes. Requirements for security must be specified and they must be precise,\nmeasurable, and testable. We need to be able to determine if we are meeting security\nrequirements.\nDifferent software systems have different security requirements so it is important to\nunderstand the security needs of the system. A website for a blog has very different\nsecurity needs than a payroll application. There is a variety of different ways that an\nattacker can compromise a system, so threats must be considered carefully. It is difficult to\njust add on security later. Security is architectural and must be considered during\nrequirements, design, development, and testing.\nThe three states of information\nThe information that we strive to protect can be in one of three states. It can either be at rest,\nin use, or in transit. Information in all of these states is vulnerable to attack and needs to be\nconsidered in the context of security.\nInformation that is at rest is currently not being accessed. It is stored in some form of\npersistent storage, such as a database or a file. It can eventually be accessed either through\nan application or directly if someone had access to the persistent storage.\nInformation that is in use refers to information that is currently being used by some process\nor application. It is data that is currently in a non-persisted state. Data currently in memory\nor in a CPU cache are examples of information that is in use.\nInformation that is in transit is in the process of being moved, perhaps over a network. It is\npotentially accessible as it is being transferred. Secure transmission of the data, using a\nsecure channel, is necessary to provide secure communication of information.\n", "page": 387, "type": "text", "section": "Page 387"}
{"text": "Security Considerations\nChapter 11\n[ 359 ]\nThe CIA triad\nThe CIA triad represents some primary goals for information security and the protection of\ninformation assets. The CIA triad summarizes the attributes that we want our software\nsystems to exhibit. CIA stands for confidentiality, integrity, and availability:\nYou may see the CIA triad referred to as the AIC triad in order to avoid confusion with the\nCentral Intelligence Agency.\nSoftware architects should strive for a balance between the confidentiality, integrity, and\navailability of information. Let's now take a closer look at the three fundamental objectives\nof the CIA triad.\nConfidentiality\nSoftware applications should protect confidentiality. The information that a software\napplication manages has value to its users and to the organization that created the\napplication. Confidentiality involves preventing unauthorized individuals from accessing\ninformation.\nThe application must protect its data, particularly when the data is private and personal.\nThis includes when data is in any of the three states of information. Information can be\nvulnerable to eavesdropping while it is being transmitted between services or authorized\nparties.\nIntegrity\nSoftware applications should ensure integrity. The goal is to prevent unauthorized\nindividuals from modifying or destroying information. No matter which of the three states\nof information the data is currently in, the application has a responsibility to ensure that it\nhas not been tampered with by an unauthorized party.\n", "page": 388, "type": "text", "section": "Page 388"}
{"text": "Security Considerations\nChapter 11\n[ 360 ]\nAvailability\nSoftware applications need to maintain availability. When implementing security,\navailability is among the desired qualities. If the security mechanisms employed are overly\nextensive given the requirements, then the usability and availability of the system will be\nlessened.\nSoftware applications must allow authorized individuals access to information in a timely\nand reliable way. Securing data serves no purpose if authorized users cannot get to it. The\ninformation that a software application manages only has value when the right people can\naccess it when they need it.\nChapter 4, Software Quality Attributes, covered quality attributes, including availability.\nPart of what this involves is being able to detect, recover from, and prevent faults to ensure\nthat the application and its data are available to users when they want it.\nThreat modeling\nThreat modeling is a structured approach to analyzing security for an application. A threat\nis a possible danger to a software system and may cause serious harm to it. Threat\nmodeling is a process that identifies and prioritizes potential security threats so that a\ndevelopment team can understand where their application is most vulnerable. Threat\nmodeling evaluates threats with the goal of reducing an application's overall security risks.\nOnce the analysis is complete, a plan can be formulated to mitigate identified security risks.\nTraditional approaches to software security may have focused on security from the\ndefender's point of view. However, modern approaches use threat modeling to focus on\nsecurity from the attacker's viewpoint. A threat agent is any individual or group that may\nattack a software system and exploit its vulnerabilities.\nWhile threat modeling can be applied to an existing software system, the inclusion of threat\nmodeling while designing and developing a software system is an effective way to ensure\nthat security is an integral part of the system.\nAt a high level, software architects should seek to decompose the software system, identify\nand categorize threats, prioritize threats, and create ways to mitigate them.\n", "page": 389, "type": "text", "section": "Page 389"}
{"text": "Security Considerations\nChapter 11\n[ 361 ]\nDecomposing an application\nWhile a new software system is being designed, or during the analysis of an existing\nsoftware system, we want to begin threat modeling with an understanding of the software\nsystem. Decomposing an application helps us to understand our software application better\nand uncover security vulnerabilities. Decomposing an application includes knowing the\nassets that an attacker may be interested in, the potential attackers of the system, the\ninteractions with external entities, and the entry points into the system.\nOrganizations and their software systems have assets, which are the things of value to\nattackers. Assets might be physical, such as obtaining login credentials or a software\nsystem's data. They can also be abstract, such as an organization's reputation.\nPart of considering the various threats to your software system includes gaining an\nunderstanding as to who may be potential attackers. Attackers may be external or internal\nto an organization. You must consider both types of attacker, and in doing so will realize\nthere may be different entry points into your application and different ways of attacking it.\nAttackers will be motivated by the assets you have identified, so taking assets into\nconsideration will help you to identify potential attackers.\nYou need to understand how the software system interacts with different external entities.\nExternal entities include users as well as external systems. These interactions will allow you\nto identify entry points into your system, which are locations where potential attackers can\ninteract with the software system. Attackers will focus their efforts on entry points as they\nprovide opportunities for them to carry out their attacks.\nIdentifying and categorizing potential threats\nOnce we have a good understanding of the software system, we need to identify and\ncategorize potential threats. In order to categorize threats, we have to agree on a threat\nclassification model. A threat classification model provides a set of threat categories with\ndefinitions so that each identified threat can be categorized in a systematic and repeatable\nway. STRIDE is one type of threat classification model.\n", "page": 390, "type": "text", "section": "Page 390"}
{"text": "Security Considerations\nChapter 11\n[ 362 ]\nSTRIDE threat model\nSTRIDE is a security threat model that was originally created by Microsoft. The name is an\nacronym that represents six threat categories:\nSpoofing identity\nTampering with data\nRepudiation\nInformation disclosure\nDenial-of-service\nElevation of Privilege\nSpoo\ufb01ng identity\nSpoofing identity is the act of representing yourself as someone else. For example, if an\nattacker gained access to someone's authentication information, such as their username and\npassword, they could use it to spoof an identity. Other examples of spoofing identity are\nforging an email address or the modification of header information in a request with the\npurpose of gaining unauthorized access to a software system.\nTampering with data\nTampering with data involves an attacker who modifies data. Examples of tampering with\ndata include modifying persisted data in a database, changing data as it travels over a\nnetwork, and modifying data in files.\nRepudiation\nRepudiation threats can occur if a software system does not properly track and log actions\nthat take place. This allows users, legitimate or otherwise, to be able to deny that they\nperformed a particular action. For example, an attacker could manipulate data and then\ndeny responsibility. Without the system being able to trace the operations properly, there\nwould be no way to prove otherwise. Such an attack could involve sending inaccurate\ninformation to log files, making the entries in the log files misleading and unusable.\nIn software systems, we seek non-repudiation, which is the assurance that a person cannot\ndeny an action that they performed. Strong authentication, accurate and thorough logging,\nand the use of digital certificates can be used to counter repudiation threats.\n", "page": 391, "type": "text", "section": "Page 391"}
{"text": "Security Considerations\nChapter 11\n[ 363 ]\nInformation disclosure\nInformation disclosure is a category of threat that involves a software system failing to\nprotect information from individuals who are not supposed to have access to the\ninformation, for example, allowing an attacker to read data from a database or while it is in\ntransit over a network.\nThe information that an attacker obtains could potentially be used for other types of attack.\nFor example, an attacker can obtain system information (server OS version, application\nframework version, and so on), source code details, information from error messages,\naccount credentials, or API keys. The information taken by an attacker can then be used as\nthe basis for further, more damaging attacks.\nDenial-of-service\nA denial-of-service (DoS) attack takes place whenever an attacker is able to deny service to\nvalid users. An attacker can flood servers with packets to the point that the servers become\nunavailable or unusable. If a large number of bogus requests are sent to servers, they can be\noverloaded such that they cannot fulfill legitimate requests.\nWhile a DoS attack may be conducted by a single computer, one type of DoS attack, known\nas a distributed denial-of-service (DDoS) attack, floods a victim from many different\nsources. This makes it much more difficult to block the source of the attack as well as to\ndifferentiate between legitimate traffic and traffic that is part of the attack.\nElevation of Privilege\nElevation of Privilege (EoP) takes places when an attacker is able to gain authorization for\noperations beyond what was originally granted. For example, an attacker obtains read and\nwrite privileges for an application when they were initially only granted read privileges.\nThe dangerous aspect of this threat is the fact that the attacker is part of the trusted system\nitself. Depending on the privileges the attacker is able to obtain, they may be able to inflict a\nhigh level of damage.\nPrioritizing potential threats\nOnce threats have been identified and categorized, we can prioritize them based on their\npotential impact on the software system, the likelihood that they may occur, and the ease\nwith which they can be exploited. These qualities can be used to give a qualitative ranking\n(for example, High, Medium, and Low) to prioritize threats.\n", "page": 392, "type": "text", "section": "Page 392"}
{"text": "Security Considerations\nChapter 11\n[ 364 ]\nAnother approach to prioritizing threats is to utilize a threat-risk ranking model. The\nDREAD risk assessment model is one example of this type of ranking model.\nDREAD risk assessment model\nDREAD is a risk assessment model that can be used to prioritize security threats. Like the\nSTRIDE model, it was created by Microsoft. DREAD is an acronym that represents the\nfollowing risk factors:\nDamage potential\nReproducibility\nExploitability\nAffected users\nDiscoverability\nEach risk factor for a given threat can be given a score (for example, 1 to 10). The sum of all\nthe factors divided by the number of factors represents the overall level of risk for the\nthreat. A higher score signifies a higher level of risk and would typically be given a higher\npriority when determining which threats should be focused on first.\nDamage potential\nDamage potential represents the level of damage that could be done to users and the\norganization if an attack were to succeed. For example, damage to an individual user's data\nwould be rated lower than an attack that could bring down the entire system. Depending\non the type of attack and the asset(s) being targeted, damage could be something concrete,\nsuch as financial liability, or abstract, such as damage to an organization's reputation.\nReproducibility\nReproducibility is a measure of how easy it is to reproduce a particular attack. An attack\nthat can be reproduced reliably would be rated higher than one that is statistically unlikely\nto be exploited or one that cannot be reproduced consistently.\n", "page": 393, "type": "text", "section": "Page 393"}
{"text": "Security Considerations\nChapter 11\n[ 365 ]\nExploitability\nThe exploitability of a threat describes how difficult it is to exploit a vulnerability. While\nsome exploits are easily understood and could be done by anyone (even perhaps\nunauthenticated users), some require advanced techniques, tools, or scripts. A threat with a\nvery low level of exploitability would be difficult to execute even with knowledge of the\nvulnerability.\nA\ufb00ected users\nThe affected users risk factor represents percentage of users that will be affected by a\nparticular threat. While some attacks may only affect a small number of users, some can\naffect almost all users. The greater the number of users who may potentially be affected, the\nhigher this risk factor should be rated.\nDiscoverability\nDiscoverability signifies how easy it is to learn about the vulnerability. A threat that is very\ndifficult to uncover would be rated lower than one that has already been disseminated in\nthe public domain.\nMany security professionals believe that discoverability should not be part of the model\nbecause the overall threat ranking should not be affected by this factor. Security by\nobscurity is a weak security control and it is not wise to consider a security risk less of a\nthreat simply because it is difficult to discover.\nSome practitioners use a DREAD-D (DREAD minus D) model and eliminate discoverability\naltogether. Alternatively, a development team can assign the maximum rating for\ndiscoverability for each threat, which effectively removes it as a factor.\nResponses to threats\nOnce we have identified, categorized, and prioritized the threats to our software system,\nwe can produce approaches that document how we want to respond to the threats. In\nthe Software risk management section of Chapter 2, Software Architecture in an Organization,\nwe discussed software risk management in the context of project management and\ndelivering a software application. The different risk management options can also be\napplied to the context of security threats. As a response to a security risk, we can avoid the\nrisk, transfer the risk to another party, accept the risk, or mitigate the risk.\n", "page": 394, "type": "text", "section": "Page 394"}
{"text": "Security Considerations\nChapter 11\n[ 366 ]\nAvoiding the risk\nRisk avoidance requires us to make changes so that the risk no longer exists or is reduced. It\nshould be noted that not all security risks can be avoided, and avoiding a security risk can\nlead to other risks.\nTransferring the risk\nTransferring the risk to another party can be a viable strategy for some security threats.\nSome common ways in which risk can be shifted to another party is through an insurance\npolicy and through contracts.\nIf an insurance policy is purchased, an insurance company will assume the financial risks\nthat might result from a security threat. Keep in mind though that it may not be possible for\ninsurance to make up for the damage that a security attack can inflict on an organization. If\nan organization's reputation is ruined and the trust of customers is lost, the consequences to\na business can be catastrophic.\nWe can also transfer a security risk to another party by contracting out the work. We will\ndiscuss the different types of security control shortly but whether it is physical or technical\nsecurity, we can opt to have another organization handle it for us through a contract. The\ncontract can contain provisions that protect the organization should requirements not be\nmet.\nAn example of this approach is hosting your application with a cloud provider. Some\naspects of security can be transferred to the cloud provider, such as the physical security of\nfacilities and servers, environmental security (for example, patching the servers for security\nthreats and providing anti-virus/malware protection), and handling the security of your\ndata.\nAccepting the risk\nAnother approach would be to simply accept the risk. If the prioritization of a security\nthreat is low based on factors such as the potential damage, reproducibility, exploitability,\nthe potential number of affected users, and the effort it would take to mitigate the risk, it is\nsometimes decided to accept the risk.\n", "page": 395, "type": "text", "section": "Page 395"}
{"text": "Security Considerations\nChapter 11\n[ 367 ]\nMitigating the risk\nRisk mitigation of a security threat involves implementing some type of security control to\nmitigate the risk. The goal is to reduce or eliminate the possibility of the threat occurring\nand/or reduce the amount of damage that can result from the threat. There are different\ntypes of security controls that can be put in place to mitigate security risks depending on\nthe nature of the threat.\nTypes of security control\nSecurity controls are countermeasures or safeguards that are used to handle security risks.\nWhen taking a holistic approach to the security of your software application, a software\narchitect must look at the different types of security controls that can be used, including\nphysical, administrative, and technical security.\nIn addition to categorizing security controls by the manner in which they work, we can\ncategorize security controls by their overall goal and purpose. A security goal can be used\nfor prevention, detection, or a response to a threat. A security control can be described by a\ncombination of these two different types of categorization. For example, a security control\ncan be both technical and for the purpose of detection.\nPhysical security controls\nPhysical security consists of security measures that are put in place to prevent\nunauthorized access to facilities, equipment (for example, servers), personnel, and other\nresources. Physical security is a combination of multiple systems working together and\nmay include gates, locks, key cards, video surveillance, lighting, alarm systems, and\nsecurity personnel.\nWhen you think about securing a software application, you may tend to focus on other\naspects of security, but physical security should not be overlooked. Some security threats\nare only made possible when an attacker gains physical access to one or more resources.\nAttackers may be internal or external to the organization, so the use of multiple techniques\nis the most effective way of enforcing physical security.\n", "page": 396, "type": "text", "section": "Page 396"}
{"text": "Security Considerations\nChapter 11\n[ 368 ]\nFor some organizations, this is yet another advantage of hosting their software applications\nin the cloud. Major cloud providers have a tremendous amount of experience operating\ndata centers and provide a high level of physical security. It can be cost-effective,\nparticularly for smaller organizations, to use a cloud provider and leverage their security as\nopposed to operating their own data center.\nAdministrative controls\nAdministrative controls include organizational policies and procedures that are put in place\nfor security. An organization must consider security concerns in their overall organizational\npolicies and procedures. Some examples of this include:\nSecurity awareness training for employees\nEscalation plan in the event of a security attack\nEmployees being required to carry and display a photo identification card\nPolicies regarding acceptable use of company hardware and networks\nPolicies related to what type of software can be installed on company hardware\nRules on how company and customer data is handled\nImplementation of company password policies, including the required level of\ncomplexity and requiring employees to change them periodically\nRequired use of anti-virus software\nProcedures regarding required software updates, including operating system\npatches\nRules related to opening and sending emails, including their attachments\nPolicies related to remote access of company hardware and networks\nProcedures regarding the monitoring of servers\nWireless network communication procedures\nA service continuity plan that can be used to keep a software system operational\nafter an event such as a security attack or natural disaster\nAn organization's hiring practices for all of their employees should also consider security.\nThey must include steps to minimize the potential of hiring someone who is or can become\na security concern.\nA pre-employment screening should be conducted prior to hiring a new employee in order\nto investigate and confirm the backgrounds of candidates. Background checks can include\nidentity and address verification, a criminal history check, speaking with references, and\nconfirming academic accomplishments.\n", "page": 397, "type": "text", "section": "Page 397"}
{"text": "Security Considerations\nChapter 11\n[ 369 ]\nPre-employment screenings also typically involve a credit check. These types of check do\nnot reveal a candidate's credit score or account numbers. An excessive number of late\npayments and the mishandling of personal finances may indicate that a candidate is not\norganized or responsible. If a candidate has excessive debt, is using a high percentage of\ntheir available credit, or is having other financial difficulties, there may be an increased\nlikelihood of theft. Employers need to notify the candidate and get their permission in\norder to conduct a credit check. Laws regarding credit checks vary by state so the human\nresources (HR) department of an organization should be aware of any laws that it must\nfollow.\nScreening employees should not be considered a one and done type of event but rather a\ncontinuous process. Many employees do not have any intention of attacking the\norganization when they are hired, but things can change over time that cause an employee\nto change their motivations later. Organizations should also have procedures for the\ntermination of an employee. A set of steps should be taken each time an employee quits or\nis otherwise terminated so that the different types of access they have been granted (for\nexample, physical access to a facility, access to their email account, and network access) are\nrevoked in a timely manner.\nTechnical security controls\nTechnical security controls utilize technology to provide security for a software system.\nThese controls are implemented through technical solutions such as software, firmware, or\nhardware. We will discuss some of the technical security controls available to us later in\nthis chapter, but examples include the use of encryption, hashing, authentication,\nauthorization, logging, monitoring, utilizing proper communication protocols,\nhardware/network protection, and database security.\nPrevention\nThe purpose of a prevention security control is to avert a security threat before it occurs.\nPreventing a security threat requires analysis and planning. Some physical, administrative,\nand technical security controls are used for prevention.\nFor example, locks and key cards are physical control measures intended to prevent a\nsecurity attack. Security awareness training and company password policies are\nadministrative examples of preventive controls. Technical controls that are preventive\ninclude encryption, hashing, authentication, authorization, installing operating system\nsecurity patches, using anti-virus and malware detection software, and the use of firewalls.\n", "page": 398, "type": "text", "section": "Page 398"}
{"text": "Security Considerations\nChapter 11\n[ 370 ]\nDetection\nSecurity controls designed to detect a security threat are an important part of an overall\nsecurity strategy. Regardless of the preventive measures that are put into place, you should\nexpect that they will fail and assume that a security attack will eventually happen. This will\nput you in the proper mindset to earnestly consider how to detect attacks. It is critical for\nyour software to have the ability to detect and properly notify individuals when an attack\noccurs.\nSecurity cameras, motion detectors, system monitoring, logging, auditing, and the use of\nanti-virus and malware detection software are all examples of detection security controls.\nResponse\nThe detection of threats has value only if there are available responses. Plans for the\nresponses to various attacks should be made in advance. You do not want to be making\nimportant decisions in the middle of an attack.\nExamples of a physical security response include the sounding of an alarm and the locking\nof doors. An administrative security control with the purpose of responding to a security\nthreat is an escalation plan that dictates the actions to be taken in the event of an attack.\nAnother example is a service continuity plan that ensures the software application will\ncontinue to be operational even in the face of an unexpected event such as a security attack,\na natural disaster, or some other event.\nTechnical security controls used in the response of a security attack include taking servers\noffline, using anti-virus software to remove and quarantine a virus, rolling back to a backup\nversion of the application, restoring data from a backup, and revoking user\npermissions/disabling a user account.\nSecure by design\nSoftware architects should strive to create software systems that are secure by design. By\nfollowing proven security principles and practices, we can make our software applications\nmore secure.\n", "page": 399, "type": "text", "section": "Page 399"}
{"text": "Security Considerations\nChapter 11\n[ 371 ]\nMinimizing the attack surface\nThe attack surface consists of all of the points that an attacker can use to get into a system.\nThe design of a software system should attempt to minimize the total attack surface area as\nmuch as possible. Using different types of security control and following security principles\nand practices that are known to improve security can reduce the attack surface. One should\nconsider the attack surface when choosing from among multiple approaches that will\nsatisfy a particular functional requirement.\nDefense in depth\nSecurity tends to be more effective when a variety of techniques is used together. No\nsecurity control is perfect, and defense in depth is the concept of using multiple techniques\nin conjunction and the belief that in doing so a software system will be made more secure.\nIf one security control fails, a threat may be prevented by another security control. Layering\ndefenses using several independent methods will make it much more difficult to exploit a\nvulnerability.\nPrinciple of least privilege (PoLP)\nOne of the security principle that can be applied to software systems is the principle of\nleast privilege (PoLP). The principle of least privilege, which is sometimes referred to as\nthe principle of least authority, informs us that the least amount of privileges that are\nnecessary should be granted to a user or process in order to reduce security risks.\nFollowing this principle is one way to minimize the attack surface.\nIn addition to each user being granted as few privileges as necessary, each component of a\nsystem should only be granted the privileges that are necessary. Components that are\ncomplex should not have a large number of privileges. If necessary, complex components\nmay need to be split up into simpler components.\nIt may take some effort to determine what the least amount of privileges actually is, but\neven though it may be easier to assign more privileges than are necessary, being as precise\nas possible with the level of privileges that are necessary will minimize the attacker surface.\n", "page": 400, "type": "text", "section": "Page 400"}
{"text": "Security Considerations\nChapter 11\n[ 372 ]\nAvoiding security by obscurity\nSecurity by obscurity, also known as security through obscurity, is the belief that a software\nsystem is secure as long as internal details are kept hidden and vulnerabilities are not\nknown or are difficult to detect.\nSoftware architects should not promote or encourage security through obscurity because it\nis a practice we would like to avoid. While it can help to provide some level of security, it is\na weak security control. If it is used, it should be used in conjunction with other, stronger\nsecurity controls.\nKeep software designs simple\nSimplicity in the overall design of the system is important for security because a system\nthat software architects and developers have a difficult time understanding is one that may\nnot be secure. A more complex system makes it more difficult to reason about all of the\ndifferent threat possibilities. There is a greater likelihood of a mistake being made during\nimplementation, configuration, or the use of a software system when it is complex.\nSoftware systems that are more complicated tend to have a larger attack surface. As long as\nrequirements are being met, simple and elegant designs should be favored over complex\nones for a variety of reasons, one of which is security. As was mentioned in discussing the\nprinciple of least privilege, complex components may need to be re-designed in such a way\nto minimize complexity.\nSecure by default\nSecure by default, also known as security by default, is the concept of delivering your\nsoftware in a state that maximizes security out of the box, without requiring any changes. If\nthe software is configurable in a way that allows security to be reduced, that should be up\nto the user to change rather than be the default behavior. For example, if a software\napplication allows two-factor authentication (2FA) to be configured, it should be turned on\nby default.\n", "page": 401, "type": "text", "section": "Page 401"}
{"text": "Security Considerations\nChapter 11\n[ 373 ]\nDefault deny\nAuthorization within the application should follow a default deny approach whereby\npermissions must be granted as opposed to being denied. In other words, access should be\ndenied by default unless it has specifically been granted, as opposed to a user being\nprovided all access except for operations that have been specifically denied. This concept is\nrelated to, and complementary with, the idea of being secure by default.\nValidating input\nA number of software vulnerabilities can be avoided by being diligent about validating\ninput from any untrusted sources. Whether it is user input from a user interface, command-\nline arguments being passed into a program, environmental variables, or data from third\nparties, the software application should be wary of it and validate it accordingly.\nIn the case of data from a third party, that party may have security policies and standards\nthat differ from your own, so a software application should check the data that it receives\nfrom an external entity to ensure that it is valid.\nSecure the weakest link\nAs the old saying goes, a chain is only as strong as its weakest link. This concept can be applied\nto software systems as well. Security of a software system is only as secure as its weakest\ncomponent. Attackers will focus on the weakest component, so be sure that the weakest\npoint in the system is secure enough.\nSecurity must be usable\nLegitimate users who are using the software in the way that it was intended to be used\nshould only be impacted to the point that is required to make the system secure. The\nsecurity controls that are used for a software application cannot be so intrusive that they\ninterfere greatly with the usability of the application. If the security controls that are used\nare too annoying, users will seek to circumvent them.\n", "page": 402, "type": "text", "section": "Page 402"}
{"text": "Security Considerations\nChapter 11\n[ 374 ]\nA software application must be as secure as it needs to be but it should not be overly secure\nto the point where usability is ruined. If usability is affected too much, users will not want\nto use the software.\nWhen designing security controls, we should strive to do so in a user-friendly way. This\nincludes ensuring that the security mechanisms are easy to understand. Without \ncompromising security, the amount that users have to remember and the amount of effort\n(for example, the number of mouse clicks) that a user has to perform to abide by security\ncontrols should be minimized where it is possible to do so.\nFail securely\nFailures are bound to happen and a software application that handles security properly will\nfail securely. Software architects should design solutions that consider what should\nhappen when something fails and ensure that the software system and its data remain in a\nsecure state after the failure. Application code should be written in such a way that there is\nproper exception handling and the default behavior is to deny access when a failure occurs.\nCryptography\nCryptography is the study and practice of keeping information secret. It is used in\ninformation security to maintain the confidentiality and integrity of data as well as to\nenable non-repudiation. It allows for secure communication of information from\nunauthorized parties. In our look at cryptography, we will be examining encryption and\ncryptographic hash functions, which are tools that can be used to secure data.\nEncryption\nEncryption is the process of transforming ordinary data, which is referred to as plaintext,\ninto a format that is unreadable, which is referred to as ciphertext. This prevents\nunauthorized parties from accessing it. Data is encrypted using an encryption algorithm in\nconjunction with an encryption key. Larger key sizes result in greater encryption strength\nbut make the process of encryption/decryption slower.\n", "page": 403, "type": "text", "section": "Page 403"}
{"text": "Security Considerations\nChapter 11\n[ 375 ]\nData that is encrypted can be reverted to its original value. Encryption is used over hashing\nwhen there is a need to know the decrypted value. For example, the encryption is used to\nsend a secure message to someone, it must be possible for the recipient to decrypt the\nmessage or it will be useless. Decryption is the process by which data is decrypted so that\nan authorized party can read it again. The pair of algorithms used for\nencryption/decryption is called a cipher. There are two types of encryption:\nSymmetric (secret key)\nAsymmetric (public key)\nSymmetric (secret key) encryption\nSymmetric encryption, also known as secret key encryption, uses a single key for both\nencryption and decryption. Although it is generally faster than asymmetric\nencryption/decryption, the main drawback is that both parties must have access to the\nsecret key.\nAsymmetric (public key) encryption\nAsymmetric encryption, also known as public key encryption, uses two keys to encrypt and\ndecrypt data. One of the keys, called the public key, can be shared with everyone, while the\nother key, called the private key, is kept secret.\nBoth keys can be used to encrypt a message and the opposite key from the one used to\nencrypt a message can be used to decrypt it. For example, a public key can be used for\nencryption and the private key can be used for decryption. It is generally slower than\nsymmetric encryption/decryption.\nCryptographic hash functions\nA hash function is a function that returns a fixed output for a given input. The input can be\nany size but the output is of fixed size. The output of a hash function is commonly called\na hash, but it can also be referred to as a message digest, digest, hash value, or hash code. If\nthere will be no need to know the original value prior to hashing, then hashing should be\nfavored over encryption\n", "page": 404, "type": "text", "section": "Page 404"}
{"text": "Security Considerations\nChapter 11\n[ 376 ]\nSome examples of hash functions include MD5, SHA-256, and SHA-512. For example, the\nfollowing is the Secure Hashing Algorithm 256 (SHA-256) hash of the string This is a\nmessage:\na826c7e389ec9f379cafdc544d7e9a4395ff7bfb58917bbebee51b3d0b1c996a\nIn the case of SHA-256, no matter how long the input is, the hash will be a 256-bit (32-byte)\nhash value. This is useful because even if the input is very long (for example, the contents\nof a file), we know that the hash will be a fixed length. Unlike encryption, where the\noriginal value can be determined through decryption, hash functions are not reversible.\nHashes can be used for purposes such as comparing two files for equality without having\nto read all of the contents of both files, as a checksum for detecting errors during\ntransmission of data, finding similar records or substrings, and in data structures such as a\nhash table or a Bloom filter.\nA cryptographic hash function is a type of hash function that guarantees certain\nproperties, making it secure and suitable for cryptography. The combination of these\nproperties makes a hash function useful for cryptography. We can use cryptographic hash\nfunctions for things such digital signatures, HTTPS certificates, and in protocols such as\nSSL/TLS and SSH. Non-cryptographic hash functions are faster but provide weaker\nguarantees. The following are the main properties of a cryptographic hash function:\nQuick: Cryptographic hash functions are quick to generate a hash value for a\ngiven message. If a hash function is not fast, the performance of the processes\nthat use it may reach unacceptable levels for the given use cases.\nDeterministic: It is deterministic in that the same message will always produce\nthe same hash. It is this property that allows us to compare two hashes in order\nto determine if they represent the same original value, without knowing the\noriginal value.\nOne-way function: It is a one-way function in that it is infeasible to generate a\nmessage from a hash without trying out all possible messages (brute-force\nsearch). Please note that, by infeasible, we mean that although it is not\nimpossible, it is impracticable. \n", "page": 405, "type": "text", "section": "Page 405"}
{"text": "Security Considerations\nChapter 11\n[ 377 ]\nCollision resistant: It is collision resistant in that it is infeasible to find two\ndifferent messages with the same hash value. A collision takes place when two\ndifferent inputs result in the same hash. There should not be any collisions with a\nsecure hash function. Some hash functions, like MD5 and SHA-1, can result in\ncollisions and should not be used for cryptographic purposes.\nSmall changes result in vastly different hashes: A small change to a message\nshould yield a new hash that is significantly different from the old one, such that\nit is not possible to correlate the two hashes. For example, the first of the two\nhashes below is from the string Hello World while the second one is from the\nstring Hello Worlds. As you can see, even though the original strings are\nalmost identical, the hashes are very different:\na591a6d40bf420404a011733cfb7b190d62c65bf0bcda32b57b277d9ad9f146e\nb0f3fe9cdc1beeb7944d90e9b2e77b416fd097b5cc2c58838f8741e8129a1a52\nIdentity and access management (IAM)\nIdentity and access management (IAM) comprises policies and tools for managing digital\nidentities and controlling access to information and functionality. Two of the fundamental\nconcepts of IAM are authentication and authorization.\nAuthentication\nAuthentication is the process of determining if someone (or something) is who (or what)\nthey claim to be. It deals primarily with validating the identity of a subject. Examples of a\nsubject that may need to be authenticated include a user, a service, a computer, or an\napplication.\nIn the early days of software development, applications would commonly maintain their\nown user profiles for authentication, which would include some type of unique identifier\n(for example, a username or email address) and a password. Users provide their identifier\nand password and, if they match with the values the application has for a user profile, then\nthe user is considered to be authenticated.\n", "page": 406, "type": "text", "section": "Page 406"}
{"text": "Security Considerations\nChapter 11\n[ 378 ]\nWhat is multi-factor authentication (MFA)?\nMulti-factor authentication (MFA) adds an extra level of security. In multi-factor\nauthentication, a person has to present two or more authentication factors. The variation of\nmulti-factor authentication in which only two authentication factors must be presented is\ncalled two-factor authentication (2FA). The following are different types of authentication\nfactor:\nKnowledge factor: Something the person knows, such as a password or PIN\nPossession factor: Something the person has, such as a cell phone that can\nreceive a code or a company identification card that can be swiped\nInherence factor: Something the person is, using such as using a fingerprint\nscanner, palm reader, retina scanner, or some other type of biometric\nauthentication\nFor example, as part of authentication, a software system may require that a user not only\nsupply a password (a knowledge factor) but also enter a numeric code that is sent to the\nuser's cell phone. The cell phone has to be in the user's possession in order for them to\nreceive the code (a possession factor). In order for a hacker to break into an account, they\nwould need to steal not only a user's password but they must also get possession of that\nuser's phone.\nAuthorization\nAuthorization is the process of determining what a subject is permitted to do and what\nresources that subject is allowed to access. It involves granting rights to allow users or\nprograms to have access to a system or parts of a system. The user or program must first be\nauthenticated in order to determine whether they are who they claim to be. Once they are\nauthenticated, they can be authorized to access parts of a system.\nSoftware architects should consider the granularity of privileges. If privileges are too\ncoarse-grained, they may be too large and encompass too many rights. This may require\nprivileges to be granted more frequently and give recipients more access than is necessary.\nIn these types of cases, consider splitting up privileges into more fine-grained privileges to\nprovide greater access control.\n", "page": 407, "type": "text", "section": "Page 407"}
{"text": "Security Considerations\nChapter 11\n[ 379 ]\nStoring plaintext passwords\nAlthough rare today, some applications persist passwords in plaintext in a data store.\nObviously, storing passwords in plaintext is an anti-pattern because either an internal or\nexternal attacker could gain access to the database and all of the passwords would be\ncompromised.\nStoring encrypted passwords\nTo provide protection of passwords, some software applications encrypt the passwords.\nDuring registration, the password is encrypted prior to being stored. In order to\nauthenticate, the encrypted password is decrypted using the appropriate algorithm and a\nkey. The plaintext password entered by the user is then compared with the decrypted\npassword.\nHowever, because an encrypted value can be decrypted back to its original value, if an\nattacker can either intercept a decrypted password or obtain the details necessary to\ndecrypt a password, security will be compromised. If you need to store passwords,\nencryption is not the method that we want to use.\nStoring hashed passwords\nAs we learned in the Cryptographic hash functions section earlier in this chapter,\ncryptographic hash functions are one-way functions with no practical way of reversing the\nhash back to the original value. This characteristic makes them useful for password storage.\nIt is imperative though that you select a cryptographic hash function that is not broken\n(does not have any known collisions).\nAs part of user registration, the password is hashed. When a user logs on, they enter their\npassword in plaintext, which is hashed and compared with the stored hash value.\nHowever, hashing alone is not sufficient for storing passwords. A dictionary attack can be\nexecuted to guess a password by comparing it with a pre-compiled list. A table of pre-\ncalculated hashes and their original values, called a rainbow table, can be used for\ncomparison with a hash to determine the password.\n", "page": 408, "type": "text", "section": "Page 408"}
{"text": "Security Considerations\nChapter 11\n[ 380 ]\nIn order to slow down dictionary attacks and the use of rainbow tables to the point where\nthey are impractical, software applications should hash a combination of the password with\nsome piece of random data, known as a salt. A new salt should be randomly generated for\neach password and be of sufficient length (for example, a 64-bit salt). It is the combination\nof the salt and the password that is hashed. When a salted hash is used in conjunction with\na hashing function such as Argon2, scrypt, brcrypt, or PBKDF2, it makes it necessary for a\nrainbow table to be large enough that it is prohibitively difficult for attackers.\nWhen a new user is registered, the plaintext password is combined with the salt, hashed,\nand the hashed value is persisted. When a user logs in, the entered password is hashed\nwith the salt and the value is then compared with the persisted hashed value. This\napproach to managing identity and storing passwords as salted hashes is still in common\nuse today. However, many modern applications have taken the responsibility of\nauthentication and the storage of passwords away from applications and given them to a\ncentral identity provider.\nUsing domain authentication\nOnce enterprises started developing applications that lived in their own local networks, it\nmade sense to leverage domain authentication. Rather than have each application\nimplement authentication independently, the functionality was centralized. On Windows\nservers, the domain controller (DC), along with a directory service such as Active\nDirectory (AD), can manage resources and users for the entire domain. When users log on\nto a company network, they are authenticated in that domain and authorization can be\naccomplished using the attributes of the user profile. For intranet applications, this\napproach works well and is still popular.\nImplementing a centralized identity provider (IdP)\nMany modern applications have to interact with APIs that are not in the same domain and\nmay not be under its control. A web application, mobile application, or API may need to \ncommunicate with other applications and APIs outside its domain, requiring them to be\npublic. Domain authentication is not sufficient for this case. The ability to grant access to\nresources across applications without sharing login credentials is a common requirement\nand can be accomplished by implementing a centralized identity provider (IdP).\n", "page": 409, "type": "text", "section": "Page 409"}
{"text": "Security Considerations\nChapter 11\n[ 381 ]\nAnother advantage of an identity provider is that the applications that we build do not\nhave to be responsible for authentication. Instead, that task becomes the responsibility of\nthe identity provider. Functionality such as user registration, password policies, password\nchanges, and handling locked out accounts can be handled by an identity provider. Once\nimplemented, all of this functionality can then be reused across multiple applications. In\naddition to reusability, maintainability is improved because, if a change is required in any\npart of this functionality, it can be modified in a single location. For instance, we have\nalready seen how best practices for authentication and password storage have changed\nover time. These could continue to change, and by using a central identity provider we\nwould only be required to make modifications in one place.\nOAuth 2/OpenID Connect (OIDC)\nOAuth 2 is an open standard for authorization. It allows an application to be granted access\nto resources from another application and share its own resources with other applications.\nOpenID Connect (OICD) is an identity layer that sits on top of OAuth 2. It can be used to\nverify the identity of an end-user. Let us take a look at how OAuth 2 and OpenID Connect\nwork together to enable us to implement a centralized identity provider/authorization\nserver for the purpose of handling authentication and authorization.\nOAuth 2 roles\nOAuth 2 defines four roles:\nResource owner: Represents the person or application who owns the resource for\nwhich we need to control access\nResource server: The server that hosts the resources; for example, a resource\nserver may be an API that stores data that an application needs to access\nClient: The application that is requesting the resource\nAuthorization server: Server that authorizes the client application to have access\nto a resource\nIt should be noted that the resource server and the authorization server can be the same\nserver, but for larger applications they are commonly separate servers.\n", "page": 410, "type": "text", "section": "Page 410"}
{"text": "Security Considerations\nChapter 11\n[ 382 ]\nAuthenticating with the identity provider\nAuthentication performed by an authorization server along with OpenID Connect allows\nclients to verify the identities of users. The client application, which is referred to as the\nrelying party because it relies on the identity provider, requires a user's identity. \nA flow determines how identity and access tokens are returned to the client. There are\nvarious flows depending on the types of application that are communicating and how we\nwant that interaction to work. In one example, the client application (relying party)\nredirects to the authorization server, which serves as the identity provider. It sends an\nauthentication request to the authorization endpoint, as it is this endpoint that the client\napplication uses to obtain authentication and grant authorization to the client application.\nIf the user is authenticated, the identity provider redirects back to the client application\nusing a redirection endpoint to return an authorization code and an identity token. The\nidentity token can then be stored in either web storage (local storage) or a cookie. In line\nwith the OpenID Connect specification, an identity token is a JSON web token (JWT).\nJSON web token (JWT)\nA JWT is an open standard for representing claims between two parties. It is lightweight,\nmaking it efficient to transport. The following is an example of a JWT:\neyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJleGFtcGxlLmNvbSIsImp0aSI6Im\nMxZDA2YWQxLTRkMTUtNGY1Mi04YmMzLWMwZmVlODI1NDA5OSIsIm5hbWUiOiJKb2huIFNtaXRoI\niwiaWF0IjoxNTU1OTk4NjEwLCJleHAiOjE1NTg1OTA2MTB9.29WdHTGR5egA5_Q4N9WXtQHO-\nhJydVJou-YiQYQpkq8\nEach JWT has three parts:\nHeader\nPayload\nSignature\nThe three parts are concatenated together with each part separated by a dot (period). If you\nlook closely at the JWT above, you will find the three periods and therefore will be able to\ndetermine the three parts.\n", "page": 411, "type": "text", "section": "Page 411"}
{"text": "Security Considerations\nChapter 11\n[ 383 ]\nHeader\nThe header of a JSON web token typically has two pieces of information: the type of token\n(\"JWT\"), and the hashing algorithm being used (for example, HMAC SHA256). A sample\nheader looks like the following:\n{\n    \"typ\": \"JWT\",\n    \"alg\": \"HS256\"\n}\nThe header is Base64Url encoded prior to concatenating it with the other parts of the JWT.\nPayload\nThe payload of a JSON web token contains the claims. Claims are statements about the\nentity being authenticated (for example, the entity may be a user). There are three types of\nclaims:\nRegistered\nPublic\nPrivate\nRegistered claims are claims that have been predefined. They are not required but represent\ncommon claims that may be useful. Some examples of registered claims include:\nIssuer (iss): Issuer of the token\nSubject (sub): Subject of the token; claims are typically statements about the\nsubject\nAudience (aud): Intended recipients of the token\nJWT ID (jti): Unique identifier for the token\nIssued at (iat): Timestamp representing when the token was issued; it can be\nused to determine the age of the token\nExpiration time (exp): Timestamp representing the time on or after which the\ntoken should no longer be accepted\nPublic claims are those that we can create ourselves. They should be defined as a URI with a\nnamespace to avoid collisions. Private claims are custom claims that two parties agree to use\nin order to share information. Private claims are neither registered nor public.\n", "page": 412, "type": "text", "section": "Page 412"}
{"text": "Security Considerations\nChapter 11\n[ 384 ]\nThe following is a sample payload:\n{\n    \"iss\": \"example.com\",\n    \"jti\": \"c1d06ad1-4d15-4f52-8bc3-c0fee8254099\",\n    \"name\": \"John Smith\",\n    \"iat\": 1555998610,\n    \"exp\": 1558590610\n}\nLike the header, the payload is Base64Url encoded.\nSignature\nThe signature of a JSON web token ensures that the token was not altered at any point. If\nthe token is signed with a secret key, then the signature also verifies the sender of the token.\nThe signature is a hash that consists of the encoded header, the encoded payload, and the\nsecret key using the hashing algorithm specified in the header. The following is an example:\nHMACSHA256(\n  base64UrlEncode(header) + \".\" +\n  base64UrlEncode(payload),\n  secretKey\n)\nAuthorizing with the authorization server\nOnce a user is authenticated and the identity token and authorization code are returned, the\nclient application can send a token request to the token endpoint in order to receive an access\ntoken. The token request should include the client ID, the client secret, and the\nauthorization code.\nAn access token is then returned from the authorization server. Access tokens are not\nrequired to be JWTs but this standard is commonly used. Access tokens can be revoked,\nscoped, and time-limited, providing flexibility for authorization.\nThe application can then use the access token to request the resource from the resource\nserver on behalf of the user. The resource server validates the access token and responds\nwith the data.\n", "page": 413, "type": "text", "section": "Page 413"}
{"text": "Security Considerations\nChapter 11\n[ 385 ]\nMost common web application security risks\nThe Open Web Application Security Project (OWASP) is an online community focused on\nweb application security. They offer useful information, including documentation,\nmethodologies, and tools. I invite you to visit their website: https:/\u200b/\u200bwww.\u200bowasp.\u200borg.\nOne of the things that they produce every year is a document listing the top web\napplication security risks. In this section, we will take a look at some more recent risks.\nInjection\nThis security risk occurs when untrusted data is sent to an interpreter and unintended\ncommands are executed. This can cause unauthorized data to be accessed or manipulated.\nAnyone who can send untrusted data, including external and internal users, are possible\nthreat agents.\nA common form of injection is SQL injection (SQLi), where SQL statements are included\nin data (such as user input) and are then unknowingly executed against the database.\nAmong other things, a SQL injection attack could be used to retrieve, alter, or delete data.\nA web application firewall (WAF), which sits between users and the web application, can\nprotect software systems from some of the more common SQL injection attacks by using\ncommon signatures to identify SQL injection code. However, using a WAF is not sufficient,\nas it is impossible to identify all possible attacks. Validating untrusted data, using SQL\nparameters, and using the principle of least privilege are some techniques that can be used\nin conjunction with a WAF to prevent or lessen the effect of SQL injection attacks.\nBroken authentication\nA flaw in authentication and/or session management can compromise the security of a\nsoftware system. Attackers can find a flaw in authentication or session management\nmanually and then use automated tools to exploit it.\nSome of the topics we have explored in this chapter, such as hashing passwords with a salt,\nusing multi-factor authentication, and being secure by default by not deploying with\ndefault credentials, can help secure your system from authentication related attacks.\n", "page": 414, "type": "text", "section": "Page 414"}
{"text": "Security Considerations\nChapter 11\n[ 386 ]\nPassword policies should be put in place to enforce minimum password length and\ncomplexity requirements, as well as to ensure passwords are rotated periodically.\nApplications should always provide a logout feature and session timeouts should be short\nenough to prevent an attacker from gaining access simply by using a computer where the\nuser did not previously log out. Session IDs should not be exposed in the URL (for\nexample, URL rewriting), and session IDs should be rotated after a successful login.\nInformation such as passwords, tokens, session IDs, and other credentials should be sent\nover secure connections.\nSensitive data exposure\nThis security risk involves not properly protecting sensitive data such as social security\nnumbers, credit card numbers, credentials, and other important data. The first step is to\nidentify what data elements (or combinations of data elements) are sensitive.\nOnly store sensitive data if it is necessary and discard it as soon as possible. Data that isn't\nretained in any way cannot be stolen. Earlier in this chapter, we discussed the different\nstates of information. When sensitive data is at rest, it should be encrypted everywhere it is\nstored long-term, including backups of the data. When sensitive data is in transit, it should\nbe encrypted with secure protocols. Strong and up-to-date encryption algorithms should be\nused along with proper key management.\nAppropriate browser directives and headers should be set to protect sensitive data\nprovided by or sent to the browser. You should consider disabling the caching of responses\nthat contain sensitive data.\nXML external entity (XXE) attack\nAn XML external entity (XXE) attack is one that can take place against an application that\nparses XML input. When XML input contains a reference to an external entity and is then\nprocessed by an XML parser that has not been configured appropriately, the application is\nvulnerable to this attack.\nDenial of service (DoS), the disclosure of sensitive data, and Server-Side Request Forgery\n(SSRF) are all possible with an XXE attack. One type of DoS attack that is made possible\nwith XXE is called a billion laughs attack. Sometimes this type of attack is referred to as an\nXML bomb or an exponential entity expansion attack.\n", "page": 415, "type": "text", "section": "Page 415"}
{"text": "Security Considerations\nChapter 11\n[ 387 ]\nRegardless of the name, it works by defining ten entities, the first of which is simply\ndefined with a string. It is called a billion laughs attack because, in one common variation,\nthe string \"lol\" is used. The following is an example of an XML document for this type of\nattack:\n<?xml version=\"1.0\"?>\n<!DOCTYPE lolz [\n <!ENTITY lol \"lol\">\n <!ELEMENT lolz (#PCDATA)>\n <!ENTITY lol1 \"&lol;&lol;&lol;&lol;&lol;&lol;&lol;&lol;&lol;&lol;\">\n <!ENTITY lol2\n\"&lol1;&lol1;&lol1;&lol1;&lol1;&lol1;&lol1;&lol1;&lol1;&lol1;\">\n <!ENTITY lol3\n\"&lol2;&lol2;&lol2;&lol2;&lol2;&lol2;&lol2;&lol2;&lol2;&lol2;\">\n <!ENTITY lol4\n\"&lol3;&lol3;&lol3;&lol3;&lol3;&lol3;&lol3;&lol3;&lol3;&lol3;\">\n <!ENTITY lol5\n\"&lol4;&lol4;&lol4;&lol4;&lol4;&lol4;&lol4;&lol4;&lol4;&lol4;\">\n <!ENTITY lol6\n\"&lol5;&lol5;&lol5;&lol5;&lol5;&lol5;&lol5;&lol5;&lol5;&lol5;\">\n <!ENTITY lol7\n\"&lol6;&lol6;&lol6;&lol6;&lol6;&lol6;&lol6;&lol6;&lol6;&lol6;\">\n <!ENTITY lol8\n\"&lol7;&lol7;&lol7;&lol7;&lol7;&lol7;&lol7;&lol7;&lol7;&lol7;\">\n <!ENTITY lol9\n\"&lol8;&lol8;&lol8;&lol8;&lol8;&lol8;&lol8;&lol8;&lol8;&lol8;\">\n]>\n<lolz>&lol9;</lolz>\nEach of the other entities is defined as consisting of ten of the previous entity. The XML\ndocument then consists of a single instance of the largest of these entities. When an XML\nparser loads the document, it expands that entity, which results in all of the other entities\nbeing expanded, to the point where there are a billion copies of the first entity. As you can\nsee, the XML document is very small in size but the expansion takes up so much memory\nand time to parse that it causes a DoS.\nOne of the most effective ways of preventing this type of attack is to simply use a different\ndata format, such as JSON. If XML must be used, disabling document type\ndefinitions (DTDs) completely is also effective. If that is not possible, then external entities\nor entity expansion must be disabled. Each parser is different so you will have to research\nhow to go about making these configurations for the particular language/framework that\nyou are using in your application. Make sure to upgrade all XML processors and libraries\nbeing used by the application so as to include the latest security fixes.\n", "page": 416, "type": "text", "section": "Page 416"}
{"text": "Security Considerations\nChapter 11\n[ 388 ]\nBroken access control\nExploitation of missing or broken access control is a common security threat. Lack of access\ncontrol can be detected manually or, in some cases, by using automated tools. This can\nallow attackers to act with elevated privileges, which may allow them to retrieve, add,\nupdate, or delete data.\nApplications must verify security rights not just on the UI side but also on the server side.\nEven if the functionality is hidden in the UI from users who do not have proper access\nrights, attackers may attempt to alter the URL, application state, identity tokens, or access\ntokens, or forge requests, to gain access to unauthorized functionality.\nFrom the client side, development teams should ensure that the UI prevents the use of\nfunctionality by users who have not been granted access. On the server side, checks should\nbe in place to prevent unauthorized access. Access tokens should be invalidated at the\nappropriate times, such as when a user logs out of the system.\nThe deny by default approach that was discussed earlier should be used to remove access\ncontrol vulnerabilities or to minimize their effect. Access control failures should be logged\nand system administrators should be automatically notified when there are too many\nrepeated failures. Software development testing must include the testing of access control.\nSecurity misconfiguration\nMisconfiguration of a software application is a major threat to security. Software\napplications that are more complex have a greater chance of being misconfigured. The\napplication must be configured to be secure prior to deployment. This includes checking all\nsettings before going into production, as many default values are not secure.\nEverything that is unnecessary in a production environment should be disabled, removed,\nor simply not installed. Examples include accounts, privileges, ports, services, and\naccounts. Any default account passwords should be changed or the accounts should be\ndisabled.\nSome software applications use a number of tools and frameworks and they may not all be\nfully understood. It is critical that all components used within an application are configured\nproperly. The software's error handling should not divulge too much information to users.\nFor example, a detailed stack trace could be used by attackers for malicious purposes.\n", "page": 417, "type": "text", "section": "Page 417"}
{"text": "Security Considerations\nChapter 11\n[ 389 ]\nDevelopment teams need to be aware of the vulnerabilities of the components they are\nusing and keep up with the latest versions and patches to correct issues. A process must\nexist that will allow for keeping all software up to date, such as the operating system, the\nweb/app server, database management system, and development frameworks. All too\noften, a vulnerability in a component has been corrected by a third party in a patch, but the\npatch is not installed.\nCross-site scripting (XSS)\nCross-site scripting (XSS) vulnerabilities allow attackers to execute scripts in the browser.\nThe scripts might be designed to hijack a user's session, replace web site content, or redirect\nusers. It is a highly prevalent security flaw. There are three major types of XSS attack:\nReflected XSS\nStored XSS\nDOM XSS\nWith reflected XSS, an application or API takes untrusted data and sends it to the browser\nwithout proper validation or escaping. Stored XSS is possible when an application or API \nstores user input data that has not been properly validated or escaped, which is viewed at a\nlater time. DOM XSS is possible when data that is controlled by an attacker is included\ndynamically by a JavaScript framework, API, or other code.\nTests from the client side can ensure that there is validation to verify that all user-supplied\ninput is safe and that all user supplied input sent back to the browser is properly escaped\nbefore it is included in the output page. The context-sensitive escaping of untrusted HTTP\nrequest data and client-side document manipulation can prevent various types of XSS\nattack.\nInsecure deserialization\nDeserialization exploits are possible when an attacker modifies an object that an application\nor API subsequently deserializes. If any classes can change their behavior either during or\nafter deserialization, then remote code execution is possible, which can cause serious\ndamage. Data structures that contain access control related data may also be tampered\nwith, which could give an attacker unauthorized access or privileges.\n", "page": 418, "type": "text", "section": "Page 418"}
{"text": "Security Considerations\nChapter 11\n[ 390 ]\nTo be completely safe from this vulnerability, an application could simply not accept any\nserialized objects from untrusted sources. When that is not possible, integrity checks should\nbe made on serialized objects (for example, using a digital signature). Code that deserializes\nobjects should be isolated and only be granted low-level privileges as much as possible.\nDeserialization exceptions should be logged.\nUsing components with known vulnerable\ncomponents\nApplications may consist of a variety of components, including third-party libraries and\nframeworks. A vulnerability in any of these components can compromise the overall\nsecurity of the application.\nDevelopment teams must be aware of existing vulnerabilities in the components they are\nusing, keep up with the latest announcements of new vulnerabilities, and apply patches\nand/or new versions that will fix vulnerabilities. If there are any unused dependencies or\ncomponents, then they should be removed to reduce the possibility of exploits.\nWhen obtaining components and libraries, be sure to get them from official sources and\nover secure links, or you will be inviting the possibility that such code has already been\ncompromised. If your development team is using a component or library that is no longer\nbeing maintained, then any security flaws that exist are not being patched. You may need\nto consider moving to a different component or library. If that is not possible, then such\ncomponents and libraries should be monitored for possible issues. If one is found and the\nsource code is available, your development team may need to provide a patch.\nInsufficient logging and monitoring\nInsufficient logging and monitoring is an important security risk because it can help to\nenable so many other types of vulnerabilities. In order for attackers to be successful, they\nneed to go undetected for as long as possible. When logging and monitoring are\ninsufficient, then an attack can go unnoticed for a period of time.\nA software system needs the ability to answer some fundamental who/what/when\nquestions. Being able to associate user accounts with an event, reconstruct what happened\nbefore, during, and after an event, and know when different events occurred can all help\nyou become aware of a security vulnerability.\n", "page": 419, "type": "text", "section": "Page 419"}
{"text": "Security Considerations\nChapter 11\n[ 391 ]\nCentralized log management is crucial because log data must be easy to consume. A system\nshould be implemented that will send automatic alerts to the appropriate individuals based\non logging and monitoring data. Do not neglect the fact that the logs themselves need to be\nsecured. Unauthorized access to log data, wherever it is persisted, must be prevented in\norder to maintain the integrity of the log.\nUnvalidated redirects and forwards\nWeb applications may redirect users to other pages and websites. Attackers can use\nredirects to send users to malicious sites or use forwards to access unauthorized pages. If\npossible, try to avoid redirects and forwards. If your application uses redirects and\nforwards, testing of them should include:\nA review of the code should be conducted for all uses of redirect or forward. For\neach use, identify if the target URL is included in any parameter values. If so,\nverify that the parameter(s) are validated to contain only an allowed destination\nor element of a destination.\nSomeone on the team should spider the site to see if it generates any redirects\n(HTTP response codes 300-307, typically 302). Look at the parameters supplied\nprior to the redirect to see if they appear to be a target URL or a portion of such a\nURL. If so, change the URL target and observe whether the site redirects to the\nnew target.\nAll parameters in the code should be analyzed to see if they look like part of a\nredirect or forward URL so that those that are can be tested.\nYou should consider forcing all redirects to go through a page that notifies users that they\nare about to leave your site, with a link users can click on to confirm.\nSummary\nWe learned about the CIA triad and its goals of providing confidentiality, integrity, and\navailability. Security is about tradeoffs, so you should try to maintain a balance between\nthese goals. Software applications should be designed to be as secure as necessary, but\nrequirements for quality attributes such as usability and availability must be met as well.\nThere are no silver bullets when it comes to implementing security. However, there are\nproven principles and practices we can use to secure our applications and data. This\nchapter examined threat modeling and different techniques to create applications that are\nsecure by design.\n", "page": 420, "type": "text", "section": "Page 420"}
{"text": "Security Considerations\nChapter 11\n[ 392 ]\nWe learned about cryptography, including encryption and hashing, and IAM. Software\narchitects who work on web applications should keep up with the latest web application\nsecurity risks so that they can be aware of them and learn how to mitigate them.\nIn the next chapter, we will learn about documenting and reviewing software architectures.\nDocumentation of a software architecture is important so that the solution can be\ncommunicated to, and used by, others. The chapter will cover using the Unified Modeling\nLanguage (UML) to model the architecture and create architecture views. We will also\nexplore the process of reviewing software architectures, including several proven review\nmethods. Reviews of architectural decisions are important to ensure that the architecture\nwill satisfy its requirements, including its quality attribute scenarios.\n", "page": 421, "type": "text", "section": "Page 421"}
{"text": "12\nDocumenting and Reviewing\nSoftware Architectures\nAn important aspect of being a successful software architect is the ability to record and\ncommunicate your architecture to others. We will begin by exploring the reasons why we\ndocument a software architecture. You will then become familiar with architecture\ndescriptions (ADs) and the architecture views that are a part of them.\nThe chapter will provide an overview of the Unified Modeling Language (UML), which is\none of the more popular and widely used modeling languages. You will learn about some\nof the most common UML diagram types.\nAs parts of a software architecture design are completed, the development team and\nrelevant stakeholders need to review the architecture to determine whether it will satisfy\nthe functional requirements and quality attribute scenarios. We will detail several different\narchitecture review methods.\nIn this chapter, we will cover the following topics:\nUses of software architecture documentation\nCreating architecture descriptions (ADs), including architecture views\nOverview of the UML\nReviewing software architectures\n", "page": 422, "type": "text", "section": "Page 422"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 394 ]\nUses of software architecture\ndocumentation\nSome software projects skip software architecture documentation altogether or do it merely\nas an afterthought. Some of the projects that complete architecture documentation only do\nso because it is required. For example, it may be required by an organization's process or\nthere may be a contractual obligation to provide it to a client.\nHowever, good software architects understand the value of documenting their software\narchitecture. Good documentation serves to communicate the architecture to others, assist\nthe development team, educate team members, facilitate architecture reviews, allow for the\nreuse of architectural knowledge, and help the software architect.\nCommunicating your architecture to others\nA good software architecture is only useful if it can be communicated to others. If it cannot\nbe communicated to others effectively, then those who need to build, use, and modify the\narchitecture may not be able to do so, at least not in the way that the architect intended.\nDuring software architecture design, we identify all of the structures that make up the\narchitecture and how they interact with each other. Documentation allows us to\ncommunicate those structures and interactions. The artifacts that are created when an\narchitecture is documented focus on communicating the solution (architecture) to various\naudiences, which can be made up of both technical and non-technical people.\nSoftware architects will need to communicate their architecture to the development team,\nmanagement, and other stakeholders. Different stakeholders will have different reasons\nand priorities behind wanting to learn about the architecture, but a software architecture is\nabstract enough that a variety of people can use it to reason about a software system.\nDifferent types of architecture view can help to communicate the architecture to everyone\nwho needs to understand it.\n", "page": 423, "type": "text", "section": "Page 423"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 395 ]\nAssisting the development team\nDocumentation is useful to the team during the design and development of the software\nsystem. Learning about the structures and elements of the architecture and how they\ninteract with each other will allow developers to understand how they should be\nimplementing their functionality. By seeing the interfaces available, developers will gain an\nunderstanding as to what needs to be implemented and what is available to use in\ncompleted implementations. The documentation enables developers to complete their work\nin a way that abides by the design decisions that have been made for the architecture.\nSoftware architecture also restricts some of the design choices available to developers and\nputs constraints on implementation, reducing the complexity of the software system.\nArchitecture documentation communicates design decisions, which helps to prevent\ndevelopers from making the wrong decisions about how a piece of functionality should be\nimplemented.\nEducates team members\nSoftware architecture documentation is also beneficial in educating the developers on the\nteam. Developers who are unfamiliar with the system, either because it is a new one that\nhas just begun its design process or because they have joined an existing project, can use it\nas a guide to become familiar with the architecture. It is useful for software developers to\nunderstand the design decisions that shape the architecture they are using.\nAs a software architecture changes, the documentation should be updated as well. Good\ndocumentation will help to communicate any changes to the development team so that they\ncan be aware of them.\nProviding input for software architecture reviews\nSoftware architectures are reviewed to ensure that they have the capability to meet\nrequirements, including quality attribute scenarios. Architecture documentation is useful\nduring this process because it contains details that will allow a review team to analyze an\narchitecture and make these types of determination.\nThe documentation can also be used to evaluate and compare alternative software\narchitectures. Architecture documentation helps those who are tasked with comparing\narchitectures to accomplish their work by providing the necessary details to make informed\nand accurate evaluations.\n", "page": 424, "type": "text", "section": "Page 424"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 396 ]\nAllowing for the reuse of architectural knowledge\nSoftware architecture documentation allows architectural knowledge to be reused for other\nprojects. The design decisions made, the design rationale that formed the decisions, and\nany lessons learned can be leveraged when other software systems need to be created or\nmaintained.\nReuse allows organizations to be more efficient and productive with their software\ndevelopment. If an organization is developing a software product line, which consists of\nmultiple products from the same company to address a particular market, the software\nproducts may have some similar functional and non-functional requirements and may\nshare a similar look and feel in terms of the user interface. Parts of an architecture made for\none software product may be useful for one or more of the other ones. Software architecture\ndocumentation can facilitate the reuse of an architecture.\nHelp the software architect\nDocumenting a software architecture helps the software architect. Software architects will\nbe asked plenty of questions about the architecture by a variety of stakeholders and the \ndocumentation can help to answer them. It supports software architects to fulfill some of\ntheir responsibilities by providing artifacts with which to communicate the architecture to\nothers, assist the development team, educate team members, review the software\narchitecture, and pass on architectural knowledge for reuse.\nSome projects are rather complex and it can be difficult to remember all of the structures\nthat make up the architecture and how they interact with each other, even for the software\narchitect who was directly involved. The documentation can help to remind a software\narchitect of this information if it has to be revisited months or even years later.  \nA software architect may be trying to get a project off the ground or to gain funding for\ntheir project. Solid documentation can help the software architect achieve these goals by\nproviding information to stakeholders. Software architects can use some of the completed\ndocumentation when they have presentations to give about the software.\nIf a software architect leaves the project or the organization, the documentation that is left\nbehind can answer questions when the software architect is no longer available.\n", "page": 425, "type": "text", "section": "Page 425"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 397 ]\nCreating architecture descriptions (ADs)\nAn architecture description (AD) is a work product used to express and communicate an\narchitecture. The actual architecture of a software system is separate from the artifacts that\ndescribe and document it, such that we can create different artifacts for a given architecture.\nADs identify the stakeholders of the software system and their concerns. Stakeholders\ninclude people such as users, administrators, domain experts, business analysts, product\nowners, management, and the development team.\nEach of these stakeholders has various concerns, which are either unique to them or shared\nwith other stakeholders. Examples of system concerns related to architecture include the\ngoals of the system, the suitability of the architecture to accomplish those goals, the ability\nof the architecture to meet quality attribute scenarios, the ease with which the software\nsystem can be developed and maintained with the architecture, and any risks to the\nstakeholders. ADs consist of one or more architecture views, with the views addressing the\ndifferent concerns of the stakeholders.\nSoftware architecture views\nMany software systems are complex, making them difficult to understand. This is\nparticularly true when one attempts to look at the whole system at once. Architecture\nviews are used to ease understanding, as each one focuses on a specific structure or\nstructures of the architecture. This allows a software architect to document and\ncommunicate only a small piece of the architecture at a time. Having multiple views\nrepresent an architecture allows the software architect to communicate it in a manageable\nway.\nDeciding which views to create depends on the goals of the documentation and the\naudience. In the Uses of software architecture documentation section earlier in this chapter, we\ncovered some of the reasons that documentation is useful. Different views will focus on\ndifferent aspects of the architecture, so the intended usage of the architecture\ndocumentation will dictate the types of view that you create.\nThere is no definitive list of views that must be created. The same goes for deciding how\nmany views to create. Each software project is different, so there is no set number of views\nthat are required for all of them. The number of artifacts that need to be created for a\nsoftware architecture is the amount necessary to effectively communicate your architecture\nto the different audiences that are interested.\n", "page": 426, "type": "text", "section": "Page 426"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 398 ]\nThere is a cost associated with creating and maintaining a view, so we only want to\nintroduce a view if it is needed and will provide a benefit. We never want the\ndocumentation to be insufficient but we also do not want to spend inordinate amounts of\ntime creating too much of it.\nDuring software architecture design, informal documentation, in the form of sketches,\nshould be made to record design decisions. These sketches may include items such as the\nstructures, elements, relationships between elements, and architecture patterns used. These\nsketches are important to document the work that was done during design. While they are\nnot complete enough to be released as the final documentation, they can be used as the\nbasis for the architecture views once it comes time to formally document the architecture.\nSoftware architecture notations\nThere are different types of notation that can be used with software architecture views.\nThey help software architects to communicate their design and some of them can be used\nby tools for code generation.\nNotations differ from each other predominately based on their level of formality. Notations\nthat are more formal typically require more effort, not just in creating the artifacts but also\nin understanding them. However, they can provide more detail and reduce ambiguities\nthat may exist with less formal notations.\nThere are many different notations. Each one is good for something but none of them are\ngood for everything. When deciding which notation to use for all or some of your views,\nconsider the purpose of the diagrams, the stakeholders that will be examining them, your\nfamiliarity with the notation, and the tools that are available to create them. The three main\ntypes of software architecture notation are:\nInformal\nSemiformal\nFormal\nInformal software architecture notations\nInformal notations are views of the architecture that are often created with general-purpose\ntools, using whatever conventions the team deems appropriate. Using this type of notation\nhelps communicate the software architecture to customers and other stakeholders who may\nnot be as technical or have the need for more formalized approaches. Software architects\nmay consider using this notation for artifacts created for the project's management team.\nThey can help project management to understand the scope of the project.\n", "page": 427, "type": "text", "section": "Page 427"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 399 ]\nNatural language is used with the notation, so these types of artifacts cannot be formally\nanalyzed and tools will not be able to automatically generate code from the artifacts.\nSemiformal software architecture notations\nA semiformal notation in views is a standardized notation that can be used in diagrams.\nHowever, fully-defined semantics are not part of a semiformal notation. Unlike informal\nnotations, this type of notation allows for some level of analysis and can potentially be used\nto automatically generate code from the models.\nUML is an example of a semiformal notation that is very popular for modeling. There are\nsome notations that can be used with UML that extend it to provide more robust semantics,\nmaking UML more formalized.\nFormal software architecture notations\nViews of the architecture that use formal notations have precise semantics, usually\nmathematically based. This allows for formal analysis of both the syntax and the semantics.\nTools can use artifacts created with a formal notation for automated code generation.\nSome software projects choose to not use a formal notation because they require more effort\nto use, they require certain skills on the part of those creating the architecture views, and\nstakeholders may find them difficult to understand. Formal notations are not as useful for\ncommunication with non-technical stakeholders.\nA number of formal notations exist, including various architecture description languages\n(ADLs). An ADL is a formal type of expression that is used to represent and model a \nsoftware architecture. Architecture Analysis and Design Language (AADL) and Systems\nModeling Language (SysML) are two examples of ADLs. AADL was originally created to\nmodel both hardware and software. It can be used to describe a software architecture,\ncreate documentation, and generate code.\nThe book Documenting Software Architectures \u2013 Views and Beyond describes AADL as follows:\n\"The AADL standard defines a textual and graphical language to represent the runtime\narchitecture of software systems as a component-based model in terms of tasks and their\ninteractions, the hardware platform the system executes on, possibly in a distributed\nfashion, and the physical environment it interfaces with, such as a plane, car, medical\ndevice, robot, satellite, or collections of such systems. This core language includes\nproperties concerning timing, resource consumption in terms of processors, memory,\nnetwork, deployment alternatives of software on different hardware platforms, and\ntraceability to the application source code.\"\n", "page": 428, "type": "text", "section": "Page 428"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 400 ]\nSysML is a general-purpose, graphical modeling language for systems. It is a standard that\nis maintained by the Object Management Group (OMG). It can be used for a number of\nactivities, including specification, analysis, design, and verification.\nSysML is a subset of UML and reuses some of the same diagram types. We will discuss\nUML in more detail shortly but here is a list of the SysML diagram types that are reused\nfrom UML without any modification:\nUse case diagrams\nSequence diagrams\nState diagrams\nPackage diagrams\nThe following diagrams have been modified in SysML from their UML counterparts:\nActivity diagrams\nBlock definition diagrams\nInternal block diagrams\nSysML also introduces some new diagram types that do not exist in UML:\nRequirement diagrams\nParametric diagrams\nIncluding design rationales\nAn architecture description should include the design rationale behind the design\ndecisions being documented. In Chapter 5, Designing Software Architectures, we discussed\nthe design rationale, which is an explanation that contains the reasons and justification for\ndesign decisions related to the architecture. Without documenting the design rationale, the\nreasons that a design decision was made will not be known. Recording the design rationale\nis beneficial even for those who are involved in a design decision, as the details of a\ndecision can be forgotten over time.\nIt is not necessary (or practical) to record every design decision that is made but any\ndecisions that are important to the architecture are candidates to be documented. When\ndocumenting a design rationale, keep in mind that in addition to including design\ndecisions, it is sometimes useful to include details on why alternative approaches were not\ntaken and why certain design decisions were not made at all.\n", "page": 429, "type": "text", "section": "Page 429"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 401 ]\nOverview of the Unified Modeling Language\n(UML)\nThe Unified Modeling Language (UML) is a general-purpose, standardized modeling\nlanguage. It is widely used and understood, making it a popular choice for modeling a \nsoftware architecture. While this section is not intended as an exhaustive tutorial on UML,\nwe will cover some of the most popular UML diagrams and their purpose. If you are\nalready familiar with UML or prefer to use a different modeling language, feel free to skip\nthis section.\nTypes of modeling\nIn UML, there are two main types of modeling: structural modeling and behavioral modeling.\nStructural modeling focuses on the static structure of the system, its parts, and how they are\nrelated to each other. They do not show details about the dynamic behavior of a system.\nSome of the structure diagrams in UML include:\nClass diagrams\nComponent diagrams\nPackage diagrams\nDeployment diagrams\nBehavioral modeling shows the dynamic behavior of the components in a system. Unlike\nthe static nature of structure diagrams, behavior diagrams describe changes to the system\nover time. Some of the behavior diagrams in UML include:\nUse case diagrams\nSequence diagrams\nActivity diagrams\nClass diagrams\nClasses are templates (blueprints) for creating objects in a software system. They include\nattributes (member variables) that hold the state of an object and operations (methods) that\nrepresent behavior. Class diagrams, which are among the most popular of UML diagrams,\nshow us the structure of a software system by allowing us to see the classes and their\nrelationships. A number of team members may find class diagrams useful, including the\nsoftware architect, developers, QA personnel, operations engineers, product owners, and\nbusiness analysts.\n", "page": 430, "type": "text", "section": "Page 430"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 402 ]\nA rectangle is used in a class diagram to graphically represent a class and each one can\nhave up to three sections in it. The upper section shows the name of the class, the middle\nsection contains the attributes of the class, and the bottom section details the operations.\nVisibility\nVisibility dictates the accessibility of a member (attribute or operation) and can be\ndesignated by placing a notation before the member's name. In general, you want to give\nonly as much accessibility as is needed. The following table details the most common\nvisibility notations:\nNotation Visibility\nDescription\n+\nPublic\nMember is accessible by other types.\n#\nProtected Member is accessible within the same type as well as types that inherit from it.\n~\nPackage\nMember is accessible from any type within the same package. It is not accessible from\noutside the package, even if it is an inheriting class.\n-\nPrivate\nMember is accessible only within the type that declares it.\nFor example, the following diagram shows the Order class, which has two private\nattributes, OrderId and OrderDate, as well as two public operations, CalculateTax and\nCalculateTotal:\nAssociation\nAn association is a broad term that refers to a semantic relationship between classes. If one\nclass uses another class (unidirectional), or two classes use each other (bidirectional), they\nhave a relationship. Relationships between classes are represented by an association, which\nis shown on a class diagram as a solid line:\n", "page": 431, "type": "text", "section": "Page 431"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 403 ]\nWhen there are no arrowheads at the end of the line, the navigability of the association is\nunspecified. However, in diagrams where arrows are used for one-way navigable\nassociations, a line with no arrows is assumed to represent bidirectional navigability. In the\npreceding example, which would mean that both Student and Instructor are accessible\nfrom each other. Alternatively, a bidirectional association can be depicted by having an\nopen arrowhead at the end of both lines.\nIf we want to model unidirectional navigability, an open arrowhead can be used. In the\nfollowing diagram, class B is navigable from class A:\nAggregation and composition are subsets of association and are used to represent specific\ntypes of association.\nAggregation\nAggregation is a relationship in which a child object can exist independently of the parent.\nIt is graphically represented by a hollow diamond shape. For example, in a domain with a\nTire object, we can say that, even though a Car object has Tire objects, a Tire object can exist\nwithout a Car object:\n", "page": 432, "type": "text", "section": "Page 432"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 404 ]\nComposition\nComposition is a relationship in which an object cannot exist independently of another\nobject. It is graphically represented by a filled diamond shape. For example, in a domain\nwith a Room object, we might say that a Room object cannot exist without a Building\nobject:\nMultiplicity\nMultiplicity allows you to define the cardinality of a relationship between classes. The\nmultiplicity of a relationship describes the number of objects that can participate in it. The\nfollowing table shows the different types of multiplicity that can be specified:\nNotation\nMultiplicity\n0..1\nZero or one\n1\nOne and only one\n1..1\nOne and only one\n0..*\nZero or more\n*\nZero or more\n1..*\nOne or more\nFor example, the following diagram depicts that each Student is taught by one or more\ninstructors and that each Instructor teaches one or more students:\n", "page": 433, "type": "text", "section": "Page 433"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 405 ]\nDependency\nA dependency is a type of relationship between UML elements, such as classes, in which\none element requires, needs, or depends on another element. The dependency is sometimes\nreferred to as a supplier/client relationship because the supplier provides something to the\nclient. The client is either semantically or structurally dependent on the supplier. A\ndependency may mean that changes to a supplier may require changes to a client.\nIn an association, one class may have a reference to the other as a member variable. A\ndependency relationship is slightly weaker. For example, a dependency may exist because\na return type or parameter for a method in one class references another class:\nA dependency is graphically represented by a dashed line with an open arrowhead. In the \npreceding example, the FileImport class (client) depends on the StreamReader class\n(supplier). There can be many dependencies on a single diagram, so you may not want to\nshow every dependency. However, you should show that are important to what you are\ntrying to communicate in a particular diagram.\nGeneralization/specialization\nGeneralization is the process of abstracting common attributes and operations into a base\nclass. The base class is sometimes referred to as the superclass, base type, or parent class.\nGeneralization is also known as inheritance. The base class contains general attributes,\noperations, and associations that are shared with all of its subclasses. Generalization is\ngraphically represented with a hollow triangle on the part of the connecting line that is\nclosest to the base class.\nSpecialization is the converse of generalization in that it involves creating subclasses from\nan existing class. Subclasses are sometimes referred to as a derived class, derived type,\ninheriting class, inheriting type, or child class.\n", "page": 434, "type": "text", "section": "Page 434"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 406 ]\nFor example, our domain may have different types of account, such as a checking account\nand a savings account. These classes may share some of the same properties and behaviors.\nRather than repeating what is shared in each of these account classes, our model may have\nan Account base class, which contains the generalized attributes and operations that are\ncommon to all account classes:\nThe CheckingAccount and SavingsAccount classes, which are subclasses, inherit from the\nAccount class and demonstrate an is a relationship. CheckingAccount is an Account, just as\nSavingsAccount is an Account. CheckingAccount and SavingsAccount are specializations\nof Account.\nDepending on the programming language used for implementation, it may be possible to\nallow some attributes or operations to be overridden in subclasses. Subclasses can also\nintroduce their own specialized attributes, operations, and associations that are specific to\ntheir class.\nRealization\nRealization denotes a relationship in which one element realizes or implements the behavior\nthat another element specifies. A common example of this is when a class implements an\ninterface. Realization is graphically represented with a hollow triangle at the end of a\ndashed line, with the hollow triangle appearing closest to the element that is specifying the\nbehavior.\n", "page": 435, "type": "text", "section": "Page 435"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 407 ]\nIn the preceding diagram, you may have noticed that ILogger is designated as an interface.\nThis designation is done through a stereotype. Stereotypes are one of the extensibility\nmechanisms available in UML, which allow you to extend vocabulary and introduce new\nelements. In this case, a stereotype has been used to indicate that ILogger is an interface.\nStereotypes are graphically represented by enclosing the name in guillemets (angle quotes).\nThey are similar to the symbols for less than and greater than, which can be used if\nguillemets are unavailable.\nComponent diagrams\nComponent diagrams detail the structural relationship between components of a system.\nThese diagrams are typically needed with complex software systems that consist of many\ncomponents, as it is helpful to view the components and their relationships. They\nessentially depict how the components of a software system are wired together, which is\nwhy they are sometimes referred to as wiring diagrams.\nComponent diagrams help us to identify the interfaces between different components of\nour software system. Components communicate with each other through interfaces, and\ncomponent diagrams allow us to see system behavior as it relates to an interface. Interfaces\ndefine a contract by defining the methods and properties that are required for\nimplementations. Implementations can be changed as long as the classes that are dependent\non them are coded for interfaces and not for specific implementations.\n", "page": 436, "type": "text", "section": "Page 436"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 408 ]\nBy identifying interfaces, we are able to identify the replaceable parts of our software\nsystem. Having this knowledge gives us the ability to know where we can potentially reuse\na component that the organization has already created or where a third-party component\ncould be used. Components that we create for a software system may also be leveraged in\nother software applications that the organization has in development or will develop in the\nfuture.\nKnowing the components of a software system also makes it easier for project decision\nmakers to divide up the work. Once the interface is agreed upon, one or more developers\non the team, or even a separate team, can work independently of others in developing a\ncomponent.\nIn a component diagram, components are graphically represented as a rectangle with a\ncomponent symbol (a rectangular block with two smaller rectangles on the left side). For\nexample, an Order component would look like the following:\nAlternatively, the component stereotype can be used, either in addition to the component\nsymbol or in place of it, to designate that an object on the diagram is a component.\nInterfaces that a component provides are graphically represented by a small circle at the end\nof a line, which is also sometimes referred to as the lollipop symbol. Interfaces that a\ncomponent requires are represented by a half circle at the end of a line, which is also\nreferred to as a socket.\nFor example, let's say that our Order component implements the IOrder interface and\nrequires an implementation of ICustomer. The Customer component implements the\nICustomer interface:\nKeep in mind that components can contain other components. For example, we could\nmodel an Order system component that contains, within it, the Order, Customer, and other\ncomponents, along with all of their relationships.\n", "page": 437, "type": "text", "section": "Page 437"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 409 ]\nPackage diagrams\nIn UML, packages logically group elements together and provide a namespace for the\ngroups. Many UML elements, such as classes, use cases, and components, can be grouped\ntogether in packages. Packages can also contain other packages. Package diagrams are used\nto show the dependencies between packages in a software system.\nThe more complex a software system is, the more difficult it can be to understand all of the\nmodels. Package diagrams make it easier for people to reason about large, complex systems\nby grouping elements together and allowing us to see the dependencies.\nIn addition to modeling standard dependencies, we can model package import and package\nmerge types of dependencies. A package import is a relationship between an importing\nnamespace and an imported package. This can allow us to reference package members\nfrom other namespaces without fully qualifying them. A package merge is a relationship\nbetween two packages in which one package is extended by the contents of another\npackage. The two packages are essentially combined.\nA package is graphically represented in UML by a symbol that looks like a file folder with a\nname. The following is an example of a high-level package diagram for a layered\napplication:\nBy looking at it, we can see that the Presentation Layer depends on the Business Layer, the\nBusiness Layer depends on the Data Layer, and all three layers depend on the Cross-\nCutting package.\n", "page": 438, "type": "text", "section": "Page 438"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 410 ]\nDeployment diagrams\nDeployment diagrams represent the physical deployment of artifacts on nodes. An artifact\nis a physical piece of information, such as a source code file, binary file, script, table in a\ndatabase, or document.\nA node is a computational resource that artifacts are deployed on for execution. Nodes can\ncontain other nodes. There are two types of node: device nodes and execution environment\nnodes (EENs).\nDevice: A device represents a physical computational resource (hardware) that\ncan execute a program. Examples include a server, laptop, tablet, or mobile\nphone. Devices may consist of other devices.\nExecution environment: An execution environment is a software container that\nresides in a device. It provides an execution environment for artifacts that are\ndeployed on it. Examples include an operating system, a JVM, or a Docker\ncontainer. Execution environments can be nested.\nDeployment diagrams are used to show the software elements in an architecture and how\nthey will be deployed to hardware elements. They provide a view of the hardware and the\nsystem's topology.\nIn a deployment diagram, nodes are graphically represented by a three-dimensional box. In\nthe following example, there are three nodes. One represents the Azure (App Service), one\nis Desktop Device, and one is Mobile Device. Notice that a <<device>> stereotype is used\nto indicate that a node is a device. The various nodes in the example contain components\nand the lines show associations between the nodes:\n", "page": 439, "type": "text", "section": "Page 439"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 411 ]\nUse case diagrams\nUse cases are text that describes a software system's behavior as it responds to requests from\nsystem users, known as actors. An actor is the role for someone or something that interacts\nwith the system and can be a person, organization, or an external system.\nJust like with classes in a class diagram, generalizations can be done on actors. Actor\ngeneralization is a relationship between actors in which one actor (descendant) inherits the\nrole and properties from another actor (ancestor).\nFor example, if our domain had different types of managers, such as an HR Manager and a\nCustomer Service Manager, they may both inherit from a Manager ancestor actor:\nActor generalization is graphically represented in the same way that generalization is with\nclasses. It is done with a hollow triangle on the part of the connecting line that is closest to\nthe ancestor actor.\nUse cases are something that the system does or something that happens to the system. Use\ncases should be easy to read and are usually brief. Actors have goals and use cases describe\nways to carry out those goals by using the software system.\n", "page": 440, "type": "text", "section": "Page 440"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 412 ]\nA use case diagram is a graphic representation of a use case, the relevant actors, and their\nrelationships. It details the actors and how they interact with the software system. Use case\ndiagrams allow people to understand the scope of the software system and the\nfunctionality that will be provided to actors. They can be useful for traceability in that we\ncan verify that a software system is meeting its functional requirements.\nIn a use case diagram, actors are typically represented by a stick figure with the name of the\nactor's role appearing underneath it. Use cases are graphically represented with a\nhorizontally shaped oval. The name of the use case appears inside the oval:\nLines are used to show associations between actors and use cases. Use case diagrams can\ndescribe context by showing the system's scope. A system boundary box can be used to\npresent what is part of the system and what is external to it:\n", "page": 441, "type": "text", "section": "Page 441"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 413 ]\nIn the preceding diagram, there are three actors, all of which are external to the Online\nOrder System. The Customer actor is a person but the Identity Provider and Payment\nProcessor actors are external systems.\nThere are four use cases shown in this simplified example. The Customer is associated with\nall of them but the Identity Provider is only involved with the Login use case and the\nPayment Processor is only associated with the Checkout use case.\nSequence diagrams\nSequence diagrams model how components in a software system interact and\ncommunicate. They are one type of interaction diagram, which is a subset of behavior\ndiagrams. Other interaction diagrams include the communication diagram, timing\ndiagram, and interaction overview diagram.\nSequence diagrams describe a sequence of events from the software system. The following\nexample shows the flow of logic within a system for price calculation:\nSequence diagrams are sometimes referred to as event diagrams or event scenarios. They\ncan be used to see how components interact with each other and in what order they do so.\nSome examples of what you might want to model using a sequence diagram include usage\nscenarios, service logic, and method logic.\n", "page": 442, "type": "text", "section": "Page 442"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 414 ]\nLifeline\nIn sequence diagrams, an object is graphically represented by a rectangle with its lifeline\ndescending from the center of its bottom:\nThe lifeline shows the lifespan of the object and is represented by a vertical dashed line. The\npassage of time starts at the top of the line and goes downward. The rectangle can show\nboth the name of the object and the class, separated by a colon, as follows:\nobjectname : classname\nThe object and class names are underlined. A lifeline can represent an object or a class. An\nobject can be left unnamed if we are modeling a class or in cases where the object's name is\nunimportant. In that situation, you will simply see a colon followed by the class's name.\nSome people prefer to just see the class's name with no colon. If you are modeling objects\nand want to differentiate between different objects of the same class, you should specify an\nobject name. When diagramming objects/classes, the attributes and operations of an object\nare not listed.\nActivation boxes\nActivation boxes on the lifelines show when an object is completing a task. For example,\nwhen a message is sent to an object, the time period from when the message is received\nuntil the response is returned can be represented with an activation box. Since activation\nboxes are on the lifelines, they also represent time. The longer the activation box, the longer\nthe task will take to complete.\nMessages\nArrows are used to graphically represent messages that are passed between objects. The\ntype of arrow indicates the type of message that is being passed: \n", "page": 443, "type": "text", "section": "Page 443"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 415 ]\nSynchronous messages are shown as a solid line with a solid arrowhead. A synchronous\nmessage is one in which the sender must wait for a response before it can continue.\nAsynchronous messages are represented by a solid line with a lined arrowhead. With an\nasynchronous message, the sender does not have to wait for a response before continuing.\nReply/Return messages and asynchronous return messages are both represented by a\ndashed line with a lined arrowhead.\nLoops\nIn order to model a loop in a sequence diagram, a box is placed over the part of the diagram\nthat is iterating through a loop. An inverted tab at the top-left corner of the box is labeled\nwith the word loop to signify that the structured control flow is a loop. The subject that is\nbeing iterated over is commonly labeled with a guard message that is placed below the\ninverted tab. In the following example, the logic is iterating over each line item in an order:\n", "page": 444, "type": "text", "section": "Page 444"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 416 ]\nOptional \ufb02ows\nIn a sequence diagram, you may need to model optional flows. These represent logic that\nwill optionally be executed based on some condition. Similar to loops, an optional control\nflow is graphically represented with a box that is placed over the part of the diagram that is\nrelated to the optional flow. An inverted tab at the top-left corner of the box is labeled with\nopt to denote that it is an optional flow.\nThe condition for the optional flow can be labeled by using a guard message that is placed\nbelow the inverted tab. In the following diagram, an optional flow is executed only if a\nmember is a platinum member:\nAlternative \ufb02ows\nWhen you want to model alternative (conditional) fragments in a sequence diagram, a box\ncan be placed over the part of the diagram that captures the alternatives. An inverted tab at\nthe top-left corner of the box is labeled with alt to denote that it is an alternative fragment.\nAlternative flows are similar to optional ones so the two should not be confused with each\nother. While an optional flow checks a single condition and may or may not execute a\nfragment, alternative flows offer multiple possibilities. Only the alternative fragment whose\ncondition is true will execute.\n", "page": 445, "type": "text", "section": "Page 445"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 417 ]\nA guard message can be placed at the start of each alternative to describe the condition and\na dotted line is used to separate each alternative. In the following diagram, there are two \nalternatives, one for platinum members and one for standard members:\nActivity diagrams\nAn activity diagram allows us to visually represent a series of actions in the form of a\nworkflow. It shows a control flow and is similar to a flowchart. Activity diagrams can be\nused to model things such as a business process, flow within a use case, and procedural\nlogic.\n", "page": 446, "type": "text", "section": "Page 446"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 418 ]\nThe following diagram shows the workflow for creating a new membership card:\nThe activities in an activity diagram can either be sequential or concurrent. An activity is\nshown as a rectangle with rounded corners. The rectangle encloses all of the elements of an\nactivity, such as its actions and control flows.\nStart/end nodes\nSome of the nodes that can appear in an activity diagram represent the different ways in\nwhich flows can begin and end:\nAn activity diagram begins with an initial state, or start point, which is graphically\nrepresented by a small, solid circle (Start/Initial Node). The activity diagram ends with a\nfinal state that is graphically represented by a small, filled circle inside another circle\n(End/Final Node).\n", "page": 447, "type": "text", "section": "Page 447"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 419 ]\nA Flow Final Node, which is a circle with an X inside, can be used to represent the end of a\nspecific process flow. Unlike the end node, which denotes the end of all control flows\nwithin an activity, a flow final node represents the end of a single control flow.\nActions/Control \ufb02ow\nActions are single steps within an activity. Like activities, they are also represented as a\nrectangle with rounded corners. A solid line with an open arrowhead is used to show\ncontrol flow:\nDecision/merge nodes\nA decision occurs in a flow when there is some condition and there are at least two paths\nthat branch from that decision. A label can be placed on each of the different branches to\nindicate the guard condition that would allow control to flow down the branch.\nWhen you want to bring multiple alternate flows back to a single outgoing flow, a merge\nnode is used. Both decision and merge nodes are graphically represented with a diamond\nsymbol:\n", "page": 448, "type": "text", "section": "Page 448"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 420 ]\nFork/join nodes\nWhen you want to model a single flow forking into two or more concurrent flows, you use\na fork node. When you want to combine two or more concurrent flows back into a single\noutgoing flow, you use a join node. The following diagram illustrates a flow that has a fork\nand join node:\nBoth fork and join nodes are graphically represented with either a horizontal or vertical bar.\nThe orientation of the bar is dependent on whether the flow is going from top to bottom or\nleft to right. When fork and join nodes are used together, they are sometimes referred to\nas synchronization.\nReviewing software architectures\nAn important step in designing a high-quality software architecture is for it to go through a\nreview process. Architecture reviews may also be conducted when an organization acquires\nsoftware or to compare architectures. A review will determine whether the functional\nrequirements and quality attribute scenarios can be satisfied with the software architecture.\nReviewing the architecture helps the team find mistakes and correct them as early as\npossible. This can greatly reduce the amount of effort it takes to fix a defect and can help to\navoid further rework.\nIn this section, we will be taking a look at the following software architecture evaluation\nmethods:\nSoftware architecture analysis method (SAAM)\nArchitecture tradeoff analysis method (ATAM)\nActive design review (ADR)\nActive reviews of intermediate designs (ARID)\n", "page": 449, "type": "text", "section": "Page 449"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 421 ]\nSoftware architecture analysis method (SAAM)\nThe software architecture analysis method (SAAM) is one of the first documented\nmethods for evaluating software architectures. The original purpose of SAAM was to assess\nthe modifiability of a software system, although some have extended it to review a software\narchitecture for a variety of quality attributes, including reliability, portability, extensibility,\nand performance.\nScenario-based analysis of software architecture\nSAAM is a scenario-based review method, and can be an effective way to review a software\narchitecture. A scenario is a description of the interaction between some source, such as a\nstakeholder, and a software system. It represents some use or expected quality of the\nsoftware system and may consist of a sequence of steps detailing the use or modification of\nit.\nScenarios can be used to test software quality attributes, which is one of the purposes\nbehind software quality attribute scenarios:\nA software quality attribute scenario consists of the following parts:\nSource of stimulus: The source is some entity, such as a stakeholder or another\nsoftware system, which generates a particular stimulus.\nStimulus: The stimulus is some condition that requires a response from the\nsoftware system.\nArtifact: The artifact is the software that is stimulated. It can be a part of the\nsoftware system, the entire software system, or a collection of multiple systems.\nEnvironment: The environment is the set of conditions under which the stimulus\noccurs. For example, a particular configuration of the software or specific values\nin the data may be necessary for the stimulus to exist.\n", "page": 450, "type": "text", "section": "Page 450"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 422 ]\nResponse: The response is the activity that takes place when the stimulus arrives\nat the artifact.\nResponse Measure: When a response occurs, it should be measurable. This\nallows us to test the response to ensure that the software system meets the\nrequirements.\nSAAM steps\nThere are six main steps in the software-architecture analysis method: develop scenarios,\ndescribe the architecture, classify and prioritize scenarios, evaluate scenarios, assess \nscenario interactions, and create an overall evaluation.\nStep 1 \u2013 Develop scenarios\nUsing requirements and quality attributes, we can identify the different types of\nfunctionality that the software system is supposed to support and the qualities it is\nexpected to have. This knowledge forms the basis for developing scenarios. Quality\nattribute scenarios, with the source of the stimulus, stimulus, artifact, environment,\nresponse, and response measure defined for each one, provide the type of information that\nmakes scenarios useful when reviewing an architecture.\nA variety of stakeholders should participate in brainstorming for scenarios, as the different\nperspectives and needs of a diverse group of people with an interest in the system will help\nto ensure that no important scenarios are overlooked. It is often useful to take an iterative\napproach when developing scenarios because identifying scenarios can lead the software\narchitect, development team, and other stakeholders to think of additional scenarios.\nStep 2 \u2013 Describe the architecture\nIn this step, the software architect describes the architecture to the review team. Completed\narchitecture documentation can be used as part of the presentation. Any notations used in\nthe documentation should be well understood by all of the review participants.\nStep 3 \u2013 Classify and prioritize scenarios\nEach scenario that is created in Step 1 \u2013 Develop scenarios, is classified and prioritized in this\nstep. Scenarios can either be direct or indirect scenarios. If the software system does not\nrequire any modifications to perform the scenario, it can be classified as a direct scenario. If\nthe scenario is not directly supported, meaning that some change has to be made to the\nsoftware system for the scenario, then it is an indirect scenario.\n", "page": 451, "type": "text", "section": "Page 451"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 423 ]\nOnce scenarios have been classified, they should be prioritized based on importance. This\ncan be accomplished by using some type of voting procedure. The scenarios that are\ndetermined to be of highest priority to the review team as a whole can be used for the\nevaluation.\nStep 4 \u2013 Evaluate scenarios\nIn this SAAM step, the scenarios are evaluated. For each of the direct scenarios, the\nsoftware architect demonstrates how the architecture can execute it. For any indirect\nscenarios, the team should identify what has to be changed (for example,\nmodification/addition/deletion of components) in order to execute each one. The team\nshould estimate the level of effort necessary to change the system so that it can execute\nindirect scenarios.\nStep 5 \u2013 Assess scenario interaction\nIn this step, the reviewers analyze the interaction of the scenarios. If multiple related\nscenarios interact with the same component, this may be acceptable. However, if multiple\nunrelated scenarios interact with the same component, it could be an indication of a poor\ndesign. Further analysis should be conducted to determine whether the component is\nlacking a clear separation of responsibilities.\nRefactoring may be necessary to avoid different scenarios interacting with the same\ncomponent. The component may have low cohesion, indicating that its elements are not\nclosely related. It may also exhibit tight coupling, signifying that the component is highly\ndependent on another component. Low cohesion and tight coupling increase complexity\nand reduce the maintainability of the system. If such a situation exists, a component may\nneed to be separated into multiple components.\nStep 6 \u2013 Create an overall evaluation\nWith the prior steps completed, the review team should have a list of scenarios that have\nbeen classified, prioritized, and evaluated. The interaction of the scenarios may reveal\npotential issues with the design. Ultimately, the review team must make a decision as to\nwhether the architecture is viable and can be accepted as is or if it has to be modified in\nsome way.\n", "page": 452, "type": "text", "section": "Page 452"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 424 ]\nArchitecture tradeoff analysis method (ATAM)\nThe architecture tradeoff analysis method (ATAM) is another scenario-based architecture\nreview method. ATAM is a successor to SAAM and improves upon it. ATAM has a focus\non reviewing design decisions and quality attributes.\nATAM participant roles\nThe main participant roles during ATAM evaluation are the evaluation team, project\ndecision makers, and stakeholders. The evaluation team, which should ideally be a group\nthat is external to the software project, consists of a team leader, evaluation leader, scenario\nscribe, proceedings scribe, and questioner.\nTeam leader: The team leader coordinates and sets up the review. They are\nresponsible for creating the evaluation team as well as ensuring that the final\nreport is produced.\nEvaluation leader: The evaluation leader runs the actual review. This includes\nfacilitating sessions that create/prioritize/select/evaluate scenarios.\nScenario scribe: The scenario scribe writes notes about the scenarios on a\nwhiteboard or flipchart as the evaluation is taking place.\nProceedings scribe: The proceedings scribe is responsible for capturing notes in\nelectronic format. Details about scenarios are an important aspect of the\nevaluation that the proceedings scribe will capture.\nQuestioner: The questioner focuses on raising issues and asking questions\nrelated to the architecture, with a particular focus on quality attributes.\nProject decision makers: The project decision makers are the individuals who\nhave the authority to make changes to the software if necessary, including the\npower to assign/approve resources for work. Project sponsors, project managers,\nand software architects typically make up the project decision-maker group.\nStakeholders: The stakeholders include anyone who has a vested interest in the\nsoftware architecture and the system as a whole.\nATAM phases\nThere are four main phases involved with an ATAM evaluation of a software architecture:\nPhase 0: Partnership and preparation\nPhase 1: Evaluation\n", "page": 453, "type": "text", "section": "Page 453"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 425 ]\nPhase 2: Evaluation (continued)\nPhase 3: Follow-up\nPhase 0 \u2013 Partnership and preparation\nThis initial phase is used to prepare for the evaluation. The leader of the evaluation team\nmeets with the project decision makers to agree on details about the evaluation. An\nagreement should be reached on the logistics of the meeting as well as which stakeholders\nwill be invited.\nAs part of the preparation, the evaluation team looks at the architecture documentation to\nbecome familiar with the software application and its architecture. Expectations are set by\nthe evaluation team as to the information they expect to be presented during Phase 1.\nPhase 1 \u2013 Evaluation\nPhase 1 is the first of two phases dedicated to the evaluation of the architecture. In this\nphase, the evaluation team meets with the project decision makers. Phase 1 consists of the\nfollowing steps:\nPresent the ATAM\n1.\nPresent the business drivers\n2.\nPresent the architecture\n3.\nIdentify architectural approaches\n4.\nGenerate the quality attribute utility tree\n5.\nAnalyze architectural approaches\n6.\nStep 1 \u2013 Present the ATAM\nIn this step, the evaluation leader explains the ATAM to the project decision makers. Any\nquestions about the ATAM can be answered during this step.\nIf everyone in the meeting is already familiar with the ATAM, this step could potentially be\nskipped. For example, a development team may go through the ATAM phases and steps\nmultiple times as it iteratively designs the architecture. If the team consists of the same\nmembers, it may not be necessary to go over the ATAM each time. However, if any\nparticipants in a given iteration are new to the method, either this step should not be\nskipped or there must be a suitable alternative for participants who are new to the method\nto learn it.\n", "page": 454, "type": "text", "section": "Page 454"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 426 ]\nStep 2 \u2013 Present the business drivers\nThis step is used to present the software system from a business perspective to the various\nparticipants. The business goals, functionality, architectural drivers, and any constraints\nwill help everyone to understand the overall context of the software system. This\ninformation is presented by one of the project decision makers.\nStep 3 \u2013 Present the architecture\nThe software architect presents the architecture to the participants in this step. The software\narchitect should provide sufficient detail about the architecture so that the participants can\nunderstand it.\nThe level of detail needed in the presentation can vary from project to project. It really\ndepends on the quality attribute scenarios of the system, how much of the architecture\ndesign is complete/documented, and how much time is available for the presentation. In\norder to be clear what level of detail is expected, the software architect should use phase\nzero, when expectations are set, as an opportunity to ask for clarification.\nStep 4 \u2013 Identify architectural approaches\nBy the time this step takes place, the participants should be familiar with the design\nconcepts used in the architecture. This includes software architecture patterns, reference\narchitectures, tactics, and any externally developed software. This information was\navailable in Phase 0 when the architecture documentation was reviewed, as well as in the\nprior step (Step 3 \u2013 Present the architecture) when the architecture was presented. This step is\nto simply record the design concepts used so that the list can be used in a subsequent step\nfor analysis.\nStep 5 \u2013 Generate the quality attribute utility tree\nQuality attribute scenarios can be represented in a utility tree, which represents the\nusefulness (utility) of the system. Utility trees help participants understand the quality\nattribute scenarios.\nThe utility tree is a set of detailed statements about the quality attributes and scenarios that\nare important to the software system. Each entry in the tree begins with the quality\nattribute itself (for example, maintainability, usability, availability, performance, or\nsecurity), followed by a subcategory that breaks it down with more detail, followed by a\nquality attribute scenario.\n", "page": 455, "type": "text", "section": "Page 455"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 427 ]\nFor example, under a software quality attribute such as Security, we may have multiple\nsubcategories (\"Authentication\" and \"Confidentiality\"). Each subcategory will have one or\nmore quality attribute scenarios:\nQuality\nattribute\nSubcategory\nScenario\nSecurity\nAuthentication\nUser passwords will be hashed using the bcrypt hashing function.\nConfidentiality\nA user playing the role of a customer-service representative will only be\nable to view the last four digits of a customer's social security number.\nA user playing the role of a customer-service manager will be able to view a\ncustomer's entire social security number.\nPerformance\nEtc.\nIn addition to identifying the quality attribute scenarios, the project decision makers should\nprioritize them. As with the SAAM, a voting scheme can be used to allow participants to\nprioritize the scenarios.\nStep 6 \u2013 Analyze architectural approaches\nThe quality attributes that were determined to be of the highest priority in Step 5 \u2013 Generate\nthe quality attribute utility tree, are analyzed, one by one, by the evaluation team in this step.\nThe software architect should be able to explain how the architecture can satisfy each one.\nThe evaluation team looks to identify, document, and ask about the architectural decisions\nthat were made to support the scenario. Any issues, risks, or tradeoffs with the architectural\ndecisions are raised and documented. The goal of the team is to match architectural\ndecisions with quality attribute scenarios and determine whether the architecture and those\narchitectural decisions can support the scenarios.\nBy the completion of this step, the team should have a good understanding of the overall\narchitecture, the design decisions that were made, the rationale behind the decisions, and\nhow the architecture supports the main goals of the system. The team should also now be\naware of any risks, issues, and tradeoffs that may exist. The completion of this step signifies\nthe end of Phase 1.\n", "page": 456, "type": "text", "section": "Page 456"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 428 ]\nPhase 2 \u2013 Evaluation (continued)\nPhase 2 is a continuation of the architecture evaluation. It is normally scheduled to occur\nafter a short hiatus (for example, one week) after the completion of Phase 1. Phase two\ninvolves a greater number of participants as compared with Phase 1. In addition to the\nevaluation team and the project decision makers, it is now time for the invited stakeholders\nto join the evaluation and participate.\nThis phase should begin with a repeat of Step 1 \u2013 Present the ATAM, if any of the new\nparticipants are unfamiliar with the approach. The evaluation team leader should also\nsummarize what was accomplished in Phase 1.\nPhase 2 consists of the following three steps:\nBrainstorm and prioritize scenarios\n7.\nAnalyze architectural approaches\n8.\nPresent results\n9.\nStep 7 \u2013 Brainstorm and prioritize scenarios\nIn this step, all of the stakeholders are asked to brainstorm scenarios. They should be\nencouraged to provide scenarios that are from their perspective and that are important to\nthe success of their roles. Having a variety of stakeholders is helpful to get a diverse set of\nscenarios.\nWhen there are enough scenarios, the group should look them over to see whether any of\nthem can be removed or merged with others because of their similarities with other\nscenarios. The scenarios should then be prioritized by the stakeholders by voting in order\nto determine the most important scenarios.\nOnce a list of scenarios is determined, it should be compared with the scenarios that the\nproject decision makers came up with for the utility tree in Step 5 \u2013 Generate the quality\nattribute utility tree. While the utility tree shows what the software architect and other\nproject decision makers saw as the goals and architectural drivers of the system, this step\nallows the stakeholders to show what is important to them.\nIf the two prioritized lists of quality attribute scenarios are similar, it is an indication that\nthe software architect and the stakeholders are in alignment. If any important quality\nattribute scenarios are uncovered that had not been considered previously, some additional\nwork will be necessary. The level of risk is dependent on the nature and size of the needed\nchanges.\n", "page": 457, "type": "text", "section": "Page 457"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 429 ]\nStep 8 \u2013 Analyze architectural approaches\nSimilar to Step 6 \u2013 Analyze architectural approaches, the software architect describes to the\ngroup how the list of scenarios created by the stakeholders can be realized by the \narchitectural approaches that have been taken with the system. The evaluation team can\nraise any issues, risks, and tradeoffs they see with the architectural approaches. The team\nshould be able to determine whether the scenarios can be achieved by the architecture.\nStep 9 \u2013 Present results\nIn the final step of the ATAM, any risks that were uncovered during the evaluation should\nbe related to one or more of the business drivers identified in Step 2 \u2013 Present the business\ndrivers. Project management will now be aware of the risks and how they relate to the goals\nof the system and will be in a position to manage those risks.\nA presentation is given to the stakeholders that summarizes all of the findings from the\nevaluation. The output of the process includes the architectural approaches, the prioritized\nlist of scenarios generated by the stakeholders, the utility tree, and documentation\nregarding the issues, risks, and tradeoffs identified. This output is presented and delivered\nby the evaluation team to the project decision makers and the stakeholders who\nparticipated.\nPhase 3 \u2013 Follow-up\nThe evaluation team produces and delivers the final evaluation report in this phase. A\ncommon timeframe for this phase is one week, but it could be shorter or longer. The report\nmay be given to various stakeholders for review, but once it is complete, the evaluation\nteam delivers it to the individual who commissioned the review.\nActive design review (ADR)\nAn active design review (ADR) is most suited for architecture designs that are in progress.\nThis type of architecture review is more focused on reviewing individual sections of the\narchitecture at a time, rather than performing a general review. The process involves\nidentifying design issues and other faults with the architecture so that they may be\ncorrected as quickly and early in the overall design process as possible.\n", "page": 458, "type": "text", "section": "Page 458"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 430 ]\nOne of the main premises of ADR is that there is too much information involved with\nreviewing an entire architecture at once and not enough time to do it properly. Many \nreviewers may not be familiar with the goals and details of every part of the design. As a\nresult, no single part of the design ends up getting a complete evaluation. In addition, with\nmore conventional review processes, there may not be enough one-on-one interaction\nbetween the reviewer and the designer.\nADR attempts to address these deficiencies by changing the focus from a more general\nreview of the entire architecture to a series of more focused reviews. Questionnaires are\nused to provide more opportunities for interaction between reviewers and designers and to\nkeep reviewers engaged.\nADR steps\nThere are five steps to the ADR process:\nPrepare the documentation for review\n1.\nIdentify the specialized reviews\n2.\nIdentify the reviewers needed\n3.\nDesign the questionnaires\n4.\nConduct the review\n5.\nWe will now take a look at each of these steps in detail.\nStep 1 \u2013 Prepare the documentation for review\nIn this step of ADR, preparation takes place for the review. This includes preparing the\ndocumentation for review and listing assumptions that are being made about the portion of\nthe architecture being reviewed. The assumptions need to be made clear so that reviewers\nwill be aware of them. These assumptions include any items that the software architect (or\nother designers) think will never change or are highly unlikely to change. In addition, any\nincorrect usage assumptions should also be provided. These are assumptions that the\nsoftware architect (or other designers) deem to be an incorrect usage of the module and\ntherefore should not take place.\n", "page": 459, "type": "text", "section": "Page 459"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 431 ]\nStep 2 \u2013 Identify the specialized reviews\nIn this step, we identify the specific properties of the design that we want the focus to be\nplaced on by the reviewers. Doing this gives reviewers a clear focus and responsibility for\nthe specialized review that they will be conducting. For example, we may want an\nindividual reviewer to be focused on one or more specific quality attributes.\nStep 3 \u2013 Identify the reviewers needed\nIn this ADR step, the reviewers for the part of the design being reviewed are identified. We\nwant reviewers to focus on the areas they are most suited to review. The goal is to get\npeople with different perspectives and sets of knowledge to participate as reviewers.\nExamples of reviewers include development team members who did not work on the part\nof the architecture being reviewed, technical staff from other projects, users of the system,\nnon-technical reviewers who are specialists or have knowledge related to the software\nsystem, reviewers who are external to the organization, and anyone else who may be adept\nat identifying potential issues with a design.\nStep 4 \u2013 Design the questionnaires\nQuestionnaires are designed in this step, which the reviewers will use while evaluating the\narchitecture in the next step. Use of the questionnaires is intended to encourage reviewers\nto take on an active role and to get them to use the architecture documentation during the\nreview. In addition to questions, questionnaires can contain exercises or other instructions\nfor the reviewers to perform.\nQuestions/instructions should be phrased in an open, active way to encourage further\nthought and a more detailed response. For example, rather than asking if the part of the\narchitecture being reviewed is sufficient, the instructions could guide the reviewer to\nprovide an implementation in pseudocode that uses the portion of the architecture to\naccomplish some task.\nStep 5 \u2013 Conduct the review\nThis step of the process is when the review takes place. Reviewers are assigned to the\nreview and then a presentation of the module being reviewed is made. The reviewers then\nconduct their reviews, including the completion of the questionnaires. Sufficient time must\nbe allotted so that the review can be completed properly.\n", "page": 460, "type": "text", "section": "Page 460"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 432 ]\nOnce the reviews are complete, a meeting is held between the reviewers and the designers.\nThe designers can read the complete questionnaires and use the meeting as an opportunity\nto communicate with the reviewers and clarify any questions. The end result of the final\nstep is to modify the architecture artifact, if necessary, based on any points made during the\nreview.\nActive reviews of intermediate designs (ARID)\nActive reviews of intermediate designs (ARID) is an architecture review method that\ncombines ADR with ATAM. This hybrid method takes from the ADR approach the focus of\nreviewing a software architecture while it is in progress and the emphasis on active\nreviewer participation. It combines this with the ATAM approach of focusing on quality\nattribute scenarios. The goal is to provide valuable feedback into the viability of the\nsoftware architecture and uncover any errors and inadequacies with it.\nARID participant roles\nThe main participants in the ARID process are the ARID review team (facilitator, scribe,\nand questioners), the software architect/lead designer, and the reviewers:\nFacilitator: The facilitator works with the software architect to prepare for the\nreview meeting and facilitates it when it takes place.\nScribe: The scribe captures the issues and results of the review meeting.\nQuestioners: One or more questioners raise issues, ask questions, and assist with\ncreating scenarios during the review meeting.\nSoftware architect/lead designer: The software architect (or designer) is the \nperson responsible for the design being reviewed. This person is responsible for\npreparing and presenting the design as well as participating in the other steps.\nReviewers: The reviewers are the individuals who will be performing the review.\nThey consist of stakeholders who have a vested interest in the architecture and\nthe software application.\nARID phases\nThere are two phases involved in the ARID process, which consist of nine steps in all. The\ntwo phases are the Step 1 \u2013 Pre-meeting and Step 2 \u2013 Review meeting phases.\n", "page": 461, "type": "text", "section": "Page 461"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 433 ]\nPhase 1 \u2013 Pre-meeting\nThe first phase is a meeting to prepare for the actual review. For a software architecture\nreview, this meeting typically takes place between the software architect and the review\nfacilitator. If someone other than the software architect is responsible for the design of the\nportion of the architecture being reviewed, this person should join the meeting with the\nreview facilitator. The pre-meeting consists of the following steps:\nIdentify reviewers\n1.\nPrepare the design presentation\n2.\nPrepare the seed scenarios\n3.\nPrepare for the review meeting\n4.\nStep 1 \u2013 Identify reviewers\nIn this step in the ARID approach, the software architect and review facilitator meet to\nidentify the group of people who will attend the review meeting. Management may also be\ninvolved with this step to help identify available resources.\nStep 2 \u2013 Prepare the design presentation\nDuring the actual review meeting, the software architect will present the design and any of\nthe relevant documentation related to it. In this step, the software architect gives a\npreliminary version of the presentation to the review facilitator. This allows the software\narchitect to practice the presentation and receive feedback from the review facilitator that\nmay help to improve the presentation.\nStep 3 \u2013 Prepare the seed scenarios\nIn this step of review preparation, the software architect and review facilitator work\ntogether to come up with seed scenarios, or a sample set of scenarios that the reviewers can\nuse during the review.\nStep 4 \u2013 Prepare for the review meeting\nThis step is used for any other tasks related to the preparation of the review meeting. It can\nbe used to identify the materials that will be distributed to all of the reviewers, such as the\narchitecture documentation, seed scenarios, questionnaires, and review agenda. A date,\ntime, and location for the review meeting must be selected and invitations must be sent out.\n", "page": 462, "type": "text", "section": "Page 462"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 434 ]\nPhase 2 \u2013 Review meeting\nThe second ARID phase is devoted to the review meeting. It consists of the following steps:\nPresent the ARID method\n5.\nPresent the design\n6.\nBrainstorm and prioritize scenarios\n7.\nPerform the review\n8.\nPresent conclusions\n9.\nStep 5 \u2013 Present the ARID method\nAt the start of the review meeting, the review facilitator should present the ARID method to\nall of the participants. This step is similar to a step we covered in the ATAM in which the\nATAM is presented to participants. As was the case in that step, if everyone who is\nparticipating is already familiar with the ARID method, this step could be skipped.\nHowever, if anyone is not familiar with ARID or needs a refresher, it should be presented.\nStep 6 \u2013 Present the design\nAfter the review facilitator has presented the ARID method, the software architect presents\nthe architecture design. Questions regarding the design rationale or comments about\nalternative solutions should be avoided. The design facilitator can help to keep the meeting\nin line with its goals. The purpose of the review is to determine whether the designed\narchitecture is usable, so factual questions for clarification and pointing out issues that\nshould be addressed are the types of feedback that should be encouraged. A person playing\nthe role of a scribe should take notes regarding any questions and issues that are raised.\nStep 7 \u2013 Brainstorm and prioritize scenarios\nDuring this step of the process, the participants brainstorm to come up with scenarios for\nthe software system that will use the designed architecture. The seed scenarios that were\ncreated in phase one are included with the scenarios that are created in this step to form the\navailable choices.\nAs with ATAM, participants should be encouraged to provide scenarios that are important\nto them. The process works best when there are a variety of stakeholders to ensure that\ndifferent perspectives are considered.\nThe group can then analyze and prioritize the scenarios. It may make sense to combine\nsome of the scenarios or identify some as being duplicates. The review group can vote on\nwhich scenarios are the most important. The scenarios that are prioritized as being the most\nimportant essentially define what makes the architecture usable.\n", "page": 463, "type": "text", "section": "Page 463"}
{"text": "Documenting and Reviewing Software Architectures\nChapter 12\n[ 435 ]\nStep 8 \u2013 Perform the review\nThe reviewers use the scenarios to determine whether the architecture solves the problem\nthat is presented. Real code or pseudocode can be written to test the scenario. When the\ngroup feels that a conclusion can be reached (or time runs out), the review ends.\nStep 9 \u2013 Present conclusions\nWith the review complete, the group should be able to draw conclusions as to whether or\nnot the architecture is suitable for the key scenarios. Any issues with the architecture can be\nreviewed so that the architecture can be refactored to correct any problems.\nSummary\nDocumenting a software architecture is an important step in delivering an architecture. The\ndocumentation communicates the architecture to others, assists the development team,\neducates team members, provides input for architecture reviews, and allows the reuse of\narchitectural knowledge.\nArchitecture views, which are representations of an architecture, allow a software architect\nto communicate their architecture in a manageable and understandable way. There is a cost\nassociated with creating and maintaining views though, so while we never want the\ndocumentation to be insufficient, we do not want to spend time working on views that are\nnot needed. In this chapter, you were provided with an overview of the UML, one of the\nmore widely used modeling languages. You learned about structural and behavioral\nmodeling.\nReviewing a software architecture is important to determine whether the architecture will\nmeet the needs of the system. This chapter provided details on several different architecture\nreview methods.\nIn the next chapter, we will gain an understanding of what software architects need to\nknow about DevOps, including its values and practices. We will learn how continuous\nintegration, continuous delivery, and continuous deployment allow an organization to\nrelease software changes quickly and reliably.\n", "page": 464, "type": "text", "section": "Page 464"}
{"text": "13\nDevOps and Software\nArchitecture\nDevOps is a combination of cultural values, practices, and tools that allow an organization\nto deliver software applications quickly. It is important for software architects to have an\nunderstanding of DevOps. An organization you work for may already be practicing it, or\nthey may be interested in transitioning to DevOps. Either way, software architects within\nan organization play a role in helping and leading others to follow DevOps values and\npractices.\nThis chapter will explain the purpose behind DevOps, its values, and the reasons why\norganizations adopt them. It will cover the different types of tools used in DevOps as well\nas the important DevOps practices. You will gain an understanding as to how DevOps may\naffect architectural decisions, and about the different ways that the cloud can be utilized for\ndeployment.\nIn this chapter, we will cover the following topics:\nDevOps\nDevOps toolchain\nDevOps practices\nArchitecting for DevOps\nDeploying to the cloud\n", "page": 465, "type": "text", "section": "Page 465"}
{"text": "DevOps and Software Architecture\nChapter 13\n[ 437 ]\nDevOps\nDevOps is the set of tools, practices, and culture that enable both the development and\noperations terms to work together during the entire life cycle of building software\napplications. DevOps enables the continuous delivery of software, allowing organizations\nto respond quickly to changing market opportunities and allowing them to get customer\nfeedback quickly.\nDevOps involves collaboration between teams and the automation of processes to achieve a\ncommon goal of adding value to customers' experiences by delivering software changes\nquickly and with high quality. DevOps requires a cultural change within an organization,\nalong with the use of new technologies.\nShift left is a common term associated with DevOps. The idea is to perform tasks earlier in\nthe life cycle, or to shift them left in the timeline. This term became popular with testing\n(shift, left testing), meaning that the process of testing a software application should be\nmoved to earlier in the overall development process.\nDevOps has embraced this shift left mentality. In addition to testing earlier in the process\n(with automated tests), DevOps also considers what else can be moved to earlier in the\nprocess. Involvement of the operations team should shift left and begin working with the\ndevelopment team much earlier than has been done in the past. Rather than begin working\ntogether at deployment time, they should be working together throughout the software\ndevelopment life cycle.\nLater in this chapter, in the Continuous integration (CI) section, we will take a look at how\nthe practice of continuous integration shifts left the integration and building of software\nchanges so that they take place sooner in the process. We'll also see how the practice of\ncontinuous delivery shifts left the deployment of software changes to production so that\nthey reach users faster.\nCALMS\nCALMS is an acronym that represents the core values of DevOps. The original version was\nCAMS and the L was added later. CALMS stands for the following:\nCulture\nAutomation\nLean\nMeasurement\nSharing\n", "page": 466, "type": "text", "section": "Page 466"}
{"text": "DevOps and Software Architecture\nChapter 13\n[ 438 ]\nLet's explore each of these core values in more detail, as they will give us a better\nunderstanding of what is involved with DevOps.\nCulture\nAt its core, DevOps is a culture and a philosophy. It requires changing the culture of an\norganization by breaking down barriers between teams. To improve how work is done,\nsometimes organizations need to learn how to work differently. DevOps necessitates cross-\nfunctional teams and encourages teams with different skills and knowledge, such as\ndevelopment and IT, to work together. DevOps builds a culture of collaboration even\nbetween teams that, historically, may not have worked closely together.\nTeamwork is an important value in DevOps. Teams such as development, operations, and\nquality assurance terms should be encouraged to communicate, work together, and get to\nknow one another. Tighter integration between these teams can allow them to be more\nefficient, increase innovation, and deliver more value to customers.\nContinuous improvement is also a part of the DevOps culture. DevOps culture values\nlearning and using any knowledge gained for improvement. Different teams in an\norganization should learn from each other. If faults or other issues occur, team members\nshould learn from those experiences so that they are not repeated. Everyone in the\norganization should participate in an ongoing effort to improve processes and products.\nAccountability also needs to be part of the organizational culture. When mistakes are made,\nindividuals and teams should take ownership of them. Rather than blaming other teams,\neveryone should be focused on fixing the issue and making improvements to prevent those\nissues from happening again.\nQuality needs to be everyone's focus in a DevOps culture. For example, the development\nteam should not rely on the quality assurance team to find defects. They need to be just as\ndedicated to finding defects and maintaining a high level of quality for the software\nsystems they develop. As the rate of releasing changes to software is accelerated with\nDevOps, an effort has to be made by all teams in the organization to ensure that quality\ndoes not suffer.\nOrganizations should empower their employees as part of their culture. No matter what\nrole an individual is playing on one of the teams, they should feel empowered to point out\nissues, stop a potential mistake that they see, or make a suggestion as to how to improve a\nproduct, service, or process.\n", "page": 467, "type": "text", "section": "Page 467"}
{"text": "DevOps and Software Architecture\nChapter 13\n[ 439 ]\nSome organizational changes can be helpful when instituting a cultural change. Altering\nteam structures, changing where teams sit in an office, and modifying processes are\nexamples of organizational changes that may need to be made to help facilitate the desired\ncultural changes.\nAutomation\nAutomation involves taking manual processes and making them operate automatically so\nthat they are repeatable. The first step in automating processes is to identify the current\nprocesses that are executed and understand how they work. We want to be able to identify\nbottlenecks and figure out where defects are most likely to be introduced. Once this\nunderstanding is in place, we can begin to select the appropriate tools to enable\nautomation.\nAutomation is a great way for teams to improve the way that they work. An automatic\nprocess ensures better consistency and accuracy compared to a person executing the\nprocess manually. People make mistakes, and automation avoids that problem. An\nadditional benefit of automating a process is that it does not tie up a team member because\nit can be executed on its own. Automatic processes can also be executed during off-peak\nhours to avoid conflicts with system resources. Automation is suited to a number of tasks,\nincluding unit testing and builds.\nAutomated processes can be executed quicker than manual ones, and are less prone to\nerror. Instituting automated testing, build, and deployment processes is a key part of\nDevOps. Automated tests can be executed as part of an automated build process, ensuring\nthat recently introduced changes do not unintentionally introduce a defect.\nAutomation also provides quick feedback when something goes wrong with a unit test,\nbuild, or deployment. This will give your organization the ability to quickly react to failures\nso that they can be corrected.\nDevOps does not require automating everything, though. In some cases, an organization\nmay have poor processes, and automating them just speeds them up. It is prudent to look\nfor ways to improve a process before attempting to automate it. Each organization is\ndifferent and has different needs, so finding the right balance of automation for your\norganization is important.\n", "page": 468, "type": "text", "section": "Page 468"}
{"text": "DevOps and Software Architecture\nChapter 13\n[ 440 ]\nYou may have a mix of different types of applications, including legacy applications. These\napplications may use different technologies and be deployed to different types of\nenvironments. Based on your organization's needs and bottlenecks, you should take into\nconsideration the return on investment for automating a process. You may choose to start\nyour focus on the processes that will most benefit the organization. Your organization may\nneed different types of tools to automate different types of process.\nLean\nLean software development (LSD) took the best practices of lean manufacturing and \napplied them to software development. It aim to optimize processes and minimize waste\nduring the software development process. Waste can be anything that increases time and\neffort, does not add business value for customers, and reduces the quality of the software\nsystem.\nThe ideas behind LSD not only complement agile software development methodologies,\nbut they are also consistent with the core values of DevOps. There are seven lean\ndevelopment principles:\nEliminate waste: Elimination of waste is important in lean processes.\nUnnecessary functionality, code, or effort is wasteful. Delaying the delivery of\nvalue to customers and inefficient processes are other examples of software\ndevelopment waste.\nBuild quality in: Quality should be a focus for everyone. The writing of tests is\none way to think about quality early in the process. The automation of tests helps\nto ensure that tests are automatically executed.\nCreate knowledge: Team members should share knowledge within the team and\nacross teams. Code reviews, documentation, pair programming, learning\nsessions, training, and collaboration tools can be used to teach and learn from\nothers.\nDefer commitment: A decision should be made only after enough information\nhas been collected to make a sound one.\nDeliver fast: Value should be delivered to customers quickly. This does not mean\nthat the organization should do so recklessly, but rather that it should provide\nincremental changes reliably, quickly, and frequently.\n", "page": 469, "type": "text", "section": "Page 469"}
{"text": "DevOps and Software Architecture\nChapter 13\n[ 441 ]\nRespect people: As teams work together (and the individuals on those teams),\nthey should do so in a respectful way. Communication and the handling of\nconflicts should be done in a respectful way.\nOptimize the whole: Processes should be optimized and bottlenecks should be\neliminated when possible. Automation of tests, builds, and deployments are\nuseful in optimization efforts.\nMeasurement\nTeams must be able to measure improvement because if they cannot measure it, they\ncannot know whether process improvements, automation, and the introduction of other\nDevOps practices have actually worked. As a software architect, you need to be able to\nmake decisions based on data that has been measured and not just your gut feeling. The\ndata that is measured should be transparent and shared with the development team,\noperations, and other key decision makers within the organization.\nSharing\nSharing is another core value of DevOps. If DevOps is to succeed in your organization,\ninformation, tools, data, and the lessons that are learned need to be shared so that the\norganization can improve as a whole. Any feedback received, whether it is from external or\ninternal sources, is more useful when it is shared with others.\nEncountering failures in a software system may be inevitable but we need to be able to\nlearn from those failures and make improvements based on them. Lessons learned should\nbe shared between development and operations so that continuous improvement is\npossible. Part of changing the culture of an organization to follow a DevOps approach is\nfostering collaboration and communication among various departments. Sharing\ninformation is one of the ways that can change the culture.\nWhy DevOps?\nAs a software architect, you want to lead your organization so that it can deploy software\nfaster, more frequently, and with fewer failures. Approaches other than DevOps do not\nfocus on continuous delivery, so providing value to customers takes a longer period of\ntime. An operations department can sometimes seem slow to react. Time can be wasted\nwaiting for environments to be created and for applications to be moved from one\nenvironment to another. When these processes are manual, they can be wasteful and\nimpede process rather than enable it.\n", "page": 470, "type": "text", "section": "Page 470"}
{"text": "DevOps and Software Architecture\nChapter 13\n[ 442 ]\nWhen asking for resources, a development team may ask for more than they need (for\nexample, asking for a larger than necessary physical server or VM), simply because the\nprocess is too troublesome and they do not want to go through the effort of upgrading it\nlater. QA teams may be waiting for development teams to finish writing code so that they\ncan test it.\nAll of this leads to the organization wasting both time and money. Software architects\nshould be striving for greater efficiency in software development processes, with less\nmanual work and automating more of the effort.\nIn today's competitive environment, organizations need the ability to deliver quickly. In\norder to gain an advantage, they need to add incremental value for their customers by\ndelivering frequent changes that contain new features, bug fixes, and other improvements.\nDevOps allows the organization to deliver value faster. Through continuous delivery, new\nversions of the software can be deployed quickly and with high quality.\nCustomers have a lower tolerance for system outages now, so it is imperative to have\nprocesses in place that detect faults quickly and allow fixes to be deployed easily. The goal\nis to minimize the number of system outages and their duration. There is an increased\nchance of errors with manual processes. Automating processes reduces the chance for\nerrors. When failures do occur, you will want to recover from them quickly. DevOps can\nhelp you to achieve these goals.\nSoftware applications today tend to be more complex and have more complicated\ndeployments. A DevOps approach will lead organizations to automate their builds,\neliminating the possibility of errors caused by mistakes in a manual process.\nPracticing DevOps properly will lead to a greater level of cooperation between\ndevelopment and operations teams, leading to a decrease in deployment time and an\nincrease in reliability. Encouraging information within the organization to be shared will\nlead to less dependence on tribal knowledge. Collaboration across teams increases the\namount of information that is shared.\nDevOps toolchain\nToolchains are a set of programming tools that are used in combination to perform a task. A\nDevOps toolchain focuses on the development and delivery of software systems. Software\narchitects responsible for selecting tools should aim to achieve consistency with the\nDevOps tools. For example, it is better to use the same deployment tool for all of our\nenvironments than use different ones for different environments.\n", "page": 471, "type": "text", "section": "Page 471"}
{"text": "DevOps and Software Architecture\nChapter 13\n[ 443 ]\nIn an organization that embraces DevOps, it is not just the development team that uses\nsoftware tools to perform their jobs. Operations staff should use many of the same\ntechniques as developers. For example, the assets related to operations should be checked\ninto source control and have automated tests.\nDevOps tools are generally categorized as supporting one or more of the following\nactivities:\nPlan: The plan category of tools help you to define the requirements of the\napplication and plan the work. The activities that take place around planning\nvary depending on the methodology being used, but generally involve product\nowners and business analysts evaluating requirements and other requests. The\nbusiness value and effort are evaluated and work is planned for a release.\nCreate: Create tools are software that help in some way with design, coding, and\nbuilding activities. Examples of these tools include integrated development\nenvironments (IDEs), version control repositories, build automation tools, and\nconfiguration management tools.\nVerify: Verification tools consist of software that help to ensure the quality of a\nsoftware release. Some verification activities occur during create activities, while\nothers occur once some part of the software is complete. Functional and non-\nfunctional testing must take place, and DevOps tools can help to automate some\nof the testing. Verification tools can help with performance, acceptance,\nregression, and release testing. Static analysis tools can help to analyze the code,\nand security tools can help to identify vulnerabilities.\nPackage: Packaging tools are used for the tasks associated with preparing a\nrelease once it has been verified and is ready for deployment. Packaging\nconfiguration and triggering releases are some of the activities involved in\npackaging.\nRelease: The tools in the release category are used for release-related activities.\nThey assist teams with the scheduling and coordination of deploying software\nsystems into different environments, including production.\nConfigure: Tools are available that assist with the configuration of software\napplications. The configure category includes tools that help with configuration\nmanagement, application provisioning, and the configuration of infrastructure\nresources.\n", "page": 472, "type": "text", "section": "Page 472"}
{"text": "DevOps and Software Architecture\nChapter 13\n[ 444 ]\nMonitor: It is critical to monitor software applications to identify issues and gain\nan understanding as to their impact. Production metrics give an organization the\ninsight necessary to understand what the problem areas are with a software\napplication. Monitoring the performance of a software application will ensure\nthat end users are not encountering bottlenecks when they use the software.\nWhen we covered CALMS, it was mentioned that measurement is one of the core\nvalues of DevOps. Monitoring tools can help teams measure their applications in\norder to determine whether, and how much, automation efforts have improved a\nprocess or product.\nDevOps practices\nThere is variation depending on the organization and the software development\nmethodology used, but in a DevOps release cycle, the following major activities generally\ntake place:\nDevelopment\nIntegration\nBuild and unit testing\nDelivery to staging\nAcceptance testing\nDeployment to production\nThe following diagram illustrates a typical DevOps release cycle:\nThree of the main DevOps practices are continuous integration, continuous delivery, and\ncontinuous deployment. These practices revolve around the key activities of a DevOps\nrelease cycle, so let's explore them in more detail.\n", "page": 473, "type": "text", "section": "Page 473"}
{"text": "DevOps and Software Architecture\nChapter 13\n[ 445 ]\nContinuous integration (CI)\nContinuous integration (CI) is the practice of developers merging their changes into a \nshared source control repository as often as possible. Using some type of version control\nsystem is a necessity. Developers should commit changes on a frequent and regular basis.\nThis reduces the number of conflicting changes and makes it easier to resolve conflicts\nwhen any exist.\nIn the DevOps release cycle, the activities covered by continuous integration include\ndevelopment, integration (the checking in of changes into a version control system), and an\nautomated build with automated unit testing:\nSome may consider the delivery to staging phase that occurs after a build is complete as\npart of continuous integration, while others see that as the next step in overall continuous\ndelivery.\nAutomated builds\nTwo of the key aspects of continuous integration is to build all commits and to have a build\nprocess that is automated. A build converts source code, files, and other assets into a\nsoftware project that is in its final, usable form. This may include things such as the\nfollowing:\nDependency checking\nCompiling source files\nPackaging compiled files into a compressed format (for example, JAR or ZIP)\nCreating installers\nCreating/updating database schema\nExecuting data change scripts to modify data in a database\nRunning automated tests\n", "page": 474, "type": "text", "section": "Page 474"}
{"text": "DevOps and Software Architecture\nChapter 13\n[ 446 ]\nAs a software architect, you will want to establish an automated build process if one is not\nalready in place. Some builds require many steps, and performing these manually creates\nmany opportunities for mistakes to take place. Automated builds eliminate the variation\nthat can take place with manual builds, ensuring consistency between builds.\nAutomated builds are what make it possible to execute builds at any time. In the practice of\ncontinuous integration, automated builds are essential, and should take place when\nchanges have been checked in to a version control system. The duration of automated\nbuilds should not be too long (for example, less than 20 minutes) to allow continuous\nintegration to be feasible.\nSoftware versioning\nAs part of the build process for your software, you will need to think about how you want\nto version your software. Software versioning is the process of assigning a unique number or\nname to the software you are building.\nIt is beneficial to use a formal convention for software versioning. Once a formal\nconvention has been established, it provides everyone who is interested in the software,\nboth internally and externally, with knowledge about the state of the software. Without the\nuse of a formal versioning convention, version numbers are meaningless to users and\nuseless for dependency management. A meaningful version number communicates to\ninterested parties information regarding the intention of the version and the extent of the\nchanges in the version. One of the software versioning conventions that can be used is\nsemantic versioning.\nSemantic versioning, also referred to as SemVer, is a popular convention that can be used\nfor software versioning. Semantic versioning uses a three-part version number that follows\nthe MAJOR.MINOR.PATCH format. For example, a version number might be 1.5.2.\nA new MAJOR version indicates that breaking changes are included with the version. A\nnew MINOR version means that additions and changes have been made, but the software\nis backward-compatible. A new PATCH version indicates that bug fixes are included with\nthe version and that the software remains backward-compatible. Following this versioning\nscheme makes, it easy to tell what type of changes are included in a new version, and\nprovides the ability to anticipate breaking changes. When your software is in its initial\ndevelopment phase, the version number typically starts with 0.1.0. From there, the minor\nversion can be incremented with each subsequent release.\n", "page": 475, "type": "text", "section": "Page 475"}
{"text": "DevOps and Software Architecture\nChapter 13\n[ 447 ]\nAs part of the version number, you can communicate the software's current stage of\ndevelopment. If you are using semantic versioning, the stage can be designated as part of\nthe pre-release identifier. Following the MAJOR.MINOR.PATCH part of the version\nnumber, you can signify that it is a pre-release version of the software by following the\nnumber with a hyphen and any series of identifiers, separated by dots. Any version\nnumber that contains a hyphen and one or more identifiers denotes that it is a pre-release\nversion, regardless of the value of the identifiers. Some valid examples of version numbers\nwith pre-release identifiers include 1.5.2-beta, 1.6.3-alpha, 1.6.3.-alpha.1, 1.3.2-0.0.1, and\n1.3.2-z.7.x.23. Pre-release versions of software may be unstable and have a lower\nprecedence than normal versions.\nAutomated testing\nAutomated tests should be executed as part of the automated build to ensure that no\ndefects have been introduced since the last successful build. Automated testing, along with\nautomated builds, allows the development team to validate the merged changes quickly.\nWith continuous integration, errors are detected sooner, making them easier to resolve.\nChangesets tend to be smaller in size, and there will be fewer of them to integrate at once.\nThis makes it easier to identify and resolve any issues that exist for a particular build. By\nintegrating work continuously, we avoid a situation in which a large number of changes are\nmerged at once.\nIt is important to have automated tests when practicing continuous integration. Although it\nis technically possible to practice continuous integration without the use of automated tests,\nhaving automated tests will increase quality and lessen the amount of time that quality\nassurance must spend using manual testing techniques. Automated tests can quickly\ninform developers when new commits have caused an issue.\nThe results of a build should be made available to the team so that it is easy to see whether\nthere is a build break. In such cases, the appropriate developer can easily be made aware so\nthat the issue can be fixed. If there is a situation in which some changes are reverted\nbecause of an issue, the amount of changes that are lost is minimized due to the continuous\nintegration of the team.\nOne thing to consider with continuous integration is that writing a complete set of\nautomated tests does take effort. The quality of the tests must be high or defects may be\nintroduced into the system that go undetected for a period of time.\n", "page": 476, "type": "text", "section": "Page 476"}
{"text": "DevOps and Software Architecture\nChapter 13\n[ 448 ]\nContinuous delivery (CD)\nContinuous delivery (CD) is the ability of an organization to release changes to users\nquickly and in a sustainable, repeatable way. The software is released in short cycles, which\nreduces the risk, cost, and effort of delivering changes to customers. Organizations that\npractice continuous delivery have automated their testing as well as their build process.\nThe aim of continuous delivery is to have your software application in a state where it can\nbe deployed to production at any time. This is an important part of DevOps. Continuous\ndelivery includes all of the major activities that take place in a DevOps release cycle. In\naddition to development, integration, and an automated build with automated unit testing,\nwhich are all part of continuous integration, continuous delivery also includes automated\ndelivery to staging, automated acceptance testing, and deployment to production:\nWith continuous delivery, as opposed to continuous deployment, which we will cover next,\ndeployment to production is a manual process. The organization can decide to release at\nwhatever time interval they want (for example, daily, weekly, or monthly). However, the\nbenefits of continuous delivery are increased if deployments take place as early as possible.\nChanges will be made available to users sooner and feedback can be received sooner. Each\nset of changes is smaller, making it easier to troubleshoot an issue if there is a problem.\nAfter deployment to production, post-deployment tests are typically executed. The extent\nof the post-deployment testing varies, but at the very least it should consist of smoke\ntesting. A smoke test ensures that all crucial functionality is working properly in the target\nenvironment. A smoke test is not as exhaustive as full software testing, but it can be\nperformed quickly to determine whether the deployment in the target environment is\nstable.\nContinuous deployment\nContinuous deployment essentially takes continuous delivery one step further. Like \ncontinuous delivery, continuous deployment aims to enable organizations to release\nchanges to software systems quickly and reliably. It differs from continuous delivery in that\nit automates the deployment to production.\n", "page": 477, "type": "text", "section": "Page 477"}
{"text": "DevOps and Software Architecture\nChapter 13\n[ 449 ]\nContinuous delivery ensures that the software application can be delivered to staging,\nalong with the execution of rigorous tests, using automation. When done properly, the\nsoftware application should be in a state that it can be deployed to production at any time.\nContinuous deployment merely automates the final step so that all changes are\nautomatically deployed to the production environment.\nWhen the organization, the business, and the nature of the software application allow for it,\ncontinuous deployment is an ideal DevOps goal in that it automates the process from\nbeginning to end, and is the quickest way to move application changes into production.\nHowever, in some cases, continuous deployment is not practical. There may be business\nreasons that an organization does not want to automatically deploy all changes to\nproduction. Some organizations prefer that the final step, the deployment to production, be\na manual process. Software architects, along with other key decision makers within the\norganization, will have to decide whether continuous deployment is appropriate for the\norganization.\nArchitecting for DevOps\nAs a software architect, you should consider DevOps when making architectural decisions.\nSome of the DevOps practices that we have discussed are independent of architecture and\ndo not require specific architectural decisions to be made. However, certain architectural\napproaches may be required in order to realize the full benefit of some DevOps practices.\nImportant quality attributes for DevOps\nSoftware systems that have been architected for DevOps should place importance on\nquality attributes, such as testability, deployability, and maintainability. Testability is a\nvalued quality attribute because the ability to test the system is crucial, particularly since\nthere will be automated tests. In order to architect a system to be testable, components need\nto be isolated from each other so that they can be tested independently. Each component\nmust be controllable so that we can dictate input and how that input will exercise the\ncapabilities of the component. It must be possible to observe the input and output of\ncomponents so that it is possible to determine whether it is working properly.\n", "page": 478, "type": "text", "section": "Page 478"}
{"text": "DevOps and Software Architecture\nChapter 13\n[ 450 ]\nIn order to make and deploy changes quickly, the system must be highly maintainable.\nMinimizing complexity as much as possible aids us in achieving this goal. It makes it easier\nto understand how the system works and how to implement changes. Minimizing\ncomplexity enables shorter cycle times because small, incremental changes are easier to\nmake. Reducing the size of components, increasing cohesion, and reducing coupling all\nhelp to make a system more maintainable. A system that is less complex and more\nmaintainable is also easier to deploy and test.\nContinuous delivery (and continuous deployment, if that is being practiced) requires a\nsoftware system to be architected for deployability. Deployability is a measure of how\neasily and reliably a software system can be deployed from staging (or development) into\nproduction. Even if a system is testable, if changes cannot be pushed to production\nproperly, then the utility of continuous delivery is lost. Increased deployability of a\nsoftware system will shorten deployment time and decrease software system downtime.\nMinimizing the difference between environments can go a long way in improving\ndeployability. Staging and production should be as similar to each other as possible. When\nthey are, successful delivery to a staging environment is a good predictor of a successful\ndeployment to production.\nDeployability is increased when an application and its configuration are kept separate.\nConfiguration information that can vary from one environment to another (for example,\ndatabase connection information) should be kept in external configuration and not be part\nof the application's code.\nIn order to enable DevOps, a software application needs to exhibit these types of quality\nattributes. Software projects that do not practice DevOps may not place an emphasis on\nthese types of requirements. However, DevOps requires software architects and\ndevelopment teams to place more of a focus on them.\nOperations should be considered when determining requirements. Requirements for robust\nmonitoring and logging can help operations to detect faults quickly and record information\nthat can be used to diagnose issues. Intrusive architectural changes are typically not\nnecessary to add these types of capabilities, but they are important when changes to a\nsoftware system are being made quickly and often. Software architects should design for\nfailure. In other words, we should expect that failures will occur, but fault tolerance should\nbe part of the system so that recovery can be completed quickly in order to minimize\ndowntime.\n", "page": 479, "type": "text", "section": "Page 479"}
{"text": "DevOps and Software Architecture\nChapter 13\n[ 451 ]\nSome architecture patterns complement DevOps\nSome software architecture approaches, such as the microservice architecture pattern, lend\nthemselves well to the types of requirements we have discussed for DevOps. It is not\nuncommon for an organization that is moving to either DevOps or microservices to then\nadopt the other. A microservice architecture works well with continuous delivery.\nYou may recall from Chapter 8, Architecting Modern Applications, that a microservice\narchitecture consists of small, focused services with well-defined interfaces. The fact that\neach microservice focuses on a small piece of functionality makes it easier and less risky to\nmake changes. Well-defined interfaces facilitate the swapping out of one microservice\nimplementation for another one. As long as the interface remains the same, the\nimplementation can change.\nEach microservice should be autonomous and independently deployable. These qualities\nallow an organization to make changes to one microservice and deploy it without affecting\nother microservices. Each microservice can have their own independent data storage,\nfurther making them independent from other ones. The downtime to deploy a single\nmicroservice can be a quick and low-risk process.\nMicroservices provide better fault isolation than other architecture patterns, such as a\nmonolithic architecture. If one microservice fails, it does not mean the entire software\nsystem will go down. Proper monitoring gives operations the ability to notice faults\nquickly, and if a recently deployed microservice is causing an issue, the team can revert\nback to the previous version.\nDevOps does not require microservices, but the point is that certain architectural patterns\nwork better than others with DevOps practices. Therefore, software architects must\nconsider DevOps when making design decisions and creating architecturally significant\nrequirements.\nDeploying to the cloud\nTransitioning to DevOps is commonly aligned with the use of the cloud. Many of the core\nvalues we have discussed, such as quick delivery and automated processes, are enhanced\nby deploying applications to the cloud.\nIn order to use the cloud in conjunction with DevOps, software architects should\nunderstand the different cloud types that are available, as well as the main cloud models. In\nthis section, we will be exploring both of these topics in detail.\n", "page": 480, "type": "text", "section": "Page 480"}
{"text": "DevOps and Software Architecture\nChapter 13\n[ 452 ]\nCloud types\nThe three main types of deployment for cloud resources are a public, private, and hybrid\ncloud. Although they provide similar benefits, an organization must select the one that\nmost matches its business needs.\nPublic cloud\nA public cloud consists of cloud resources that are owned and operated by a third-party\ncloud provider. The resources and services are provided over the Internet and shared with\nother organizations. Public clouds are often multi-tenant, meaning that an organization's\napplications and data are hosted on the same hardware and networks with other\norganizations.\nAn organization will experience high reliability and practically unlimited scalability from\nthe large number of resources that cloud providers own and manage. Organizations only\nhave to pay for the services that are used and don't have to worry about the maintenance of\nthe resources.\nAlthough public clouds provide the best economies of scale among the different cloud\ntypes, it may not be the appropriate choice for some organizations. For data that is overly\nsensitive and/or subject to regulations, a public cloud may not meet all of a software\napplication's requirements. With a public cloud, you lose some control over your data,\nwhich raises regulatory and compliance concerns regarding data storage and privacy. One\nexample of this is the Health Insurance Portability and Accountability Act (HIPAA) of\n1996, which protects patient information. You must be aware of regulations that may affect\nyour software, and ensure that either your team or the cloud provider can meet those\nrequirements.\nPrivate cloud\nA private cloud is made up of resources that are used exclusively by one organization. The\nphysical resources can either be located at an organization's own data center or that of a\nthird-party provider. Either way, all of the infrastructure is dedicated to a single\norganization. In contrast with a public cloud, it is a single tenant implementation.\n", "page": 481, "type": "text", "section": "Page 481"}
{"text": "DevOps and Software Architecture\nChapter 13\n[ 453 ]\nPrivate clouds are costlier with more modest economies of scale. However, an organization\nwill have greater flexibility in customizing the environment to meet its needs. One of the\nmain reasons to opt for a private cloud is the greater level of control and the stronger\nsecurity that is possible. Organizations can take advantage of cloud services while\nmaintaining more control over their enterprise data, which is sometimes a requirement for\nsecurity and privacy reasons.\nHowever, a private cloud can be implemented by the organization or through a third-party\ncloud provider. When using a cloud provider, an organization can still benefit from high\nlevels of reliability, efficiency, and scalability, just as they would with a public cloud. The\nmain benefit of an on-premises approach is that an organization has complete control over\nthe processes, data management policies, and physical resources.\nA software architect should be mindful though, particularly if a private cloud is being\nchosen for security reasons, that a public cloud can be just as secure or even more secure\nthan a private one if the private one is not implemented properly. A security hole turns a\nprivate cloud into a public vulnerability. If an organization implements their own private\ncloud, it is fully responsible for all aspects of it. Many of the steps required to implement a\nprivate cloud raise security issues that must be considered and addressed.\nCloud providers know that a secure cloud environment is essential to their business, so\nthey devote resources to ensure that their environments are secure. Among the top\npriorities and core competencies of a cloud provider is security, which is typically not the\ncase for organizations attempting to implement their own private cloud. For small and\nmedium-sized businesses, it is difficult to achieve the same levels of reliability, efficiency,\nand scalability as a major cloud provider. Ultimately, though, deciding between a public\ncloud, on-premises infrastructure for a private cloud, and having a third party host a\nprivate cloud really depends on the needs and long-term strategy of your organization.\nHybrid cloud\nA hybrid cloud is a combination of the public and private cloud types. For some\norganizations, it is a best of both worlds approach. With a hybrid cloud, an organization can\nhave the benefits of a private cloud, but can still leverage the public cloud when it is\nneeded.\n", "page": 482, "type": "text", "section": "Page 482"}
{"text": "DevOps and Software Architecture\nChapter 13\n[ 454 ]\nFunctionality that has higher volume and lower security needs could be hosted in a public\ncloud, while more mission-critical functionality and sensitive data can be served from a\nprivate cloud. Using a hybrid cloud can give an organization the control it needs from a\nprivate cloud with the flexibility to leverage the public cloud where it makes sense to do so.\nFor organizations with on-premises infrastructure, a hybrid approach allows it to transition\nto the cloud gradually. Legacy applications that have particular environment needs could\nbe hosted on-premises while other applications are hosted in a public cloud.\nA hybrid cloud approach allows for cloud bursting, also known as the burst compute\npattern. The concept is that an application will run in a private cloud until it experiences a \nspike in demand that is large enough that it bursts into the public cloud.\nThe burst compute pattern is used when an application needs additional compute\ncapability on an as-needed basis. Keeping hardware in-house to handle periodically\nincreased needs can be costly, and the burst compute pattern is more cost-effective. It\navoids having idle, over-provisioned resources. The public cloud handles the extra\ndemand, and you are only charged for extra computing power when you need it.\nA hybrid cloud can also come in handy for predictable outages, such as scheduled\nmaintenance, rolling brown/blackouts, and natural disasters, such as hurricanes. Traffic can\nbe handled by the public cloud during these periods.\nCloud models\nThe services that are provided for cloud computing can be categorized into different cloud\nmodels. They are as follows:\nInfrastructure as a Service (IaaS)\nContainers as a Service (CaaS)\nPlatform as a Service (PaaS)\nServerless/Function as a Service (FaaS)\nSoftware as a Service (SaaS)\n", "page": 483, "type": "text", "section": "Page 483"}
{"text": "DevOps and Software Architecture\nChapter 13\n[ 455 ]\nThe models differ in their level of abstraction and the amount of control and responsibility\nthat you have:\nAs you move to the right in the diagram, the level of abstraction is higher, and you have\nless control and fewer responsibilities in regards to the infrastructure. Having fewer \nresponsibilities in terms of the infrastructure means that you can then devote more time to\nwriting the actual application logic and adding more business value for your customers.\nInfrastructure as a service (IaaS)\nOn the far left of the spectrum is Infrastructure as a Service (IaaS), in which you rent the\nhardware that you need. You are not responsible for maintaining the hardware that you\nuse as it is taken care of for you by the provider. Things such as the storage drives, network\ncomponents, and cooling systems are maintained for you.\nHowever, you are responsible for just about everything else. In addition to your own\napplication, you have to purchase, install, configure, and maintain things such as the\noperating system (OS), patches and security updates, anti-virus software, and middleware.\nYou are also responsible for server and logical network configuration. Although you have\nquite a bit of control, you need to devote time to the management of these resources.\n", "page": 484, "type": "text", "section": "Page 484"}
{"text": "DevOps and Software Architecture\nChapter 13\n[ 456 ]\nCommon reasons to use IaaS include website hosting, web apps, test/development\nenvironments, storage/backup/recovery needs, and big data analysis. An important\nadvantage of IaaS is that it eliminates the capital expenditure on hardware. The cost of\nongoing maintenance for that hardware is also reduced. Not having to devote resources to\nthe purchase and maintenance of hardware allows organizations to focus on their core\nbusiness. It also increases organizational agility as teams can respond quickly to new\nmarket opportunities and release applications faster.\nWhen organizations first adopt cloud technologies, IaaS is a common starting point as it\nmakes a migration to the cloud fast and easy. IaaS is typically similar to how organizations\nalready run their information technology (IT) departments, further easing the transition.\nContainers as a Service (CaaS)\nOne of the application development and deployment models used for cloud-native\napplications is Containers as a Service (CaaS). It builds upon IaaS by adding a container\norchestration platform, such as Kubernetes, Docker Swarm, or Apache Mesos. With CaaS,\ndevelopers and IT staff work together to build, ship, and run applications.\nCloud-native applications are containerized and dynamically orchestrated. A CaaS\napproach gives development teams control over how their application and dependencies\nare packaged, making them fully portable. Containerization allows applications to run\nanywhere. Teams can deploy the application to different environments without needing to\nreconfigure them. Containers can run on virtual machines (VMs), on a developer's\nmachine, in a private cloud with on-premises equipment, or in a public cloud.\nHaving the ability to dictate the dependencies of the application, along with their specific\nversions, and have them deployed along with their configuration provides a consistent\nenvironment in which your application will execute. It gives you predictability as to how\nthe application will behave, and increases reusability. It's a great way for a development\nteam to deploy their cloud-native, containerized application to the cloud.\nWhile this model gives quite a bit of control to development teams, it does mean that there\nare greater responsibilities. In a similar way to IaaS, you are still responsible for the\noperating system, patches, and security updates for the OS, logging/monitoring, capacity\nmanagement, and the scaling of the application.\n", "page": 485, "type": "text", "section": "Page 485"}
{"text": "DevOps and Software Architecture\nChapter 13\n[ 457 ]\nThe difference between CaaS and IaaS is the use of containers rather than virtual machines:\nVMs are commonly used with IaaS as a way to run multiple applications on a single server\nand isolate them from each other. When using virtual machines under IaaS,\na hypervisor runs the virtual machines as guests. Hypervisors are situated between the OS\nand the hardware, forming the virtualization layer. Pools of hypervisors have the ability to\nsupport and manage large numbers of virtual machines.\nThe machine (host) has a full copy of the operating system, but so does each virtual\nmachine that is running an instance of your application. The application and operating\nsystem are bundled together, such that each application has a copy of the operating system\nfor themselves. It allows a single physical machine to host multiple applications and have\neach one be isolated from the others. Utilization of server resources increase and costs\ndecrease when sharing a physical machine, as compared to having a separate machine for\neach application.\nWhile the first generation of cloud applications was made possible by VMs, the advent of\ncontainers improved upon some of the characteristics of VMs. With containers, the rough\nequivalent of a hypervisor is the container engine. Applications are still isolated from each\nother, but instead of virtualizing the hardware stack as is done with VMs, a container\nengine provides OS-level virtualization. Resources such as the CPU, memory, network\nresources, and storage are all virtualized at the OS level, allowing each container to have a\nview of the OS that is logically isolated from other containers.\n", "page": 486, "type": "text", "section": "Page 486"}
{"text": "DevOps and Software Architecture\nChapter 13\n[ 458 ]\nContainerization is much more lightweight than using a VM because the OS kernel is\nshared. In contrast with VMs, which each have a copy of the OS, there is only one full copy\nof the operating system. As a result, containers have much lower overhead. They can start\nup very quickly, use less space on the disk, and use up far less memory. It is possible to put\nmany more containers on a single server compared with virtual machines.\nVirtual machines can be slow to start up and shut down since they come with a large\noverhead that includes a full copy of the operating system. This can lead to slow recovery\nshould one fail. With a container engine and an orchestrator, the placement of containers\ncan be dynamic and spread out, allowing for rapid recovery if there is a failure of the\ninfrastructure or the application.\nPlatform as a Service (PaaS)\nPlatform as a Service (PaaS) provides development teams with a complete platform for\ndeveloping and deploying applications in the cloud. This cloud computing model provides\na higher level of abstraction than IaaS and CaaS. The hardware is provided and maintained\nas it is with IaaS, but there isn't as much interaction with the infrastructure. The platform\nabstracts away this interaction with everything that it provides.\nUnlike IaaS, the operating system is provided and maintained for you, including the\ninstallation of patches and security updates. With PaaS, the development team does not\nhave to manage the OS. In addition to the OS, providers make available a number of\nsupporting software, tools, services, database management systems, and middleware.\nThe deployment and management of an application are easier, as an increased number of\nservices are provided to assist with the common needs of application development and\ndeployment. The availability of supporting services can greatly assist a DevOps\norganization. Having hardware, the OS, and other supporting software maintained for\nthem saves organizations time and allows them to focus their resources on more important\nthings, such as using their domain knowledge to build application code. It reduces\ndevelopment time and enables capabilities without adding any additional staff or time to\nthe project. \nOne of the disadvantages with PaaS to be aware of is the fact that you lose control over the\nOS. There is also the potential for provider lock-in if an application relies on many of the\nspecific tools and technologies that are made available by the provider.\nIn the early days of PaaS, the number of programming languages that a cloud provider\nsupported might be limited. However, most cloud providers today support a variety of\nprogramming languages and runtimes.\n", "page": 487, "type": "text", "section": "Page 487"}
{"text": "DevOps and Software Architecture\nChapter 13\n[ 459 ]\nServerless/Function as a Service (FaaS)\nServerless architecture was covered in Chapter 8, Architecting Modern Applications, but let's\ndifferentiate it from PaaS. You may be thinking to yourself that serverless seems a lot like\nPaaS. Although there are similarities between PaaS and serverless solutions, serverless is\nnot PaaS.\nWith PaaS, you need the ability to forecast your demand so that you can provide (and pay\nfor) the capacity to meet your forecast. As a result, you still have to concern yourself with\nthe amount of compute capacity you will need.\nUnlike PaaS, with a serverless architecture, you will be charged based on a per-execution\nmodel and not for the time that the code is hosted. This is one of the main differences\nbetween PaaS and serverless. Serverless architectures are also designed to be capable of\nbringing an entire application up and down based on a request. You cannot under or\noverprovision your compute capacity with serverless. You will no longer find yourself in a\nsituation where you do not have enough capacity during peak times, and during off-peak\ntimes your servers sit idle while you get charged.\nSoftware as a Service (SaaS)\nOn the far right of the spectrum, we have Software as a Service (SaaS), which is sometimes\nreferred to as on-demand software. SaaS makes cloud-based software available to users\nover the Internet. With SaaS, everything is provided for you, including the software itself.\nThe only thing you are responsible for is the configuration of the software. Unlike other\nsoftware pricing models, customers don't have to pay for licenses to use the software. SaaS\nsoftware is commonly charged using a subscription model based on a timeframe and the\nnumber of users.\nExamples of SaaS are Salesforce, a customer relationship management (CRM) product,\nGoogle's Gmail, Microsoft's Office 365 productivity suite (Word, Excel, PowerPoint,\nOutlook, OneDrive), and storage solutions such as Dropbox. These are all hosted in the\ncloud and made available to users without them being responsible for anything other than\ntheir configuration.\nThe software is not installed on the user's own machine, which simplifies both maintenance\nand support. Technical staff is not required to install, manage, or upgrade the software.\n", "page": 488, "type": "text", "section": "Page 488"}
{"text": "DevOps and Software Architecture\nChapter 13\n[ 460 ]\nSummary\nOrganizations can gain a competitive advantage by delivering software applications at high\nvelocity, adding incremental value for their customers frequently and quickly. Having the\nability to reliably deliver new improvements of software products to customers on a\nregular basis can differentiate an organization from its competitors.\nThe culture, practices, and technologies of DevOps can enable organizations to achieve\nthese goals. Continuous integration, with automated builds and testing, allows changes to\nbe validated quickly. Frequent check-ins make it easier to detect and resolve any problems.\nThe practice of continuous delivery keeps software systems in a state where they can be\ndeployed to production at any time. This type of organizational agility provides the option\nof releasing changes to users quickly and in a repeatable way.\nSome DevOps practices do not require any architectural changes, but the needs of DevOps\nshould be considered during requirements, as well as when making architectural design\ndecisions. Consideration must be given to quality attributes, such as testability,\ndeployability, and maintainability, when architecting a solution.\nIn the next chapter, we will learn what it takes to work on and with legacy applications.\nRefactoring and integrating legacy applications bring unique challenges that a software\narchitect must be prepared to face.\n", "page": 489, "type": "text", "section": "Page 489"}
{"text": "14\nArchitecting Legacy\nApplications\nThe prevalent use of legacy software applications means that many software architects will\nend up working on one at some point in their career. As part of learning how to become a\ncomplete software architect, time needs to be invested in learning how to properly handle\nlegacy software systems.\nWe will learn what a legacy application is and how to refactor it. In addition to a legacy\napplication's code, we will examine other aspects of a legacy application that can be\nmodernized, including the software development methodology, the build process, and the\ndeployment process. The chapter will conclude by looking at topics related to integrating\nwith a legacy application.\nIn this chapter, we will cover the following topics:\nLegacy applications\nRefactoring legacy applications\nMoving to an agile approach\nModernizing build and deployment processes\nIntegrating with legacy applications\nLegacy applications\nMany of us enjoy working on greenfield software systems, which are completely new\nsystems that we can design from the ground up. Such systems do not have constraints\nbased on prior work and do not need to integrate with existing systems. However, the new\napplications of today eventually become the legacy systems of tomorrow.\n", "page": 490, "type": "text", "section": "Page 490"}
{"text": "Architecting Legacy Applications\nChapter 14\n[ 462 ]\nSoftware architects will often find themselves working on existing software systems and\nbeing able to do so well is a valuable skill. A legacy application is an existing application\nthat is still in use but is difficult to maintain. There are a number of challenges with\nmaintaining and using a legacy application.\nIssues with legacy applications\nWorking with a legacy application usually brings with it various problems that need to be\novercome. This is a part of the challenge when working with a legacy system. Perhaps most\nimportantly, a legacy application tends to be difficult to maintain and extend. It may use\nolder, possibly outdated, technologies and it may not follow best practices for software\ndevelopment.\nA legacy application tends to be older, and over the life of the project many different\ndevelopers may make modifications to it. Software entropy, or disorder in a software\nsystem, increases over time. This concept comes from the Second Law of Thermodynamics,\nwhich states that the level of entropy either stays the same or increases but it does not\ndecrease. As modifications continue to be made on a codebase, the code can become brittle\nand messy, resulting in spaghetti code. If you have ever needed to make a modification to a\nlegacy application but were afraid of breaking something, you know firsthand the\ndifficulties of dealing with a legacy application.\nA legacy system will almost certainly bear some amount of technical debt. Any design\ndecisions that have been made for an application, in which an easier, quicker solution was\nselected over one that was a cleaner solution but would have taken longer to implement,\nincurs technical debt. Similar to a financial debt, technical debt is the cost of these decisions,\nsuch as the cost of any rework necessary to improve the system. Technical debt increases\nsoftware entropy.\nDeciding to incur technical debt is not necessarily a bad thing. Sometimes it makes sense to\ntake the easier approach. For example, you may want to complete a feature faster to take\nadvantage of a market opportunity. Whatever the reasons are, though, a software architect\nshould realize that a legacy system will come with technical debt.\nSome legacy applications rely on older versions of operating systems, other software,\nand/or hardware. These environments can become increasingly difficult to maintain over\ntime. It also increases the possibility of vulnerabilities if security patches are either not\navailable or not applied.\n", "page": 491, "type": "text", "section": "Page 491"}
{"text": "Architecting Legacy Applications\nChapter 14\n[ 463 ]\nA legacy application may be one that has been inherited from another development team,\nwho may no longer be available to answer questions about it. This may lead to a general\nlack of knowledge about the system. Documentation and unit tests for a legacy application\nmay be lacking, making it more difficult to gain an understanding of the application and to\nmake changes to it.\nWhy are legacy applications used?\nWhile there may be challenges to using legacy applications, enterprises continue to use\nthem for a variety of reasons. If they remain useful and still work as intended, an\norganization may see no reason to stop using them. In addition, the cost of replacing a\nlegacy system may not outweigh the benefits. For example, the difficulties and cost of\nreplacing a system that requires almost constant availability may be prohibitively high.\nReplacement of a large and complicated system can be a long and difficult process.\nRewriting a legacy system may be desirable for a development team, as it allows them to\nwork on a greenfield system, but it may not be a good decision from a business perspective.\nReplacing a legacy system with a new system that performs the same functionality will cost\nan organization money while potentially not adding much new business value. A\ndevelopment team can certainly point out that a rewritten application will have increased\nmaintainability and alleviate the other types of issue with legacy applications that we just\ncovered. These benefits can result in cost savings over time, depending on how long the\nsystem will be in use. However, rewriting a legacy application, particularly a complex one\nthat currently works, can be a tough sell to Management when it may not make financial\nsense. Modernization of a legacy application, during which it is refactored with the purpose\nof improving it, can be a less expensive alternative that poses fewer risks.\nAnother reason why a legacy application might continue to be used is if it is not well\nunderstood. If the original development team is no longer with the organization, the system\nis complex, or there is a lack of documentation, it will take more effort to replace the\nsystem. This type of barrier can be yet another reason why rewriting an application is put\noff.\n", "page": 492, "type": "text", "section": "Page 492"}
{"text": "Architecting Legacy Applications\nChapter 14\n[ 464 ]\nMore than just code\nSoftware architects should understand that a legacy application does not just consist of the\ncode. A legacy software system includes requirements related to the environment needed\nfor it to run, such as any dependencies with particular versions of an operating system. It\nmay require particular tools or certain versions of a tool, such as source code editing, build,\nversion control, code coverage, code review, debugging, integration, documentation, static\ncode analysis, and unit testing tools. Some of the tools required by a legacy application may\nnot be supported any longer.\nA legacy software system is also made up of its dependencies, such as those for other\nsystems, third-party software, frameworks, and packages. Similar to tools, these\ndependencies can become outdated or may no longer be supported. Finally, a legacy\nsystem also includes its internal and external documentation. The documentation for some\nlegacy systems is either sorely lacking or outdated. A combination of all of these factors\nmakes up a legacy system and its overall level of quality. Each one, to some degree, affects\nquality attributes such as the maintainability of the overall system.\nWhere legacy systems exist, software architects overseeing them may need to be involved\nwith refactoring, replacing, or integrating with them.\nRefactoring legacy applications\nWhen you begin working on a legacy application, you will want to refactor it in order to\nmake it more maintainable. You may need to implement new features, fix defects, improve\nthe design, increase quality, or optimize the application. In order to perform these types of\ntask, the legacy system must be in a state where changes can easily be made and without\nmuch risk.\nIn the classic book Refactoring: Improving the Design of Existing Code, author Martin Fowler\ndefines refactoring as \u2026the process of changing a software system in such a way that it does not\nalter the external behavior of the code yet improves its internal structure. Any refactoring that is\nperformed should safely improve the code in some way, without affecting the business\nlogic and expected functionality.\nBefore making any changes, it is helpful to have the right attitude when approaching a\nlegacy codebase. All too often, a software architect or developer will be highly critical of a\nlegacy application before even fully understanding the codebase. You should have respect\nfor the original development team because there may be reasons why things were done a\ncertain way and you may not always be aware of all of the decisions that took place and the\nrationale behind them.\n", "page": 493, "type": "text", "section": "Page 493"}
{"text": "Architecting Legacy Applications\nChapter 14\n[ 465 ]\nAs a software architect, you want to modernize and improve the legacy application that\nyou have been tasked with overseeing. You do not want to focus on making unnecessary\nmodifications, particularly if you do not yet fully understand the impact of the changes. For\nexample, rather than making changes because you do not like how something was coded\nstylistically, you should focus your attention on making substantive changes that will have\na positive impact on the codebase. \nRefactoring a legacy application with the purpose of modernizing and improving it\nincludes performing the following tasks:\nMaking legacy code testable\nRemoving redundant code\nUse tools to refactor the code\nMaking small, incremental changes\nTransforming monoliths to microservices\nMaking legacy code testable\nMany legacy software systems lack automated unit tests and only some of them have\nadequate code coverage. What may be even more problematic is that some legacy systems\nhave not been developed with unit testing in mind, making it difficult to add tests later.\nAdding unit tests to a legacy system that does not have them should be given a high\npriority. There are a number of benefits to making legacy systems (and all software\nsystems, for that matter) unit-testable.\nBenefits of unit testing\nOne of the biggest benefits of using unit tests on a legacy system is that it facilitates making\nmodifications to the system, particularly for individuals who may not be familiar with the\nsystem. As you make changes to a legacy application, unit tests will ensure that the changes\ndid not introduce new defects and that the functionality still works properly.\nRegularly executing unit tests, such as after a change is made or as part of the build process,\nwill make debugging any issues that are found easier. A developer will be able to narrow\ndown the source of the problem to one of the recent changes.\n", "page": 494, "type": "text", "section": "Page 494"}
{"text": "Architecting Legacy Applications\nChapter 14\n[ 466 ]\nThe documentation for legacy applications may not be suitable but if the system has a good\nsuite of unit tests, the tests can serve as a source of documentation. They help team\nmembers understand the system and allow them to learn what a particular unit of code was\ndeveloped to do. When unit tests are lacking, just the act of writing the unit tests will help\nthe team become more familiar with the codebase.\nRefactoring for unit tests\nThe application may not have been designed to be unit-testable. Ideally, unit tests would be\nin place prior to performing any refactoring work. However, as a software architect, you\nmay be faced with a dilemma if you need to refactor an application just so you can write\nunit tests.\nWhen faced with a situation in which automated unit tests do not already exist, some initial\nrefactoring may be necessary, just to allow for the creation of the initial set of tests. From\nthat point, further refactoring and the addition of more unit tests will be possible.\nAnother approach for this situation is to write integration tests. Integration tests can then be\nexecuted before any refactoring changes to confirm that the original functionality works as\nintended. As refactoring work takes place, the integration tests can be executed often to\nensure that nothing was broken. Integration tests do not use mocks or stubs but rather\nexecute logic in dependencies. For this reason, they do require some more setup to ensure\nall of the components work. One added benefit of this approach is that once you are done,\nyou will already have integration tests to go along with your unit tests.\nOnce unit tests have been written, they must be maintained going forward. As code is\nadded or modified (for example, to implement a new feature or fix a bug), unit tests should\nbe added or modified, as necessary, to keep the test suite up to date.\nWhere to start writing tests?\nIt can be difficult to know where to start when introducing unit testing in a legacy\napplication. One approach is to start with the logic that is related to the most critical\nbusiness functionality. With this approach, you will have unit test coverage for the most\nimportant components first.\nAnother approach is to consider the level of complexity of the components. Some people\nprefer to get the most complicated work out of the way first. An experienced development\nteam may prefer this approach. Others would prefer to begin with less complex\ncomponents and build up to the ones that will require more work. This approach may be\nmore suited to a less experienced team.\n", "page": 495, "type": "text", "section": "Page 495"}
{"text": "Architecting Legacy Applications\nChapter 14\n[ 467 ]\nWhen using the complexity level as a factor when deciding where to start, it doesn't have to\nbe one way or the other. Some members of the team could begin with highly complex\ncomponents while others start with less complex ones.\nRemoving redundant code\nAny software application may contain redundant code, but legacy applications, which are \nolder and are more likely to have been maintained by a variety of people, tend to have\nincreased instances of code that is either duplicated or no longer needed.\nWhen taking over a legacy application, a software architect should look to remove\nredundant code. Reducing the total lines of code minimizes complexity and makes the\nsoftware system easier to understand. Code analysis tools can help to identify some types\nof code that are unnecessary. Refactoring unreachable, dead, commented-out, and\nduplicate code will improve the maintainability of the system. Let's look at each of these\ntypes of code in more detail. \nUnreachable code\nUnreachable code is code that can never be executed, regardless of the conditions at\nruntime. There is simply no control flow path that will lead to the code being executed.\nCode can become unreachable for a variety of reasons. Some examples include developers\nforgetting to delete obsolete code, code that was intentionally made unreachable so that it\ncould potentially be used later, code strictly for debugging/test purposes that was not\nremoved, modifications to other code that unknowingly made code unreachable, business\nlogic/data changes that made code unreachable, and a programming error.\nStatic analysis tools can help you to find this type of code. If the unreachable code is the\nresult of a bug and it is determined after analysis that the code is actually needed, then the\ndefect should be corrected so that the code is no longer unreachable. However, if after\nanalysis it is confirmed that the code is unnecessary, then it should be removed. The\nremoval of unreachable code increases the overall maintainability of the software system. If\nunreachable logic was being kept in the code in case it would eventually be needed, version\ncontrol software can be used to obtain it again rather than leaving it in the code.\n", "page": 496, "type": "text", "section": "Page 496"}
{"text": "Architecting Legacy Applications\nChapter 14\n[ 468 ]\nDead code\nSome people use the terms unreachable code and dead code interchangeably but there is a\nsubtle difference. While unreachable code can never be executed, dead code can be\nexecuted. However, when dead code is executed it has no effect on the output. For\nexample, dead code might perform some logic that produces a result but then the result is\nnot used anywhere.\nCommented-out code\nCommented-out code is code that is currently not in use and has been commented out by a\ndeveloper rather than deleted. Code that is commented out is among the easiest types of\ncode to remove. Sometimes a developer may comment out code with the intention of it only\nbeing temporary but then does not follow up later to either remove or uncomment the\ncode.\nLines of code are sometimes commented out to serve as a record of previous logic.\nHowever, having a change history is one of the purposes of a version control system.\nLeaving code in the codebase that has been commented out only increases the size of the\ncode and makes it less readable.\nDuplicate code\nDuplicate code is identical (or very similar) code that exists in multiple places. The parts of \nyour codebase that are duplicated violate the Don't Repeat Yourself (DRY) principle. As\nwe learned in Chapter 6, Software Development Principles and Practices, following the DRY\nprinciple means eliminating duplication in our codebase. Duplication of code is wasteful\nand makes the code more difficult to maintain. Some of you may have had the experience\nof needing to make the same code change in multiple places because of duplicate code.\nAny instances of duplicate code should be abstracted out and placed in a single location.\nAll of the places in the codebase that need the logic can then be routed through the\nabstraction. Eliminating code duplication will increase maintainability and the quality of\nthe software. When the logic must be modified, it can be done in one place, eliminating the\nrisk of missing one of the locations or not making a consistent change when the code is\nduplicated.\n", "page": 497, "type": "text", "section": "Page 497"}
{"text": "Architecting Legacy Applications\nChapter 14\n[ 469 ]\nUsing tools to refactor\nWhen possible, take advantage of development tools available to you that will assist with\nrefactoring a legacy application. Your organization may already have licenses for an\nintegrated development environment (IDE) or other tools that can identify areas of the\ncodebase that can be refactored. Some of them will perform some types of refactoring for\nyou and provide you with a preview of the changes before it actually makes them. If your\norganization doesn't have a particular tool, it may be possible to purchase licenses for it\nbased on your recommendations. If such purchases are not going to be possible (or simply\nbecause there are better options which happen to be free), consider using an open-source\ntool.\nMaking small, incremental changes\nWhen refactoring a legacy application, some of the changes that you want to make may be\nlarge ones. However, keep in mind that in order to improve a legacy application, the\nchanges do not have to be large and they do not have to be made all at once. Refactoring a\nlegacy application can take time. Small, incremental changes can sometimes be the best\napproach to improving a legacy codebase. Write and execute unit tests to go along with\nyour changes to ensure that your modifications do not have unintended consequences.\nWith each refactoring, we want to leave the code better than it was before, without\nchanging any of the functionality.\nYou and your development team may not even be granted time to make improvements. If\none of you is tasked with some work, such as fixing a bug in the legacy application, take the\nopportunity to improve the area of the code that is being changed. Over time, more and\nmore parts of the code will be improved.\nTransforming monoliths to microservices\nYour legacy application may have a monolithic architecture. In Chapter 8, Architecting\nModern Applications, we learned that a monolithic application is designed to work as a\nsingle, self-contained unit. The code may have components that are tightly coupled and\nhighly interdependent. If the application is large and complicated, it can be difficult to\nmake changes to a monolithic application.\n", "page": 498, "type": "text", "section": "Page 498"}
{"text": "Architecting Legacy Applications\nChapter 14\n[ 470 ]\nOne approach to modernizing a legacy application is to begin introducing microservices.\nMicroservices are small, focused, autonomous services that are independently deployable.\nUnlike a monolithic application, the microservice approach reduces the level of complexity\nby partitioning an application into small services that are easier to maintain and manage. In\na legacy application, we can take pieces of logic and place them in microservices.\nPlaces in the legacy application that need the functionality contained in a microservice can\ninteract with it through its interface. Implementations of each service can be changed as\nlong as the interface remains the same. It also provides the ability for a development team\nto modify a microservice independently of other services. In contrast with modifications\nmade to a monolithic application, changes to individual microservices are less likely to\nrequire modifications in other places in the application.\nThere is better fault tolerance with microservices. If a legacy application has a monolithic\narchitecture, a fault could bring down a large part of the application or even the entire\napplication. When a single microservice fails, it will not cause the entire application to\ncrash. Combined with proper monitoring, which will alert administrators to a fault quickly,\na microservice can be restarted quickly. If there is an issue with the currently deployed\nversion of a microservice, a new version that contains a fix can be deployed without having\nto deploy the entire application. It also provides the option of reverting to a prior version of\nthe microservice if it does not contain the issue.\nMigrating to the cloud\nMigrating a legacy application to the cloud is another way that it can be modernized.\nAlthough not every legacy application will be compatible with every cloud service, there\nare migration paths that will allow you to take most legacy applications to the cloud.\nCertain cloud services will be more suited to a particular application.\nThere are a number of reasons why a legacy application will benefit from a move to the\ncloud. It can reduce costs while at the same time providing greater levels of availability and\nscalability. A cloud provider will be responsible for the hardware and infrastructure.\nDepending on the cloud model (for example, IaaS, PaaS, or FaaS), it may also be\nresponsible for the operating system and other services.\nSome legacy applications have security vulnerabilities because they rely on older hardware\nand software. If they are migrated to the cloud, the cloud provider can potentially handle\ntasks such as operating system updates, including security patches.\n", "page": 499, "type": "text", "section": "Page 499"}
{"text": "Architecting Legacy Applications\nChapter 14\n[ 471 ]\nThe 6 R's\nThe migration path to the cloud will vary from organization to organization and among\ndifferent applications. The concept of the 6 R's is used to describe different approaches to\ncloud migration. The 6 R's are:\nRemove (or retire)\nRetain\nReplatform\nRehost\nRepurchase\nRefactor (or re-architect)\nWe will now explore each of these concepts in further detail.\nRemove (or retire)\nWhen taking into consideration a particular application, one option is to simply remove it.\nSome applications that an organization is hosting might not even be needed anymore, but\nhave been kept running. If your organization is undergoing an overall migration to the\ncloud for a number of applications and services, this is an ideal time to evaluate your\ncurrent applications. You may be able to identify some that no longer need to be kept\navailable.\nRetain\nSome applications either cannot be migrated to the cloud or an organization may make a\ndecision to not migrate them. In these cases, you will simply be retaining the application in\nits current environment. There are a number of reasons why an application may simply be\nretained. There may not be a business justification for moving an application to the cloud.\nFor example, the cost of migrating to the cloud might be too high given the expected\nbenefits.\nOrganizations that want a hybrid cloud will retain some of their applications in their on-\npremises infrastructure. In a hybrid approach, some applications will be migrated while\nothers will remain hosted on-premise.\n", "page": 500, "type": "text", "section": "Page 500"}
{"text": "Architecting Legacy Applications\nChapter 14\n[ 472 ]\nReplatform\nSome legacy applications cannot be migrated to a cloud platform but an organization does\nnot want to simply retain them in their current environment. For this type of situation, the\napplication can be run on cloud-based IaaS servers using emulators. The application is\nemulated through a virtual machine to make it compatible with cloud technologies. This\napproach allows you to migrate to a newer platform/operating system and take advantage\nof its features. There are a number of tools and services available to assist with\nreplatforming.\nRehost\nLift and shift is a migration strategy that involves moving a software application from one\nenvironment to another without redesigning the application. Rehosting is a lift and shift\napproach in which an application and its data are copied from physical or virtual servers to\nan IaaS solution. The application does not need to be changed and the cloud provider hosts\nthe infrastructure. This is a quick, low-risk, and easy way to migrate an application to the\ncloud.\nRehosting can be the first step in a cloud migration for an application. Once the legacy\napplication is running in the cloud, it may be refactored later in order to optimize it for its\nnew environment. The fact that it is already hosted in the cloud may make that process\neasier.\nRepurchase\nRather than move a legacy application to the cloud, an organization might decide to\npurchase a newer product and start using that one instead. This option is commonly\nassociated with a move to a SaaS cloud model. Everything is provided for you with SaaS,\nincluding the hosting and the software. The organization only has to configure the software\nfor use.\nRefactor (or re-architect)\nWhen refactoring an application for a cloud migration, you are making modifications to the\napplication, possibly including its architecture. You may be refactoring the application to\ntake advantage of the cloud-native features available in its new environment.\n", "page": 501, "type": "text", "section": "Page 501"}
{"text": "Architecting Legacy Applications\nChapter 14\n[ 473 ]\nLegacy applications that are refactored properly will have greater availability, improved\nscalability, and faster performance. In addition to these advantages, an organization can\nrealize cost benefits after the application has been migrated. The refactoring/re-architecting\napproach can potentially be quite involved though. Depending on what needs to be done,\nthe migration process itself may take longer and cost more to execute.\nMoving to an agile approach\nA legacy application does not just entail the technology that comes with it. It was\ndeveloped using a particular software development methodology. In some cases, that\nmethodology might still be in use. If the methodology used is not a modern one, such as the\nuse of a Waterfall methodology, part of modernizing a legacy application might include\nchanging the development approach.\nThe agile methodology overcomes some of the limitations of older methodologies. While\ntraditional software development methodologies focus on a lot of up-front planning and\ndesign, an agile methodology expects and embraces change. Agile methodologies are\nadaptive rather than predictive. Rather than focusing on predicting the outcome, it places\nemphasis on adapting. It is more responsive to change, which is important in today's\ncompetitive landscape.\nAn agile methodology will enable your team to accomplish some of the things we have\ndiscussed to improve a legacy application, such as refactoring it by making small,\nincremental changes. Each sprint can be focused on particular goals to improve the legacy\napplication.\nIf you are introducing microservices, an agile methodology complements that approach\nwell. An agile approach provides a structure that will allow a development team to work\ntogether effectively. It is also particularly suited to modern build and deployment\nprocesses.\nModernizing build and deployment\nprocesses\nAnother aspect of modernizing a legacy application is to update the build and deployment\nprocesses. If outdated, these processes cannot be completed as easily or quickly as modern\nprocesses. A legacy application might even need to be built and deployed manually, which\ncan make it error-prone.\n", "page": 502, "type": "text", "section": "Page 502"}
{"text": "Architecting Legacy Applications\nChapter 14\n[ 474 ]\nIf the processes are complex and the documentation is poor, knowledge of the intricacies\ninvolved in the processes can be lost. There may be a reliance on certain individuals who\nknow all of the details about the processes, which can be problematic when those people\nare away from work or decide to leave the organization entirely.\nThe initial step in updating these processes is to gain a detailed understanding of the\ncurrent way that builds and deployments are done. You should also know about any\ndifferences that exist in the processes based on the environment (for example, development,\nstaging, or production).\nAutomating the build and deployment processes\nIf the processes are not already automated, serious consideration should be given to\nautomating them. Automation will allow builds and deployments to be completed quickly.\nIf there are many steps, mistakes can be made when conducting the processes. Automation\nensures consistency between builds and across different environments.\nWithout automation, there may be variation in the processes depending on who is doing\nthem. Developers and operations engineers may also take it upon themselves to automate\ndifferent aspects of the process and each may do so in a different way.\nAutomation provides quick feedback regarding any issues. It allows us to fail fast, which\nmeans we can also resolve any issue quicker. By combining automated builds with\nautomated testing, we further improve the process. Executing automated unit tests during\nthe build process will help to improve quality and allow us to become aware of any\nproblems in a timely manner.\nIn Chapter 13, DevOps and Software Architecture, we learned about the DevOps practices of\ncontinuous integration (CI) and continuous delivery (CD). These practices are possible\nwith automation, as will be explained in the following sections.\nPracticing continuous integration (CI)\nIf they are not already doing so, developers working on legacy systems should begin\npracticing CI. As we learned in the last chapter, continuous integration involves\ncommitting code changes on a frequent basis.\nContinuous integration will inform a developer of any issues with their code check-in\nearlier in the process. It will reduce the potential for conflicting changes. It will be easier to\nidentify and resolve any issues when the set of changes are smaller and they are merged\nmore frequently.\n", "page": 503, "type": "text", "section": "Page 503"}
{"text": "Architecting Legacy Applications\nChapter 14\n[ 475 ]\nPracticing continuous delivery (CD)\nA legacy application may not have been designed for CD. In Chapter 13, DevOps and\nSoftware Architecture, we learned that continuous delivery is the ability to release software\nchanges to users quickly and in a reliable way. The build and deployment processes of a\nlegacy application can be slow and tedious, particularly if they are performed manually.\nIf it is feasible to do so, a goal should be made to practice continuous delivery, so that the\nlegacy application will be in such a state that it can be deployed to production at any time.\nAutomating the build process and executing automated unit tests as part of that process is\nan important step in enabling continuous delivery.\nA continuous delivery process for a legacy application should build and deploy the\napplication to staging automatically. If the organization desires to do so, continuous\ndelivery of the software system can be taken a step further by practicing continuous\ndeployment. If continuous deployment is enabled, then the last step of the process is\nautomated: deployment to production.\nUpdating the build tools\nA legacy application may be using older build tools. Similar to other development tools,\nbuild tools are constantly evolving. If the legacy application you are working on is using an\noutdated build tool, there are probably better options available to you (or possibly newer\nversions of the tools already being used).\nSoftware architects should look to update outdated build tools to improve the build and\ndeployment processes. Good build and deployment tools can help you achieve your goals\nof practicing continuous integration and continuous delivery.\nIntegrating with legacy applications\nSoftware architects may need to integrate a legacy application with another software\napplication. If you are responsible for the legacy application, refactoring it may be\nnecessary in order to modernize it to the point that it can be integrated with newer\napplications.\nWhen integrating a legacy application with another application, some considerations\nshould be made by the software architect.\n", "page": 504, "type": "text", "section": "Page 504"}
{"text": "Architecting Legacy Applications\nChapter 14\n[ 476 ]\nConfirming the need for integration\nPrior to expending the effort to integrate with a legacy application, thought should be given\nto ensure that the integration is necessary. Depending on the situation, it may be easier to\nmigrate the functionality that is needed from the legacy application rather than move\nforward with integration.\nWhen deciding to migrate or integrate, consider the amount of functionality that is needed\nfrom the legacy system and the complexity of that functionality. If there is a large amount\nof functionality and/or the functionality is highly complex, then more effort will be\nnecessary to migrate it. It may be more cost-effective to integrate with the legacy system\nand to keep using it.\nAnother factor to consider is the long-term goals of the organization and the expected\nremaining lifetime of the legacy system. If there is a goal to retire the legacy system in the\nnear future, then perhaps it makes sense to begin the process sooner, rather than using\nresources to perform an integration.\nDetermining the type of integration\nThere are different types of integration possible and there should be an understanding of\nwhat type is needed based on the business drivers and requirements. Some integrations\nmay be required to take place in real time, where an action in one system triggers an action\nin an other system that must take place immediately. These types of integration can be\nmore complex and expensive to implement.\nIn some cases, near real-time integration is sufficient. Near real-time integration is still\nquick but processing may be measured in minutes rather than seconds. If there is no\nrequirement for such a quick action, batch integration may make the most sense. In contrast\nwith real-time or near real-time integration, batch integration may take hours, or even days,\nto complete. Batch processing can be scheduled and processing can be executed during off-\npeak hours.\nSome types of integration will be easier to implement and less disruptive to existing\nenvironments, so in addition to understanding business needs, having knowledge about\nthe current environments is important.\n", "page": 505, "type": "text", "section": "Page 505"}
{"text": "Architecting Legacy Applications\nChapter 14\n[ 477 ]\nSharing functionality between systems\nWhen a newer software application is integrated with a legacy one, some of the\nfunctionality may be duplicated across the two systems. Software architects should be\naware of any overlaps because decisions will need to be made as to which system will be\nresponsible for functionality that is shared. Any differences in shared business logic\nbetween the two systems should be identified, as those differences may play a factor in\nsuch a decision.\nAny logic that is redundant between the two systems is a candidate to be exposed and\nshared so that it is no longer duplicated. This will ensure consistency of the logic. Quality\nand maintainability will be increased as the logic will only need to be maintained and\ntested in one location.\nPerforming data integration\nWhen integrating and sharing data between two systems, we have to ensure that data can\nbe combined from disparate sources in a meaningful way. There should be an\nunderstanding of the data in both systems and what they represent. We need the ability to\nidentify whether or not a piece of data represents the same thing in both systems.\nData mapping between systems may be required to transform data from one system to\nanother. When possible, redundancy in data should be removed and decisions need to be\nmade as to which system will be responsible for certain pieces of data.\nSummary\nDespite the challenges of maintaining a legacy application, enterprises continue to use them\nfor a variety of reasons. Working on a legacy application is a common task for a software\ndevelopment professional. Most of us have worked on a legacy application, and if you have\nnot already, it is likely that you will at some point in your career. Software architects should\nbe knowledgeable about how to effectively oversee one.\n", "page": 506, "type": "text", "section": "Page 506"}
{"text": "Architecting Legacy Applications\nChapter 14\n[ 478 ]\nThere is typically a need to make modifications to a legacy application to fix bugs and add\nnew features. To make it easier for a development team to make these types of change,\nsoftware architects may seek to refactor legacy applications to make them more\nmaintainable. Software architects may also lead efforts to improve a legacy application by\nmigrating it to the cloud and modernizing its build and deployment processes.\nUp to this point, we have mainly focused our attention on technical topics. In the next\nchapter, we will learn the soft skills that software architects should possess. As you move\nfrom being a developer to a software architect, you will find that specific soft skills are\nuseful.\n", "page": 507, "type": "text", "section": "Page 507"}
{"text": "15\nThe Soft Skills of Software\nArchitects\nWhen thinking about the software architect role, many people focus on technical skills.\nHowever, possessing soft skills is essential to being a successful software architect. Soft\nskills can be the difference between an adequate software architect and an exceptional one.\nIn this chapter, we will gain an understanding of what soft skills are and explore some of\nthe soft skills that are useful to have for the software architecture role. These include\ncommunication, listening, leadership, and negotiation skills. We will also cover how to\neffectively work with remote team members.\nIn this chapter, we will cover the following topics:\nSoft skills\nCommunication\nLeadership\nNegotiation\nWorking with remote resources\nSoft skills\nHard skills are concrete skills that can be defined and measured. They are typically job-\nspecific and you can obtain them through education, training (both internal and external to\nthe organization), and certifications. For a software development professional, these would\ninclude technical skills such as knowing a particular programming language or the use of a\nspecific framework.\n", "page": 508, "type": "text", "section": "Page 508"}
{"text": "The Soft Skills of Software Architects\nChapter 15\n[ 480 ]\nIn contrast, soft skills are not as tangible and are much more difficult to define and measure.\nSoft skills are more related to interpersonal skills, such as leadership, communication,\nlistening skills, empathy, negotiation, and patience. While you can improve soft skills\nthrough training, they are more innate than hard skills.\nIt is useful for software development professionals to have certain soft skills and to spend\ntime improving them. As you move from a developer role to that of a software architect,\nthe importance of soft skills increases. Being a software architect involves more than just\ntechnical knowledge. You must also have soft skills in order to be effective. The focus of\nthis chapter is on the soft skills that are important for software architects.\nCommunication\nCommunication may well be one of the most important soft skills that a software architect\ncan possess. While it is a useful skill for any software development professional, software\narchitects, in particular, are required to communicate with a variety of team members and\nstakeholders.\nWhen you need to communicate something, it is imperative to understand the message that\nyou want to convey and to know your audience. This will allow you to choose an\nappropriate style with which to communicate your thoughts. It is not just about what you\nwant to say but, rather, how you say it.\nThe chances that a project will be successful are increased when there is plenty of\ncommunication between everyone involved with it. This includes communication with\ndevelopers, testers, business analysts, customers, management, and other stakeholders.\nSome of the people that you will need to communicate with will be technical while others\nwill not. A software architect must be able to tailor their communications based on the\naudience.\nCommunicating the architecture\nOne of the more significant responsibilities of a software architect is to communicate details\nabout the software system and its architecture to others. An architecture must be\ncommunicated to the development team so that developers can learn about the different\nstructures and elements that make up the architecture and how they interact with each\nother. It is through this understanding that they will be able to implement the functionality.\n", "page": 509, "type": "text", "section": "Page 509"}
{"text": "The Soft Skills of Software Architects\nChapter 15\n[ 481 ]\nA software architecture imposes some constraints on implementations and prevents\ndevelopers from making incorrect design decisions. The details of a software architecture\nmust be communicated to developers so that they can complete their tasks and understand\nthe constraints, to avoid developing components that do not conform to the architecture.\nCommunicating about quality attributes\nA software architecture can both enable and inhibit software quality attributes. We learned\nabout quality attributes in Chapter 4, Software Quality Attributes. Examples of software\nquality attributes include maintainability, availability, performance, and security. A\nsoftware architect has to communicate details of how design decisions will affect the\nquality attributes of a software system. Communication with product owners, business\nanalysts, and other stakeholders might be necessary to make design decisions based on\nquality attribute tradeoffs.\nCommunicating expectations\nSoftware architects will need to communicate with project management to assist with\nproject scheduling and resource planning. A software architect's input from a technical\nperspective is necessary for effective project planning.\nOnce the project begins, a software architect will need to continue to communicate with\nmanagement on the status of the project. In some cases, a software architect may be\ninvolved with communicating expectations to one or more customers. If a project does not\nmeet the expectations of the client, it cannot be considered a success. An organization must\nensure that it is on the same page as its clients, and one way to do this is to keep them in the\nloop regarding the current state of the project. The goal is to eliminate surprises to ensure\nthat customers are satisfied.\nThe 7 Cs of communication\nA set of tips regarding effective communication is known as the 7 Cs of communication.\nThere is some variation among these tips, both in the number (for example, some list only\nfive Cs) as well as the terms used. The following section details some of the most common\ntips.\n", "page": 510, "type": "text", "section": "Page 510"}
{"text": "The Soft Skills of Software Architects\nChapter 15\n[ 482 ]\nClarity\nClarity is important to have in your communications. If your audience is not clear about\nwhat you are trying to say, your communication cannot be effective. The term clear is\noccasionally used in place of clarity. The message you are communicating should be\nunderstandable and clear to the people who are receiving it.\nConsider your audience as well as your purpose when communicating with others. For\nexample, if your audience is not very technical but you are discussing a technical topic,\nthink about the language that you choose to use and how you elect to communicate your\nmessage. If you are going to use terms that a recipient may not understand, clarify their\nmeaning.\nConciseness\nWhen communicating, be concise without sacrificing any of the other Cs. Less is sometimes\nmore. Use as many words as is necessary but do not be overly verbose. Being excessive\nwith your wording has the potential to reduce clarity as others will struggle to understand\nthe meaning of what you are communicating. Your audience will have a tendency to tune\nyou out when you are not concise.\nConciseness emphasizes the important parts of your message by eliminating any\nunnecessary words. When you are concise, you avoid being repetitive. Conciseness will\nsave time, which also means that there are cost savings. The people you are communicating\nwith will probably prefer a concise message. When someone is known to be overly verbose,\nthere may be others who try to avoid that person entirely.\nIf you want to be concise, think about what is really essential to the message you want to\ncommunicate. If it helps, put yourself in the place of the people you are communicating\nwith and consider what they need to know in order to understand your message. If any\nmaterial is irrelevant, you can exclude it.\nIf you are planning your remarks ahead of time, such as for a presentation, it may take\nsome extra time to organize your thoughts and choose precise language that will allow you\nto be concise. However, this extra effort is worth it and your audience will appreciate it.\n", "page": 511, "type": "text", "section": "Page 511"}
{"text": "The Soft Skills of Software Architects\nChapter 15\n[ 483 ]\nConcreteness\nConcreteness involves being specific about what you say or write, as opposed to being\nvague and general. Concreteness helps to bring clarity to your message as it is less likely to\nbe misinterpreted. Misunderstandings can create problems for both the sender and the\nrecipient of a message, so it is best to be concrete when possible.\nUse specific facts and figures rather than imprecise descriptions. Doing so will enhance\nyour message and make it more interesting. Perhaps most importantly, being concrete\nmakes it easier to communicate the full meaning of your message. However, do not give\nspecific details if you do not have that level of information or if the exact figures are\nirrelevant to your message.\nUse vivid language that will help the recipient build an image of what you are trying to\ncommunicate. Putting action in your verbs can also add to concreteness. When possible, use\nan active voice rather than a passive one. In the active voice, the subject of the sentence\nperforms the action, as opposed to the passive voice, in which the subject receives the\naction. An active voice tends to be more concrete than vague.\nCourteousness\nWhen communicating with others, be courteous. Courtesy in your communications is being\nrespectful of others. Politeness and fairness in all of your communications will make those\nwho are listening to you more willing to embrace and absorb your message. Courteous\ncommunication fosters a positive work environment and will strengthen work\nrelationships.\nBe thoughtful, tactful, and respectful of the recipients of your communication. Be aware of\nany cultural differences that may exist between you and the recipients of your message to\nensure that you do not offend anyone. Use nondiscriminatory words and expressions to\nensure everyone is treated equally in terms of gender, race, religion, and ethnicity.\nConsideration\nConsideration focuses on keeping the recipients of your message in mind when you\ncommunicate something. Place an emphasis on you instead of I and we. Think about your\nrecipients and take into consideration their viewpoints. Be aware of, and empathetic to,\ntheir views, emotions, and attitudes. As the sender of a message, put yourself in the shoes\nof your recipients to see whether there is a better way to communicate your message.\n", "page": 512, "type": "text", "section": "Page 512"}
{"text": "The Soft Skills of Software Architects\nChapter 15\n[ 484 ]\nConsideration in your communications goes hand in hand with courteousness. Giving\nconsideration to others, by giving careful thought to your message and your audience, is\npart of being courteous to others.\nCorrectness\nYou want to have correctness in your communications. Whatever information you are\nconveying should be accurate. Even if you follow the other best practices, if your message is\nincorrect, the purpose of your communication will not be fulfilled. If any facts and data are\nprovided in your communication, ensure that they are accurate.\nIn addition to ensuring the correctness of your message, the grammar and vocabulary that\nyou use in your communications, written or verbal, should be correct as well. In any\nwritten communication, be sure to use proper spelling and punctuation. Correctness will\nincrease the audience's confidence level in you as the speaker (or writer) and it will help to\nconvey your message.\nCompleteness\nYour message should be complete. Completeness entails ensuring that your communication\nincludes all of the information needed to understand and use it. To ensure completeness,\nask yourself the five W questions (who, what, where, when, and why) and ensure that your\nmessage includes answers to any of the questions that are relevant.\nIf action needs to be taken or decisions need to be made related to the message you are\ncommunicating, providing a complete message will help to ensure that the right action\nand/or decision is made. Completeness saves time and reduces costs because extra effort is\nnot needed to provide missing information or to correct decisions that have already been\nmade based on incomplete information.\nIf someone asks a question about something you are trying to communicate, answer the\nquestion to the best of your ability. If you do not know the answer, say so honestly and\nclearly.\n", "page": 513, "type": "text", "section": "Page 513"}
{"text": "The Soft Skills of Software Architects\nChapter 15\n[ 485 ]\nListening skills\nListening skills are a crucial part of having communication skills. Listening is the ability to\nreceive and understand what someone is saying to you. It allows you to receive a message\nfrom someone and be able to comprehend it. To be an effective communicator, you must\nalso have an ability and willingness to listen. Communication does not just go in one\ndirection but rather flows in both.\nSoftware architects need to listen so that they can learn, understand, and obtain\ninformation. If you do not have adequate listening skills, having effective communication is\ndifficult. A message can be lost or misunderstood by a recipient who is not listening.\nHearing is not listening\nKeep in mind that listening and hearing are two different things. While the ability to hear is\na physical process that happens automatically as long as someone does not have any\nhearing problems, listening requires effort. When someone is speaking to you, you must\npay attention and put forth a concentrated effort to understand what the person is saying to\nyou.\nShowing empathy\nTo take listening skills to the next level, we need to be able to exhibit empathy towards\nothers. Empathy involves understanding another person's point of view. Try to put\nyourself in the other person's position to feel what someone else is going through and what\nthey may be thinking.\nOne way to develop empathy is to listen more and talk less. Ask questions that will help\nyou to gain better insight into what someone is feeling, thinking, or needing. Having\nempathy will allow you to better understand the needs and challenges of your developers,\ncustomers, management, and others. Empathy can help to motivate your team and build\nbetter products for your customers.\nTips for effective listening\nWhen you are listening to someone speak to you, face them and maintain eye contact. This\nwill tell the speaker that you are engaged and it will help you to maintain focus. Be present\nand attentive to the speaker.\n", "page": 514, "type": "text", "section": "Page 514"}
{"text": "The Soft Skills of Software Architects\nChapter 15\n[ 486 ]\nA common issue that prevents effective listening is when someone gets distracted and\nstarts thinking about something else. A person might even start thinking about what they\nwant to say next, which will make them miss part of the message. Try not to get distracted\nby other things, such as the speaker's appearance or another conversation that is going on\nnear you.\nDo not interrupt someone while they are speaking to you. Try to defer any judgment on\nwhat is being said and do not interrupt to impose your opinion or solutions on what is\nbeing said. Wait until there is a natural pause in the conversation to respond or ask\nquestions. Once you can ask questions, you can inquire not just about anything that you\ndid not understand, but also to confirm your understanding of what the speaker is saying.\nGive a speaker physical and verbal feedback, such as nodding, making a facial expression,\nor saying something. Feedback will allow the speaker to know that you are listening. Show\nthat you understand a speaker's perspective by reflecting on their feelings and providing\nfeedback that you understand.\nIf you are not engaged with what is being said, the speaker may pick up on that. If you are\nnot making eye contact, seem distracted, or are not giving any verbal and physical\nfeedback, the speaker may notice that you are not paying attention. This can cause them to\nstop speaking and they may even become offended.\nGiving presentations\nGiving presentations is one form of business communication. Software architects will need\nto give presentations to different types of audience, so it is useful to work on this skill. A\nsoftware architect may need to give technical presentations to development teams, sales\npresentations to potential customers, executive updates to management, and proof-of-\nconcept demos to domain experts.\nBecoming a good speaker does take practice. Public speaking does not come naturally to\neveryone but, like many other things, you will improve as you do it more. Practice it as\nmuch as possible because it will make you better at it.\n", "page": 515, "type": "text", "section": "Page 515"}
{"text": "The Soft Skills of Software Architects\nChapter 15\n[ 487 ]\nThe 4 Ps of presentations\nThe 4 Ps of presentations represent a series of steps you can follow to give an effective\npresentation. The steps in this approach are:\nPlan\n1.\nPrepare\n2.\nPractice\n3.\nPresent\n4.\nLet's take a look at each of these steps in more detail.\nPlan\nThe plan step is the initial step. You begin by determining the subject and purpose of your\npresentation. Are you looking to inform, persuade, or motivate the audience? As part of\nyour planning, you should have an idea about who will be attending your presentation and\nwhat they expect to get out of it. What information do they want to know? Is the\npresentation a formal or informal one? Tailoring your speech to your audience will allow\nyou to better connect with them and will help to ensure that it will be successful.\nIf it is not determined for you, you may have to determine the logistics of your\npresentation, including the date/time and the location. If your presentation will be done in\nperson and not just online, become familiar with the venue if at all possible. Doing so is\nparticularly helpful if you will be speaking to many people at a large venue, such as at a\ntechnical conference.\nMany of your presentations may simply be conducted in your organization's office, which\nyou will already be familiar with, but some of them will take place in other locations. You\nmay not have access ahead of time to the location, but if you can arrive even just a little bit\nearly, you can become somewhat familiar with the venue to minimize the chance of\nsurprises. For example, it might be helpful to see what the stage, seating, lighting, and\naudio/visual equipment will be like ahead of time.\nPrepare\nOnce the basics of the speech have been determined, it is time to begin preparing your\npresentation. It may be helpful to break down the content into an introduction, body, and a\nconclusion. The introduction should be kept fairly short and establish the overall topic and\ndirection of your presentation. Audiences like to be entertained, so include some humor, if\nappropriate, in any part of your speech.\n", "page": 516, "type": "text", "section": "Page 516"}
{"text": "The Soft Skills of Software Architects\nChapter 15\n[ 488 ]\nThe body of the speech has the main content of your presentation. It should flow smoothly\nfrom the introduction and into the conclusion. It is useful to begin by making note of the\nmajor points you want to make.\nIf you are preparing any visuals for your presentation, such as slides, images, or code, be\nsure that the font size, resolution, and zoom level will be such that everything will be\nclearly visible to the audience. You should also spend time thinking about background and\nfont colors to ensure optimal viewing. All too often, text or other visual items are too small\nto be clearly seen. You should avoid having to make adjustments, such as altering the font\nsize or the zoom level, during the presentation.\nWhen preparing slides, try not to place too much text on a single slide. This might not only\nbe visually unappealing but your audience may end up spending too much time just\nreading your slide instead of listening to you. Slides should summarize what you are\nsaying but you do not want to just be reading from your slides.\nIf your presentation involves a live demo, prepare the software you will need and perform\nany necessary setup in advance. As part of your preparation, consider things that could\npotentially go wrong and how you might handle those situations. For example, if your\ndemo requires the use of a database server, consider what would happen if you were\nunable to connect to it for some reason. Perhaps you could use a local copy of the database\nto eliminate that risk altogether.\nIf you are going to be accessing certain files or executing software, prepare by creating\nshortcuts to those items so that everything that you will need can be accessed easily and\nquickly. You do not want to be struggling to find something as the audience just sits there\nand waits.\nPractice\nOnce you have prepared your presentation, practice it until you are intimately familiar with\nit. Practice it as many times as you can because it will increase your confidence and reduce\nyour nerves. You do not have to memorize it but you should know it well enough that you\nare very comfortable with the material.\nYou should consider practicing in front of other people if possible, such as family, friends,\nor colleagues. If your presentation has a time limit, practicing will help you to determine\nwhether it is the appropriate length. If you will be performing a live demo, such as showing\noff functionality with the software being developed, or a technical one that involves coding,\npractice it many times so that you know what you will be clicking on, executing, and\ncoding.\n", "page": 517, "type": "text", "section": "Page 517"}
{"text": "The Soft Skills of Software Architects\nChapter 15\n[ 489 ]\nPresent\nPrior to actually presenting, you should have already planned, prepared, and practiced\nyour presentation. When it actually comes time to present, your audience deserves a\nspeaker who is prepared, punctual, and keeps to the allotted time (if any). Dress\nappropriately based on the type of presentation and the audience.\nStart your presentation off right by making a good first impression. Establish eye contact,\nbe as relaxed as you can, and try to be enthusiastic. If you do not have a microphone, be\nsure to speak loudly enough so that you will be easily heard. During your presentation, use\nthe tips previously given for the 7 Cs of communication, such as being clear, concise,\nconcrete, courteous, considerate, correct, and complete.\nIf something goes wrong during the presentation, do not panic. Do your best to recover and\nmove forward. Try to conclude your presentation on a positive note so that the audience\ncan leave with an upbeat impression.\nLeadership\nLeadership is a key skill for software architects. There is no single approach to leadership\nthat will fit every leader. You will have to discover for yourself the type of leadership style\nthat goes with your personality and skills.\nSimilarly, no single leadership style works for everyone or every situation. The qualities\npeople look for in a leader will vary from person to person. A single approach to leadership\nmay not work for everyone you are leading. Try to be aware of the differences among the\npeople you are working with and tailor your leadership accordingly. Attempt to provide\nwhat each person needs in a leader while at the same time remaining yourself. Let's take a\ndeeper look at what it means to be a leader.\nGetting others to follow you\nA major component of being a good leader is how well you influence others in a positive\nway. Your actions, words, and overall attitude will influence others and it is your job as a\nleader to inspire your team to take positive and productive action.\nHow do you get others to follow you? Leaders have certain qualities that inspire others to\nfollow them. You should work to earn respect and gain credibility among your colleagues.\nIf you consistently deliver high-quality work, can be depended on, and are always there to\nhelp others, you will begin to earn respect and credibility. Conducting yourself with\nintegrity will, over time, help to build trust between you and your fellow employees.\n", "page": 518, "type": "text", "section": "Page 518"}
{"text": "The Soft Skills of Software Architects\nChapter 15\n[ 490 ]\nDealing with challenges\nSoftware projects can be difficult and involve many challenges. A leader stays fully\ncommitted through any setbacks. When a development team faces a barrier during the\ncourse of a project, a leader should provide motivation to the team and help them move\nforward. A software architect should be prepared to recognize and acknowledge setbacks\nand, when one inevitably arises, they should meet it with enthusiasm and help to resolve it\nThose around you will sense your positive attitude and that can be contagious.\nBeing a technical leader\nAnother crucial part of leadership for software architects is being a technical leader. If you\nare a senior software developer or engineer, you probably already exhibit some technical\nleadership skills and have put them to use. Software architects use these skills even more as\none of their primary responsibilities is to provide technical leadership.\nSoftware architects are ultimately the ones who are responsible for the software\narchitecture and the technical direction of the system. Technical leaders provide the team\nwith technical guidance and support.\nThey should demonstrate technical excellence in their work and guide others to produce\nwork that is of similar quality. Technical leaders are innovators. They provide innovative\nsolutions to complex problems and come up with innovative ideas for new features and\nproducts.\nTechnical leaders have a vision, which consists of ideas, a purpose, and a direction that they\nsee a software project or an entire organization moving toward. Leaders should be able to\nclearly articulate their vision. A vision is important for a team or organization because it\nkeeps them focused on the goals for the future, provides motivation, and challenges\nindividuals to reach beyond what they may have aspired to without it. Employees want to\nfeel that they are a part of something bigger than themselves.\nTaking responsibility\nLeaders take responsibility. If things go wrong, a leader clearly takes responsibility rather\nthan seeking to blame others. Placing blame is looking back to the past while taking\nresponsibility is forward-looking. The focus should not be on whose fault something is but\non helping to fix the problem.\n", "page": 519, "type": "text", "section": "Page 519"}
{"text": "The Soft Skills of Software Architects\nChapter 15\n[ 491 ]\nIf something could have been done better, rather than focusing on the negative software\narchitects should seek to understand what lessons can be learned from the situation.\nImprovements can then be suggested so that a similar mistake does not happen again. Once\nthe issue is resolved, move forward and do not dwell on it.\nFocusing on others\nAnother key difference between being a software developer and a software architect is a\nshift in focus regarding your priorities. As a software developer, your focus is more on\nyourself and improving your skills. When you are a leader, it is not about you but about the\npeople who are following you. As you move from being a junior developer to a senior\ndeveloper to a software architect, the focus increasingly shifts to others and helping them\nimprove.\nA leader should focus on everyone they can affect in a positive way, so your attention\nshould go beyond just your developers. You can assist and enable others, such as\ncustomers, management, business analysts, product managers, and quality assurance\nprofessionals.\nDelegating tasks\nUnlike a developer role, a software architect may be involved in delegating tasks to others.\nYou may now be delegating coding tasks to someone else rather than completing them\nyourself. Depending on how your team is organized, assigning tasks may be more the\nproject manager's responsibility, but a software architect may be involved with\nmanagement to some degree.\nThe responsibility for delegating tasks may be a departure from what you are accustomed\nto as a developer. However, delegation provides you with an opportunity to build trust\nwith your team. A developer will realize that you trust them to perform a task and this\nresponsibility will motivate them to perform at a high level.\n", "page": 520, "type": "text", "section": "Page 520"}
{"text": "The Soft Skills of Software Architects\nChapter 15\n[ 492 ]\nDriving change\nAs a software architect, you may need to drive change within a project or organization.\nLeaders look ahead to take advantage of opportunities and notice what is currently not\nworking around them. Leaders see these as catalysts to making changes. As was mentioned\nearlier in the chapter, being a leader means having a vision and being able to communicate\nthat vision to others. However, we need to do more than just have a vision. We need to\ninitiate the change that is necessary to fulfill our vision.\nRather than fearing change, initiating it to depart from how things are currently being done\nis how improvements for the future are made and allows an organization to grow. Whether\nit is the use of a new technology, moving to a better software development methodology, or\nimprovements in an organizational process, software architects should be champions of\nchange.\nSometimes a change may take place that is not initiated by you and is beyond your control,\nbut leaders should expect and be able to handle such changes. In general, people are\nresistant to change, but leaders embrace it and motivate others to do so as well.\nExperimentation is one way that change can happen. Don't be afraid to experiment and try\nsomething new. It is through experimentation that we can create new opportunities and\nmake improvements. Experimentation does take effort and sometimes it does not result in a\nfruitful outcome. At the very least, you will have learned something from the process.\nSpending some time prototyping new ideas and exploring new ways to solve a problem\ncan lead to positive change and growth.\nCommunication and leadership   \nOne of the most important aspects of leadership is effective communication. It is because of\nthis that communication was covered first in this chapter. Leadership and communication\nskills are highly interconnected. Whether or not you can be an effective leader is dependent\non your communication skills. Communication skills help you to connect with others and\nthis connection is necessary in order to lead. Whether you are sharing your vision,\nproviding technical guidance, delegating tasks, mentoring others, or reporting status to\nmanagement, you need to be able to communicate effectively.\n", "page": 521, "type": "text", "section": "Page 521"}
{"text": "The Soft Skills of Software Architects\nChapter 15\n[ 493 ]\nMentoring others\nServing as a mentor to others on the team is part of being an effective leader. Being a\nmentor can mean different things but it is essentially making yourself available to support,\nadvise, and teach someone else. If you are open and available to help others, that is a\ncomponent of being a great leader.\nYou should develop a relationship with anyone who you mentor because it will increase\nthe chances that a mentorship will succeed. In addition to imparting professional advice\nabout technical topics, do not forget to provide mentorship for soft skills, too!\nWhen you are in the process of mentoring someone, put more weight on their importance\nthan that of the organization. Encourage your mentee to discover and follow their goals\nand passions. Sometimes it might mean that an employee and job are not an ideal fit or that\nit might be time for the employee to go somewhere else to continue to grow. You should\ngive the best advice you can to your mentee, without worrying about how it will affect the\norganization.\nMentorship does not just benefit the mentee. Most people enjoy the feeling they get from\nhelping others. You may have been mentored by someone in the past and it's a good feeling\nto be in a position to pay it forward. Being a mentor can also help you become better at\nyour own job, improve your leadership skills, and help you to gain credibility with your\ncolleagues.\nLeading by example\nLeading by example is one way to lead others. If you say one thing but do another, you lose\ntrust and credibility with those you are leading. As you go about your tasks, whether it is\nrequirement gathering, architecture design, technical guidance, coding, or interacting with\nstakeholders, approach them in the right way and with a good attitude. Set an example as\nyou perform your job, which will provide an opportunity for those around you to observe\nhow you perform your tasks.\nHow you deal with different tasks, challenges, people, and situations will be observed by\nothers. Your employees will learn from you and will ultimately follow your lead. Taking\nresponsibility, delivering on your promises, and producing high-quality work will set an\nexample that others will want to follow.\n", "page": 522, "type": "text", "section": "Page 522"}
{"text": "The Soft Skills of Software Architects\nChapter 15\n[ 494 ]\nDepending on others\nGood leaders are humble and are not afraid to admit when they don't know something.\nThey should ask plenty of questions and shouldn't hesitate to lean on, and learn from,\nothers. If you become a software architect, you may feel the pressure of people expecting\nyou to have a wide range of technical knowledge. Don't avoid saying that you don't know\nabout a particular topic or be reluctant to ask questions when you are unclear about\nsomething or need further information.\nSometimes people don't ask enough questions, either because they lack the time due to\ndeadlines, are hesitant to reveal that they don't know something, don't want to bother\nothers, or are simply lazy. Avoid being one of those people, because if you do not have a\ngood enough understanding of the work at hand, you may make incorrect decisions that\ncan be difficult to fix later.\nEveryone has different strengths and weaknesses, and you will not be an expert on every\ntopic. Your team will consist of a variety of individuals with their own strengths and\nweaknesses. Recognize the person who has more knowledge or experience on the topic in\nquestion and reach out to them. Collaboration with others will foster trust, which is a\nnecessary component of leadership.\nListening to the suggestions of others may provide you with ideas you would not have\nthought of otherwise. It will give you different ways of looking at a problem and it will\nallow you to see things from another point of view. Keep an open mind and look forward\nto learning from those you lead.\nNegotiation\nAs a software architect, you will be involved in obtaining buy-in and making decisions\nwith your development team, customers, management, and other stakeholders. All of the\nindividuals involved in making a decision may not be in agreement initially, so having\nnegotiation skills is useful.\nNegotiating is a way to settle differences, and a successful one ends with an agreement that\nis acceptable to all. In some cases, a compromise is reached to settle a negotiation. A\nsuccessful negotiation is not just about achieving the best outcome from your point of view\nbut also about seeking a result that is fair to everyone. You want to maintain relationships\nafter a negotiation is over and there should be no surprises when the final outcome of the\nnegotiation is reached.\n", "page": 523, "type": "text", "section": "Page 523"}
{"text": "The Soft Skills of Software Architects\nChapter 15\n[ 495 ]\nA number of soft skills are interconnected, and improving one will help you with others.\nHaving communication skills helps with negotiations and having strong negotiation skills\nis part of leadership. In order to be a good negotiator, you need to be a good listener,\ncommunicator, and collaborator. It also helps to have good interpersonal skills. No matter\nwhat your personal traits, improving your negotiation skills will come with more\nexperience.\nHow negotiation skills may be used\nAs a software architect, you will utilize negotiation skills in a variety of situations. For\nexample, you may be working with stakeholders to make decisions regarding quality\nattribute tradeoffs. These decisions will affect the software architecture. Further, enabling\none quality attribute could hinder another one and there will be some negotiating as\nstakeholders come to this understanding.\nA software architect may use negotiation skills when they need to obtain developer buy-in\nto take a particular approach or use a certain technology in a solution. There may be\ndiffering viewpoints among the multiple developers that make up a development team and\nit's the responsibility of a technical leader to persuade the team to take the approach or to\nhelp the team reach a consensus.\nAnother example is when a software architect is involved with negotiating with\nmanagement regarding the usefulness/viability of a project, the number of resources that\nwill be allocated to a project, the amount of time that will be given to complete a task, or the\napproval of different costs, such as licenses for a particular development tool.\nSoftware architects may also be involved in negotiations with a customer. They may take\npart in sales pitches to potential new clients, participating in discussions regarding what\nfeatures will be included, the technology stack that will be used, and the level of effort/cost\nrequired.\nInformal/formal negotiations\nIn many cases, a software architect will use negotiation skills informally during discussions\nwith others or in meetings. However, formal negotiations can take place as well. With a\nmore formal negotiation, you may want to consider taking a more systematic approach to\nit.\n", "page": 524, "type": "text", "section": "Page 524"}
{"text": "The Soft Skills of Software Architects\nChapter 15\n[ 496 ]\nBefore a formal negotiation takes place, you should spend time preparing for it.\nPreparations should include devoting time to making logistical decisions, such as agreeing\non a date, time, duration, and location for the negotiation.\nMore importantly, one of the first steps in a negotiation is to understand the interests of\neach party. A good place to start is to invest time in thinking about the varying viewpoints\nof the participants, possible compromises that can be made, and the possible alternatives\nthat could be discussed.\nYour team should know its best alternative to a negotiated agreement (BATNA). The\nBATNA is the most preferred alternative if an agreement cannot be reached.\nUnderstanding your BATNA is valuable because it is difficult to make a wise decision\nwithout knowing all of the possible alternatives.\nOnce the negotiation begins, a discussion should take place where both parties explain their\nunderstanding of the situation and their viewpoint. Both parties should listen carefully to\neach other in a sincere effort to understand each other. Each party should describe their\ngoals and what they hope to achieve as an end result. A software architect may need to\nprovide clarification during this step, particularly on technical topics. Anything that is not\nclearly understood should be clarified. Anyone should feel free to ask questions or to seek\nclarification.\nAfter everyone has a clear understanding of each other and their goals, the negotiation step\ncan begin. The focus should be on achieving a win-win outcome so that both parties can\nwalk away from the negotiation satisfied. A software architect can help during this stage by\ndriving the parties to reach a consensus. Compromises may need to be made by one or both\nparties. Sometimes alternative approaches need to be explored in an attempt to reach a\npositive outcome.\nThe next step, agreement, takes place once the negotiation is complete. At this point, a\nsuccessful negotiation will lead the parties to reach an agreement. If an agreement is not\nreached, further meetings may be necessary. If an agreement is reached, everyone should\nbe clear on what has been decided. In the final step, the agreed course of action is\nimplemented based on the group's decision.\n", "page": 525, "type": "text", "section": "Page 525"}
{"text": "The Soft Skills of Software Architects\nChapter 15\n[ 497 ]\nWorking with remote resources\nPlenty of organizations use remote resources for a software project. A remote employee is\nanyone who performs work away from the office. It is sometimes referred to as\ntelecommuting. Remote resources may be internal employees or those outsourced from\nanother organization. A software architect should be aware of the benefits and challenges\nof having team members who are not co-located work together.\nBenefits of using remote resources\nOrganizations may allow telecommuting because there are benefits for both the employee\nand the employer. Employees tend to like it and it can improve morale. Flexible work\noptions may increase their productivity. Employers who allow employees to work remotely\nmay find it easier to recruit and retain employees, which can lead to cost savings. In\naddition, an organization may not have to spend as much on office space.\nIf resources are outsourced, it is common for them to work remotely. Outsourcing can\nprovide operational and recruitment cost savings as well, since an organization will not\nneed to hire as much internal staff and provide them with benefits. You can scale teams up\nand down easier with outsourced resources than you can with in-house resources.\nOutsourcing also provides a partner to share risk, as some of the responsibilities are given\nto the outsourced vendor.\nChallenges when using remote resources\nWhatever the reasons are for an organization allowing employees to telecommute, a\nsoftware architect may be required to work with remote resources. Despite the advantages\nof using remote resources, there are challenges and risks associated with it. It takes some\nadditional skills to ensure that a project that uses remote resources will be successful.\nUsing remote resources effectively requires some planning, experience, and a good\nsoftware development process. If you are not careful, using remote/outsourced resources\ncan cause the project to take longer than expected and/or cause the quality of the software\nto suffer.\n", "page": 526, "type": "text", "section": "Page 526"}
{"text": "The Soft Skills of Software Architects\nChapter 15\n[ 498 ]\nCommunication\nOne challenge with remote resources is communication. With modern teleconferencing\ntechnology it is not as much of an issue, but sometimes face-to-face communication is\nsimply better. Remote resources may be located in different time zones, which can make it\nchallenging to have meetings. This is especially true if a resource is located far away, such\nas on the other side of the world. You have to be cognizant of everyone's time zones when\nscheduling meetings and be aware that it may mean that some of the resources will have to\njoin a meeting very late or early in their day.\nCultural differences\nIf a remote employee, internal or outsourced, lives in a different country, you should be\naware of any cultural differences when communicating with them. Individuals from both\ncountries should make an attempt to learn the cultural norms of the other country to avoid\nmisunderstandings or saying something that will offend someone. Different cultures may\nprefer to use different phrases or approaches. Taking the time to learn about each other's\ncultures will maximize the clarity of your communications.\nImpromptu meetings\nImpromptu meetings can sometimes be more difficult when everybody is not in the same\nlocation. With messaging tools, it can be just as easy to have a spontaneous meeting as\nbeing co-located, but if people are in different time zones, you are more likely to have to\nschedule things ahead of time. For example, if people are located many time zones away\nfrom each other, one person's work hours may be the middle of the night for someone else.\nRather than have a spontaneous meeting, it might have to be scheduled for the next day.\nOne advantage of this situation is that you may not be interrupted as much during your\nday for impromptu meetings. Remote employees are probably less likely to have\ninterruptions in general, which reduces context switching and distractions. This can\nincrease the productivity of a remote resource.\nNew employee onboarding\nIt can sometimes be more difficult for a software architect to incorporate a new team\nmember when the person is remote. You have to transfer technical (and business)\nknowledge to a new team member as well as any other relevant information. This can\ncertainly be accomplished with only online meetings but you have to make sure the new\nemployee has all of the information they need to succeed. Pair programming can\nsometimes be an effective way of bringing a new technical resource up to speed.\n", "page": 527, "type": "text", "section": "Page 527"}
{"text": "The Soft Skills of Software Architects\nChapter 15\n[ 499 ]\nAnother method of transferring knowledge to a remote employee is to have them on-site\nfor a small amount of time. It will give them an opportunity to learn from the team directly\nand they will be able to absorb some of the company's culture, norms, processes, and\nstandards. Once acclimated, remote resources can then potentially train other employees.\nFor example, if a trained employee is an outsourced one, they can return to their own\ncompany's office and train fellow employees who are working on the same project.\nWork quality\nOne possible difficulty with outsourced resources is that the quality of work from these\nemployees can vary. This can be true of any resource, but an organization may have less\ncontrol over outsourced resources because they may not be directly involved in\ninterviewing, hiring, training, and managing the resources.\nA software architect should try to mitigate this risk. One way is to ensure that there are\nestablished code review and quality assurance processes. This can uncover defects, which\ncan then be corrected, and provide an opportunity to give employees feedback. If a\nparticular resource's work is not up to scratch, you can perhaps request that another\nresource replace that person.\nConfidential company data\nWhen you are working with outsourced resources, be careful about exposing confidential\ndata to another company. One way that this risk can be mitigated is through legal\nagreements.\nAnother is to somehow mask, redact, or remove confidential data. Some data is sensitive\nand should be modified so that personally identifiable information (PII), such as names,\nsocial security numbers, address/phone/email information, and other data elements, is\nsomehow protected. Protecting sensitive data may even be applicable when the data will\nonly be seen by internal resources, but more careful consideration should be given when\nany data will be seen and used outside the company.\n", "page": 528, "type": "text", "section": "Page 528"}
{"text": "The Soft Skills of Software Architects\nChapter 15\n[ 500 ]\nSummary\nWhile technical skills are important for a software architect, having soft skills is crucial to\nbeing effective in the role. Soft skills are not as easy to define and measure as hard skills\nand are more interpersonal in nature. Your personality traits play a role in how natural\nsome soft skills come to you, but anyone can improve a soft skill by learning, practice, and\nexperience.\nThis chapter looked at some of the soft skills that are particularly important for software\narchitects, such as communication, leadership, and negotiation skills. Being an effective\ncommunicator is central to being a software architect. In addition to communicating your\nmessage, being a good communicator also involves being a good listener.\nThe ability to lead others is another soft skill that software architects should possess. Your\nresponsibilities include providing technical leadership for your team, helping your team\nmembers succeed, and mentoring them. To get team members to follow you, you have to\nearn their respect and gain credibility. Communication skills are highly interconnected with\nleadership skills. Software architects also find negotiating skills valuable because they are\ninvolved in gaining buy-in and helping to reach consensus on a variety of issues and with\ndifferent types of stakeholder.  \nIn the next chapter, we will focus on evolutionary architecture. Change is inevitable, and\nmodern software architectures are focused on being able to adapt. By making incremental\nchanges and designing loosely coupled architectures, a software system is more likely to be\nsuccessful in the marketplace and give its organization a competitive advantage.\n", "page": 529, "type": "text", "section": "Page 529"}
{"text": "16\nEvolutionary Architecture\nMany software applications are under constant pressure from both technical and business\nforces to change and must have the ability to adapt to change. Software architects must\nexpect and embrace change by designing software architectures that have the ability to\nevolve.\nSoftware development methodologies, in which large amounts of time were spent creating\na Big Design Up Front (BDUF) and software architects tried to anticipate, plan for, and\nimplement solutions for a wide variety of contingencies, are over. Rather than attempting\nto design an architecture that would almost never have to be changed, modern software\ndevelopment methodologies understand that changes will occur. By reading this chapter,\nyou will learn how to create a software architecture that can adapt to change.\nIn this chapter, we will cover the following topics:\nChange is inevitable\nLehman's laws of software evolution\nDesigning evolutionary architectures\nChange is inevitable\nChange is one of the few things that you can count on and it is inevitable for software\nsystems. It occurs during the course of the initial development as well as throughout its life\nduring maintenance.\nThere was a time when we had a greater sense of control over real-world processes and\nwhat needed to take place as part of those processes. A big, up-front design would take\nplace before any code was written. In an effort to exert control, software architects tried to\npredict and plan for every future contingency as we designed our systems. Software\nsystems were viewed as being more static than we view them today, with behavior that\nwould stay consistent over time.\n", "page": 530, "type": "text", "section": "Page 530"}
{"text": "Evolutionary Architecture\nChapter 16\n[ 502 ]\nAs we know, software systems modeled after the real world are hardly static. Changes\nconstantly take place and the software systems that are modeled after it must change as\nwell. Although software does not wear out like a physical product can, it does undergo\nchange over time.\nReasons for change\nChanges might be initiated for technical or business reasons. From a technical perspective,\nsome of the reasons that a software system may change include the use of new\nprogramming languages, frameworks, persistence technologies, and tools. New operating\nsystems or versions of operating systems may need to be supported. There may be changes\nin the hardware that affect the software in some way. Your organization may decide to alter\nhow an application is deployed, such as moving it to the cloud.\nMore so than in the early days of software development, a greater number of technology-\nrelated choices is available to us and it is now common to use an increased number of\ndifferent technologies in a single project. This fact means that the technology stacks for our\nsoftware systems are more complex and there are more opportunities for change to take\nplace.\nAn organization's business is also a constant source of change. There may be changing\nfunctional requirements, a desire for new features, requests to improve quality attribute\nscenarios, a need to keep up with (or get ahead of) competitors, changes in the market, and\nregulatory changes. Organizations may also experience mergers, acquisitions, a change in\ntheir business/revenue models, and a need to introduce new products. More so than in the\npast, users have greater expectations of their software systems and what users want from\nthem may change over time. An organization must be able to respond to changes in their\nmarket and with their users.\nExpecting change\nWhatever the reasons are that serve as the catalysts for change, most software systems will\nbe affected by some of them over their lifetime. While we may not be able to predict what\ntechnical or business changes may take place, we do know that, with very rare exceptions,\nsoftware systems will undergo change.\n", "page": 531, "type": "text", "section": "Page 531"}
{"text": "Evolutionary Architecture\nChapter 16\n[ 503 ]\nSoftware architects should expect change and design their software systems so that they are\ncapable of withstanding and adapting to it. Designing a software system to have an\nevolutionary architecture is how we can handle inevitable change. In the Designing\nevolutionary architectures section later in the chapter, we will cover how to design software\narchitectures that are capable of adapting to change, but first let's look at the origins of\nsoftware evolution.\nLehman's laws of software evolution\nSoftware evolution refers to the process of initially developing a software system and then\niteratively making changes to it. Beginning in the 1970s, Manny Lehman and his colleagues\nstudied the evolution of software systems. They identified a set of behaviors that became\nknown as Lehman's laws of software evolution. Lehman's dedication to the study of\ncontinuous software modifications and their long-term effects earned him the nickname the\nfather of software evolution.\nLehman's software categories\nLehman's laws take into consideration the fact that there are different types of software\nsystems. In his paper Programs, Life Cycles, and Laws of Software Evolution, Lehman\ndistinguished between three different types of system:\nS-type systems\nP-type systems\nE-type systems\nIn this section, let's look at these three types of system and learn which one is applicable to\nLehman's laws of software evolution.\nS-type systems\nAn S-type system is specifiable in that it has a well-known, exact specification and can be\ndeveloped to that specification. It can be described formally and the solutions to such\nsystems are well understood. It is not only possible to definitively determine whether the\nprogram is correct, it is also possible to provide a completely correct solution. The\nrequirements of S-type systems are unlikely to change and they do not evolve.\n", "page": 532, "type": "text", "section": "Page 532"}
{"text": "Evolutionary Architecture\nChapter 16\n[ 504 ]\nAn example of such a system is a calculator program or a program that performs very\nspecific mathematical computations. These types of program have logic that will not\nchange. Systems of this type are the simplest of the three types and are rare. As there is\nlittle chance of change with these systems, Lehman's laws do not apply to them.\nP-type systems\nA P-type system is one in which the problem may be precisely stated. The end result may be\nwell known and it may even be possible to create an exact specification for the system.\nHowever, unlike an S-type system, either the solution is not well understood or it is simply\nimpractical to implement a solution.\nA common example of P-type systems is a program that can play chess by always making\nthe best possible move with each turn. While it is theoretically possible to develop all of the\nsets of logic to determine what the program can do, in practice, it will not be feasible. The\ncomplexity of the logic is so high that the system will take too much time to calculate each\nmove. If we were not allowed to apply heuristics to reduce the computational effort and\ntake logical shortcuts, the solution would not be practical. Lehman's laws also do not apply\nto P-type systems.\nE-type systems\nAn E-type, or embedded type, system is modeled after real-world processes and people. The\nmajority of software systems are E-type systems. The term embedded does not mean the\nsoftware is embedded in some device but rather that the system is embedded in the real\nworld.\nAn E-type system affects the world it is in, which may create an evolutionary pressure for\nchange. In addition, the world that is being modeled may undergo change. The needs of the\nbusiness or its users may change, which will require the system to change. E-type systems\nmust evolve in order to remain useful. For these reasons, Lehman's laws of software\nevolution apply to E-type systems.\nThe laws\nObservations and behaviors related to the evolution of software that Lehman and his\ncolleagues identified are known as Lehman's laws of software evolution. There are eight laws:\nLaw I: Continuing change\nLaw II: Increasing complexity\n", "page": 533, "type": "text", "section": "Page 533"}
{"text": "Evolutionary Architecture\nChapter 16\n[ 505 ]\nLaw III: Self-regulation\nLaw IV: Conservation of organizational stability\nLaw V: Conservation of familiarity\nLaw VI: Continuing growth\nLaw VII: Declining quality\nLaw VIII: Feedback system\nLet's look at each law in more detail.\nLaw I \u2013 Continuing change\nSoftware systems must go through continuous change or they will progressively become\nless useful. If a software system does not adapt to the changing needs of the business and\nusers, satisfaction with it will progressively decrease.\nLaw II \u2013 Increasing complexity\nOver time, as a software system evolves and the number of changes performed on it\nincreases, the complexity of the software system will increase as well unless effort is taken\nto reduce that complexity. The concept of software entropy was discussed in Chapter 14,\nArchitecting Legacy Applications. Disorder in a software system, known as software entropy,\nincreases as the number of modifications to the system increases.\nLaw III \u2013 Self-regulation\nThe evolution of a software system is self-regulating. Particularly with large systems, there\nare structural and organizational factors that affect and constrain changes to a software\nsystem. Structural factors include the size and complexity of the software system. As a\nsoftware system grows, it becomes larger and more complex, making it increasingly more\ndifficult to make more changes. For this reason, as a typical software system grows older,\nits growth will inevitably slow down.\nOrganizational factors such as gaining consensus on decisions and getting approvals to\nmove forward with proposed changes influence the amount of change that can actually\ntake place in a software system.\n", "page": 534, "type": "text", "section": "Page 534"}
{"text": "Evolutionary Architecture\nChapter 16\n[ 506 ]\nLaw IV \u2013 Conservation of organizational stability\nOver a software system's lifetime, its overall rate of development remains relatively\nconstant and is independent of the resources assigned to its design and development. Some\nresources are more productive than others but regardless of the resources, the work output\nof a software project is fairly constant.\nLaw V \u2013 Conservation of familiarity\nAs a software system evolves, the group of individuals who are working on it must\nmaintain the same level of familiarity with the system so that it can evolve without\ncompromising the quality of the system. If we want the ability to handle whatever forces of\nchange might affect the software system and make modifications in an efficient way, we\nhave to maintain a deep understanding of the system.\nToo much growth can decrease the level of familiarity as it becomes increasingly difficult to\nmaintain the same level of knowledge over the technical and functional aspects of the\nsystem. For this reason, it is ideal if, during the lifetime of a software system, the amount of\nchange in each release remains roughly the same.\nLaw VI \u2013 Continuing growth\nThis law states that, as a software system evolves so that it can continue to be useful and\nsatisfy the needs of its users, it will continue to increase in size. The amount of functionality\nwill increase, which means the technical implementation will grow larger as well. This law\nis related to Law II (increasing complexity) because, as a software system grows in size, the\nlevel of complexity will increase.\nLaw VII \u2013 Declining quality\nAs a software system evolves, its quality will decrease unless a concerted effort is made to\nmaintain a high level of quality. Focus, discipline, and a rigorous effort must be sustained\nduring the lifetime of the software system to minimize the number of defects that are\nintroduced.\nAs more code is added to the system during its active life, there is the potential for an\nincrease in the number of defects. Poorly designed enhancements may contain bugs or fail\nto satisfy some requirements. Maintaining the same level of quality is difficult as a software\nsystem increases in both size and complexity.\n", "page": 535, "type": "text", "section": "Page 535"}
{"text": "Evolutionary Architecture\nChapter 16\n[ 507 ]\nLaw VIII \u2013 Feedback system\nSoftware evolution is a complex process and requires feedback from a variety of\nstakeholders to ensure that the proper changes are being made and that the software\nevolves in a direction that yields noticeable and useful improvement.\nProcesses should be put into place so that feedback can be received and properly analyzed.\nOnce that is done, appropriate action must be taken to incorporate the feedback into the\nsystem. To evolve the system properly, changes must be made to it based on feedback to\nensure that the system continues to be useful.\nDesigning evolutionary architectures\nWe know that changes to the software systems that we work on are inevitable and that\nthese software systems must adapt so that they can continue to be useful. As software\narchitects, how do we create an architecture that can evolve to support change?\nIn Chapter 1, The Meaning of Software Architecture, we discussed what software architecture\nconsists of. In part, it consists of design decisions for important aspects of the software\nsystem. They are among the earliest decisions that are made for a software system and are\npotentially the most difficult to change. Just as we strive to design other aspects of a\nsoftware system to be highly maintainable and easy to change, at the heart of evolutionary\narchitecture is the idea that we should be able to change software architecture easily as\nwell. An evolutionary architecture should also be one that supports other types of\nmodification to the system that do not involve changes to the architecture.\nIn the book, Building Evolutionary Architectures, authors Rebecca Parsons, Neal Ford, and\nPatrick Kua define evolutionary architecture in the following way:\n\"An evolutionary architecture supports guided incremental change across multiple\ndimensions.\"\nThere are several important concepts in this definition, so let's examine them further.\n", "page": 536, "type": "text", "section": "Page 536"}
{"text": "Evolutionary Architecture\nChapter 16\n[ 508 ]\nMaking guided architectural changes\nSoftware architects should guide any changes to a software system's architecture so that the\ncharacteristics of the architecture remain intact. A software architecture is shaped by the\ndesign decisions that were made, which give it certain properties and enable particular\nquality attributes.\nWhen an architecture must adapt to some technical or business change, it is not sufficient to\nsimply make any change that will allow it to adapt. Software systems are dynamic and\nmodifications to them can have unintended consequences. As an increasing number of\nchanges are made to the software system and its architecture over time, a lack of oversight\ncan allow the quality to erode. A software architecture's characteristics may change in\nunforeseen ways and it may no longer meet its intended purposes.\nSoftware architects must guide the changes to ensure that the software system's\nrequirements continue to be met and that the architecture continues to meet all of its goals.\nOne way to guide architecture changes is through the use of fitness functions, which can\nhelp a software architect determine the impact of modifications.\nFitness functions\nIn evolutionary computing, a fitness function is a type of objective function that is used to \ndetermine how close a given solution is to achieving the desired result. They return the\nfitness of the solution. They are used in the design of genetic algorithms to generate an\noptimal solution for a given problem.\nFitness functions can be applied to a software architecture to determine how close the\ndesigned solution is to achieving the desired architectural characteristics. They are an\nobjective way to assess a software architecture's characteristics. Fitness functions should be\nclearly defined and provide a quantitative measure of how fit a solution is for a particular\nproblem. A quantitative result is what will allow us to compare the architecture before and\nafter a change is introduced. They also allow us to compare different solutions to a problem\nand determine which one is optimal.\n", "page": 537, "type": "text", "section": "Page 537"}
{"text": "Evolutionary Architecture\nChapter 16\n[ 509 ]\nCategories of \ufb01tness functions\nThere are different categories of fitness function. They are not mutually exclusive. For\nexample, a fitness function could be both atomic and temporal. The different fitness\nfunction categories include:\nAtomic versus holistic\nAtomic fitness functions focus on a single context and on one architectural characteristic.\nFor example, a unit test that is designed to test one architectural characteristic is atomic. A\nholistic fitness function takes multiple architectural characteristics into consideration at the\nsame time.\nHaving both atomic and holistic fitness functions is useful because a feature that works fine\nwhen tested atomically could fail when it is combined with other features. It isn't feasible to\ntest every combination of all the architecture's characteristics, but a software architect can\nchoose which important combination of characteristics should be tested.\nTriggered versus continuous\nTriggered fitness functions are executed based on some event. For example, they could be\ntriggered as part of a build or unit test. A continuous fitness function runs constantly and\nits execution is not based on the occurrence of some event. An example of a continuous\nfitness function is a test, which monitoring tool may be executing constantly, that will\nproduce an alert when a certain condition is met.\nStatic versus dynamic\nA static fitness function is one in which the value for the condition that we are testing for is\nconstant. A test may be looking to ensure that the result is less than some static numeric\nvalue or that a test that returns true or false returns the value that we expect. In contrast,\nthe acceptable values of a dynamic fitness function may change based on a different\ncontext. For example, the desired result of a performance test might be different depending\non the current level of scalability. At a much higher level of scalability, a lower level of\nperformance might be acceptable.\nAutomated versus manual\nAutomated fitness functions are triggered automatically. They could be part of automated\nunit testing or an automated build process. When possible, automated fitness functions are\nideal. However, there may be times when you may either need or want to execute a fitness\nfunction manually.\n", "page": 538, "type": "text", "section": "Page 538"}
{"text": "Evolutionary Architecture\nChapter 16\n[ 510 ]\nTemporal\nTemporal fitness functions are based on a designated amount of time. While other fitness\nfunctions may focus on a change in the system, temporal ones are triggered based on time.\nFor example, a fitness function may be created to ensure that, if a patch becomes available\nfor a framework that is being used, it is applied within a certain number of days.\nIntentional versus emergent\nMany fitness functions can be defined early on in a project, once some of the characteristics\nof the architecture are known. These are known as intentional fitness functions. However,\nsome characteristics of the architecture are not known right from the beginning, but emerge\nas the system continues its development. These fitness functions are known as emergent\nones.\nDomain-specific\nDomain-specific fitness functions are based on specific concerns related to the business\ndomain. Some examples include regulatory, security, and personally identifiable\ninformation (PII) requirements. A domain-specific fitness function can ensure that the\narchitecture continues to conform to these requirements.\nExamples of \ufb01tness functions\nFitness functions may come in the form of tests (automated or manual), monitoring, and the\ncollection of metrics. Not all tests are fitness functions. Only those that actually assess a\nparticular architectural characteristic are fitness functions.\nFor example, fitness functions can be created to calculate and use various software metrics\nto determine whether a software architecture continues to meet maintainability\nrequirements. In Chapter 4, Software Quality Attributes, we discussed software metrics, such\nas cyclomatic complexity, lines of code (LOC), and the depth of inheritance tree (DIT) as\nmeasurements for maintainability. Fitness functions can allow you to know when a\nsoftware system has exceeded its predefined acceptable levels for these metrics, providing\nyou with an opportunity to analyze recent changes and determine whether refactoring is\nnecessary.\nPerformance tests can be executed to ensure that the architecture continues to meet its\nrequirements and that any recent changes to the software system have not negatively\nimpacted its performance. Security tests can focus on the security dimension of a software\nsystem to ensure that changes have not introduced a security vulnerability.\n", "page": 539, "type": "text", "section": "Page 539"}
{"text": "Evolutionary Architecture\nChapter 16\n[ 511 ]\nAnother example of a fitness function is the use of a resilience engineering (also known as\nchaos engineering) tool such as Chaos Monkey. Chaos Monkey is an example of a holistic,\ncontinuous fitness function that is used to reveal systemic weakness in your software\napplication. This tool, created by Netflix, disables a computer in the system, on purpose, to\ndetermine whether it can properly tolerate a system failure. This particular fitness function\nis executed periodically and is always scheduled to run during business hours (and not on\nweekends and holidays) to ensure that, if the system does not behave as expected,\nengineers will be available to respond to the issue.\nUsing these various types of fitness function provides a software architect with information\non how fit the overall architecture continues to be as changes are introduced and it\ncontinues to evolve. They provide a way to give a software architect confidence that the\nsystem continues to be capable and informs you if it is starting to decline in quality. Fitness\nfunctions facilitate the creation of an evolvable architecture.\nMaking incremental changes\nOne of the characteristics of an evolutionary architecture is that changes are made to the\nsoftware system incrementally. Making incremental changes includes how we introduce\nchanges to the system, how we build the system, and how the system is deployed.\nIn order to design an evolutionary architecture, we must make it easy to understand what\nis going on in the software system. The ease with which we can make a change is related to\nhow much we understand the system. Making changes that are smaller in scope allows us\nto more easily understand what is being changed and reduces the risk of introducing a\ndefect. It also makes it easier to code-review and test the changes that are being made to the\nsystem.\nYou may recall from the Lehman's laws of software evolution section earlier in this chapter that\nLaw VIII stresses the importance of having a feedback system that allows important\nstakeholders to provide feedback on the system. Changes can then be made to the system\nbased on the feedback. The iterative nature of agile software development methodologies\nand DevOps practices, such as continuous integration (CI) and continuous delivery (CD),\nfacilitates giving of and allows that feedback to be received faster, which enables beneficial\nimprovements to be delivered to customers faster.\n", "page": 540, "type": "text", "section": "Page 540"}
{"text": "Evolutionary Architecture\nChapter 16\n[ 512 ]\nIn Chapter 13, DevOps and Software Architecture, we discussed CI and CD. An aspect of\nmaking incremental changes is having developers continuously integrate their changes into\nthe system. By committing changes to a source control repository frequently, there is a\nreduced chance of merge conflicts, making it easier to resolve any that do occur. Automated\nbuilds that include automated testing give us feedback on our changes quickly. If there is a\nproblem with a set of changes, they will be easier to fix because there will only have been so\nmany changes committed since the last build.\nCD is the practice of being able to release changes into production in a safe, repeatable, and\nsustainable way. A system that is highly adaptable has the ability to quickly react to change\nand release new versions quickly. Releasing incremental changes to users quickly makes\nyour system highly evolvable. CD is about being able to produce low-risk, high-quality\nreleases with a faster time-to-market.\nArchitectural changes across multiple\ndimensions\nThe final part of the definition for evolutionary architecture put forth by Rebecca Parsons,\nNeal Ford, and Patrick Kua is that an evolutionary architecture is one that can support\nchange across multiple dimensions. A software system consists of different dimensions and\na software architect must consider all of them to build a system that can evolve.\nSoftware architects may naturally focus on the technical aspects of an architecture, such as\nits programming language, frameworks, and third-party libraries. However, software\narchitects must also concern themselves with other aspects of the software system. The\ndatabase (and its data), security, performance, and its deployment environment are all\nexamples of different dimensions of the software system. To successfully maintain an\nevolvable architecture, the software architect must consider all of these dimensions as\nchanges are made to it.\nLoosely coupled architectures\nWhen designing an evolutionary architecture, its components should be loosely coupled. In\nChapter 6, Software Development Principles and Practices, we covered the importance of\nloosely coupled code. Coupling refers to the degree of dependency between components.\n", "page": 541, "type": "text", "section": "Page 541"}
{"text": "Evolutionary Architecture\nChapter 16\n[ 513 ]\nWhen modules are tightly coupled, it is more difficult to make changes. Changes to one\ncomponent have a greater chance of affecting other components, increasing the total\nnumber of changes that have to be made. When you need to modify a tightly coupled\nsystem, more time and effort will be required for development and testing.\nTo ease the effort of making changes to the software system, dependencies between\ncomponents should be minimized and all components should be designed so that they are\nas independent of each other as possible. Loosely coupled components reduce complexity\n(and typically increase cohesion), making your architecture more maintainable.\nOne example of loose coupling for an evolutionary architecture is to loosely couple the\nsoftware system's cross-cutting concerns from the logic of the application's other concerns.\nBy decoupling this type of logic, each cross-cutting concern can evolve separately from any\nlogic that uses it. As we learned in Chapter 9, Cross-Cutting Concerns, logic for cross-cutting\nconcerns may be needed throughout an application, so you want it to be loosely coupled to\nthe other parts of your application. This will reduce code duplication and increase\nmaintainability.\nOne way to do this is to make each cross-cutting concern its own service. Logic that needs\none of those services will be dependent on its interface, allowing the implementation to\nchange without affecting other parts of the application. This will allow cross-cutting\nconcerns, such as caching and logging, to evolve over time.\nDesigning evolvable APIs\nAnother aspect of designing an evolutionary architecture is the proper design of the API.\nWe cannot anticipate what changes may become necessary for an API. Message contracts\nthat are used to communicate with an API may require modifications as a software system\nadapts to change. Some examples of the changes that can be made to an API include the\namount of information that is sent or received, the names or data types of individual pieces\nof data, and the introduction of new representations of data to support different types of\nclient.\nMaintaining a system that can evolve requires that changes made to an API, particularly\nonce it has been published, are given proper consideration. Your APIs should be designed\nto support evolution, allowing them to adapt to changes without breaking clients that\nalready depend on them.\n", "page": 542, "type": "text", "section": "Page 542"}
{"text": "Evolutionary Architecture\nChapter 16\n[ 514 ]\nApplying Postel's Law to APIs\nA helpful design guideline with regards to message contracts is the Robustness Principle,\nwhich is also known as Postel's Law. The idea behind Postel's Law is that you should be\nconservative in what you do and liberal in what you accept from others. This principle was\noriginally proposed when designing the TCP protocol, but it is applicable to message\ncontracts. We should be conservative in what we send but liberal in what we accept. In\nother words, the data that is sent out of our system should be kept to the minimum that is\nnecessary. A message sender must conform to the message contract, and reducing the\namount of data that an API exposes lessens the chances that a breaking change may be\nnecessary for the contract.\nIn terms of being liberal in what is accepted, when an API is consumed by a software\nsystem, it should only extract what is needed from a message and ignore the parts that are\nnot needed. Taking this approach minimizes what a software system depends on from a\nparticular message and increases its resilience to change. If the parts that are not needed are\nchanged in the future, the software system will not be affected.\nUsing standards in your software system\nUsing standards in your software system helps to create an evolutionary architecture.\nWhen making technology choices and design decisions, leveraging standards can make it\neasier to adapt the software system over time. Using a standard, such as a programming\nlanguage, framework, third-party library, communication protocol, database, data\ninterchange format, development tool, or some other design choice, can increase\nmaintainability and allow your system to evolve more easily.\nThe use of standards generally makes integration with other systems easier to accomplish.\nIn addition, finding resources to work on your software system who are familiar with the\ntechnologies that you have selected, not just during the initial development but during a\npotentially long maintenance life will be easier.\nWhen faced with different alternatives, electing to use a standard approach may not always\nbe the best choice. However, it is one factor that a software architect should consider given\nthe possible benefits.\n", "page": 543, "type": "text", "section": "Page 543"}
{"text": "Evolutionary Architecture\nChapter 16\n[ 515 ]\nLast responsible moment (LRM)\nThe last responsible moment (LRM) is the strategy of delaying a decision until the \nmoment when the cost of not making the decision is greater than the cost of making it.\nDesign decisions made for a software architecture can be among the most important for a\nsoftware system and they can be among the most difficult to change later.\nWith traditional software architectures, decisions were made very early in the project. In\norder to design an evolutionary architecture, it is beneficial to delay a decision until the\nLRM. This lessens the possibility that a premature decision will be made. Decisions that are\nmade too early are very risky because they may end up being incorrect and then they will\nresult in costly rework.\nAs long as the cost of delaying the decision is not greater than the cost of making one,\nwaiting as late as you can to commit to a decision ensures that you will have the most\ninformation available to you. This will allow you to make an informed decision.\nThe challenge with the LRM strategy is that it can be difficult to determine when the LRM\nis going to take place. Costs and benefits are subject to change and it is not always clear\nwhen the optimal time is to make a decision until after that moment has passed. It is\ngenerally not as beneficial, if at all, to delay decisions that are not so important to the\narchitecture. However, for important decisions, you will want to gather as much\ninformation as needed so that you can make a good decision when the appropriate time\narrives.\nSummary\nChanges to a software system are inevitable as there is a variety of reasons why an\napplication needs to be changed. Software architects should expect change and design their\nsoftware architecture with that in mind.\nTo create an evolutionary architecture that is capable of adapting to change, software\narchitects should guide architecture modifications to ensure that the characteristics of the\narchitecture and its level of quality remain the same. Fitness functions can be used to help\ndetermine whether the architecture continues to achieve the required architectural\ncharacteristics.\n", "page": 544, "type": "text", "section": "Page 544"}
{"text": "Evolutionary Architecture\nChapter 16\n[ 516 ]\nWhen changes do need to be made to an application, following practices, such as making\nincremental changes and ensuring that architectural components are loosely coupled, will\nhelp to facilitate making modifications.\nIn the next chapter, we will take a close look at how to become a better software architect.\nExcelling as a software architect requires continuous improvement to maintain your skills\nand gain new ones. We will detail a number of different activities that you can do that will\nallow you to grow in the role.\n", "page": 545, "type": "text", "section": "Page 545"}
{"text": "17\nBecoming a Better Software\nArchitect\nOnce you advance in your career and become a software architect, the process of self-\nimprovement and the advancement of your skills must continue. The role of software\narchitect is a challenging one and the field of software development is constantly changing.\nSoftware architects must keep up with the latest trends and ensure that their skills remain\nrelevant.\nThis chapter focuses on various ways that you can become a better software architect. From\nlearning new things, participating in open source projects, writing your own blog, trying\nnew technologies, and attending conferences, let's dive into how we can improve as\nsoftware architects.\nIn this chapter, we will cover the following topics:\nPracticing continuous learning\nParticipating in open source projects\nWriting your own blog\nSpending time teaching others\nTrying new technologies\nContinuing to write code\nAttending user groups and conferences\nTaking responsibility for your work\nBeing proud of your work\n", "page": 546, "type": "text", "section": "Page 546"}
{"text": "Becoming a Better Software Architect\nChapter 17\n[ 518 ]\nPracticing continuous learning\nPart of being a professional software architect involves continuously improving your\nknowledge portfolio. In the software development industry, technologies and development\napproaches are constantly changing. Just as software systems must adapt to a changing\nlandscape, software architects must adapt as well. Existing skills can become obsolete and a\nvital part of maintaining your value is ensuring your skills remain relevant. Each software\narchitect should remain humble and understand that there is so much that they do not\nknow.\nOne of the things that can separate an average software architect or developer from a really\ngood one is that individuals who set themselves apart are constantly trying to improve\nthemselves. They are not satisfied with their current knowledge base and want to always\nlearn something new. Great software architects understand that the field of software\ndevelopment is constantly changing and, in order to stay great in their profession, they\nhave to keep up.\nEven if you are strong in a particular language, tool, or framework, that skill can become\nstale as technologies evolve. For example, if you consider yourself an expert C#/.NET\ndeveloper, but get away from practicing those skills for a while, you can lose some degree\nof that competence. In addition, that particular language and/or framework will have\nevolved over that time, and if you do not keep up with the changes, you may not maintain\nyour expert level of knowledge.\nImproving the breadth and depth of your\nknowledge\nIn Chapter 1, The Meaning of Software Architecture, we discussed having both breadth and\ndepth of knowledge. Depth of knowledge refers to the extent of your expertise for a specific\ntopic within a subject, while breadth of knowledge refers to the full scope of your expertise\nin a subject.\nIf you are playing the role of a software architect, you undoubtedly have some level of\nbreadth and depth of knowledge. Continuous learning is not just about improving breadth\nor depth, it involves increasing both over time. There will be times when you want to focus\non a particular topic to increase your depth of knowledge, while there will be other times\nwhen you will want to focus on increasing your depth of knowledge by expanding your\nbody of knowledge and exploring new topics.\n", "page": 547, "type": "text", "section": "Page 547"}
{"text": "Becoming a Better Software Architect\nChapter 17\n[ 519 ]\nSoftware architects are expected to have breadth of knowledge because they need to be\nknowledgeable about a variety of topics related to software development. This book covers\nmany of these topics. However, a single book cannot do justice to the amount of knowledge\nyou may need for each topic. You may already have a lot of knowledge about particular\ntopics, but for the ones you are less familiar with, you should invest time in learning more\nabout them. Take a knowledge area that you perceive to be one of your weaknesses and\nturn it into a strength. It is that type of attitude that will lead to improvement and make\nyou a better software architect.\nAvoiding the law of the instrument\nWhen considering what to focus on in your learning, it's not just about maintaining your\ncurrent skill set but also about learning new things. Keeping up with current trends is an\nintegral part of continuous learning.\nContinuous learning is important for any software development professional, but this may\nbe particularly true for software architects. If you are not aware of rising new technologies\nand software development approaches that you can leverage in your solutions, then when\nyou are confronted with a design problem, you may only be able to recommend a\ntechnology that you have already been using. This idea is known as the law of the\ninstrument, which is sometimes referred to as the law of the hammer or Maslow's hammer.\nThe concept is summarized by the well-known saying: If the only tool you have is a\nhammer, everything begins to look like a nail.\nHowever, solutions familiar to you may not be ideal for a given problem. Here is one area\nwhere having breadth of knowledge is useful. By expanding your knowledge, you will\nhave more tools at your disposal. You may not be an expert in everything, but being aware\nof a variety of solutions to a problem and being able to understand the pros and cons of\ndifferent technologies will allow you to select the most suitable approach for a situation.\nFinding the time for learning\nIt can be challenging to find the time for learning and improvement. Between your work\nand personal life, devoting more time to your profession can be difficult. However, if you\nare passionate about software, you probably enjoy spending time learning new aspects of\nthe field.\n", "page": 548, "type": "text", "section": "Page 548"}
{"text": "Becoming a Better Software Architect\nChapter 17\n[ 520 ]\nIt is important to find a balance between your work and personal life so that you have\nsufficient time for both. A key to finding time for continuous learning is to make a\ncommitment to it. Set realistic goals for yourself that reflect what you want to achieve. If\nyou can dedicate some amount of time each week, it can really help your career. If you are\nvery busy, try to find ways to multitask. You can listen to a podcast during your commute\nor watch educational videos while you use a treadmill.\nWays to keep your skills sharp\nThere are a number of ways to accomplish your learning goals. The very fact that you are\nreading this book shows that you are dedicated to learning and improving your skills.\nBooks are a great way to learn new things. There are books on a wide variety of topics and\nnew ones are constantly being published. E-books can be particularly convenient because\nthey are portable, searchable, and instantly available. Since they do not take up physical\nspace, you do not need a lot of space to store them, even if you have a lot of them.\nFormal learning is what comes to mind for many of us when we think about learning. Even\nif you have already completed formal education, taking classes again, either individually or\nas part of a program, at a formal institution can be a good way to continue your learning.\nMany institutions now offer online classes, which can be more convenient for working\nprofessionals.\nClasses do not have to be taken at formal institutions of learning. Many excellent classes are\noffered online by learning sites, such as Pluralsight, Lynda.com (LinkedIn Learning),\nUdemy, Coursea, and Microsoft Virtual Academy. Learning sites can be great for technical\ntopics because sometimes formal institutions are slower to keep up with the latest trends.\nOther ways of keeping up with technology news and the latest developments include\nreading articles on websites and blogs, listening to podcasts, and watching videos online.\nThese activities do not necessarily require as much time as other ways of learning. You can\nread an article or blog post in a short amount of time and be selective about how many you\nread in a single sitting. You can listen to a podcast while doing something else, such as\ncommuting to work.\nWhile it can take time for books to be published and courses to be created, online posts,\npodcasts, and videos are published more frequently and can cover very current topics of\ninterest.\nOne benefit of many of the other topics in this chapter, such as participating in open source\nprojects, writing your own blog, and trying new technologies, is that they are additional\nways you can maintain your skills and learn new ones.\n", "page": 549, "type": "text", "section": "Page 549"}
{"text": "Becoming a Better Software Architect\nChapter 17\n[ 521 ]\nParticipating in open source projects\nWorking on an open source project or creating a new one can be a stimulating and\nrewarding way to become a better software architect. Unlike code you might write\nprivately for learning or practice, the code that you write for an open source project is\npublic. Many people have a tendency to write better code if it will be viewed by others. The\ntransparency of an open source project may help make the code that you write better.\nAnother benefit from participating in open source projects is that it can improve your\npersonal brand and/or your organization's brand. If people become familiar with your\nwork or the work of your organization, it can advance your career and/or bring positive\nattention to your organization.\nOrganizations that become active in helping to maintain the open source software that they\nuse become much more familiar with it as compared with just using it. The knowledge\ngained from having worked on it makes it easier for the organization to make additional\nchanges later. The organization will also benefit from the knowledge of the other members\nof the community, who can answer questions and provide useful feedback.\nWorking on an increasingly higher number of open source projects can help you to get a\njob. This is not simply due to the fact that you are improving your skills, but also because\nsome companies now look at a technical resource's open source portfolio and activity as\npart of the hiring process. While it may not play a large role in you obtaining a position\nsomewhere, it can be beneficial even if it just helps you land an interview.\nCreating your own open source project\nYou are not limited to working on existing open source projects. You can also create your\nown. If you are aware of some need or a problem that people are facing, you can create a\nsolution to that problem and make it publicly available. It may be a problem that you have\nalready faced and you may have already coded a solution.\nWrap up any solution that you want to make open source so that it can easily be used by\nothers. Ensure that your code is understandable. Follow the principles and practices that\nwe have covered in this book to make it maintainable as others will be using and modifying\nthe code. When creating your own project, provide unit tests, documentation, and a\nREADME.md file for it.\nYou will also want to consider what open source license, if any, is appropriate for your\nproject. An open source license defines the terms and conditions under which your\nsoftware can be used, modified, and shared.\n", "page": 550, "type": "text", "section": "Page 550"}
{"text": "Becoming a Better Software Architect\nChapter 17\n[ 522 ]\nOne set of open source licenses that is popular is a group of licenses that have been \napproved by the Open Source Initiative (OSI). Some of the OSI-approved licenses include\n(in alphabetical order):\nApache License 2.0\nBSD 2-Clause \"Simplified\" or \"FreeBSD\" license\nBSD 3-Clause \"New\" or \"Revised\" license\nCommon Development and Distribution License\nEclipse Public License\nGNU General Public License (GPL)\nGNU Lesser General Public License (LGPL)\nMIT license\nMozilla Public License 2.0\nIf you are contributing to an existing project, the easiest (and possibly required) approach\nwould be to use the project's existing license. If you are creating a new project within a\ncommunity that prefers to use a particular license, you will want to use that license for your\nproject.\nFor some of your projects, you can choose to not use any license. By default, any creative\nwork, which includes software, is copyrighted. Unless there is something specifying\notherwise, no one is allowed to use, modify, distribute, or copy your work without the\nauthorization of the author. There was a time in the past that the creator had to explicitly\nassert a copyright but that is no longer the case.\nWriting your own blog\nMaintaining your own blog is another activity that can make you a better software\narchitect. Writing about technical topics can make you more familiar with them. Taking the\nopportunity to learn about something that you have been wanting to know more about and\nthen writing about it can help to reinforce what you have learned. It provides a forum to\ndemonstrate what you know.\nIt is beneficial to share your knowledge with other software development professionals.\nInforming others about a subject and knowing that it can potentially help them is a\nrewarding endeavor. Unlike a tweet or other type of social media post, a well-written\narticle can remain visible and useful for a long time.\n", "page": 551, "type": "text", "section": "Page 551"}
{"text": "Becoming a Better Software Architect\nChapter 17\n[ 523 ]\nIncreasing your visibility\nA site that has well-written blog posts will increase your overall visibility and can earn you\na positive reputation with others in the industry. A personal site with just your professional\ncredentials and no blog may come across as stale and will not generate as much traffic. If\nyou regularly create new posts, there will be a reason for visitors to come to the site more\nthan once and you will be more likely to get new visitors.\nOnce visitors come to your site, they may see your online resume and credentials. Having a\nblog may lead to additional opportunities in your career. Just as with your public coding\nactivity for open source projects, some employers will come across your personal site if you\nhave one. It could be a factor in you getting an interview. Whether you are an independent\nconsultant, run your own business, or work for an organization, your blog can also serve as\na marketing tool in that it may attract new customers.\nStarting your own blog\nIt is pretty easy to start your own blog as there is a low barrier to entry. Anyone can get one\nup and running in a relatively short amount of time. Even though you may have the skills\nto do so, rather than creating your site from scratch, you should consider taking advantage\nof the many blogging platforms that are available to you. It can take some time to write\nyour blog's functionality, so instead of spending time coding that, you could be spending\ntime writing content for your blog.\nA blogging platform such as WordPress can provide all of the functionality that you need.\nMost platforms provide a number of ways that you can customize your site, including a\nvariety of themes that let you select the look and feel of your site. After choosing a blogging\nplatform, you will need to select and register a domain name, pick a host, and\nconfigure/customize your site. After that, it is all about writing and posting new content.\nYou will want to think about the overall theme and focus of your blog. Some blogs have a\nbroad scope while others are much narrower in scope. If you are interested in blogging\nabout a wide variety of topics, you will want your blog to cover a broad range. However,\nyou may want your blog to be more focused. A broader focus will allow you to write about\na lot more topics, but more visitors may subscribe to a more specialized blog that focuses\non the topics that are of interest to them. Whatever you decide for the focus and scope of\nyour blog, try to always honor it so that the expectations of your readers are met.\n", "page": 552, "type": "text", "section": "Page 552"}
{"text": "Becoming a Better Software Architect\nChapter 17\n[ 524 ]\nThe most challenging part of maintaining your personal blog will be consistently writing\nnew posts for it. However, there will be almost a limitless number of topics that you can\nuse as a subject for a post. There will always be something that you are working on that\nmight be beneficial to share or you will want to learn more about some new topic in a blog\npost.\nThings to avoid\nIf your blog focuses on your professional life, you probably do not want to include too\nmuch content that is unrelated to your career. Some people like to include personal details\non their site so that people can get to know them on that level. A moderate amount of that\nis fine if you want, but for the most part, you will want to focus on your career.\nAnother thing that you want to avoid is having a blog and then not posting to it for many\nmonths or even years. If your last posts are very old, visitors will assume the whole site is\noutdated and not being maintained. Contributing to your blog takes time, just like the other\nactivities that are being suggested in this chapter. However, you should think of it as an\ninvestment in your overall career.\nSpending time teaching others\nA productive and rewarding way to become a better software architect is to devote time to\nteaching other developers and software architects. When you teach others a particular\nsubject, it deepens your own level of understanding for that subject.\nIn your preparations for teaching, you will be reviewing material and consulting others. As\npart of that process, you may learn something new or be reminded of something when you\nhad forgotten. There is also a good chance that you will learn something from the people\nyou are teaching.\nTeaching others may improve your organization and interpersonal skills. Preparing for a\nsession in which you will be teaching requires you to organize it and the interaction with\nyour students can improve your interpersonal skills. These types of skills are related to\nothers, such as leadership, and may ultimately help to advance your career. \n", "page": 553, "type": "text", "section": "Page 553"}
{"text": "Becoming a Better Software Architect\nChapter 17\n[ 525 ]\nFinding opportunities to teach\nTeaching can be done in a variety of ways. It is not as if it has to be conducted in a formal\nmeeting or classroom setting. Perhaps one of the best and most frequent opportunities for\nyou to guide others will be at your job. There will be teaching opportunities for you on any\ngiven workday.\nAs you work with your development team, you will undoubtedly encounter many\nsituations in which you will want to explain something, provide guidance, or suggest an\napproach. If you conduct design or code reviews, those are excellent times that you can\npoint things out to your team members. Remember to remain positive and encouraging\nwhen providing your feedback.\nSharing past experiences is a great way to teach others. As a software architect, you have\nplenty of experience from your previous work. Sharing real stories from your career can\nreally benefit your colleagues.\nOne way to teach others is to simply set an example. As you go about your daily work,\nthose around you will observe how you conduct yourself, solve problems, and complete\ntasks. If you do things the right way, others will learn from your example.\nWe have already discussed one way in which you can teach other people, which is to write\nand maintain a blog. It is an effective way of teaching others and you will be able to reach a\nlarger audience. A blog allows you to easily engage with people who you do not even\nknow and who may live far away from you. Presenting at a user group or conference\nprovides another way to pass on your knowledge to others.\nLearning new things can be challenging and it takes work to master a subject, so remember\nto be patient when teaching others. Patience is an important quality for both the teacher\nand the student. Some topics may be difficult for someone to comprehend at first. You do\nnot want to get disheartened by this and you also want to prevent your students from\ngetting too frustrated.\nBeing a mentor\nMentoring is one way to impart knowledge to others. In Chapter 15, The Soft Skills of\nSoftware Architects, when we covered soft skills, mentoring people was mentioned as part of\nleadership. Mentoring your development team is part of your role as a software architect.\n", "page": 554, "type": "text", "section": "Page 554"}
{"text": "Becoming a Better Software Architect\nChapter 17\n[ 526 ]\nMentoring involves advising, supporting, and teaching someone else. While mentoring is\nbeneficial to your mentees, it also provides advantages to you as the mentor. Not only will\nit make you feel good, but it improves your leadership skills and gives you more credibility\nwith your colleagues.\nBe sure to listen to your mentees so that you understand what they want to learn from you.\nSoftware architects can teach and mentor others not just by focusing on technical skills but\nalso by giving advice about the organization, office politics, and soft skills. Using your own\npersonal experiences, you can assist others by instructing them on topics such as their\ncareers.\nRaise the expectations of your mentees. The expectations that you have for them (and that\nthey have for themselves) will have an impact on their performance. Raised expectations\nshow that you have confidence in them and will encourage them to move outside their\ncomfort zones, which is where true growth can happen.\nTrying new technologies\nGood software architects care about their craft and enjoy trying out new technologies and\ntechniques. This does not necessarily mean that you need to be an early adopter of new\ntechnologies in your applications but it is important to have an understanding of trending\ntechnologies.\nSoftware architects should learn a new technology, framework, language, tool, or other\ntechnology so that they understand the situations in which these should be applied. The\nmore technologies you are familiar with, the greater the number of tools you have at your\ndisposal. As you encounter different problems or want to take advantage of certain\nopportunities, having knowledge of a diverse array of tools will give you the power to\nselect the most ideal one for a particular job. It will give you the ability to speak\nintelligently about different alternatives and allow you to articulate why one solution is\nbetter than another.\nWe have discussed ways in which you might go about learning new things, and these\ntechniques can be applied to gaining an understanding of new technologies. We want to go\nbeyond just being aware of these technologies. In order to understand them more deeply, a\nsoftware architect should try them out.\nEither as part of your job or in your own time, try out new technologies. Many of them will\nbe free for you to try. A product that requires you to purchase it may have a free trial\nlicense available for the precise purpose of allowing people to give it a try.\n", "page": 555, "type": "text", "section": "Page 555"}
{"text": "Becoming a Better Software Architect\nChapter 17\n[ 527 ]\nSupplement your other types of learning by getting hands-on experience with the\ntechnology. Use a real or contrived problem and attempt to use the technology to solve it.\nYou can create a proof of concept (POC), which is a prototype of a solution to determine its\nfeasibility for a real application.\nAs you get to know it better, you will form opinions about its applicability, ease of use, and\nother characteristics. You will begin to understand the advantages and disadvantages of the\ntechnology. When there comes an opportunity to use it, you will be able to give an\ninformed opinion on whether or not it should be used.\nContinuing to write code\nIn order for software architects to continuously improve, they should continue to write\ncode. In Chapter 1, The Meaning of Software Architecture, we discussed ivory-tower software\narchitects. They are architects who are isolated in some way from the rest of the team.\nRather than having a hands-on approach, they design solutions at a high level but do not\nget involved with the actual coding of solutions.\nIf you do not exercise a particular skill, such as coding, over time you begin to lose it. You\nmay also grow out of touch with what is involved in the implementation of different types\nof solution. You will lose your understanding of the challenges and issues that are facing\nyour developers.\nAssigning yourself coding tasks\nOne way to stay involved with coding is to have some of the development tasks on your\nproject assigned to you. Having coding tasks assigned to you will allow you to stay close to\nthe rest of the development team.\nIf there are no plans to have the software architect code, speak with your project manager\nto explain what you want to do. To accommodate your other responsibilities, you can agree\nto keep your coding assignments limited so that they do not take up all of your time.\nWorking on your own project\nWe have already discussed working on open source projects and trying out new\ntechnologies. Both of these activities will allow you to keep your programming skills sharp.\nIn addition to those, working on your own side project is another way you can grow as a\nsoftware architect.\n", "page": 556, "type": "text", "section": "Page 556"}
{"text": "Becoming a Better Software Architect\nChapter 17\n[ 528 ]\nThe benefits of working on a side project are very similar to those of working on an open\nsource project. In fact, your side project may be an open source project. However, you may\nnot always find an existing open source project that you want to work on and you may not\nhave any ideas for a new open source project. This should not prevent you from creating\nyour own side project, as you can simply work on one privately. There may be times when\nyou just want to work on something that will not be shared publicly.\nWork on a project that challenges your current skills to facilitate your own personal\ngrowth. For example, if you are stronger working with backend code, focus on frontend\ncode, and vice versa.\nReading code\nAnother way to keep your programming skills sharp when you may not be coding as much\nis to read other people's code. When performing the software architect role, you may find\nyourself coding less than when you played a developer role.\nPerforming code reviews for your project will keep you intimately familiar with the\nimplementations created by the development team. Looking beyond your project, examine\nhigh-quality code that experienced professionals have written for other projects. Open\nsource projects make it easy to analyze all types of code. Select a project that is of interest to\nyou and examine the code.\nWatching live-streamed programming sessions or online videos from well-known and\nrespected programmers can inspire you and give you good ideas that you might be able to\nuse in your own code. Pair programming with colleagues is another way to become\nfamiliar with code that you have not seen before.\nAttending user groups and conferences\nAttending and participating in user groups and conferences is a fun way to become a better\nsoftware architect. There are many different conferences that take place each year and they\ncover a wide variety of topics.\nListening to the various presentations is a great learning experience. Some of the speakers\nare industry leaders and you can learn a lot from them. Some sessions reserve time for\nquestions and you can take advantage of that to follow up on something that was\npresented. After the session is over, you may have an opportunity to talk with the presenter\nand ask questions.\n", "page": 557, "type": "text", "section": "Page 557"}
{"text": "Becoming a Better Software Architect\nChapter 17\n[ 529 ]\nThe benefit of attending a user group or conference is that, after it is over, you can share\nwhat you learned with others who did not attend. You can take any new knowledge you\ngained and create a presentation of your own that you can then share with those in your\norganization.\nEven if you cannot travel to some of the biggest and best conferences in the world, there\nmay be some in your local area. Some conferences live-stream their sessions and/or make\nvideos of their sessions available.\nPresenting at a user group or conference\nIn addition to attending a user group or conference, you may be able to participate in some\nof them. Conferences and user groups need speakers and they may have openings at their\nnext event.\nWhile you may not have the opportunity to speak at any conference of your choosing, and\nsome conferences may be prohibitively difficult for you to attend due to factors such as\nlocation and cost, you may be able to find one that you can attend. Search online and ask \ncolleagues about user groups that meet up in your area, and about upcoming conferences\nthat will be taking place nearby.\nMany user groups and conferences have websites and other social media accounts that can\nprovide you with more information. Come up with a clear idea for a topic that is of interest\nto you and is something you are excited to present. Once you have a topic, become familiar\nwith the audience that will be attending. Keep the audience in mind when you create your\npresentation, including what you think they will want to get out of it.\nFind out what it will take to allow you to participate and get on the schedule. You may\nneed to submit a proposal that will need to get approved. Follow any guidelines that are\nprovided by the user group or conference when submitting your proposal.\nOnce you find out that you have been granted the opportunity to give a presentation, you\nmust start your preparations. In Chapter 15, The Soft Skills of Software Architects, we took a\nlook at some of the things to consider when preparing to give a presentation, including the\n4 Ps of presentations. Those include what to do when planning, preparing, practicing, and\npresenting. Use those guidelines when developing your presentation.\n", "page": 558, "type": "text", "section": "Page 558"}
{"text": "Becoming a Better Software Architect\nChapter 17\n[ 530 ]\nMeeting new people\nMeeting new people in your industry is another benefit of attending user groups and\nconferences. Discussing your profession with people outside your current professional and\nsocial circles exposes you to different viewpoints and ideas. If the only people you interact\nwith regarding your craft are on your development team, this limits the range of insights\nthat you can gain.\nNetworking before and after user groups and conference sessions will increase your\npersonal network and can lead to additional opportunities. Discussing your experiences,\nincluding some of the challenges that you have faced, with other industry veterans can give\nyou new ideas and insights.\nTaking responsibility for your work\nSoftware architects who excel in their role take responsibility for their work. As a leader of\nthe team, it is important to take responsibility for your work and not make excuses. It is\neasy to take responsibility when things go well, but being a leader means accepting\nresponsibility when things do not go well. When things go wrong, rather than making\nexcuses or blaming others, it is more productive to use that time and energy to come up\nwith options for how to alleviate the issue. I think that most people, including myself, really\nrespect colleagues who take ownership and responsibility.\nIf everyone on the team takes responsibility, that type of collective attitude prevents\nsoftware rot or disorder within a software system. Degradation of software can occur for a\nvariety of reasons. One typical cause is the overall culture of the work environment. If a\nknown issue is left unfixed, it becomes easier to leave other issues unresolved. Even if there\nis not sufficient time to resolve an issue immediately, take steps to protect the rest of the\nsoftware from the offending code, such as commenting it out. As a software architect, you\ncan create a culture that makes degradation unacceptable.\nSometimes problems are small and happen slowly over time. It can be easy not to notice\nthem at first but, eventually, problems can get out of hand. The team should be\ncontinuously aware of the project's current state and of the big picture. Software architects\nshould take the lead to ensure degradation does not take the team by surprise.\n", "page": 559, "type": "text", "section": "Page 559"}
{"text": "Becoming a Better Software Architect\nChapter 17\n[ 531 ]\nAttending to your well-being\nA key aspect of being a better software architect has nothing to do with technical skills. We\nmust not forget about your life outside software development and how it can have a\npositive or negative effect on your work.\nA part of being great at your job is taking care of yourself both mentally and physically. In\norder to perform at your best, you need to do things such as get enough rest, and exercise,\nand eat healthy foods.\nYou must balance your life so that you are not overly focused on work. Remember to have\nfun and to enjoy your personal life. Take breaks and vacations from work to recharge and\nensure that you are spending enough time doing other things.\nBeing proud of your work\nDeveloping software is fun, rewarding, stressful, and full of challenges. It is not always\neasy, but it can be very enjoyable. As you strive to be the best software architect that you\ncan be, do not forget to take pride in all of your accomplishments. There are only so many\npeople in the world who get to do what we do: make software. Everyone can affect the\nworld, even if it is only in some small way. One of the ways, and certainly not the only\nway, that you and I can make a positive difference is through the software that we create.\nPerhaps we are making someone's job easier, helping someone complete a mundane task in\na faster way, allowing someone to be more productive so that they have more spare time in\ntheir personal lives, or bringing additional joy into someone's life. You can derive great\nsatisfaction from knowing that others are benefiting from your work and appreciate what\nyou do. If you are not proud of your work, figure out what you can do to change that.\nSummary\nBecoming a software architect is a great accomplishment. However, you must seek to keep\nimproving to excel in your role. You must continuously learn new things to increase both\nthe breadth and depth of your knowledge. Reading books, taking classes, listening to\npodcasts, reading blog posts, and watching videos are all ways that you can learn new\nthings.\n", "page": 560, "type": "text", "section": "Page 560"}
{"text": "Becoming a Better Software Architect\nChapter 17\n[ 532 ]\nParticipating in open source projects, writing your own blog, teaching others, trying new\ntechnologies, writing code, and attending user groups and conferences are all ways you can\ncontinue to get better at what you do.\nStay curious, keep learning, and never stop asking questions. Have an open mind and\ncontinue to create new things. Always strive to make the things you put out in the world as\ngood as they can be. Be a software architect.\n", "page": 561, "type": "text", "section": "Page 561"}
{"text": "Other Books You May Enjoy\nIf you enjoyed this book, you may be interested in these other books by Packt:\nBeginning Java Data Structures and Algorithms\nJames Cutajar\nISBN: 978-1-78953-717-8\nUnderstand some of the fundamental concepts behind key algorithms\nExpress space and time complexities using Big O notation.\nCorrectly implement classic sorting algorithms such as merge and quicksort\nCorrectly implement basic and complex data structures\nLearn about different algorithm design paradigms, such as greedy, divide and\nconquer, and dynamic programming\nApply powerful string matching techniques and optimize your application logic\nMaster graph representations and learn about different graph algorithms\n", "page": 562, "type": "text", "section": "Page 562"}
{"text": "Other Books You May Enjoy\n[ 534 ]\nArchitectural Patterns\nPethuru Raj, Anupama Raman, Harihara Subramanian\nISBN: 978-1-78728-749-5\nUnderstand how several architectural and design patterns work to systematically\ndevelop multitier web, mobile, embedded, and cloud applications\nLearn object-oriented and component-based software engineering principles and\npatterns\nExplore the frameworks corresponding to various architectural patterns\nImplement domain-driven, test-driven, and behavior-driven methodologies\nDeploy key platforms and tools effectively to enable EA design and solutioning\nImplement various patterns designed for the cloud paradigm\n", "page": 563, "type": "text", "section": "Page 563"}
{"text": "Other Books You May Enjoy\n[ 535 ]\nLeave a review - let other readers know what\nyou think\nPlease share your thoughts on this book with others by leaving a review on the site that you\nbought it from. If you purchased the book from Amazon, please leave us an honest review\non this book's Amazon page. This is vital so that other potential readers can see and use\nyour unbiased opinion to make purchasing decisions, we can understand what our\ncustomers think about our products, and our authors can see your feedback on the title that\nthey have worked with Packt to create. It will only take a few minutes of your time, but is\nvaluable to other potential customers, our authors, and Packt. Thank you!\n", "page": 564, "type": "text", "section": "Page 564"}
{"text": "Index\n6\n6 R's\n   about  471\n   refactor  472\n   rehost  472\n   remove  471\n   replatform  472\n   repurchase  472\n   retain  471\n7\n7 Cs of communication\n   about  481\n   clarity  482\n   completeness  484\n   conciseness  482\n   concreteness  483\n   consideration  483\n   correctness  484\n   courteousness  483\nA\nAAA pattern\n   about  204\n   Act  204\n   Arrange  204\n   Assert  204\naccessibility\n   designing for  88\nAccount base class  406\nACID consistency model\n   about  354\n   atomicity  354\n   consistency  354\n   durability  355\n   isolation  354\nactive design review (ADR)\n   about  429, 430\n   documentation for review, preparing  430\n   questionnaires, designing  431\n   review, conducting  431, 432\n   reviewers, identifying  431\n   specialized reviews, identifying  431\n   steps  430\nactive reviews of intermediate designs (ARID)\n   about  432\n   participant roles  432\n   phases  432\nactivity diagrams, UML\n   about  417, 418\n   actions/control flow  419\n   decision/merge nodes  419\n   fork/join nodes  420\n   start/end nodes  418\nadaptability  99\nAdvanced Message Queueing Protocol (AMQP) \n266\nadvantages, cloud\n   automatic updates  286\n   cost reduction  286\n   disaster recovery  287\n   greater flexibility and scalability  286\nadvantages, serverless architectures\n   core products, building  279\n   cost savings  279\n   polyglot development  280\n   scalable and flexible  279\nAgile approach  473\nAgile Manifesto  33\nAgile software development methodologies\n   about  32\n   adaptive  34\n   Agile principles  33\n", "page": 565, "type": "text", "section": "Page 565"}
{"text": "[ 537 ]\n   Agile values  33\n   daily stand-up meetings  35\n   iterative methodology  34\n   predictive  34\nagnostic functional context  253\nAmazon Elastic Container Service (Amazon ECS) \n288\nAmazon Elastic Container Service for Kubernetes\n(Amazon EKS)  288\nAmazon Web Services (AWS)  288\napplication architect  25\napplication overview\n    application type, determining  152\n   creating  151\n   deployment constraints, identifying  152\n   important architecture design styles, identifying \n152\n   relevant technologies, determining  152\napplication, profiling\n   about  326\n   instrumentation  327\n   statistical profilers  327\narchitectural drivers\n   about  124\n   architectural concerns  128\n   constraints  127\n   design objectives  125\n   primary functional requirements  126\n   quality-attribute scenarios  126\narchitecturally-significant requirements (ASRs) \n142\nArchitecture Analysis and Design Language\n(AADL)  399\narchitecture description languages (ADLs)  399\narchitecture descriptions (ADs)\n   about  393, 397\n   creating  397\n   software architecture views  397, 398\n   Unified Modeling Language (UML)  401\narchitecture development method (ADM)\n   about  111, 157\n   architecture vision  160\n   business architecture  160\n   change management  162\n   governance, implementation  162\n   information systems architectures  161\n   migration planning  162\n   opportunities and solutions  161\n   phases  159\n   technology architecture  161\n   The Open Group Architecture Framework\n(TOGAF)  157\narchitecture pattern  215\narchitecture style  215\narchitecture tradeoff analysis method (ATAM)\n   participant roles  424\n   phases  424\narchitecture views  397\narchitecture-centric design method (ACDM)\n   about  111, 153, 154\n   architectural drivers, discovering  155\n   experiment-planning  156\n   experimenting with  156\n   notional architecture, creating  155\n   production  157\n   production go/no-go  156\n   production planning  157\n   project scope, establishing  155\n   refining  156\n   review  155\nARID participant roles\n   facilitator  432\n   lead designer  432\n   questioners  432\n   reviewers  432\n   scribe  432\n   software architects  432\nARID phases\n   pre-meeting  433\n   review meeting  434\nartifact  410\naspect-oriented programming (AOP)\n   about  297, 306\n   fundamental concepts  307\n   types of advice  307\n   weaving  308\nasymmetric (public key) encryption  375\nasynchronous messages  415\nATAM participant roles\n   evaluation leader  424\n   proceedings scribe  424\n", "page": 566, "type": "text", "section": "Page 566"}
{"text": "[ 538 ]\n   project decision makers  424\n   questioner  424\n   scenario scribe  424\n   stakeholders  424\n   team leader  424\nATAM phases\n   evaluation  425, 428\n   follow up  429\n   partnership and preparation  425\nattribute-driven design (ADD)\n    current design analysis, performing  149\n   about  111, 145, 146\n   architectural elements, instantiating  148\n   design concepts, selecting  148\n   design decisions, recording  149\n   input, reviewing  147\n   input, selecting  147\n   interfaces, defining  148\n   iterating process  149\n   iteration goal, establishing  147\n   iteration goal, reviewing  149\n   responsibilities, allocating  148\n   system elements, selecting  148\n   views, sketching  149\nauthentication, IAM\n   about  377\n   multi-factor authentication (MFA)  378\n   two-factor authentication (2FA)  378\nautomated builds\n   about  445, 446\n   software versioning  446, 447\navailability\n   about  91\n   based on request success rate, calculating  93\n   based on time, calculating  92\n   errors  93\n   failures  93\n   faults  93\nAzure Container Service (AKS)  288\nB\nbacklog, prioritizing\n   DIVE criteria  164\nBASE consistency model\n   about  355\n   basic availability  355\n   eventual consistency  355\n   soft state  355\nbeats\n   about  317\n   Auditbeat  317\n   Filebeat  317\n   Heartbeat  317\n   Libbeat  317\n   Metricbeat  317\n   Packetbeat  317\n   Winlogbeat  317\nbehavioral modeling  401\nbest alternative to a negotiated agreement\n(BATNA)  496\nbest practices, for team success\n   about  201\n   deliverables, reviewing  209\n   development environments, setting up  206, 207\n   pair programming  208\n   unit testing  201\nBig Design Up Front (BDUF)  118, 501\nbits per second (bps)  324\nblack box  263\nblockers  80\nblog\n   starting  523, 524\n   visibility, increasing  523\n   writing  522\nbottlenecks  328\nbottom-up approach\n   about  119\n   advantages  120, 121\n   disadvantages  120\n   using  121, 122\nbounce rate  322\nbounded contexts  61, 63\nbrownfield software system  124\nbuild and deployment processes\n   automating  474\n   modernizing  473, 474\nbuild tools\n   updating  475\nbuilt-in self-test (BIST)  95\nbuilt-in test (BIT)  95\n", "page": 567, "type": "text", "section": "Page 567"}
{"text": "[ 539 ]\nbundling  338\nburst compute pattern  454\nbusiness acumen\n   developing  53\nbusiness entity services  253\nBusiness Process Execution Language (BPEL) \n227\nBusiness Process Manager (BPM)  227\nBusiness Process Model and Notation (BPMN) \n227\nBusiness Tier  223\nbusiness topics  54\nC\ncache usage patterns\n   about  332\n   read-through pattern  332\n   write-behind pattern  333\n   write-through pattern  333\ncached data, invalidating\n   data, evicting  331\n   data, expiring  331\ncaching  310\nCALMS  439\n   about  437\n   culture  438, 439\n   Lean  440\n   measurement  441\n   sharing  441\nCAP theorem\n   about  353\n   availability  353\n   consistency  353\n   partition tolerance  353\nCascading Style Sheets (CSS)  183\ncategories, fitness functions\n   atomic, versus holistic  509\n   automated, versus manual  509\n   domain-specific  510\n   intentional, versus emergent  510\n   static, versus dynamic  509\n   temporal  510\n   triggered, versus continuous  509\ncentralized identity provider (IdP)\n   implementing  380\nchallenges, remote resources\n   communication  498\n   confidential company data  499\n   cultural differences  498\n   impromptu meetings  498\n   new employee onboarding  498, 499\n   work quality  499\nchange control board (CCB)  47\ncharacteristics, MSA\n   autonomous and independently deployable\nservices  265\n   communicating, with lightweight message\nprotocols  266\n   fault isolation  266\n   focused services  264\n   independent data storage  265\n   well-defined service interfaces  265\nCIA triad\n   about  357, 359\n   availability  360\n   confidentiality  359\n   integrity  359\ncipher  375\nciphertext  374\nclass diagrams, UML\n   about  401\n   aggregation  403\n   association  402, 403\n   composition  404\n   dependency  405\n   generalization  405\n   multiplicity  404\n   realization  406, 407\n   visibility  402\nclient-server architecture\n   about  220\n   stored procedures, using for application logic \n221, 222\ncloud architect  28\ncloud bursting  454\ncloud models\n   about  454, 455\n   Containers as a Service (CaaS)  456, 458\n   Function as a Service (FaaS)  459\n   Infrastructure as a Service (IaaS)  455, 456\n", "page": 568, "type": "text", "section": "Page 568"}
{"text": "[ 540 ]\n   Platform as a Service (PaaS)  458\n   Software as a Service (SaaS)  459\nCloud Native Computing Foundation (CNCF)  287\ncloud types, DevOps\n   hybrid cloud  453, 454\n   private cloud  452, 453\n   public cloud  452\ncloud-native applications\n   about  285, 287\n   containerized  287, 288\n   continuous delivery  289\n   dynamically orchestrated  288\n   microservices-oriented  289\n   migrating, to cloud  286\n   no downtime  289\n   twelve-factor apps  290\n   variety of devices support  290\nclustered table  349\ncohesion\n   about  172\n   coincidental cohesion  173\n   communicational cohesion  174\n   designing  175\n   functional cohesion  174\n   logical cohesion  173\n   procedural cohesion  173, 174\n   sequential cohesion  174\n   temporal cohesion  173\n   types  172\ncold start  283\nCommand Model  239\nCommand Query Responsibility Segregation\n(CQRS)\n   about  213, 238\n   advantages  240\n   command model  239\n   disadvantages  241\n   event-sourcing, using  240\n   query model  239\ncommented-out code  468\ncommercial off-the-shelf (COTS)  73\ncommon web application security risks\n   about  385\n   broken access control  388\n   broken authentication  385\n   components, using with known vulnerable\ncomponents  390\n   cross-site scripting (XSS)  389\n   injection  385\n   insecure deserialization  389, 390\n   insufficient logging and monitoring  390\n   security misconfiguration  388\n   sensitive data exposure  386\n   unvalidated redirects and forwards  391\n   XML external entity (XXE) attack  386\ncomplex event processing (CEP)  228, 229\ncomplexity\n   minimizing  175\ncomponent under test (CUT)  105\ncompression\n   about  335\n   content-encoding (end-to-end) compression  337\n   file compression  335\nConcreteComponent  303\nconfiguration management\n   about  46, 311\n   changing  47, 48\nContainers as a Service (CaaS)  456, 458\ncontent delivery networks (CDNs)\n   about  344\n   using  344\ncontinuous delivery (CD)\n   about  448, 474, 511\n   practicing  475\ncontinuous deployment  448, 449\ncontinuous integration (CI)\n   about  47, 445, 474, 511\n   automated builds  445, 446\n   automated testing  447\n   practicing  474\ncore assets  50\ncore concern  298\ncreate, read, update, and delete (CRUD)  238\ncritical rendering path (CRP)\n   optimizing  346, 347\ncross-cutting concerns implementation\n   about  301\n   decorator pattern, using  302, 304, 306\n   dependency injection (DI), using  301\ncross-cutting concerns, types\n", "page": 569, "type": "text", "section": "Page 569"}
{"text": "[ 541 ]\n   auditing  312\n   caching  310\n   configuration management  311\n   exception management  312\n   logging  313\n   security  312\ncross-cutting concerns\n   about  298\n   for microservices  317\n   guidelines  299\n   types  310\ncross-site scripting (XSS)\n   about  389\n   DOM XSS  389\n   reflected XSS  389\n   stored XSS  389\ncryptographic hash function\n   about  376\n   collision resistant  377\n   deterministic  376\n   one-way function  376\n   quick  376\ncryptography\n   about  374\n   encryption  374\n   hash functions  375\nCrystal  33\nCSS Object Model (CSSOM)  346\ncustomer relationship management (CRM)  459\ncyclomatic complexity  84, 85, 510\nD\ndaily scrum  35\ndata architect  26\ndata caching, in distributed applications\n   about  330\n   private caching strategy, using  330\n   shared caching strategy, using  330\ndata Tier  224\ndata-transfer object (DTO)  238\ndatabase administrator (DBA)  328\ndatabase concurrency\n   about  352\n   ACID model  354\n   BASE model  355\n   CAP theorem  353\n   database transactions  352\n   optimistic, versus pessimistic concurrency\ncontrol  352, 353\ndatabase indexes\n   many indexes  350\n   primary/clustered indexes  349\n   secondary/non-clustered indexes  350\n   using  349\ndatabase management system (DBMS)  81, 351\ndatabase performance\n   about  347\n   database concurrency  352\n   database indexes, using  349\n   efficient database schema, designing  347\n   scaling out  351\n   scaling up  351\ndead code  468\nDecorator class  303\ndefault deny approach  373\ndeliverables\n   code reviews  209, 210\n   formal inspections  210\n   reviewing  209\n   walkthrough  212\ndelivery strategies, SOA\n   agile strategy  250\n   bottom-up strategy  249\n   top-down strategy  249\ndenial-of-service (DoS) attack  363\nDependency Injection (DI)\n   about  196, 297\n   benefits  196\n   containers  201\n   patterns  197\nDependency Inversion Principle (DIP)\n   about  167, 194\n   Dependency Injection (DI)  196\n   Inversion of Control (IoC)  195\ndepth of inheritance tree (DIT)  86, 510\ndesign principles\n   design concept, selecting  129\n   externally-developed software  132\n   externally-developed software, buy or build\nscenario  133\n", "page": 570, "type": "text", "section": "Page 570"}
{"text": "[ 542 ]\n   leveraging  129\n   reference architectures  130\n   software architecture patterns  130\n   tactics  132\ndesign rationale\n   about  400\n   documenting  138\n   for design communication  140\n   for design documentation  140\n   for design evaluation  139\n   for design knowledge transfer  139\n   for design maintenance  140\n   for design reuse  140\n   for design verification  139\ndevelopment environments\n   README file, providing  207\nDevOps\n   about  437\n   architecting  449\n   CALMS  437\n   cloud models  454, 455\n   cloud types  452\n   continuous delivery (CD)  448\n   continuous deployment  448, 449\n   continuous integration (CI)  445\n   deploying, to cloud  451\n   important quality attributes  449, 450\n   need for  441, 442\n   practices  444\n   toolchain  442, 444\nDI patterns\n   constructor injection  197\n   method injection  200\n   property injection  198\n   service locator  200\ndisadvantages, serverless architectures\n   debugging and monitoring, limitations  280\n   designing many functions complexity  281\n   multitenancy issues  280\n   runtime optimizations  281\n   still immature  281\n   vendor lock-in  281\ndistributed denial-of-service (DDoS) attack  344,\n363\nDIVE criteria\n   business value  164\n   dependencies  164\n   estimated effort  164\n   insure against risks  164\nDLL/EXE  309\nDocument Object Model (DOM)  346\ndocument type definitions (DTDs)  387\nDOM XSS  389\ndomain sharding  340\ndomain-driven design (DDD)\n   about  56, 265\n   benefits  56\n   bounded contexts  61, 63\n   domain, separating, into subdomains  60\n   fundamentals  59\n   ubiquitous language  57, 59\ndomain-specific language (DSL)  227\nDon't Repeat Yourself (DRY) principle\n   about  176, 178, 300, 468\n   copy-and-paste programming  178\n   duplication, avoiding  180\n   magic strings  178\n   overly DRY things  180\nDREAD risk assessment model\n   about  364\n   affected users  365\n   damage potential  364\n   discoverability  365\n   exploitability  365\n   reproducibility  364\nduplicate code  468\nduties, software architect\n   technical leaders  17\nE\nefficient database schema, designing\n   appropriate data types, selecting  349\n   database, denormalizing  348\n   database, normalizing  347\n   primary and foreign keys, identifying  348\nElastic Stack\n   about  315\n   beats  317\n   Elasticsearch  315\n   Kibana  316\n", "page": 571, "type": "text", "section": "Page 571"}
{"text": "[ 543 ]\n   Logstash  316\n   using  315\nElevation of Privilege (EoP)  363\nencryption\n   about  374\n   asymmetric (public key) encryption  375\n   symmetric (secret key) encryption  375\nenterprise architect  24\nEnterprise Service Bus (ESB)  264\nentity-centric business services  253\nevaluation, ATAM phases\n   about  425\n   architectural approaches, analyzing  427, 429\n   architectural approaches, identifying  426\n   architecture, presenting  426\n   ATAM, presenting  425\n   business drivers, presenting  426\n   quality attribute utility tree, generating  426\n   results, presenting  429\n   scenarios, brainstorming  428\n   scenarios, prioritizing  428\nevent channels\n   about  225\n   message queues  225\n   message topics  225\n   point-to-point channel pattern  225\n   publish-subscribe pattern  225\nevent processing styles\n   about  228\n   complex event processing (CEP)  229\n   event stream processing (ESP)  229\n   simple event processing (SEP)  229\nevent stream processing (ESP)  228, 229\nevent-driven architecture (EDA)\n   about  213, 224\n   broker topology  227\n   event channels  224\n   event processing styles  228\n   event-driven functionality, types  230\n   mediator topology  226, 227\n   topologies  226\nevent-driven functionality\n   event-carried state transfer  230, 231\n   event-sourcing  231, 232\nevolutionary architectures\n   changes across multiple dimensions  512\n   changes, making  508\n   designing  507\n   evolvable APIs, designing  513\n   fitness functions  508\n   incremental changes, making  511, 512\n   last responsible moment (LRM)  515\n   loosely coupled  512, 513\n   standards, using in software system  514\nevolvable APIs\n   designing  513\n   Postel's Law, applying  514\nexisting solutions\n   leveraging  129\nextensibility  82\nexternally-developed software\n   building, advantages/disadvantages  133\n   buying, advantages/disadvantages  134\n   external software, researching  134\n   open source software (OSS), using  135\nExtreme Programming (XP)  33, 182\nF\nfacilitator  35\nfault detection detection\n   condition monitoring  95\n   self-tests  95\nfault detection\n   heartbeat  94\n   ping/echo reply  94\n   sanity test/sanity checking  95\n   timestamp  94\n   voting  94\nfaults recovery, ways\n   about  95\n   exception handling  95\n   faulty behavior, ignoring  97\n   graceful degradation  97\n   retry strategy  96\n   rollback  97\n   varying levels of redundancy  96\nfaults\n   detecting  94\nfile compression\n   about  336\n", "page": 572, "type": "text", "section": "Page 572"}
{"text": "[ 544 ]\n   lossless compression  336\n   lossy compression  336\nFilebeat  317\nFileImport class  405\nfitness functions\n   about  508\n   categories  509\n   examples  510\nflexibility  82\nFlow Final Node  419\nformal inspection\n   about  210\n   author  211\n   leader/moderator  211\n   recorder/scribe  211\n   reviewer  211\nformal inspections\n   meeting and follow-up  211\n   roles  211\nformal negotiations  495, 496\nformats, web fonts\n   EOT  345\n   TTF  345\n   WOFF  345\n   WOFF 2.0  345\nfrequently asked questions (FAQs)  91\nFunction as a Service (FaaS)  459\nfunction invocation\n   asynchronous request (message queue)  284\n   batch job  285\n   message stream  284\n   synchronous request  283\nfundamental concepts, AOP\n   advice  307\n   aspect  307\n   join point  307\n   pointcut  307\nfundamentals, domain-driven design (DDD)\n   aggregates entities  60\n   entities  59\n   root entities  60\n   value objects  59\nG\ng11n  100\ngigabits per second (Gbps)  324\ngood help system\n   providing  91\nGraphics Interchange File (GIF)  336\ngreenfield systems\n   about  123\n   versus brownfield software systems  123\nguidelines, cross-cutting concerns\n   about  299\n   consistency, maintaining  299\n   cross-cutting concerns, identifying  299\n   open-source, using  299\n   scattered solutions, avoiding  300\n   tangled solutions, avoiding  300, 301\n   third-party solutions, using  299\ngzip compression  337\nH\nhash function  375\nheader compression  343\nHealth Insurance Portability and Accountability Act\n(HIPAA)  452\nheap  349\nHTTP caching\n   cache-control directives, specifying  334, 335\n   leveraging  333, 334\n   validation token, using  334\nHTTP/2\n   about  341\n   header compression  343\n   implementing  343\n   multiplexing  342\n   server push  342, 343\n   using  341\nhuman resources (HR) department  369\nHypertext Markup Language (HTML)  183\nHypertext Transfer Protocol Secure (HTTPS)  104\nI\ni18n  100\nIComponent interface  303\nidentity and access management (IAM)\n", "page": 573, "type": "text", "section": "Page 573"}
{"text": "[ 545 ]\n   about  377\n   authentication  377\n   authorization  378\n   centralized identity provider (IdP), implementing \n380, 381\n   domain authentication, using  380\n   encrypted passwords, storing  379\n   hashed passwords, storing  379, 380\n   OAuth 2  381\n   OpenID Connect (OIDC)  381\n   plaintext passwords, storing  379\ninformal negotiations  495, 496\ninformation architect  26\ninformation technology (IT)  456\ninformation-hiding\n   about  180\n   exposed/hidde itemsn  181\n   reasons  181\ninfrastructure architect\n   about  27\n   facilities  27\n   network elements  27\n   servers  27\n   storage systems  27\nInfrastructure as a Service (IaaS)  279, 455\ninstallability  99\nInstitute of Electrical and Electronics Engineers\n(IEEE)  9\nintegrated development environment (IDE)  84,\n176, 262, 443, 469\nIntegrated Information Infrastructure Reference\nModel (III-RM)  158\ninter-process communication (IPC)  319\nInterface Segregation Principle (ISP)  167, 191\nInternational Organization for Standardization\n(ISO)  9\ninternationalization  100, 101\nInternet of Things (IoT)  277\ninternet service provider (ISP)  324\ninteroperability\n   about  102\n   challenges  102, 103\n   information, exchanging with another system \n103\n   information, locating  103\n   standards  104\n   testing  104\nivory tower architect  18, 19\nJ\nJavaScript  183\nJavaScript Object Notation (JSON)  104, 266, 316\nJoint Photographic Experts Group (JPEG)  336\nJSON web token (JWT)\n   about  382\n   header  383\n   payload  383\n   signature  384\nK\nKanban  33\nKeep It Simple, Stupid (KISS)  176\nKibana  316\nKISS principle\n   about  176\n   applying, to software  177\n   origin  176\n   over simplicity, avoiding  177\nL\nlast responsible moment (LRM)  515\nlast-mile latency  324\nlaw of the instrument  519\nlayer  217\nlayered architecture\n   about  216\n   advantages  218, 219\n   client-server architecture  220\n   disadvantages  219, 220\n   N-tier architecture  222\n   open layers, versus closed layers  216, 217\n   tiers, versus layers  217, 218\nleadership  489, 490, 491, 492, 494\nlean software development (LSD)  440\nlearnability  87\nlegacy application\n   migrating, to cloud  470\nlegacy applications\n   about  461, 462, 464\n   changes, making  469\n", "page": 574, "type": "text", "section": "Page 574"}
{"text": "[ 546 ]\n   data integration, performing  477\n   functionality, sharing between systems  477\n   integrating with  475\n   issues  462, 463\n   monoliths, transforming to microservices  469,\n470\n   need for  463\n   need, confirming  476\n   redundant code, removing  467\n   refactoring  464, 465\n   tools, using to refactor  469\n   type, determining  476\nlegacy code\n   testable, making  465\n   tests, writing  466\n   unit testing, benefits  465, 466\n   unit tests, refactoring  466\nLehman's laws of software evolution\n   about  503, 504\n   change, continuing  505\n   complexity, increasing  505\n   conservation of familiarity  506\n   conservation of organizational stability  506\n   feedback system  507\n   growth, continuing  506\n   quality, declining  506\n   self-regulation  505\nLehman's software categories\n   about  503\n   E-type systems  504\n   P-type systems  504\n   S-type systems  503\nLibbeat  317\nlines of code (LOC)  77, 84, 210, 510\nLiskov Substitution Principle (LSP)  167, 189\nlocalization  100, 101\nlog entries\n   characteristics  313\nlog levels\n   DEBUG  313\n   ERROR  314\n   FATAL  314\n   INFO  314\n   TRACE  313\n   WARN  314\nlogging  313\nlogging, cross-cutting concerns\n   Elastic Stack, using  315\n   log entries  313\n   log entries, routing  314\n   log levels  313\nLogstash  316\nloose coupling\n   about  169\n   common coupling  169\n   content coupling  169\n   control coupling  170\n   data coupling  170, 171\n   designing for  171\n   external coupling  170\n   Law of Demeter (LoD) / principle of least\nknowledge  171\n   message coupling  171\n   no coupling  171\n   stamp coupling (data-structured coupling)  170\n   types  169\nlossless compression  336\nlossy compression  336\nM\nmaintainability\n   about  79\n   cohesion, increasing  83\n   coupling, reducing  83\n   cyclomatic complexity  84, 85\n   depth of inheritance tree (DIT)  86\n   designing  82\n   Lines of code (LOC)  84\n   measuring  83\n   size, reducing  83\nmediator topology\n   about  226, 227\n   implementations  227\nmegabits per second (Mbps)  324\nmessage-delivery guarantees, AMQP\n   at least once  266\n   at most once  266\n   exactly once  266\nmicroservice chassis\n   leveraging  317, 318\n", "page": 575, "type": "text", "section": "Page 575"}
{"text": "[ 547 ]\nmicroservices architecture (MSA)\n   about  260, 263, 451\n   audience, selecting  275, 276\n   characteristics  264\n   dependencies, sharing  270\n   granularity  269\n   nanoservices  269, 270\n   polyglot microservices, designing  267\n   service discovery  271\n   service-oriented architecture (SOA)  263\n   stateless, versus stateful microservices  271\nmicroservices, cross-cutting concerns\n   microservice chassis, leveraging  317, 318\n   sidecar pattern, using  318, 319\nminification  337\nModel-View-Controller (MVC) pattern\n   about  213, 232\n   advantages  234\n   controller  234\n   disadvantages  234\n   model  233\n   view  233\nModel-View-Presenter (MVP) pattern\n   about  213, 235\n   model  235\n   presenter  236\n   view  235\nModel-View-ViewModel (MVVM) pattern\n   about  213, 236\n   model  237\n   view  237\n   ViewModel  237\nmodeling, UML\n   behavioral modeling  401\n   structural modeling  401\nmodifiability  81\nmodifications\n   scope  82\nmonolithic architecture  260\nmulti-factor authentication (MFA)  378\nN\nn-tier architecture\n   about  222\n   Business Tier  223\n   data Tier  224\n   Presentation Tier  223\nnaming conventions\n   for unit tests  204\n   unit test class names  205\n   unit test method names  205\nnegotiation\n   about  494\n   using  495\nnetwork-attached storage (NAS)  27\nno design up front (NDUF)  120\nnodes\n   about  410\n   device nodes  410\n   execution environment nodes (EENs)  410\nnon-agnostic functional context  253\nnon-clustered indexes  350\nO\nOAuth 2\n   about  381\n   roles  381\nobject-oriented programming (OOP)  306\nobject-relational mapping (ORM) tool  224\noffice politics\n   about  40\n   cultural differences  43\n   goals  42\n   negotiating  43\n   organization's goals  42\n   stakeholders, addressing  42\nonolithic architecture\n   benefits  261\n   drawbacks  262\nOpen Source Initiative (OSI)  135, 522\nopen source projects\n   creating  521\n   participating  521\nopen source software (OSS)\n   advantages  136\n   disadvantages  136, 137\nOpen Web Application Security Project (OWASP)\n   about  385\n   reference link  385\nOpen/Closed Principle (OCP)  167, 187, 301\n", "page": 576, "type": "text", "section": "Page 576"}
{"text": "[ 548 ]\nOpenID Connect (OICD)\n   about  381\n   authenticating, with identity provider  382\n   authorizing, with authorization server  384\noperating system (OS)\n   about  327\n   adaptive maintenance  81\norchestrated task service  253\norganization's business  55\northogonal software systems\n   designing  168\n   high cohesion  172\n   loose coupling  168\nP\npair programming\n   about  208\n   benefits  208, 209\n   using  209\nperformance improvement\n   application, profiling  326\n   changes, implementing  328\n   results, analyzing  328\n   results, monitoring  329\n   systematic approach  326\nperformance terminology\n   bandwidth  324\n   defining  323\n   latency  324\n   processing time  325\n   response time  325\n   throughput  324\n   utilization  325\n   workload  325\nperformance\n   about  321, 322\n   bounce rate  322\n   conversion rate  322\n   need for  323\n   page speed  323\npersonally identifiable information (PII)  499, 510\nplaintext  374\nPlatform as a Service (PaaS)  279, 458\npolicies, data eviction\n   explicitly evicting data  332\n   first-in, first-out (FIFO)  332\n   last-in, first-out (LIFO)  332\n   least recently used (LRU)  331\n   most recently used (MRU)  331\npolyglot microservices\n   designing  267\n   polyglot persistence  268\n   polyglot programming  268\n   technologies, using  269\nportability\n   about  99\n   maintaining  102\nPortable Network Graphics (PNG)  336\npre-meeting phase, ARID\n   design presentation, preparing  433\n   review meeting, preparing for  433\n   reviewers, identifying  433\n   seed scenarios, preparing  433\nPresentation Tier  223\npresentations\n   about  486\n   plan  487\n   practice  488\n   prepare  487, 488\n   present  489\nprimary index  349\nprinciple of least privilege (PoLP)  371\nproduct line engineering (PLE)  49\nprofilers  327\nprofiling  326\nproject back\n   acting  40\n   obtaining, on schedule  38\n   overtime, working  38\n   problem areas, identifying  40\n   resources, adding  39\n   resources, reallocating  39\n   scope, reducing  38\nproject management\n   about  36\n   estimates  36\n   project back, obtaining on schedule  38\n   project schedule changes  38\n   realist  37\n   software project estimation, importance  36\n", "page": 577, "type": "text", "section": "Page 577"}
{"text": "[ 549 ]\n   team and situational factors  37\nproof of concept (POC)  125, 527\nQ\nquality attributes\n   about  76, 77\n   and software development life cycle (SDLC)  78\n   external  77\n   internal  77\n   testing  78\nquality-attribute scenarios\n   about  126\n   prioritizing  127\nQuery Model  239\nR\nredundant legacy code\n   commented-out code  468\n   dead code  468\n   duplicate code  468\n   removing  467\n   unreachable code  467\nreference architectures\n   benefits  130\n   creating  131\n   refactoring  131\nreflected XSS  389\nregistered claims  383\nrelational database management system (RDBMS) \n220, 266, 271\nremote resources\n   benefits  497\n   challenges  497\n   working with  497\nreplaceability  100\nreply/return messages  415\nrequirements elicitation\n   about  68\n   stakeholders access, getting  74\n   techniques  69\nrequirements engineering\n   about  63\n   need for  66\n   software requirements, types  63\nresilience engineering  511\nreturn on investment (ROI)  44, 55\nreview meeting phase, ARID\n   about  434\n   ARID method, presenting  434\n   design, presenting  434\n   scenarios, brainstorming  434\n   scenarios, prioritizing  434\nrisk acceptance  46\nrisk avoidance  45\nrisk management  43\nrisk management options, threat modeling\n   risk, accepting  366\n   risk, avoiding  366\n   risk, mitigating  367\n   risk, transferring  366\n   security controls  367\nrisk mitigation  46\nRobustness Principle  514\nroles, OAuth 2\n   authorization server  381\n   client  381\n   resource owner  381\n   resource server  381\nS\nsampling  327\nscripts.js  340\nScrum  33\nsecondary indexes  350\nsecure by default  372\nsecure by design\n   about  370\n   attack surface, minimizing  371\n   default deny  373\n   defense in depth  371\n   fail securely  374\n   input validation  373\n   principle of least privilege (PoLP)  371\n   secure by default  372\n   security by obscurity, avoiding  372\n   security, must be usable  373, 374\n   simple software designs  372\n   weakest link, securing  373\nSecure Hashing Algorithm 256 (SHA-256)  376\nsecurity architect  28\n", "page": 578, "type": "text", "section": "Page 578"}
{"text": "[ 550 ]\nsecurity controls\n   about  367\n   administrative controls  368, 369\n   detection  370\n   physical security controls  367\n   prevention  369\n   response  370\n   technical security controls  369\nsemantic versioning  446\nSeparation of Concerns (SoC)  167, 176, 183,\n218, 298\nsequence diagrams, UML\n   activation boxes  414\n   alternative flows  416, 417\n   lifeline  414\n   loops  415\n   messages  414\n   optional flows  416\nserver push  342, 343\nserver-side caching\n   about  329\n   cache usage patterns  332\n   cache, priming  331\n   cached data, invalidating  331\n   data, caching in distributed applications  330\nServer-Side Request Forgery (SSRF)  386\nserverless architecture\n   about  276\n   advantages  279\n   Backend as a service (BaaS)  278\n   disadvantages  280\n   Function as a service (FaaS)  277\n   function deployment  282\n   function invocation  283\n   hybrid approach  282\nservice discovery\n    service registry, using  271\n   about  271\n   client-side discovery pattern  274\n   server-side discovery pattern  274, 275\n   types  273\nservice registry\n   self-registration pattern  272\n   third-party registration pattern  272\n   using  271\nservice-level agreements (SLAs)  29\nservice-oriented architecture (SOA)\n   about  213, 241, 242, 263\n   analysis  250\n   benefits  242, 243, 244\n   business automation requirements, defining  250\n   candidate services, modeling  251\n   challenges  245\n   cost-benefit analysis  244\n   delivery strategies  248\n   design  253, 254\n   entity service  253\n   existing automation systems, identifying  251\n   key principles  246\n   namespaces, structuring  257\n   service abstraction  247\n   service autonomy  247\n   service choreography  258\n   service composability  248\n   service descriptions  256\n   service discoverability  248\n   service layers  251, 252\n   service loose coupling  246\n   service models  251, 252\n   service orchestration  258\n   service registries  255, 256\n   service reusability  247\n   service statelessness  248\n   service-interface design  254\n   service-interface granularity  255\n   standardized service contract  246\n   task service  253\n   utility service  253\nsidecar pattern\n   using  318, 319\nsimple event processing (SEP)  228, 229\nSingle Responsibility Principle (SRP)  63, 167,\n184, 186, 278, 300\nsoftware architects\n   7 Cs of communication  481\n   about  20, 21, 518, 519, 520\n   application architect  25\n   cloud architect  28\n   communicating  480, 481\n   communication  480\n", "page": 579, "type": "text", "section": "Page 579"}
{"text": "[ 551 ]\n   conferences, attending  528, 529, 530\n   continuous involvement, with coding  527, 528\n   data architect  26\n   duties  18\n   enterprise architect  24\n   expectations, communicating  481\n   information architect  26\n   information security architect  28\n   infrastructure architect  27\n   life outside software development  531\n   listening skills  485, 486\n   preparations, for teaching  524, 525, 526\n   presentations  486\n   quality attributes, communicating  481\n   responsibility  530\n   soft skills  479, 480\n   solution architect  25\n   technical qualities  19, 20\n   technologies, using  526\n   types  24\n   user groups, attending  528, 529, 530\n   working  17\nsoftware architecture analysis method (SAAM)\n   about  421\n   architecture, developing  422\n   overall evaluation, creating  423\n   scenario interaction, assessing  423\n   scenario-based analysis  421\n   scenarios, classifying  422\n   scenarios, developing  422\n   scenarios, evaluating  423\n   scenarios, prioritizing  422\n   steps  422\nsoftware architecture design\n   about  112\n   active backlogs  165\n   architectural analysis  142\n   architectural evaluation  143\n   architectural synthesis  143\n   as iterative process  143\n   backlog, prioritizing  163\n   communicating, to others  116\n   component  115\n   design decisions, avoiding  116\n   design decisions, making  112, 113\n   design terms  113\n   documenting  137\n   dynamic backlogs  164\n   element  114\n   guidance, providing to developers  117\n   importance  115\n   key decisions, creating  115\n   model  142\n   module  114\n   non-technical parts, influencing  117\n   progress, tracking  162\n   progress, tracking with backlog  163\n   selecting  144, 145\n   sketching  137, 138\n   structure  113\n   subsystem  114\n   system  114\n   systematic approach, using  141\nsoftware architecture documentation, uses\n   about  394\n   architectural knowledge reuse, allowing  396\n   architecture, communicating to others  394\n   development team, assisting  395\n   input, providing for reviews  395\n   software architect, helping  396\n   team members, educating  395\nsoftware architecture notations\n   about  398\n   formal software architecture notations  399\n   informal software architecture notations  398\n   semiformal software architecture notations  399\n   types  398\nsoftware architecture patterns\n   about  214, 216\n   overusing  215\n   using  214, 215\n   versus, architecture styles  215, 216\nsoftware architecture views\n   about  397, 398\n   design rationales, including  400\n   notations  398\nsoftware architecture\n   about  8, 10, 16\n   active design review (ADR)  429\n   active reviews of intermediate designs (ARID) \n", "page": 580, "type": "text", "section": "Page 580"}
{"text": "[ 552 ]\n432\n   architecture tradeoff analysis method (ATAM) \n424\n   change, managing  13\n   communication ease  13\n   consumers  16\n   contents  10\n   cost estimates, improving  14, 15\n   effort estimates, improving  14, 15\n   implementation constraints, imposing  14\n   need for  10, 11\n   quality attributes, enabling  12\n   quality attributes, inhibiting  12\n   rationale, documenting  138\n   requirements  11\n   reusable model, providing  14\n   reviewing  420\n   serving, as training for team members  15\n   software architecture analysis method (SAAM) \n421\n   system qualities, predicting  12\nsoftware architectures design, Microsoft\n   application overview, creating  151\n   architecture objectives, identifying  150\n   candidate solutions, defining  153\n   key issues, identifying  153\n   key scenarios, identifying  151\n   techniques  150\nSoftware as a Service (SaaS)  459\nsoftware configuration management (SCM)  46\nsoftware development life cycle (SDLC)  78\nsoftware development methodologies\n   about  29\n   Agile software development methodologies  32\n   Waterfall model  30\nsoftware maintenance\n   adaptive maintenance  81\n   corrective maintenance  80\n   perfective maintenance  80\n   preventive maintenance  81\n   types  80\nsoftware product line\n   benefits  50\nsoftware product lines\n   about  49\n   core assets, of organization  50\n   product line engineering, risks  51\nsoftware quality attribute scenario\n   artifact  421\n   environment  421\n   response  422\n   response measure  422\n   source of stimulus  421\n   stimulus  421\nsoftware requirements\n   impact on architecture  67, 68\n   measurable  67\n   testable  67\n   types  63\nsoftware risk management\n   about  43\n   financial risks  44\n   functional risks  44\n   legal risks  44\n   management risks  44\n   personnel risks  44\n   risk acceptance  46\n   risk avoidance  45\n   risk mitigation  46\n   risk, transferring  45\n   technical risks  44\nsoftware systems security\n   about  358\n   CIA triad  359\n   three states of information  358\nsoftware systems\n   change  501, 502\nsoftware\n   uses  91\nSOLID design principles\n   about  184\n   Dependency Inversion Principle (DIP)  194\n   Interface Segregation Principle (ISP)  191\n   Liskov Substitution Principle (LSP)  189\n   Open/Closed Principle (OCP)  187\n   Single Responsibility Principle (SRP)  184, 186\nsolution architect  25\nsource lines of code (SLOC)  84\nSQL injection (SQLi)  385\nstandard error (stderr)  315\nstandard output (stdout)  315\n", "page": 581, "type": "text", "section": "Page 581"}
{"text": "[ 553 ]\nstorage area networks (SAN)  27\nstored XSS  389\nstrategies, for fault prevention\n   about  98\n   competence set, increasing  98\n   exception prevention  98\n   removal from service  98\n   transactions  98\nStreamReader class  405\nSTRIDE threat model\n   about  362\n   data, tampering with  362\n   denial-of-service (DoS)  363\n   Elevation of Privilege (EoP)  363\n   information disclosure  363\n   repudiation  362\n   spoofing identity  362\nStructured Query Language (SQL)  221\nstyles.css  340\nsynchronous messages  415\nSystem Under Test (SUT)  105, 204, 205\nSystems Modeling Language (SysML)\n   about  399, 400\n   diagram types  400\nT\nTCP Connection  342\ntechniques, requirements elicitation\n   brainstorming sessions  70, 71\n   document analysis  73\n   focus groups  72\n   interviews  69\n   observation  71\n   prototyping  73, 74\n   requirements workshops  69, 70\n   reverse engineering  74\n   surveys  72\ntestability\n   about  105\n   automatability  106\n   controllability  105\n   isolability  106\n   observability  106\n   software complexity  107\n   test documentation, importance  108\ntester\n   qualities  109\nThe Open Group Architecture Framework (TOGAF)\n   architecture domains  158\n   documentation sections  158\nthreat  360\nthreat classification model\n   about  361\n   STRIDE threat model  362\nthreat modeling\n   about  360\n   application, decomposing  361\n   potential threats, categorizing  361\n   potential threats, identifying  361\n   potential threats, prioritizing  363\n   responses, to threats  365\n   risk management  365\ntier  217\ntop-down approach\n   about  117, 118\n   advantages  118, 121, 122\n   disadvantages  118, 119\n   versus bottom-up design approach  117\ntriple modular redundancy (TMR)  94\ntwelve-factor apps\n   about  290\n   administrative processes  295\n   backing services  292\n   Build/Release/Run  292, 293\n   codebase  291\n   concurrency  294\n   configuration  292\n   dependencies  291\n   development/production parity  294\n   disposability  294\n   logs  295\n   port binding  294\n   processes  293\ntwo-factor authentication (2FA)  372, 378\ntypes of advice, AOP\n   after advice (finally)  307\n   after returning advice (on success)  307\n   after throwing advice (on error)  307\n   around advice  307\n   before advice (on start)  307\n", "page": 582, "type": "text", "section": "Page 582"}
{"text": "[ 554 ]\ntypes, event-driven functionality\n   event notification  230\ntypes, software requirements\n   business requirements  64\n   constraints  65, 66\n   functional requirements  64\n   non-functional requirements  65\nU\nubiquitous language  57, 59\nUnified Modeling Language (UML)\n   about  393\n   activity diagrams  417\n   class diagram  401\n   component diagrams  407\n   deployment diagrams  410\n   modeling, types  401\n   overview  401\n   package diagrams  409\n   sequence diagrams  413\n   structure diagrams  401\n   use case diagrams  411, 412\nUniform Resource Locator (URI)  257\nunit testing, properties\n   atomic  202\n   automated and repeatable  203\n   deterministic  203\n   easy implementation  203\n   fast  203\n   isolated and independent  203\nunit testing\n   AAA pattern  204\n   about  201\n   benefits  202\n   code coverage  205\n   naming conventions  204\n   properties  202\n   updating  206\nusability\n   about  87\n   need for  89\n   testing  89\nuseful feedback\n   providing  88\nuser experience (UX)\n   conversion rate  323\n   performance affects  322\nusers\n   task completion, allowing  87\nutility tree  426\nV\nvariation points  50\nvirtual machines (VMs)  456\nvisual design\n   appealing  89\nW\nwarm start  283\nWaste Everyone's Time  178\nWaterfall model\n   about  30\n   issues  32\n   phases  31\nWe Enjoy Typing)  178\nweaving\n   about  308\n   compile-time weaving  309\n   runtime weaving  309\nweb application firewall (WAF)  385\nweb application performance, improving\n   compression, using  335\n   content delivery networks (CDNs)  344\n   critical rendering path, optimizing  346, 347\n   HTTP caching, leveraging  333, 334\n   HTTP/2, using  341\n   resources, bundling  338, 339, 340, 341\n   resources, minifying  337\n   techniques  333\n   web fonts, optimizing  344, 345\nweb applications\n   designing  89\nweb fonts\n   about  345\n   font formats  345\n   optimizing  345\nWindows desktop applications\n   designing  90\nWindows Presentation Foundation (WPF)  237\nWrite Everything Twice (WET)  178\n", "page": 583, "type": "text", "section": "Page 583"}
{"text": "X\nXML external entity (XXE) attack  386\nY\nYou Aren't Gonna Need It (YAGNI)\n   about  176, 182\n   non applicable situations  182\n   over-engineering a solution, avoiding  182\n", "page": 584, "type": "text", "section": "Page 584"}
