{"text": "Amy Brown (editorial): Amy worked in the software industry for ten years before quitting to: create a freelance editing and book production business. She has an underused degree in Math from\nthe University of Waterloo. She can be found online at http://www.amyrbrown.ca/. Michael Droettboom (matplotlib): Michael Droettboom works for STScI developing science\nand calibration software for the Hubble and James Webb Space Telescopes. He has worked on the\nmatplotlib project since 2007. Elizabeth Flanagan (Yocto): Elizabeth Flanagan works for the Open Source Technologies Center\nat Intel Corp as the Yocto Project\u2019s Build and Release engineer. She is the maintainer of the Yocto\nAutobuilder and contributes to the Yocto Project and OE-Core. She lives in Portland, Oregon and\ncan be found online at http://www.hacklikeagirl.com. Je\ufb00Hardy (Iron Languages): Je\ufb00started programming in high school, which led to a bachelor\u2019s\ndegree in Software Engineering from the University of Alberta and his current position writing\nPython code for Amazon.com in Seattle. He has also led IronPython\u2019s development since 2010. You\ncan \ufb01nd more information about him at http://jdhardy.ca. Sumana Harihareswara (MediaWiki): Sumana is the community manager for MediaWiki as the\nvolunteer development coordinator for the Wikimedia Foundation. She previously worked with the\nGNOME, Empathy, Telepathy, Miro, and AltLaw projects. Sumana is an advisory board member for\nthe Ada Initiative, which supports women in open technology and culture. She lives in New York\nCity. Her personal site is at http://www.harihareswara.net/. Tim Hunt (Moodle): Tim Hunt started out as a mathematician, getting as far as a PhD in non-linear\ndynamics from the University of Cambridge before deciding to do something a bit less esoteric with\nhis life. He now works as a Leading Software Developer at the Open University in Milton Keynes,\nUK, working on their learning and teaching systems which are based on Moodle. Since 2006 he has\nbeen the maintainer of the Moodle quiz module and the question bank code, a role he still enjoys. From 2008 to 2009, Tim spent a year in Australia working at the Moodle HQ o\ufb03ces. He blogs at\nhttp://tjhunt.blogspot.com and can be found @tim_hunt on Twitter. John Hunter (matplotlib): John Hunter is a Quantitative Analyst at TradeLink Securities. He\nreceived his doctorate in neurobiology at the University of Chicago for experimental and numerical\nmodeling work on synchronization, and continued his work on synchronization processes as a postdoc\nin Neurology working on epilepsy. He left academia for quantitative \ufb01nance in 2005.", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 10", "position": 10, "chunk_type": "semantic", "token_estimate": 401}
{"text": "Amy Brown (editorial): Amy worked in the software industry for ten years before quitting to: He\nreceived his doctorate in neurobiology at the University of Chicago for experimental and numerical\nmodeling work on synchronization, and continued his work on synchronization processes as a postdoc\nin Neurology working on epilepsy. He left academia for quantitative \ufb01nance in 2005. An avid Python\nprogrammer and lecturer in scienti\ufb01c computing in Python, he is original author and lead developer\nof the scienti\ufb01c visualization package matplotlib. Luis Ib\u00e1\u00f1ez (ITK): Luis has worked for 12 years on the development of the Insight Toolkit (ITK),\nan open source library for medical imaging analysis. Luis is a strong supporter of open access and\nthe revival of reproducibility veri\ufb01cation in scienti\ufb01c publishing. Luis has been teaching a course on\nOpen Source Software Practices at Rensselaer Polytechnic Institute since 2007. Mike Kamermans (Processing.js): Mike started his career in computer science by failing technical\nComputer Science and promptly moved on to getting a master\u2019s degree in Arti\ufb01cial Intelligence,\ninstead. He\u2019s been programming in order not to have to program since 1998, with a focus on getting\npeople the tools they need to get the jobs they need done, done. He has focussed on many other things\nas well, including writing a book on Japanese grammar, and writing a detailed explanation of the math\nbehind B\u00e9zier curves. His under-used home page is at http://pomax.nihongoresources.com. Luke Kanies (Puppet): Luke founded Puppet and Puppet Labs in 2005 out of fear and desperation,\nwith the goal of producing better operations tools and changing how we manage systems. He has been\npublishing and speaking on his work in Unix administration since 1997, focusing on development\nsince 2001. He has developed and published multiple simple sysadmin tools and contributed to\nestablished products like Cfengine, and has presented on Puppet and other tools around the world,\nx\nIntroduction", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 10", "position": 10, "chunk_type": "semantic", "token_estimate": 310}
{"text": "1996, recently switching to building multi-tenant systems for software \ufb01rms. Susan is a passionate: power user of Git, Linux, and Vim. You can \ufb01nd her tweeting random thoughts on Erlang, Haskell,\nScala, and (of course) Git @SusanPotter. Eric Raymond (GPSD): Eric S. Raymond is a wandering anthropologist and trouble-making\nphilosopher. He\u2019s written some code, too. If you\u2019re not laughing by now, why are you reading this\nbook? Jennifer Ruttan (OSCAR): Jennifer Ruttan lives in Toronto. Since graduating from the University\nof Toronto with a degree in Computer Science, she has worked as a software engineer for Indivica, a\ncompany devoted to improving patient health care through the use of new technology. Follow her on\nTwitter @jenruttan. Stan Shebs (GDB): Stan has had open source as his day job since 1989, when a colleague at Apple\nneeded a compiler to generate code for an experimental VM and GCC 1.31 was conveniently at hand. After following up with the oft-disbelieved Mac System 7 port of GCC (it was the experiment\u2019s\ncontrol case), Stan went to Cygnus Support, where he maintained GDB for the FSF and helped on\nmany embedded tools projects. Returning to Apple in 2000, he worked on GCC and GDB for Mac\nOS X. A short time at Mozilla preceded a jump to CodeSourcery, now part of Mentor Graphics,\nwhere he continues to develop new features for GDB. Stan\u2019s professorial tone is explained by his\nPhD in Computer Science from the University of Utah. Michael Snoyman (Yesod): Michael Snoyman received his BS in Mathematics from UCLA. After\nworking as an actuary in the US, he moved to Israel and began a career in web development. In order\nto produce high-performance, robust sites quickly, he created the Yesod Web Framework and its\nassociated libraries. Je\ufb00rey M. Squyres (Open MPI): Je\ufb00works in the rack server division at Cisco; he is Cisco\u2019s\nrepresentative to the MPI Forum standards body and is a chapter author of the MPI-2 standard. Je\ufb00\nis Cisco\u2019s core software developer in the open source Open MPI project. He has worked in the\nHigh Performance Computing (HPC) \ufb01eld since his early graduate-student days in the mid-1990s. After some active duty tours in the military, Je\ufb00received his doctorate in Computer Science and\nEngineering from the University of Notre Dame in 2004. Martin S\u00fastrik (ZeroMQ): Martin S\u00fastrik is an expert in the \ufb01eld of messaging middleware,\nand participated in the creation and reference implementation of the AMQP standard.", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 12", "position": 12, "chunk_type": "semantic", "token_estimate": 406}
{"text": "1996, recently switching to building multi-tenant systems for software \ufb01rms. Susan is a passionate: After some active duty tours in the military, Je\ufb00received his doctorate in Computer Science and\nEngineering from the University of Notre Dame in 2004. Martin S\u00fastrik (ZeroMQ): Martin S\u00fastrik is an expert in the \ufb01eld of messaging middleware,\nand participated in the creation and reference implementation of the AMQP standard. He has\nbeen involved in various messaging projects in the \ufb01nancial industry. He is a founder of the \u00d8MQ\nproject, and currently is working on integration of messaging technology with operating systems and\nthe Internet stack. He can be reached at sustrik@250bpm.com, http://www.250bpm.com and on\nTwitter as @sustrik. Christopher Svec (FreeRTOS): Chris is an embedded software engineer who currently develops\n\ufb01rmware for low-power wireless chips. In a previous life he designed x86 processors, which comes\nin handy more often than you\u2019d think when working on non-x86 processors. Chris has bachelor\u2019s and\nmaster\u2019s degrees in Electrical and Computer Engineering, both from Purdue University. He lives in\nBoston with his wife and golden retriever. You can \ufb01nd him on the web at http://saidsvec.com. Barry Warsaw (Mailman): Barry Warsaw is the project leader for GNU Mailman. He has\nbeen a core Python developer since 1995, and release manager for several Python versions. He\ncurrently works for Canonical as a software engineer on the Ubuntu Platform Foundations\nteam. He can be reached at barry@python.org or @pumpichank on Twitter. His home page is\nhttp://barry.warsaw.us. Greg Wilson (editorial): Greg has worked over the past 25 years in high-performance scienti\ufb01c\ncomputing, data visualization, and computer security, and is the author or editor of several computing\nxii\nIntroduction", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 12", "position": 12, "chunk_type": "semantic", "token_estimate": 274}
{"text": "[chapter1]: Scalable Web Architecture and Distributed\nSystems\nKate Matsudaira\nOpen source software has become a fundamental building block for some of the biggest websites. And as those websites have grown, best practices and guiding principles around their architectures\nhave emerged. This chapter seeks to cover some of the key issues to consider when designing large\nwebsites, as well as some of the building blocks used to achieve these goals. This chapter is largely focused on web systems, although some of the material is applicable to\nother distributed systems as well. 1.1\nPrinciples of Web Distributed Systems Design\nWhat exactly does it mean to build and operate a scalable web site or application? At a primitive\nlevel it\u2019s just connecting users with remote resources via the Internet\u2014the part that makes it scalable\nis that the resources, or access to those resources, are distributed across multiple servers. Like most things in life, taking the time to plan ahead when building a web service can help in\nthe long run; understanding some of the considerations and tradeo\ufb00s behind big websites can result\nin smarter decisions at the creation of smaller web sites. Below are some of the key principles that\nin\ufb02uence the design of large-scale web systems:\nAvailability: The uptime of a website is absolutely critical to the reputation and functionality of\nmany companies. For some of the larger online retail sites, being unavailable for even minutes\ncan result in thousands or millions of dollars in lost revenue, so designing their systems to be\nconstantly available and resilient to failure is both a fundamental business and a technology\nrequirement. High availability in distributed systems requires the careful consideration of\nredundancy for key components, rapid recovery in the event of partial system failures, and\ngraceful degradation when problems occur. Performance: Website performance has become an important consideration for most sites. The\nspeed of a website a\ufb00ects usage and user satisfaction, as well as search engine rankings, a\nfactor that directly correlates to revenue and retention. As a result, creating a system that is\noptimized for fast responses and low latency is key. Reliability: A system needs to be reliable, such that a request for data will consistently return the\nsame data. In the event the data changes or is updated, then that same request should return", "domains": ["Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 15", "position": 15, "chunk_type": "semantic", "token_estimate": 382}
{"text": "the new data. Users need to know that if something is written to the system, or stored, it will: persist and can be relied on to be in place for future retrieval. Scalability: When it comes to any large distributed system, size is just one aspect of scale that needs\nto be considered. Just as important is the e\ufb00ort required to increase capacity to handle greater\namounts of load, commonly referred to as the scalability of the system. Scalability can refer\nto many di\ufb00erent parameters of the system: how much additional tra\ufb03c can it handle, how\neasy is it to add more storage capacity, or even how many more transactions can be processed. Manageability: Designing a system that is easy to operate is another important consideration. The\nmanageability of the system equates to the scalability of operations: maintenance and updates. Things to consider for manageability are the ease of diagnosing and understanding problems\nwhen they occur, ease of making updates or modi\ufb01cations, and how simple the system is to\noperate. (I.e., does it routinely operate without failure or exceptions?) Cost: Cost is an important factor. This obviously can include hardware and software costs, but\nit is also important to consider other facets needed to deploy and maintain the system. The\namount of developer time the system takes to build, the amount of operational e\ufb00ort required\nto run the system, and even the amount of training required should all be considered. Cost is\nthe total cost of ownership. Each of these principles provides the basis for decisions in designing a distributed web architecture. However, they also can be at odds with one another, such that achieving one objective comes at\nthe cost of another. A basic example: choosing to address capacity by simply adding more servers\n(scalability) can come at the price of manageability (you have to operate an additional server) and\ncost (the price of the servers). When designing any sort of web application it is important to consider these key principles, even\nif it is to acknowledge that a design may sacri\ufb01ce one or more of them. 1.2\nThe Basics\nWhen it comes to system architecture there are a few things to consider: what are the right pieces,\nhow these pieces \ufb01t together, and what are the right tradeo\ufb00s. Investing in scaling before it is needed\nis generally not a smart business proposition; however, some forethought into the design can save\nsubstantial time and resources in the future.", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 16", "position": 16, "chunk_type": "semantic", "token_estimate": 409}
{"text": "be requested for a web page or other application). This is very similar functionality to what a web: server or Content Delivery Network (CDN) edge server (a server CDN uses to store content in many\nlocations so content is geographically/physically closer to users, resulting in faster performance)\nmight provide. Other important aspects of the system are:\n\u2022 There is no limit to the number of images that will be stored, so storage scalability, in terms of\nimage count needs to be considered. \u2022 There needs to be low latency for image downloads/requests. \u2022 If a user uploads an image, the image should always be there (data reliability for images). \u2022 The system should be easy to maintain (manageability). \u2022 Since image hosting doesn\u2019t have high pro\ufb01t margins, the system needs to be cost-e\ufb00ective. Figure 1.1 is a simpli\ufb01ed diagram of the functionality. Figure 1.1: Simpli\ufb01ed architecture diagram for image hosting application\nIn this image hosting example, the system must be perceivably fast, its data stored reliably and\nall of these attributes highly scalable. Building a small version of this application would be trivial\nand easily hosted on a single server; however, that would not be interesting for this chapter. Let\u2019s\nassume that we want to build something that could grow as big as Flickr. Services\nWhen considering scalable system design, it helps to decouple functionality and think about each\npart of the system as its own service with a clearly de\ufb01ned interface. In practice, systems designed\nin this way are said to have a Service-Oriented Architecture (SOA). For these types of systems, each\nservice has its own distinct functional context, and interaction with anything outside of that context\ntakes place through an abstract interface, typically the public-facing API of another service. Kate Matsudaira\n3", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 17", "position": 17, "chunk_type": "semantic", "token_estimate": 294}
{"text": "Deconstructing a system into a set of complementary services decouples the operation of those: pieces from one another. This abstraction helps establish clear relationships between the service, its\nunderlying environment, and the consumers of that service. Creating these clear delineations can\nhelp isolate problems, but also allows each piece to scale independently of one another. This sort of\nservice-oriented design for systems is very similar to object-oriented design for programming. In our example, all requests to upload and retrieve images are processed by the same server;\nhowever, as the system needs to scale it makes sense to break out these two functions into their own\nservices. Fast-forward and assume that the service is in heavy use; such a scenario makes it easy to see\nhow longer writes will impact the time it takes to read the images (since they two functions will be\ncompeting for shared resources). Depending on the architecture this e\ufb00ect can be substantial. Even\nif the upload and download speeds are the same (which is not true of most IP networks, since most\nare designed for at least a 3:1 download-speed:upload-speed ratio), read \ufb01les will typically be read\nfrom cache, and writes will have to go to disk eventually (and perhaps be written several times in\neventually consistent situations). Even if everything is in memory or read from disks (like SSDs),\ndatabase writes will almost always be slower than reads1. Another potential problem with this design is that a web server like Apache or lighttpd typically\nhas an upper limit on the number of simultaneous connections it can maintain (defaults are around\n500, but can go much higher) and in high tra\ufb03c, writes can quickly consume all of those. Since reads\ncan be asynchronous, or take advantage of other performance optimizations like gzip compression or\nchunked transfer encoding, the web server can switch serve reads faster and switch between clients\nquickly serving many more requests per second than the max number of connections (with Apache\nand max connections set to 500, it is not uncommon to serve several thousand read requests per\nsecond). Writes, on the other hand, tend to maintain an open connection for the duration for the\nupload, so uploading a 1MB \ufb01le could take more than 1 second on most home networks, so that web\nserver could only handle 500 such simultaneous writes.", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 18", "position": 18, "chunk_type": "semantic", "token_estimate": 389}
{"text": "Deconstructing a system into a set of complementary services decouples the operation of those: Since reads\ncan be asynchronous, or take advantage of other performance optimizations like gzip compression or\nchunked transfer encoding, the web server can switch serve reads faster and switch between clients\nquickly serving many more requests per second than the max number of connections (with Apache\nand max connections set to 500, it is not uncommon to serve several thousand read requests per\nsecond). Writes, on the other hand, tend to maintain an open connection for the duration for the\nupload, so uploading a 1MB \ufb01le could take more than 1 second on most home networks, so that web\nserver could only handle 500 such simultaneous writes. Planning for this sort of bottleneck makes a good case to split out reads and writes of images\ninto their own services, shown in Figure 1.2. This allows us to scale each of them independently\n(since it is likely we will always do more reading than writing), but also helps clarify what is going\non at each point. Finally, this separates future concerns, which would make it easier to troubleshoot\nand scale a problem like slow reads. The advantage of this approach is that we are able to solve problems independently of one\nanother\u2014we don\u2019t have to worry about writing and retrieving new images in the same context. Both\nof these services still leverage the global corpus of images, but they are free to optimize their own\nperformance with service-appropriate methods (for example, queuing up requests, or caching popular\nimages\u2014more on this below). And from a maintenance and cost perspective each service can scale\nindependently as needed, which is great because if they were combined and intermingled, one could\ninadvertently impact the performance of the other as in the scenario discussed above. Of course, the above example can work well when you have two di\ufb00erent endpoints (in fact this\nis very similar to several cloud storage providers\u2019 implementations and Content Delivery Networks). There are lots of ways to address these types of bottlenecks though, and each has di\ufb00erent tradeo\ufb00s. For example, Flickr solves this read/write issue by distributing users across di\ufb00erent shards such\nthat each shard can only handle a set number of users, and as users increase more shards are added to\n1Pole Position, an open source tool for DB benchmarking, http://polepos.org/ and results http://polepos. sourceforge.net/results/PolePositionClientServer.pdf. 4\nScalable Web Architecture and Distributed Systems", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 18", "position": 18, "chunk_type": "semantic", "token_estimate": 405}
{"text": "Figure 1.2: Splitting out reads and writes: the cluster2. In the \ufb01rst example it is easier to scale hardware based on actual usage (the number of\nreads and writes across the whole system), whereas Flickr scales with their user base (but forces the\nassumption of equal usage across users so there can be extra capacity). In the former an outage or\nissue with one of the services brings down functionality across the whole system (no-one can write\n\ufb01les, for example), whereas an outage with one of Flickr\u2019s shards will only a\ufb00ect those users. In the\n\ufb01rst example it is easier to perform operations across the whole dataset\u2014for example, updating the\nwrite service to include new metadata or searching across all image metadata\u2014whereas with the\nFlickr architecture each shard would need to be updated or searched (or a search service would need\nto be created to collate that metadata\u2014which is in fact what they do). When it comes to these systems there is no right answer, but it helps to go back to the principles\nat the start of this chapter, determine the system needs (heavy reads or writes or both, level of concur-\nrency, queries across the data set, ranges, sorts, etc. ), benchmark di\ufb00erent alternatives, understand\nhow the system will fail, and have a solid plan for when failure happens. Redundancy\nIn order to handle failure gracefully a web architecture must have redundancy of its services and\ndata. For example, if there is only one copy of a \ufb01le stored on a single server, then losing that server\nmeans losing that \ufb01le. Losing data is seldom a good thing, and a common way of handling it is to\ncreate multiple, or redundant, copies. This same principle also applies to services. If there is a core piece of functionality for an\napplication, ensuring that multiple copies or versions are running simultaneously can secure against\nthe failure of a single node. 2Presentation on Flickr\u2019s scaling: http://mysqldba.blogspot.com/2008/04/mysql-uc-2007-presentation-file. html\nKate Matsudaira\n5", "domains": ["Design Principles"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 19", "position": 19, "chunk_type": "semantic", "token_estimate": 329}
{"text": "Creating redundancy in a system can remove single points of failure and provide a backup or: spare functionality if needed in a crisis. For example, if there are two instances of the same service\nrunning in production, and one fails or degrades, the system can failover to the healthy copy. Failover\ncan happen automatically or require manual intervention. Another key part of service redundancy is creating a shared-nothing architecture. With this\narchitecture, each node is able to operate independently of one another and there is no central \u201cbrain\u201d\nmanaging state or coordinating activities for the other nodes. This helps a lot with scalability since\nnew nodes can be added without special conditions or knowledge. However, and most importantly,\nthere is no single point of failure in these systems, so they are much more resilient to failure. For example, in our image server application, all images would have redundant copies on another\npiece of hardware somewhere (ideally in a di\ufb00erent geographic location in the event of a catastrophe\nlike an earthquake or \ufb01re in the data center), and the services to access the images would be redundant,\nall potentially servicing requests. (See Figure 1.3.) (Load balancers are a great way to make this\npossible, but there is more on that below). Figure 1.3: Image hosting application with redundancy\nPartitions\nThere may be very large data sets that are unable to \ufb01t on a single server. It may also be the case\nthat an operation requires too many computing resources, diminishing performance and making it\nnecessary to add capacity. In either case you have two choices: scale vertically or horizontally. Scaling vertically means adding more resources to an individual server. So for a very large data\nset, this might mean adding more (or bigger) hard drives so a single server can contain the entire\ndata set. In the case of the compute operation, this could mean moving the computation to a bigger\nserver with a faster CPU or more memory. In each case, vertical scaling is accomplished by making\nthe individual resource capable of handling more on its own. To scale horizontally, on the other hand, is to add more nodes. In the case of the large data\nset, this might be a second server to store parts of the data set, and for the computing resource it\nwould mean splitting the operation or load across some additional nodes. To take full advantage of\n6\nScalable Web Architecture and Distributed Systems", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 20", "position": 20, "chunk_type": "semantic", "token_estimate": 409}
{"text": "horizontal scaling, it should be included as an intrinsic design principle of the system architecture,: otherwise it can be quite cumbersome to modify and separate out the context to make this possible. When it comes to horizontal scaling, one of the more common techniques is to break up your\nservices into partitions, or shards. The partitions can be distributed such that each logical set of\nfunctionality is separate; this could be done by geographic boundaries, or by another criteria like\nnon-paying versus paying users. The advantage of these schemes is that they provide a service or\ndata store with added capacity. In our image server example, it is possible that the single \ufb01le server used to store images could\nbe replaced by multiple \ufb01le servers, each containing its own unique set of images. (See Figure 1.4.) Such an architecture would allow the system to \ufb01ll each \ufb01le server with images, adding additional\nservers as the disks become full. The design would require a naming scheme that tied an image\u2019s\n\ufb01lename to the server containing it. An image\u2019s name could be formed from a consistent hashing\nscheme mapped across the servers. Or alternatively, each image could be assigned an incremental\nID, so that when a client makes a request for an image, the image retrieval service only needs to\nmaintain the range of IDs that are mapped to each of the servers (like an index). Figure 1.4: Image hosting application with redundancy and partitioning\nOf course there are challenges distributing data or functionality across multiple servers. One of\nthe key issues is data locality; in distributed systems the closer the data to the operation or point of\ncomputation, the better the performance of the system. Therefore it is potentially problematic to\nhave data spread across multiple servers, as any time it is needed it may not be local, forcing the\nservers to perform a costly fetch of the required information across the network. Another potential issue comes in the form of inconsistency. When there are di\ufb00erent services\nreading and writing from a shared resource, potentially another service or data store, there is the\nchance for race conditions\u2014where some data is supposed to be updated, but the read happens prior\nto the update\u2014and in those cases the data is inconsistent.", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 21", "position": 21, "chunk_type": "semantic", "token_estimate": 378}
{"text": "There are certainly some obstacles associated with partitioning data, but partitioning allows each: problem to be split\u2014by data, load, usage patterns, etc.\u2014into manageable chunks. This can help\nwith scalability and manageability, but is not without risk. There are lots of ways to mitigate risk\nand handle failures; however, in the interest of brevity they are not covered in this chapter. If you are\ninterested in reading more, you can check out my blog post on fault tolerance and monitoring3. 1.3\nThe Building Blocks of Fast and Scalable Data Access\nHaving covered some of the core considerations in designing distributed systems, let\u2019s now talk\nabout the hard part: scaling access to the data. Most simple web applications, for example, LAMP stack applications, look something like\nFigure 1.5. Figure 1.5: Simple web applications\nAs they grow, there are two main challenges: scaling access to the app server and to the database. In a highly scalable application design, the app (or web) server is typically minimized and often\nembodies a shared-nothing architecture. This makes the app server layer of the system horizontally\nscalable. As a result of this design, the heavy lifting is pushed down the stack to the database server\nand supporting services; it\u2019s at this layer where the real scaling and performance challenges come\ninto play. The rest of this chapter is devoted to some of the more common strategies and methods for\nmaking these types of services fast and scalable by providing fast access to data. Figure 1.6: Oversimpli\ufb01ed web application\nMost systems can be oversimpli\ufb01ed to Figure 1.6. This is a great place to start. If you have a lot\nof data, you want fast and easy access, like keeping a stash of candy in the top drawer of your desk. Though overly simpli\ufb01ed, the previous statement hints at two hard problems: scalability of storage\nand fast access of data. For the sake of this section, let\u2019s assume you have many terabytes (TB) of data and you want\nto allow users to access small portions of that data at random. (See Figure 1.7.) This is similar to\nlocating an image \ufb01le somewhere on the \ufb01le server in the image application example. 3http://katemats.com/2011/11/13/distributed-systems-basics-handling-failure-fault-tolerance-and-\nmonitoring/\n8\nScalable Web Architecture and Distributed Systems", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 22", "position": 22, "chunk_type": "semantic", "token_estimate": 371}
{"text": "clients. However, there are some cases where the second implementation makes more sense. For: example, if the cache is being used for very large \ufb01les, a low cache hit percentage would cause the\ncache bu\ufb00er to become overwhelmed with cache misses; in this situation it helps to have a large\npercentage of the total data set (or hot data set) in the cache. Another example is an architecture\nwhere the \ufb01les stored in the cache are static and shouldn\u2019t be evicted. (This could be because of\napplication requirements around that data latency\u2014certain pieces of data might need to be very fast\nfor large data sets\u2014where the application logic understands the eviction strategy or hot spots better\nthan the cache.) Distributed Cache\nIn a distributed cache (Figure 1.12), each of its nodes own part of the cached data, so if a refrigerator\nacts as a cache to the grocery store, a distributed cache is like putting your food in several locations\u2014\nyour fridge, cupboards, and lunch box\u2014convenient locations for retrieving snacks from, without a\ntrip to the store. Typically the cache is divided up using a consistent hashing function, such that if\na request node is looking for a certain piece of data it can quickly know where to look within the\ndistributed cache to determine if that data is available. In this case, each node has a small piece\nof the cache, and will then send a request to another node for the data before going to the origin. Therefore, one of the advantages of a distributed cache is the increased cache space that can be had\njust by adding nodes to the request pool. Figure 1.12: Distributed cache\nA disadvantage of distributed caching is remedying a missing node. Some distributed caches get\naround this by storing multiple copies of the data on di\ufb00erent nodes; however, you can imagine how\nthis logic can get complicated quickly, especially when you add or remove nodes from the request\n12\nScalable Web Architecture and Distributed Systems", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 26", "position": 26, "chunk_type": "semantic", "token_estimate": 333}
{"text": "layer. Although even if a node disappears and part of the cache is lost, the requests will just pull: from the origin\u2014so it isn\u2019t necessarily catastrophic! The great thing about caches is that they usually make things much faster (implemented correctly,\nof course!) The methodology you choose just allows you to make it faster for even more requests. However, all this caching comes at the cost of having to maintain additional storage space, typically\nin the form of expensive memory; nothing is free. Caches are wonderful for making things generally\nfaster, and moreover provide system functionality under high load conditions when otherwise there\nwould be complete service degradation. One example of a popular open source cache is Memcached5 (which can work both as a local\ncache and distributed cache); however, there are many other options (including many language- or\nframework-speci\ufb01c options). Memcached is used in many large web sites, and even though it can be very powerful, it is simply\nan in-memory key value store, optimized for arbitrary data storage and fast lookups (O(1)). Facebook uses several di\ufb00erent types of caching to obtain their site performance6. They use\n$GLOBALS and APC caching at the language level (provided in PHP at the cost of a function call)\nwhich helps make intermediate function calls and results much faster. (Most languages have these\ntypes of libraries to improve web page performance and they should almost always be used.) Facebook\nthen use a global cache that is distributed across many servers7, such that one function call accessing\nthe cache could make many requests in parallel for data stored on di\ufb00erent Memcached servers. This allows them to get much higher performance and throughput for their user pro\ufb01le data, and\nhave one central place to update data (which is important, since cache invalidation and maintaining\nconsistency can be challenging when you are running thousands of servers). Now let\u2019s talk about what to do when the data isn\u2019t in the cache... Proxies\nAt a basic level, a proxy server is an intermediate piece of hardware/software that receives requests\nfrom clients and relays them to the backend origin servers. Typically, proxies are used to \ufb01lter requests,\nlog requests, or sometimes transform requests (by adding/removing headers, encrypting/decrypting,\nor compression). Figure 1.13: Proxy server\nProxies are also immensely helpful when coordinating requests from multiple servers, providing\nopportunities to optimize request tra\ufb03c from a system-wide perspective.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 27", "position": 27, "chunk_type": "semantic", "token_estimate": 395}
{"text": "layer. Although even if a node disappears and part of the cache is lost, the requests will just pull: Typically, proxies are used to \ufb01lter requests,\nlog requests, or sometimes transform requests (by adding/removing headers, encrypting/decrypting,\nor compression). Figure 1.13: Proxy server\nProxies are also immensely helpful when coordinating requests from multiple servers, providing\nopportunities to optimize request tra\ufb03c from a system-wide perspective. One way to use a proxy to\nspeed up data access is to collapse the same (or similar) requests together into one request, and then\nreturn the single result to the requesting clients. This is known as collapsed forwarding. Imagine there is a request for the same data (let\u2019s call it littleB) across several nodes, and that\npiece of data is not in the cache. If that request is routed thought the proxy, then all of those requests\n5http://memcached.org/\n6Facebook caching and performance, http://sizzo.org/talks/. 7Scaling memcached at Facebook, http://www.facebook.com/note.php?note_id=39391378919. Kate Matsudaira\n13", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 27", "position": 27, "chunk_type": "semantic", "token_estimate": 155}
{"text": "can be collapsed into one, which means we only have to read littleB o\ufb00disk once. (See Figure 1.14.): There is some cost associated with this design, since each request can have slightly higher latency,\nand some requests may be slightly delayed to be grouped with similar ones. But it will improve\nperformance in high load situations, particularly when that same data is requested over and over. This is similar to a cache, but instead of storing the data/document like a cache, it is optimizing the\nrequests or calls for those documents and acting as a proxy for those clients. In a LAN proxy, for example, the clients do not need their own IPs to connect to the Internet, and\nthe LAN will collapse calls from the clients for the same content. It is easy to get confused here\nthough, since many proxies are also caches (as it is a very logical place to put a cache), but not all\ncaches act as proxies. Figure 1.14: Using a proxy server to collapse requests\nAnother great way to use the proxy is to not just collapse requests for the same data, but also to\ncollapse requests for data that is spatially close together in the origin store (consecutively on disk). Employing such a strategy maximizes data locality for the requests, which can result in decreased\nrequest latency. For example, let\u2019s say a bunch of nodes request parts of B: partB1, partB2, etc. We\ncan set up our proxy to recognize the spatial locality of the individual requests, collapsing them into\na single request and returning only bigB, greatly minimizing the reads from the data origin. (See\nFigure 1.15.) This can make a really big di\ufb00erence in request time when you are randomly accessing\nacross TBs of data! Proxies are especially helpful under high load situations, or when you have\nlimited caching, since they can essentially batch several requests into one. Figure 1.15: Using a proxy to collapse requests for data that is spatially close together\nIt is worth noting that you can use proxies and caches together, but generally it is best to put the\ncache in front of the proxy, for the same reason that it is best to let the faster runners start \ufb01rst in a\n14\nScalable Web Architecture and Distributed Systems", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 28", "position": 28, "chunk_type": "semantic", "token_estimate": 383}
{"text": "crowded marathon race. This is because the cache is serving data from memory, it is very fast, and it: doesn\u2019t mind multiple requests for the same result. But if the cache was located on the other side of\nthe proxy server, then there would be additional latency with every request before the cache, and this\ncould hinder performance. If you are looking at adding a proxy to your systems, there are many options to consider; Squid8\nand Varnish9 have both been road tested and are widely used in many production web sites. These\nproxy solutions o\ufb00er many optimizations to make the most of client-server communication. Installing\none of these as a reverse proxy (explained in the load balancer section below) at the web server layer\ncan improve web server performance considerably, reducing the amount of work required to handle\nincoming client requests. Indexes\nUsing an index to access your data quickly is a well-known strategy for optimizing data access\nperformance; probably the most well known when it comes to databases. An index makes the\ntrade-o\ufb00s of increased storage overhead and slower writes (since you must both write the data and\nupdate the index) for the bene\ufb01t of faster reads. Figure 1.16: Indexes\nJust as to a traditional relational data store, you can also apply this concept to larger data sets. The trick with indexes is you must carefully consider how users will access your data. In the case of\ndata sets that are many TBs in size, but with very small payloads (e.g., 1 KB), indexes are a necessity\nfor optimizing data access. Finding a small payload in such a large data set can be a real challenge\nsince you can\u2019t possibly iterate over that much data in any reasonable time. Furthermore, it is very\nlikely that such a large data set is spread over several (or many!) physical devices\u2014this means you\n8http://www.squid-cache.org/\n9https://www.varnish-cache.org/\nKate Matsudaira\n15", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 29", "position": 29, "chunk_type": "semantic", "token_estimate": 317}
{"text": "CPU utilization. Load balancers can be implemented as software or hardware appliances. One open: source software load balancer that has received wide adoption is HAProxy11. In a distributed system, load balancers are often found at the very front of the system, such that\nall incoming requests are routed accordingly. In a complex distributed system, it is not uncommon\nfor a request to be routed to multiple load balancers as shown in Figure 1.19. Figure 1.19: Multiple load balancers\nLike proxies, some load balancers can also route a request di\ufb00erently depending on the type of\nrequest it is. (Technically these are also known as reverse proxies.) One of the challenges with load balancers is managing user-session-speci\ufb01c data. In an e-\ncommerce site, when you only have one client it is very easy to allow users to put things in their\nshopping cart and persist those contents between visits (which is important, because it is much\nmore likely you will sell the product if it is still in the user\u2019s cart when they return). However, if a\nuser is routed to one node for a session, and then a di\ufb00erent node on their next visit, there can be\ninconsistencies since the new node may be missing that user\u2019s cart contents. (Wouldn\u2019t you be upset\nif you put a 6 pack of Mountain Dew in your cart and then came back and it was empty?) One way\naround this can be to make sessions sticky so that the user is always routed to the same node, but\nthen it is very hard to take advantage of some reliability features like automatic failover. In this case,\nthe user\u2019s shopping cart would always have the contents, but if their sticky node became unavailable\nthere would need to be a special case and the assumption of the contents being there would no longer\nbe valid (although hopefully this assumption wouldn\u2019t be built into the application). Of course, this\nproblem can be solved using other strategies and tools in this chapter, like services, and many not\ncovered (like browser caches, cookies, and URL rewriting). If a system only has a couple of a nodes, systems like round robin DNS may make more sense\nsince load balancers can be expensive and add an unneeded layer of complexity.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 32", "position": 32, "chunk_type": "semantic", "token_estimate": 379}
{"text": "CPU utilization. Load balancers can be implemented as software or hardware appliances. One open: Of course, this\nproblem can be solved using other strategies and tools in this chapter, like services, and many not\ncovered (like browser caches, cookies, and URL rewriting). If a system only has a couple of a nodes, systems like round robin DNS may make more sense\nsince load balancers can be expensive and add an unneeded layer of complexity. Of course in larger\nsystems there are all sorts of di\ufb00erent scheduling and load-balancing algorithms, including simple\nones like random choice or round robin, and more sophisticated mechanisms that take things like\nutilization and capacity into consideration. All of these algorithms allow tra\ufb03c and requests to be\ndistributed, and can provide helpful reliability tools like automatic failover, or automatic removal of\na bad node (such as when it becomes unresponsive). However, these advanced features can make\nproblem diagnosis cumbersome. For example, when it comes to high load situations, load balancers\nwill remove nodes that may be slow or timing out (because of too many requests), but that only\nexacerbates the situation for the other nodes. In these cases extensive monitoring is important,\nbecause overall system tra\ufb03c and throughput may look like it is decreasing (since the nodes are\nserving less requests) but the individual nodes are becoming maxed out. Load balancers are an easy way to allow you to expand system capacity, and like the other\ntechniques in this article, play an essential role in distributed system architecture. Load balancers\n11http://haproxy.1wt.eu/\n18\nScalable Web Architecture and Distributed Systems", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 32", "position": 32, "chunk_type": "semantic", "token_estimate": 262}
{"text": "also provide the critical function of being able to test the health of a node, such that if a node is: unresponsive or over-loaded, it can be removed from the pool handling requests, taking advantage of\nthe redundancy of di\ufb00erent nodes in your system. Queues\nSo far we have covered a lot of ways to read data quickly, but another important part of scaling the\ndata layer is e\ufb00ective management of writes. When systems are simple, with minimal processing\nloads and small databases, writes can be predictably fast; however, in more complex systems writes\ncan take an almost non-deterministically long time. For example, data may have to be written several\nplaces on di\ufb00erent servers or indexes, or the system could just be under high load. In the cases where\nwrites, or any task for that matter, may take a long time, achieving performance and availability\nrequires building asynchrony into the system; a common way to do that is with queues. Figure 1.20: Synchronous request\nImagine a system where each client is requesting a task to be remotely serviced. Each of these\nclients sends their request to the server, where the server completes the tasks as quickly as possible\nand returns the results to their respective clients. In small systems where one server (or logical\nservice) can service incoming clients just as fast as they come, this sort of situation should work\njust \ufb01ne. However, when the server receives more requests than it can handle, then each client is\nforced to wait for the other clients\u2019 requests to complete before a response can be generated. This is\nan example of a synchronous request, depicted in Figure 1.20. This kind of synchronous behavior can severely degrade client performance; the client is forced\nto wait, e\ufb00ectively performing zero work, until its request can be answered. Adding additional\nservers to address system load does not solve the problem either; even with e\ufb00ective load balancing\nKate Matsudaira\n19", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 33", "position": 33, "chunk_type": "semantic", "token_estimate": 323}
{"text": "in place it is extremely di\ufb03cult to ensure the even and fair distribution of work required to maximize: client performance. Further, if the server handling requests is unavailable, or fails, then the clients\nupstream will also fail. Solving this problem e\ufb00ectively requires abstraction between the client\u2019s\nrequest and the actual work performed to service it. Figure 1.21: Using queues to manage requests\nEnter queues. A queue is as simple as it sounds: a task comes in, is added to the queue and then\nworkers pick up the next task as they have the capacity to process it. (See Figure 1.21.) These tasks\ncould represent simple writes to a database, or something as complex as generating a thumbnail\npreview image for a document. When a client submits task requests to a queue they are no longer\nforced to wait for the results; instead they need only acknowledgement that the request was properly\nreceived. This acknowledgement can later serve as a reference for the results of the work when the\nclient requires it. Queues enable clients to work in an asynchronous manner, providing a strategic abstraction\nof a client\u2019s request and its response. On the other hand, in a synchronous system, there is no\ndi\ufb00erentiation between request and reply, and they therefore cannot be managed separately. In an\nasynchronous system the client requests a task, the service responds with a message acknowledging\nthe task was received, and then the client can periodically check the status of the task, only requesting\nthe result once it has completed. While the client is waiting for an asynchronous request to be\ncompleted it is free to perform other work, even making asynchronous requests of other services. The latter is an example of how queues and messages are leveraged in distributed systems. Queues also provide some protection from service outages and failures. For instance, it is quite\neasy to create a highly robust queue that can retry service requests that have failed due to transient\nserver failures. It is more preferable to use a queue to enforce quality-of-service guarantees than to\nexpose clients directly to intermittent service outages, requiring complicated and often-inconsistent\nclient-side error handling. Queues are fundamental in managing distributed communication between di\ufb00erent parts of any\n20\nScalable Web Architecture and Distributed Systems", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 34", "position": 34, "chunk_type": "semantic", "token_estimate": 377}
{"text": "[chapter2]: This chapter describes the mechanics of how we generate release builds for Firefox. Most of this\nchapter details the signi\ufb01cant steps that occur in a release process once the builds start, but there is\nalso plenty of complex cross-group communication to deal with before Release Engineering even\nstarts to generate release builds, so let\u2019s start there. 2.1\nLook N Ways Before You Start a Release\nWhen we started on the project to improve Mozilla\u2019s release process, we began with the premise that\nthe more popular Firefox became, the more users we would have, and the more attractive a target\nFirefox would become to blackhat hackers looking for security vulnerabilities to exploit. Also, the\nmore popular Firefox became, the more users we would have to protect from a newly discovered", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 37", "position": 37, "chunk_type": "semantic", "token_estimate": 130}
{"text": "Figure 2.1: Getting from code to \u201cGo to build\u201d: security vulnerability, so the more important it would be to be able to deliver a security \ufb01x as quickly\nas possible. We even have a term for this: a \u201cchemspill\u201d release1. Instead of being surprised by the\noccasional need for a chemspill release in between our regularly scheduled releases, we decided to\nplan as if every release could be a chemspill release, and designed our release automation accordingly. This mindset has three important consequences:\n1. We do a postmortem after every release, and look to see where things could be made smoother,\neasier, and faster next time. If at all possible, we \ufb01nd and \ufb01x at least one thing, no matter\nhow small, immediately\u2014before the next release. This constant polishing of our release\nautomation means we\u2019re always looking for new ways to rely on less human involvement\nwhile also improving robustness and turnaround time. A lot of e\ufb00ort is spent making our\ntools and processes bulletproof so that \u201crare\u201d events like network hiccups, disk space issues\nor typos made by real live humans are caught and handled as early as possible. Even though\nwe\u2019re already fast enough for regular, non-chemspill releases, we want to reduce the risk of\nany human error in a future release. This is especially true in a chemspill release. 2. When we do have a chemspill release, the more robust the release automation, the less stressed\nthe humans in Release Engineering are. We\u2019re used to the idea of going as fast as possible\nwith calm precision, and we\u2019ve built tools to do this as safely and robustly as we know how. Less stress means more calm and precise work within a well-rehearsed process, which in turn\nhelps chemspill releases go smoothly. 1Short for \u201cchemical spill\u201d. 24\nFirefox Release Engineering", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 38", "position": 38, "chunk_type": "semantic", "token_estimate": 302}
{"text": "3. We created a Mozilla-wide \u201cgo to build\u201d process. When doing a regular (non-chemspill): release, it\u2019s possible to have everyone look through the same bug triage queries, see clearly\nwhen the last \ufb01x was landed and tested successfully, and reach consensus on when to start\nbuilds. However, in a chemspill release\u2014where minutes matter\u2014keeping track of all the\ndetails of the issue as well as following up bug con\ufb01rmations and \ufb01xes gets very tricky very\nquickly. To reduce complexity and the risk of mistakes, Mozilla now has a full-time person\nto track the readiness of the code for a \u201cgo to build\u201d decision. Changing processes during\na chemspill is risky, so in order to make sure everyone is familiar with the process when\nminutes matter, we use this same process for chemspill and regular releases. QA does \nmanual testing \nof bug that \ncaused \nchemspill\n12:50   13:18   13:42     16:16   17:02   17:46   18:02  18:30   21:30      1:02   5:18   6:31         12:38   13:16   14:37   14:59       16:47 \nGo-to-\nbuild \nemail \nfrom \ndriver\nTaggine \nends, builds \nand source \nbundle \nbuilder \ntriggered\nLinux \nl10n \nrepacks \nstart\nQA does \nautomated \ntesting of \nbuilds\nWin32 \nrepacks \nstart\nMac, \nWin32, \nLinux64 \npartner \nrepacks \nstart\nUpdates \ncomplete \nand \ntrigger \nupdate \nverify\nDriver sends \nemail to \npush mobile \nFirefox to \nMarket\nQA signs off \non mobile \nupdates \nfrom Android \nMarket\nQA signs off \non release \nchannel \nupdates\nTagging \nstarts\nAutosign is \nstarted on \nkeymaster \nwhile builds \nare created\nLinux \npartner \nrepacks \nstart\nAndroid \nbuilds are \nsigned \nand \nveri\ufb01ed\nMac \nrepacks \nstart\nLinux64 \nrepacks \nstart\nSigning is \ncompleted \nand l10n \nveri\ufb01cation \nand updates \ntriggered\nQA signs off \non mobile \nbuilds\nUpdates are \npushed to \ninternal \nmirrors and \nQA runs \nautomated \nupdate \nveri\ufb01cation \ntests\nDriver sends \nemail \nrequest to \npush \ndesktop \nFirefox to \nrelease \nchannel\nFigure 2.2: Complete release timeline, using a chemspill as example\n2.2\n\"Go to Build\"\nWho Can Send the \u201cGo to Build\u201d? Before the start of the release, one person is designated to assume responsibility for coordinating the\nentire release. This person needs to attend triage meetings, understand the background context on all\nthe work being landed, referee bug severity disputes fairly, approve landing of late-breaking changes,\nand make tough back-out decisions. Additionally, on the actual release day this person is on point\nfor all communications with the di\ufb00erent groups (developers, QA, Release Engineering, website\ndevelopers, PR, marketing, etc.). Di\ufb00erent companies use di\ufb00erent titles for this role. Some titles we\u2019ve heard include Release\nManager, Release Engineer, Program Manager, Project Manager, Product Manager, Product Czar,\nRelease Driver.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 39", "position": 39, "chunk_type": "semantic", "token_estimate": 414}
{"text": "show-stopper problems are redirected to the Release Coordinator, who will evaluate, make the \ufb01nal: go/no-go decision and communicate that decision to everyone in a timely manner. In the heat of\nthe moment of a release day, we all have to abide by, and trust, the coordination decisions that this\nperson makes. How to Send the \u201cGo to Build\u201d? Early experiments with sending \u201cgo to build\u201d in IRC channels or verbally over the phone led to\nmisunderstandings, occasionally causing problems for the release in progress. Therefore, we now\nrequire that the \u201cgo to build\u201d signal for every release is done by email, to a mailing list that includes\neveryone across all groups involved in release processes. The subject of the email includes \u201cgo to\nbuild\u201d and the explicit product name and version number; for example:\ngo to build Firefox 6.0.1\nSimilarly, if a problem is found in the release, the Release Coordinator will send a new \u201call stop\u201d\nemail to the same mailing list, with a new subject line. We found that it was not su\ufb03cient to just hit\nreply on the most recent email about the release; email threading in some email clients caused some\npeople to not notice the \u201call stop\u201d email if it was way down a long and unrelated thread. What Is In the \u201cGo to Build\u201d Email? 1. The exact code to be used in the build; ideally, the URL to the speci\ufb01c change in the source\ncode repository that the release builds are to be created from. (a) Instructions like \u201cuse the latest code\u201d are never acceptable; in one release, after the\n\u201cgo to build\u201d email was sent and before builds started, a well-intentioned developer\nlanded a change, without approval, in the wrong branch. The release included that\nunwanted change in the builds. Thankfully the mistake was caught before we shipped,\nbut we did have to delay the release while we did a full stop and rebuilt everything. (b) In a time-based version control system like CVS, be fully explicit about the exact\ntime to use; give the time down to seconds, and specify timezone. In one release,\nwhen Firefox was still based on CVS, the Release Coordinator speci\ufb01ed the cuto\ufb00\ntime to be used for the builds but did not give the timezone. By the time Release\nEngineering noticed the missing timezone info, the Release Coordinator was asleep.", "domains": ["Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 40", "position": 40, "chunk_type": "semantic", "token_estimate": 394}
{"text": "(b) Some releases are \u201cchemspills\u201d, and are urgent, where minutes matter. These are: typically to \ufb01x a published security exploit, or to \ufb01x a newly introduced top-crash\nproblem impacting a large percentage of our user base. Chemspills need to be created\nas quickly as possible and are typically not pre-scheduled releases. (c) Some releases change from routine to chemspill or from chemspill to routine. For\nexample, if a security \ufb01x in a routine release was accidentally leaked, it is now a\nchemspill release. If a business requirement like a \u201cspecial sneak preview\u201d release for\nan upcoming conference announcement was delayed for business reasons, the release\nnow changes from chemspill to routine. (d) Some releases have di\ufb00erent people holding di\ufb00erent opinions on whether the release\nis normal or urgent, depending on their perspective on the \ufb01xes being shipped in the\nrelease. It is the role of the Release Coordinator to balance all the facts and opinions, reach a decision,\nand then communicate that decision about urgency consistently across all groups. If new information\narrives, the Release Coordinator reassesses, and then communicates the new urgency to all the same\ngroups. Having some groups believe a release is a chemspill, while other groups believe the same\nrelease is routine can be destructive to cross-group cohesion. Finally, these emails also became very useful to measure where time was spent during a release. While they are only accurate to wall-clock time resolution, this accuracy is really helpful when\n\ufb01guring out where next to focus our e\ufb00orts on making things faster. As the old adage goes, before\nyou can improve something, you have to be able to measure it. Throughout the beta cycle for Firefox we also do weekly releases from our mozilla-beta\nrepository2. Each one of these beta releases goes through our usual full release automation, and is\ntreated almost identically to our regular \ufb01nal releases. To minimize surprises during a release, our\nintent is to have no new untested changes to release automation or infrastructure by the time we start\nthe \ufb01nal release builds. 2.3\nTagging, Building, and Source Tarballs\nIn preparation for starting automation, we recently started to use a script, release_sanity.py3, that\nwas originally written by a Release Engineering summer intern. This Python script assists a release\nengineer with double-checking that all con\ufb01gurations for a release match what is checked into our\ntools and con\ufb01guration repositories.", "domains": ["Software Quality Attributes", "Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 41", "position": 41, "chunk_type": "semantic", "token_estimate": 394}
{"text": "(b) Some releases are \u201cchemspills\u201d, and are urgent, where minutes matter. These are: 2.3\nTagging, Building, and Source Tarballs\nIn preparation for starting automation, we recently started to use a script, release_sanity.py3, that\nwas originally written by a Release Engineering summer intern. This Python script assists a release\nengineer with double-checking that all con\ufb01gurations for a release match what is checked into our\ntools and con\ufb01guration repositories. It also checks what is in the speci\ufb01ed release code revisions for\nmozilla-release and all the (human) languages for this release, which will be what the builds and\nlanguage repacks are generated from. The script accepts the buildbot con\ufb01g \ufb01les for any release con\ufb01gurations that will be used (such\nas desktop or mobile), the branch to look at (e.g., mozilla-release), the build and version number,\nand the names of the products that are to be built (such as \u201cfennec\u201d or \u201c\ufb01refox\u201d). It will fail if the\nrelease repositories do not match what\u2019s in the con\ufb01gurations, if locale repository changesets don\u2019t\nmatch our shipping locales and localization changeset \ufb01les, or if the release version and build number\ndon\u2019t match what has been given to our build tools with the tag generated using the product, version,\nand build number. If all the tests in the script pass, it will recon\ufb01gure the buildbot master where the\nscript is being run and where release builders will be triggered, and then generate the \u201csend change\u201d\nthat starts the automated release process. 2http://hg.mozilla.org/releases/mozilla-beta/\n3http://mxr.mozilla.org/build/source/tools/buildbot-helpers/release_sanity.py\nChris AtLee, Lukas Blakk, John O\u2019Duinn, and Armen Zambrano Gasparnian\n27", "domains": ["Design Patterns", "Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 41", "position": 41, "chunk_type": "semantic", "token_estimate": 256}
{"text": "Figure 2.3: Automated tagging: After a release engineer kicks o\ufb00builders, the \ufb01rst automated step in the Firefox release process is\ntagging all the related source code repositories to record which revision of the source, language reposi-\ntories, and related tools are being used for this version and build number of a release candidate. These\ntags allow us to keep a history of Firefox and Fennec (mobile Firefox) releases\u2019 version and build num-\nbers in our release repositories. For Firefox releases, one example tag set is FIREFOX_10_0_RELEASE\nFIREFOX_10_0_BUILD1 FENNEC_10_0_RELEASE FENNEC_10_0_BUILD1. A single Firefox release uses code from about 85 version control repositories that host things\nsuch as the product code, localization strings, release automation code, and helper utilities. Tagging\nall these repositories is critical to ensure that future steps of the release automation are all executed\nusing the same set of revisions. It also has a number of other bene\ufb01ts: Linux distributions and other\ncontributors can reproduce builds with exactly the same code and tools that go into the o\ufb03cial builds,\nand it also records the revisions of source and tools used on a per-release basis for future comparison\nof what changed between releases. Once all the repositories are branched and tagged, a series of dependent builders automatically\nstart up: one builder for each release platform plus a source bundle that includes all source used\nin the release. The source bundle and built installers are all uploaded to the release directory as\nthey become available. This allows anyone to see exactly what code is in a release, and gives a\nsnapshot that would allow us to re-create the builds if we ever needed to (for example, if our VCS\nfailed somehow). For the Firefox build\u2019s source, sometimes we need to import code from an earlier repository. For example, with a beta release this means pulling in the signed-o\ufb00revision from Mozilla-Aurora\n(our more-stable-than-Nightly repository) for Firefox 10.0b1. For a release it means pulling in\nthe approved changes from Mozilla-Beta (typically the same code used for 10.0b6) to the Mozilla-\nRelease repository. This release branch is then created as a named branch whose parent changeset\nis the signed-o\ufb00revision from the \u2018go to build\u2019 provided by the Release Coordinator. The release\nbranch can be used to make release-speci\ufb01c modi\ufb01cations to the source code, such as bumping\n28\nFirefox Release Engineering", "domains": ["Design Patterns", "Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 42", "position": 42, "chunk_type": "semantic", "token_estimate": 386}
{"text": "version numbers or \ufb01nalizing the set of locales that will be built. If a critical security vulnerability is: discovered in the future that requires an immediate \ufb01x\u2014a chemspill\u2014a minimal set of changes to\naddress the vulnerability will be landed on this relbranch and a new version of Firefox generated and\nreleased from it. When we have to do another round of builds for a particular release, buildN, we\nuse these relbranches to grab the same code that was signed o\ufb00on for \u2018go to build\u2019, which is where\nany changes to that release code will have been landed. The automation starts again and bumps the\ntagging to the new changeset on that relbranch. Our tagging process does a lot of operations with local and remote Mercurial repositories. To\nstreamline some of the most common operations we\u2019ve written a few tools to assist us: retry.py4\nand hgtool.py5. retry.py is a simple wrapper that can take a given command and run it, retrying\nseveral times if it fails. It can also watch for exceptional output conditions and retry or report failure\nin those cases. We\u2019ve found it useful to wrap retry.py around most of the commands which can\nfail due to external dependencies. For tagging, the Mercurial operations could fail due to temporary\nnetwork outages, web server issues, or the backend Mercurial server being temporarily overloaded. Being able to automatically retry these operations and continue saves a lot of our time, since we don\u2019t\nhave to manually recover, clean up any fallout and then get the release automation running again. hgtool.py is a utility that encapsulates several common Mercurial operations, like cloning,\npulling, and updating with a single invocation. It also adds support for Mercurial\u2019s share extension,\nwhich we use extensively to avoid having several full clones of repositories in di\ufb00erent directories\non the same machine. Adding support for shared local repositories signi\ufb01cantly sped up our tagging\nprocess, since most full clones of the product and locale repositories could be avoided. An important motivation for developing tools like these is making our automation as testable as\npossible. Because tools like hgtool.py are small, single-purpose utilities built on top of reusable\nlibraries, they\u2019re much easier to test in isolation. Today our tagging is done in two parallel jobs: one for desktop Firefox which takes around 20\nminutes to complete as it includes tagging 80+ locale repositories, and another for mobile Firefox\nwhich takes around 10 minutes to complete since we have fewer locales currently available for our\nmobile releases.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 43", "position": 43, "chunk_type": "semantic", "token_estimate": 415}
{"text": "version numbers or \ufb01nalizing the set of locales that will be built. If a critical security vulnerability is: Because tools like hgtool.py are small, single-purpose utilities built on top of reusable\nlibraries, they\u2019re much easier to test in isolation. Today our tagging is done in two parallel jobs: one for desktop Firefox which takes around 20\nminutes to complete as it includes tagging 80+ locale repositories, and another for mobile Firefox\nwhich takes around 10 minutes to complete since we have fewer locales currently available for our\nmobile releases. In the future we would like to streamline our release automation process so that we\ntag all the various repositories in parallel. The initial builds can be started as soon as the product\ncode and tools requirement repository is tagged, without having to wait for all the locale repositories\nto be tagged. By the time these builds are \ufb01nished, the rest of the repositories will have been tagged\nso that localization repackages and future steps can be completed. We estimate this can reduce the\ntotal time to have builds ready by 15 minutes. 2.4\nLocalization Repacks and Partner Repacks\nOnce the desktop builds are generated and uploaded to ftp.mozilla.org, our automation triggers\nthe localization repackaging jobs. A \u201clocalization repack\u201d takes the original build (which contains\nthe en-US locale), unpacks it, replaces the en-US strings with the strings for another locale that we\nare shipping in this release, then repackages all the \ufb01les back up again (this is why we call them\nrepacks). We repeat this for each locale shipping in the release. Originally, we did all repacks serially. However, as we added more locales, this took a long time to complete, and we had to restart from\nthe beginning if anything failed out mid-way through. 4http://hg.mozilla.org/build/tools/file/7adc08bd1386/lib/python/util/retry.py\n5http://hg.mozilla.org/build/mozharness/file/a0fce0162fd5/scripts/hgtool.py\nChris AtLee, Lukas Blakk, John O\u2019Duinn, and Armen Zambrano Gasparnian\n29", "domains": ["Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 43", "position": 43, "chunk_type": "semantic", "token_estimate": 307}
{"text": "Figure 2.4: Repacking Firefox for each localization: Now, we instead split the entire set of repacks into six jobs, each processed concurrently on six\ndi\ufb00erent machines. This approach completes the work in approximately a sixth of the time. This\nalso allows us to redo a subset of repacks if an individual repack fails, without having to redo all\nrepacks. (We could split the repacks into even more, smaller, concurrent jobs, but we found it took\naway too many machines from the pool, which a\ufb00ected other unrelated jobs triggered by developers\non our continuous integration system.) The process for mobile (on Android) is slightly di\ufb00erent, as we produce only two installers: an\nEnglish version and a multi-locale version with over a dozen languages built into the installer instead\nof a separate build per locale. The size of this multi-locale version is an issue, especially with slow\ndownload speeds onto small mobile devices. One proposal for the future is to have other languages\nbe requested on demand as add-ons from addons.mozilla.org. In Figure 2.4, you can see that we currently rely on three di\ufb00erent sources for our locale in-\nformation: shipped_locales, l10_changesets and l10n-changesets_mobile-release.json. (There is a plan to move all three into a uni\ufb01ed JSON \ufb01le.) These \ufb01les contain information about the\ndi\ufb00erent localizations we have, and certain platform exceptions. Speci\ufb01cally, for a given localization\nwe need to know which revision of the repository to use for a given release and we need to know if\nthe localization can build on all of our supported platforms (e.g., Japanese for Mac comes from a\ndi\ufb00erent repository all together). Two of these \ufb01les are used for the Desktop releases and one for the\nMobile release (this JSON \ufb01le contains both the list of platforms and the changesets). Who decides which languages we ship? First of all, localizers themselves nominate their speci\ufb01c\n30\nFirefox Release Engineering", "domains": ["Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 44", "position": 44, "chunk_type": "semantic", "token_estimate": 312}
{"text": "changeset for a given release. The nominated changeset gets reviewed by Mozilla\u2019s localization team: and shows up in a web dashboard that lists the changesets needed for each language. The Release\nCoordinator reviews this before sending the \u201cgo to build\u201d email. On the day of a release, we retrieve\nthis list of changesets and we repackage them accordingly. Besides localization repackages we also generate partner repackages. These are customized\nbuilds for various partners we have who want to customize the experience for their customers. The\nmain type of changes are custom bookmarks, custom homepage and custom search engines but many\nother things can be changed. These customized builds are generated for the latest Firefox release and\nnot for betas. 2.5\nSigning\nIn order for users to be sure that the copy of Firefox they have downloaded is indeed the unmodi\ufb01ed\nbuild from Mozilla, we apply a few di\ufb00erent types of digital signatures to the builds. The \ufb01rst type of signing is for our Windows builds. We use a Microsoft Authenticode (signcode)\nsigning key to sign all our .exe and .dll \ufb01les. Windows can use these signatures to verify that\nthe application comes from a trusted source. We also sign the Firefox installer executable with the\nAuthenticode key. Next we use GPG to generate a set of MD5 and SHA1 checksums for all the builds on all\nplatforms, and generate detached GPG signatures for the checksum \ufb01les as well as all the builds\nand installers. These signatures are used by mirrors and other community members to validate their\ndownloads. For security purposes, we sign on a dedicated signing machine that is blocked o\ufb00via \ufb01rewall\nand VPN from outside connections. Our keyphrases, passwords, and keystores are passed among\nrelease engineers only in secure channels, often in person, to minimize the risk of exposure as much\nas possible. Until recently this signing process involved a release engineer working on a dedicated server (the\n\u201csigning master\u201d) for almost an hour manually downloading builds, signing them, and uploading them\nback to ftp.mozilla.org before the automation could continue. Once signing on the master was\ncompleted and all \ufb01les were uploaded, a log \ufb01le of all the signing activities was uploaded to the release\ncandidates directory on ftp.mozilla.org. The appearance of this log \ufb01le on ftp.mozilla.org\nsigni\ufb01ed the end of human signing work and from that point, dependent builders watching for\nthat log \ufb01le could resume automation. Recently we\u2019ve added an additional wrapper of automation\naround the signing steps.", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 45", "position": 45, "chunk_type": "semantic", "token_estimate": 412}
{"text": "client tool which would contact the signing server, authenticate itself as a machine trusted to request: signing, upload the \ufb01les to be signed, wait for the build to be signed, download the signed bits, and\nthen include them as part of the packaged build. Once these enhancements are in production, we\ncan discontinue our current all-at-once signing process, as well as our all-at-once generate-updates\nprocess (more on this below). We expect this work to trim a few hours o\ufb00our current end-to-end\ntimes for a release. 2.6\nUpdates\nUpdates are created so users can update to the latest version of Firefox quickly and easily using\nour built-in updater, without having to download and run a standalone installer. From the user\u2019s\nperspective, the downloading of the update package happens quietly in the background. Only after\nthe update \ufb01les are downloaded, and ready to be applied, will Firefox prompt the user with the option\nto apply the update and restart. The catch is, we generate a lot of updates. For a series of releases on a product line, we generate\nupdates from all supported previous releases in the series to the new latest release for that product\nline. For Firefox LATEST, that means generating updates for every platform, every locale, and every\ninstaller from Firefox LATEST-1, LATEST-2, LATEST-3, ... in both complete and partial forms. We\ndo all this for several di\ufb00erent product lines at a time. Our update generation automation modi\ufb01es the update con\ufb01guration \ufb01les of each release\u2019s build\no\ufb00a branch to maintain our canonical list of what version numbers, platforms, and localizations\nneed to have updates created to o\ufb00er users this newest release. We o\ufb00er updates as \u201csnippets\u201d. As\nyou can see in the example below, this snippet is simply an XML pointer \ufb01le hosted on our AUS\n(Application Update Service) that informs the user\u2019s Firefox browser where the complete and/or\npartial .mar (Mozilla Archive) \ufb01les are hosted. Major Updates vs. Minor Updates\nAs you can see in Figure 2.6, update snippets have a type attribute which can be either major or\nminor. Minor updates keep people updated to the latest version available in their release train; for\nexample, it would update all 3.6. * release users to the latest 3.6 release, all rapid-release beta users to\nthe latest beta, all Nightly users to the latest Nightly build, etc. Most of the time, updates are minor\nand don\u2019t require any user interaction other than a con\ufb01rmation to apply the update and restart the\nbrowser.", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 47", "position": 47, "chunk_type": "semantic", "token_estimate": 413}
{"text": "<updates>: <update type=\"minor\" version=\"7.0.1\" extensionVersion=\"7.0.1\"\nbuildID=\"20110928134238\"\ndetailsURL=\"https://www.mozilla.com/en-US/firefox/7.0.1/releasenotes/\">\n<patch type=\"complete\"\nURL=\"http://download.mozilla.org/?product=firefox-7.0.1-complete& os=osx&\\\nlang=en-US&force=1\"\nhashFunction=\"SHA512\"\nhashValue=\"7ecdbc110468b9b4627299794d793874436353dc36c80151550b08830f9d8c\\\n5afd7940c51df9270d54e11fd99806f41368c0f88721fa17e01ea959144f47\\\n3f9d\"\nsize=\"28680122\"/>\n<patch type=\"partial\"\nURL=\"http://download.mozilla.org/?product=firefox-7.0.1-partial-6.0.2&\\\nos=osx&lang=en-US&force=1\"\nhashFunction=\"SHA512\"\nhashValue=\"e9bb49bee862c7a8000de6508d006edf29778b5dbede4deaf3cfa05c22521f\\\nc775da126f5057621960d327615b5186b27d75a378b00981394716e93fc5cc\\\na11a\"\nsize=\"10469801\"/>\n</update>\n</updates>\nFigure 2.6: Sample update snippet\nlook on beta channel, etc.) so that we can serve updates to, for example, release users at a di\ufb00erent\ntime than we serve updates to beta users. Partial update .mar \ufb01les are created by comparing the complete .mar for the old release with the\ncomplete .mar for the new release to create a \u201cpartial-update\u201d .mar \ufb01le containing the binary di\ufb00of\nany changed \ufb01les, and a manifest \ufb01le. As you can see in the sample snippet in Figure 2.6, this results\nin a much smaller \ufb01le size for partial updates. This is very important for users with slower or dial-up\nInternet connections. In older versions of our update automation the generation of partial updates for all locales and\nplatforms could take six to seven hours for one release, as the complete .mar \ufb01les were downloaded,\ndi\ufb00ed, and packaged into a partial-update .mar \ufb01le. Eventually it was discovered that even across\nplatforms, many component changes were identical, therefore many di\ufb00s could be re-used. With\na script that cached the hash for each part of the di\ufb00, our partial update creation time was brought\ndown to approximately 40 minutes. After the snippets are uploaded and are hosted on AUS, an update veri\ufb01cation step is run to a)\ntest downloading the snippets and b) run the updater with the downloaded .mar \ufb01le to con\ufb01rm that\nthe updates apply correctly. Generation of partial-update .mar \ufb01les, as well as all the update snippets, is currently done after\nsigning is complete. We do this because generation of the partial updates must be done between\nsigned \ufb01les of the two releases, and therefore generation of the snippets must wait until the signed\nbuilds are available. Once we\u2019re able to integrate signing into the build process, we can generate\npartial updates immediately after completing a build or repack. Together with improvements to our\nAUS software, this means that once we\u2019re \ufb01nished builds and repacks we would be able to push\nimmediately to mirrors. This e\ufb00ectively parallelizes the creation of all the updates, trimming several\nhours from our total time. 34\nFirefox Release Engineering", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 48", "position": 48, "chunk_type": "semantic", "token_estimate": 377}
{"text": "2.7: We rely on\nour community mirrors to be able to handle a few hundred million users downloading updates over\nthe next few days. All the installers, as well as the complete and partial updates for all platforms and\nlocales, are already on our internal mirror network at this point. Publishing the \ufb01les to our external\nmirrors involves making a change to an rsync exclude \ufb01le for the public mirrors module. Once this\nchange is made, the mirrors will start to synchronize the new release \ufb01les. Each mirror has a score\nor weighting associated with it; we monitor which mirrors have synchronized the \ufb01les and sum their\nindividual scores to compute a total \u201cuptake\u201d score. Once a certain uptake threshold is reached, we\nnotify the Release Coordinator that the mirrors have enough uptake to handle the release. This is the point at which the release becomes \u201co\ufb03cial\u201d. After the Release Coordinator sends\nthe \ufb01nal \u201cgo live\u201d email, Release Engineering will update the symlinks on the web server so that\nvisitors to our web and ftp sites can \ufb01nd the latest new version of Firefox. We also publish all the\nupdate snippets for users on past versions of Firefox to AUS. Chris AtLee, Lukas Blakk, John O\u2019Duinn, and Armen Zambrano Gasparnian\n35", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 49", "position": 49, "chunk_type": "semantic", "token_estimate": 211}
{"text": "OF THE STRUCT. */: xListItem\nxGenericListItem;\n/* List item used to place\nthe TCB in ready and\nblocked queues. */\nxListItem\nxEventListItem;\n/* List item used to place\nthe TCB in event lists. */\nunsigned portBASE_TYPE uxPriority;\n/* The priority of the task\npriority. */\nportSTACK_TYPE *pxStack;\n/* Points to the start of\nthe stack. */\nsigned char\npcTaskName[ configMAX_TASK_NAME_LEN ];\n/* Descriptive name given\nto the task when created. Facilitates debugging\nonly. */\n#if ( portSTACK_GROWTH > 0 )\nportSTACK_TYPE *pxEndOfStack;\n/* Used for stack overflow\nchecking on architectures\nwhere the stack grows up\nfrom low memory. */\n#endif\n#if ( configUSE_MUTEXES == 1 )\nunsigned portBASE_TYPE uxBasePriority;\n/* The priority last\nassigned to the task -\nused by the priority\ninheritance mechanism. */\n#endif\n} tskTCB;\nThe TCB stores the address of the stack start address in pxStack and the current top of stack in\npxTopOfStack. It also stores a pointer to the end of the stack in pxEndOfStack to check for stack\nover\ufb02ow if the stack grows \u201cup\u201d to higher addresses. If the stack grows \u201cdown\u201d to lower addresses\nthen stack over\ufb02ow is checked by comparing the current top of stack against the start of stack memory\nin pxStack. The TCB stores the initial priority of the task in uxPriority and uxBasePriority. A task is\ngiven a priority when it is created, and a task\u2019s priority can be changed. If FreeRTOS implements\npriority inheritance then it uses uxBasePriority to remember the original priority while the task is\ntemporarily elevated to the \u201cinherited\u201d priority. (See the discussion about mutexes below for more\non priority inheritance.) Each task has two list items for use in FreeRTOS\u2019s various scheduling lists. When a task is\ninserted into a list FreeRTOS doesn\u2019t insert a pointer directly to the TCB. Instead, it inserts a pointer\nto either the TCB\u2019s xGenericListItem or xEventListItem. These xListItem variables let the\nFreeRTOS lists be smarter than if they merely held a pointer to the TCB. We\u2019ll see an example of\nthis when we discuss lists later. A task can be in one of four states: running, ready to run, suspended, or blocked. You might\nexpect each task to have a variable that tells FreeRTOS what state it\u2019s in, but it doesn\u2019t. Instead,\nFreeRTOS tracks task state implicitly by putting tasks in the appropriate list: ready list, suspended\n44\nFreeRTOS", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 58", "position": 58, "chunk_type": "semantic", "token_estimate": 392}
{"text": "list, etc. The presence of a task in a particular list indicates the task\u2019s state. As a task changes from: one state to another, FreeRTOS simply moves it from one list to another. Task Setup\nWe\u2019ve already touched on how a task is selected and scheduled with the pxReadyTasksLists array;\nnow let\u2019s look at how a task is initially created. A task is created when the xTaskCreate() function\nis called. FreeRTOS uses a newly allocated TCB object to store the name, priority, and other details\nfor a task, then allocates the amount of stack the user requests (assuming there\u2019s enough memory\navailable) and remembers the start of the stack memory in TCB\u2019s pxStack member. The stack is initialized to look as if the new task is already running and was interrupted by a\ncontext switch. This way the scheduler can treat newly created tasks exactly the same way as it\ntreats tasks that have been running for a while; the scheduler doesn\u2019t need any special case code for\nhandling new tasks. The way that a task\u2019s stack is made to look like it was interrupted by a context switch depends on\nthe architecture FreeRTOS is running on, but this ARM Cortex-M3 processor\u2019s implementation is a\ngood example:\nunsigned int *pxPortInitialiseStack( unsigned int *pxTopOfStack,\npdTASK_CODE pxCode,\nvoid *pvParameters )\n{\n/* Simulate the stack frame as it would be created by a context switch interrupt. */\npxTopOfStack--; /* Offset added to account for the way the MCU uses the stack on\nentry/exit of interrupts. */\n*pxTopOfStack = portINITIAL_XPSR;\n/* xPSR */\npxTopOfStack--;\n*pxTopOfStack = ( portSTACK_TYPE ) pxCode;\n/* PC */\npxTopOfStack--;\n*pxTopOfStack = 0;\n/* LR */\npxTopOfStack -= 5;\n/* R12, R3, R2 and R1. */\n*pxTopOfStack = ( portSTACK_TYPE ) pvParameters;\n/* R0 */\npxTopOfStack -= 8;\n/* R11, R10, R9, R8, R7, R6, R5 and R4. */\nreturn pxTopOfStack;\n}\nThe ARM Cortex-M3 processor pushes registers on the stack when a task is interrupted. pxPortInitialiseStack() modi\ufb01es the stack to look like the registers were pushed even though\nthe task hasn\u2019t actually started running yet. Known values are stored to the stack for the ARM\nregisters xPSR, PC, LR, and R0. The remaining registers R1 \u2013 R12 get stack space allocated for\nthem by decrementing the top of stack pointer, but no speci\ufb01c data is stored in the stack for those\nregisters. The ARM architecture says that those registers are unde\ufb01ned at reset, so a (non-buggy)\nprogram will not rely on a known value.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 59", "position": 59, "chunk_type": "semantic", "token_estimate": 416}
{"text": "list, etc. The presence of a task in a particular list indicates the task\u2019s state. As a task changes from: The remaining registers R1 \u2013 R12 get stack space allocated for\nthem by decrementing the top of stack pointer, but no speci\ufb01c data is stored in the stack for those\nregisters. The ARM architecture says that those registers are unde\ufb01ned at reset, so a (non-buggy)\nprogram will not rely on a known value. After the stack is prepared, the task is almost ready to run. First though, FreeRTOS disables\ninterrupts: We\u2019re about to start mucking with the ready lists and other scheduler structures and we\ndon\u2019t want anyone else changing them underneath us. If this is the \ufb01rst task to ever be created, FreeRTOS initializes the scheduler\u2019s task lists. Free-\nRTOS\u2019s scheduler has an array of ready lists, pxReadyTasksLists[], which has one ready list for\nChristopher Svec\n45", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 59", "position": 59, "chunk_type": "semantic", "token_estimate": 148}
{"text": "Stan Shebs: GDB, the GNU Debugger, was among the \ufb01rst programs to be written for the Free Software Foun-\ndation, and it has been a staple of free and open source software systems ever since. Originally\ndesigned as a plain Unix source-level debugger, it has since been expanded to a wide range of uses,\nincluding use with many embedded systems, and has grown from a few thousand lines of C to over\nhalf a million. This chapter will delve into the overall internal structure of GDB, showing how it has gradually\ndeveloped as new user needs and new features have come in over time. 4.1\nThe Goal\nGDB is designed to be a symbolic debugger for programs written in compiled imperative languages\nsuch as C, C++, Ada, and Fortran. Using its original command-line interface, a typical usage looks\nsomething like this:\n% gdb myprog\n[...]\n(gdb) break buggy_function\nBreakpoint 1 at 0x12345678: file myprog.c, line 232. (gdb) run 45 92\nStarting program: myprog\nBreakpoint 1, buggy_function (arg1=45, arg2=92) at myprog.c:232\n232\nresult = positive_variable * arg1 + arg2;\n(gdb) print positive_variable\n$$1 = -34\n(gdb)\nGDB shows something that is not right, the developer says \u201caha\u201d or \u201chmmm\u201d, and then has to decide\nboth what the mistake is and how to \ufb01x it. The important point for design is that a tool like GDB is basically an interactive toolbox for\npoking around in a program, and as such it needs to be responsive to an unpredictable series of\nrequests. In addition, it will be used with programs that have been optimized by the compiler, and\nprograms that exploit every hardware option for performance, so it needs to have detailed knowledge\ndown to the lowest levels of a system. GDB also needs to be able to debug programs compiled by di\ufb00erent compilers (not just the GNU\nC compiler), to debug programs compiled years earlier by long-obsolete versions of compilers, and to", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 67", "position": 67, "chunk_type": "semantic", "token_estimate": 320}
{"text": "debug programs whose symbolic info is missing, out of date, or simply incorrect; so, another design: consideration is that GDB should continue to work and be useful even if data about the program is\nmissing, or corrupted, or simply incomprehensible. The following sections assume a passing familiarity with using GDB from the command line. If\nyou\u2019re new to GDB, give it a try and peruse the manual. [SPS+00]\n4.2\nOrigins of GDB\nGDB is an old program. It came into existence around 1985, written by Richard Stallman along with\nGCC, GNU Emacs, and other early components of GNU. (In those days, there were no public source\ncontrol repositories, and much of the detailed development history is now lost.) The earliest readily available releases are from 1988, and comparison with present-day sources\nshows that only a handful of lines bear much resemblance; nearly all of GDB has been rewritten at\nleast once. Another striking thing about early versions of GDB is that the original goals were rather\nmodest, and much of the work since then has been extension of GDB into environments and usages\nthat were not part of the original plan. 4.3\nBlock Diagram\nAt the largest scale, GDB can be said to have two sides to it:\n1. The \u201csymbol side\u201d is concerned with symbolic information about the program. Symbolic\ninformation includes function and variable names and types, line numbers, machine register\nusage, and so on. The symbol side extracts symbolic information from the program\u2019s exe-\ncutable \ufb01le, parses expressions, \ufb01nds the memory address of a given line number, lists source\ncode, and in general works with the program as the programmer wrote it. 2. The \u201ctarget side\u201d is concerned with the manipulation of the target system. It has facilities to\nstart and stop the program, to read memory and registers, to modify them, to catch signals,\nand so on. The speci\ufb01cs of how this is done can vary drastically between systems; most\nUnix-type systems provide a special system call named ptrace that gives one process the\nability to read and write the state of a di\ufb00erent process. Thus, GDB\u2019s target side is mostly\nabout making ptrace calls and interpreting the results. For cross-debugging an embedded\nsystem, however, the target side constructs message packets to send over a wire, and waits for\nresponse packets in return. The two sides are somewhat independent of each other; you can look around your program\u2019s\ncode, display variable types, etc., without actually running the program.", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 68", "position": 68, "chunk_type": "semantic", "token_estimate": 412}
{"text": "debug programs whose symbolic info is missing, out of date, or simply incorrect; so, another design: For cross-debugging an embedded\nsystem, however, the target side constructs message packets to send over a wire, and waits for\nresponse packets in return. The two sides are somewhat independent of each other; you can look around your program\u2019s\ncode, display variable types, etc., without actually running the program. Conversely, it is possible to\ndo pure machine-language debugging even if no symbols are available. In the middle, tying the two sides together, is the command interpreter and the main execution\ncontrol loop. 4.4\nExamples of Operation\nTo take a simple case of how it all ties together, consider the print command from above. The\ncommand interpreter \ufb01nds the print command function, which parses the expression into a simple\ntree structure and then evaluates it by walking the tree. At some point the evaluator will consult the\n54", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 68", "position": 68, "chunk_type": "semantic", "token_estimate": 153}
{"text": "The single-stepping command step conceals a complicated dance going on behind the scenes.: When the user asks to step to the next line in the program, the target side is asked to execute only a\nsingle instruction of the program and then stop it again (this is one of the things that ptrace can do). Upon being informed that the program has stopped, GDB asks for the program counter (PC) register\n(another target side operation) and then compares it with the range of addresses that the symbol side\nsays is associated with the current line. If the PC is outside that range, then GDB leaves the program\nstopped, \ufb01gures out the new source line, and reports that to the user. If the PC is still in the range of\nthe current line, then GDB steps by another instruction and checks again, repeating until the PC gets\nto a di\ufb00erent line. This basic algorithm has the advantage that it always does the right thing, whether\nthe line has jumps, subroutine calls, etc., and does not require GDB to interpret all the details of the\nmachine\u2019s instruction set. A disadvantage is that there are many interactions with the target for each\nsingle-step which, for some embedded targets, results in noticeably slow stepping. 4.5\nPortability\nAs a program needing extensive access all the way down to the physical registers on a chip, GDB\nwas designed from the beginning to be portable across a variety of systems. However, its portability\nstrategy has changed considerably over the years. Originally, GDB started out similar to the other GNU programs of the time; coded in a minimal\ncommon subset of C, and using a combination of preprocessor macros and Make\ufb01le fragments to\nadapt to a speci\ufb01c architecture and operating system. Although the stated goal of the GNU project\nwas a self-contained \u201cGNU operating system\u201d, bootstrapping would have to be done on a variety of\nexisting systems; the Linux kernel was still years in the future. The configure shell script is the\n\ufb01rst key step of the process. It can do a variety of things, such as making a symbolic link from a\nsystem-speci\ufb01c \ufb01le to a generic header name, or constructing \ufb01les from pieces, more importantly the\nMake\ufb01le used to build the program. Programs like GCC and GDB have additional portability needs over something like cat or diff,\nand over time, GDB\u2019s portability bits came to be separated into three classes, each with its own\nMake\ufb01le fragment and header \ufb01le.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 70", "position": 70, "chunk_type": "semantic", "token_estimate": 413}
{"text": "The single-stepping command step conceals a complicated dance going on behind the scenes.: It can do a variety of things, such as making a symbolic link from a\nsystem-speci\ufb01c \ufb01le to a generic header name, or constructing \ufb01les from pieces, more importantly the\nMake\ufb01le used to build the program. Programs like GCC and GDB have additional portability needs over something like cat or diff,\nand over time, GDB\u2019s portability bits came to be separated into three classes, each with its own\nMake\ufb01le fragment and header \ufb01le. \u2022 \u201cHost\u201d de\ufb01nitions are for the machine that GDB itself runs on, and might include things\nlike the sizes of the host\u2019s integer types. Originally done as human-written header \ufb01les, it\neventually occurred to people that they could be calculated by having configure run little test\nprograms, using the same compiler that was going to be used to build the tool. This is what\nautoconf[aut12] is all about, and today nearly all GNU tools and many (if not most) Unix\nprograms use autoconf-generated con\ufb01gure scripts. \u2022 \u201cTarget\u201d de\ufb01nitions are speci\ufb01c to the machine running the program being debugged. If the\ntarget is the same as the host, then we are doing \u201cnative\u201d debugging, otherwise it is \u201ccross\u201d\ndebugging, using some kind of wire connecting the two systems. Target de\ufb01nitions fall in turn\ninto two main classes:\n\u2013 \u201cArchitecture\u201d de\ufb01nitions: These de\ufb01ne how to disassemble machine code, how to walk\nthrough the call stack, and which trap instruction to insert at breakpoints. Originally\ndone with macros, they were migrated to regular C accessed by via \u201cgdbarch\u201d objects,\ndescribed in more depth below. \u2013 \u201cNative\u201d de\ufb01nitions: These de\ufb01ne the speci\ufb01cs of arguments to ptrace (which vary\nconsiderably between \ufb02avors of Unix), how to \ufb01nd shared libraries that have been loaded,\n56", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 70", "position": 70, "chunk_type": "semantic", "token_estimate": 295}
{"text": "and so forth, which only apply to the native debugging case. Native de\ufb01nitions are a last: holdout of 1980s-style macros, although most are now \ufb01gured out using autoconf. 4.6\nData Structures\nBefore drilling down into the parts of GDB, let\u2019s take a look at the major data structures that GDB\nworks with. As GDB is a C program, these are implemented as structs rather than as C++-style\nobjects, but in most cases they are treated as objects, and here we follow GDBers\u2019 frequent practice\nin calling them objects. Breakpoints\nA breakpoint is the main kind of object that is directly accessible to the user. The user creates a\nbreakpoint with the break command, whose argument speci\ufb01es a location, which can be a function\nname, a source line number, or a machine address. GDB assigns a small positive integer to the\nbreakpoint object, which the user subsequently uses to operate on the breakpoint. Within GDB, the\nbreakpoint is a C struct with a number of \ufb01elds. The location gets translated to a machine address,\nbut is also saved in its original form, since the address may change and need recomputation, for\ninstance if the program is recompiled and reloaded into a session. Several kinds of breakpoint-like objects actually share the breakpoint struct, including watch-\npoints, catchpoints, and tracepoints. This helps ensure that creation, manipulation, and deletion\nfacilities are consistently available. The term \u201clocation\u201d also refers to the memory addresses at which the breakpoint is to be installed. In the cases of inline functions and C++ templates, it may be that a single user-speci\ufb01ed breakpoint\nmay correspond to several addresses; for instance, each inlined copy of a function entails a separate\nlocation for a breakpoint that is set on a source line in the function\u2019s body. Symbols and Symbol Tables\nSymbol tables are a key data structure to GDB, and can be quite large, sometimes growing to occupy\nmultiple gigabytes of RAM. To some extent, this is unavoidable; a large application in C++ can have\nmillions of symbols in its own right, and it pulls in system header \ufb01les which can have millions more\nsymbols. Each local variable, each named type, each value of an enum\u2014all of these are separate\nsymbols. GDB uses a number of tricks to reduce symbol table space, such as partial symbol tables (more\nabout those later), bit \ufb01elds in structs, etc.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 71", "position": 71, "chunk_type": "semantic", "token_estimate": 394}
{"text": "I proposed solving this with an object-based design, and starting in 1998 Cygnus Solutions1 funded: Andrew Cagney to start the changeover. It took several years and contributions from dozens of\nhackers to \ufb01nish the job, a\ufb00ecting perhaps 80,000 lines of code in all. The introduced constructs are called gdbarch objects, and at this point may contain as many as\n130 methods and variables de\ufb01ning a target architecture, although a simple target might only need a\ndozen or so of these. To get a sense of how the old and new ways compare, see the declaration that x86 long doubles\nare 96 bits in size from gdb/config/i386/tm-i386.h, circa 2002:\n#define TARGET_LONG_DOUBLE_BIT 96\nand from gdb/i386-tdep.c, in 2012:\ni386_gdbarch_init( [...] )\n{\n[...]\nset_gdbarch_long_double_bit (gdbarch, 96);\n[...]\n}\nExecution Control\nThe heart of GDB is its execution control loop. We touched on it earlier when describing single-\nstepping over a line; the algorithm entailed looping over multiple instructions until \ufb01nding one\nassociated with a di\ufb00erent source line. The loop is called wait_for_inferior, or \u201cw\ufb01\u201d for short. Conceptually it is inside the main command loop, and is only entered for commands that cause\nthe program to resume execution. When the user types continue or step and then waits while\nnothing seems to be happening, GDB may in fact be quite busy. In addition to the single-stepping\nloop mentioned above, the program may be hitting trap instructions and reporting the exception\nto GDB. If the exception is due to the trap being a breakpoint inserted by GDB, it then tests the\nbreakpoint\u2019s condition, and if false, it removes the trap, single-steps the original instruction, re-inserts\nthe trap, and then lets the program resume. Similarly, if a signal is raised, GDB may choose to ignore\nit, or handle it one of several ways speci\ufb01ed in advance. All of this activity is managed by wait_for_inferior. Originally this was a simple loop,\nwaiting for the target to stop and then deciding what to do about it, but as ports to various systems\nneeded special handling, it grew to a thousand lines, with goto statements criss-crossing it for poorly\nunderstood reasons.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 75", "position": 75, "chunk_type": "semantic", "token_estimate": 353}
{"text": "I proposed solving this with an object-based design, and starting in 1998 Cygnus Solutions1 funded: All of this activity is managed by wait_for_inferior. Originally this was a simple loop,\nwaiting for the target to stop and then deciding what to do about it, but as ports to various systems\nneeded special handling, it grew to a thousand lines, with goto statements criss-crossing it for poorly\nunderstood reasons. For instance, with the proliferation of Unix variants, there was no one person\nwho understood all their \ufb01ne points, nor did we have access to all of them for regression testing, so\nthere was a strong incentive to modify the code in a way that exactly preserved behavior for existing\nports\u2014and a goto skipping over part of the loop was an all-too-easy tactic. The single big loop was also a problem for any kind of asynchronous handling or debugging of\nthreaded programs, in which the user wants to start and stop a single thread while allowing the rest\nof the program to continue running. The conversion to an event-oriented model took several years. I broke up wait_for_inferior in\n1999, introducing an execution control state structure to replace the pile of local and global variables,\n1Cygnus Solutions was a company founded in 1989 to provide commercial support for free software. It was acquired in 2000\nby Red Hat. Stan Shebs\n61", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 75", "position": 75, "chunk_type": "semantic", "token_estimate": 227}
{"text": "and converting the tangle of jumps into smaller independent functions. At the same time Elena: The GDB sources include a few\nexample stubs, which are typically about 1,000 lines of low-level C. On a totally bare board with\nno OS, the stub must install its own handlers for hardware exceptions, most importantly to catch\ntrap instructions. It will also need serial driver code if the hardware link is a serial line. The actual\nprotocol handling is simple, since all the required packets are single characters that can be decoded\nwith a switch statement. Another approach to remote protocol is to build a \u201csprite\u201d that interfaces between GDB and\ndedicated debugging hardware, including JTAG devices, \u201cwigglers\u201d, etc. Oftentimes these devices\nhave a library that must run on the computer that is physically connected to a target board, and often\nthe library API is not architecturally compatible with GDB\u2019s internals. So, while con\ufb01gurations of\nGDB have called hardware control libraries directly, it has proven simpler to run the sprite as an\nindependent program that understands remote protocol and translates the packets into device library\ncalls. GDBserver\nThe GDB sources do include one complete and working implementation of the target side of the\nremote protocol: GDBserver. GDBserver is a native program that runs under the target\u2019s operating\nsystem, and controls other programs on the target OS using its native debugging support, in response\nto packets received via remote protocol. In other words, it acts as a sort of proxy for native debugging. 62", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 76", "position": 76, "chunk_type": "semantic", "token_estimate": 250}
{"text": "GDBserver doesn\u2019t do anything that native GDB can\u2019t do; if your target system can run GDBserver,: then theoretically it can run GDB. However, GDBserver is 10 times smaller and doesn\u2019t need to\nmanage symbol tables, so it is very convenient for embedded GNU/Linux usages and the like. Figure 4.2: GDBserver\nGDB and GDBserver share some code, but while it is an obvious idea to encapsulate OS-speci\ufb01c\nprocess control, there are practical di\ufb03culties with separating out tacit dependencies in native GDB,\nand the transition has gone slowly. 4.9\nInterfaces to GDB\nGDB is fundamentally a command-line debugger. Over time people have tried various schemes to\nmake it into a graphical windowed debugger but, despite all the time and e\ufb00ort, none of these are\nuniversally accepted. Command-Line Interface\nThe command-line interface uses the standard GNU library readline to handle the character-by-\ncharacter interaction with the user. Readline takes care of things like line editing and command\ncompletion; the user can do things like use cursor keys to go back in a line and \ufb01x a character. GDB then takes the command returned by readline and looks it up using a cascaded structure\nof command tables, where each successive word of the command selects an additional table. For\ninstance set print elements 80 involves three tables; the \ufb01rst is the table of all commands, the\nsecond is a table of options that can be set, and the third is a table of value printing options, of\nwhich elements is the one that limits the number of objects printed from an aggregate like a string\nor array. Once the cascaded tables have called an actual command-handling function, it takes control,\nand argument parsing is completely up to the function. Some commands, such as run, handle their\narguments similarly to traditional C argc/argv standards, while others, such as print, assume that\nthe remainder of the line is a single programming language expression, and give the entire line over\nto a language-speci\ufb01c parser. Stan Shebs\n63", "domains": ["Design Patterns", "Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 77", "position": 77, "chunk_type": "semantic", "token_estimate": 330}
{"text": "Machine Interface: One way to provide a debugging GUI is to use GDB as a sort of \u201cbackend\u201d to a graphical interface\nprogram, translating mouse clicks into commands and formatting print results into windows. This\nhas been made to work several times, including KDbg and DDD (Data Display Debugger), but it\u2019s\nnot the ideal approach because sometimes results are formatted for human readability, omitting\ndetails and relying on human ability to supply context. To solve this problem, GDB has an alternate \u201cuser\u201d interface, known as the Machine Interface\nor MI for short. It is still fundamentally a command-line interface, but both commands and results\nhave additional syntax that makes everything explicit\u2014each argument is bounded by quotes, and\ncomplex output has delimiters for subgroups and parameter names for component pieces. In addition,\nMI commands can be pre\ufb01xed with sequence identi\ufb01ers that are echoed back in results, ensuring\nreported results are matched up with the right commands. To see how the two forms compare, here is a normal step command and GDB\u2019s response:\n(gdb) step\nbuggy_function (arg1=45, arg2=92) at ex.c:232\n232\nresult = positive_variable * arg1 + arg2;\nWith the MI, the input and output are more verbose, but easier for other software to parse accurately:\n4321-exec-step\n4321^done,reason=\"end-stepping-range\",\nframe={addr=\"0x00000000004004be\",\nfunc=\"buggy_function\",\nargs=[{name=\"arg1\",value=\"45\"},\n{name=\"arg2\",value=\"92\"}],\nfile=\"ex.c\",\nfullname=\"/home/sshebs/ex.c\",\nline=\"232\"}\nThe Eclipse[ecl12] development environment is the most notable client of the MI. Other User Interfaces\nAdditional frontends include a tcl/tk-based version called GDBtk or Insight, and a curses-based\ninterface called the TUI, originally contributed by Hewlett-Packard. GDBtk is a conventional\nmulti-paned graphical interface built using the tk library, while the TUI is a split-screen interface. 4.10\nDevelopment Process\nMaintainers\nAs an original GNU program, GDB development started out following the \u201ccathedral\u201d model of\ndevelopment. Originally written by Stallman, GDB then went through a succession of \u201cmaintainers\u201d,\neach of whom was a combination of architect, patch reviewer, and release manager, with access to\nthe source repository limited to a handful of Cygnus employees. 64", "domains": ["Design Patterns", "Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 78", "position": 78, "chunk_type": "semantic", "token_estimate": 326}
{"text": "In 1999, GDB migrated to a public source repository and expanded to a team of several dozen: maintainers, aided by scores of individuals with commit privileges. This has accelerated development\nconsiderably, with the 10-odd commits each week growing to 100 or more. Testing Testing\nAs GDB is highly system-speci\ufb01c, has a great many ports to systems ranging from the smallest to\nthe largest in computerdom, and has hundreds of commands, options, and usage styles, it is di\ufb03cult\nfor even an experienced GDB hacker to anticipate all the e\ufb00ects of a change. This is where the test suite comes in. The test suite consists of a number of test programs\ncombined with expect scripts, using a tcl-based testing framework called DejaGNU. The basic\nmodel is that each script drives GDB as it debugs a test program, sending commands and then\npattern-matching the output against regular expressions. The test suite also has the ability to run cross-debugging to both live hardware and simulators,\nand to have tests that are speci\ufb01c to a single architecture or con\ufb01guration. At the end of 2011, the test suite includes some 18,000 test cases, which include tests of basic\nfunctionality, language-speci\ufb01c tests, architecture-speci\ufb01c tests, and MI tests. Most of these are\ngeneric and are run for any con\ufb01guration. GDB contributors are expected to run the test suite on\npatched sources and observe no regressions, and new tests are expected to accompany each new\nfeature. However, as no one has access to all platforms that might be a\ufb00ected by a change, it is rare to\nget all the way to zero failures; 10\u201320 failures is usually reasonable for a trunk snapshot con\ufb01gured\nfor native debugging, and some embedded targets will have more failures. 4.11\nLessons Learned\nOpen Development Wins\nGDB started out as an exemplar of the \u201ccathedral\u201d development process, in which the maintainer\nkeeps close control of the sources, with the outside world only seeing progress via periodic snapshots. This was rationalized by the relative infrequence of patch submissions, but the closed process was\nactually discouraging patches. Since the open process has been adopted, the number of patches is\nmuch larger than ever before, and quality is just as good or better. Make a Plan, but Expect It to Change\nThe open source development process is intrinsically somewhat chaotic, as di\ufb00erent individuals work\non the code for a while, then fall away, leaving others to continue on. However, it still makes sense to make a development plan and publish it.", "domains": ["Design Patterns", "Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 79", "position": 79, "chunk_type": "semantic", "token_estimate": 413}
{"text": "In 1999, GDB migrated to a public source repository and expanded to a team of several dozen: Make a Plan, but Expect It to Change\nThe open source development process is intrinsically somewhat chaotic, as di\ufb00erent individuals work\non the code for a while, then fall away, leaving others to continue on. However, it still makes sense to make a development plan and publish it. It helps guide developers\nas they work on related tasks, it can be shown to potential funders, and it lets volunteers think about\nwhat they can do to advance it. But don\u2019t try to force dates or time frames; even if everyone is enthusiastic about a direction, it is\nunlikely that people can guarantee full-time e\ufb00ort for long enough to \ufb01nish by a chosen date. For that matter, don\u2019t cling to the plan itself if it has become outdated. For a long time, GDB had\na plan to restructure as a library, libgdb, with a well-de\ufb01ned API, that could be linked into other\nprograms (in particular ones with GUIs); the build process was even changed to build a libgdb.a as\nan intermediate step. Although the idea has come up periodically since then, the primacy of Eclipse\nStan Shebs\n65", "domains": ["Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 79", "position": 79, "chunk_type": "semantic", "token_estimate": 204}
{"text": "and MI meant that the library\u2019s main rationale has been sidestepped, and as of January 2012 we have: abandoned the library concept and are expunging the now-pointless bits of code. Things Would Be Great If We Were In\ufb01nitely Intelligent\nAfter seeing some of the changes we made, you might be thinking: Why didn\u2019t we do things right in\nthe \ufb01rst place? Well, we just weren\u2019t smart enough. Certainly we could have anticipated that GDB was going to be tremendously popular, and was\ngoing to be ported to dozens and dozens of architectures, both native and cross. If we had known\nthat, we could have started with the gdbarch objects, instead of spending years upgrading old macros\nand global variables; ditto for the target vector. Certainly we could have anticipated GDB was going to be used with GUIs. After all in 1986\nboth the Mac and the X Window System had already been out for two years! Instead of designing a\ntraditional command interface, we could have set it up to handle events asynchronously. The real lesson though is that not that GDBers were dumb, but that we couldn\u2019t possibly have\nbeen smart enough to anticipate how GDB would need to evolve. In 1986 it was not at all clear that\nthe windows-and-mouse interface was going to become ubiquitous; if the \ufb01rst version of GDB was\nperfectly adapted for GUI use, we\u2019d have looked like geniuses, but it would have been sheer luck. Instead, by making GDB useful in a more limited scope, we built a user base that enabled more\nextensive development and re-engineering later. Learn to Live with Incomplete Transitions\nTry to complete transitions, but they may take a while; expect to live with them being incomplete. At the GCC Summit in 2003, Zack Weinberg lamented the \u201cincomplete transitions\u201d in GCC,\nwhere new infrastructure had been introduced, but the old infrastructure could not be removed. GDB\nhas these also, but we can point to a number of transitions that have been completed, such as the\ntarget vector and gdbarch. Even so, they can take a number of years to complete, and in the meantime\none has to keep the debugger running. Don\u2019t Get Too Attached to the Code\nWhen you spend a long time with a single body of code, and it\u2019s an important program that also pays\nthe bills, it\u2019s easy to get attached to it, and even to mold your thinking to \ufb01t the code, rather than the\nother way around. Don\u2019t.", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 80", "position": 80, "chunk_type": "semantic", "token_estimate": 415}
{"text": "[chapter5]: The Glasgow Haskell Compiler\nSimon Marlow and Simon Peyton Jones\nThe Glasgow Haskell Compiler (GHC) started as part of an academic research project funded by the\nUK government at the beginning of the 1990s, with several goals in mind:\n\u2022 To make freely available a robust and portable compiler for Haskell that generates high\nperformance code;\n\u2022 To provide a modular foundation that other researchers can extend and develop;\n\u2022 To learn how real programs behave, so that we can design and build better compilers. GHC is now over 20 years old, and has been under continuous active development since its\ninception. Today, GHC releases are downloaded by hundreds of thousands of people, the online\nrepository of Haskell libraries has over 3,000 packages, GHC is used to teach Haskell in many\nundergraduate courses, and there are a growing number of instances of Haskell being depended upon\ncommercially. Over its lifetime GHC has generally had around two or three active developers, although the\nnumber of people who have contributed some code to GHC is in the hundreds. While the ultimate\ngoal for us, the main developers of GHC, is to produce research rather than code, we consider\ndeveloping GHC to be an essential prerequisite: the artifacts of research are fed back into GHC,\nso that GHC can then be used as the basis for further research that builds on these previous ideas. Moreover, it is important that GHC is an industrial-strength product, since this gives greater credence\nto research results produced with it. So while GHC is stu\ufb00ed full of cutting-edge research ideas, a\ngreat deal of e\ufb00ort is put into ensuring that it can be relied on for production use. There has often\nbeen some tension between these two seemingly contradictory goals, but by and large we have found\na path that is satisfactory both from the research and the production-use angles. In this chapter we want to give an overview of the architecture of GHC, and focus on a handful\nof the key ideas that have been successful in GHC (and a few that haven\u2019t). Hopefully throughout the\nfollowing pages you will gain some insight into how we managed to keep a large software project\nactive for over 20 years without it collapsing under its own weight, with what is generally considered\nto be a very small development team. 5.1\nWhat is Haskell?", "domains": ["Design Patterns", "Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 81", "position": 81, "chunk_type": "semantic", "token_estimate": 395}
{"text": "of the academic research community interested in functional languages, to address the lack of a: common language that could be used as a focus for their research. Two features of Haskell stand out amongst the programming languages crowd:\n\u2022 It is purely functional. That is, functions cannot have side e\ufb00ects or mutate data; for a given\nset of inputs (arguments) a function always gives the same result. The bene\ufb01ts of this model\nfor reasoning about code (and, we believe, writing code) are clear, but integrating input/output\ninto the purely functional setting proved to be a signi\ufb01cant challenge. Fortunately an elegant\nsolution in the form of monads was discovered, which not only allowed input/output to be\nneatly integrated with purely functional code, but introduced a powerful new abstraction that\nrevolutionised coding in Haskell (and subsequently had an impact on other languages too). \u2022 It is lazy. This refers to the evaluation strategy of the language: most languages use strict\nevaluation in which the arguments to a function are evaluated before the function is called,\nwhereas in Haskell the arguments to a function are passed unevaluated, and only evaluated on\ndemand. This aspect of Haskell also has bene\ufb01ts for reasoning about programs, but more than\nanything else serves as a barrier to prevent the leakage of impure non-functional features into\nthe language: such features fundamentally cannot work in conjunction with lazy semantics. Haskell is also strongly-typed, while supporting type inference which means that type annotations\nare rarely necessary. Those interested in a complete history of Haskell should read [HHPW07]. 5.2\nHigh-Level Structure\nAt the highest level, GHC can be divided into three distinct chunks:\n\u2022 The compiler itself. This is essentially a Haskell program whose job is to convert Haskell\nsource code into executable machine code. \u2022 The Boot Libraries. GHC comes with a set of libraries that we call the boot libraries, because\nthey constitute the libraries that the compiler itself depends on. Having these libraries in\nthe source tree means that GHC can bootstrap itself. Some of these libraries are very tightly\ncoupled to GHC, because they implement low-level functionality such as the Int type in terms\nof primitives de\ufb01ned by the compiler and runtime system. Other libraries are more high-level\nand compiler-independent, such as the Data.Map library. \u2022 The Runtime System (RTS). This is a large library of C code that handles all the tasks associated\nwith running the compiled Haskell code, including garbage collection, thread scheduling,\npro\ufb01ling, exception handling and so on.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 82", "position": 82, "chunk_type": "semantic", "token_estimate": 414}
{"text": "Code Metrics: The last time we measured the number of lines in GHC was in 19921, so it is interesting to look at\nhow things have changed since then. Figure 5.1 gives a breakdown of the number of lines of code in\nGHC divided up into the major components, comparing the current tallies with those from 1992. Module\nLines (1992)\nLines (2011)\nIncrease\nCompiler\nMain\n997\n11,150\n11.2\nParser\n1,055\n4,098\n3.9\nRenamer\n2,828\n4,630\n1.6\nType checking\n3,352\n24,097\n7.2\nDesugaring\n1,381\n7,091\n5.1\nCore transformations\n1,631\n9,480\n5.8\nSTG transformations\n814\n840\n1\nData-Parallel Haskell\n\u2014\n3,718\n\u2014\nCode generation\n2913\n11,003\n3.8\nNative code generation\n\u2014\n14,138\n\u2014\nLLVM code generation\n\u2014\n2,266\n\u2014\nGHCi\n\u2014\n7,474\n\u2014\nHaskell abstract syntax\n2,546\n3,700\n1.5\nCore language\n1,075\n4,798\n4.5\nSTG language\n517\n693\n1.3\nC-- (was Abstract C)\n1,416\n7,591\n5.4\nIdenti\ufb01er representations\n1,831\n3,120\n1.7\nType representations\n1,628\n3,808\n2.3\nPrelude de\ufb01nitions\n3,111\n2,692\n0.9\nUtilities\n1,989\n7,878\n3.96\nPro\ufb01ling\n191\n367\n1.92\nCompiler Total\n28,275\n139,955\n4.9\nRuntime System\nAll C and C-- code\n43,865\n48,450\n1.10\nFigure 5.1: Lines of code in GHC, past and present\nThere are some notable aspects of these \ufb01gures:\n\u2022 Despite nearly 20 years of non-stop development the compiler has only increased in size by\na factor of 5, from around 28,000 to around 140,000 lines of Haskell code. We obsessively\nrefactor while adding new code, keeping the code base as fresh as possible. \u2022 There are several new components, although these only account for about 28,000 new lines. Much of the new components are concerned with code generation: native code generators\nfor various processors, and an LLVM2 code generator. The infrastructure for the interactive\ninterpreter GHCi also added over 7,000 lines. 1\u201cThe Glasgow Haskell compiler: a technical overview\u201d, JFIT technical conference digest, 1992\n2Formerly the \u201cLow Level Virtual Machine\u201d, the LLVM project includes a generic code-generator with targets for many\ndi\ufb00erent processors. For more information see http://llvm.org/, and the chapter on LLVM in Volume 1 of The Architecture\nof Open Source Applications. Simon Marlow and Simon Peyton Jones\n69", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 83", "position": 83, "chunk_type": "semantic", "token_estimate": 351}
{"text": "\u2022 The biggest increase in a single component is the type checker, where over 20,000 lines were: added. This is unsurprising given that much of the recent research using GHC has been into\nnew type system extensions (for example GADTs [PVWW06] and Type Families [CKP05]). \u2022 A lot of code has been added to the Main component; this is partly because there was previously\na 3,000-line Perl script called the \u201cdriver\u201d that was rewritten in Haskell and moved into GHC\nproper, and also because support for compiling multiple modules was added. \u2022 The runtime system has barely grown: it is only 10% larger, despite having accumulated a lot\nof new functionality and being ported to more platforms. We rewrote it completely around\n1997. \u2022 GHC has a complex build system, which today comprises about 6,000 lines of GNU make code. It is on its fourth complete rewrite, the latest being about two years ago, and each successive\niteration has reduced the amount of code. The Compiler\nWe can divide the compiler into three:\n\u2022 The compilation manager, which is responsible for the compilation of multiple Haskell source\n\ufb01les. The job of the compilation manager is to \ufb01gure out in which order to compile the\ndi\ufb00erent \ufb01les, and to decide which modules do not need to be recompiled because none of\ntheir dependencies have changed since the last time they were compiled. \u2022 The Haskell compiler (we abbreviate this as Hsc inside GHC), which handles the compilation\nof a single Haskell source \ufb01le. As you might imagine, most of the action happens in here. The\noutput of Hsc depends on what backend is selected: assembly, LLVM code, or bytecode. \u2022 The pipeline, which is responsible for composing together any necessary external programs\nwith Hsc to compile a Haskell source \ufb01le to object code. For example, a Haskell source \ufb01le\nmay need preprocessing with the C preprocessor before feeding to Hsc, and the output of Hsc\nis usually an assembly \ufb01le that must be fed into the assembler to create an object \ufb01le. The compiler is not simply an executable that performs these functions; it is itself a library with\na large API that can be used to build other tools that work with Haskell source code, such as IDEs\nand analysis tools. More about this later in Section 5.4.", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 84", "position": 84, "chunk_type": "semantic", "token_estimate": 388}
{"text": "\u2022 The biggest increase in a single component is the type checker, where over 20,000 lines were: The compiler is not simply an executable that performs these functions; it is itself a library with\na large API that can be used to build other tools that work with Haskell source code, such as IDEs\nand analysis tools. More about this later in Section 5.4. Compiling Haskell Code\nAs with most compilers, compiling a Haskell source \ufb01le proceeds in a sequence of phases, with\nthe output of each phase becoming the input of the subsequent phase. The overall structure of the\ndi\ufb00erent phases is illustrated in Figure 5.2. Parsing\nWe start in the traditional way with parsing, which takes as input a Haskell source \ufb01le and produces\nas output abstract syntax. In GHC the abstract syntax datatype HsSyn is parameterised by the types\nof the identi\ufb01ers it contains, so an abstract syntax tree has type HsSyn t for some type of identi\ufb01ers t.\nThis enables us to add more information to identi\ufb01ers as the program passes through the various\nstages of the compiler, while reusing the same type of abstract syntax trees. The output of the parser is an abstract syntax tree in which the identi\ufb01ers are simple strings,\nwhich we call RdrName. Hence, the abstract syntax produced by the parser has type HsSyn RdrName. 70\nThe Glasgow Haskell Compiler", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 84", "position": 84, "chunk_type": "semantic", "token_estimate": 229}
{"text": "\uff65 The Simpli\ufb01er: \uff65 Rewrite rules\n\uff65 Strictness analysis\n\uff65 Let-\ufb02oating (inwards and outwards)\n\uff65 Specialise overloaded functions\n\uff65 Constructor specialisation\nM.hs\nParse\nRename\nTypecheck\nDesugar\nSimplify\nCoreTidy\nCorePrep\nConvert to IfaceSyn\nSerialise\nM.hi\n(interface file)\nConvert to STG\nCode generation\nPretty-print \nC code\nGenerate \nmachine code\nGenerate \nLLVM code\nM.hc\n(C code)\nM.s\n(asm code)\nM.ll\n(LLVM code)\nFigure 5.2: The compiler phases\nGHC uses the tools Alex and Happy to generate its lexical analysis and parsing code respectively,\nwhich are analogous to the tools lex and yacc for C.\nGHC\u2019s parser is purely functional. In fact, the API of the GHC library provides a pure function\ncalled parser that takes a String (and a few other things) and returns either the parsed abstract\nsyntax or an error message. Simon Marlow and Simon Peyton Jones\n71", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 85", "position": 85, "chunk_type": "semantic", "token_estimate": 137}
{"text": "Renaming: Renaming is the process of resolving all of the identi\ufb01ers in the Haskell source code into fully quali\ufb01ed\nnames, at the same time identifying any out-of-scope identi\ufb01ers and \ufb02agging errors appropriately. In Haskell it is possible for a module to re-export an identi\ufb01er that it imported from another\nmodule. For example, suppose module A de\ufb01nes a function called f, and module B imports module\nA and re-exports f. Now, if a module C imports module B, it can refer to f by the name B.f\u2014even\nthough f is originally de\ufb01ned in module A. This is a useful form of namespace manipulation; it\nmeans that a library can use whatever module structure it likes internally, but expose a nice clean\nAPI via a few interface modules that re-export identi\ufb01ers from the internal modules. The compiler however has to resolve all this, so that it knows what each name in the source\ncode corresponds to. We make a clean distinction between the entities, the \u201cthings themselves\u201d (in\nour example, A.f), and the names by which the entities can be referred to (e.g., B.f). At any given\npoint in the source code, there are a set of entities in scope, and each may be known by one or more\ndi\ufb00erent names. The job of the renamer is to replace each of the names in the compiler\u2019s internal\nrepresentation of the code by a reference to a particular entity. Sometimes a name can refer to several\ndi\ufb00erent entities; by itself that is not an error, but if the name is actually used, then the renamer will\n\ufb02ag an ambiguity error and reject the program. Renaming takes Haskell abstract syntax (HsSyn RdrName) as input, and also produces abstract\nsyntax as output (HsSyn Name). Here a Name is a reference to a particular entity. Resolving names is the main job of the renamer, but it performs a plethora of other tasks too:\ncollecting the equations of a function together and \ufb02agging an error if they have di\ufb00ering numbers of\narguments; rearranging in\ufb01x expressions according to the \ufb01xity of the operators; spotting duplicate\ndeclarations; generating warnings for unused identi\ufb01ers, and so on. Type Checking\nType checking, as one might imagine, is the process of checking that the Haskell program is type-\ncorrect.", "domains": ["Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 86", "position": 86, "chunk_type": "semantic", "token_estimate": 376}
{"text": "Renaming: Resolving names is the main job of the renamer, but it performs a plethora of other tasks too:\ncollecting the equations of a function together and \ufb02agging an error if they have di\ufb00ering numbers of\narguments; rearranging in\ufb01x expressions according to the \ufb01xity of the operators; spotting duplicate\ndeclarations; generating warnings for unused identi\ufb01ers, and so on. Type Checking\nType checking, as one might imagine, is the process of checking that the Haskell program is type-\ncorrect. If the program passes the type checker, then it is guaranteed to not crash at runtime.3\nThe input to the type checker is HsSyn Name (Haskell source with quali\ufb01ed names), and the\noutput is HsSyn Id. An Id is a Name with extra information: notably a type. In fact, the Haskell\nsyntax produced by the type checker is fully decorated with type information: every identi\ufb01er has its\ntype attached, and there is enough information to reconstruct the type of any subexpression (which\nmight be useful for an IDE, for example). In practice, type checking and renaming may be interleaved, because the Template Haskell feature\ngenerates code at runtime that itself needs to be renamed and type checked. Desugaring, and the Core language\nHaskell is a rather large language, containing many di\ufb00erent syntactic forms. It is intended to\nbe easy for humans to read and write\u2014there is a wide range of syntactic constructs which gives\nthe programmer plenty of \ufb02exibility in choosing the most appropriate construct for the situation\nat hand. However, this \ufb02exibility means that there are often several ways to write the same code;\nfor example, an if expression is identical in meaning to a case expression with True and False\nbranches, and list-comprehension notation can be translated into calls to map, filter, and concat. 3The term \u201ccrash\u201d here has a formal de\ufb01nition that includes hard crashes like \u201csegmentation fault\u201d, but not things like\npattern-matching failure. The non-crash guarantee can be subverted by using certain unsafe language features, such as the\nForeign Function Interface. 72\nThe Glasgow Haskell Compiler", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 86", "position": 86, "chunk_type": "semantic", "token_estimate": 338}
{"text": "In fact, the de\ufb01nition of the Haskell language de\ufb01nes all these constructs by their translation into: simpler constructs; the constructs that can be translated away like this are called \u201csyntactic sugar\u201d. It is much simpler for the compiler if all the syntactic sugar is removed, because the subsequent\noptimisation passes that need to work with the Haskell program have a smaller language to deal\nwith. The process of desugaring therefore removes all the syntactic sugar, translating the full Haskell\nsyntax into a much smaller language that we call Core. We\u2019ll talk about Core in detail in Section 5.3. Optimisation\nNow that the program is in Core, the process of optimisation begins. One of GHC\u2019s great strengths\nis in optimising away layers of abstraction, and all of this work happens at the Core level. Core is\na tiny functional language, but it is a tremendously \ufb02exible medium for expressing optimisations,\nranging from the very high-level, such as strictness analysis, to the very low-level, such as strength\nreduction. Each of the optimisation passes takes Core and produces Core. The main pass here is called the\nSimpli\ufb01er, whose job it is to perform a large collection of correctness-preserving transformations,\nwith the goal of producing a more e\ufb03cient program. Some of these transformations are simple\nand obvious, such as eliminating dead code or reducing a case expression when the value being\nscrutinised is known, and some are more involved, such as function inlining and applying rewrite\nrules (Section 5.4). The simpli\ufb01er is normally run between the other optimisation passes, of which there are about\nsix; which passes are actually run and in which order depends on the optimisation level selected by\nthe user. Code Generation\nOnce the Core program has been optimised, the process of code generation begins. After a couple of\nadministrative passes, the code takes one of two routes: either it is turned into byte code for execution\nby the interactive interpreter, or it is passed to the code generator for eventual translation to machine\ncode. The code generator \ufb01rst converts the Core into a language called STG, which is essentially just\nCore annotated with more information required by the code generator. Then, STG is translated to Cmm,\na low-level imperative language with an explicit stack. From here, the code takes one of three routes:\n\u2022 Native code generation: GHC contains simple native code generators for a few processor\narchitectures. This route is fast, and generates reasonable code in most cases.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 87", "position": 87, "chunk_type": "semantic", "token_estimate": 409}
{"text": "to run it? Moreover, making Core typed carries signi\ufb01cant costs, because every transformation or: optimisation pass must produce a well-typed program, and generating all those type annotations is\noften non-trivial. Nevertheless, it has been a huge win to have an explicitly-typed intermediate language, for several\nreasons:\n\u2022 Running the Core type checker (we call it Lint) is a very powerful consistency check on the\ncompiler itself. Imagine that you write an \u201coptimisation\u201d that accidentally generates code that\ntreats an integer value as a function, and tries to call it. The chances are that the program will\nsegmentation fault, or fail at runtime in a bizarre way. Tracing a seg-fault back to the particular\noptimisation pass that broke the program is a long road. Now imagine instead that we run Lint after every optimisation pass (and we do, if you use\nthe \ufb02ag -dcore-lint): it will report a precisely located error immediately after the o\ufb00ending\noptimisation. What a blessing. Of course, type soundness is not the same as correctness: Lint will not signal an error if you\n\u201coptimise\u201d (x \u22171) to 1 instead of to x. But if the program passes Lint, it will guarantee to\nrun without seg-faults; and moreover in practice we have found that it is surprisingly hard to\naccidentally write optimisations that are type-correct but not semantically correct. \u2022 The type inference algorithm for Haskell is very large and very complex: a glance at Figure 5.1\ncon\ufb01rms that the type checker is by far the largest single component of GHC. Large and complex\nmeans error-prone. But Lint serves as an 100% independent check on the type inference\nengine; if the type inference engine accepts a program that is not, in fact, type-correct, Lint\nwill reject it. So Lint serves as a powerful auditor of the type inference engine. \u2022 The existence of Core has also proved to be a tremendous sanity check on the design of\nthe source language. Our users constantly suggest new features that they would like in the\nlanguage. Sometimes these features are manifestly \u201csyntactic sugar\u201d, convenient new syntax\nfor something you can do already. But sometimes they are deeper, and it can be hard to tell\nhow far-reaching the feature is. Core gives us a precise way to evaluate such features. If the feature can readily be translated\ninto Core, that reassures us that nothing fundamentally new is going on: the new feature is\nsyntactic-sugar-like.", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 89", "position": 89, "chunk_type": "semantic", "token_estimate": 402}
{"text": "\u2022 On the other hand, type checking after desugaring would impose a signi\ufb01cant new obligation:: that desugaring does not a\ufb00ect which programs are type-correct. After all, desugaring implies\na deliberate loss of information. It is probably the case that in 95% of the cases there is no\nproblem, but any problem here would force some compromise in the design of Core to preserve\nsome extra information. \u2022 Most seriously of all, type checking a desugared program would make it much harder to report\nerrors that relate to the original program text, and not to its (sometimes elaborate) desugared\nversion. Most compilers type check after desugaring, but for GHC we made the opposite choice: we type\ncheck the full original Haskell syntax, and then desugar the result. It sounds as if adding a new\nsyntactic construct might be complicated, but (following the French school) we have structured the\ntype inference engine in a way that makes it easy. Type inference is split into two parts:\n1. Constraint generation: walk over the source syntax tree, generating a collection of type\nconstraints. This step deals with the full syntax of Haskell, but it is very straightforward code,\nand it is easy to add new cases. 2. Constraint solving: solve the gathered constraints. This is where the subtlety of the type\ninference engine lies, but it is independent of the source language syntax, and would be the\nsame for a much smaller or much larger language. On the whole, the type-check-before-desugar design choice has turned out to be a big win. Yes,\nit adds lines of code to the type checker, but they are simple lines. It avoids giving two con\ufb02icting\nroles to the same data type, and makes the type inference engine less complex, and easier to modify. Moreover, GHC\u2019s type error messages are pretty good. No Symbol Table\nCompilers usually have one or more data structures known as symbol tables, which are mappings\nfrom symbols (e.g., variables) to some information about the variable, such as its type, or where in\nthe source code it was de\ufb01ned. In GHC we use symbol tables quite sparingly; mainly in the renamer and type checker. As far as\npossible, we use an alternative strategy: a variable is a data structure that contains all the information\nabout itself.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 90", "position": 90, "chunk_type": "semantic", "token_estimate": 381}
{"text": "\u2022 On the other hand, type checking after desugaring would impose a signi\ufb01cant new obligation:: In GHC we use symbol tables quite sparingly; mainly in the renamer and type checker. As far as\npossible, we use an alternative strategy: a variable is a data structure that contains all the information\nabout itself. Indeed, a large amount of information is reachable by traversing the data structure of a\nvariable: from a variable we can see its type, which contains type constructors, which contain their\ndata constructors, which themselves contain types, and so on. For example, here are some data types\nfrom GHC (heavily abbreviated and simpli\ufb01ed):\ndata Id\n= MkId Name Type\ndata Type\n= TyConApp TyCon [Type]\n| ....\ndata TyCon\n= AlgTyCon Name [DataCon]\n| ...\ndata DataCon = MkDataCon Name Type ...\nAn Id contains its Type. A Type might be an application of a type constructor to some arguments\n(e.g., Maybe Int), in which case it contains the TyCon. A TyCon can be an algebraic data type, in\nwhich case it includes a list of its data constructors. Each DataCon includes its Type, which of course\nmentions the TyCon. And so on. The whole structure is highly interconnected. Indeed it is cyclic;\nfor example, a TyCon may contain a DataCon which contains a Type, which contains the very TyCon\nwe started with. 76\nThe Glasgow Haskell Compiler", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 90", "position": 90, "chunk_type": "semantic", "token_estimate": 230}
{"text": "This approach has some advantages and disadvantages:: \u2022 Many queries that would require a lookup in a symbol table are reduced to a simple \ufb01eld\naccess, which is great for e\ufb03ciency and code clarity. \u2022 There is no need to carry around extra symbol tables, the abstract syntax tree already contains\nall the information. \u2022 The space overheads are better: all instances of the same variable share the same data structure,\nand there is no space needed for the table. \u2022 The only di\ufb03culties arise when we need to change any of the information associated with a\nvariable. This is where a symbol table has the advantage: we would just change the entry in the\nsymbol table. In GHC we have to traverse the abstract syntax tree and replace all the instances\nof the old variable with the new one; indeed the simpli\ufb01er does this regularly, as it needs to\nupdate certain optimisation-related information about each variable. It is hard to know whether it would be better or worse overall to use symbol tables, because this\naspect of the design is so fundamental that it is almost impossible to change. Still, avoiding symbol\ntables is a natural choice in the purely functional setting, so it seems likely that this approach is a\ngood choice for Haskell. Inter-Module Optimisation\nFunctional languages encourage the programmer to write small de\ufb01nitions. For example, here is the\nde\ufb01nition of && from the standard library:\n(&&) :: Bool -> Bool -> Bool\nTrue && True = True\n_\n&& _\n= False\nIf every use of such a function really required a function call, e\ufb03ciency would be terrible. One\nsolution is to make the compiler treat certain functions specially; another is to use a pre-processor to\nreplace a \u201ccall\u201d with the desired inline code. All of these solutions are unsatisfactory in one way or\nanother, especially as another solution is so obvious: simply inline the function. To \u201cinline a function\u201d\nmeans to replace the call by a copy of the function body, suitably instantiating its parameters. In GHC we have systematically adopted this approach [PM02]. Virtually nothing is built into the\ncompiler. Instead, we de\ufb01ne as much as possible in libraries, and use aggressive inlining to eliminate\nthe overheads. This means that programmers can de\ufb01ne their own libraries that will be inlined and\noptimised as well as the ones that come with GHC.", "domains": ["Design Patterns", "Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 91", "position": 91, "chunk_type": "semantic", "token_estimate": 397}
{"text": "This approach has some advantages and disadvantages:: Instead, we de\ufb01ne as much as possible in libraries, and use aggressive inlining to eliminate\nthe overheads. This means that programmers can de\ufb01ne their own libraries that will be inlined and\noptimised as well as the ones that come with GHC. A consequence is that GHC must be able to do cross-module, and indeed cross-package, inlining. The idea is simple:\n\u2022 When compiling a Haskell module Lib.hs, GHC produces object code in Lib.o and an\n\u201cinterface \ufb01le\u201d in Lib.hi. This interface \ufb01le contains information about all the functions that\nLib exports, including both their types and, for su\ufb03ciently small functions, their de\ufb01nitions. \u2022 When compiling a module Client.hs that imports Lib, GHC reads the interface Lib.hi. So\nif Client calls a function Lib.f de\ufb01ned in Lib, GHC can use the information in Lib.hi to\ninline Lib.f. By default GHC will expose the de\ufb01nition of a function in the interface \ufb01le only if the function\nis \u201csmall\u201d (there are \ufb02ags to control this size threshold). But we also support an INLINE pragma, to\ninstruct GHC to inline the de\ufb01nition aggressively at call sites, regardless of size, thus:\nSimon Marlow and Simon Peyton Jones\n77", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 91", "position": 91, "chunk_type": "semantic", "token_estimate": 200}
{"text": "foo :: Int -> Int: {-# INLINE foo #-}\nfoo x = <some big expression>\nCross-module inlining is absolutely essential for de\ufb01ning super-e\ufb03cient libraries, but it does\ncome with a cost. If the author upgrades his library, it is not enough to re-link Client.o with the new\nLib.o, because Client.o contains inlined fragments of the old Lib.hs, and they may well not be\ncompatible with the new one. Another way to say this is that the ABI (Application Binary Interface)\nof Lib.o has changed in a way that requires recompilation of its clients. In fact, the only way for compilation to generate code with a \ufb01xed, predictable ABI is to disable\ncross-module optimisation, and this is typically too high a price to pay for ABI compatibility. Users\nworking with GHC will usually have the source code to their entire stack available, so recompiling is\nnot normally an issue (and, as we will describe later, the package system is designed around this\nmode of working). However, there are situations where recompiling is not practical: distributing bug\n\ufb01xes to libraries in a binary OS distribution, for example. In the future we hope it may be possible\nto \ufb01nd a compromise solution that allows retaining ABI compatibility while still allowing some\ncross-module optimisation to take place. 5.4\nExtensibility\nIt is often the case that a project lives or dies according to how extensible it is. A monolithic piece of\nsoftware that is not extensible has to do everything and do it right, whereas an extensible piece of\nsoftware can be a useful base even if it doesn\u2019t provide all the required functionality out of the box. Open source projects are of course extensible by de\ufb01nition, in that anyone can take the code and\nadd their own features. But modifying the original source code of a project maintained by someone\nelse is not only a high-overhead approach, it is also not conducive to sharing your extension with\nothers. Therefore successful projects tend to o\ufb00er forms of extensibility that do not involve modifying\nthe core code, and GHC is no exception in this respect. User-De\ufb01ned Rewrite Rules\nThe core of GHC is a long sequence of optimisation passes, each of which performs some semantics-\npreserving transformation, Core into Core. But the author of a library de\ufb01nes functions that often\nhave some non-trivial, domain-speci\ufb01c transformations of their own, ones that cannot possibly be\npredicted by GHC.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 92", "position": 92, "chunk_type": "semantic", "token_estimate": 400}
{"text": "pragmas, GHC is able to fuse together list-transforming functions. For example, the two loops in: (map f (map g xs)) are fused into one. Although rewrite rules are simple and easy to use, they have proved to be a very powerful\nextension mechanism. When we \ufb01rst introduced the feature into GHC ten years ago we expected\nit to be an occasionally useful facility. But in practice it has turned out to be useful in very many\nlibraries, whose e\ufb03ciency often depends crucially on rewrite rules. For example, GHC\u2019s own base\nlibrary contains upward of 100 rules, while the popular vector library uses several dozen. Compiler Plugins\nOne way in which a compiler can o\ufb00er extensibility is to allow programmers to write a pass that is\ninserted directly into the compiler\u2019s pipeline. Such passes are often called \u201cplugins\u201d. GHC supports\nplugins in the following way:\n\u2022 The programmer writes a Core to Core pass, as an ordinary Haskell function in a module P.hs,\nsay, and compiles it to object code. \u2022 When compiling some module, the programmer uses the command-line \ufb02ag -plugin P.\n(Alternatively, he can give the \ufb02ag in a pragma at the start of the module.) \u2022 GHC searches for P.o, dynamically links it into the running GHC binary, and calls it at the\nappropriate point in the pipeline. But what is \u201cthe appropriate point in the pipeline\u201d? GHC does not know, and so it allows the\nplugin to make that decision. As a result of this and other matters, the API that the plugin must o\ufb00er\nis a bit more complicated than a single Core to Core function\u2014but not much. Plugins sometimes require, or produce, auxiliary plugin-speci\ufb01c data. For example, a plugin\nmight perform some analysis on the functions in the module being compiled (M.hs, say), and might\nwant to put that information in the interface \ufb01le M.hi, so that the plugin has access to that information\nwhen compiling modules that import M. GHC o\ufb00ers an annotation mechanism to support this. Plugins and annotations are relatively new to GHC. They have a higher barrier to entry than\nrewrite rules, because the plugin is manipulating GHC\u2019s internal data structures, but of course they\ncan do much more. It remains to be seen how widely they will be used. GHC as a Library: The GHC API\nOne of GHC\u2019s original goals was to be a modular foundation that others could build on.", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 93", "position": 93, "chunk_type": "semantic", "token_estimate": 402}
{"text": "pragmas, GHC is able to fuse together list-transforming functions. For example, the two loops in: It remains to be seen how widely they will be used. GHC as a Library: The GHC API\nOne of GHC\u2019s original goals was to be a modular foundation that others could build on. We wanted\nthe code of GHC to be as transparent and well-documented as possible, so that it could be used as\nthe basis for research projects by others; we imagined that people would want to make their own\nmodi\ufb01cations to GHC to add new experimental features or optimisations. Indeed, there have been\nsome examples of this: for example, there exists a version of GHC with a Lisp front-end, and a\nversion of GHC that generates Java code, both developed entirely separately by individuals with little\nor no contact with the GHC team. However, producing modi\ufb01ed versions of GHC represents only a small subset of the ways in\nwhich the code of GHC can be re-used. As the popularity of the Haskell language has grown, there\nhas been an increasing need for tools and infrastructure that understand Haskell source code, and\nGHC of course contains a lot of the functionality necessary for building these tools: a Haskell parser,\nabstract syntax, type checker and so on. With this in mind, we made a simple change to GHC: rather than building GHC as a monolithic\nprogram, we build GHC as a library, that is then linked with a small Main module to make the GHC\nexecutable itself, but also shipped in library form so that users can call it from their own programs. Simon Marlow and Simon Peyton Jones\n79", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 93", "position": 93, "chunk_type": "semantic", "token_estimate": 276}
{"text": "At the same time we built an API to expose GHC\u2019s functionality to clients. The API provides enough: functionality to implement the GHC batch compiler and the GHCi interactive environment, but it\nalso provides access to individual passes such as the parser and type checker, and allows the data\nstructures produced by these passes to be inspected. This change has given rise to a wide range of\ntools built using the GHC API, including:\n\u2022 A documentation tool, Haddock4, which reads Haskell source code and produces HTML\ndocumentation. \u2022 New versions of the GHCi front end with additional features; e.g., ghci-haskeline5 which was\nsubsequently merged back into GHC. \u2022 IDEs that o\ufb00er advanced navigation of Haskell source code; e.g., Leksah6. \u2022 hint7, a simpler API for on-the-\ufb02y evaluation of Haskell source code. The Package System\nThe package system has been a key factor in the growth in use of the Haskell language in recent\nyears. Its main purpose is to enable Haskell programmers to share code with each other, and as such\nit is an important aspect of extensibility: the package system extends the shared codebase beyond\nGHC itself. The package system embodies various pieces of infrastructure that together make sharing code\neasy. With the package system as the enabler, the community has built a large body of shared\ncode; rather than relying on libraries from a single source, Haskell programmers draw on libraries\ndeveloped by the whole community. This model has worked well for other languages; CPAN for\nPerl, for example, although Haskell being a predominantly compiled rather than interpreted language\npresents a somewhat di\ufb00erent set of challenges. Basically, the package system lets a user manage libraries of Haskell code written by other people,\nand use them in their own programs and libraries. Installing a Haskell library is as simple as uttering\na single command, for example:\n$ cabal install zlib\ndownloads the code for the zlib package from http://hackage.haskell.org, compiles it using\nGHC, installs the compiled code somewhere on your system (e.g., in your home directory on a Unix\nsystem), and registers the installation with GHC. Furthermore, if zlib depends on any other packages\nthat are not yet installed, those will also be downloaded, compiled and installed automatically before\nzlib itself is compiled. It is a tremendously smooth way to work with libraries of Haskell code\nshared by others.", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 94", "position": 94, "chunk_type": "semantic", "token_estimate": 391}
{"text": "At the same time we built an API to expose GHC\u2019s functionality to clients. The API provides enough: Furthermore, if zlib depends on any other packages\nthat are not yet installed, those will also be downloaded, compiled and installed automatically before\nzlib itself is compiled. It is a tremendously smooth way to work with libraries of Haskell code\nshared by others. The package system is made of four components, only the \ufb01rst of which is strictly part of the\nGHC project:\n\u2022 Tools for managing the package database, which is simply a repository for information about\nthe packages installed on your system. GHC reads the package database when it starts up, so\nthat it knows which packages are available and where to \ufb01nd them. \u2022 A library called Cabal (Common Architecture for Building Applications and Libraries), which\nimplements functionality for building, installing and registering individual packages. 4http://www.haskell.org/haddock/\n5http://hackage.haskell.org/package/ghci-haskeline\n6http://hackage.haskell.org/package/leksah\n7http://hackage.haskell.org/package/hint\n80\nThe Glasgow Haskell Compiler", "domains": ["Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 94", "position": 94, "chunk_type": "semantic", "token_estimate": 155}
{"text": "\u2022 A website at http://hackage.haskell.org which hosts packages written and uploaded by: users. The website automatically builds documentation for the packages which can be browsed\nonline. At the time of writing, Hackage is hosting over 3,000 packages covering functionality\nincluding database libraries, web frameworks, GUI toolkits, data structures, and networking. \u2022 The cabal tool which ties together the Hackage website and the Cabal library: it downloads\npackages from Hackage, resolves dependencies, and builds and installs packages in the right\norder. New packages can also be uploaded to Hackage using cabal from the command line. These components have been developed over several years by members of the Haskell commu-\nnity and the GHC team, and together they make a system that \ufb01ts perfectly with the open source\ndevelopment model. There are no barriers to sharing code or using code that others have shared\n(provided you respect the relevant licenses, of course). You can be using a package that someone\nelse has written literally within seconds of \ufb01nding it on Hackage. Hackage has been so successful that the remaining problems it has are now those of scale: users\n\ufb01nd it di\ufb03cult to choose amongst the four di\ufb00erent database frameworks, for example. Ongoing\ndevelopments are aimed at solving these problems in ways that leverage the community. For example,\nallowing users to comment and vote on packages will make it easier to \ufb01nd the best and most popular\npackages, and collecting data on build success or failures from users and reporting the results will\nhelp users avoid packages that are unmaintained or have problems. 5.5\nThe Runtime System\nThe Runtime System is a library of mostly C code that is linked into every Haskell program. It\nprovides the support infrastructure needed for running the compiled Haskell code, including the\nfollowing main components:\n\u2022 Memory management, including a parallel, generational, garbage collector;\n\u2022 Thread management and scheduling;\n\u2022 The primitive operations provided by GHC;\n\u2022 A bytecode interpreter and dynamic linker for GHCi. The rest of this section is divided into two: \ufb01rst we focus on a couple of the aspects of the design\nof the RTS that we consider to have been successful and instrumental in making it work so well, and\nsecondly we talk about the coding practices and infrastructure we have built in the RTS for coping\nwith what is a rather hostile programming environment.", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 95", "position": 95, "chunk_type": "semantic", "token_estimate": 393}
{"text": "\u2022 A website at http://hackage.haskell.org which hosts packages written and uploaded by: It\nprovides the support infrastructure needed for running the compiled Haskell code, including the\nfollowing main components:\n\u2022 Memory management, including a parallel, generational, garbage collector;\n\u2022 Thread management and scheduling;\n\u2022 The primitive operations provided by GHC;\n\u2022 A bytecode interpreter and dynamic linker for GHCi. The rest of this section is divided into two: \ufb01rst we focus on a couple of the aspects of the design\nof the RTS that we consider to have been successful and instrumental in making it work so well, and\nsecondly we talk about the coding practices and infrastructure we have built in the RTS for coping\nwith what is a rather hostile programming environment. Key Design Decisions\nIn this section we describe two of the design decisions in the RTS that we consider to have been\nparticularly successful. The Block Layer\nThe garbage collector is built on top of a block layer that manages memory in units of blocks, where\na block is a multiple of 4 KB in size. The block layer has a very simple API:\ntypedef struct bdescr_ {\nvoid *\nstart;\nstruct bdescr_ *\nlink;\nSimon Marlow and Simon Peyton Jones\n81", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 95", "position": 95, "chunk_type": "semantic", "token_estimate": 206}
{"text": "struct generation_ * gen;: // generation\n// .. various other fields\n} bdescr;\nbdescr * allocGroup (int n);\nvoid\nfreeGroup\n(bdescr *p);\nbdescr * Bdescr\n(void *p);\n// a macro\nThis is the only API used by the garbage collector for allocating and deallocating memory. Blocks\nof memory are allocated with allocGroup and freed with freeGroup. Every block has a small\nstructure associated with it called a block descriptor (bdescr). The operation Bdescr(p) returns the\nblock descriptor associated with an arbitrary address p; this is purely an address calculation based\non the value of p and compiles to a handful of arithmetic and bit-manipulation instructions. Blocks may be linked together into chains using the link \ufb01eld of the bdescr, and this is the real\npower of the technique. The garbage collector needs to manage several distinct areas of memory\nsuch as generations, and each of these areas may need to grow or shrink over time. By representing\nmemory areas as linked lists of blocks, the GC is freed from the di\ufb03culties of \ufb01tting multiple resizable\nmemory areas into a \ufb02at address space. The implementation of the block layer uses techniques that are well-known from C\u2019s\nmalloc()/free() API; it maintains lists of free blocks of various sizes, and coalesces free ar-\neas. The operations freeGroup() and allocGroup() are carefully designed to be O(1). One major advantage of this design is that it needs very little support from the OS, and hence is\ngreat for portability. The block layer needs to allocate memory in units of 1 MB, aligned to a 1 MB\nboundary. While none of the common OSs provide this functionality directly, it is implementable\nwithout much di\ufb03culty in terms of the facilities they do provide. The payo\ufb00is that GHC has no\ndependence on the particular details of the address-space layout used by the OS, and it coexists\npeacefully with other users of the address space, such as shared libraries and operating system\nthreads. There is a small up-front complexity cost for the block layer, in terms of managing chains of\nblocks rather than contiguous memory. However, we have found that this cost is more than repaid in\n\ufb02exibility and portability; for example, the block layer enabled a particularly simple algorithm for\nparallel GC to be implemented [MHJP08].", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 96", "position": 96, "chunk_type": "semantic", "token_estimate": 378}
{"text": "struct generation_ * gen;: There is a small up-front complexity cost for the block layer, in terms of managing chains of\nblocks rather than contiguous memory. However, we have found that this cost is more than repaid in\n\ufb02exibility and portability; for example, the block layer enabled a particularly simple algorithm for\nparallel GC to be implemented [MHJP08]. Lightweight Threads and Parallelism\nWe consider concurrency to be a vitally important programming abstraction, particularly for building\napplications like web servers that need to interact with large numbers of external agents simultaneously. If concurrency is an important abstraction, then it should not be so expensive that programmers\nare forced to avoid it, or build elaborate infrastructure to amortise its cost (e.g., thread pools). We\nbelieve that concurrency should just work, and be cheap enough that you don\u2019t worry about forking\nthreads for small tasks. All operating systems provide threads that work perfectly well, the problem is that they are far\ntoo expensive. Typical OSs struggle to handle thousands of threads, whereas we want to manage\nthreads by the million. Green threads, otherwise known as lightweight threads or user-space threads, are a well-known\ntechnique for avoiding the overhead of operating system threads. The idea is that threads are managed\nby the program itself, or a library (in our case, the RTS), rather than by the operating system. 82\nThe Glasgow Haskell Compiler", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 96", "position": 96, "chunk_type": "semantic", "token_estimate": 229}
{"text": "Managing threads in user space should be cheaper, because fewer traps into the operating system are: required. In the GHC RTS we take full advantage of this idea. A context switch only occurs when the\nthread is at a safe point, where very little additional state needs to be saved. Because we use accurate\nGC, the stack of the thread can be moved and expanded or shrunk on demand. Contrast these with\nOS threads, where every context switch must save the entire processor state, and where stacks are\nimmovable so a large chunk of address space has to be reserved up front for each thread. Green threads can be vastly more e\ufb03cient than OS threads, so why would anyone want to use\nOS threads? It comes down to three main problems:\n\u2022 Blocking and foreign calls. A thread should be able to make a call to an OS API or a foreign\nlibrary that blocks, without blocking all the other threads in the system. \u2022 Parallelism. Threads should automatically run in parallel if there are multiple processor cores\non the system. \u2022 Some external libraries (notably OpenGL and some GUI libraries) have APIs that must be\ncalled from the same OS thread each time, because they use thread-local state. It turns out that all of these are di\ufb03cult to arrange with green threads. Nevertheless, we persevered\nwith green threads in GHC and found solutions to all three:\n\u2022 When a Haskell thread makes a foreign call, another OS thread takes over the execution of\nthe remaining Haskell threads [MPT04]. A small pool of OS threads are maintained for this\npurpose, and new ones are created on demand. \u2022 GHC\u2019s scheduler multiplexes many lightweight Haskell threads onto a few heavyweight OS\nthreads; it implements a transparent M:N threading model. Typically N is chosen to be the\nsame as the number of processor cores in the machine, allowing real parallelism to take place\nbut without the overhead of having a full OS thread for each lightweight Haskell thread. In order to run Haskell code, an OS thread must hold a Capability8: a data structure that\nholds the resources required to execute Haskell code, such as the nursery (memory where new\nobjects are created). Only one OS thread may hold a given Capability at a time.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 97", "position": 97, "chunk_type": "semantic", "token_estimate": 383}
{"text": "Managing threads in user space should be cheaper, because fewer traps into the operating system are: In order to run Haskell code, an OS thread must hold a Capability8: a data structure that\nholds the resources required to execute Haskell code, such as the nursery (memory where new\nobjects are created). Only one OS thread may hold a given Capability at a time. \u2022 We provide an API for creating a bound thread: a Haskell thread that is tied to one speci\ufb01c\nOS thread, such that any foreign calls made by this Haskell thread are guaranteed to be made\nby that OS thread. So in the vast majority of cases, Haskell\u2019s threads behave exactly like OS threads: they can make\nblocking OS calls without a\ufb00ecting other threads, and they run in parallel on a multicore machine. But they are orders of magnitude more e\ufb03cient, in terms of both time and space. Having said that, the implementation does have one problem that users occasionally run into,\nespecially when running benchmarks. We mentioned above that lightweight threads derive some\nof their e\ufb03ciency by only context-switching at \u201csafe points\u201d, points in the code that the compiler\ndesignates as safe, where the internal state of the virtual machine (stack, heap, registers, etc.) is in\na tidy state and garbage collection could take place. In GHC, a safe point is whenever memory is\nallocated, which in almost all Haskell programs happens regularly enough that the program never\nexecutes more than a few tens of instructions without hitting a safe point. However, it is possible in\nhighly optimised code to \ufb01nd loops that run for many iterations without allocating memory. This\ntends to happen often in benchmarks (e.g., functions like factorial and Fibonacci). It occurs less\noften in real code, although it does happen. The lack of safe points prevents the scheduler from\nrunning, which can have detrimental e\ufb00ects. It is possible to solve this problem, but not without\n8We have also called it a \u201cHaskell Execution Context\u201d, but the code currently uses the Capability terminology. Simon Marlow and Simon Peyton Jones\n83", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 97", "position": 97, "chunk_type": "semantic", "token_estimate": 347}
{"text": "impacting the performance of these loops, and often people care about saving every cycle in their: inner loops. This may just be a compromise we have to live with. 5.6\nDeveloping GHC\nGHC is a single project with a twenty-year life span, and is still in a ferment of innovation and\ndevelopment. For the most part our infrastructure and tooling has been conventional. For example,\nwe use a bug tracker (Trac), a wiki (also Trac), and Git for revision control. (This revision-control\nmechanism evolved from purely manual, then CVS, then Darcs, before \ufb01nally moving to Git in 2010.) There are a few points that may be less universal, and we o\ufb00er them here. Comments and Notes\nOne of the most serious di\ufb03culties in a large, long-lived project is keeping technical documentation\nup to date. We have no silver bullet, but we o\ufb00er one low-tech mechanism that has served us\nparticularly well: Notes. When writing code, there is often a moment when a careful programmer will mentally say\nsomething like \u201cThis data type has an important invariant\u201d. She is faced with two choices, both\nunsatisfactory. She can add the invariant as a comment, but that can make the data type declaration\ntoo long, so that it is hard to see what the constructors are. Alternatively, she can document the\ninvariant elsewhere, and risk it going out of date. Over twenty years, everything goes out of date! Thus motivated, we developed the following very simple convention:\n\u2022 Comments of any signi\ufb01cant size are not interleaved with code, but instead set o\ufb00by themselves,\nwith a heading in standard form, thus:\nNote [Equality-constrained types]\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nThe type\nforall ab. (a ~ [b]) => blah\nis encoded like this:\nForAllTy (a:*) $ ForAllTy (b:*) $\nFunTy (TyConApp (~) [a, [b]]) $\nblah\n\u2022 A the point where the comment is relevant, we add a short comment referring to the Note:\ndata Type\n= FunTy Type Type -- See Note [Equality-constrained types]\n| ... The comment highlights that something interesting is going on, and gives a precise reference to\nthe comment that explains. It sounds trivial, but the precision is vastly better than our previous\nhabit of saying \u201csee the comment above\u201d, because it often was not clear which of the many\ncomments above was intended, and after a few years the comment was not even above (it was\nbelow, or gone altogether). 84\nThe Glasgow Haskell Compiler", "domains": ["Design Patterns", "Software Quality Attributes", "Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 98", "position": 98, "chunk_type": "semantic", "token_estimate": 402}
{"text": "Not only is it possible to go from the code that refers to the Note to the Note itself, but the reverse: is also possible, and that is often useful. Moreover, the same Note may be referred to from multiple\npoints in the code. This simple, ASCII-only technique, with no automated support, has transformed our lives: GHC\nhas around 800 Notes, and the number grows daily. How to Keep On Refactoring\nThe code of GHC is churning just as quickly as it was ten years ago, if not more so. There is no\ndoubt that the complexity of the system has increased manyfold over that same time period; we saw\nmeasures of the amount of code in GHC earlier. Yet, the system remains manageable. We attribute\nthis to three main factors:\n\u2022 There\u2019s no substitute for good software engineering. Modularity always pays o\ufb00: making\nthe APIs between components as small as possible makes the individual components more\n\ufb02exible because they have fewer interdependencies. For example, GHC\u2019s Core{} datatype\nbeing small reduces the coupling between Core-to-Core passes, to the extent that they are\nalmost completely independent and can be run in arbitrary order. \u2022 Developing in a strongly-typed language makes refactoring a breeze. Whenever we need to\nchange a data type, or change the number of arguments or type of a function, the compiler\nimmediately tells us what other places in the code need to be \ufb01xed. Simply having an absolute\nguarantee that a large class of errors have been statically ruled out saves a huge amount of\ntime, especially when refactoring. It is scary to imagine how many hand-written test cases we\nwould need to provide the same level of coverage that the type system provides. \u2022 When programming in a purely functional language, it is hard to introduce accidental depen-\ndencies via state. If you decide that you suddenly need access to a piece of state deep in an\nalgorithm, in an imperative language you might be tempted to just make the state globally\nvisible rather than explicitly pass it down to the place that needs it. This way eventually leads\nto a tangle of invisible dependencies, and brittle code: code that breaks easily when modi\ufb01ed. Pure functional programming forces you to make all the dependencies explicit, which exerts\nsome negative pressure on adding new dependencies, and fewer dependencies means greater\nmodularity.", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 99", "position": 99, "chunk_type": "semantic", "token_estimate": 393}
{"text": "Not only is it possible to go from the code that refers to the Note to the Note itself, but the reverse: This way eventually leads\nto a tangle of invisible dependencies, and brittle code: code that breaks easily when modi\ufb01ed. Pure functional programming forces you to make all the dependencies explicit, which exerts\nsome negative pressure on adding new dependencies, and fewer dependencies means greater\nmodularity. Certainly when it is necessary to add a new dependency then purity makes you\nwrite more code to express the dependency, but in our view it is a worthwhile price to pay for\nthe long-term health of the code base. As an added bene\ufb01t, purely functional code is thread-safe by construction and tends to be\neasier to parallelise. Crime Doesn\u2019t Pay\nLooking back over the changes we\u2019ve had to make to GHC as it has grown, a common lesson emerges:\nbeing less than purely functional, whether for the purposes of e\ufb03ciency or convenience, tends to\nhave negative consequences down the road. We have a couple of great examples of this:\n\u2022 GHC uses a few data structures that rely on mutation internally. One is the FastString type,\nwhich uses a single global hash table; another is a global NameCache that ensures all external\nnames are assigned a unique number. When we tried to parallelise GHC (that is, make GHC\ncompile multiple modules in parallel on a multicore processor), these data structures based on\nSimon Marlow and Simon Peyton Jones\n85", "domains": ["Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 99", "position": 99, "chunk_type": "semantic", "token_estimate": 247}
{"text": "mutation were the only sticking points. Had we not resorted to mutation in these places, GHC: would have been almost trivial to parallelise. In fact, although we did build a prototype parallel version of GHC, GHC does not currently\ncontain support for parallel compilation, but that is largely because we have not yet invested\nthe e\ufb00ort required to make these mutable data structures thread-safe. \u2022 GHC\u2019s behaviour is governed to a large extent by command-line \ufb02ags. These command-line\n\ufb02ags are by de\ufb01nition constant over a given run of GHC, so in early versions of GHC we made\nthe values of these \ufb02ags available as top-level constants. For example, there was a top-level\nvalue opt_GlasgowExts of type Bool, that governed whether certain language extensions\nshould be enabled or not. Top-level constants are highly convenient, because their values don\u2019t\nhave to be explicitly passed as arguments to all the code that needs access to them. Of course these options are not really constants, because they change from run to run, and\nthe de\ufb01nition of opt_GlasgowExts involves calling unsafePerformIO because it hides a side\ne\ufb00ect. Nevertheless, this trick is normally considered \u201csafe enough\u201d because the value is\nconstant within any given run; it doesn\u2019t invalidate compiler optimisations, for example. However, GHC was later extended from a single-module compiler to a multi-module compiler. At this point the trick of using top-level constants for \ufb02ags broke, because the \ufb02ags may have\ndi\ufb00erent values when compiling di\ufb00erent modules. So we had to refactor large amounts of\ncode to pass around the \ufb02ags explicitly. Perhaps you might argue that treating the \ufb02ags as state in the \ufb01rst place, as would be natural\nin an imperative language, would have sidestepped the problem. To some extent this is true,\nalthough purely functional code has a number of other bene\ufb01ts, not least of which is that\nrepresenting the \ufb02ags by an immutable data structure means that the resulting code is already\nthread-safe and will run in parallel without modi\ufb01cation. Developing the RTS\nGHC\u2019s runtime system presents a stark contrast to the compiler in many ways. There is the obvious\ndi\ufb00erence that the runtime system is written in C rather than Haskell, but there are also considerations\nunique to the RTS that give rise to a di\ufb00erent design philosophy:\n1. Every Haskell program spends a lot of time executing code in the RTS: 20\u201330% is typical,\nbut characteristics of Haskell programs vary a lot and so \ufb01gures greater or less than this range\nare also common.", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 100", "position": 100, "chunk_type": "semantic", "token_estimate": 415}
{"text": "The symptoms of an RTS bug are often indistinguishable from two other kinds of failure:: hardware failure, which is more common than you might think, and misuse of unsafe Haskell\nfeatures like the FFI (Foreign Function Interface). The \ufb01rst job in diagnosing a runtime crash\nis to rule out these two other causes. 4. The RTS is low-level code that runs on several di\ufb00erent architectures and operating systems,\nand is regularly ported to new ones. Portability is important. Every cycle and every byte is important, but correctness is even more so. Moreover, the tasks\nperformed by the runtime system are inherently complex, so correctness is hard to begin with. Reconciling these has lead us to some interesting defensive techniques, which we describe in the\nfollowing sections. Coping With Complexity\nThe RTS is a complex and hostile programming environment. In contrast to the compiler, the RTS\nhas almost no type safety. In fact, it has even less type safety than most other C programs, because it\nis managing data structures whose types live at the Haskell level and not at the C level. For example,\nthe RTS has no idea that the object pointed to by the tail of a cons cell is either [] or another cons:\nthis information is simply not present at the C level. Moreover, the process of compiling Haskell\ncode erases types, so even if we told the RTS that the tail of a cons cell is a list, it would still have no\ninformation about the pointer in the head of the cons cell. So the RTS code has to do a lot of casting\nof C pointer types, and it gets very little help in terms of type safety from the C compiler. So our \ufb01rst weapon in this battle is to avoid putting code in the RTS. Wherever possible, we put\nthe minimum amount of functionality into the RTS and write the rest in a Haskell library. This has\nrarely turned out badly; Haskell code is far more robust and concise than C, and performance is\nusually perfectly acceptable. Deciding where to draw the line is not an exact science, although in\nmany cases it is reasonably clear. For example, while it might be theoretically possible to implement\nthe garbage collector in Haskell, in practice it is extremely di\ufb03cult because Haskell does not allow\nthe programmer precise control of memory allocation, and so dropping down to C for this kind of\nlow-level task makes practical sense.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 101", "position": 101, "chunk_type": "semantic", "token_estimate": 413}
{"text": "the heap. Tracking down these kinds of bugs can be extremely time-consuming10 because by the: time the program eventually crashes, execution might have progressed a long way from where the\ndangling pointer was originally introduced. There are good debugging tools available, but they tend\nnot to be good at executing the program in reverse.11\nThe general principle is: if a program is going to crash, it should crash as soon, as noisily, and\nas often as possible.12\nThe problem is, the no-dangling-pointer invariant is not something that can be checked with a\nconstant-time assertion. The assertion that checks it must do a full traversal of the heap! Clearly we\ncannot run this assertion after every heap allocation, or every time the GC scans an object (indeed,\nthis would not even be enough, as dangling pointers don\u2019t appear until the end of GC, when memory\nis freed). So, the debug RTS has an optional mode that we call sanity checking. Sanity checking enables all\nkinds of expensive assertions, and can make the program run many times more slowly. In particular,\nsanity checking runs a full scan of the heap to check for dangling pointers (amongst other things),\nbefore and after every GC. The \ufb01rst job when investigating a runtime crash is to run the program\nwith sanity checking turned on; sometimes this will catch the invariant violation well before the\nprogram actually crashes. 5.7\nConclusion\nGHC has consumed a signi\ufb01cant portion of the authors\u2019 lives over the last 20 years, and we are rather\nproud of how far it has come. It is not the only Haskell implementation, but it is the only one in\nregular use by hundreds of thousands of people to get real work done. We are constantly surprised\nwhen Haskell turns up being used in unusual places; one recent example is Haskell being used to\ncontrol the systems in a garbage truck13. For many, Haskell and GHC are synonymous: it was never intended to be so, and indeed in\nmany ways it is counterproductive to have just one implementation of a standard, but the fact is that\nmaintaining a good implementation of a programming language is a lot of work. We hope that our\ne\ufb00orts in GHC, to support the standard and to clearly delimit each separate language extension, will\nmake it feasible for more implementations to emerge and to integrate with the the package system\nand other infrastructure. Competition is good for everyone!", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 102", "position": 102, "chunk_type": "semantic", "token_estimate": 407}
{"text": "the heap. Tracking down these kinds of bugs can be extremely time-consuming10 because by the: We hope that our\ne\ufb00orts in GHC, to support the standard and to clearly delimit each separate language extension, will\nmake it feasible for more implementations to emerge and to integrate with the the package system\nand other infrastructure. Competition is good for everyone! We are deeply indebted to Microsoft in particular for giving us the opportunity to develop GHC\nas part of our research and to distribute it as open source. 10It is, however, one of the author\u2019s favourite activities! 11Recent versions of GDB and the Microsoft Visual Studio debugger do have some support for reverse execution, however. 12This quote comes from the GHC coding style guidelines, and was originally written by Alastair Reid, who worked on an\nearly version of the RTS. 13http://www.haskell.org/pipermail/haskell-cafe/2010-April/075647.html\n88\nThe Glasgow Haskell Compiler", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 102", "position": 102, "chunk_type": "semantic", "token_estimate": 145}
{"text": "[chapter6]: Git\nSusan Potter\n6.1\nGit in a Nutshell\nGit enables the maintenance of a digital body of work (often, but not limited to, code) by many\ncollaborators using a peer-to-peer network of repositories. It supports distributed work\ufb02ows, allowing\na body of work to either eventually converge or temporarily diverge. This chapter will show how various aspects of Git work under the covers to enable this, and how\nit di\ufb00ers from other version control systems (VCSs). 6.2\nGit\u2019s Origin\nTo understand Git\u2019s design philosophy better it is helpful to understand the circumstances in which\nthe Git project was started in the Linux Kernel Community. The Linux kernel was unusual, compared to most commercial software projects at that time,\nbecause of the large number of committers and the high variance of contributor involvement and\nknowledge of the existing codebase. The kernel had been maintained via tarballs and patches for\nyears, and the core development community struggled to \ufb01nd a VCS that satis\ufb01ed most of their needs. Git is an open source project that was born out of those needs and frustrations in 2005. At that\ntime the Linux kernel codebase was managed across two VCSs, BitKeeper and CVS, by di\ufb00erent\ncore developers. BitKeeper o\ufb00ered a di\ufb00erent view of VCS history lineage than that o\ufb00ered by the\npopular open source VCSs at this time. Days after BitMover, the maker of BitKeeper, announced it would revoke the licenses of some\ncore Linux kernel developers, Linus Torvalds began development, in haste, of what was to become\nGit. He began by writing a collection of scripts to help him manage email patches to apply one after\nthe other. The aim of this initial collection of scripts was to be able to abort merges quickly so the\nmaintainer could modify the codebase mid-patch-stream to manually merge, then continue merging\nsubsequent patches. From the outset, Torvalds had one philosophical goal for Git\u2014to be the anti-CVS\u2014plus three\nusability design goals:\n\u2022 Support distributed work\ufb02ows similar to those enabled by BitKeeper\n\u2022 O\ufb00er safeguards against content corruption\n\u2022 O\ufb00er high performance", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 103", "position": 103, "chunk_type": "semantic", "token_estimate": 342}
{"text": "These design goals have been accomplished and maintained, to a degree, as I will attempt to: show by dissecting Git\u2019s use of directed acyclic graphs (DAGs) for content storage, reference pointers\nfor heads, object model representation, and remote protocol; and \ufb01nally how Git tracks the merging\nof trees. Despite BitKeeper in\ufb02uencing the original design of Git, it is implemented in fundamentally\ndi\ufb00erent ways and allows even more distributed plus local-only work\ufb02ows, which were not possible\nwith BitKeeper. Monotone1, an open source distributed VCS started in 2003, was likely another\ninspiration during Git\u2019s early development. Distributed version control systems o\ufb00er great work\ufb02ow \ufb02exibility, often at the expense of\nsimplicity. Speci\ufb01c bene\ufb01ts of a distributed model include:\n\u2022 Providing the ability for collaborators to work o\ufb04ine and commit incrementally. \u2022 Allowing a collaborator to determine when his/her work is ready to share. \u2022 O\ufb00ering the collaborator access to the repository history when o\ufb04ine. \u2022 Allowing the managed work to be published to multiple repositories, potentially with di\ufb00erent\nbranches or granularity of changes visible. Around the time the Git project started, three other open source distributed VCS projects were\ninitiated. (One of them, Mercurial, is discussed in Volume 1 of The Architecture of Open Source\nApplications.) All of these dVCS tools o\ufb00er slightly di\ufb00erent ways to enable highly \ufb02exible work\ufb02ows,\nwhich centralized VCSs before them were not capable of handling directly. Note: Subversion has an\nextension named SVK maintained by di\ufb00erent developers to support server-to-server synchronization. Today popular and actively maintained open source dVCS projects include Bazaar, Darcs, Fossil,\nGit, Mercurial, and Veracity. 6.3\nVersion Control System Design\nNow is a good time to take a step back and look at the alternative VCS solutions to Git. Understanding\ntheir di\ufb00erences will allow us to explore the architectural choices faced while developing Git. A version control system usually has three core functional requirements, namely:\n\u2022 Storing content\n\u2022 Tracking changes to the content (history including merge metadata)\n\u2022 Distributing the content and history with collaborators\nNote: The third requirement above is not a functional requirement for all VCSs. Content Storage\nThe most common design choices for storing content in the VCS world are with a delta-based\nchangeset, or with directed acyclic graph (DAG) content representation. Delta-based changesets encapsulate the di\ufb00erences between two versions of the \ufb02attened content,\nplus some metadata.", "domains": ["Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 104", "position": 104, "chunk_type": "semantic", "token_estimate": 386}
{"text": "These design goals have been accomplished and maintained, to a degree, as I will attempt to: Content Storage\nThe most common design choices for storing content in the VCS world are with a delta-based\nchangeset, or with directed acyclic graph (DAG) content representation. Delta-based changesets encapsulate the di\ufb00erences between two versions of the \ufb02attened content,\nplus some metadata. Representing content as a directed acyclic graph involves objects forming\na hierarchy which mirrors the content\u2019s \ufb01lesystem tree as a snapshot of the commit (reusing the\nunchanged objects inside the tree where possible). Git stores content as a directed acyclic graph\nusing di\ufb00erent types of objects. The \u201cObject Database\u201d section later in this chapter describes the\ndi\ufb00erent types of objects that can form DAGs inside the Git repository. 1http://www.monotone.ca/\n90\nGit", "domains": ["Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 104", "position": 104, "chunk_type": "semantic", "token_estimate": 129}
{"text": "Commit and Merge Histories: On the history and change-tracking front most VCS software uses one of the following approaches:\n\u2022 Linear history\n\u2022 Directed acyclic graph for history\nAgain Git uses a DAG, this time to store its history. Each commit contains metadata about its\nancestors; a commit in Git can have zero or many (theoretically unlimited) parent commits. For\nexample, the \ufb01rst commit in a Git repository would have zero parents, while the result of a three-way\nmerge would have three parents. Another primary di\ufb00erence between Git and Subversion and its linear history ancestors is its\nability to directly support branching that will record most merge history cases. Figure 6.1: Example of a DAG representation in Git\nGit enables full branching capability using directed acyclic graphs to store content. The history\nof a \ufb01le is linked all the way up its directory structure (via nodes representing directories) to the root\ndirectory, which is then linked to a commit node. This commit node, in turn, can have one or more\nparents. This a\ufb00ords Git two properties that allow us to reason about history and content in more\nde\ufb01nite ways than the family of VCSs derived from RCS do, namely:\n\u2022 When a content (i.e., \ufb01le or directory) node in the graph has the same reference identity (the\nSHA in Git) as that in a di\ufb00erent commit, the two nodes are guaranteed to contain the same\ncontent, allowing Git to short-circuit content di\ufb03ng e\ufb03ciently. \u2022 When merging two branches we are merging the content of two nodes in a DAG. The DAG\nallows Git to \"e\ufb03ciently\" (as compared to the RCS family of VCS) determine common\nancestors. Susan Potter\n91", "domains": ["Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 105", "position": 105, "chunk_type": "semantic", "token_estimate": 279}
{"text": "Distribution: VCS solutions have handled content distribution of a working copy to collaborators on a project in\none of three ways:\n\u2022 Local-only: for VCS solutions that do not have the third functional requirement above. \u2022 Central server: where all changes to the repository must transact via one speci\ufb01c repository\nfor it to be recorded in history at all. \u2022 Distributed model: where there will often be publicly accessible repositories for collaborators\nto \"push\" to, but commits can be made locally and pushed to these public nodes later, allowing\no\ufb04ine work. To demonstrate the bene\ufb01ts and limitations of each major design choice, we will consider a\nSubversion repository and a Git repository (on a server), with equivalent content (i.e., the HEAD of\nthe default branch in the Git repository has the same content as the Subversion repository\u2019s latest\nrevision on trunk). A developer, named Alex, has a local checkout of the Subversion repository and\na local clone of the Git repository. Let us say Alex makes a change to a 1 MB \ufb01le in the local Subversion checkout, then commits\nthe change. Locally, the checkout of the \ufb01le mimics the latest change and local metadata is updated. During Alex\u2019s commit in the centralized Subversion repository, a di\ufb00is generated between the\nprevious snapshot of the \ufb01les and the new changes, and this di\ufb00is stored in the repository. Contrast this with the way Git works. When Alex makes the same modi\ufb01cation to the equivalent\n\ufb01le in the local Git clone, the change will be recorded locally \ufb01rst, then Alex can \"push\" the local\npending commits to a public repository so the work can be shared with other collaborators on the\nproject. The content changes are stored identically for each Git repository that the commit exists\nin. Upon the local commit (the simplest case), the local Git repository will create a new object\nrepresenting a \ufb01le for the changed \ufb01le (with all its content inside). For each directory above the\nchanged \ufb01le (plus the repository root directory), a new tree object is created with a new identi\ufb01er. A\nDAG is created starting from the newly created root tree object pointing to blobs (reusing existing\nblob references where the \ufb01les content has not changed in this commit) and referencing the newly\ncreated blob in place of that \ufb01le\u2019s previous blob object in the previous tree hierarchy. (A blob\nrepresents a \ufb01le stored in the repository.)", "domains": ["Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 106", "position": 106, "chunk_type": "semantic", "token_estimate": 401}
{"text": "Distribution: A\nDAG is created starting from the newly created root tree object pointing to blobs (reusing existing\nblob references where the \ufb01les content has not changed in this commit) and referencing the newly\ncreated blob in place of that \ufb01le\u2019s previous blob object in the previous tree hierarchy. (A blob\nrepresents a \ufb01le stored in the repository.) At this point the commit is still local to the current Git clone on Alex\u2019s local device. When Alex\n\"pushes\" the commit to a publicly accessible Git repository this commit gets sent to that repository. After the public repository veri\ufb01es that the commit can apply to the branch, the same objects are\nstored in the public repository as were originally created in the local Git repository. There are a lot more moving parts in the Git scenario, both under the covers and for the user,\nrequiring them to explicitly express intent to share changes with the remote repository separately\nfrom tracking the change as a commit locally. However, both levels of added complexity o\ufb00er the\nteam greater \ufb02exibility in terms of their work\ufb02ow and publishing capabilities, as described in the\n\u201cGit\u2019s Origin\u201d section above. In the Subversion scenario, the collaborator did not have to remember to push to the public\nremote repository when ready for others to view the changes made. When a small modi\ufb01cation\nto a larger \ufb01le is sent to the central Subversion repository the delta stored is much more e\ufb03cient\nthan storing the complete \ufb01le contents for each version. However, as we will see later, there is a\nworkaround for this that Git takes advantage of in certain scenarios. 92\nGit", "domains": ["Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 106", "position": 106, "chunk_type": "semantic", "token_estimate": 272}
{"text": "6.4: The Toolkit\nToday the Git ecosystem includes many command-line and UI tools on a number of operating systems\n(including Windows, which was originally barely supported). Most of these tools are mostly built on\ntop of the Git core toolkit. Due to the way Git was originally written by Linus, and its inception within the Linux community,\nit was written with a toolkit design philosophy very much in the Unix tradition of command line\ntools. The Git toolkit is divided into two parts: the plumbing and the porcelain. The plumbing consists\nof low-level commands that enable basic content tracking and the manipulation of directed acyclic\ngraphs (DAG). The porcelain is the smaller subset of git commands that most Git end users are likely\nto need to use for maintaining repositories and communicating between repositories for collaboration. While the toolkit design has provided enough commands to o\ufb00er \ufb01ne-grained access to function-\nality for many scripters, application developers complained about the lack of a linkable library for\nGit. Since the Git binary calls die(), it is not reentrant and GUIs, web interfaces or longer running\nservices would have to fork/exec a call to the Git binary, which can be slow. Work is being done to improve the situation for application developers; see the \u201cCurrent And\nFuture Work\u201d section for more information. 6.5\nThe Repository, Index and Working Areas\nLet\u2019s get our hands dirty and dive into using Git locally, if only to understand a few fundamental\nconcepts. First to create a new initialized Git repository on our local \ufb01lesystem (using a Unix inspired\noperating system) we can do:\n$ mkdir testgit\n$ cd testgit\n$ git init\nNow we have an empty, but initialized, Git repository sitting in our testgit directory. We can branch,\ncommit, tag and even communicate with other local and remote Git repositories. Even communication\nwith other types of VCS repositories is possible with just a handful of git commands. The git init command creates a .git subdirectory inside of testgit. Let\u2019s have a peek inside it:\ntree .git/\n.git/\n|-- HEAD\n|-- config\n|-- description\n|-- hooks\n|\n|-- applypatch-msg.sample\n|\n|-- commit-msg.sample\n|\n|-- post-commit.sample\n|\n|-- post-receive.sample\n|\n|-- post-update.sample\n|\n|-- pre-applypatch.sample\n|\n|-- pre-commit.sample\n|\n|-- pre-rebase.sample\nSusan Potter\n93", "domains": ["Design Patterns", "Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 107", "position": 107, "chunk_type": "semantic", "token_estimate": 376}
{"text": "|: |-- prepare-commit-msg.sample\n|\n|-- update.sample\n|-- info\n|\n|-- exclude\n|-- objects\n|\n|-- info\n|\n|-- pack\n|-- refs\n|-- heads\n|-- tags\nThe .git directory above is, by default, a subdirectory of the root working directory, testgit. It contains a few di\ufb00erent types of \ufb01les and directories:\n\u2022 Con\ufb01guration: the .git/config, .git/description and .git/info/exclude \ufb01les essen-\ntially help con\ufb01gure the local repository. \u2022 Hooks: the .git/hooks directory contains scripts that can be run on certain lifecycle events\nof the repository. \u2022 Staging Area: the .git/index \ufb01le (which is not yet present in our tree listing above) will\nprovide a staging area for our working directory. \u2022 Object Database: the .git/objects directory is the default Git object database, which\ncontains all content or pointers to local content. All objects are immutable once created. \u2022 References: the .git/refs directory is the default location for storing reference pointers for\nboth local and remote branches, tags and heads. A reference is a pointer to an object, usually\nof type tag or commit. References are managed outside of the Object Database to allow the\nreferences to change where they point to as the repository evolves. Special cases of references\nmay point to other references, e.g. HEAD. The .git directory is the actual repository. The directory that contains the working set of \ufb01les\nis the working directory, which is typically the parent of the .git directory (or repository). If you\nwere creating a Git remote repository that would not have a working directory, you could initialize it\nusing the git init --bare command. This would create just the pared-down repository \ufb01les at the\nroot, instead of creating the repository as a subdirectory under the working tree. Another \ufb01le of great importance is the Git index: .git/index. It provides the staging area\nbetween the local working directory and the local repository. The index is used to stage speci\ufb01c\nchanges within one \ufb01le (or more), to be committed all together. Even if you make changes related to\nvarious types of features, the commits can be made with like changes together, to more logically\ndescribe them in the commit message. To selectively stage speci\ufb01c changes in a \ufb01le or set of \ufb01les\nyou can using git add -p.\nThe Git index, by default, is stored as a single \ufb01le inside the repository directory. The paths to\nthese three areas can be customized using environment variables.", "domains": ["Design Patterns", "Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 108", "position": 108, "chunk_type": "semantic", "token_estimate": 399}
{"text": "|: To selectively stage speci\ufb01c changes in a \ufb01le or set of \ufb01les\nyou can using git add -p.\nThe Git index, by default, is stored as a single \ufb01le inside the repository directory. The paths to\nthese three areas can be customized using environment variables. It is helpful to understand the interactions that take place between these three areas (the repository,\nindex and working areas) during the execution of a few core Git commands:\n\u2022 git checkout [branch]\nThis will move the HEAD reference of the local repository to branch reference path (e.g. refs/heads/master), populate the index with this head data and refresh the working directory\nto represent the tree at that head. \u2022 git add [files]\n94\nGit", "domains": ["Design Patterns", "Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 108", "position": 108, "chunk_type": "semantic", "token_estimate": 120}
{"text": "This will cross reference the checksums of the \ufb01les speci\ufb01ed with the corresponding entries in: the Git index to see if the index for staged \ufb01les needs updating with the working directory\u2019s\nversion. Nothing changes in the Git directory (or repository). Let us explore what this means more concretely by inspecting the contents of \ufb01les under the\n.git directory (or repository). $ GIT_DIR=$PWD/.git\n$ cat $GIT_DIR/HEAD\nref: refs/heads/master\n$ MY_CURRENT_BRANCH=$(cat .git/HEAD | sed \u2019s/ref: //g\u2019)\n$ cat $GIT_DIR/$MY_CURRENT_BRANCH\ncat: .git/refs/heads/master: No such file or directory\nWe get an error because, before making any commits to a Git repository at all, no branches exist\nexcept the default branch in Git which is master, whether it exists yet or not. Now if we make a new commit, the master branch is created by default for this commit. Let us\ndo this (continuing in the same shell, retaining history and context):\n$ git commit -m \"Initial empty commit\" --allow-empty\n$ git branch\n* master\n$ cat $GIT_DIR/$MY_CURRENT_BRANCH\n3bce5b130b17b7ce2f98d17b2998e32b1bc29d68\n$ git cat-file -p $(cat $GIT_DIR/$MY_CURRENT_BRANCH)\nWhat we are starting to see here is the content representation inside Git\u2019s object database. 6.6\nThe Object Database\nGit has four basic primitive objects that every type of content in the local repository is built around. Each object type has the following attributes: type, size and content. The primitive object types are:\n\u2022 Tree: an element in a tree can be another tree or a blob, when representing a content directory. \u2022 Blob: a blob represents a \ufb01le stored in the repository. \u2022 Commit: a commit points to a tree representing the top-level directory for that commit as well\nas parent commits and standard attributes. \u2022 Tag: a tag has a name and points to a commit at the point in the repository history that the tag\nrepresents. All object primitives are referenced by a SHA, a 40-digit object identity, which has the following\nproperties:\n\u2022 If two objects are identical they will have the same SHA. \u2022 if two objects are di\ufb00erent they will have di\ufb00erent SHAs. Susan Potter\n95", "domains": ["Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 109", "position": 109, "chunk_type": "semantic", "token_estimate": 343}
{"text": "Figure 6.2: Git objects: \u2022 If an object was only copied partially or another form of data corruption occurred, recalculating\nthe SHA of the current object will identify such corruption. The \ufb01rst two properties of the SHA, relating to identity of the objects, is most useful in enabling\nGit\u2019s distributed model (the second goal of Git). The latter property enables some safeguards against\ncorruption (the third goal of Git). Despite the desirable results of using DAG-based storage for content storage and merge histories,\nfor many repositories delta storage will be more space-e\ufb03cient than using loose DAG objects. 6.7\nStorage and Compression Techniques\nGit tackles the storage space problem by packing objects in a compressed format, using an index \ufb01le\nwhich points to o\ufb00sets to locate speci\ufb01c objects in the corresponding packed \ufb01le. We can count the number of loose (or unpacked) objects in the local Git repository using git\ncount-objects. Now we can have Git pack loose objects in the object database, remove loose\nobjects already packed, and \ufb01nd redundant pack \ufb01les with Git plumbing commands if desired. The pack \ufb01le format in Git has evolved, with the initial format storing CRC checksums for the\npack \ufb01le and index \ufb01le in the index \ufb01le itself. However, this meant there was the possibility of\nundetectable corruption in the compressed data since the repacking phase did not involve any further\nchecks. Version 2 of the pack \ufb01le format overcomes this problem by including the CRC checksums\n96\nGit", "domains": ["Design Patterns", "Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 110", "position": 110, "chunk_type": "semantic", "token_estimate": 246}
{"text": "Figure 6.3: Diagram of a pack \ufb01le with corresponding index \ufb01le: of each compressed object in the pack index \ufb01le. Version 2 also allows pack\ufb01les larger than 4 GB,\nwhich the initial format did not support. As a way to quickly detect pack \ufb01le corruption the end\nof the pack \ufb01le contains a 20-byte SHA1 sum of the ordered list of all the SHAs in that \ufb01le. The\nemphasis of the newer pack \ufb01le format is on helping ful\ufb01ll Git\u2019s second usability design goal of\nsafeguarding against data corruption. For remote communication Git calculates the commits and content that need to be sent over the\nwire to synchronize repositories (or just a branch), and generates the pack \ufb01le format on the \ufb02y to\nsend back using the desired protocol of the client. 6.8\nMerge Histories\nAs mentioned previously, Git di\ufb00ers fundamentally in merge history approach than the RCS family\nof VCSs. Subversion, for example, represents \ufb01le or tree history in a linear progression; whatever\nhas a higher revision number will supercede anything before it. Branching is not supported directly,\nonly through an unenforced directory structure within the repository. Let us \ufb01rst use an example to show how this can be problematic when maintaining multiple\nbranches of a work. Then we will look at a scenario to show its limitations. When working on a \"branch\" in Subversion at the typical root branches/branch-name, we are\nworking on directory subtree adjacent to the trunk (typically where the live or master equivalent\ncode resides within). Let us say this branch is to represent parallel development of the trunk tree. For example, we might be rewriting a codebase to use a di\ufb00erent database. Part of the way\nthrough our rewrite we wish to merge in upstream changes from another branch subtree (not trunk). We merge in these changes, manually if necessary, and proceed with our rewrite. Later that day\nwe \ufb01nish our database vendor migration code changes on our branches/branch-name branch and\nmerge our changes into trunk. The problem with the way linear-history VCSs like Subversion handle\nthis is that there is no way to know that the changesets from the other branch are now contained\nwithin the trunk. Susan Potter\n97", "domains": ["Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 111", "position": 111, "chunk_type": "semantic", "token_estimate": 367}
{"text": "Figure 6.4: Diagram showing merge history lineage: DAG-based merge history VCSs, like Git, handle this case reasonably well. Assuming the other\nbranch does not contain commits that have not been merged into our database vendor migration\nbranch (say, db-migration in our Git repository), we can determine\u2014from the commit object parent\nrelationships\u2014that a commit on the db-migration branch contained the tip (or HEAD) of the other\nupstream branch. Note that a commit object can have zero or more (bounded by only the abilities\nof the merger) parents. Therefore the merge commit on the db-migration branch knows it merged\nin the current HEAD of the current branch and the HEAD of the other upstream branch through\nthe SHA hashes of the parents. The same is true of the merge commit in the master (the trunk\nequivalent in Git). A question that is hard to answer de\ufb01nitively using DAG-based (and linear-based) merge histories\nis which commits are contained within each branch. For example, in the above scenario we assumed\nwe merged into each branch all the changes from both branches. This may not be the case. For simpler cases Git has the ability to cherry pick commits from other branches in to the current\nbranch, assuming the commit can cleanly be applied to the branch. 6.9\nWhat\u2019s Next? As mentioned previously, Git core as we know it today is based on a toolkit design philosophy\nfrom the Unix world, which is very handy for scripting but less useful for embedding inside or\nlinking with longer running applications or services. While there is Git support in many popular\n98\nGit", "domains": ["Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 112", "position": 112, "chunk_type": "semantic", "token_estimate": 266}
{"text": "Integrated Development Environments today, adding this support and maintaining it has been more: challenging than integrating support for VCSs that provide an easy-to-link-and-share library for\nmultiple platforms. To combat this, Shawn Pearce (of Google\u2019s Open Source Programs O\ufb03ce) spearheaded an\ne\ufb00ort to create a linkable Git library with more permissive licensing that did not inhibit use of the\nlibrary. This was called libgit22. It did not \ufb01nd much traction until a student named Vincent Marti\nchose it for his Google Summer of Code project last year. Since then Vincent and Github engineers\nhave continued contributing to the libgit2 project, and created bindings for numerous other popular\nlanguages such as Ruby, Python, PHP, .NET languages, Lua, and Objective-C.\nShawn Pearce also started a BSD-licensed pure Java library called JGit that supports many\ncommon operations on Git repositories3. It is now maintained by the Eclipse Foundation for use in\nthe Eclipse IDE Git integration. Other interesting and experimental open source endeavours outside of the Git core project are a\nnumber of implementations using alternative datastores as backends for the Git object database such\nas:\n\u2022 jgit_cassandra4, which o\ufb00ers Git object persistence using Apache Cassandra, a hybrid datastore\nusing Dynamo-style distribution with BigTable column family data model semantics. \u2022 jgit_hbase5, which enables read and write operations to Git objects stored in HBase, a dis-\ntributed key-value datastore. \u2022 libgit2-backends6, which emerged from the libgit2 e\ufb00ort to create Git object database backends\nfor multiple popular datastores such as Memcached, Redis, SQLite, and MySQL. All of these open source projects are maintained independently of the Git core project. As you can see, today there are a large number of ways to use the Git format. The face of Git is\nno longer just the toolkit command line interface of the Git Core project; rather it is the repository\nformat and protocol to share between repositories. As of this writing, most of these projects, according to their developers, have not reached a stable\nrelease, so work in the area still needs to be done but the future of Git appears bright. 6.10\nLessons Learned\nIn software, every design decision is ultimately a trade-o\ufb00. As a power user of Git for version control\nand as someone who has developed software around the Git object database model, I have a deep\nfondness for Git in its present form.", "domains": ["Design Patterns", "Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 113", "position": 113, "chunk_type": "semantic", "token_estimate": 390}
{"text": "Integrated Development Environments today, adding this support and maintaining it has been more: 6.10\nLessons Learned\nIn software, every design decision is ultimately a trade-o\ufb00. As a power user of Git for version control\nand as someone who has developed software around the Git object database model, I have a deep\nfondness for Git in its present form. Therefore, these lessons learned are more of a re\ufb02ection of\ncommon recurring complaints about Git that are due to design decisions and focus of the Git core\ndevelopers. One of the most common complaints by developers and managers who evaluate Git has been the\nlack of IDE integration on par with other VCS tools. The toolkit design of Git has made this more\nchallenging than integrating other modern VCS tools into IDEs and related tools. Earlier in Git\u2019s history some of the commands were implemented as shell scripts. These shell\nscript command implementations made Git less portable, especially to Windows. I am sure the\nGit core developers did not lose sleep over this fact, but it has negatively impacted adoption of\n2https://github.com/libgit2/libgit2\n3https://github.com/eclipse/jgit\n4https://github.com/spearce/jgit_cassandra\n5https://github.com/spearce/jgit_hbase\n6https://github.com/libgit2/libgit2-backends\nSusan Potter\n99", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 113", "position": 113, "chunk_type": "semantic", "token_estimate": 187}
{"text": "Git in larger organizations due to portability issues that were prevalent in the early days of Git\u2019s: development. Today a project named Git for Windows has been started by volunteers to ensure new\nversions of Git are ported to Windows in a timely manner. An indirect consequence of designing Git around a toolkit design with a lot of plumbing com-\nmands is that new users get lost quickly; from confusion about all the available subcommands to not\nbeing able to understand error messages because a low level plumbing task failed, there are many\nplaces for new users to go astray. This has made adopting Git harder for some developer teams. Even with these complaints about Git, I am excited about the possibilities of future development\non the Git Core project, plus all the related open source projects that have been launched from it. 100\nGit", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 114", "position": 114, "chunk_type": "semantic", "token_estimate": 145}
{"text": "If applications had to handle all this complexity themselves the result would be huge amounts of: brittle and duplicative code, leading to high rates of user-visible defects and constant problems as\nhardware gradually mutated out from under the applications. GPSD isolates location-aware applications from hardware interface details by knowing about\nall the protocols itself (at time of writing we support about 20 di\ufb00erent ones), managing serial and\nUSB devices so the applications don\u2019t have to, and reporting sensor payload information in a simple\ndevice-independent JSON format. GPSD further simpli\ufb01es life by providing client libraries so client\napplications need not even know about that reporting format. Instead, getting sensor information\nbecomes a simple procedure call. GPSD also supports precision timekeeping; it can act as a time source for ntpd (the Network\nTime Protocol Daemon) if any of its attached sensors have PPS (pulse-per-second) capability. The\nGPSD developers cooperate closely with the ntpd project in improving the network time service. We are presently (mid-2011) working on completing support for the AIS network of marine\nnavigational receivers. In the future, we expect to support new kinds of location-aware sensors\u2014such\nas receivers for second-generation aircraft transponders\u2014as protocol documentation and test devices\nbecome available. To sum up, the single most important theme in GPSD\u2019s design is hiding all the device-dependent\nugliness behind a simple client interface talking to a zero-con\ufb01guration service. 7.2\nThe External View\nThe main program in the GPSD suite is the gpsd service daemon. It can collect the take from a set of\nattached sensor devices over RS232, USB, Bluetooth, TCP/IP, and UDP links. Reports are normally\nshipped to TCP/IP port 2947, but can also go out via a shared-memory or D-BUS interface. The GPSD distribution ships with client libraries for C, C++, and Python. It includes sample\nclients in C, C++, Python, and PHP. A Perl client binding is available via CPAN. These client libraries\nare not merely a convenience for application developers; they save GPSD\u2019s developers headaches\ntoo, by isolating applications from the details of GPSD\u2019s JSON reporting protocol. Thus, the API\nexposed to clients can remain the same even as the protocol grows new features for new sensor types. Other programs in the suite include a utility for low-level device monitoring (gpsmon), a pro\ufb01ler\nthat produces reports on error statistics and device timing (gpsprof), a utility for tweaking device\nsettings (gpsctl), and a program for batch-converting sensor logs into readable JSON (gpsdecode).", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 116", "position": 116, "chunk_type": "semantic", "token_estimate": 403}
{"text": "7.3: The Software Layers\nThere is a lot more going on inside GPSD than the \u201cplug a sensor in and it just works\u201d experience\nmight lead people to assume. gpsd\u2019s internals break naturally into four pieces: the drivers, the packet\nsni\ufb00er, the core library and the multiplexer. We\u2019ll describe these from the bottom up. Multiplexer\nCore library\nPacket Sniffer\nDriver\nDriver\nDriver\nDriver\n. . . sensor\nsensor\nsensor\nsensor\nclient \nsession\nclient \nsession\nclient \nsession\nclient \nsession\nclient \nsession\nFigure 7.1: Software layers\nThe drivers are essentially user-space device drivers for each kind of sensor chipset we support. The key entry points are methods to parse a data packet into time-position-velocity or status informa-\ntion, change its mode or baud rate, probe for device subtype, etc. Auxiliary methods may support\ndriver control operations, such as changing the serial speed of the device. The entire interface to a\ndriver is a C structure full of data and method pointers, deliberately modeled on a Unix device driver\nstructure. The packet sni\ufb00er is responsible for mining data packets out of serial input streams. It\u2019s basically\na state machine that watches for anything that looks like one of our 20 or so known packet types\n(most of which are checksummed, so we can have high con\ufb01dence when we think we have identi\ufb01ed\none). Because devices can hotplug or change modes, the type of packet that will come up the wire\nfrom a serial or USB port isn\u2019t necessarily \ufb01xed forever by the \ufb01rst one recognized. The core library manages a session with a sensor device. The key entry points are:\n\u2022 starting a session by opening the device and reading data from it, hunting through baud rates\nand parity/stopbit combinations until the packet sni\ufb00er achieves synchronization lock with a\nknown packet type;\n\u2022 polling the device for a packet; and\n\u2022 closing the device and wrapping up the session. A key feature of the core library is that it is responsible for switching each GPS connection to\nusing the correct device driver depending on the packet type that the sni\ufb00er returns. This is not\ncon\ufb01gured in advance and may change over time, notably if the device switches between di\ufb00erent\nreporting protocols. (Most GPS chipsets support NMEA and one or more vendor binary protocols,\nand devices like AIS receivers may report packets in two di\ufb00erent protocols on the same wire.) Eric Raymond\n103", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 117", "position": 117, "chunk_type": "semantic", "token_estimate": 399}
{"text": "Finally, the multiplexer is the part of the daemon that handles client sessions and device assign-: ment. It is responsible for passing reports up to clients, accepting client commands, and responding\nto hotplug noti\ufb01cations. It is essentially all contained in one source \ufb01le, gpsd.c, and never talks to\nthe device drivers directly. The \ufb01rst three components (other than the multiplexer) are linked together in a library called\nlibgpsd and can be used separately from the multiplexer. Our other tools that talk to sensors directly,\nsuch as gpsmon and gpsctl, do it by calling into the core library and driver layer directly. The most complex single component is the packet sni\ufb00er at about two thousand lines of code. This is irreducible; a state machine that can recognize as many di\ufb00erent protocols as it does is bound\nto be large and gnarly. Fortunately, the packet sni\ufb00er is also easy to isolate and test; problems in it\ndo not tend to be coupled to other parts of the code. The multiplexer layer is about same size, but somewhat less gnarly. The device drivers make up\nthe bulk of the daemon code at around 15 KLOC. All the rest of the code\u2014all the support tools and\nlibraries and test clients together\u2014adds up to about the size of the daemon (some code, notably the\nJSON parser, is shared between the daemon and the client libraries). The success of this layering approach is demonstrated in a couple of di\ufb00erent ways. One is that\nnew device drivers are so easy to write that several have been contributed by people not on the core\nteam: the driver API is documented, and the individual drivers are coupled to the core library only\nvia pointers in a master device types table. Another bene\ufb01t is that system integrators can drastically reduce GPSD\u2019s footprint for embedded\ndeployment simply by electing not to compile in unused drivers. The daemon is not large to begin\nwith, and a suitably stripped-down build runs quite happily on low-power, low-speed, small-memory\nARM devices1. A third bene\ufb01t of the layering is that the daemon multiplexer can be detached from atop the core\nlibrary and replaced with simpler logic, such as the straight batch conversion of sensor log\ufb01les to\nJSON reports that the gpsdecode utility does. There is nothing novel about this part of the GPSD architecture.", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 118", "position": 118, "chunk_type": "semantic", "token_estimate": 388}
{"text": "When a client issues a watch request, the multiplexer layer opens the navigation sensors in its list: and begins accepting data from them (by adding their \ufb01le descriptors to the set in the main select\ncall). Otherwise all GPS devices are closed (but remain in the list) and the daemon is quiescent. Devices that stop sending data get timed out of the device list. sensor\nPacket \nsniffer\nCore \nlibrary\nDriver\nExporter \nSniffer\nclient \nsession\nFigure 7.2: Data\ufb02ow\nWhen data comes in from a navigation sensor, it\u2019s fed to the packet sni\ufb00er, a \ufb01nite-state machine\nthat works like the lexical analyzer in a compiler. The packet sni\ufb00er\u2019s job is to accumulate data from\neach port (separately), recognizing when it has accumulated a packet of a known type. A packet may contain a position \ufb01x from a GPS, a marine AIS datagram, a sensor reading from a\nmagnetic compass, a DGPS (Di\ufb00erential GPS) broadcast packet, or any of several other things. The\npacket sni\ufb00er doesn\u2019t care about the content of the packet; all it does is tell the core library when it\nhas accumulated one and pass back the payload and the packet type. The core library then hands the packet to the driver associated with its type. The driver\u2019s job is\nto mine data out of the packet payload into a per-device session structure and set some status bits\ntelling the multiplexer layer what kind data it got. One of those bits is an indication that the daemon has accumulated enough data to ship a report\nto its clients. When this bit is raised after a data read from a sensor device, it means we\u2019ve seen the\nend of a packet, the end of a packet group (which may be one or more packets), and the data in the\ndevice\u2019s session structure should be passed to one of the exporters. The main exporter is the \u201csocket\u201d one; it generates a report object in JSON and ships it to\nall the clients watching the device. There\u2019s a shared-memory exporter that copies the data to a\nshared-memory segment instead. In either of these cases, it is expected that a client library will\nunmarshal the data into a structure in the client program\u2019s memory space. A third exporter, which\nships position updates via DBUS, is also available. The GPSD code is as carefully partitioned horizontally as it vertically.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 119", "position": 119, "chunk_type": "semantic", "token_estimate": 394}
{"text": "7.5: Defending the Architecture\nAs an open source program like gpsd evolves, one of the recurring themes is that each contributor\nwill do things to solve his or her particular problem case which gradually leak more information\nbetween layers or stages that were originally designed with clean separation. One that we\u2019re concerned about at the time of writing is that some information about input source\ntype (USB, RS232, pty, Bluetooth, TCP, UDP) seems to need to be passed up to the multiplexer\nlayer, to tell it, for example, whether probe strings should be sent to an unidenti\ufb01ed device. Such\nprobes are sometimes required to wake up RS232C sensors, but there are good reasons not to ship\nthem to any more devices than we have to. Many GPSs and other sensor devices are designed on low\nbudgets and in a hurry; some can be confused to the point of catatonia by unexpected control strings. For a similar reason, the daemon has a -b option that prevents it from attempting baud-rate\nchanges during the packet-sni\ufb00er hunt loop. Some poorly made Bluetooth devices handle these so\npoorly that they have to be power-cycled to function again; in one extreme case a user actually had to\nunsolder the backup battery to unwedge his! Both these cases are necessary exceptions to the project\u2019s design rules. Much more usually,\nthough, such exceptions are a bad thing. For example, we\u2019ve had some patches contributed to make\nPPS time service work better that messed up the vertical layering, making it impossible for PPS to\nwork properly with more than the one driver they were intended to help. We rejected these in favor\nof working harder at device-type-independent improvement. On one occasion some years ago, we had a request to support a GPS with the odd property that\nthe checksums in its NMEA packets may be invalid when the device doesn\u2019t have a location \ufb01x. To support this device, we would have had to either (a) give up on validating the checksum on any\nincoming data that looked like an NMEA packet, risking that the packet-sni\ufb00er would hand garbage\nto the NMEA driver, or (b) add a command-line option to force the sensor type. The project lead (the author of this chapter) refused to do either. Giving up on NMEA packet\nvalidation was an obvious bad idea.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 120", "position": 120, "chunk_type": "semantic", "token_estimate": 388}
{"text": "The GPSD distribution supplies these scripts for Linux. The result is that end users can plug a: USB GPS into their laptop and expect it to immediately begin supplying reports that location-aware\napplications can read\u2014no muss, no fuss, and no editing a dot\ufb01le or preferences registry. The bene\ufb01ts of this ripple all the way up the application stack. Among other things, it means that\nlocation-aware applications don\u2019t have to have a con\ufb01guration panel dedicated to tweaking the GPS\nand port settings until the whole mess works. This saves a lot of e\ufb00ort for application writers as well\nas users: they get to treat location as a service that is nearly as simple as the system clock. One consequence of the zero-con\ufb01guration philosophy is that we do not look favorably on\nproposals to add a con\ufb01g \ufb01le or additional command-line options. The trouble with this is that\ncon\ufb01guration which can be edited, must be edited. This implies adding setup hassle for end users,\nwhich is precisely what a well-designed service daemon should avoid. The GPSD developers are Unix hackers working from deep inside the Unix tradition, in which\ncon\ufb01gurability and having lots of knobs is close to being a religion. Nevertheless, we think open\nsource projects could be trying a lot harder to throw away their dot\ufb01les and autocon\ufb01gure to what\nthe running environment is actually doing. 7.7\nEmbedded Constraints Considered Helpful\nDesigning for embedded deployment has been a major goal of GPSD since 2005. This was originally\nbecause we got a lot of interest from system integrators working with single-board computers, but\nit has since paid o\ufb00in an unexpected way: deployment on GPS-enabled smartphones. (Our very\nfavorite embedded-deployment reports are still the ones from the robot submarines, though.) Designing for embedded deployment has in\ufb02uenced GPSD in important ways. We think a lot\nabout ways to keep memory footprint and CPU usage low so the code will run well on low-speed,\nsmall-memory, power-constrained systems. One important attack on this issue, as previously mentioned, is to ensure that gpsd builds don\u2019t\nhave to carry any deadweight over the speci\ufb01c set of sensor protocols that a system integrator needs\nto support. In June 2011 a minimum static build of gpsd on an x86 system has a memory footprint\nof about 69K (that is with all required standard C libraries linked in) on 64-bit x86. For comparison,\nthe static build with all drivers is about 418K.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 121", "position": 121, "chunk_type": "semantic", "token_estimate": 402}
{"text": "The GPSD distribution supplies these scripts for Linux. The result is that end users can plug a: In June 2011 a minimum static build of gpsd on an x86 system has a memory footprint\nof about 69K (that is with all required standard C libraries linked in) on 64-bit x86. For comparison,\nthe static build with all drivers is about 418K. Another is that we pro\ufb01le for CPU hotspots with a slightly di\ufb00erent emphasis than most projects. Because location sensors tend to report only small amounts of data at intervals on the order of 1\nsecond, performance in the normal sense isn\u2019t a GPSD issue\u2014even grossly ine\ufb03cient code would\nbe unlikely to introduce enough latency to be visible at the application level. Instead, our focus is\non decreasing processor usage and power consumption. We\u2019ve been quite successful at this: even\non low-power ARM systems without an FPU, gpsd\u2019s fraction of CPU is down around the level of\npro\ufb01ler noise. While designing the core code for low footprint and good power e\ufb03ciency is at this point largely\na solved problem, there is one respect in which targeting embedded deployments still produces\ntension in the GPSD architecture: use of scripting languages. On the one hand, we want to minimize\ndefects due to low-level resource management by moving as much code as possible out of C. On the\nother hand, Python (our preferred scripting language) is simply too heavyweight and slow for most\nembedded deployments. We\u2019ve split the di\ufb00erence in the obvious way: the gpsd service daemon is C, while the test\nframework and several of the support utilities are written in Python. Over time, we hope to migrate\nEric Raymond\n107", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 121", "position": 121, "chunk_type": "semantic", "token_estimate": 279}
{"text": "navigation software has a tendency to try to evade this problem by shipping with disclaimers that: say, \u201cDon\u2019t rely on this if doing so might put lives at risk.\u201d\nWe think such disclaimers are futile and dangerous: futile because system integrators are quite\nlikely to treat them as pro-forma and ignore them, and dangerous because they encourage developers\nto fool themselves that code defects won\u2019t have serious consequences, and that cutting corners in\nquality assurance is acceptable. The GPSD project developers believe that the only acceptable policy is to design for zero defects. Software complexity being what it is, we have not quite achieved this\u2014but for a project GPSD\u2019s size\nand age and complexity we come very close. Our strategy for doing this is a combination of architecture and coding policies that aim to exclude\nthe possibility of defects in shipped code. One important policy is this: the gpsd daemon never uses dynamic storage allocation\u2014no malloc\nor calloc, and no calls to any functions or libraries that require it. At a stroke this banishes the\nsingle most notorious defect attractor in C coding. We have no memory leaks and no double-malloc\nor double-free bugs, and we never will. We get away with this because all of the sensors we handle emit packets with relatively small\n\ufb01xed maximum lengths, and the daemon\u2019s job is to digest them and ship them to clients with minimal\nbu\ufb00ering. Still, banishing malloc requires coding discipline and some design compromises, a few\nof which we previously noted in discussing the JSON parser. We pay these costs willingly to reduce\nour defect rate. A useful side e\ufb00ect of this policy is that it increases the e\ufb00ectiveness of static code checkers such\nas splint, cppcheck, and Coverity. This feeds into another major policy choice; we make extremely\nheavy use of both these code-auditing tools and a custom framework for regression testing. (We\ndo not know of any program suite larger than GPSD that is fully splint-annotated, and strongly\nsuspect that none such yet exist.) The highly modular architecture of GPSD aids us here as well. The module boundaries serve as\ncut points where we can rig test harnesses, and we have very systematically done so. Our normal\nregression test checks everything from the \ufb02oating-point behavior of the host hardware up through\nJSON parsing to correct reporting behavior on over seventy di\ufb00erent sensor logs.", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 123", "position": 123, "chunk_type": "semantic", "token_estimate": 395}
{"text": "navigation software has a tendency to try to evade this problem by shipping with disclaimers that: The module boundaries serve as\ncut points where we can rig test harnesses, and we have very systematically done so. Our normal\nregression test checks everything from the \ufb02oating-point behavior of the host hardware up through\nJSON parsing to correct reporting behavior on over seventy di\ufb00erent sensor logs. Admittedly, we have a slightly easier time being rigorous than many applications would because\nthe daemon has no user-facing interfaces; the environment around it is just a bunch of serial data\nstreams and is relatively easy to simulate. Still, as with banishing malloc, actually exploiting that\nadvantage requires the right attitude, which very speci\ufb01cally means being willing to spend as much\ndesign and coding time on test tools and harnesses as we do on the production code. This is a policy\nwe think other open-source projects can and should emulate. As I write (July 2011), GPSD\u2019s project bug tracker is empty. It has been empty for weeks, and\nbased on past rates of bug submissions we can expect it to stay that way for a good many more. We\nhaven\u2019t shipped code with a crash bug in six years. When we do have bugs, they tend to be the sort of\nminor missing feature or mismatch with speci\ufb01cation that is readily \ufb01xed in a few minutes of work. This is not to say that the project has been an uninterrupted idyll. Next, we\u2019ll review some of our\nmistakes...\n7.10\nLessons Learned\nSoftware design is di\ufb03cult; mistakes and blind alleys are all too normal a part of it, and GPSD has\nbeen no exception to that rule. The largest mistake in this project\u2019s history was the design of the\nEric Raymond\n109", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 123", "position": 123, "chunk_type": "semantic", "token_estimate": 295}
{"text": "original pre-JSON protocol for requesting and reporting GPS information. Recovering from it took: years of e\ufb00ort, and there are lessons in both the original mis-design and the recovery. There were two serious problems with the original protocol:\n1. Poor extensibility. It used requests and response tags consisting of a single letter each, case-\ninsensitive. Thus, for example, the request to report longitude and latitude was \"P\" and a\nresponse looked like \"P -75.32 40.05\". Furthermore, the parser interpreted a request like\n\"PA\" as a \"P\" request followed by an \"A\" (altitude) request. As the daemon\u2019s capabilities\ngradually broadened, we literally ran out of command space. 2. A mismatch between the protocol\u2019s implicit model of sensor behavior and how they actually\nbehave. The old protocol was request/response: send a request for position (or altitude, or\nwhatever) get back a report sometime later. In reality, it is usually not possible to request a\nreport from a GPS or other navigation-related sensors; they stream out reports, and the best\na request can do is query a cache. This mismatch encouraged sloppy data-handling from\napplications; too often, they would ask for location data without also requesting a timestamp\nor any check information about the \ufb01x quality, a practice which could easily result in stale or\ninvalid data getting presented to the user. It became clear as early as 2006 that the old protocol design was inadequate, but it took nearly\nthree years of design sketches and false starts to design a new one. The transition took two years\nafter that, and caused some pain for developers of client applications. It would have cost a lot more\nif the project had not shipped client-side libraries that insulated users from most of the protocol\ndetails\u2014but we didn\u2019t get the API of those libraries quite right either at \ufb01rst. If we had known then what we know now, the JSON-based protocol would have been introduced\n\ufb01ve years sooner, and the API design of the client libraries would have required many fewer revisions. But there are some kinds of lessons only experience and experiment can teach. There are at least two design guidelines that future service daemons could bear in mind to avoid\nreplicating our mistakes:\n1. Design for extensibility. If your daemon\u2019s application protocol can run out of namespace\nthe way our old one did, you\u2019re doing it wrong.", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 124", "position": 124, "chunk_type": "semantic", "token_estimate": 392}
{"text": "original pre-JSON protocol for requesting and reporting GPS information. Recovering from it took: Design for extensibility. If your daemon\u2019s application protocol can run out of namespace\nthe way our old one did, you\u2019re doing it wrong. Overestimating the short-term costs and\nunderestimating the long-term bene\ufb01ts of metaprotocols like XML and JSON is an error\nthat\u2019s all too common. 2. Client-side libraries are a better idea than exposing the application protocol details. A library\nmay be able to adapt its internals to multiple versions of the application protocol, substantially\nreducing both interface complexity and defect rates compared to the alternative, in which\neach application writer needs to develop an ad hoc binding. This di\ufb00erence will translate\ndirectly into fewer bug reports on your project\u2019s tracker. One possible reply to our emphasis on extensibility, not just in GPSD\u2019s application protocol\nbut in other aspects of the project architecture like the packet-driver interface, is to dismiss it as an\nover-elaboration brought about by mission creep. Unix programmers schooled in the tradition of\n\u201cdo one thing well\u201d may ask whether gpsd\u2019s command set really needs to be larger in 2011 than it\nwas in 2006, why gpsd now handles non-GPS sensors like magnetic compasses and Marine AIS\nreceivers, and why we contemplate possibilities like ADS-B aircraft tracking. These are fair questions. We can approach an answer by looking at the actual complexity cost\nof adding a new device type. For very good reasons, including relatively low data volumes and the\nhigh electrical-noise levels historically associated with serial wires to sensors, almost all reporting\nprotocols for GPSs and other navigation-related sensors look broadly similar: small packets with\na validation checksum of some sort. Such protocols are \ufb01ddly to handle but not really di\ufb03cult to\n110", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 124", "position": 124, "chunk_type": "semantic", "token_estimate": 290}
{"text": "distinguish from each other and parse, and the incremental cost of adding a new one tends to be: less than a KLOC each. Even the most complex of our supported protocols with their own report\ngenerators attached, such as Marine AIS, only cost on the order of 3 KLOC each. In aggregate, the\ndrivers plus the packet-sni\ufb00er and their associated JSON report generators are about 18 KLOC total. Comparing this with 43 KLOC for the project as a whole, we see that most of the complexity\ncost of GPSD is actually in the framework code around the drivers\u2014and (importantly) in the test\ntools and framework for verifying the daemon\u2019s correctness. Duplicating these would be a much\nlarger project than writing any individual packet parser. So writing a GPSD-equivalent for a packet\nprotocol that GPSD doesn\u2019t handle would be a great deal more work than adding another driver\nand test set to GPSD itself. Conversely, the most economical outcome (and the one with the lowest\nexpected cumulative rate of defects) is for GPSD to grow packet drivers for many di\ufb00erent sensor\ntypes. The \u201cone thing\u201d that GPSD has evolved to do well is handle any collection of sensors that ship\ndistinguishable checksummed packets. What looks like mission creep is actually preventing many\ndi\ufb00erent and duplicative handler daemons from having to be written. Instead, application developers\nget one relatively simple API and the bene\ufb01t of our hard-won expertise at design and testing across\nan increasing range of sensor types. What distinguishes GPSD from a mere mission-creepy pile of features is not luck or black magic\nbut careful application of known best practices in software engineering. The payo\ufb00from these begins\nwith a low defect rate in the present, and continues with the ability to support new features with little\ne\ufb00ort or expected impact on defect rates in the future. Perhaps the most important lesson we have for other open-source projects is this: reducing\ndefect rates asymptotically close to zero is di\ufb03cult, but it\u2019s not impossible\u2014not even for a project as\nwidely and variously deployed as GPSD is. Sound architecture, good coding practice, and a really\ndetermined focus on testing can achieve it\u2014and the most important prerequisite is the discipline to\npursue all three. 111", "domains": ["Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 125", "position": 125, "chunk_type": "semantic", "token_estimate": 371}
{"text": "[chapter8]: The Dynamic Language Runtime and the\nIron Languages\nJe\ufb00Hardy\nThe Iron languages are an informal group of language implementations with \u201cIron\u201d in their names, in\nhonour of the \ufb01rst one, IronPython. All of these languages have at least one thing in common\u2014they\nare dynamic languages that target the Common Language Runtime (CLR), which is more commonly\nknown as the .NET Framework1, and they are built on top of the Dynamic Language Runtime (DLR). The DLR is a set of libraries for the CLR that provide much better support for dynamic languages on\nthe CLR. IronPython and IronRuby are both used in a few dozen closed and open source projects,\nand are both under active development; the DLR, which started as an open-source project, is included\nas part of the .NET Framework and Mono. Architecturally, IronPython, IronRuby, and the DLR are both simple and devilishly complex. From a high level, the designs are similar to many other language implementations, with parsers and\ncompilers and code generators; however, look a little closer and the interesting details begin to emerge:\ncall sites, binders, adaptive compilation, and other techniques are used to make dynamic languages\nperform nearly as fast as static languages on a platform that was designed for static languages. 8.1\nHistory\nThe history of the Iron languages begins in 2003. Jim Hugunin had already written an implementation\nof Python, called Jython, for the Java Virtual Machine (JVM). At the time, the then-new .NET\nFramework Common Language Runtime (CLR) was considered by some (exactly who, I\u2019m not\nsure) to be poorly suited for implementing dynamic languages such as Python. Having already\nimplemented Python on the JVM, Jim was curious as to how Microsoft could have made .NET so\nmuch worse than Java. In a September 2006 blog post2, he wrote:\nI wanted to understand how Microsoft could have screwed up so badly that the CLR\nwas a worse platform for dynamic languages than the JVM. My plan was to take a couple\nof weeks to build a prototype implementation of Python on the CLR and then to use\nthat work to write a short pithy article called, \u201cWhy the CLR is a terrible platform for\ndynamic languages\u201d. My plans quickly changed as I worked on the prototype, because I\n1\u201cCLR\u201d is the generic term; the .NET Framework is Microsoft\u2019s implementation, and there is also the open-source Mono\nimplementation. 2http://blogs.msdn.com/b/hugunin/archive/2006/09/05/741605.aspx", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 127", "position": 127, "chunk_type": "semantic", "token_estimate": 397}
{"text": "to be generated at runtime and then executed with only sightly more overhead than a static method: call7. The result of the DLR design is that languages like IronPython and IronRuby can call each\nother\u2019s objects (and those of any other DLR language), because they have a common dynamic object\nmodel. Support for this object model was also added to C# 4 (with the dynamic keyword) and\nVisual Basic 10 (in addition to VB\u2019s existing method of \u201clate binding\u201d) so that they can perform\ndynamic calls on objects as well. The DLR thus makes dynamic languages \ufb01rst-class citizens on\n.NET. Interestingly, the DLR is entirely implemented as a set of libraries and can be built and run on\n.NET 2.0 as well. No changes to the CLR are required to implement it. 8.3\nLanguage Implementation Details\nEvery language implementation has two basic stages\u2014parsing (the front end) and code generation\n(the backend). In the DLR, each language implements its own front end, which contains the language\nparser and syntax tree generator; the DLR provides a common backend that takes expression trees\nto produce Intermediate Language (IL) for the CLR to consume; the CLR will pass the IL to a\nJust-In-Time (JIT) compiler, which produces machine code to run on the processor. Code that is\nde\ufb01ned at runtime (and run using eval) is handled similarly, except that everything happens at the\neval call site instead of when the \ufb01le is loaded. There are a few di\ufb00erent way to implement the key pieces of a language front end, and while\nIronPython and IronRuby are very similar (they were developed side-by-side, after all) they di\ufb00er\nin a few key areas. Both IronPython and IronRuby have fairly standard parser designs\u2014both use\na tokenizer (also known as a lexer) to split the text into tokens, and then the parser turns those\ntokens into an abstract syntax tree (AST) that represents the program. However, the languages have\ncompletely di\ufb00erent implementations of these pieces. 8.4\nParsing\nIronPython\u2019s tokenizer is in the IronPython.Compiler.Tokenizer class and the parser is in the\nIronPython.Compiler.Parser class. The tokenizer is a hand-written state machine that recognizes\nPython keywords, operators, and names and produces the corresponding tokens. Each token also\ncarries with it any additional information (such as the value of a constant or name), as well as where\nin the source the token was found, to aid in debugging.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 129", "position": 129, "chunk_type": "semantic", "token_estimate": 395}
{"text": "to be generated at runtime and then executed with only sightly more overhead than a static method: The tokenizer is a hand-written state machine that recognizes\nPython keywords, operators, and names and produces the corresponding tokens. Each token also\ncarries with it any additional information (such as the value of a constant or name), as well as where\nin the source the token was found, to aid in debugging. The parser then takes this set of tokens and\ncompares them to the Python grammar to see if it matches legal Python constructs. IronPython\u2019s parser is an LL(1) recursive descent parser. The parser will look at the incoming\ntoken, call a function if the token is allowed and return an error if it is not. A recursive descent\nparser is built from a set of mutually recursive functions; these functions ultimately implement a\nstate machine, with each new token triggering a state transition. Like the tokenizer, IronPython\u2019s\nparser is written by hand. IronRuby, on the other hand, has a tokenizer and parser generated by the Gardens Point Parser\nGenerator (GPPG). The parser is is described in the Parser.y \ufb01le8, which is a yacc-format \ufb01le that\ndescribes the grammar of IronRuby at a high level using rules that describe the grammar. GPPG then\n7The JVM acquired a similar mechanism with invokedynamic in Java 7. 8Languages/Ruby/Ruby/Compiler/Parser/Parser.y\nJe\ufb00Hardy\n115", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 129", "position": 129, "chunk_type": "semantic", "token_estimate": 225}
{"text": "takes Parser.y and creates the actual parser functions and tables; the result is a table-based LALR(1): parser. The generated tables are long arrays of integers, where each integer represents a state; based\non the current state and the current token, the tables determine which state should be transitioned to\nnext. While IronPython\u2019s recursive descent parser is quite easy to read, IronRuby\u2019s generated parser\nis not. The transition table is enormous (540 distinct states and over 45,000 transitions) and it is next\nto impossible to modify it by hand. Ultimately, this is an engineering tradeo\ufb00\u2014IronPython\u2019s parser is simple enough to modify by\nhand, but complex enough that it obscures the structure of the language. The IronRuby parser, on the\nother hand, makes it much easier to understand the structure of the language in the Parser.y \ufb01le,\nbut it is now dependent on a third-party tool that uses a custom (albeit well-known) domain-speci\ufb01c\nlanguage and may have its own bugs or quirks. In this case, the IronPython team didn\u2019t want to\ncommit to a dependency on an external tool, while the IronRuby team didn\u2019t mind. What is clear, however, is how important state machines are to parsing, at every phase. For any\nparsing task, no matter how simple, a state machine is always the right answer. The output of the parser for either language is an abstract syntax tree (AST). This describes the\nstructure of the program at a high level, with each node mapping directly to a language construct\u2014a\nstatement or expression. These trees can be manipulated at runtime, often to make optimizations to\nthe program before compilation. However, a language\u2019s AST is tied to the language; the DLR needs\nto operate on trees that do not contain any language-speci\ufb01c constructs, only general ones. 8.5\nExpression Trees\nAn expression tree is also a representation of a program that can be manipulated at\nruntime, but in a lower-level, language-independent form. In .NET, the node types are in the\nSystem.Linq.Expressions namespace9, and all of the node types are derived from the abstract\nExpression class. These expression trees cover more than just expressions, however, as there are\nnode types for if statements, try blocks, and loops as well; in some languages (Ruby, for one) these\nare expressions and not statements. There are nodes to cover almost every feature a programming language could want.", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 130", "position": 130, "chunk_type": "semantic", "token_estimate": 389}
{"text": "takes Parser.y and creates the actual parser functions and tables; the result is a table-based LALR(1): These expression trees cover more than just expressions, however, as there are\nnode types for if statements, try blocks, and loops as well; in some languages (Ruby, for one) these\nare expressions and not statements. There are nodes to cover almost every feature a programming language could want. However,\nthey tend to be de\ufb01ned at a fairly low level\u2014instead of having ForExpression, WhileExpression,\netc., there is a single LoopExpression which, when combined with a GotoExpression, can describe\nany type of loop. To describe a language at a higher level, languages can de\ufb01ne their own node\ntypes by deriving from Expression and overriding the Reduce() method, which returns another\nexpression tree. In IronPython, the parse tree is also a DLR expression tree, but it contains many\ncustom nodes that the DLR would not normally understand (such as ForStatement). These custom\nnodes can be reduced to expression trees that the DLR does understand (such as a combination of\nLoopExpressions and GotoExpressions). A custom expression node can reduce to other custom\nexpression nodes, so the reduction proceeds recursively until only the intrinsic DLR nodes remain. One key di\ufb00erence between IronPython and IronRuby is that while IronPython\u2019s AST is also an\nexpression tree, IronRuby\u2019s is not. Instead, IronRuby\u2019s AST is transformed into an expression tree\nbefore moving onto the next stage. It\u2019s arguable whether having the AST also be an expression tree\nis actually useful, so IronRuby did not implement it that way. Each node type knows how to reduce itself, and it can usually only be reduced in one way. For\ntransformations that come from code outside the tree\u2014optimizations such as constant folding, for ex-\n9The namespace is a historical artifact; expression trees were originally added in .NET 3.5 to implement LINQ\u2014Language\nIntegrated Query\u2014and the DLR expression trees extended that. 116\nThe Dynamic Language Runtime and the Iron Languages", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 130", "position": 130, "chunk_type": "semantic", "token_estimate": 322}
{"text": "ample, or IronPython\u2019s implementation of Python generators\u2014a subclass of the ExpressionVisitor: class is used. ExpressionVisitor has a Visit() method that calls the Accept() method on\nExpression, and subclasses of Expression override Accept() to call a speci\ufb01c Visit() method\non ExpressionVisitor, such as VisitBinary(). This is a textbook implementation of the Visitor\npattern from Gamma et al.\u2014there\u2019s a \ufb01xed set of node types to visit, and an in\ufb01nite number of\noperations that could be performed upon them. When the expression visitor visits a node, it usu-\nally recursively visits its children as well, and its children, and so on down the tree. However, an\nExpressionVisitor can\u2019t actually modify the expression tree it is visiting, because expression trees\nare immutable. If the expression visitor needs to modify a node (such as removing children), it must\nproduce a new node that replaces the old one instead, and all of its parents as well. Once an expression tree has been created, reduced, and visited, it ultimately needs to be executed. While expression trees can be compiled directly to IL code, IronPython and IronRuby pass them to\nan interpreter \ufb01rst, because compiling directly to IL is expensive for code that may only be executed\na handful of times. 8.6\nInterpreting and Compilation\nOne of the downsides to using a JIT compiler, like .NET does, is that it imposes a time penalty when\nstarting up because it takes time to convert the IL bytecode into machine code that the processor\ncan run. JIT compilation makes the code much faster while running than using an interpreter, but\nthe initial cost can be prohibitive, depending on what is being done. For example, a long-lived\nserver process such as a web application will bene\ufb01t from the JIT because the startup time is mostly\nirrelevant but the per-request time is critical, and it tends to run the same code repeatedly. On the\nother hand, a program that is run often but only for short periods of time, such as the Mercurial\ncommand-line client, would be better o\ufb00with a short startup time because it likely only runs each\nchunk of code once, and the fact that the JIT\u2019d code is faster doesn\u2019t overcome the fact that it takes\nlonger to start running. .NET can\u2019t execute IL code directly; it always gets JIT compiled into machine code, and this takes\ntime.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 131", "position": 131, "chunk_type": "semantic", "token_estimate": 389}
{"text": "ample, or IronPython\u2019s implementation of Python generators\u2014a subclass of the ExpressionVisitor: On the\nother hand, a program that is run often but only for short periods of time, such as the Mercurial\ncommand-line client, would be better o\ufb00with a short startup time because it likely only runs each\nchunk of code once, and the fact that the JIT\u2019d code is faster doesn\u2019t overcome the fact that it takes\nlonger to start running. .NET can\u2019t execute IL code directly; it always gets JIT compiled into machine code, and this takes\ntime. In particular, program startup times are one of the weak spots of the .NET Framework because\nmuch of the code needs to be JIT compiled. While there are ways to avoid the JIT penalty in static .NET\nprograms10, they don\u2019t work for dynamic programs. Rather than always compile directly to IL, Iron-\nRuby and IronPython will use their own interpreter (found in Microsoft.Scripting.Interpreter)\nthat isn\u2019t as fast as JIT-compiled code but takes much less time to get started. The interpreter is also\nuseful in situations where dynamic code generation is not allowed, such as on mobile platforms;\notherwise the DLR languages would not be able to run at all. Before execution, the entire expression tree must be wrapped in a function so that it can be\nexecuted. In the DLR, functions are represented as LambdaExpression nodes. While in most\nlanguages a lambda is an anonymous function, the DLR has no concept of names; all functions are\nanonymous. The LambdaExpression is unique in that it is the only node type that can be converted\nto a delegate, which is what .NET calls \ufb01rst-class functions, using its Compile() method. A delegate\nis similar to a C function pointer\u2014it is simply a handle to a piece of code that can be called. Initially, the expression tree is wrapped in a LightLambdaExpression, which can also produce\na delegate that can be executed, but rather than generate IL code (which would then invoke the\nJIT), it instead compiles the expression tree to a list of instructions that are then executed on the\ninterpreter\u2019s simple VM. The interpreter is a simple stack-based one; instructions pop values o\ufb00\n10Native Image Generation, or NGEN\u2014http://msdn.microsoft.com/en-us/library/6t9t5wcf.aspx. Je\ufb00Hardy\n117", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 131", "position": 131, "chunk_type": "semantic", "token_estimate": 368}
{"text": "of the stack, perform an operation, and then push the result back on the stack. Each instruction is: an instance of a class derived from Microsoft.Scripting.Interpreter.Instruction (such as\nAddInstruction or BranchTrueInstruction) that has properties describing how many items it\ntakes o\ufb00of the stack, how many it will put on, and a Run() method that executes the instruction\nby popping and pushing values on the stack and returning the o\ufb00set of the next instruction. The\ninterpreter takes the list of instructions and executes them one by one, jumping forward or backwards\ndepending on the return value of the Run() method. Once a a piece of code has been executed a certain number of times, it will be converted\nto a full LambdaExpression by calling LightLambdaExpression.Reduce(), then compiled to a\nDynamicMethod delegate (on a background thread for a bit of parallelism), and the old delegate\ncall site will be replaced with the newer, faster one. This greatly reduces the cost of executing\nfunctions that may only be called a few times, such as the main function of a program, while making\ncommonly called functions run as fast as possible. By default, the compilation threshold is set at 32\nexecutions, but this can be changed with a command-line option or by the host program, and can\ninclude disabling either compilation or the interpreter entirely. Whether running through the interpreter or compiled to IL, the language\u2019s operations are not\nhard-coded by the expression tree compiler. Instead, the compiler generates call sites for each\noperation that may be dynamic (which is nearly all of them). These call sites give the objects a\nchance to implement dynamic behaviour while still keeping performance high. 8.7\nDynamic Call Sites\nIn a static .NET language, all of the decisions about what code should be called are made at compile\ntime. For example, consider the following line of C#:\nvar z = x + y;\nThe compiler knows what the types of \u2018x\u2019 and \u2018y\u2019 are and whether or not they can be added. The\ncompiler can emit the proper code for handling overloaded operators, type conversions, or whatever\nelse might be needed to make the code run properly, based solely on the static information it knows\nabout the types involved.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 132", "position": 132, "chunk_type": "semantic", "token_estimate": 371}
{"text": "of the stack, perform an operation, and then push the result back on the stack. Each instruction is: For example, consider the following line of C#:\nvar z = x + y;\nThe compiler knows what the types of \u2018x\u2019 and \u2018y\u2019 are and whether or not they can be added. The\ncompiler can emit the proper code for handling overloaded operators, type conversions, or whatever\nelse might be needed to make the code run properly, based solely on the static information it knows\nabout the types involved. Now, consider the following line of Python code:\nz = x + y\nThe IronPython compiler has no idea what this might do when it encounters it, because it doesn\u2019t\nknow what the types of x and y are11, and even if it did know, the ability of x and y to be added\ncould change at runtime anyway. Instead of emitting the IL code for adding numbers, the IronPython\nemits a call site that will be resolved at runtime. A call site is a placeholder for an operation to be determined at runtime; they are implemented\nas instances of the System.Runtime.CompilerServices.CallSite class. In a dynamic language\nlike Ruby or Python, just about every operation has a dynamic component; these dynamic operations\nare represented in the expression trees as DynamicExpression nodes, which the expression tree\ncompiler knows to convert to a call site. When a call site is created, it is does not yet know how to\nperform the desired operation; however, it is created with an instance of the proper call site binder\nthat is speci\ufb01c to the language in use, and contains all of the necessary information about how to\nperform the operation. 11In principle it could, but neither IronPython nor IronRuby do type inference. 118\nThe Dynamic Language Runtime and the Iron Languages", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 132", "position": 132, "chunk_type": "semantic", "token_estimate": 304}
{"text": "The binder then produces a delegate from the expression trees, which means the rule is compiled: to IL and then to machine code. In the case of adding two numbers, this will likely become a quick\ntype check and then a machine instruction to add the numbers. Even with all of the machinery\ninvolved, the ultimate end result is only marginally slower than static code. IronPython and IronRuby\nalso include a set of precompiled rules for common operations like addition of primitive types, which\nsaves time because they don\u2019t have to be created at runtime, but does cost some extra space on disk. 8.8\nMeta-Object Protocol\nBesides the language infrastructure, the other key part of the DLR is the ability for a language\n(the host language) to make dynamic calls on objects de\ufb01ned in another language (the source\nlanguage). To make this possible, the DLR must be able to understand what operations are valid\non an object, no matter the language it was written in. Python and Ruby have fairly similar object\nmodels, but JavaScript has a radically di\ufb00erent prototype-based (as opposed class-based) type system. Instead of trying to unify the various type systems, the DLR treats them all as if they were based on\nSmalltalk-style message passing. In a message-passing object-oriented system, objects send messages to other objects (with\nparameters, usually), and the object can return another object as a result. Thus, while each language\nhas its own idea of what an object is, they can almost all be made equivalent by viewing method\ncalls as messages that are sent between objects. Of course, even static OO languages \ufb01t this model to\nsome extent; what makes dynamic languages di\ufb00erent is that the method being called does not have\nto be known at compile time, or even exist on the object at all (e.g., Ruby\u2019s method_missing), and\nthe target object usually has a chance to intercept the message and process it di\ufb00erently if necessary\n(e.g., Python\u2019s __getattr__). The DLR de\ufb01nes the following messages:\n\u2022 Get|Set|DeleteMember: operations for manipulating an object\u2019s members\n\u2022 Get|Set|DeleteIndex: operations for indexed objects (such as arrays or dictionaries)\n\u2022 Invoke, InvokeMember: invoke an object or member of an object\n\u2022 CreateInstance: create an instance of an object\n\u2022 Convert: convert an object from one type to another\n\u2022 UnaryOperation, BinaryOperation: perform operator-based operations, such as negate (!) or add (+)\nTaken together, these operations should be su\ufb03cient for implementing just about any language\u2019s\nobject model.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 135", "position": 135, "chunk_type": "semantic", "token_estimate": 409}
{"text": "The binder then produces a delegate from the expression trees, which means the rule is compiled: The DLR de\ufb01nes the following messages:\n\u2022 Get|Set|DeleteMember: operations for manipulating an object\u2019s members\n\u2022 Get|Set|DeleteIndex: operations for indexed objects (such as arrays or dictionaries)\n\u2022 Invoke, InvokeMember: invoke an object or member of an object\n\u2022 CreateInstance: create an instance of an object\n\u2022 Convert: convert an object from one type to another\n\u2022 UnaryOperation, BinaryOperation: perform operator-based operations, such as negate (!) or add (+)\nTaken together, these operations should be su\ufb03cient for implementing just about any language\u2019s\nobject model. Because the CLR is inherently statically typed, dynamic language objects must still be rep-\nresented by static classes. The usual technique is to have a static class such as PythonObject\nand have the actual Python objects be instances of this class or its subclasses. For reasons of\ninteroperability and performance, the DLR\u2019s mechanism is a lot more complicated. Instead of\ndealing with language-speci\ufb01c objects the DLR deals with meta-objects, which are subclasses of\nSystem.Dynamic.DynamicMetaObject and have methods for handling all of the above messages. Each language has its own subclasses of DynamicMetaObject that implement the language\u2019s object\nmodel, such as IronPython\u2019s MetaPythonObject. The meta classes also have corresponding concrete\nclasses that implement the System.Dynamic.IDynamicMetaObjectProtocol interface, which is\nhow the DLR identi\ufb01es dynamic objects. From a class that implements IDynamicMetaObjectProtocol, the DLR can get a Dynamic-\nMetaObject by calling GetMetaObject(). This DynamicMetaObject is provided by the language\nJe\ufb00Hardy\n121", "domains": ["Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 135", "position": 135, "chunk_type": "semantic", "token_estimate": 246}
{"text": "+GetMetaObject(in\t\r \u00a0parameter\t\r \u00a0:\t\r \u00a0Expression)\t\r \u00a0:\t\r \u00a0DynamicMetaObject: \u00abinterface\u00bb\nIDynamicMetaObjectProvider\n+BindInvoke(in\t\r \u00a0binder\t\r \u00a0:\t\r \u00a0InvokeMemberBinder,\t\r \u00a0in\t\r \u00a0args\t\r \u00a0:\t\r \u00a0DynamicMetaObject[])\t\r \u00a0:\t\r \u00a0DynamicMetaObject\n+BindGetMember(in\t\r \u00a0binder\t\r \u00a0:\t\r \u00a0GetMemberBinder,\t\r \u00a0in\t\r \u00a0args\t\r \u00a0:\t\r \u00a0DynamicMetaObject[])\t\r \u00a0:\t\r \u00a0DynamicMetaObject\n-\u00ad\u2010...()\n+Expression\t\r \u00a0:\t\r \u00a0Expression\n+Value\t\r \u00a0:\t\r \u00a0object\n+RunLmeType\t\r \u00a0:\t\r \u00a0Type\n-\u00ad\u2010...\nDynamicMetaObject\n+GetMetaObject(in\t\r \u00a0parameter\t\r \u00a0:\t\r \u00a0Expression)\t\r \u00a0:\t\r \u00a0DynamicMetaObject\nOldInstance\n+BindInvoke(in\t\r \u00a0binder\t\r \u00a0:\t\r \u00a0InvokeMemberBinder,\t\r \u00a0in\t\r \u00a0args\t\r \u00a0:\t\r \u00a0DynamicMetaObject[])\t\r \u00a0:\t\r \u00a0DynamicMetaObject\n+BindGetMember(in\t\r \u00a0binder\t\r \u00a0:\t\r \u00a0GetMemberBinder,\t\r \u00a0in\t\r \u00a0args\t\r \u00a0:\t\r \u00a0DynamicMetaObject[])\t\r \u00a0:\t\r \u00a0DynamicMetaObject\n+...()\nMetaOldInstance\nMetaPythonObject\nFigure 8.3: IDMOP class diagram\nand implements the binding functions as required by that object. Each DynamicMetaObject also\nhas the value and type, if available, of the underlying object. Finally, a DynamicMetaObject stores\nan expression tree representing the call site so far and any restrictions on that expression, similar to\nthe call site binders. When the DLR is compiling a call to a method on a user-de\ufb01ned class, it \ufb01rst creates a call\nsite (i.e., an instance of the CallSite class). The call site initiates the binding process as de-\nscribed above in \u201cDynamic Call Sites\u201d, which results in it eventually calling GetMetaObject()\non an instance of OldInstance13, which returns a MetaOldInstance. Next, a binder is called\n(PythonGetMemberBinder.Bind()) which in turn calls MetaOldInstance.BindGetMember();\nit returns a new DynamicMetaObject that describes how to look up the method name on\nthe object. Then another binder, PythonInvokeBinder.Bind(), is called, which calls\nMetaOldInstance.BindInvoke(), wrapping the \ufb01rst DynamicMetaObject with a new one rep-\nresenting how to call the method that was looked up. This includes the original object, the expression\ntree for looking up the method name, and DynamicMetaObjects representing the arguments to the\nmethod. Once the \ufb01nal DynamicMetaObject in an expression has been built, its expression tree and\n13Python has old-style and new-style classes, but that\u2019s not relevant here. 122\nThe Dynamic Language Runtime and the Iron Languages", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 136", "position": 136, "chunk_type": "semantic", "token_estimate": 294}
{"text": "restrictions are used to build a delegate which is then returned to the call site that initiated the binding.: From there the code can be stored in the call site caches, making operations on objects as fast as\nother dynamic calls, and almost as fast as static calls. Host languages that want to perform dynamic operations on dynamic languages must derive\ntheir binders from DynamicMetaObjectBinder. The DynamicMetaObjectBinder will \ufb01rst ask the\ntarget object to bind the operation (by calling GetMetaObject() and going through the binding\nprocess described above) before falling back on the host language\u2019s binding semantics. As a result,\nif an IronRuby object is accessed from an IronPython program, the binding is \ufb01rst attempted with\nRuby (target language) semantics; if that fails, the DynamicMetaObjectBinder will fall back on\nthe Python (host language) semantics. If the object being bound is not dynamic (i.e., it does not\nimplement IDynamicMetaObjectProvider), such as classes from the .NET base class library, then\nit is accessed with the host language\u2019s semantics using .NET re\ufb02ection. Languages do have some freedom in how they implement this; IronPython\u2019s PythonInvokeBinder\ndoes not derive from InvokeBinder because it needs to do some extra processing speci\ufb01c to Python\nobjects. As long as it only deals with Python objects, there are no issues; if it encounters an ob-\nject that implements IDynamicMetaObjectProvider but is not a Python object, it forwards to a\nCompatibilityInvokeBinder class that does inherit from InvokeBinder and can handle foreign\nobjects correctly. If the fallback cannot bind the operation, it doesn\u2019t throw an exception; instead, it returns a\nDynamicMetaObject representing the error. The host language\u2019s binder will then handle this in an\nappropriate manner for the host language; for example, accessing a missing member on an IronPython\nobject from a hypothetical JavaScript implementation could return undefined, while doing the same\nto a JavaScript object from IronPython would raise an AttributeError. The ability for languages to work with dynamic objects is rather useless without the ability to\n\ufb01rst load and execute code written in other languages, and for this the DLR provides a common\nmechanism for hosting other languages. 8.9\nHosting\nIn addition to providing common language implementation details, the DLR also provides a shared\nhosting interface. The hosting interface is used by the host language (usually a static language like\nC#) to execute code written in another language such as Python or Ruby.", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 137", "position": 137, "chunk_type": "semantic", "token_estimate": 394}
{"text": "restrictions are used to build a delegate which is then returned to the call site that initiated the binding.: 8.9\nHosting\nIn addition to providing common language implementation details, the DLR also provides a shared\nhosting interface. The hosting interface is used by the host language (usually a static language like\nC#) to execute code written in another language such as Python or Ruby. This is a common technique\nthat allows end users to extend an application, and the DLR takes it step further by making it trivial\nto use any scripting language that has a DLR implementation. There are four key parts to the hosting\ninterface: the runtime, engines, sources, and scopes. The ScriptRuntime is generally shared amongst all dynamic languages in an application. The\nruntime handles all of the current assembly references that are presented to the loaded languages,\nprovides methods for quick execution of a \ufb01le, and provides the methods for creating new engines. For simple scripting tasks, the runtime is the only interface that needs to be used, but the DLR also\nprovides classes with more control over how scripts are run. Usually, only one ScriptEngine is used for each scripting language. The DLR\u2019s meta-object pro-\ntocol means that a program can load scripts from multiple languages, and the objects created by each\nlanguage can all seamlessly interoperate. The engine wraps a language-speci\ufb01c LanguageContext\n(such as PythonContext or RubyContext) and is used for executing code from \ufb01les or strings and\nperforming operations on dynamic objects from languages that don\u2019t natively support the DLR (such\nas C# prior to .NET 4). Engines are thread-safe, and can execute multiple scripts in parallel, as long\nJe\ufb00Hardy\n123", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 137", "position": 137, "chunk_type": "semantic", "token_estimate": 278}
{"text": "as each thread has its own scope. It also provides methods for creating script sources, which allow: for more \ufb01ne-grained control of script execution. A ScriptSource holds a chunk of code to be executed; it binds a SourceUnit object, which\nholds the actual code, to the ScriptEngine that created the source. This class allows code to be\ncompiled (which produces a CompiledCode object that can be cached) or executed directly. If a\nchunk of code is going to be executed repeatedly, it\u2019s best to compile \ufb01rst, and then execute the\ncompiled code; for scripts that will only be executed once, it\u2019s best to just execute it directly. Finally, however the code gets to be executed, a ScriptScope must be provided for the code to\nexecute in. The scope is used to hold all of script\u2019s variables, and can be pre-loaded with variables\nfrom the host, if necessary. This allows a host to provide custom objects to the script when it starts\nrunning\u2014for example, an image editor may provide a method to access the pixels of the image\nthe script is working on. Once a script has executed, any variables it created can be read from the\nscope. The other main use of scopes is to provide isolation, so that multiple scripts can be loaded\nand executed at the same time without interfering with each other. It\u2019s important to note that all of these classes are provided by the DLR, not the language; only\nthe LanguageContext used by the engine comes from the language implementation. The language\ncontext provides all of the functionality\u2014loading code, creating scopes, compilation, execution, and\noperations on dynamic objects\u2014that is needed by a host, and the DLR hosting classes provide a\nmore usable interface to access that functionality. Because of this, the same hosting code can be\nused to host any DLR-based language. For dynamic language implementations written in C (such as the original Python and Ruby),\nspecial wrapper code must be written to access code not written in the dynamic language, and\nit must be repeated for each supported scripting language. While software like SWIG exists to\nmake this easier, it\u2019s still not trivial to add a Python or Ruby scripting interface to a program and\nexpose its object model for manipulation by external scripts. For .NET programs, however, adding\nscripting is as simple as setting up a runtime, loading the program\u2019s assemblies into the runtime, and\nusing ScriptScope.SetVariable() to make the program\u2019s objects available to the scripts.", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 138", "position": 138, "chunk_type": "semantic", "token_estimate": 412}
{"text": "as each thread has its own scope. It also provides methods for creating script sources, which allow: While software like SWIG exists to\nmake this easier, it\u2019s still not trivial to add a Python or Ruby scripting interface to a program and\nexpose its object model for manipulation by external scripts. For .NET programs, however, adding\nscripting is as simple as setting up a runtime, loading the program\u2019s assemblies into the runtime, and\nusing ScriptScope.SetVariable() to make the program\u2019s objects available to the scripts. Adding\nsupport for scripting to a .NET application can be done in a matter of minutes, which is a huge bonus\nof the DLR. 8.10\nAssembly Layout\nBecause of how the DLR evolved from a separate library into part of the CLR, there are parts that\nare in the CLR (call sites, expression trees, binders, code generation, and dynamic meta objects) and\nparts that are part of IronLanguages open-source project (hosting, the interpreter, and a few other bits\nnot discussed here). The parts that are in the CLR are also included in the IronLanguages project in\nMicrosoft.Scripting.Core. The DLR parts are split into two assemblies, Microsoft.Scripting\nand Microsoft.Dynamic\u2014the former contains the hosting APIs and the latter contains code for\nCOM interop, the interpreter, and some other pieces common to dynamic languages. The languages themselves are split in two as well: IronPython.dll and IronRuby.dll im-\nplement the languages themselves (parsers, binders, etc.) while IronPython.Modules.dll and\nIronRuby.Libraries.dll implement the portions of the standard library that are implemented in\nC in the classic Python and Ruby implementations. 124\nThe Dynamic Language Runtime and the Iron Languages", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 138", "position": 138, "chunk_type": "semantic", "token_estimate": 267}
{"text": "8.11: Lessons Learned\nThe DLR is a useful example of a language-neutral platform for dynamic languages built on top of\na static runtime. The techniques it uses to achieve high-performance dynamic code are tricky to\nimplement properly, so the DLR takes these techniques and makes them available to every dynamic\nlanguage implementation. IronPython and IronRuby are good examples of how to build a language on top of the DLR. The\nimplementations are very similar because they were developed at the same time by close teams,\nyet they still have signi\ufb01cant di\ufb00erences in implementation. Having multiple di\ufb00erent languages\nco-developed14, along with C#\u2019s and VB\u2019s dynamic features, made sure that the DLR design got\nplenty of testing during development. The actual development of IronPython, IronRuby, and the DLR was handled very di\ufb00erently\nthan most projects within Microsoft at the time\u2014it was a very agile, iterative development model\nwith continuous integration running from day one. This enabled them to change very quickly when\nthey had to, which was good because the DLR became tied into C#\u2019s dynamic features early in its\ndevelopment. While the DLR tests are very quick, only taking a dozen seconds or so, the language\ntests take far too long to run (the IronPython test suite takes about 45 minutes, even with parallel\nexecution); improving this would have improved the iteration speed. Ultimately, these iterations\nconverged on the current DLR design, which seems overly complicated in parts but \ufb01ts together quite\nnicely in total. Having the DLR tied to C# was critically important because it made sure the DLR had a place and\na \u201cpurpose\u201d, but once the C# dynamic features were done the political climate changed (coinciding\nwith an economic downturn) and the Iron languages lost their support within the company. The\nhosting APIs, for example, never made it into the .NET Framework (and it\u2019s highly unlikely they\never will); this means that PowerShell 3, which is also based on the DLR, uses a completely di\ufb00erent\nset of hosting APIs than IronPython and IronRuby, although all of their objects can still interact as\ndescribed above15. But, thanks to the wonder of open source licensing, they will continue to survive\nand even thrive. 14IronPython, IronRuby, a prototype JavaScript, and the mysterious VBx\u2014a fully dynamic version of VB.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 139", "position": 139, "chunk_type": "semantic", "token_estimate": 377}
{"text": "8.11: But, thanks to the wonder of open source licensing, they will continue to survive\nand even thrive. 14IronPython, IronRuby, a prototype JavaScript, and the mysterious VBx\u2014a fully dynamic version of VB. 15Some of the DLR team members went on to work on the C# compiler-as-a-service library code-named \u201cRoslyn\u201d, which\nbears a striking resemblance to the IronPython and IronRuby hosting APIs. 125", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 139", "position": 139, "chunk_type": "semantic", "token_estimate": 62}
{"text": "File: Reader\nNoise\nReducion\nEdge\nDetection\nFile\nWriter\nFigure 9.1: Image processing pipeline\nTo illustrate this point, Figure 9.2 shows an image of a brain from a magnetic resonance image\n(MRI), and the result of processing it with a median \ufb01lter to reduce its level of noise, as well as the\noutcome of an edge detection \ufb01lter used to identify the borders of anatomical structures. Figure 9.2: From left to right: MRI brain image, median \ufb01lter, edge detection \ufb01lter\nFor each one of these tasks, the image analysis community has developed a variety of algorithms,\nand continue developing new ones. Why do they continue doing this?, you may ask, and the answer\nis that image processing is a combination of science, engineering, art, and \u201ccooking\u201d skills. Claiming\nthat there is an algorithmic combination that is the \u201cright\u201d answer to an image processing task is as\nmisleading as claiming that there is such a thing as the \u201cright\u201d type of chocolate dessert for a dinner. Instead of pursuing perfection, the community strives to produce a rich set of tools that ensures that\nthere will be no shortage of options to try when facing a given image processing challenge. This\nstate of a\ufb00airs, of course, comes at a price. The cost is that the image analyst has the di\ufb03cult task\nof choosing among dozens of di\ufb00erent tools that can be used in di\ufb00erent combinations to achieve\nsimilar results. The image analysis community is closely integrated with the research community. It is common\nto \ufb01nd that speci\ufb01c research groups become attached to the algorithmic families they have developed. This custom of \u201cbranding\u201d, and up to some level \u201cmarketing\u201d, leads to a situation where the best\nthat the software toolkit can do for the community is to o\ufb00er a very complete set of algorithmic\nimplementations that they can try, and then mix and match to create a recipe that satis\ufb01es their needs. These are some of the reasons why ITK was designed and implemented as a large collection\nof somewhat independent but coherent tools, the image \ufb01lters, many of which can be used to solve\nsimilar problems.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 142", "position": 142, "chunk_type": "semantic", "token_estimate": 353}
{"text": "The toolkit was also conceived as a resource that grows and renews itself continuously as new: algorithms and better implementations become available, superseding existing ones, and as new\ntools are developed in response to the emerging needs of new medical imaging technologies. Armed with this quick insight into the daily routine of the image analysts in the ITK community,\nwe can now dive into the main features of the architecture:\n\u2022 Modularity\n\u2022 Data Pipeline\n\u2022 Factories\n\u2022 IO Factories\n\u2022 Streaming\n\u2022 Reusability\n\u2022 Maintainability\nModularity\nModularity is one of the main characteristics of ITK. This is a requirement that emerges from\nthe way people in the image analysis community work when solving their problems. Most image\nanalysis problems put one or more input images through a combination of processing \ufb01lters that\nenhance or extract particular pieces of information from the images. Therefore there is no single\nlarge processing object, but rather myriad small ones. This structural nature of the image processing\nproblem logically implies implementing the software as a large collection of image processing \ufb01lters\nthat can be combined in many di\ufb00erent ways. It is also the case that certain types of processing \ufb01lters are clustered into families, inside which\nsome of their implementation features can be factorized. This leads to natural grouping of the image\n\ufb01lters into modules and groups of modules. Modularity, therefore occurs at three natural levels in ITK:\n\u2022 Filter Level\n\u2022 Filter Family Level\n\u2022 Filter Family Group Level\nAt the image \ufb01lter level, ITK has a collection of about 700 \ufb01lters. Given that ITK is implemented\nin C++, this is a natural level at which every one of those \ufb01lters is implemented by a C++ Class\nfollowing object-oriented design patterns. At the \ufb01lter family level, ITK groups \ufb01lters together\naccording to the nature of the processing that they perform. For example, all \ufb01lters that are related to\nFourier transforms will be put together into a Module. At the C++ level, Modules map to directories\nin the source tree, and to libraries once the software is compiled to its binary form. ITK has about\n120 of these Modules. Each module contains:\n1. The source code of the image \ufb01lters that belong to that family. 2. A set of con\ufb01guration \ufb01les that describe how to build the module and list dependencies\nbetween this module and other modules. 3. The set of unit tests corresponding to each one of the \ufb01lters.", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 143", "position": 143, "chunk_type": "semantic", "token_estimate": 406}
{"text": "Expat: Figure 9.4: Size distribution of 50 largest ITK modules in KB\nThe modularization in ITK also applies to a set of third-party libraries that are not directly part\nof the toolkit, but that the toolkit depends upon, and that are distributed along with the rest of the\ncode for the convenience of users. Particular examples of these third-party libraries are the image \ufb01le\nformat libraries: HDF5, PNG, TIFF, JPEG and OpenJPEG among others. The third party libraries\nare highlighted here because they account for about 56 percent of the size of ITK. This re\ufb02ects the\nusual nature of open source applications that build upon existing platforms. The size distribution\nof the third-party libraries does not necessarily re\ufb02ect the architectural organization of ITK, since\nwe have adopted these useful libraries just as they have been developed upstream. However, the\nthird-party code is distributed along with the toolkit, and partitioning it was one of the key driving\ndirectives for the modularization process. The module size distribution is presented here because it is a measure of the proper modularization\nof the code. One can see the modularization of the code as a continuous spectrum that ranges from\nthe extremes of having all the code in a single module, the monolithic version, to partitioning the\ncode in a very large collection of equally sized modules. This size distribution was a tool used to\nmonitor the progression of the modularization process, particularly to ensure that no big blocks of\ncode were left in the same module unless true logical dependencies called for such grouping. The modular architecture of ITK enables and facilitates:\n\u2022 Reduction and clari\ufb01cation of cross-dependencies\n\u2022 Adoption of code contributed by the community\n\u2022 Evaluation of quality metrics per module (for example, code coverage)\n\u2022 Building selected subsets of the toolkit\n\u2022 Packaging selected subsets of the toolkit for redistribution\n\u2022 Continued growth by progressive addition of new modules\nLuis Ib\u00e1\u00f1ez and Brad King\n131", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 145", "position": 145, "chunk_type": "semantic", "token_estimate": 324}
{"text": "The modularization process made it possible to explicitly identify and declare the dependencies: between di\ufb00erent portions of the toolkit as they were put into modules. In many cases, this exercise\nrevealed arti\ufb01cial or incorrect dependencies that had been introduced in the toolkit over time, and\nthat passed unnoticed when most of the code was put together in a few large groups. The usefulness of evaluating quality metrics per module is twofold. First, it makes it easier to\nhold developers accountable for the modules which they maintain. Second, it makes it possible to\nengage in clean-up initiatives in which a few developers focus for a short period of time on raising\nthe quality of a speci\ufb01c module. When concentrating on a small portion of the toolkit, it is easier to\nsee the e\ufb00ect of the e\ufb00ort and to keep developers engaged and motivated. To reiterate, we note that the structure of the toolkit re\ufb02ects the organization of the community\nand in some cases the processes that have been adopted for the continuous growth and quality control\nof the software. Data Pipeline\nThe staged nature of most image analysis tasks led naturally to the selection of a Data Pipeline\narchitecture as the backbone infrastructure for data processing. The Data Pipeline enables:\n\u2022 Filter Concatenation: A set of image \ufb01lters can be concatenated one after another, composing\na processing chain in which a sequence of operations are applied to the input images. \u2022 Parameter Exploration: Once a processing chain is put together, it is easy to change the\nparameters of any \ufb01lter in the chain, and to explore the e\ufb00ects that such change will have on\nthe \ufb01nal output image. \u2022 Memory Streaming: Large images can be managed by processing only sub-blocks of the image\nat a time. In this way, it becomes possible to process large images that otherwise would not\nhave \ufb01t into main memory. Figures 9.1 and 9.2 have already presented a simpli\ufb01ed representation of a data pipeline from the\nimage processing point of view. Image \ufb01lters typically have numeric parameters that are used to\nregulate the behavior of the \ufb01lter. Every time one of the numeric parameters is modi\ufb01ed, the data\npipeline marks its output as \u201cdirty\u201d and knows that this particular \ufb01lter, and all the downstream ones\nthat use its output, should be executed again. This feature of the pipeline facilitates the exploration\nof parameter space while using a minimum amount of processing power for each instance of an\nexperiment.", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 146", "position": 146, "chunk_type": "semantic", "token_estimate": 412}
{"text": "computes the amount of image data that can be produced as output with the given amount of data: The third pass is intended to trigger the computation on the data inside the Requested Region. This pass also goes in the upstream direction and it is codi\ufb01ed in the UpdateOutputData() method. Since every \ufb01lter needs its input data before it can compute its output data, the call is passed to\nthe respective upstream \ufb01lter \ufb01rst, hence the upstream propagation. Upon return the current \ufb01lter\nactually proceeds to computes its data. The fourth and \ufb01nal pass proceeds downstream, and consists of the actual execution of computa-\ntion by every \ufb01lter. The call is codi\ufb01ed in the GenerateData() method. The downstream direction\nis not a consequence of one \ufb01lter making calls on its downstream partner, but rather of the fact that\nthe UpdateOutputData() calls are executing in order from the \ufb01rst \ufb01lter to the last \ufb01lter. That is,\nthe sequence happens downstream due to timing of the calls, and not due to what \ufb01lter is driving the\ncalls. This clari\ufb01cation is important because the ITK pipeline is by nature a Pull Pipeline, in which\ndata is requested from the end, and the logic is also controlled from the end. Factories\nOne of the fundamental design requirements of ITK is to provide support for multiple platforms. This\nrequirement emerges from the desire to maximize the impact of the toolkit by making it usable to a\nbroad community regardless of their platform of choice. ITK adopted the Factory design pattern to\naddress the challenge of supporting fundamental di\ufb00erences among the many hardware and software\nplatforms, without sacri\ufb01cing the \ufb01tness of a solution to each one of the individual platforms. The Factory pattern in ITK uses class names as keys to a registry of class constructors. The\nregistration of factories happens at run time, and can be done by simply placing dynamic libraries in\nspeci\ufb01c directories that ITK applications search at start-up time. This last feature provides a natural\nmechanism for implementing a plugin architecture in a clean and transparent way. The outcome is to\nfacilitate the development of extensible image analysis applications, satisfying the need to provide\nan ever-growing set of image analysis capabilities. 136", "domains": ["Design Patterns", "Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 150", "position": 150, "chunk_type": "semantic", "token_estimate": 370}
{"text": "IO Factories: The factory mechanism is particularly important when performing IO. Embracing Diversity with Facades\nThe image analysis community has developed a very large set of \ufb01le formats to store image data. Many of these \ufb01le formats are designed and implemented with speci\ufb01c uses in mind, and therefore are\n\ufb01ne-tuned to speci\ufb01c types of images. As a consequence, on a regular basis, new image \ufb01le formats\nare conceived and promoted across the community. Aware of this situation, the ITK development\nteam designed an IO architecture suitable for ease of extensibility, in which it is easy to add support\nfor more and more \ufb01le formats on a regular basis. ImageIOFactory\nPNGImageIO\nPNGImageIOFactory\nJPEGImageIO\nJPEGImageIOFactory\nGDCMImageIO\nGDCMImageIOFactory\n...\nImageFileReader\nImageFileReader\nFigure 9.8: IO Factories dependencies\nThis IO extensible architecture is built upon the Factory mechanism described in the previous\nsection. The main di\ufb00erence is that in the case of IO, the IO Factories are registered in a specialized\nregistry that is managed by the ImageIOFactory base class, shown on the upper left corner of Fig-\nure 9.8. The actual functionality of reading and writing data from image \ufb01le formats is implemented\nin a family of ImageIO classes, shown on the right side of Figure 9.8. These service classes are\nintended to be instantiated on demand when the user requests to read or write an image. The service\nclasses are not exposed to the application code. Instead, applications are expected to interact with\nthe facade classes:\n\u2022 ImageFileReader\n\u2022 ImageFileWriter\nThese are the two classes with which the application will invoke code such as:\nreader->SetFileName(\u2018\u2018image1.png\u2019\u2019);\nreader->Update();\nor\nwriter->SetFileName(\u2018\u2018image2.jpg\u2019\u2019);\nwriter->Update();\nLuis Ib\u00e1\u00f1ez and Brad King\n137", "domains": ["Design Patterns", "Software Quality Attributes", "Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 151", "position": 151, "chunk_type": "semantic", "token_estimate": 272}
{"text": "In both cases the call to Update() triggers the execution of the upstream pipeline to which these: ProcessObjects are connected. Both the reader and the writer behave as one \ufb01lter more in a pipeline. In the particular case of the reader, the call to Update() triggers the reading of the corresponding\nimage \ufb01le into memory. In the case of the writer, the call to Update() triggers the execution of\nthe upstream pipeline that is providing the input to the writer, and \ufb01nally results in an image being\nwritten out to disk into a particular \ufb01le format. These facade classes hide from the application developer the internal di\ufb00erences that are inherent\nto the particularities of each \ufb01le format. They even hide the existence of the \ufb01le format itself. The\nfacades are designed in such a way that most of the time application developers do not need to know\nwhat \ufb01le formats are expected to be read by the application. The typical application will simply\ninvoke code such as\nstd::string filename = this->GetFileNameFromGUI();\nwriter->SetFileName( filename );\nwriter->Update();\nThese calls will work \ufb01ne regardless of whether the content of the filename variable is any of the\nfollowing strings:\n\u2022 image1.png\n\u2022 image1.jpeg\n\u2022 image1.ti\ufb00\n\u2022 image1.dcm\n\u2022 image1.mha\n\u2022 image1.nii\n\u2022 image1.nii.gz\nwhere the \ufb01le name extensions identify a di\ufb00erent image \ufb01le format in every case. Know Thy Pixel Type\nDespite the assistance that the \ufb01le reader and writer facades provide, it is still up to the application\ndeveloper to be aware of the pixel type that the application needs to process. In the context of\nmedical imaging, it is reasonable to expect that the application developer will know whether the\ninput image will contain a MRI, a mammogram or a CT scan, and therefore be mindful of selecting\nthe appropriate pixel type and image dimensionality for each one of these di\ufb00erent image modalities. This speci\ufb01city of image type might not be convenient for application settings where users wants\nto read any image type, which are most commonly found in the scenarios of rapid prototyping and\nteaching. In the context of deploying a medical image application for production in a clinical setting,\nhowever, it is expected that the pixel type and dimension of the images will be clearly de\ufb01ned and\nspeci\ufb01ed based on the image modality to be processed.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 152", "position": 152, "chunk_type": "semantic", "token_estimate": 386}
{"text": "reading RAW images, the application developer may have to insert extra calls to further specify: the characteristics of the \ufb01le format at hand. DICOM \ufb01les will be the most commonly found in\nclinical environments, while RAW images are still a necessary evil for exchanging data in the research\nenvironment. Together But Separate\nThe self-contained nature of every IO Factory and ImageIO service class is also re\ufb02ected in the\nmodularization. Typically, an ImageIO class depends on a specialized library that is dedicated to\nmanaging a speci\ufb01c \ufb01le format. That is the case for PNG, JPEG, TIFF and DICOM, for example. In those cases, the third-party library is managed as a self-contained module, and the specialized\nImageIO code that interfaces ITK to that third-party library is also put in a module by itself. In\nthis way, speci\ufb01c applications may disable many \ufb01le formats that are not relevant to their domain,\nand can focus on o\ufb00ering only those \ufb01le formats that are useful for the anticipated scenarios of that\napplication. Just as with standard factories, the IO factories can be loaded at run-time from dynamic libraries. This \ufb02exibility facilitates the use of specialized and in-house developed \ufb01le formats without requiring\nall such \ufb01le formats to be incorporated directly into the ITK toolkit itself. The loadable IO factories\nhas been one of the most successful features in the architectural design of ITK. It has made it possible\nto easily manage a challenging situation without placing a burden on the code or obscuring its\nimplementation. More recently, the same IO architecture has been adapted to manage the process\nof reading and writing \ufb01les containing spatial transformations represented by the Transform class\nfamily. Streaming\nITK was initially conceived as a set of tools for processing the images acquired by the Visible Human\nProject4. At the time, it was clear that such a large dataset would not \ufb01t in the RAM of computers\nthat were typically available to the medical imaging research community. It is still the case that\nthe dataset will not \ufb01t in the typical desktop computers that we use today. Therefore, one of the\nrequirements for developing the Insight Toolkit was to enable the streaming of image data through\nthe data pipeline. More speci\ufb01cally, to be able to process large images by pushing sub-blocks of the\nimage throughout the data pipeline, and then assembling the resulting blocks on the output side of\nthe pipeline.", "domains": ["Design Patterns", "Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 153", "position": 153, "chunk_type": "semantic", "token_estimate": 400}
{"text": "Median: Image\nFilter\nInput Image\nOutput Image\nRegion 1\nRegion 2\nRegion 3\nRegion 4\nRegion 1\nRegion 2\nRegion 3\nRegion 4\nNeighborhood Radius = 2\nSize = 100x25 pixels\nSize = 100x29 pixels\nFigure 9.9: Illustration of image streaming process\neach. Note that the horizontal dimension is not enlarged in this case because it is already at the\nmaximum that the input image can provide; therefore, the enlarged request of 104 pixels (100 pixels\nplus two borders of 2 pixels) gets cropped to the maximum size of the image, which is 100 pixels in\nthe horizontal dimension. ITK \ufb01lters that operate on neighborhoods will take care of the boundary conditions by using one\nof the three typical approaches: considering a null value outside of the image, mirroring the pixels\u2019\nvalues across the border, or repeating the border value on the outside. In the case of the median \ufb01lter,\na zero-\ufb02ux Neumann boundary condition is used, which simply means that the pixels outside of the\nregion border are assumed to be a repetition of the pixel values found in the last pixels inside the\nborder. It is a well-kept dirty little secret of the image processing literature that most of the implementation\ndi\ufb03culties with image \ufb01lters are related to proper management of boundary conditions. This is a\nparticular symptom of the disconnection between the theoretical training found in many textbooks\nand the software practice of image processing. In ITK this was managed by implementing a collection\nof image iterator classes and an associated family of boundary condition calculators. These two\nhelper classes families hide from image \ufb01lters the complexities of managing boundary conditions in\nN-dimensions. The streaming process is driven from outside the \ufb01lter, typically by the ImageFileWriter or\nthe StreamingImageFilter. These two classes implement the streaming functionality of taking the\ntotal size of the image and partitioning it into a number of divisions requested by the application\ndeveloper. Then, during their Update() call, they go in an iteration loop asking for each one of the\nintermediate pieces of the image. At that stage, they take advantage of the SetRequestedRegion()\nAPI described in Figure 9.7 in Section 9.2. That constrains the computation of the upstream pipeline\nto a subregion of the image.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 154", "position": 154, "chunk_type": "semantic", "token_estimate": 373}
{"text": "9.3: Lessons Learned\nReusability\nThe principle of reusability can also be read as \u201cavoidance of redundancy\u201d. In the case of ITK, this\nhas been achieved with a three-pronged approach. \u2022 First, the adoption of object-oriented programming, and in particular the proper creation of\nclass hierarchies where common functionalities are factorized in base classes. \u2022 Second, the adoption of generic programming, implemented via the heavy use of C++ templates,\nfactorizing behaviors that are identi\ufb01ed as patterns. \u2022 Third, the generous use of C++ macros has also permitted reuse of standard snippets of code\nthat are needed in myriad places across the toolkit. Many of these items may sound like platitudes and appear obvious today, but when ITK develop-\nment started in 1999 some of them were not that obvious. In particular, at the time the support most\nC++ compilers o\ufb00ered for templates did not quite follow a consistent standard. Even today, decisions\nsuch as the adoption of generic programming and the use of a widely templated implementation\ncontinue to be controversial in the community. This is manifested in the communities that prefer to\nuse ITK via the wrapping layers to Python, Tcl or Java. Generic Programming\nThe adoption of generic programming was one of the de\ufb01ning implementation features of ITK. It\nwas a di\ufb03cult decision in 1999 when the compiler support for C++ templates was rather fragmented,\nand the Standard Template Library (STL) was still considered a bit exotic. Generic programming was adopted in ITK by embracing the use of C++ templates for imple-\nmenting generalization of concepts and in this way increasing code reuse. The typical example of\nC++ template parameterization in ITK is the Image class, that can be instantiated in the following\nway:\ntypedef unsigned char PixelType;\nconst unsigned int Dimension = 3;\ntypedef itk::Image< PixelType, Dimension > ImageType;\nImageType::Pointer image = ImageType::New();\nIn this expression, the application developer chooses the type to be used to represent pixels in\nthe image, as well as the dimension of the image as a grid in space. In this particular example, we\nchose to use an 8-bit pixel represented in an unsigned char type, for a 3D image. Thanks to the\nunderlying generic implementation, it is possible to instantiate images of any pixel type and any\ndimension in ITK.", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 156", "position": 156, "chunk_type": "semantic", "token_estimate": 378}
{"text": "type while some other \ufb01lters expect the pixel type to be a vector of \ufb02oating point numbers. When: instantiated with inappropriate pixel types, these \ufb01lters will produce compilation errors or will\nresult in erroneous computational results. To prevent incorrect instantiations and to facilitate the\ntroubleshooting of compilation errors, ITK adopted the use of concept checking that is based on\nforcing the exercise of certain expected features of the types, with the goal of producing early failures\ncombined with human-readable error messages. C++ templates are also exploited in certain sections of the toolkit in the form of Template\nMetaprogramming, with the goal of increasing run-time speed performance of the code, in particular\nfor unrolling loops that control the computation of low-dimensional vectors and matrices. Ironically,\nwe have found over time that certain compilers have become smarter at \ufb01guring out when to unroll\nloops, and no longer need the help of Template MetaProgramming expressions in some cases. Knowing When to Stop\nThere is also the general risk of doing \u201ctoo much of a good thing\u201d, meaning, there is a risk of\noverusing templates, or overusing macros. It is easy to go overboard and end up creating a new\nlanguage on top of C++ that is essentially based on the use of templates and macros. This is a \ufb01ne\nline, and it demands continuous attention from the development team to make sure that the language\nfeatures are properly used without being abused. As a concrete example, the widespread use of explicitly naming types via C++ typedefs has\nproved to be particularly important. This practice plays two roles: on the one hand it provides a\nhuman-readable informative name describing the nature of the type and its purpose; on the other\nhand, it ensures that the type is used consistently across the toolkit. As an example, during the\nrefactoring of the toolkit for its 4.0 version, a massive e\ufb00ort was invested in collecting the cases where\nC++ integer types such as int, unsigned int, long and unsigned long were used and to replace\nthem with types named after the proper concept that the associated variables were representing. This\nwas the most costly part of the task of ensuring that the toolkit was able to take advantage of 64-bit\ntypes for managing images larger than four gigabytes in all platforms. This task was of the utmost\nimportance for promoting the use of ITK in the \ufb01elds of microscopy and remote sensing, where\nimage of tens of gigabytes in size are common.", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 157", "position": 157, "chunk_type": "semantic", "token_estimate": 415}
{"text": "type while some other \ufb01lters expect the pixel type to be a vector of \ufb02oating point numbers. When: This\nwas the most costly part of the task of ensuring that the toolkit was able to take advantage of 64-bit\ntypes for managing images larger than four gigabytes in all platforms. This task was of the utmost\nimportance for promoting the use of ITK in the \ufb01elds of microscopy and remote sensing, where\nimage of tens of gigabytes in size are common. Maintainability\nThe architecture satis\ufb01es the constraints that minimize maintenance cost. \u2022 Modularity (at the class level)\n\u2022 Many small \ufb01les\n\u2022 Code reuse\n\u2022 Repeated patterns\nThese characteristics reduce maintenance cost in the following ways:\n\u2022 Modularity (at the class level) makes it possible to enforce test-driven development techniques\nat the image \ufb01lter level, or in general the ITK class level. Stringent testing discipline applied to\nsmall and modular pieces of code has the advantage of reducing the pools of code where bugs\ncan hide, and with the natural decoupling that results from modularization, it is a lot easier to\nlocate defects and eliminate them. \u2022 Many small \ufb01les facilitate the assignment of portions of the code to speci\ufb01c developers, and\nsimplify the tracking of defects when they are associated with speci\ufb01c commits in the revision\nLuis Ib\u00e1\u00f1ez and Brad King\n143", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 157", "position": 157, "chunk_type": "semantic", "token_estimate": 223}
{"text": "control system. The discipline of keeping small \ufb01les also leads to the enforcement of the: golden rule of functions and classes: Do one thing, and do it right. \u2022 Code reuse: When code is reused (instead of being copy-pasted and reimplemented) the code\nitself bene\ufb01ts from the higher level of scrutiny that results from being exercised in many\ndi\ufb00erent circumstances. It leads more eyes to look at the code, or at least at the e\ufb00ects of\nthe code, and so the code bene\ufb01ts from Linus\u2019s Law: \u201cGiven enough eyeballs, all bugs are\nshallow.\u201d\n\u2022 Repeated patterns simplify the work of maintainers, who in reality account for more than 75%\nof the cost of software development over the lifetime of a project. Using coding patterns that\nare consistently repeated in di\ufb00erent places in the code makes it a lot easier for a developer to\nopen a \ufb01le and quickly understand what the code is doing, or what it is intended to do. As the developers got involved in regular maintenance activities they were exposed to some\n\u201ccommon failures\u201d, in particular:\n\u2022 Assumptions that some \ufb01lters make regarding speci\ufb01c pixel types for their input or output\nimages, but that are not enforced via types or concept checking, and that are not speci\ufb01ed in\nthe documentation. \u2022 Not writing for readability. This is one of the most common challenges for any software whose\nnew algorithm implementations originate in the research community. It is common in that\nenvironment to write code that \u201cjust works\u201d, and to forget that the purpose of code is not just\nto be executed at run time, but to be easily read by the next developer. Typical good rules\nof \u201cclean code\u201d writing\u2014for example, write small functions that do one thing and one thing\nonly (the Single Responsibility Principle and the Principle of Least Surprise), adhere to proper\nnaming of variables and functions\u2014tend to be ignored when researchers are excited about\ngetting their new shiny algorithm to work. \u2022 Ignoring failure cases and error management. It is common to focus on the \u201cnice cases\u201d of data\nprocessing and to fail to provide code for managing all the cases that can go wrong. Adopters\nof the toolkit quickly run into such cases once they start developing and deploying applications\nin the real world. \u2022 Insu\ufb03cient testing. It requires a lot of discipline to follow the practice of test-driven develop-\nment, especially the notion of writing the tests \ufb01rst and only implementing functionalities as\nyou test them.", "domains": ["Design Principles"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 158", "position": 158, "chunk_type": "semantic", "token_estimate": 415}
{"text": "This opportunity for widespread refactoring is very rare. For the previous ten years, we had: relied on the daily e\ufb00ort of performing small local refactorings, cleaning up speci\ufb01c corners of the\ntoolkit as we ran into them. This continuous process of clean up and improvement takes advantage\nof the massive collaboration of open source communities, and it is safely enabled by the testing\ninfrastructure driven by CDash, which regularly exercises about 84% of the code in the toolkit. Note\nthat in contrast, the average code coverage of software industry testing is estimated to be only 50%. Among the many things that were changed in the refactoring e\ufb00ort, the ones that are most relevant\nto the architecture are:\n\u2022 Modularization was introduced in the toolkit\n\u2022 Integer types were standardized\n\u2022 Typedefs were \ufb01xed to allow management of images larger than 4 GB on all platforms\n\u2022 The software process was revised:\n\u2013 Migrated from CVS to Git\n\u2013 Introduced code review with Gerrit\n\u2013 Introduced testing on demand with CDash@home\n\u2013 Improved method for downloading data for unit testing\n\u2022 Deprecated support for obsolete compilers\n\u2022 Improved support for many IO image \ufb01le formats including:\n\u2013 DICOM\n\u2013 JPEG2000\n\u2013 TIFF (BigTIFF)\n\u2013 HDF5\n\u2022 Introduced a framework for supporting GPU computation\n\u2022 Introduced support for video processing\n\u2013 Added a bridge to OpenCV\n\u2013 Added a bridge to VXL\nMaintenance based on incremental modi\ufb01cations\u2014tasks such as adding features to an image \ufb01lter,\nimproving performance of a given algorithm, addressing bug reports, and improving documentation\nof speci\ufb01c image \ufb01lters\u2014works \ufb01ne for the local improvement of speci\ufb01c C++ classes. However, a\nmassive refactoring is needed for infrastructure modi\ufb01cations that a\ufb00ect a large number of classes\nacross the board, such as the ones listed above. For example, the set of changes needed to support\nimages larger than 4 GB was probably one of the largest patches ever applied to ITK. It required the\nmodi\ufb01cation of hundreds of classes and could not have been done incrementally without incurring in\na great deal of pain. The modularization is another example of a task that could not have been done\nincrementally. It truly a\ufb00ected the entire organization of the toolkit, how its testing infrastructure\nworks, how testing data is managed, how the toolkit is packaged and distributed, and how new\ncontributions will be encapsulated to be added to the toolkit in the future.", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 160", "position": 160, "chunk_type": "semantic", "token_estimate": 398}
{"text": "now, and is one of the key reasons that Mailman is as ubiquitous as it is today. Despite modernizing: this subsystem in Mailman 3, the design and implementation remains largely unchanged. 10.1\nThe Anatomy of a Message\nOne of the core data structures in Mailman is the email message, represented by a message object. Many of the interfaces, functions, and methods in the system take three arguments: the mailing list\nobject, the message object, and a metadata dictionary that is used to record and communicate state\nwhile a message is processed through the system. Figure 10.1: A MIME multipart/mixed message containing text, images, and an audio \ufb01le\nOn the face of it, an email message is a simple object. It consists of a number of colon-separated\nkey-value pairs, called the headers, followed by an empty line which separates the headers from\nthe message body. This textural representation should be easy to parse, generate, reason about, and\nmanipulate, but in fact it quickly gets quite complicated. There are countless RFCs that describe\nall the variations that can occur, such as handling complex data types like images, audio, and more. Email can contain ASCII English, or just about any language and character set in existence. The\n150\nGNU Mailman", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 164", "position": 164, "chunk_type": "semantic", "token_estimate": 207}
{"text": "basic structure of an email message has been borrowed over and over again for other protocols,: such as NNTP and HTTP, yet each is slightly di\ufb00erent. Our work on Mailman has spawned several\nlibraries just to deal with the vagaries of this format (often called \u201cRFC822\u201d for the founding\n1982 IETF standard5). The email libraries originally developed for use by GNU Mailman have\nfound their way into the Python standard library, where development continues to make them more\nstandards-compliant and robust. Email messages can act as containers for other types of data, as de\ufb01ned in the various MIME\nstandards. A container message part can encode an image, some audio, or just about any type of\nbinary or text data, including other container parts. In mail reader applications, these are known as\nattachments. Figure 10.1 shows the structure of a complex MIME message. The boxes with solid\nborders are the container parts, the boxes with dashed borders are Base64 encoded binary data, and\nthe box with a dotted border is a plain text message. Container parts can also be arbitrarily nested; these are called multiparts and can in fact get quite\ndeep. But any email message, regardless of its complexity, can be modeled as a tree with a single\nmessage object at its root. Within Mailman, we often refer to this as the message object tree, and we\npass this tree around by reference to the root message object. Figure 10.2 shows the object tree of\nthe multipart message in Figure 10.1. Figure 10.2: Message object tree of a complex MIME email message\nMailman will almost always modify the original message in some way. Sometimes the trans-\nformations can be fairly benign, such as adding or removing headers. Sometimes we\u2019ll completely\nchange the structure of the message object tree, such as when the content \ufb01lter removes certain\ncontent types like HTML, images, or other non-text parts. Mailman might even collapse \u201cmulti-\npart/alternatives\u201d, where a message appears as both plain text and as some rich text type, or add\nadditional parts containing information about the mailing list itself. Mailman generally parses the on the wire bytes representation of a message just once, when it\n\ufb01rst comes into the system. From then on, it deals only with the message object tree until it\u2019s ready to\nsend it back out to the outgoing mail server. It\u2019s at that point that Mailman \ufb02attens the tree back into\na bytes representation.", "domains": ["Design Principles"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 165", "position": 165, "chunk_type": "semantic", "token_estimate": 405}
{"text": "Python object, including all its subobjects, to a byte stream, and it\u2019s perfectly suited to optimizing: the handling of email message object trees. Unpickling is deserializing this byte stream back into a\nlive object. By storing these byte streams in a \ufb01le, Python programs gain low-cost persistence. 10.2\nThe Mailing List\nThe mailing list is obviously another core object in the Mailman system, and most of the operations\nin Mailman are mailing list-centric, such as:\n\u2022 Membership is de\ufb01ned in terms of a user or address being subscribed to a speci\ufb01c mailing list. \u2022 Mailing lists have a large number of con\ufb01guration options that are stored in the database, and\nwhich control everything from posting privileges to how messages are modi\ufb01ed before \ufb01nal\ndelivery. \u2022 Mailing lists have owners and moderators which have greater permission to change aspects of\nthe list, or to approve and reject questionable postings. \u2022 Every mailing list has its own archive. \u2022 Users post new messages to a speci\ufb01c mailing list. and so on. Almost every operation in Mailman takes a mailing list as an argument\u2014it\u2019s that\nfundamental. Mailing list objects have undergone a radical redesign in Mailman 3 to make them\nmore e\ufb03cient and to expand their \ufb02exibility. One of John\u2019s earliest design decisions was how to represent a mailing list object inside the\nsystem. For this central data type, he chose a Python class with multiple base classes, each of\nwhich implements a small part of the mailing list\u2019s responsibility. These cooperating base classes,\ncalled mixin classes, were a clever way to organize the code so that it was easy to add entirely\nnew functionality. By grafting on a new mixin base class, the core MailList class could easily\naccommodate something new and cool. For example, to add an auto-responder to Mailman 2, a mixin class was created that held the data\nspeci\ufb01c to that feature. The data would get automatically initialized when a new mailing list was\ncreated. The mixin class also provided the methods necessary to support the auto-responder feature. This structure was even more useful when it came to the design of the mailing MailList object\u2019s\npersistence. Another of John\u2019s early design decisions was to use Python pickles for storing MailList state\npersistence. In Mailman 2, the MailList object\u2019s state is stored in a \ufb01le called config.pck, which is just\nthe pickled representation of the MailList object\u2019s dictionary. Every Python object has an attribute\ndictionary called __dict__.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 166", "position": 166, "chunk_type": "semantic", "token_estimate": 407}
{"text": "Python object, including all its subobjects, to a byte stream, and it\u2019s perfectly suited to optimizing: In Mailman 2, the MailList object\u2019s state is stored in a \ufb01le called config.pck, which is just\nthe pickled representation of the MailList object\u2019s dictionary. Every Python object has an attribute\ndictionary called __dict__. So saving a mailing list object then is simply a matter of pickling its\n__dict__ to a \ufb01le, and loading it just involves reading the pickle from the \ufb01le and reconstituting its\n__dict__. Thus, when a new mixin class was added to implement some new functionality, all the attributes\nof the mixin were automatically pickled and unpickled appropriately. The only extra work we had\nto do was to maintain a schema version number to automatically upgrade older mailing list objects\nwhen new attributes were added via the mixin, since the pickled representation of older MailList\nobjects would be missing the new attributes. As convenient as this was, both the mixin architecture and pickle persistence eventually crumbled\nunder their own weight. Site administrators often requested ways to access the mailing list con\ufb01gura-\ntion variables via external, non-Python systems. But the pickle protocol is entirely Python-speci\ufb01c,\nso sequestering all that useful data inside a pickle wouldn\u2019t work for them. Also, because the entire\n152\nGNU Mailman", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 166", "position": 166, "chunk_type": "semantic", "token_estimate": 214}
{"text": "state of a mailing list was contained in the config.pck, and Mailman has multiple processes that: need to read, modify, and write the mailing list state, we had to implement exclusive \ufb01le-based and\nNFS-safe locks to ensure data consistency. Every time some part of Mailman wants to change the\nstate of a mailing list, it must acquire the lock, write out the change, then release the lock. Even read\noperations can require a re-load of the list\u2019s config.pck \ufb01le, since some other process may have\nchanged it before the read operation. This serialization of operations on a mailing list turned out to\nbe horribly slow and ine\ufb03cient. For these reasons, Mailman 3 stores all of its data in a SQL database. By default SQLite3 is\nused, though this is easily changed since Mailman 3 uses the Object Relational Mapper called Storm,\nwhich supports a wide variety of databases. PostgreSQL support was added with just a few lines of\ncode, and a site administrator can enable it by changing one con\ufb01guration variable. Another, bigger problem is that in Mailman 2, each mailing list is a silo. Often operations span\nacross many mailing lists, or even all of them. For example, a user might want to temporarily suspend\nall their subscriptions when they go on vacation. Or a site administrator might want to add some\ndisclaimer to the welcome message of all of the mailing lists on her system. Even the simple matter\nof \ufb01guring out which mailing lists a single address is subscribed to required unpickling the state of\nevery mailing list on the system, since membership information was kept in the config.pck \ufb01le too. Another problem was that each config.pck \ufb01le lived in a directory named after the mailing list,\nbut Mailman was originally designed without consideration for virtual domains. This lead to a very\nunfortunate problem where two mailing lists could not have the same name in di\ufb00erent domains. For example, if you owned both the example.com and example.org domains, and you wanted them\nto act independently and allow for a di\ufb00erent support mailing list in each, you cannot do this in\nMailman 2, without modi\ufb01cations to the code, a barely supported hook, or conventional workarounds\nthat forced a di\ufb00erent list name under the covers, which is the approach used by large sites such as\nSourceForge. This has been solved in Mailman 3 by changing the way mailing lists are identi\ufb01ed, along with mov-\ning all the data into a traditional database.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 167", "position": 167, "chunk_type": "semantic", "token_estimate": 413}
{"text": "state of a mailing list was contained in the config.pck, and Mailman has multiple processes that: For example, if you owned both the example.com and example.org domains, and you wanted them\nto act independently and allow for a di\ufb00erent support mailing list in each, you cannot do this in\nMailman 2, without modi\ufb01cations to the code, a barely supported hook, or conventional workarounds\nthat forced a di\ufb00erent list name under the covers, which is the approach used by large sites such as\nSourceForge. This has been solved in Mailman 3 by changing the way mailing lists are identi\ufb01ed, along with mov-\ning all the data into a traditional database. The primary key for the mailing list table is the fully qual-\ni\ufb01ed list name or as you\u2019d probably recognize it, the posting address. Thus support@example.com\nand support@example.org are now completely independent rows in the mailing list table, and can\neasily co-exist in a single Mailman system. 10.3\nRunners\nMessages \ufb02ow through the system by way of a set of independent processes called runners. Originally\nconceived as a way of predictably processing all the queued message \ufb01les found in a particular\ndirectory, there are now a few runners which are simply independent, long-running processes that\nperform a speci\ufb01c task and are managed by a master process; more on that later. When a runner does\nmanage \ufb01les in a directory, it is called a queue runner. Mailman is religiously single-threaded, even though there is signi\ufb01cant parallelism to exploit. For\nexample, Mailman can accept messages from the mail server at the same time it\u2019s sending messages\nout to recipients, or processing bounces, or archiving a message. Parallelism in Mailman is achieved\nthrough the use of multiple processes, in the form of these runners. For example, there is an incoming\nqueue runner with the sole job of accepting (or rejecting) messages from the upstream mail server. There is an outgoing queue runner with the sole job of communicating with the upstream mail server\nover SMTP in order to send messages out to the \ufb01nal recipients. There\u2019s an archiver queue runner,\na bounce processing queue runner, a queue runner for forwarding messages to an NNTP server, a\nBarry Warsaw\n153", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 167", "position": 167, "chunk_type": "semantic", "token_estimate": 365}
{"text": "runner for composing digests, and several others. Runners which don\u2019t manage a queue include a: Local Mail Transfer Protocol (LMTP)7 server and an administrative HTTP server. Each queue runner is responsible for a single directory, i.e., its queue. While the typical Mailman\nsystem can perform perfectly well with a single process per queue, we use a clever algorithm for\nallowing parallelism within a single queue directory, without requiring any kind of cooperation or\nlocking. The secret is in the way we name the \ufb01les within the queue directory. As mentioned above, every message that \ufb02ows through the system is also accompanied by a\nmetadata dictionary that accumulates state and allows independent components of Mailman to\ncommunicate with each other. Python\u2019s pickle library is able to serialize and deserialize multiple\nobjects to a single \ufb01le, so we can pickle both the message object tree and metadata dictionary into\none \ufb01le. There is a core Mailman class called Switchboard which provides an interface for enqueuing\n(i.e., writing) and dequeuing (i.e., reading) the message object tree and metadata dictionary to \ufb01les\nin a speci\ufb01c queue directory. Every queue directory has at least one switchboard instance, and every\nqueue runner instance has exactly one switchboard. Pickle \ufb01les all end in the .pck su\ufb03x, though you may also see .bak, .tmp, and .psv \ufb01les in a\nqueue. These are used to ensure the two sacrosanct tenets of Mailman: no \ufb01le should ever get lost,\nand no message should ever be delivered more than once. But things usually work properly and these\n\ufb01les can be pretty rare. As indicated, for really busy sites Mailman supports running more than one runner process per\nqueue directory, completely in parallel, with no communication between them or locking necessary\nto process the \ufb01les. It does this by naming the pickle \ufb01les with a SHA1 hash, and then allowing a\nsingle queue runner to manage just a slice of the hash space. So if a site wants to run two runners on\nthe bounces queue, one would process \ufb01les from the top half of the hash space, and the other would\nprocess \ufb01les from the bottom half of the hash space. The hashes are calculated using the contents of\nthe pickled message object tree, the name of the mailing list that the message is destined for, and\na time stamp. The SHA1 hashes are e\ufb00ectively random, and thus on average a two-runner queue\ndirectory will have about equal amounts of work per process.", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 168", "position": 168, "chunk_type": "semantic", "token_estimate": 412}
{"text": "runner for composing digests, and several others. Runners which don\u2019t manage a queue include a: The hashes are calculated using the contents of\nthe pickled message object tree, the name of the mailing list that the message is destined for, and\na time stamp. The SHA1 hashes are e\ufb00ectively random, and thus on average a two-runner queue\ndirectory will have about equal amounts of work per process. And because the hash space can be\nstatically divided, these processes can operate on the same queue directory with no interference or\ncommunication necessary. There\u2019s an interesting limitation to this algorithm. Since the splitting algorithm allots one or\nmore bits of the hash to each space, the number of runners per queue directory must be a power of 2. This means there can be 1, 2, 4, or 8 runner processes per queue, but not, for example, 5. In practice\nthis has never been a problem, since few sites will ever need more than 4 processes to handle their\nload. There\u2019s another side e\ufb00ect of this algorithm that did cause problems during the early design of\nthis system. Despite the unpredictability of email delivery in general, the best user experience is\nprovided by processing the queue \ufb01les in FIFO order, so that replies to a mailing list get sent out in\nroughly chronological order. Not making a best e\ufb00ort attempt at doing so can cause confusion for\nmembers. But using SHA1 hashes as \ufb01le names obliterates any timestamps, and for performance\nreasons stat() calls on queue \ufb01les, or unpickling the contents (e.g., to read a time stamp in the\nmetadata) should be avoided. Mailman\u2019s solution was to extend the \ufb01le naming algorithm to include a time stamp pre\ufb01x, as\nthe number of seconds since the epoch (e.g., <timestamp>+<sha1hash>.pck). Each loop through\nthe queue runner starts by doing an os.listdir(), which returns all the \ufb01les in the queue directory. 7http://tools.ietf.org/html/rfc2033\n154\nGNU Mailman", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 168", "position": 168, "chunk_type": "semantic", "token_estimate": 319}
{"text": "Then for each \ufb01le, it splits the \ufb01le name and ignores any \ufb01le names where the SHA1 hash doesn\u2019t: match its slice of responsibility. The runner then sorts the remaining \ufb01les based on the timestamp\npart of the \ufb01le name. It\u2019s true that with multiple queue runners each managing di\ufb00erent slices of the\nhash space, this could lead to ordering problems between the parallel runners, but in practice, the\ntimestamp ordering is enough to preserve end-user perception of best-e\ufb00ort sequential delivery. In practice this has worked extremely well for at least a decade, with only the occasional minor\nbug \ufb01x or elaboration to handle obscure corner cases and failure modes. It\u2019s one of the most stable\nparts of Mailman and was largely ported untouched from Mailman 2 to Mailman 3. 10.4\nThe Master Runner\nWith all these runner processes, Mailman needed a simple way to start and stop them consistently;\nthus the master watcher process was born. It must be able to handle both queue runners and runners\nwhich do not manage a queue. For example, in Mailman 3, we accept messages from the incoming\nupstream mail server via LMTP, which is a protocol similar to SMTP, but which operates only for\nlocal delivery and thus can be much simpler as it doesn\u2019t need to deal with the vagaries of delivering\nmail over an unpredictable Internet. The LMTP runner simply listens on a port, waiting for its\nupstream mail server to connect and send it a byte stream. It then parses this byte stream into a\nmessage object tree, creates an initial metadata dictionary, and enqueues this into a processing queue\ndirectory. Mailman also has a runner that listens on another port and processes REST requests over HTTP. This process doesn\u2019t handle queue \ufb01les at all. A typical running Mailman system might have eight or ten processes, and they all need to be\nstopped and started appropriately and conveniently. They can also crash occasionally; for example,\nwhen a bug in Mailman causes an unexpected exception to occur. When this happens, the message\nbeing delivered is shunted to a holding area, with the state of the system at the time of the exception\npreserved in the message metadata. This ensures that an uncaught exception does not cause multiple\ndeliveries of the message. In theory, the Mailman site administrator could \ufb01x the problem, and\nthen unshunt the o\ufb00ending messages for redelivery, picking up where it left o\ufb00.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 169", "position": 169, "chunk_type": "semantic", "token_estimate": 405}
{"text": "Then for each \ufb01le, it splits the \ufb01le name and ignores any \ufb01le names where the SHA1 hash doesn\u2019t: This ensures that an uncaught exception does not cause multiple\ndeliveries of the message. In theory, the Mailman site administrator could \ufb01x the problem, and\nthen unshunt the o\ufb00ending messages for redelivery, picking up where it left o\ufb00. After shunting the\nproblematic message, the master restarts the crashed queue runner, which begins processing the\nremaining messages in its queue. When the master watcher starts, it looks in a con\ufb01guration \ufb01le to determine how many and which\ntypes of child runners to start. For the LMTP and REST runners, there is usually a single process. For the queue runners, as mentioned above, there can be a power-of-2 number of parallel processes. The master fork()s and exec()s all the runner processes based on the con\ufb01guration \ufb01le, passing in\nthe appropriate command line arguments to each (e.g., to tell the subprocess which slice of the hash\nspace to look at). Then the master basically sits in an in\ufb01nite loop, blocking until one of its child\nprocesses exits. It keeps track of the process ID for each child, along with a count of the number of\ntimes the child has been restarted. This count prevents a catastrophic bug from causing a cascade of\nunstoppable restarts. There\u2019s a con\ufb01guration variable which speci\ufb01es how many restarts are allowed,\nafter which an error is logged and the runner is not restarted. When a child does exit, the master looks at both the exit code and the signal that killed the\nsubprocess. Each runner process installs a number of signal handlers with the following semantics:\n\u2022 SIGTERM: intentionally stop the subprocess. It is not restarted. SIGTERM is what init will kill\nthe process with when changing run levels, and it\u2019s also the signal that Mailman itself uses to\nstop the subprocess. Barry Warsaw\n155", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 169", "position": 169, "chunk_type": "semantic", "token_estimate": 315}
{"text": "\u2022 SIGINT: also used to intentionally stop the subprocess, it\u2019s the signal that occurs when control-C: is used in a shell. The runner is not restarted. \u2022 SIGHUP: tells the process to close and reopen its log \ufb01les, but to keep running. This is used\nwhen rotating log \ufb01les. \u2022 SIGUSR1: initially stop the subprocess, but allow the master to restart the process. This is used\nin the restart command of init scripts. The master also responds to all four of these signals, but it doesn\u2019t do much more than forward\nthem to all its subprocesses. So if you sent SIGTERM to the master, all the subprocesses would get\nSIGTERM\u2019d and exit. The master would know that the subprocess exited because of SIGTERM and it\nwould know that this was an intentional stoppage, so it would not restart the runner. To ensure that only one master is running at any time, it acquires a lock with a lifetime of about a\nday and a half. The master installs a SIGALRM handler, which wakes the master up once per day so\nthat it can refresh the lock. Because the lock\u2019s lifetime is longer than the wake up interval, the lock\nshould never time out or be broken while Mailman is running, unless of course the system crashes or\nthe master is killed with an uncatchable signal. In those cases, the command line interface to the\nmaster process provides an option to override a stale lock. This leads to the last bit of the master watcher story, the command line interface to it. The\nactual master script takes very few command line options. Both it and the queue runner scripts are\nintentionally kept simple. This wasn\u2019t the case in Mailman 2, where the master script was fairly\ncomplex and tried to do too much, which made it more di\ufb03cult to understand and debug. In Mailman\n3, the real command line interface for the master process is in the bin/mailman script, a kind of\nmeta-script that contains a number of subcommands, in a style made popular by programs like\nSubversion. This reduces the number of programs that need to be installed on your shell\u2019s PATH. bin/mailman has subcommands to start, stop, and restart the master, as well as all the subprocesses,\nand also to cause all the log \ufb01les to be reopened.", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 170", "position": 170, "chunk_type": "semantic", "token_estimate": 389}
{"text": "The pipeline-of-handlers architecture proved to be quite powerful. It provided an easy way that: people could extend and modify Mailman to do custom operations. The interface for a handler was\nfairly straightforward, and it was a simple matter to implement a new handler, ensuring it got added\nto the right pipeline in the right location to accomplish the custom operation. One problem with this was that mixing moderation and modi\ufb01cation in the same pipeline became\nproblematic. The handlers had to be sequenced in the pipeline just so, or unpredictable or undesirable\nthings would happen. For example, if the handler that added the RFC 23698 List-* headers came\nafter the handler to copy the message to the digest collator, then folks receiving digests would get\nincorrect copies of the list posts. In di\ufb00erent cases, it might be bene\ufb01cial to moderate the message\nbefore or after modifying it. In Mailman 3, the moderation and modi\ufb01cation operations have been\nsplit into separate subsystems for better control over the sequencing. As described previously, the LMTP runner parses an incoming byte stream into a message object\ntree and creates an initial metadata dictionary for the message. It then enqueues these to one or\nanother queue directory. Some messages may be email commands (e.g., to join or leave a mailing\nlist, to get automated help, etc.) which are handled by a separate queue. Most messages are postings\nto the mailing list, and these get put in the incoming queue. The incoming queue runner processes\neach message sequentially through a chain consisting of any number of links. There is a built-in\nchain that most mailing lists use, but even this is con\ufb01gurable. Figure 10.3 illustrates the default set of chains in the Mailman 3 system. Each link in the chain is\nillustrated by a rounded rectangle. The built-in chain is where the initial rules of moderation are\napplied to the incoming message, and in this chain, each link is associated with a rule. Rules are\nsimply pieces of code that get passed the three typical parameters: the mailing list, the message\nobject tree, and the metadata dictionary. Rules are not supposed to modify the message; they just\nmake a binary decision and return a Boolean answering the question, \u201cDid the rule match or not?\u201d. Rules can also record information in the metadata dictionary. In the \ufb01gure, solid arrows indicates message \ufb02ow when the rule matches, while dotted arrows\nindicate message \ufb02ow when the rule does not match.", "domains": ["Design Patterns", "Design Principles", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 171", "position": 171, "chunk_type": "semantic", "token_estimate": 410}
{"text": "The pipeline-of-handlers architecture proved to be quite powerful. It provided an easy way that: Rules can also record information in the metadata dictionary. In the \ufb01gure, solid arrows indicates message \ufb02ow when the rule matches, while dotted arrows\nindicate message \ufb02ow when the rule does not match. The outcome of each rule is recorded in\nthe metadata dictionary so that later on, Mailman will know (and be able to report) exactly which\nrules matched and which ones missed. The dashed arrows indication transitions which are taken\nunconditionally, regardless of whether the rule matches or not. It\u2019s important to note that the rules themselves do not dispatch based on outcome. In the built-in\nchain, each link is associated with an action which is performed when the rule matches. So for\nexample, when the \u201cloop\u201d rule matches (meaning, the mailing list has seen this message before), the\nmessage is immediate handed o\ufb00to the \u201cdiscard\u201d chain, which throws the message away after some\nbookkeeping. If the \u201cloop\u201d rule does not match, the next link in the chain will process the message. In Figure 10.3, the links associated with \u201cadministrivia\u201d, \u201cmax-size\u201d, and \u201ctruth\u201d rules have\nno binary decision. In case of the \ufb01rst two, this is because their action is deferred, so they simply\nrecord the match outcome and processing continues to the next link. The \u201cany\u201d rule then matches if\nany previous rule matches. This way, Mailman can report on all the reasons why a message is not\nallowed to be posted, instead of just the \ufb01rst reason. There are several more such rules not illustrated\nhere for simplicity. The \u201ctruth\u201d rule is a bit di\ufb00erent. It\u2019s always associated with the last link in the chain, and it\nalways matches. With the combination of the penultimate \u201cany\u201d rule sweeping aside all previously\nmatching messages, the last link then knows that any message making it this far is allowed to be\nposted to the mailing list, so it unconditionally moves the message to the \u201caccept\u201d chain. 8http://www.faqs.org/rfcs/rfc2369.html\nBarry Warsaw\n157", "domains": ["Design Principles"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 171", "position": 171, "chunk_type": "semantic", "token_estimate": 336}
{"text": "10.6: Handlers and Pipelines\nOnce a message has made its way through the chains and rules and is accepted for posting, the\nmessage must be further processed before it can be delivered to the \ufb01nal recipients. For example, some\nheaders may get added or deleted, and some messages may get some extra decorations that provide\nimportant disclaimers or information, such as how to leave the mailing list. These modi\ufb01cations\nare performed by a pipeline which contains a sequence of handlers. In a manner similar to chains\nand rules, pipelines and handlers are extensible, but there are a number of built-in pipelines for the\ncommon cases. Handlers have a similar interface as rules, accepting a mailing list, message object,\nand metadata dictionary. However, unlike rules, handlers can and do modify the message. Figure 10.4\nillustrates the default pipeline and set of handlers (some handlers are omitted for simplicity). Figure 10.4: Pipeline queue handlers\nFor example, a posted message needs to have a Precedence: header added, which tells other\nautomated software that this message came from a mailing list. This header is a de facto standard to\nprevent vacation programs from responding back to the mailing list. Adding this header (among other\nheader modi\ufb01cations) is done by the \u201cadd headers\u201d handler. Unlike rules, handler order generally\ndoesn\u2019t matter, and messages always \ufb02ow through all handlers in the pipeline. Some handlers send copies of the message to other queues. As shown in Figure 10.4, there is a\nhandler that makes a copy of the message for folks who want to receive digests. Copies are also sent\nBarry Warsaw\n159", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 173", "position": 173, "chunk_type": "semantic", "token_estimate": 266}
{"text": "One of the key architectural changes in Mailman 3 addresses a common request over the years: to: allow Mailman to be more easily integrated with external systems. When I was hired by Canonical,\nthe corporate sponsor of the Ubuntu project, in 2007 my job was originally to add mailing lists to\nLaunchpad, a collaboration and hosting platform for software projects. I knew that Mailman 2 could\ndo the job, but there was a requirement to use Launchpad\u2019s web user interface instead of Mailman\u2019s\ndefault user interface. Since Launchpad mailing lists were almost always going to be discussion\nlists, we wanted very little variability in the way they operated. List administrators would not need\nthe plethora of options available in the typical Mailman site, and what few options they would need\nwould be exposed through the Launchpad web user interface. At the time, Launchpad was not free software (this changed in 2009), so we had to design the\nintegration in such a way that Mailman 2\u2019s GPLv2 code could not infect Launchpad. This led\nto a number of architectural decisions during that integration design that were quite tricky and\nsomewhat ine\ufb03cient. Because Launchpad is now free software licensed under the AGPLv3, these\nhacks wouldn\u2019t be necessary today, but having to do it this way did provide some very valuable\nlessons on how a web-user-interface-less Mailman could be integrated with external systems. The\nvision that emerged was of a core engine that implemented mailing list operations e\ufb03ciently and\nreliably, and that could be managed by any kind of web front-end, including ones written in Zope,\nDjango, or PHP, or with no web user interface at all. There were a number of technologies at the time that would allow this, and in fact Mailman\u2019s\nintegration with Launchpad is based on XMLRPC. But XMLRPC has a number of problems that\nmake it a less-than-ideal protocol. Mailman 3 has adopted the Representational State Transfer (REST) model for external admin-\nistrative control. REST is based on HTTP, and Mailman\u2019s default object representation is JSON. These protocols are ubiquitous and well-supported in a large variety of programming languages and\nenvironments, making it fairly easy to integrate Mailman with third party systems. REST was the\nperfect \ufb01t for Mailman 3, and now much of its functionality is exposed through a REST API. This is a powerful paradigm that more applications should adopt: deliver a core engine that\nimplements its basic functionality well, exposing a REST API to query and control it.", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 175", "position": 175, "chunk_type": "semantic", "token_estimate": 413}
{"text": "One of the key architectural changes in Mailman 3 addresses a common request over the years: to: REST was the\nperfect \ufb01t for Mailman 3, and now much of its functionality is exposed through a REST API. This is a powerful paradigm that more applications should adopt: deliver a core engine that\nimplements its basic functionality well, exposing a REST API to query and control it. The REST API\nprovides yet another way of integrating with Mailman, the others being utilizing the command line\ninterface, and writing Python code to access the internal API. This architecture is extremely \ufb02exible\nand can be used and integrated in ways that are beyond the initial vision of the system designers. Not only does this design allow for much greater choices for deployment, but it even allowed the\no\ufb03cial components of the system to be designed and implemented independently. For example, the\nnew o\ufb03cial web user interface for Mailman 3 is technically a separate project with its own code base,\ndriven primarily by experienced web designers. These outstanding developers are empowered to\nmake decisions, create designs, and execute implementations without the core engine development\nBarry Warsaw\n161", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 175", "position": 175, "chunk_type": "semantic", "token_estimate": 193}
{"text": "being a bottleneck. The web user interface work feeds back into the core engine implementation: by requesting additional functionality, exposed through the REST API, but they needn\u2019t wait for it,\nsince they can mock up the server side on their end and continue experimenting and developing the\nweb user interface while the core engine catches up. We plan to use the REST API for many more things, including allowing the scripting of common\noperations and integration with IMAP or NNTP servers for alternative access to the archives. 10.9\nInternationalization\nGNU Mailman was one of the \ufb01rst Python programs to embrace internationalization. Of course,\nbecause Mailman does not usually modify the contents of email messages posted through it, those\nmessages can be in any language of the original author\u2019s choosing. However, when interacting\ndirectly with Mailman, either through the web interface or via email commands, users would prefer\nto use their own natural language. Mailman pioneered many of the internationalization technologies used in the Python world, but it\nis actually much more complex than most applications. In a typical desktop environment, the natural\nlanguage is chosen when the user logs in, and remains static throughout the desktop session. However,\nMailman is a server application, so it must be able to handle dozens of languages, separate from the\nlanguage of the system on which it runs. In fact, Mailman must somehow determine the language\ncontext that a response is to be returned under, and translate its text to that language. Sometimes a\nresponse may even involve multiple languages; for example, if a bounce message from a Japanese\nuser is to be forwarded to list administrators who speak German, Italian, and Catalan. Again, Mailman pioneered some key Python technologies to handle complex language contexts\nsuch as these. It utilizes a library that manages a stack of languages which can be pushed onto\nand popped from as the context changes, even within the processing of a single message. It also\nimplements an elaborate scheme for customizing its response templates based on site preferences, list\nowner preferences, and language choice. For example, if a list owner wants to customize a response\ntemplate for one of her lists, but only for Japanese users, she would place the speci\ufb01c template in the\nappropriate place on the \ufb01le system, and this would override more generic defaults.", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 176", "position": 176, "chunk_type": "semantic", "token_estimate": 388}
{"text": "being a bottleneck. The web user interface work feeds back into the core engine implementation: It also\nimplements an elaborate scheme for customizing its response templates based on site preferences, list\nowner preferences, and language choice. For example, if a list owner wants to customize a response\ntemplate for one of her lists, but only for Japanese users, she would place the speci\ufb01c template in the\nappropriate place on the \ufb01le system, and this would override more generic defaults. 10.10\nLessons Learned\nWhile this article has provided an overview of Mailman 3\u2019s architecture and insight into how that\narchitecture has evolved over the 15 years of its existence (through three major rewrites), there are\nlots of other interesting architectural decisions in Mailman which I can\u2019t cover. These include the\ncon\ufb01guration subsystem, the testing infrastructure, the database layer, the programmatic use of\nformal interfaces, archiving, mailing list styles, the email commands and command-line interface,\nand integration with the outgoing mail server. Contact us on the mailman-developers mailing list12\nif you\u2019re interested in more details. Here are some lessons we\u2019ve learned while rewriting a popular, established, and stable piece of\nthe open source ecosystem. \u2022 Use test driven development (TDD). There really is no other way! Mailman 2 largely lacks an\nautomated test suite, and while it\u2019s true that not all of the Mailman 3 code base is covered by\nits test suite, most of it is, and all new code is required to be accompanied by tests, using either\n12http://mail.python.org/mailman/listinfo/mailman-developers\n162\nGNU Mailman", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 176", "position": 176, "chunk_type": "semantic", "token_estimate": 252}
{"text": "[chapter11]: matplotlib\nJohn Hunter and Michael Droettboom\nmatplotlib is a Python-based plotting library with full support for 2D and limited support for 3D\ngraphics, widely used in the Python scienti\ufb01c computing community. The library targets a broad\nrange of use cases. It can embed graphics in the user interface toolkit of your choice, and currently\nsupports interactive graphics on all major desktop operating systems using the GTK+, Qt, Tk, FLTK,\nwxWidgets and Cocoa toolkits. It can be called interactively from the interactive Python shell to\nproduce graphics with simple, procedural commands, much like Mathematica, IDL or MATLAB. matplotlib can also be embedded in a headless webserver to provide hardcopy in both raster-based\nformats like Portable Network Graphics (PNG) and vector formats like PostScript, Portable Document\nFormat (PDF) and Scalable Vector Graphics (SVG) that look great on paper. 11.1\nThe Dongle Problem\nmatplotlib\u2019s origin dates to an attempt by one of us (John Hunter) to free himself and his fellow\nepilepsy researchers from a proprietary software package for doing electrocorticography (ECoG)\nanalysis. The laboratory in which he worked had only one license for the software, and the various\ngraduate students, medical students, postdocs, interns, and investigators took turns sharing the\nhardware key dongle. MATLAB is widely used in the biomedical community for data analysis\nand visualization, so Hunter set out, with some success, to replace the proprietary software with a\nMATLAB-based version that could be utilized and extended by multiple investigators. MATLAB,\nhowever, naturally views the world as an array of \ufb02oating point numbers, and the complexities of\nreal-world hospital records for epilepsy surgery patients with multiple data modalities (CT, MRI,\nECoG, EEG) warehoused on di\ufb00erent servers pushed MATLAB to its limits as a data management\nsystem. Unsatis\ufb01ed with the suitability of MATLAB for this task, Hunter began working on a new\nPython application built on top of the user interface toolkit GTK+, which was at the time the leading\ndesktop windowing system for Linux. matplotlib was thus originally developed as an EEG/ECoG visualization tool for this GTK+\napplication, and this use case directed its original architecture. matplotlib was originally designed to\nserve a second purpose as well: as a replacement for interactive command-driven graphics generation,\nsomething that MATLAB does very well. The MATLAB design makes the simple task of loading a\ndata \ufb01le and plotting very straightforward, where a full object-oriented API would be too syntactically\nheavy.", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 179", "position": 179, "chunk_type": "semantic", "token_estimate": 397}
{"text": "[chapter11]: matplotlib was originally designed to\nserve a second purpose as well: as a replacement for interactive command-driven graphics generation,\nsomething that MATLAB does very well. The MATLAB design makes the simple task of loading a\ndata \ufb01le and plotting very straightforward, where a full object-oriented API would be too syntactically\nheavy. So matplotlib also provides a stateful scripting interface for quick and easy generation of\ngraphics similar to MATLAB\u2019s. Because matplotlib is a library, users have access to all of the rich\nbuilt-in Python data structures such as lists, dictionaries, sets and more.", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 179", "position": 179, "chunk_type": "semantic", "token_estimate": 94}
{"text": "Figure 11.1: The original matplotlib application: an ECoG viewer: 11.2\nOverview of matplotlib Architecture\nThe top-level matplotlib object that contains and manages all of the elements in a given graphic\nis called the Figure. One of the core architectural tasks matplotlib must solve is implementing a\nframework for representing and manipulating the Figure that is segregated from the act of rendering\nthe Figure to a user interface window or hardcopy. This enables us to build increasingly sophisticated\nfeatures and logic into the Figures, while keeping the \u201cbackends\u201d, or output devices, relatively\nsimple. matplotlib encapsulates not just the drawing interfaces to allow rendering to multiple devices,\nbut also the basic event handling and windowing of most popular user interface toolkits. Because of\nthis, users can create fairly rich interactive graphics and toolkits incorporating mouse and keyboard\ninput that can be plugged without modi\ufb01cation into the six user interface toolkits we support. The architecture to accomplish this is logically separated into three layers, which can be viewed\nas a stack. Each layer that sits above another layer knows how to talk to the layer below it, but the\nlower layer is not aware of the layers above it. The three layers from bottom to top are: backend,\nartist, and scripting. 166\nmatplotlib", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 180", "position": 180, "chunk_type": "semantic", "token_estimate": 210}
{"text": "Backend Layer: At the bottom of the stack is the backend layer, which provides concrete implementations of the\nabstract interface classes:\n\u2022 FigureCanvas encapsulates the concept of a surface to draw onto (e.g. \u201cthe paper\u201d). \u2022 Renderer does the drawing (e.g. \u201cthe paintbrush\u201d). \u2022 Event handles user inputs such as keyboard and mouse events. For a user interface toolkit such as Qt, the FigureCanvas has a concrete implementation which\nknows how to insert itself into a native Qt window (QtGui.QMainWindow), transfer the matplotlib\nRenderer commands onto the canvas (QtGui.QPainter), and translate native Qt events into the\nmatplotlib Event framework, which signals the callback dispatcher to generate the events so upstream\nlisteners can handle them. The abstract base classes reside in matplotlib.backend_bases and\nall of the derived classes live in dedicated modules like matplotlib.backends.backend_qt4agg. For a pure image backend dedicated to producing hardcopy output like PDF, PNG, SVG, or PS, the\nFigureCanvas implementation might simply set up a \ufb01le-like object into which the default headers,\nfonts, and macro functions are de\ufb01ned, as well as the individual objects (lines, text, rectangles, etc.) that the Renderer creates. The job of the Renderer is to provide a low-level drawing interface for putting ink onto the\ncanvas. As mentioned above, the original matplotlib application was an ECoG viewer in a GTK+\napplication, and much of the original design was inspired by the GDK/GTK+ API available at\nthat time. The original Renderer API was motivated by the GDK Drawable interface, which\nimplements such primitive methods as draw_point, draw_line, draw_rectangle, draw_image,\ndraw_polygon, and draw_glyphs. Each additional backend we implemented\u2014the earliest were\nthe PostScript backend and the GD backend\u2014implemented the GDK Drawable API and translated\nthese into native backend-dependent drawing commands. As we discuss below, this unnecessarily\ncomplicated the implementation of new backends with a large proliferation of methods, and this API\nhas subsequently been dramatically simpli\ufb01ed, resulting in a simple process for porting matplotlib to\na new user interface toolkit or \ufb01le speci\ufb01cation. One of the design decisions that has worked quite well for matplotlib is support for a core\npixel-based renderer using the C++ template library Anti-Grain Geometry or \u201cagg\u201d [She06]. This is\na high-performance library for rendering anti-aliased 2D graphics that produces attractive images. matplotlib provides support for inserting pixel bu\ufb00ers rendered by the agg backend into each user\ninterface toolkit we support, so one can get pixel-exact graphics across UIs and operating systems.", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 181", "position": 181, "chunk_type": "semantic", "token_estimate": 399}
{"text": "Backend Layer: This is\na high-performance library for rendering anti-aliased 2D graphics that produces attractive images. matplotlib provides support for inserting pixel bu\ufb00ers rendered by the agg backend into each user\ninterface toolkit we support, so one can get pixel-exact graphics across UIs and operating systems. Because the PNG output matplotlib produces also uses the agg renderer, the hardcopy is identical to\nthe screen display, so what you see is what you get across UIs, operating systems and PNG output. The matplotlib Event framework maps underlying UI events like key-press-event or\nmouse-motion-event to the matplotlib classes KeyEvent or MouseEvent. Users can connect to\nthese events to callback functions and interact with their \ufb01gure and data; for example, to pick a data\npoint or group of points, or manipulate some aspect of the \ufb01gure or its constituents. The following\ncode sample illustrates how to toggle all of the lines in an Axes window when the user types \u2018t\u2019. import numpy as np\nimport matplotlib.pyplot as plt\ndef on_press(event):\nif event.inaxes is None: return\nfor line in event.inaxes.lines:\nif event.key==\u2019t\u2019:\nJohn Hunter and Michael Droettboom\n167", "domains": ["Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 181", "position": 181, "chunk_type": "semantic", "token_estimate": 184}
{"text": "visible = line.get_visible(): line.set_visible(not visible)\nevent.inaxes.figure.canvas.draw()\nfig, ax = plt.subplots(1)\nfig.canvas.mpl_connect(\u2019key_press_event\u2019, on_press)\nax.plot(np.random.rand(2, 20))\nplt.show()\nThe abstraction of the underlying UI toolkit\u2019s event framework allows both matplotlib developers\nand end-users to write UI event-handling code in a \u201cwrite once run everywhere\u201d fashion. For example,\nthe interactive panning and zooming of matplotlib \ufb01gures that works across all user interface toolkits\nis implemented in the matplotlib event framework. Artist Layer\nThe Artist hierarchy is the middle layer of the matplotlib stack, and is the place where much of the\nheavy lifting happens. Continuing with the analogy that the FigureCanvas from the backend is the\npaper, the Artist is the object that knows how to take the Renderer (the paintbrush) and put ink on\nthe canvas. Everything you see in a matplotlib Figure is an Artist instance; the title, the lines, the\ntick labels, the images, and so on all correspond to individual Artist instances (see Figure 11.3). The base class is matplotlib.artist.Artist, which contains attributes that every Artist shares:\nthe transformation which translates the artist coordinate system to the canvas coordinate system\n(discussed in more detail below), the visibility, the clip box which de\ufb01nes the region the artist can\npaint into, the label, and the interface to handle user interaction such as \u201cpicking\u201d; that is, detecting\nwhen a mouse click happens over the artist. Figure 11.2: A \ufb01gure\n168\nmatplotlib", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 182", "position": 182, "chunk_type": "semantic", "token_estimate": 228}
{"text": "create some objects and use renderer to draw self here: renderer.draw_path(graphics_context, path, transform)\nThere are two types of Artists in the hierarchy. Primitive artists represent the kinds of objects you\nsee in a plot: Line2D, Rectangle, Circle, and Text. Composite artists are collections of Artists\nsuch as the Axis, Tick, Axes, and Figure. Each composite artist may contain other composite\nartists as well as primitive artists. For example, the Figure contains one or more composite Axes\nand the background of the Figure is a primitive Rectangle. The most important composite artist is the Axes, which is where most of the matplotlib API\nplotting methods are de\ufb01ned. Not only does the Axes contain most of the graphical elements that\nmake up the background of the plot\u2014the ticks, the axis lines, the grid, the patch of color which is\nthe plot background\u2014it contains numerous helper methods that create primitive artists and add them\nto the Axes instance. For example, Table 11.1 shows a small sampling of Axes methods that create\nplot objects and store them in the Axes instance. John Hunter and Michael Droettboom\n169", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 183", "position": 183, "chunk_type": "semantic", "token_estimate": 183}
{"text": "Decorate the figure with a title and save it.: ax.set_title(\u2019Normal distribution with $\\mu=0,\\ \\sigma=1$\u2019)\nfig.savefig(\u2019matplotlib_histogram.png\u2019)\nScripting Layer (pyplot)\nThe script using the API above works very well, especially for programmers, and is usually the\nappropriate programming paradigm when writing a web application server, a UI application, or\nperhaps a script to be shared with other developers. For everyday purposes, particularly for interactive\nexploratory work by bench scientists who are not professional programmers, it is a bit syntactically\nheavy. Most special-purpose languages for data analysis and visualization provide a lighter scripting\ninterface to simplify common tasks, and matplotlib does so as well in its matplotlib.pyplot\ninterface. The same code above, using pyplot, reads\nimport matplotlib.pyplot as plt\nimport numpy as np\n170\nmatplotlib", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 184", "position": 184, "chunk_type": "semantic", "token_estimate": 122}
{"text": "x = np.random.randn(10000): plt.hist(x, 100)\nplt.title(r\u2019Normal distribution with $\\mu=0, \\sigma=1$\u2019)\nplt.savefig(\u2019matplotlib_histogram.png\u2019)\nplt.show()\nFigure 11.4: A histogram created using pyplot\npyplot is a stateful interface that handles much of the boilerplate for creating \ufb01gures and axes and\nconnecting them to the backend of your choice, and maintains module-level internal data structures\nrepresenting the current \ufb01gure and axes to which to direct plotting commands. Let\u2019s dissect the important lines in the script to see how this internal state is managed. \u2022 import matplotlib.pyplot as plt: When the pyplot module is loaded, it parses a local\ncon\ufb01guration \ufb01le in which the user states, among many other things, their preference for a\ndefault backend. This might be a user interface backend like QtAgg, in which case the script\nabove will import the GUI framework and launch a Qt window with the plot embedded, or it\nmight be a pure image backend like Agg, in which case the script will generate the hard-copy\noutput and exit. \u2022 plt.hist(x, 100): This is the \ufb01rst plotting command in the script. pyplot will check its\ninternal data structures to see if there is a current Figure instance. If so, it will extract the\ncurrent Axes and direct plotting to the Axes.hist API call. In this case there is none, so it\nwill create a Figure and Axes, set these as current, and direct the plotting to Axes.hist. \u2022 plt.title(r\u2019Normal distribution with $\\mu=0, \\sigma=1$\u2019): As above, pyplot will\nlook to see if there is a current Figure and Axes. Finding that there is, it will not create new\ninstances but will direct the call to the existing Axes instance method Axes.set_title. \u2022 plt.show(): This will force the Figure to render, and if the user has indicated a default GUI\nbackend in their con\ufb01guration \ufb01le, will start the GUI mainloop and raise any \ufb01gures created to\nthe screen. John Hunter and Michael Droettboom\n171", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 185", "position": 185, "chunk_type": "semantic", "token_estimate": 314}
{"text": "A somewhat stripped-down and simpli\ufb01ed version of pyplot\u2019s frequently used line plotting: function matplotlib.pyplot.plot is shown below to illustrate how a pyplot function wraps func-\ntionality in matplotlib\u2019s object-oriented core. All other pyplot scripting interface functions follow\nthe same design. @autogen_docstring(Axes.plot)\ndef plot(*args, **kwargs):\nax = gca()\nret = ax.plot(*args, **kwargs)\ndraw_if_interactive()\nreturn ret\nThe Python decorator @autogen_docstring(Axes.plot) extracts the documentation string\nfrom the corresponding API method and attaches a properly formatted version to the pyplot.plot\nmethod; we have a dedicated module matplotlib.docstring to handle this docstring magic. The\n*args and **kwargs in the documentation signature are special conventions in Python to mean all\nthe arguments and keyword arguments that are passed to the method. This allows us to forward them\non to the corresponding API method. The call ax = gca() invokes the stateful machinery to \u201cget\ncurrent Axes\u201d (each Python interpreter can have only one \u201ccurrent axes\u201d), and will create the Figure\nand Axes if necessary. The call to ret = ax.plot(*args, **kwargs) forwards the function call\nand its arguments to the appropriate Axes method, and stores the return value to be returned later. Thus the pyplot interface is a fairly thin wrapper around the core Artist API which tries to avoid\nas much code duplication as possible by exposing the API function, call signature and docstring in\nthe scripting interface with a minimal amount of boilerplate code. 11.3\nBackend Refactoring\nOver time, the drawing API of the output backends grew a large number of methods, including:\ndraw_arc, draw_image, draw_line_collection, draw_line, draw_lines, draw_point,\ndraw_quad_mesh, draw_polygon_collection, draw_polygon, draw_rectangle,\ndraw_regpoly_collection\nUnfortunately, having more backend methods meant it took much longer to write a new backend,\nand as new features were added to the core, updating the existing backends took considerable work. Since each of the backends was implemented by a single developer who was expert in a particular\noutput \ufb01le format, it sometimes took a long time for a new feature to arrive in all of the backends,\ncausing confusion for the user about which features were available where. For matplotlib version 0.98, the backends were refactored to require only the minimum necessary\nfunctionality in the backends themselves, with everything else moved into the core. The number of\nrequired methods in the backend API was reduced considerably, to only:\n\u2022 draw_path: Draws compound polygons, made up of line and B\u00e9zier segments. This interfaces\nreplaces many of the old methods: draw_arc, draw_line, draw_lines, and draw_rectangle. \u2022 draw_image: Draws raster images.", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 186", "position": 186, "chunk_type": "semantic", "token_estimate": 409}
{"text": "It\u2019s possible to implement all of the drawing necessary for a new backend using only these: methods.1 This is useful for getting a new backend up and running more easily. However, in some\ncases, a backend may want to override the behavior of the core in order to create more e\ufb03cient\noutput. For example, when drawing markers (small symbols used to indicate the vertices in a line\nplot), it is more space-e\ufb03cient to write the marker\u2019s shape only once to the \ufb01le, and then repeat it as\na \u201cstamp\u201d everywhere it is used. In that case, the backend can implement a draw_markers method. If it\u2019s implemented, the backend writes out the marker shape once and then writes out a much shorter\ncommand to reuse it in a number of locations. If it\u2019s not implemented, the core simply draws the\nmarker multiple times using multiple calls to draw_path. The full list of optional backend API methods is:\n\u2022 draw_markers: Draws a set of markers. \u2022 draw_path_collection: Draws a collection of paths. \u2022 draw_quad_mesh: Draws a quadrilateral mesh. 11.4\nTransforms\nmatplotlib spends a lot of time transforming coordinates from one system to another. These coordinate\nsystems include:\n\u2022 data: the original raw data values\n\u2022 axes: the space de\ufb01ned by a particular axes rectangle\n\u2022 \ufb01gure: the space containing the entire \ufb01gure\n\u2022 display: the physical coordinates used in the output (e.g. points in PostScript, pixels in PNG)\nEvery Artist has a transformation node that knows how to transform from one coordinate system\nto another. These transformation nodes are connected together in a directed graph, where each node is\ndependent on its parent. By following the edges to the root of the graph, coordinates in data space can\nbe transformed all the way to coordinates in the \ufb01nal output \ufb01le. Most transformations are invertible,\nas well. This makes it possible to click on an element of the plot and return its coordinate in data\nspace. The transform graph sets up dependencies between transformation nodes: when a parent\nnode\u2019s transformation changes, such as when an Axes\u2019s limits are changed, any transformations\nrelated to that Axes are invalidated since they will need to be redrawn. Transformations related to\nother Axes in the \ufb01gure, of course, may be left alone, preventing unnecessary recomputations and\ncontributing to better interactive performance. Transform nodes may be either simple a\ufb03ne transformations and non-a\ufb03ne transformations. A\ufb03ne transformations are the family of transformations that preserve straight lines and ratios of\ndistances, including rotation, translation, scale and skew.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 187", "position": 187, "chunk_type": "semantic", "token_estimate": 416}
{"text": "Using this approach, we were able to bootstrap a reasonably e\ufb00ective testing framework from: scratch more easily than if we had gone on to write many low-level unit tests. Still, it is not perfect;\nthe code coverage of the tests is not very complete, and it takes a long time to run all of the tests.2\nTherefore, some regressions do still fall through the cracks, but overall the quality of the releases has\nimproved considerably since the testing framework was implemented. 11.8\nLessons Learned\nOne of the important lessons from the development of matplotlib is, as Le Corbusier said, \u201cGood\narchitects borrow\u201d. The early authors of matplotlib were largely scientists, self-taught programmers\ntrying to get their work done, not formally trained computer scientists. Thus we did not get the\ninternal design right on the \ufb01rst try. The decision to implement a user-facing scripting layer largely\ncompatible with the MATLAB API bene\ufb01ted the project in three signi\ufb01cant ways: it provided a\ntime-tested interface to create and customize graphics, it made for an easy transition to matplotlib\nfrom the large base of MATLAB users, and\u2014most importantly for us in the context of matplotlib\narchitecture\u2014it freed developers to refactor the internal object-oriented API several times with\nminimal impact to most users because the scripting interface was unchanged. While we have had\nAPI users (as opposed to scripting users) from the outset, most of them are power users or developers\nable to adapt to API changes. The scripting users, on the other hand, can write code once and pretty\nmuch assume it is stable for all subsequent releases. For the internal drawing API, while we did borrow from GDK, we did not spend enough\ne\ufb00ort determining whether this was the right drawing API, and had to expend considerable e\ufb00ort\nsubsequently after many backends were written around this API to extend the functionality around\na simpler and more \ufb02exible drawing API. We would have been well-served by adopting the PDF\ndrawing speci\ufb01cation [Ent11b], which itself was developed from decades of experience Adobe had\nwith its PostScript speci\ufb01cation; it would have given us mostly out-of-the-box compatibility with PDF\nitself, the Quartz Core Graphics framework, and the Enthought Enable Kiva drawing kit [Ent11a]. One of the curses of Python is that it is such an easy and expressive language that developers\noften \ufb01nd it easier to re-invent and re-implement functionality that exists in other packages than work\nto integrate code from other packages.", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 192", "position": 192, "chunk_type": "semantic", "token_estimate": 406}
{"text": "[chapter12]: MediaWiki\nSumana Harihareswara and Guillaume Paumier\nFrom the start, MediaWiki was developed speci\ufb01cally to be Wikipedia\u2019s software. Developers\nhave worked to facilitate reuse by third-party users, but Wikipedia\u2019s in\ufb02uence and bias have shaped\nMediaWiki\u2019s architecture throughout its history. Wikipedia is one of the top ten websites in the world, currently getting about 400 million unique\nvisitors a month. It gets over 100,000 hits per second. Wikipedia isn\u2019t commercially supported by\nads; it is entirely supported by a non-pro\ufb01t organization, the Wikimedia Foundation, which relies on\ndonations as its primary funding model. This means that MediaWiki must not only run a top-ten\nwebsite, but also do so on a shoestring budget. To meet these demands, MediaWiki has a heavy\nbias towards performance, caching and optimization. Expensive features that can\u2019t be enabled on\nWikipedia are either reverted or disabled through a con\ufb01guration variable; there is an endless balance\nbetween performance and features. The in\ufb02uence of Wikipedia on MediaWiki\u2019s architecture isn\u2019t limited to performance. Unlike\ngeneric content management systems (CMSes), MediaWiki was originally written for a very speci\ufb01c\npurpose: supporting a community that creates and curates freely reusable knowledge on an open\nplatform. This means, for example, that MediaWiki doesn\u2019t include regular features found in corporate\nCMSes, like a publication work\ufb02ow or access control lists, but does o\ufb00er a variety of tools to handle\nspam and vandalism. So, from the start, the needs and actions of a constantly evolving community of Wikipedia\nparticipants have a\ufb00ected MediaWiki\u2019s development, and vice versa. The architecture of MediaWiki\nhas been driven many times by initiatives started or requested by the community, such as the creation\nof Wikimedia Commons, or the Flagged Revisions feature. Developers made major architectural\nchanges because the way that MediaWiki was used by Wikipedians made it necessary. MediaWiki has also gained a solid external user base by being open source software from the\nbeginning. Third-party reusers know that, as long as such a high-pro\ufb01le website as Wikipedia uses\nMediaWiki, the software will be maintained and improved. MediaWiki used to be really focused on\nWikimedia sites, but e\ufb00orts have been made to make it more generic and better accommodate the\nneeds of these third-party users. For example, MediaWiki now ships with an excellent web-based\ninstaller, making the installation process much less painful than when everything had to be done via\nthe command line and the software contained hardcoded paths for Wikipedia.", "domains": ["Design Patterns", "Design Principles", "Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 193", "position": 193, "chunk_type": "semantic", "token_estimate": 397}
{"text": "[chapter12]: MediaWiki used to be really focused on\nWikimedia sites, but e\ufb00orts have been made to make it more generic and better accommodate the\nneeds of these third-party users. For example, MediaWiki now ships with an excellent web-based\ninstaller, making the installation process much less painful than when everything had to be done via\nthe command line and the software contained hardcoded paths for Wikipedia. Still, MediaWiki is and remains Wikipedia\u2019s software, and this shows throughout its history and\narchitecture.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 193", "position": 193, "chunk_type": "semantic", "token_estimate": 80}
{"text": "This chapter is organized as follows:: UseModWiki also in\ufb02uenced some of MediaWiki\u2019s features; for example, its markup\nlanguage. The Nostalgia Wikipedia1 contains a complete copy of the Wikipedia database from\nDecember 2001, when Wikipedia still used UseModWiki. Phase II: The PHP Script\nIn 2001, Wikipedia was not yet a top ten website; it was an obscure project sitting in a dark corner of\nthe Interwebs, unknown to most search engines, and hosted on a single server. Still, performance\nwas already an issue, notably because UseModWiki stored its content in a \ufb02at \ufb01le database. At the\n1http://nostalgia.wikipedia.org\n180\nMediaWiki", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 194", "position": 194, "chunk_type": "semantic", "token_estimate": 98}
{"text": "time, Wikipedians were worried about being inundated with tra\ufb03c following articles in the New: York Times, Slashdot and Wired. So in summer 2001, Wikipedia participant Magnus Manske (then a university student) started\nto work on a dedicated Wikipedia wiki engine in his free time. He aimed to improve Wikipedia\u2019s\nperformance using a database-driven app, and to develop Wikipedia-speci\ufb01c features that couldn\u2019t\nbe provided by a \u201cgeneric\u201d wiki engine. Written in PHP and MySQL-backed, the new engine was\nsimply called the \u201cPHP script\u201d, \u201cPHP wiki\u201d, \u201cWikipedia software\u201d or \u201cphase II\u201d. The PHP script was made available in August 2001, shared on SourceForge in September, and\ntested until late 2001. As Wikipedia su\ufb00ered from recurring performance issues because of increasing\ntra\ufb03c, the English language Wikipedia eventually switched from UseModWiki to the PHP script in\nJanuary 2002. Other language versions also created in 2001 were slowly upgraded as well, although\nsome of them would remain powered by UseModWiki until 2004. As PHP software using a MySQL database, the PHP script was the \ufb01rst iteration of what would\nlater become MediaWiki. It introduced many critical features still in use today, like namespaces to\norganize content (including talk pages), skins, and special pages (including maintenance reports, a\ncontributions list and a user watchlist). Phase III: MediaWiki\nDespite the improvements from the PHP script and database backend, the combination of increasing\ntra\ufb03c, expensive features and limited hardware continued to cause performance issues on Wikipedia. In 2002, Lee Daniel Crocker rewrote the code again, calling the new software \u201cPhase III\u201d2. Because\nthe site was experiencing frequent di\ufb03culties, Lee thought there \u201cwasn\u2019t much time to sit down and\nproperly architect and develop a solution\u201d, so he \u201cjust reorganized the existing architecture for better\nperformance and hacked all the code\u201d. Pro\ufb01ling features were added to track down slow functions. The Phase III software kept the same basic interface, and was designed to look and behave as\nmuch like the Phase II software as possible. A few new features were also added, like a new \ufb01le\nupload system, side-by-side di\ufb00s of content changes, and interwiki links. Other features were added over 2002, like new maintenance special pages, and the \u201cedit on\ndouble click\u201d option. Performance issues quickly reappeared, though. For example, in November\n2002, administrators had to temporarily disable the \u201cview count\u201d and \u201csite\u201d statistics which were\ncausing two database writes on every page view.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 195", "position": 195, "chunk_type": "semantic", "token_estimate": 395}
{"text": "time, Wikipedians were worried about being inundated with tra\ufb03c following articles in the New: Performance issues quickly reappeared, though. For example, in November\n2002, administrators had to temporarily disable the \u201cview count\u201d and \u201csite\u201d statistics which were\ncausing two database writes on every page view. They would also occasionally switch the site to\nread-only mode to maintain the service for readers, and disable expensive maintenance pages during\nhigh-access times because of table locking problems. In early 2003, developers discussed whether they should properly re-engineer and re-architect\nthe software from scratch, before the \ufb01re-\ufb01ghting became unmanageable, or continue to tweak and\nimprove the existing code base. They chose the latter solution, mostly because most developers\nwere su\ufb03ciently happy with the code base, and con\ufb01dent enough that further iterative improvements\nwould be enough to keep up with the growth of the site. In June 2003, administrators added a second server, the \ufb01rst database server separate from the web\nserver. (The new machine was also the web server for non-English Wikipedia sites.) Load-balancing\nbetween the two servers would be set up later that year. Admins also enabled a new page-caching\nsystem that used the \ufb01le system to cache rendered, ready-to-output pages for anonymous users. June 2003 is also when Jimmy Wales created the non-pro\ufb01t Wikimedia Foundation to support\nWikipedia and manage its infrastructure and day-to-day operations. The \u201cWikipedia software\u201d was\n2http://article.gmane.org/gmane.science.linguistics.wikipedia.technical/2794\nSumana Harihareswara and Guillaume Paumier\n181", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 195", "position": 195, "chunk_type": "semantic", "token_estimate": 234}
{"text": "PHP was chosen as the framework for Wikipedia\u2019s \u201cPhase II\u201d software in 2001; MediaWiki has: grown organically since then, and is still evolving. Most MediaWiki developers are volunteers\ncontributing in their free time, and there were very few of them in the early years. Some software\ndesign decisions or omissions may seem wrong in retrospect, but it\u2019s hard to criticize the founders\nfor not implementing some abstraction which is now found to be critical, when the initial code base\nwas so small, and the time taken to develop it so short. For example, MediaWiki uses unpre\ufb01xed class names, which can cause con\ufb02icts when PHP core\nand PECL (PHP Extension Community Library) developers add new classes: MediaWiki Namespace\nclass had to be renamed to MWNamespace to be compatible with PHP 5.3. Consistently using a pre\ufb01x\nfor all classes (e.g., \u201cMW\u201d) would have made it easier to embed MediaWiki inside another application\nor library. Relying on PHP was probably not the best choice for performance, since it has not bene\ufb01tted\nfrom improvements that some other dynamic languages have seen. Using Java would have been\nmuch better for performance, and simpli\ufb01ed execution scaling for back-end maintenance tasks. On\nthe other hand, PHP is very popular, which facilitates recruiting new developers. Even if MediaWiki still contains \u201cugly\u201d legacy code, major improvements have been made over\nthe years, and new architectural elements have been introduced to MediaWiki throughout its history. They include the Parser, SpecialPage, and Database classes, the Image class and the FileRepo\nclass hierarchy, ResourceLoader, and the Action hierarchy. MediaWiki started without any of these\nthings, but all of them support features that have been around since the beginning. Many developers\nare interested primarily in feature development and architecture is often left behind, only to catch up\nlater as the cost of working within an inadequate architecture becomes apparent. Security\nBecause MediaWiki is the platform for high-pro\ufb01le sites such as Wikipedia, core developers and code\nreviewers have enforced strict security rules3. To make it easier to write secure code, MediaWiki\ngives developers wrappers around HTML output and database queries to handle escaping. To sanitize\nuser input, a develop uses the WebRequest class, which analyzes data passed in the URL or via a\nPOSTed form. It removes \u201cmagic quotes\u201d and slashes, strips illegal input characters and normalizes\nUnicode sequences. Cross-site request forgery (CSRF) is avoided by using tokens, and cross-site\nscripting (XSS) by validating inputs and escaping outputs, usually with PHP\u2019s htmlspecialchars()\nfunction.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 196", "position": 196, "chunk_type": "semantic", "token_estimate": 409}
{"text": "Con\ufb01guration: MediaWiki o\ufb00ers hundreds of con\ufb01guration settings, stored in global PHP variables. Their default\nvalue is set in DefaultSettings.php, and the system administrator can override them by editing\nLocalSettings.php. MediaWiki used to over-depend on global variables, including for con\ufb01guration and context\nprocessing. Globals cause serious security implications with PHP\u2019s register_globals function\n(which MediaWiki hasn\u2019t needed since version 1.2). This system also limits potential abstractions for\ncon\ufb01guration, and makes it more di\ufb03cult to optimize the start-up process. Moreover, the con\ufb01guration\nnamespace is shared with variables used for registration and object context, leading to potential\ncon\ufb02icts. From a user perspective, global con\ufb01guration variables have also made MediaWiki seem\ndi\ufb03cult to con\ufb01gure and maintain. MediaWiki development has been a story of slowly moving\ncontext out of global variables and into objects. Storing processing context in object member\nvariables allows those objects to be reused in a much more \ufb02exible way. 12.3\nDatabase and Text Storage\nMediaWiki has been using a relational database backend since the Phase II software. The default\n(and best-supported) database management system (DBMS) for MediaWiki is MySQL, which is\nthe one that all Wikimedia sites use, but other DBMSes (such as PostgreSQL, Oracle, and SQLite)\nhave community-supported implementations. A sysadmin can choose a DBMS while installing\nMediaWiki, and MediaWiki provides both a database abstraction and a query abstraction layer that\nsimplify database access for developers. The current layout contains dozens of tables. Many are about the wiki\u2019s content (e.g., page,\nrevision, category, and recentchanges). Other tables include data about users (user,\nuser_groups),\nmedia \ufb01les (image,\nfilearchive),\ncaching (objectcache,\nl10n_cache,\nquerycache) and internal tools (job for the job queue), among others4. (See Figure 12.1.) In-\ndices and summary tables are used extensively in MediaWiki, since SQL queries that scan huge\nnumbers of rows can be very expensive, particularly on Wikimedia sites. Unindexed queries are\nusually discouraged. The database went through dozens of schema changes over the years, the most notable being the\ndecoupling of text storage and revision tracking in MediaWiki 1.5. In the 1.4 model, the content was stored in two important tables, cur (containing the text and\nmetadata of the current revision of the page) and old (containing previous revisions); deleted pages\nwere kept in archive. When an edit was made, the previously current revision was copied to the\nold table, and the new edit was saved to cur.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 197", "position": 197, "chunk_type": "semantic", "token_estimate": 390}
{"text": "name, because he\u2019s the one who renamed it. This is done by storing the master\u2019s position in the: user\u2019s session if a request they made resulted in a write query. The next time the user makes a read\nrequest, the load balancer reads this position from the session, and tries to select a slave that has\ncaught up to that replication position to serve the request. If none is available, it will wait until one\nis. It may appear to other users as though the action hasn\u2019t happened yet, but the chronology remains\nconsistent for each user. 12.4\nRequests, Caching and Delivery\nExecution Work\ufb02ow of a Web Request\nindex.php is the main entry point for MediaWiki, and handles most requests processed by the\napplication servers (i.e., requests that were not served by the caching infrastructure; see below). The code executed from index.php performs security checks, loads default con\ufb01guration settings\nfrom includes/DefaultSettings.php, guesses con\ufb01guration with includes/Setup.php and\nthen applies site settings contained in LocalSettings.php. Next it instantiates a MediaWiki object\n($mediawiki), and creates a Title object ($wgTitle) depending on the title and action parameters\nfrom the request. index.php can take a variety of action parameters in the URL request; the default action is\nview, which shows the regular view of an article\u2019s content. For example, the request https://en. wikipedia.org/w/index.php?title=Apple\\&action=view displays the content of the article\n\u201cApple\u201d on the English Wikipedia5. Other frequent actions include edit (to open an article for\nediting), submit (to preview or save an article), history (to show an article\u2019s history) and watch\n(to add an article to the user\u2019s watchlist). Administrative actions include delete (to delete an article)\nand protect (to prevent edits to an article). MediaWiki::performRequest() is then called to handle most of the URL request. It checks\nfor bad titles, read restrictions, local interwiki redirects, and redirect loops, and determines whether\nthe request is for a normal or a special page. Normal page requests are handed over to MediaWiki::initializeArticle(), to create an\nArticle object for the page ($wgArticle), and then to MediaWiki::performAction(), which\nhandles \u201cstandard\u201d actions. Once the action has been completed, MediaWiki::finalCleanup()\n\ufb01nalizes the request by committing database transactions, outputting the HTML and launching\ndeferred updates through the job queue. MediaWiki::restInPeace() commits the deferred updates\nand closes the task gracefully. If the page requested is a Special page (i.e., not a regular wiki content page, but a special\nsoftware-related page such as Statistics), SpecialPageFactory::executePath is called instead\nof initializeArticle(); the corresponding PHP script is then called.", "domains": ["Design Patterns", "Software Quality Attributes", "Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 199", "position": 199, "chunk_type": "semantic", "token_estimate": 412}
{"text": "requests, to preserve performance. MediaWiki sends UDP packets to a central server that collects: them and produces pro\ufb01ling data. Caching\nMediaWiki itself is improved for performance because it plays a central role on Wikimedia sites, but it\nis also part of a larger operational ecosystem that has in\ufb02uenced its architecture. Wikimedia\u2019s caching\ninfrastructure (structured in layers) has imposed limitations in MediaWiki; developers worked around\nthe issues, not by trying to shape Wikimedia\u2019s extensively optimized caching infrastructure around\nMediaWiki, but rather by making MediaWiki more \ufb02exible, so it could work within that infrastructure\nwithout compromising on performance and caching needs. For example, by default MediaWiki\ndisplays the user\u2019s IP in the top-right corner of the interface (for left-to-right languages) as a reminder\nthat that\u2019s how they\u2019re known to the software when they\u2019re not logged in. The $wgShowIPinHeader\ncon\ufb01guration variable allows the system administrator to disable this feature, thus making the page\ncontent independent of the user: all anonymous visitors can then be served the exact same version of\neach page. The \ufb01rst level of caching (used on Wikimedia sites) consists of reverse caching proxies (Squids)\nthat intercept and serve most requests before they make it to the MediaWiki application servers. Squids contain static versions of entire rendered pages, served for simple reads to users who aren\u2019t\nlogged in to the site. MediaWiki natively supports Squid and Varnish, and integrates with this caching\nlayer by, for example, notifying them to purge a page from the cache when it has been changed. For\nlogged-in users, and other requests that can\u2019t be served by Squids, Squid forwards the requests to the\nweb server (Apache). The second level of caching happens when MediaWiki renders and assembles the page from\nmultiple objects, many of which can be cached to minimize future calls. Such objects include\nthe page\u2019s interface (sidebar, menus, UI text) and the content proper, parsed from wikitext. The\nin-memory object cache has been available in MediaWiki since the early 1.1 version (2003), and is\nparticularly important to avoid re-parsing long and complex pages. Login session data can also be stored in memcached, which lets sessions work transparently on\nmultiple front-end web servers in a load-balancing setup (Wikimedia heavily relies on load balancing,\nusing LVS with PyBal). Since version 1.16, MediaWiki uses a dedicated object cache for localized UI text; this was\nadded after noticing that a large part of the objects cached in memcached consisted of UI messages\nlocalized into the user\u2019s language.", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 200", "position": 200, "chunk_type": "semantic", "token_estimate": 409}
{"text": "requests, to preserve performance. MediaWiki sends UDP packets to a central server that collects: Login session data can also be stored in memcached, which lets sessions work transparently on\nmultiple front-end web servers in a load-balancing setup (Wikimedia heavily relies on load balancing,\nusing LVS with PyBal). Since version 1.16, MediaWiki uses a dedicated object cache for localized UI text; this was\nadded after noticing that a large part of the objects cached in memcached consisted of UI messages\nlocalized into the user\u2019s language. The system is based on fast fetches of individual messages from\nconstant databases (CDB), e.g., \ufb01les with key-value pairs. CDBs minimize memory overhead and\nstart-up time in the typical case; they\u2019re also used for the interwiki cache. The last caching layer consists of the PHP opcode cache, commonly enabled to speed up PHP\napplications. Compilation can be a lengthy process; to avoid compiling PHP scripts into opcode\nevery time they\u2019re invoked, a PHP accelerator can be used to store the compiled opcode and execute\nit directly without compilation. MediaWiki will \u201cjust work\u201d with many accelerators such as APC,\nPHP accelerator and eAccelerator. Because of its Wikimedia bias, MediaWiki is optimized for this complete, multi-layer, distributed\ncaching infrastructure. Nonetheless, it also natively supports alternate setups for smaller sites. For\nexample, it o\ufb00ers an optional simplistic \ufb01le caching system that stores the output of fully rendered\npages, like Squid does. Also, MediaWiki\u2019s abstract object caching layer lets it store the cached\nobjects in several places, including the \ufb01le system, the database, or the opcode cache. 186\nMediaWiki", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 200", "position": 200, "chunk_type": "semantic", "token_estimate": 259}
{"text": "ResourceLoader: As in many web applications, MediaWiki\u2019s interface has become more interactive and responsive\nover the years, mostly through the use of JavaScript. Usability e\ufb00orts initiated in 2008, as well\nas advanced media handling (e.g., online editing of video \ufb01les), called for dedicated front-end\nperformance improvements. To optimize the delivery of JavaScript and CSS assets, the ResourceLoader module was developed\nto optimize delivery of JS and CSS. Started in 2009, it was completed in 2011 and has been a core\nfeature of MediaWiki since version 1.17. ResourceLoader works by loading JS and CSS assets on\ndemand, thus reducing loading and parsing time when features are unused, for example by older\nbrowsers. It also mini\ufb01es the code, groups resources to save requests, and can embed images as data\nURIs6. 12.5\nLanguages\nContext and Rationale\nA central part of e\ufb00ectively contributing and disseminating free knowledge to all is to provide it in as\nmany languages as possible. Wikipedia is available in more than 280 languages, and encyclopedia\narticles in English represent less than 20% of all articles. Because Wikipedia and its sister sites exist\nin so many languages, it is important not only to provide the content in the readers\u2019 native language,\nbut also to provide a localized interface, and e\ufb00ective input and conversion tools, so that participants\ncan contribute content. For this reason, localization and internationalization (l10n and i18n) are central components of\nMediaWiki. The i18n system is pervasive, and impacts many parts of the software; it\u2019s also one of the\nmost \ufb02exible and feature-rich7. Translator convenience is usually preferred to developer convenience,\nbut this is believed to be an acceptable cost. MediaWiki is currently localized in more than 350 languages, including non-Latin and right-\nto-left (RTL) languages, with varying levels of completion. The interface and content can be in\ndi\ufb00erent languages, and have mixed directionality. Content Language\nMediaWiki originally used per-language encoding, which led to a lot of issues; for example, foreign\nscripts could not be used in page titles. UTF-8 was adopted instead. Support for character sets other\nthan UTF-8 was dropped in 2005, along with the major database schema change in MediaWiki 1.5;\ncontent must now be encoded in UTF-8. Characters not available on the editor\u2019s keyboard can be customized and inserted via Medi-\naWiki\u2019s Edittools, an interface message that appears below the edit window; its JavaScript version\nautomatically inserts the character clicked into the edit window.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 201", "position": 201, "chunk_type": "semantic", "token_estimate": 399}
{"text": "ResourceLoader: Support for character sets other\nthan UTF-8 was dropped in 2005, along with the major database schema change in MediaWiki 1.5;\ncontent must now be encoded in UTF-8. Characters not available on the editor\u2019s keyboard can be customized and inserted via Medi-\naWiki\u2019s Edittools, an interface message that appears below the edit window; its JavaScript version\nautomatically inserts the character clicked into the edit window. The WikiEditor extension for\nMediaWiki, developed as part of a usability e\ufb00ort, merges special characters with the edit toolbar. Another extension, called Narayam, provides additional input methods and key mapping features for\nnon-ASCII characters. 6For more on ResourceLoader, see https://www.mediawiki.org/wiki/ResourceLoader for the o\ufb03cial documentation,\nand the talk Low Hanging Fruit vs. Micro-optimization: Creative Techniques for Loading Web Pages Faster given by Trevor\nParscal and Roan Kattouw at OSCON 2011. 7For an exhaustive guide to internationalization and localization in MediaWiki, see https://www.mediawiki.org/wiki/\nLocalisation. Sumana Harihareswara and Guillaume Paumier\n187", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 201", "position": 201, "chunk_type": "semantic", "token_estimate": 154}
{"text": "Interface Language: Interface messages have been stored in PHP arrays of key-values pairs since the Phase III software\nwas created. Each message is identi\ufb01ed by a unique key, which is assigned di\ufb00erent values across\nlanguages. Keys are determined by developers, who are encouraged to use pre\ufb01xes for extensions;\nfor example, message keys for the UploadWizard extension will start with mwe-upwiz-, where mwe\nstands for MediaWiki extension. MediaWiki messages can embed parameters provided by the software, which will often in\ufb02uence\nthe grammar of the message. In order to support virtually any possible language, MediaWiki\u2019s\nlocalization system has been improved and complexi\ufb01ed over time to accommodate languages\u2019\nspeci\ufb01c traits and exceptions, often considered oddities by English speakers. For example, adjectives are invariable words in English, but languages like French require adjec-\ntive agreement with nouns. If the user speci\ufb01ed their gender in their preferences, the GENDER: switch\ncan be used in interface messages to appropriately address them. Other switches include PLURAL:,\nfor \u201csimple\u201d plurals and languages like Arabic with dual, trial or paucal numbers, and GRAMMAR:,\nproviding grammatical transformation functions for languages like Finnish whose grammatical cases\ncause alterations or in\ufb02ections. Localizing Messages\nLocalized interface messages for MediaWiki reside in MessagesXx.php \ufb01les, where Xx is the ISO-\n639 code of the language (e.g. MessagesFr.php for French); default messages are in English\nand stored in MessagesEn.php. MediaWiki extensions use a similar system, or host all localized\nmessages in an <Extension-name>.i18n.php \ufb01le. Along with translations, Message \ufb01les also\ninclude language-dependent information such as date formats. Contributing translations used to be done by submitting PHP patches for the MessagesXx.php\n\ufb01les. In December 2003, MediaWiki 1.1 introduced \u201cdatabase messages\u201d, a subset of wiki pages in the\nMediaWiki namespace containing interface messages. The content of the wiki page\nMediaWiki:<Message-key> is the message\u2019s text, and overrides its value in the PHP \ufb01le. Local-\nized versions of the message are at MediaWiki:<Message-key>/<language-code>; for example,\nMediaWiki:Rollbacklink/de. This feature has allowed power users to translate (and customize) interface messages locally\non their wiki, but the process doesn\u2019t update i18n \ufb01les shipping with MediaWiki. In 2006, Niklas\nLaxstr\u00f6m\ncreated\na\nspecial,\nheavily\nhacked\nMediaWiki\nwebsite\n(now\nhosted\nat\nhttp://translatewiki.net) where translators can easily localize interface messages in all lan-\nguages simply by editing a wiki page. The MessagesXx.php \ufb01les are then updated in the MediaWiki\ncode repository, where they can be automatically fetched by any wiki, and updated using the Localisa-\ntionUpdate extension.", "domains": ["Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 202", "position": 202, "chunk_type": "semantic", "token_estimate": 401}
{"text": "Interface Language: In 2006, Niklas\nLaxstr\u00f6m\ncreated\na\nspecial,\nheavily\nhacked\nMediaWiki\nwebsite\n(now\nhosted\nat\nhttp://translatewiki.net) where translators can easily localize interface messages in all lan-\nguages simply by editing a wiki page. The MessagesXx.php \ufb01les are then updated in the MediaWiki\ncode repository, where they can be automatically fetched by any wiki, and updated using the Localisa-\ntionUpdate extension. On Wikimedia sites, database messages are now only used for customization,\nand not for localization any more. MediaWiki extensions and some related programs, such as bots,\nare also localized at translatewiki.net. To help translators understand the context and meaning of an interface message, it is consid-\nered a good practice in MediaWiki to provide documentation for every message. This documen-\ntation is stored in a special Message \ufb01le, with the qqq language code which doesn\u2019t correspond\nto a real language. The documentation for each message is then displayed in the translation in-\nterface on translatewiki.net. Another helpful tool is the qqx language code; when used with the\n&uselang parameter to display a wiki page (e.g., https://en.wikipedia.org/wiki/Special:\n188\nMediaWiki", "domains": ["Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 202", "position": 202, "chunk_type": "semantic", "token_estimate": 179}
{"text": "RecentChanges?uselang=qqx), MediaWiki will display the message keys instead of their values in: the user interface; this is very useful to identify which message to translate or change. Registered users can set their own interface language in their preferences, to override the site\u2019s\ndefault interface language. MediaWiki also supports fallback languages: if a message isn\u2019t available\nin the chosen language, it will be displayed in the closest possible language, and not necessarily in\nEnglish. For example, the fallback language for Breton is French. 12.6\nUsers\nUsers are represented in the code using instances of the User class, which encapsulates all of the user-\nspeci\ufb01c settings (user id, name, rights, password, email address, etc.). Client classes use accessors to\naccess these \ufb01elds; they do all the work of determining whether the user is logged in, and whether\nthe requested option can be satis\ufb01ed from cookies or whether a database query is needed. Most of\nthe settings needed for rendering normal pages are set in the cookie to minimize use of the database. MediaWiki provides a very granular permissions system, with a user permission for, basically, ev-\nery possible action. For example, to perform the \u201cRollback\u201d action (i.e., to \u201cquickly rollback the edits\nof the last user who edited a particular page\u201d), a user needs the rollback permission, included by\ndefault in MediaWiki\u2019s sysop user group. But it can also be added to other user groups, or have a ded-\nicated user group only providing this permission (this is the case on the English Wikipedia, with the\nRollbackers group). Customization of user rights is done by editing the $wgGroupPermissions ar-\nray in LocalSettings.php; for instance, $wgGroupPermissions[\u2019user\u2019][\u2019movefile\u2019] = true;\nallows all registered users to rename \ufb01les. A user can belong to several groups, and inherits the\nhighest rights associated with each of them. However, MediaWiki\u2019s user permissions system was really designed with Wikipedia in mind: a\nsite whose content is accessible to all, and where only certain actions are restricted to some users. MediaWiki lacks a uni\ufb01ed, pervasive permissions concept; it doesn\u2019t provide traditional CMS features\nlike restricting read or write access by topic or type of content. A few MediaWiki extensions provide\nsuch features to some extent. 12.7\nContent\nContent Structure\nThe concept of namespaces was used in the UseModWiki era of Wikipedia, where talk pages were\nat the title \u201c<article name>/Talk\u201d. Namespaces were formally introduced in Magnus Manske\u2019s \ufb01rst\n\u201cPHP script\u201d.", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 203", "position": 203, "chunk_type": "semantic", "token_estimate": 399}
{"text": "RecentChanges?uselang=qqx), MediaWiki will display the message keys instead of their values in: 12.7\nContent\nContent Structure\nThe concept of namespaces was used in the UseModWiki era of Wikipedia, where talk pages were\nat the title \u201c<article name>/Talk\u201d. Namespaces were formally introduced in Magnus Manske\u2019s \ufb01rst\n\u201cPHP script\u201d. They were reimplemented a few times over the years, but have kept the same function:\nto separate di\ufb00erent kinds of content. They consist of a pre\ufb01x separated from the page title by a colon\n(e.g. Talk: or File: and Template:); the main content namespace has no pre\ufb01x. Wikipedia users\nquickly adopted them, and they provided the community with di\ufb00erent spaces to evolve. Namespaces\nhave proven to be an important feature of MediaWiki, as they create the necessary preconditions for\na wiki\u2019s community and set up meta-level discussions, community processes, portals, user pro\ufb01les,\netc. The default con\ufb01guration for MediaWiki\u2019s main content namespace is to be \ufb02at (no subpages),\nbecause it\u2019s how Wikipedia works, but it is trivial to enable subpages. They are enabled in other\nnamespaces (e.g., User:, where people can, for instance, work on draft articles) and display bread-\ncrumbs. Sumana Harihareswara and Guillaume Paumier\n189", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 203", "position": 203, "chunk_type": "semantic", "token_estimate": 193}
{"text": "Namespaces separate content by type; within the same namespace, pages can be organized by: topic using categories, a pseudo-hierarchical organization scheme introduced in MediaWiki 1.3. Content Processing: MediaWiki Markup Language and Parser\nThe user-generated content stored by MediaWiki isn\u2019t in HTML, but in a markup language speci\ufb01c\nto MediaWiki, sometimes called \u201cwikitext\u201d. It allows users to make formatting changes (e.g. bold,\nitalic using quotes), add links (using square brackets), include templates, insert context-dependent\ncontent (like a date or signature), and make an incredible number of other magical things happen8. To display a page, this content needs to be parsed, assembled from all the external or dynamic\npieces it calls, and converted to proper HTML. The parser is one of the most essential parts of\nMediaWiki, which makes it di\ufb03cult to change or improve. Because hundreds of millions of wiki\npages worldwide depend on the parser to continue outputting HTML the way it always has, it has to\nremain extremely stable. The markup language wasn\u2019t formally specced from the beginning; it started based on Use-\nModWiki\u2019s markup, then morphed and evolved as needs demanded. In the absence of a formal\nspeci\ufb01cation, the MediaWiki markup language has become a complex and idiosyncratic language,\nbasically only compatible with MediaWiki\u2019s parser; it can\u2019t be represented as a formal grammar. The\ncurrent parser\u2019s speci\ufb01cation is jokingly referred to as \u201cwhatever the parser spits out from wikitext,\nplus a few hundred test cases\u201d. There have been many attempts at alternative parsers, but none has succeeded so far. In 2004 an\nexperimental tokenizer was written by Jens Frank to parse wikitext, and enabled on Wikipedia; it had\nto be disabled three days later because of the poor performance of PHP array memory allocations. Since then, most of the parsing has been done with a huge pile of regular expressions, and a ton of\nhelper functions. The wiki markup, and all the special cases the parser needs to support, have also\nbecome considerably more complex, making future attempts even more di\ufb03cult. A notable improvement was Tim Starling\u2019s preprocessor rewrite in MediaWiki 1.12, whose\nmain motivation was to improve the parsing performance on pages with complex templates. The\npreprocessor converts wikitext to an XML DOM tree representing parts of the document (template\ninvocations, parser functions, tag hooks, section headings, and a few other structures), but can skip\n\u201cdead branches\u201d, such as unfollowed #switch cases and unused defaults for template arguments, in\ntemplate expansion.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 204", "position": 204, "chunk_type": "semantic", "token_estimate": 404}
{"text": "Namespaces separate content by type; within the same namespace, pages can be organized by: A notable improvement was Tim Starling\u2019s preprocessor rewrite in MediaWiki 1.12, whose\nmain motivation was to improve the parsing performance on pages with complex templates. The\npreprocessor converts wikitext to an XML DOM tree representing parts of the document (template\ninvocations, parser functions, tag hooks, section headings, and a few other structures), but can skip\n\u201cdead branches\u201d, such as unfollowed #switch cases and unused defaults for template arguments, in\ntemplate expansion. The parser then iterates through the DOM structure and converts its content to\nHTML. Recent work on a visual editor for MediaWiki has made it necessary to improve the parsing\nprocess (and make it faster), so work has resumed on the parser and intermediate layers between\nMediaWiki markup and \ufb01nal HTML (see Future, below). Magic Words and Templates\nMediaWiki o\ufb00ers \u201cmagic words\u201d that modify the general behavior of the page or include dynamic\ncontent into it. They consist of: behavior switches like __NOTOC__ (to hide the automatic table of\ncontent) or __NOINDEX__ (to tell search engines not to index the page); variables like CURRENTTIME\nor SITENAME; and parser functions, i.e., magic words that can take parameters, like lc:<string> (to\noutput <string> in lowercase). Constructs like GENDER:, PLURAL: and GRAMMAR:, used to localize\nthe UI, are parser functions. 8Detailed documentation is available at https://www.mediawiki.org/wiki/Markup_spec and the associated pages. 190\nMediaWiki", "domains": ["Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 204", "position": 204, "chunk_type": "semantic", "token_estimate": 234}
{"text": "The most common way to include content from other pages in a MediaWiki page is to use: templates. Templates were really intended to be used to include the same content on di\ufb00erent pages,\ne.g., navigation panels or maintenance banners on Wikipedia articles; having the ability to create\npartial page layouts and reuse them in thousands of articles with central maintenance made a huge\nimpact on sites like Wikipedia. However, templates have also been used (and abused) by users for a completely di\ufb00erent purpose. MediaWiki 1.3 made it possible for templates to take parameters that change their output; the ability\nto add a default parameter (introduced in MediaWiki 1.6) enabled the construction of a functional\nprogramming language implemented on top of PHP, which was ultimately one of the most costly\nfeatures in terms of performance. Tim Starling then developed additional parser functions (the ParserFunctions extension), as a\nstopgap measure against insane constructs created by Wikipedia users with templates. This set\nof functions included logical structures like #if and #switch, and other functions like #expr (to\nevaluate mathematical expressions) and #time (for time formatting). Soon enough, Wikipedia users started to create even more complex templates using the new\nfunctions, which considerably degraded the parsing performance on template-heavy pages. The\nnew preprocessor introduced in MediaWiki 1.12 (a major architectural change) was implemented to\npartly remedy this issue. Recently, MediaWiki developers have discussed the possibility of using an\nactual scripting language, perhaps Lua, to improve performance. Media Files\nUsers upload \ufb01les through the Special:Upload page; administrators can con\ufb01gure the allowed \ufb01le\ntypes through an extension whitelist. Once uploaded, \ufb01les are stored in a folder on the \ufb01le system,\nand thumbnails in a dedicated thumb directory. Because of Wikimedia\u2019s educational mission, MediaWiki supports \ufb01le types that may be uncom-\nmon in other web applications or CMSes, like SVG vector images, and multipage PDFs and DjVus. They are rendered as PNG \ufb01les, and can be thumbnailed and displayed inline, as are more common\nimage \ufb01les like GIFs, JPGs and PNGs. When a \ufb01le is uploaded, it is assigned a File: page containing information entered by the\nuploader; this is free text and usually includes copyright information (author, license) and items\ndescribing or classifying the content of the \ufb01le (description, location, date, categories, etc.). While\nprivate wikis may not care much about this information, on media libraries like Wikimedia Commons\nit are critical to organise the collection and ensure the legality of sharing these \ufb01les.", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 205", "position": 205, "chunk_type": "semantic", "token_estimate": 408}
{"text": "The most common way to include content from other pages in a MediaWiki page is to use: When a \ufb01le is uploaded, it is assigned a File: page containing information entered by the\nuploader; this is free text and usually includes copyright information (author, license) and items\ndescribing or classifying the content of the \ufb01le (description, location, date, categories, etc.). While\nprivate wikis may not care much about this information, on media libraries like Wikimedia Commons\nit are critical to organise the collection and ensure the legality of sharing these \ufb01les. It has been\nargued that most of these metadata should, in fact, be stored in a queryable structure like a database\ntable. This would considerably facilitate search, but also attribution and reuse by third parties\u2014for\nexample, through the API. Most Wikimedia sites also allow \u201clocal\u201d uploads to each wiki, but the community tries to store\nfreely licensed media \ufb01les in Wikimedia\u2019s free media library, Wikimedia Commons. Any Wikimedia\nsite can display a \ufb01le hosted on Commons as if it were hosted locally. This custom avoids having to\nupload a \ufb01le to every wiki to use it there. As a consequence, MediaWiki natively supports foreign media repositories, i.e., the ability to\naccess media \ufb01les hosted on another wiki through its API and the ForeignAPIRepo system. Since\nversion 1.16, any MediaWiki website can easily use \ufb01les from Wikimedia Commons through the\nInstantCommons feature. When using a foreign repository, thumbnails are stored locally to save\nbandwidth. However, it is not (yet) possible to upload to a foreign media repository from another\nwiki. Sumana Harihareswara and Guillaume Paumier\n191", "domains": ["Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 205", "position": 205, "chunk_type": "semantic", "token_estimate": 266}
{"text": "12.8: Customizing and Extending MediaWiki\nLevels\nMediaWiki\u2019s architecture provides di\ufb00erent ways to customize and extend the software. This can be\ndone at di\ufb00erent levels of access:\n\u2022 System administrators can install extensions and skins, and con\ufb01gure the wiki\u2019s separate\nhelper programs (e.g., for image thumbnailing and TeX rendering) and global settings (see\nCon\ufb01guration above). \u2022 Wiki sysops (sometimes called \u201cadministrators\u201d too) can edit site-wide gadgets, JavaScript\nand CSS settings. \u2022 Any registered user can customize their own experience and interface using their preferences\n(for existing settings, skins and gadgets) or make their own modi\ufb01cations (using their personal\nJS and CSS pages). External programs can also communicate with MediaWiki through its machine API, if it\u2019s\nenabled, basically making any feature and data accessible to the user. JavaScript and CSS\nMediaWiki can read and apply site-wide or skin-wide JavaScript and CSS using custom wiki\npages; these pages are in the MediaWiki: namespace, and thus can only be edited by sysops;\nfor example, JavaScript modi\ufb01cations from MediaWiki:Common.js apply to all skins, CSS from\nMediaWiki:Common.css applies to all skins, but MediaWiki:Vector.css only applies to users with\nthe Vector skin. Users can do the same types of changes, which will only apply to their own interface, by\nediting subpages of their user page (e.g. User:<Username>/common.js for JavaScript on all skins,\nUser:<Username>/common.css for CSS on all skins, or User:<Username>/vector.css for CSS\nmodi\ufb01cations that only apply to the Vector skin). If the Gadgets extension is installed, sysops can also edit gadgets, i.e., snippets of JavaScript code,\nproviding features that can be turned on and o\ufb00by users in their preferences. Upcoming developments\non gadgets will make it possible to share gadgets across wikis, thus avoiding duplication. This set of tools has had a huge impact and greatly increased the democratization of MediaWiki\u2019s\nsoftware development. Individual users are empowered to add features for themselves; power users\ncan share them with others, both informally and through globally con\ufb01gurable sysop-controlled\nsystems. This framework is ideal for small, self-contained modi\ufb01cations, and presents a lower barrier\nto entry than heavier code modi\ufb01cations done through hooks and extensions. Extensions and Skins\nWhen JavaScript and CSS modi\ufb01cations are not enough, MediaWiki provides a system of hooks\nthat let third-party developers run custom PHP code before, after, or instead of MediaWiki code for\nparticular events9. MediaWiki extensions use hooks to plug into the code.", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 206", "position": 206, "chunk_type": "semantic", "token_estimate": 388}
{"text": "The other main entry point for MediaWiki, besides index.php, is api.php, used to access its: machine-readable web query API (Application Programming Interface). Wikipedia users originally created \u201cbots\u201d that worked by screen scraping the HTML content\nserved by MediaWiki; this method was very unreliable and broke many times. To improve this\nsituation, developers introduced a read-only interface (located at query.php), which then evolved into\na full-\ufb02edged read and write machine API providing direct, high-level access to the data contained\nin the MediaWiki database10. Client programs can use the API to login, get data, and post changes. The API supports thin\nweb-based JavaScript clients and end-user applications. Almost anything that can be done via the\nweb interface can basically be done through the API. Client libraries implementing the MediaWiki\nAPI are available in many languages, including Python and .NET. 12.9\nFuture\nWhat started as a summer project done by a single volunteer PHP developer has grown into MediaWiki,\na mature, stable wiki engine powering a top-ten website with a ridiculously small operational\ninfrastructure. This has been made possible by constant optimization for performance, iterative\narchitectural changes and a team of awesome developers. The evolution of web technologies, and the growth of Wikipedia, call for ongoing improvements\nand new features, some of which require major changes to MediaWiki\u2019s architecture. This is, for\nexample, the case for the ongoing visual editor project, which has prompted renewed work on the\nparser and on the wiki markup language, the DOM and \ufb01nal HTML conversion. MediaWiki is a tool used for very di\ufb00erent purposes. Within Wikimedia projects, for instance,\nit\u2019s used to create and curate an encyclopedia (Wikipedia), to power a huge media library (Wiki-\nmedia Commons), to transcribe scanned reference texts (Wikisource), and so on. In other contexts,\nMediaWiki is used as a corporate CMS, or as a data repository, sometimes combined with a semantic\nframework. These specialized uses that weren\u2019t planned for will probably continue to drive constant\nadjustments to the software\u2019s internal structure. As such, MediaWiki\u2019s architecture is very much\nalive, just like the immense community of users it supports. 10Exhaustive documentation of the API is available at https://www.mediawiki.org/wiki/API. Sumana Harihareswara and Guillaume Paumier\n193", "domains": ["Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 207", "position": 207, "chunk_type": "semantic", "token_estimate": 361}
{"text": "[chapter13]: Moodle\nTim Hunt\nMoodle is a web application used in educational settings. While this chapter will try to give an\noverview of all aspects of how Moodle works, it focuses on those areas where Moodle\u2019s design is\nparticularly interesting:\n\u2022 The way the application is divided into plugins;\n\u2022 The permission system, which controls which users can perform which actions in di\ufb00erent\nparts of the system;\n\u2022 The way output is generated, so that di\ufb00erent themes (skins) can be used to give di\ufb00erent\nappearances, and so that the interface can be localised. \u2022 The database abstraction layer. Moodle1 provides a place online where students and teachers can come together to teach and\nlearn. A Moodle site is divided into courses. A course has users enrolled in it with di\ufb00erent roles,\nsuch as Student or Teacher. Each course comprises a number of resources and activities. A resource\nmight be a PDF \ufb01le, a page of HTML within Moodle, or a link to something elsewhere on the web. An activity might be a forum, a quiz or a wiki. Within the course, these resources and activities will\nbe structured in some way. For example they may be grouped into logical topics, or into weeks on a\ncalendar. Figure 13.1: Moodle course\n1http://moodle.org/", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 209", "position": 209, "chunk_type": "semantic", "token_estimate": 211}
{"text": "Moodle can be used as a standalone application. Should you wish to teach courses on software: architecture (for example) you could download Moodle to your web host, install it, start creating\ncourses, and wait for students to come and self-register. Alternatively, if you are a large institution,\nMoodle would be just one of the systems you run. You would probably also have the infrastructure\nshown in Figure 13.2. Figure 13.2: Typical university systems architecture\n\u2022 An authentication/identity provider (for example LDAP) to control user accounts across all\nyour systems. \u2022 A student information system; that is, a database of all your students, which program of study\nthey are on, and hence which courses they need to complete; and their transcript\u2014a high-level\nsummary of the results of the courses they have completed. This would also deal with other\nadministrative functions, like tracking whether they have paid their fees. \u2022 A document repository (for example, Alfresco); to store \ufb01les, and track work\ufb02ow as users\ncollaborate to create \ufb01les. \u2022 An ePortfolio; this is a place where students can assemble assets, either to build a CV (resume),\nor to provide evidence that they have met the requirements of a practice-based course. \u2022 A reporting or analytics tool; to generate high-level information about what is going on in your\ninstitution. Moodle focuses on providing an online space for teaching and learning, rather than any of the\nother systems that an educational organisation might need. Moodle provides a basic implementation\nof the other functionalities, so that it can function either as a stand-alone system or integrated with\nother systems. The role Moodle plays is normally called a virtual learning environment (VLE), or\nlearning or course management system (LMS, CMS or even LCMS). Moodle is open source or free software (GPL). It is written in PHP. It will run on most common\nweb servers, on common platforms. It requires a database, and will work with MySQL, PostgreSQL,\nMicrosoft SQL Server or Oracle. The Moodle project was started by Martin Dougiamas in 1999, while he was working at Curtin\nUniversity, Australia. Version 1.0 was released in 2002, at which time PHP4.2 and MySQL 3.23\nwere the technologies available. This limited the kind of architecture that was possible initially, but\nmuch has changed since then. The current release is the Moodle 2.2.x series. 196\nMoodle", "domains": ["Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 210", "position": 210, "chunk_type": "semantic", "token_estimate": 387}
{"text": "13.1: An Overview of How Moodle Works\nA Moodle installation comprises three parts:\n1. The code, typically in a folder like /var/www/moodle or ~/htdocs/moodle. This should not\nbe writable by the web server. 2. The database, managed by one of the supported RDMSs. In fact, Moodle adds a pre\ufb01x to all\nthe table names, so it can share a database with other applications if desired. 3. The moodledata folder. This is a folder where Moodle stores uploaded and generated \ufb01les,\nand so needs to be writable by the web server. For security reasons, the should be outside the\nweb root. These can all be on a single server. Alternatively, in a load-balanced set-up, there will be multiple\ncopies of the code on each web server, but just one shared copy of the database and moodledata,\nprobably on other servers. The con\ufb01guration information about these three parts is stored in a \ufb01le called config.php in the\nroot of the moodle folder when Moodle is installed. Request Dispatching\nMoodle is a web applications, so users interact with it using their web browser. From Moodle\u2019s\npoint of view that means responding to HTTP requests. An important aspect of Moodle\u2019s design is,\ntherefore, the URL namespace, and how URLs get dispatched to di\ufb00erent scripts. Moodle uses the standard PHP approach to this. To view the main page for a course,\nthe URL would be .../course/view.php?id=123, where 123 is the unique id of the course\nin the database. To view a forum discussion,\nthe URL would be something like\n.../mod/forum/discuss.php?id=456789. That is, these particular scripts, course/view.php\nor mod/forum/discuss.php, would handle these requests. This is simple for the developer. To understand how Moodle handles a particular request, you\nlook at the URL and start reading code there. It is ugly from the user\u2019s point of view. These URLs\nare, however, permanent. The URLs do not change if the course is renamed, or if a moderator moves\na discussion to a di\ufb00erent forum.2\nThe alternative approach one could take is to have a single entry point .../index.php/[extra-\ninformation-to-make-the-request-unique]. The single script index.php would then dispatch\nthe requests in some way. This approach adds a layer of indirection, which is something software\ndevelopers always like to do. The lack of this layer of indirection does not seem to hurt Moodle.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 211", "position": 211, "chunk_type": "semantic", "token_estimate": 382}
{"text": "Moodle to their needs, and to share customisations, while still being able to upgrade the core Moodle: system. There are various ways a system can be built as a core surrounded by plugins. Moodle has a\nrelatively fat core, and the plugins are strongly-typed. When I say a fat core, I mean that there is a lot\nof functionality in the core. This contrasts with the kind of architecture where just about everything,\nexcept for a small plugin-loader stub, is a plugin. When I say plugins are strongly typed, I mean that depending on which type of functionality\nyou want to implement, you have to write a di\ufb00erent type of plugin, and implement a di\ufb00erent API. For example, a new Activity module plugin would be very di\ufb00erent from a new Authentication\nplugin or a new Question type. At the last count there are about 35 di\ufb00erent types of plugin3. This\ncontrasts with the kind of architecture where all plugins use basically the same API and then, perhaps,\nsubscribe to the subset of hooks or events they are interested in. Generally, the trend in Moodle has been to try to shrink the core, by moving more functionality\ninto plugins. This e\ufb00ort has only been somewhat successful, however, because an increasing feature-\nset tends to expand the core. The other trend has been to try to standardise the di\ufb00erent types of\nplugin as much as possible, so that in areas of common functionality, like install and upgrade, all\ntypes of plugins work the same way. A plugin in Moodle takes the form of a folder containing \ufb01les. The plugin has a type and a name,\nwhich together make up the \u201cFrankenstyle\u201d component name of the plugin4. The plugin type and\nname determine the path to the plugin folder. The plugin type gives a pre\ufb01x, and the foldername is\nthe plugin name. Here are some examples:\nPlugin type\nPlugin name\nFrankenstyle\nFolder\nmod (Activity module)\nforum\nmod_forum\nmod/forum\nmod (Activity module)\nquiz\nmod_quiz\nmod/quiz\nblock (Side-block)\nnavigation\nblock_navigation\nblocks/navigation\nqtype (Question type)\nshortanswer\nqtype_shortanswer\nquestion/type/shortanswer\nquiz (Quiz report)\nstatistics\nquiz_statistics\nmod/quiz/report/statistics\nThe last example shows that each activity module is allowed to declare sub-plugin types. At the\nmoment only activity modules can do this, for two reasons. If all plugins could have sub-plugins that\nmight cause performance problems. Activity modules are the main educational activities in Moodle,\nand so are the most important type of plugin, thus they get special privileges.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 212", "position": 212, "chunk_type": "semantic", "token_estimate": 408}
{"text": "Moodle to their needs, and to share customisations, while still being able to upgrade the core Moodle: If all plugins could have sub-plugins that\nmight cause performance problems. Activity modules are the main educational activities in Moodle,\nand so are the most important type of plugin, thus they get special privileges. An Example Plugin\nI will explain a lot of details of the Moodle architecture by considering a speci\ufb01c example plugin. As is traditional, I have chosen to implement a plugin that displays \u201cHello world\u201d. This plugin does not really \ufb01t naturally into any of the standard Moodle plugin types. It is just a\nscript, with no connection to anything else, so I will choose to implement it as a \u201clocal\u201d plugin. This\nis a catch-all plugin type for miscellaneous functionality that does not \ufb01t anywhere better. I will name\nmy plugin greet, to give a Frankensyle name of local_greet, and a folder path of local/greet.5\nEach plugin must contain a \ufb01le called version.php which de\ufb01nes some basic metadata about\nthe plugin. This is used by the Moodle\u2019s plugin installer system to install and upgrade the plugin. For example, local/greet/version.php contains:\n3For a full list of Moodle plugin types see http://docs.moodle.org/dev/Plugins. 4The word \u201cFrankenstyle\u201d arose out of an argument in the developers\u2019 Jabber channel, but everyone liked it and it stuck. 5The plugin code can be downloaded from https://github.com/timhunt/moodle-local_greet. 198\nMoodle", "domains": ["Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 212", "position": 212, "chunk_type": "semantic", "token_estimate": 231}
{"text": "<?php: $plugin->component\n= \u2019local_greet\u2019;\n$plugin->version\n= 2011102900;\n$plugin->requires\n= 2011102700;\n$plugin->maturity\n= MATURITY_STABLE;\nIt may seem redundant to include the component name, since this can be deduced from the path,\nbut the installer uses this to verify that the plugin has been installed in the right place. The version\n\ufb01eld is the version of this plugin. Maturity is ALPHA, BETA, RC (release candidate), or STABLE. Requires is the minimum version of Moodle that this plugin is compatible with. If necessary, one\ncan also document other plugins that this one depends on. Here is the main script for this simple plugin (stored in local/greet/index.php):\n<?php\nrequire_once(dirname(__FILE__) . \u2019/../../config.php\u2019);\n// 1\nrequire_login();\n// 2\n$context = context_system::instance();\n// 3\nrequire_capability(\u2019local/greet:begreeted\u2019, $context);\n// 4\n$name = optional_param(\u2019name\u2019, \u2019\u2019, PARAM_TEXT);\n// 5\nif (!$name) {\n$name = fullname($USER);\n// 6\n}\nadd_to_log(SITEID, \u2019local_greet\u2019, \u2019begreeted\u2019,\n\u2019local/greet/index.php?name=\u2019 . urlencode($name));\n// 7\n$PAGE->set_context($context);\n// 8\n$PAGE->set_url(new moodle_url(\u2019/local/greet/index.php\u2019),\narray(\u2019name\u2019 => $name));\n// 9\n$PAGE->set_title(get_string(\u2019welcome\u2019, \u2019local_greet\u2019));\n// 10\necho $OUTPUT->header();\n// 11\necho $OUTPUT->box(get_string(\u2019greet\u2019, \u2019local_greet\u2019,\nformat_string($name)));\n// 12\necho $OUTPUT->footer();\n// 13\nLine 1: Bootstrapping Moodle\nrequire_once(dirname(__FILE__) . \u2019/../../config.php\u2019);\n// 1\nThe single line of this script that does the most work is the \ufb01rst. I said above that config.php\ncontains the details Moodle needs to connect to the database and \ufb01nd the moodledata folder. It ends,\nhowever, with the line require_once(\u2019lib/setup.php\u2019). This:\n1. loads all the standard Moodle libraries using require_once;\n2. starts the session handling;\n3. connects to the database; and\n4. sets up a number of global variables, which we shall meet later. Tim Hunt\n199", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 213", "position": 213, "chunk_type": "semantic", "token_estimate": 260}
{"text": "Line 2: Checking the User Is Logged In: require_login();\n// 2\nThis line causes Moodle to check that the current user is logged in, using whatever authentication\nplugin the administrator has con\ufb01gured. If not, the user will be redirected to the log-in form, and\nthis function will never return. A script that was more integrated into Moodle would pass more arguments here, to say which\ncourse or activity this page is part of, and then require_login would also verify that the user is\nenrolled in, or otherwise allowed to access this course, and is allowed to see this activity. If not, an\nappropriate error would be displayed. 13.2\nMoodle\u2019s Roles and Permissions System\nThe next two lines of code show how to check that the user has permission to do something. As you\ncan see, from the developer\u2019s point of view, the API is very simple. Behind the scenes, however,\nthere is a sophisticated access system which gives the administrator great \ufb02exibility to control who\ncan do what. Line 3: Getting the Context\n$context = context_system::instance();\n// 3\nIn Moodle, users can have di\ufb00erent permissions in di\ufb00erent places. For example, a user might be\na Teacher in one course, and a Student in another, and so have di\ufb00erent permissions in each place. These places are called contexts. Contexts in Moodle form a hierarchy rather like a folder hierarchy in\na \ufb01le-system. At the top level is the System context (and, since this script is not very well integrated\ninto Moodle, it uses that context). Within the System context are a number of contexts for the di\ufb00erent categories that have been\ncreated to organise courses. These can be nested, with one category containing other categories. Category contexts can also contain Course contexts. Finally, each activity in a course will have its\nown Module context. Line 4: Checking the User Has Permission to Use This Script\nrequire_capability(\u2019local/greet:begreeted\u2019, $context);\n// 4\nHaving got the context\u2014the relevant area of Moodle\u2014the permission can be checked. Each\nbit of functionality that a user may or may not have is called a capability. Checking a capability\nprovides more \ufb01ne-grained access control than the basic checks performed by require_login. Our\nsimple example plugin has just one capability: local/greet:begreeted. The check is done using the require_capability function, which takes the capability name\nand the context. Like other require_... functions, it will not return if the user does not have the\ncapability. It will display an error instead.", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 214", "position": 214, "chunk_type": "semantic", "token_estimate": 408}
{"text": "Figure 13.3: Contexts: 1. Start from the current Context. 2. Get a list of the Roles that the user has in this Context. 3. Then work out what the Permission is for each Role in this Context. 4. Aggregate those permissions to get a \ufb01nal answer. De\ufb01ning Capabilities\nAs the example shows, a plugin can de\ufb01ne new capabilities relating to the particular functionality it\nprovides. Inside each Moodle plugin there is a sub-folder of the code called db. This contains all the\ninformation required to install or upgrade the plugin. One of those bits of information is a \ufb01le called\naccess.php that de\ufb01nes the capabilities. Here is the access.php \ufb01le for our plugin, which lives in\nlocal/greet/db/access.php:\n<?php\n$capabilities = array(\u2019local/greet:begreeted\u2019 => array(\n\u2019captype\u2019 => \u2019read\u2019,\n\u2019contextlevel\u2019 => CONTEXT_SYSTEM,\n\u2019archetypes\u2019 => array(\u2019guest\u2019 => CAP_ALLOW, \u2019user\u2019 => CAP_ALLOW)\n));\nThis gives some metadata about each capability which are used when constructing the permissions\nmanagement user interface. It also give default permissions for common types of role. Roles\nThe next part of the Moodle permissions system is roles. A role is really just a named set of\npermissions. When you are logged into Moodle, you will have the \u201cAuthenticated user\u201d role in\nthe System context, and since the System context is the root of the hierarchy, that role will apply\neverywhere. Within a particular course, you may be a Student, and that role assignment will apply in the\nCourse context and all the Module contexts within it. In another course, however, you may have a\nTim Hunt\n201", "domains": ["Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 215", "position": 215, "chunk_type": "semantic", "token_estimate": 256}
{"text": "di\ufb00erent role. For example, Mr Gradgrind may be Teacher in the \u201cFacts, Facts, Facts\u201d course, but a: Student in the professional development course \u201cFacts Aren\u2019t Everything\u201d. Finally, a user might be\ngiven the Moderator role in one particular forum (Module context). Permissions\nA role de\ufb01nes a permission for each capability. For example the Teacher role will probably ALLOW\nmoodle/course:manage, but the Student role will not. However, both Student and Teacher will\nallow mod/forum:startdiscussion. The roles are normally de\ufb01ned globally, but they can be re-de\ufb01ned in each context. For exam-\nple, one particular wiki can be made read-only to students by overriding the permission for the\nmod/wiki:edit capability for the Student role in that wiki (Module) context, to PREVENT. There are four Permissions:\n\u2022 NOT SET/INHERIT (default)\n\u2022 ALLOW\n\u2022 PREVENT\n\u2022 PROHIBIT\nIn a given context, a role will have one of these four permissions for each capability. One di\ufb00erence\nbetween PROHIBIT and PREVENT is that a PROHIBIT cannot be overridden in sub-contexts. Permission Aggregation\nFinally the permissions for all the roles the user has in this context are aggregated. \u2022 If any role gives the permission PROHIBIT for this capability, return false. \u2022 Otherwise, if any role gives ALLOW for this capability, return true. \u2022 Otherwise return false. A use case for PROHIBIT is this: Suppose a user has been making abusive posts in a number\nof forums, and we want to stop them immediately. We can create a Naughty user role, which sets\nmod/forum:post and other such capabilities to PROHIBIT. We can then assign this role to the\nabusive user in the System context. That way, we can be sure that the user will not be able to post any\nmore in any forum. (We would then talk to the student, and having reached a satisfactory outcome,\nremove that role assignment so that they may use the system again.) So, Moodle\u2019s permissions system gives administrators a huge amount of \ufb02exibility. They can\nde\ufb01ne whichever roles they like with di\ufb00erent permissions for each capability; they can alter the role\nde\ufb01nitions in sub-contexts; and then they can assign di\ufb00erent roles to users in di\ufb00erent contexts. 13.3\nBack to Our Example Script\nThe next part of the script illustrates some miscellaneous points:\nLine 5: Get Data From the Request\n$name = optional_param(\u2019name\u2019, \u2019\u2019, PARAM_TEXT);\n// 5\n202\nMoodle", "domains": ["Design Patterns", "Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 216", "position": 216, "chunk_type": "semantic", "token_estimate": 387}
{"text": "Dates are a similar problem. Di\ufb00erent users may be in di\ufb00erent time-zones. Moodle stores: all dates as Unix time-stamps, which are integers, and so work in all databases. There is then a\nuserdate function to display the time-stamp to the user using the appropriate timezone and locale\nsettings. Line 7: Logging\nadd_to_log(SITEID, \u2019local_greet\u2019, \u2019begreeted\u2019,\n\u2019local/greet/index.php?name=\u2019 . urlencode($name));\n// 7\nAll signi\ufb01cant actions in Moodle are logged. Logs are written to a table in the database. This is\na trade-o\ufb00. It makes sophisticated analysis quite easy, and indeed various reports based on the logs\nare included with Moodle. On a large and busy site, however, it is a performance problem. The log\ntable gets huge, which makes backing up the database more di\ufb03cult, and makes queries on the log\ntable slow. There can also be write contention on the log table. These problems can be mitigated in\nvarious ways, for example by batching writes, or archiving or deleting old records to remove them\nfrom the main database. 13.4\nGenerating Output\nOutput is mainly handled via two global objects. Line 8: The $PAGE Global\n$PAGE->set_context($context);\n// 8\n$PAGE stores the information about the page to be output. This information is then readily\navailable to the code that generates the HTML. This script needs to explicitly specify the current\ncontext. (In other situations, this might have been set automatically by require_login.) The URL\nfor this page must also be set explicitly. This may seem redundant, but the rationale for requiring it is\nthat you might get to a particular page using any number of di\ufb00erent URLs, but the URL passed to\nset_url should be the canonical URL for the page\u2014a good permalink, if you like. The page title is\nalso set. This will end up in the head element of the HTML. Line 9: Moodle URL\n$PAGE->set_url(new moodle_url(\u2019/local/greet/index.php\u2019),\narray(\u2019name\u2019 => $name));\n// 9\nI just wanted to \ufb02ag this nice little helper class which makes manipulating URLs much easier. As\nan aside, recall that the add_to_log function call above did not use this helper class. Indeed, the log\nAPI cannot accept moodle_url objects. This sort of inconsistency is a typical sign of a code-base as\nold as Moodle\u2019s. Line 10: Internationalisation\n$PAGE->set_title(get_string(\u2019welcome\u2019, \u2019local_greet\u2019));\n// 10\n204\nMoodle", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 218", "position": 218, "chunk_type": "semantic", "token_estimate": 374}
{"text": "Moodle uses its own system to allow the interface to be translated into any language. There may: now be good PHP internationalisation libraries, but in 2002 when it was \ufb01rst implemented there was\nnot one available that was adequate. The system is based around the get_string function. Strings\nare identi\ufb01ed by a key and the plugin Frankenstyle name. As can be seen on line 12, it is possible to\ninterpolate values into the string. (Multiple values are handled using PHP arrays or objects.) The strings are looked up in language \ufb01les that are just plain PHP arrays. Here is the language\n\ufb01le local/greet/lang/en/local_greet.php for our plugin:\n<?php\n$string[\u2019greet:begreeted\u2019] = \u2019Be greeted by the hello world example\u2019;\n$string[\u2019welcome\u2019] = \u2019Welcome\u2019;\n$string[\u2019greet\u2019] = \u2019Hello, {$a}!\u2019;\n$string[\u2019pluginname\u2019] = \u2019Hello world example\u2019;\nNote that, as well as the two string used in our script, there are also strings to give a name to the\ncapability, and the name of the plugin as it appears in the user interface. The di\ufb00erent languages are identi\ufb01ed by the two-letter country code (en here). Languages packs\nmay derive from other language packs. For example the fr_ca (French Canadian) language pack\ndeclares fr (French) as the parent language, and thus only has to de\ufb01ne those strings that di\ufb00er from\nthe French. Since Moodle originated in Australia, en means British English, and en_us (American\nEnglish) is derived from it. Again, the simple get_string API for plugin developers hides a lot of complexity, including\nworking out the current language (which may depend on the current user\u2019s preferences, or the settings\nfor the particular course they are currently in), and then searching through all the language packs and\nparent language packs to \ufb01nd the string. Producing the language pack \ufb01les, and co-ordinating the translation e\ufb00ort is managed at http:\n//lang.moodle.org/, which is Moodle with a custom plugin8. It uses both Git and the database as\na backend to store the language \ufb01les with full version history. Line 11: Starting Output\necho $OUTPUT->header();\n// 11\nThis is another innocuous-looking line that does much more than it seems. The point is that\nbefore any output can be done, the applicable theme (skin) must be worked out. This may depend on\na combination of the page context and the user\u2019s preferences. $PAGE->context was, however, only\nset on line 8, so the $OUTPUT global could not have been initialised at the start of the script.", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 219", "position": 219, "chunk_type": "semantic", "token_estimate": 399}
{"text": "Moodle uses its own system to allow the interface to be translated into any language. There may: This may depend on\na combination of the page context and the user\u2019s preferences. $PAGE->context was, however, only\nset on line 8, so the $OUTPUT global could not have been initialised at the start of the script. In order\nto solve this problem, some PHP magic is used to create the proper $OUTPUT object based on the\ninformation in $PAGE the \ufb01rst time any output method is called. Another thing to consider is that every page in Moodle may contain blocks. These are extra\ncon\ufb01gurable bits of content that are normally displayed to the left or right of the main content. (They\nare a type of plugin.) Again, the exact collection of blocks to display depends, in a \ufb02exible way\n(that the administrator can control) on the page context and some other aspects of the page identity. Therefore, another part of preparing for output is a call to $PAGE->blocks->load_blocks(). Once all the necessary information has been worked out, the theme plugin (that controls the\noverall look of the page) is called to generate the overall page layout, including whatever standard\nheader and footer is desired. This call is also responsible for adding the output from the blocks\n8local_amos, http://docs.moodle.org/22/en/AMOS. Tim Hunt\n205", "domains": ["Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 219", "position": 219, "chunk_type": "semantic", "token_estimate": 219}
{"text": "<?php: class local_greet_renderer extends plugin_renderer_base {\npublic function greeting_page($name) {\n$output = \u2019\u2019;\n$output .= $this->header();\n$output .= $this->box(get_string(\u2019greet\u2019, \u2019local_greet\u2019, $name));\n$output .= $this->footer();\nreturn $output;\n}\n}\nIf the theme wished to completely change this output, it would de\ufb01ne a subclass of this renderer\nthat overrides the greeting_page method. $PAGE->get_renderer() determines the appropriate\nrenderer class to instantiate depending on the current theme. Thus, the output (view) code is fully\nseparated from the controller code in index.php, and the plugin has been refactored from typical\nlegacy Moodle code to a clean MVC architecture. 13.5\nDatabase Abstraction\nThe \"Hello world\" script was su\ufb03ciently simple that it did not need to access the database, although\nseveral of the Moodle library calls used did do database queries. I will now brie\ufb02y describe the\nMoodle database layer. Moodle used to use the ADOdb library as the basis of its database abstraction layer, but there were\nissues for us, and the extra layer of library code had a noticeable impact on performance. Therefore,\nin Moodle 2.0 we switched to our own abstraction layer, which is a thin wrapper around the various\nPHP database libraries. The moodle_database Class\nThe heart of the library is the moodle_database class. This de\ufb01nes the interface provided by the\n$DB global variable, which gives access to the database connection. A typical usage might be:\n$course = $DB->get_record(\u2019course\u2019, array(\u2019id\u2019 => $courseid));\nThat translates into the SQL:\nSELECT * FROM mdl_course WHERE id = $courseid;\nand returns the data as a plain PHP object with public \ufb01elds, so you could access $course->id,\n$course->fullname, etc. Simple methods like this deal with basic queries, and simple updates and inserts. Sometimes it\nis necessary to do more complex SQL, for example to run reports. In that case, there are methods to\nexecute arbitrary SQL:\n$courseswithactivitycounts = $DB->get_records_sql(\n\u2019SELECT c.id, \u2019 . $DB->sql_concat(\u2019shortname\u2019, \"\u2019 \u2019\", \u2019fullname\u2019) . \u2019 AS coursename,\nCOUNT(1) AS activitycount\nFROM {course} c\nJOIN {course_modules} cm ON cm.course = c.id\nTim Hunt\n207", "domains": ["Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 221", "position": 221, "chunk_type": "semantic", "token_estimate": 329}
{"text": "[chapter14]: nginx\nAndrew Alexeev\nnginx (pronounced \u201cengine x\u201d) is a free open source web server written by Igor Sysoev, a Russian\nsoftware engineer. Since its public launch in 2004, nginx has focused on high performance, high\nconcurrency and low memory usage. Additional features on top of the web server functionality, like\nload balancing, caching, access and bandwidth control, and the ability to integrate e\ufb03ciently with a\nvariety of applications, have helped to make nginx a good choice for modern website architectures. Currently nginx is the second most popular open source web server on the Internet. 14.1\nWhy Is High Concurrency Important? These days the Internet is so widespread and ubiquitous it\u2019s hard to imagine it wasn\u2019t exactly there,\nas we know it, a decade ago. It has greatly evolved, from simple HTML producing clickable text,\nbased on NCSA and then on Apache web servers, to an always-on communication medium used by\nmore than 2 billion users worldwide. With the proliferation of permanently connected PCs, mobile\ndevices and recently tablets, the Internet landscape is rapidly changing and entire economies have\nbecome digitally wired. Online services have become much more elaborate with a clear bias towards\ninstantly available live information and entertainment. Security aspects of running online business\nhave also signi\ufb01cantly changed. Accordingly, websites are now much more complex than before,\nand generally require a lot more engineering e\ufb00orts to be robust and scalable. One of the biggest challenges for a website architect has always been concurrency. Since\nthe beginning of web services, the level of concurrency has been continuously growing. It\u2019s not\nuncommon for a popular website to serve hundreds of thousands and even millions of simultaneous\nusers. A decade ago, the major cause of concurrency was slow clients\u2014users with ADSL or dial-\nup connections. Nowadays, concurrency is caused by a combination of mobile clients and newer\napplication architectures which are typically based on maintaining a persistent connection that\nallows the client to be updated with news, tweets, friend feeds, and so on. Another important factor\ncontributing to increased concurrency is the changed behavior of modern browsers, which open four\nto six simultaneous connections to a website to improve page load speed. To illustrate the problem with slow clients, imagine a simple Apache-based web server which\nproduces a relatively short 100 KB response\u2014a web page with text or an image.", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 225", "position": 225, "chunk_type": "semantic", "token_estimate": 389}
{"text": "who have requested similar content. If only 1 MB of additional memory is allocated per client, it: would result in 1000 MB (about 1 GB) of extra memory devoted to serving just 1000 clients 100 KB\nof content. In reality, a typical web server based on Apache commonly allocates more than 1 MB of\nadditional memory per connection, and regrettably tens of kbps is still often the e\ufb00ective speed of\nmobile communications. Although the situation with sending content to a slow client might be, to\nsome extent, improved by increasing the size of operating system kernel socket bu\ufb00ers, it\u2019s not a\ngeneral solution to the problem and can have undesirable side e\ufb00ects. With persistent connections the problem of handling concurrency is even more pronounced,\nbecause to avoid latency associated with establishing new HTTP connections, clients would stay\nconnected, and for each connected client there\u2019s a certain amount of memory allocated by the web\nserver. Consequently, to handle the increased workloads associated with growing audiences and hence\nhigher levels of concurrency\u2014and to be able to continuously do so\u2014a website should be based on a\nnumber of very e\ufb03cient building blocks. While the other parts of the equation such as hardware (CPU,\nmemory, disks), network capacity, application and data storage architectures are obviously important,\nit is in the web server software that client connections are accepted and processed. Thus, the web\nserver should be able to scale nonlinearly with the growing number of simultaneous connections and\nrequests per second. Isn\u2019t Apache Suitable? Apache, the web server software that still largely dominates the Internet today, has its roots in the\nbeginning of the 1990s. Originally, its architecture matched the then-existing operating systems\nand hardware, but also the state of the Internet, where a website was typically a standalone physical\nserver running a single instance of Apache. By the beginning of the 2000s it was obvious that the\nstandalone web server model could not be easily replicated to satisfy the needs of growing web\nservices. Although Apache provided a solid foundation for future development, it was architected\nto spawn a copy of itself for each new connection, which was not suitable for nonlinear scalability\nof a website. Eventually Apache became a general purpose web server focusing on having many\ndi\ufb00erent features, a variety of third-party extensions, and universal applicability to practically any\nkind of web application development.", "domains": ["Design Patterns", "Design Principles", "Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 226", "position": 226, "chunk_type": "semantic", "token_estimate": 393}
{"text": "who have requested similar content. If only 1 MB of additional memory is allocated per client, it: Although Apache provided a solid foundation for future development, it was architected\nto spawn a copy of itself for each new connection, which was not suitable for nonlinear scalability\nof a website. Eventually Apache became a general purpose web server focusing on having many\ndi\ufb00erent features, a variety of third-party extensions, and universal applicability to practically any\nkind of web application development. However, nothing comes without a price and the downside to\nhaving such a rich and universal combination of tools in a single piece of software is less scalability\nbecause of increased CPU and memory usage per connection. Thus, when server hardware, operating systems and network resources ceased to be major\nconstraints for website growth, web developers worldwide started to look around for a more e\ufb03cient\nmeans of running web servers. Around ten years ago, Daniel Kegel, a prominent software engineer,\nproclaimed that \u201cit\u2019s time for web servers to handle ten thousand clients simultaneously\u201d1 and\npredicted what we now call Internet cloud services. Kegel\u2019s C10K manifest spurred a number of\nattempts to solve the problem of web server optimization to handle a large number of clients at the\nsame time, and nginx turned out to be one of the most successful ones. Aimed at solving the C10K problem of 10,000 simultaneous connections, nginx was written\nwith a di\ufb00erent architecture in mind\u2014one which is much more suitable for nonlinear scalability in\nboth the number of simultaneous connections and requests per second. nginx is event-based, so it\ndoes not follow Apache\u2019s style of spawning new processes or threads for each web page request. The\nend result is that even as load increases, memory and CPU usage remain manageable. nginx can\nnow deliver tens of thousands of concurrent connections on a server with typical hardware. 1http://www.kegel.com/c10k.html\n212\nnginx", "domains": ["Design Principles", "Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 226", "position": 226, "chunk_type": "semantic", "token_estimate": 313}
{"text": "When the \ufb01rst version of nginx was released, it was meant to be deployed alongside Apache: such that static content like HTML, CSS, JavaScript and images were handled by nginx to o\ufb04oad\nconcurrency and latency processing from Apache-based application servers. Over the course of\nits development, nginx has added integration with applications through the use of FastCGI, uswgi\nor SCGI protocols, and with distributed memory object caching systems like memcached. Other\nuseful functionality like reverse proxy with load balancing and caching was added as well. These\nadditional features have shaped nginx into an e\ufb03cient combination of tools to build a scalable web\ninfrastructure upon. In February 2012, the Apache 2.4.x branch was released to the public. Although this latest\nrelease of Apache has added new multi-processing core modules and new proxy modules aimed\nat enhancing scalability and performance, it\u2019s too soon to tell if its performance, concurrency and\nresource utilization are now on par with, or better than, pure event-driven web servers. It would be\nvery nice to see Apache application servers scale better with the new version, though, as it could\npotentially alleviate bottlenecks on the backend side which still often remain unsolved in typical\nnginx-plus-Apache web con\ufb01gurations. Are There More Advantages to Using nginx? Handling high concurrency with high performance and e\ufb03ciency has always been the key bene\ufb01t of\ndeploying nginx. However, there are now even more interesting bene\ufb01ts. In the last few years, web architects have embraced the idea of decoupling and separating their\napplication infrastructure from the web server. However, what would previously exist in the form\nof a LAMP (Linux, Apache, MySQL, PHP, Python or Perl)-based website, might now become not\nmerely a LEMP-based one (\u2018E\u2019 standing for \u2018Engine x\u2019), but more and more often an exercise in\npushing the web server to the edge of the infrastructure and integrating the same or a revamped set\nof applications and database tools around it in a di\ufb00erent way. nginx is very well suited for this, as it provides the key features necessary to conveniently o\ufb04oad\nconcurrency, latency processing, SSL (secure sockets layer), static content, compression and caching,\nconnections and requests throttling, and even HTTP media streaming from the application layer to a\nmuch more e\ufb03cient edge web server layer. It also allows integrating directly with memcached/Redis\nor other \u201cNoSQL\u201d solutions, to boost performance when serving a large number of concurrent users.", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 227", "position": 227, "chunk_type": "semantic", "token_estimate": 395}
{"text": "When the \ufb01rst version of nginx was released, it was meant to be deployed alongside Apache: nginx is very well suited for this, as it provides the key features necessary to conveniently o\ufb04oad\nconcurrency, latency processing, SSL (secure sockets layer), static content, compression and caching,\nconnections and requests throttling, and even HTTP media streaming from the application layer to a\nmuch more e\ufb03cient edge web server layer. It also allows integrating directly with memcached/Redis\nor other \u201cNoSQL\u201d solutions, to boost performance when serving a large number of concurrent users. With recent \ufb02avors of development kits and programming languages gaining wide use, more\nand more companies are changing their application development and deployment habits. nginx has\nbecome one of the most important components of these changing paradigms, and it has already\nhelped many companies start and develop their web services quickly and within their budgets. The \ufb01rst lines of nginx were written in 2002. In 2004 it was released to the public under the\ntwo-clause BSD license. The number of nginx users has been growing ever since, contributing ideas,\nand submitting bug reports, suggestions and observations that have been immensely helpful and\nbene\ufb01cial for the entire community. The nginx codebase is original and was written entirely from scratch in the C programming\nlanguage. nginx has been ported to many architectures and operating systems, including Linux,\nFreeBSD, Solaris, Mac OS X, AIX and Microsoft Windows. nginx has its own libraries and with\nits standard modules does not use much beyond the system\u2019s C library, except for zlib, PCRE and\nOpenSSL which can be optionally excluded from a build if not needed or because of potential license\ncon\ufb02icts. A few words about the Windows version of nginx. While nginx works in a Windows environment,\nthe Windows version of nginx is more like a proof-of-concept rather than a fully functional port. Andrew Alexeev\n213", "domains": ["Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 227", "position": 227, "chunk_type": "semantic", "token_estimate": 310}
{"text": "There are certain limitations of the nginx and Windows kernel architectures that do not interact well: at this time. The known issues of the nginx version for Windows include a much lower number\nof concurrent connections, decreased performance, no caching and no bandwidth policing. Future\nversions of nginx for Windows will match the mainstream functionality more closely. 14.2\nOverview of nginx Architecture\nTraditional process- or thread-based models of handling concurrent connections involve handling\neach connection with a separate process or thread, and blocking on network or input/output operations. Depending on the application, it can be very ine\ufb03cient in terms of memory and CPU consumption. Spawning a separate process or thread requires preparation of a new runtime environment, including\nallocation of heap and stack memory, and the creation of a new execution context. Additional CPU\ntime is also spent creating these items, which can eventually lead to poor performance due to thread\nthrashing on excessive context switching. All of these complications manifest themselves in older\nweb server architectures like Apache\u2019s. This is a tradeo\ufb00between o\ufb00ering a rich set of generally\napplicable features and optimized usage of server resources. From the very beginning, nginx was meant to be a specialized tool to achieve more performance,\ndensity and economical use of server resources while enabling dynamic growth of a website, so it\nhas followed a di\ufb00erent model. It was actually inspired by the ongoing development of advanced\nevent-based mechanisms in a variety of operating systems. What resulted is a modular, event-driven,\nasynchronous, single-threaded, non-blocking architecture which became the foundation of nginx\ncode. nginx uses multiplexing and event noti\ufb01cations heavily, and dedicates speci\ufb01c tasks to separate\nprocesses. Connections are processed in a highly e\ufb03cient run-loop in a limited number of single-\nthreaded processes called workers. Within each worker nginx can handle many thousands of\nconcurrent connections and requests per second. Code Structure\nThe nginx worker code includes the core and the functional modules. The core of nginx is responsible\nfor maintaining a tight run-loop and executing appropriate sections of modules\u2019 code on each stage of\nrequest processing. Modules constitute most of the presentation and application layer functionality. Modules read from and write to the network and storage, transform content, do outbound \ufb01ltering,\napply server-side include actions and pass the requests to the upstream servers when proxying is\nactivated. nginx\u2019s modular architecture generally allows developers to extend the set of web server features\nwithout modifying the nginx core.", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 228", "position": 228, "chunk_type": "semantic", "token_estimate": 403}
{"text": "There are certain limitations of the nginx and Windows kernel architectures that do not interact well: Modules read from and write to the network and storage, transform content, do outbound \ufb01ltering,\napply server-side include actions and pass the requests to the upstream servers when proxying is\nactivated. nginx\u2019s modular architecture generally allows developers to extend the set of web server features\nwithout modifying the nginx core. nginx modules come in slightly di\ufb00erent incarnations, namely\ncore modules, event modules, phase handlers, protocols, variable handlers, \ufb01lters, upstreams and\nload balancers. At this time, nginx doesn\u2019t support dynamically loaded modules; i.e., modules are\ncompiled along with the core at build stage. However, support for loadable modules and ABI is\nplanned for the future major releases. More detailed information about the roles of di\ufb00erent modules\ncan be found in Section 14.4. While handling a variety of actions associated with accepting, processing and managing network\nconnections and content retrieval, nginx uses event noti\ufb01cation mechanisms and a number of disk I/O\nperformance enhancements in Linux, Solaris and BSD-based operating systems, like kqueue, epoll,\nand event ports. The goal is to provide as many hints to the operating system as possible, in regards\n214\nnginx", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 228", "position": 228, "chunk_type": "semantic", "token_estimate": 198}
{"text": "to obtaining timely asynchronous feedback for inbound and outbound tra\ufb03c, disk operations, reading: from or writing to sockets, timeouts and so on. The usage of di\ufb00erent methods for multiplexing and\nadvanced I/O operations is heavily optimized for every Unix-based operating system nginx runs on. A high-level overview of nginx architecture is presented in Figure 14.1. Figure 14.1: Diagram of nginx\u2019s architecture\nWorkers Model\nAs previously mentioned, nginx doesn\u2019t spawn a process or thread for every connection. Instead,\nworker processes accept new requests from a shared \u201clisten\u201d socket and execute a highly e\ufb03cient\nrun-loop inside each worker to process thousands of connections per worker. There\u2019s no specialized\narbitration or distribution of connections to the workers in nginx; this work is done by the OS kernel\nmechanisms. Upon startup, an initial set of listening sockets is created. workers then continuously\naccept, read from and write to the sockets while processing HTTP requests and responses. The run-loop is the most complicated part of the nginx worker code. It includes comprehensive\ninner calls and relies heavily on the idea of asynchronous task handling. Asynchronous operations\nare implemented through modularity, event noti\ufb01cations, extensive use of callback functions and\n\ufb01ne-tuned timers. Overall, the key principle is to be as non-blocking as possible. The only situation\nwhere nginx can still block is when there\u2019s not enough disk storage performance for a worker process. Because nginx does not fork a process or thread per connection, memory usage is very conserva-\ntive and extremely e\ufb03cient in the vast majority of cases. nginx conserves CPU cycles as well because\nthere\u2019s no ongoing create-destroy pattern for processes or threads. What nginx does is check the\nstate of the network and storage, initialize new connections, add them to the run-loop, and process\nasynchronously until completion, at which point the connection is deallocated and removed from the\nrun-loop. Combined with the careful use of syscalls and an accurate implementation of supporting\nAndrew Alexeev\n215", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 229", "position": 229, "chunk_type": "semantic", "token_estimate": 321}
{"text": "interfaces like pool and slab memory allocators, nginx typically achieves moderate-to-low CPU: usage even under extreme workloads. Because nginx spawns several workers to handle connections, it scales well across multiple\ncores. Generally, a separate worker per core allows full utilization of multicore architectures, and\nprevents thread thrashing and lock-ups. There\u2019s no resource starvation and the resource controlling\nmechanisms are isolated within single-threaded worker processes. This model also allows more\nscalability across physical storage devices, facilitates more disk utilization and avoids blocking on\ndisk I/O. As a result, server resources are utilized more e\ufb03ciently with the workload shared across\nseveral workers. With some disk use and CPU load patterns, the number of nginx workers should be adjusted. The rules are somewhat basic here, and system administrators should try a couple of con\ufb01gurations\nfor their workloads. General recommendations might be the following: if the load pattern is CPU\nintensive\u2014for instance, handling a lot of TCP/IP, doing SSL, or compression\u2014the number of nginx\nworkers should match the number of CPU cores; if the load is mostly disk I/O bound\u2014for instance,\nserving di\ufb00erent sets of content from storage, or heavy proxying\u2014the number of workers might be\none and a half to two times the number of cores. Some engineers choose the number of workers\nbased on the number of individual storage units instead, though e\ufb03ciency of this approach depends\non the type and con\ufb01guration of disk storage. One major problem that the developers of nginx will be solving in upcoming versions is how to\navoid most of the blocking on disk I/O. At the moment, if there\u2019s not enough storage performance to\nserve disk operations generated by a particular worker, that worker may still block on reading/writing\nfrom disk. A number of mechanisms and con\ufb01guration \ufb01le directives exist to mitigate such disk I/O\nblocking scenarios. Most notably, combinations of options like send\ufb01le and AIO typically produce a\nlot of headroom for disk performance. An nginx installation should be planned based on the data set,\nthe amount of memory available for nginx, and the underlying storage architecture. Another problem with the existing worker model is related to limited support for embedded\nscripting. For one, with the standard nginx distribution, only embedding Perl scripts is supported. There is a simple explanation for that: the key problem is the possibility of an embedded script to\nblock on any operation or exit unexpectedly.", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 230", "position": 230, "chunk_type": "semantic", "token_estimate": 395}
{"text": "\u2022 Compiling embedded Perl scripts: The worker processes accept, handle and process connections from clients, provide reverse\nproxying and \ufb01ltering functionality and do almost everything else that nginx is capable of. In regards\nto monitoring the behavior of an nginx instance, a system administrator should keep an eye on\nworkers as they are the processes re\ufb02ecting the actual day-to-day operations of a web server. The cache loader process is responsible for checking the on-disk cache items and populating\nnginx\u2019s in-memory database with cache metadata. Essentially, the cache loader prepares nginx\ninstances to work with \ufb01les already stored on disk in a specially allocated directory structure. It\ntraverses the directories, checks cache content metadata, updates the relevant entries in shared\nmemory and then exits when everything is clean and ready for use. The cache manager is mostly responsible for cache expiration and invalidation. It stays in memory\nduring normal nginx operation and it is restarted by the master process in the case of failure. Brief Overview of nginx Caching\nCaching in nginx is implemented in the form of hierarchical data storage on a \ufb01lesystem. Cache\nkeys are con\ufb01gurable, and di\ufb00erent request-speci\ufb01c parameters can be used to control what gets into\nthe cache. Cache keys and cache metadata are stored in the shared memory segments, which the\ncache loader, cache manager and workers can access. Currently there is not any in-memory caching\nof \ufb01les, other than optimizations implied by the operating system\u2019s virtual \ufb01lesystem mechanisms. Each cached response is placed in a di\ufb00erent \ufb01le on the \ufb01lesystem. The hierarchy (levels and naming\ndetails) are controlled through nginx con\ufb01guration directives. When a response is written to the\ncache directory structure, the path and the name of the \ufb01le are derived from an MD5 hash of the\nproxy URL. The process for placing content in the cache is as follows: When nginx reads the response from\nan upstream server, the content is \ufb01rst written to a temporary \ufb01le outside of the cache directory\nstructure. When nginx \ufb01nishes processing the request it renames the temporary \ufb01le and moves it to\nthe cache directory. If the temporary \ufb01les directory for proxying is on another \ufb01le system, the \ufb01le\nwill be copied, thus it\u2019s recommended to keep both temporary and cache directories on the same \ufb01le\nsystem. It is also quite safe to delete \ufb01les from the cache directory structure when they need to be\nexplicitly purged.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 231", "position": 231, "chunk_type": "semantic", "token_estimate": 400}
{"text": "Apache-style distributed con\ufb01gurations (i.e., .htaccess \ufb01les). All of the con\ufb01guration relevant to: nginx web server behavior should reside in a centralized set of con\ufb01guration \ufb01les. The con\ufb01guration \ufb01les are initially read and veri\ufb01ed by the master process. A compiled read-only\nform of the nginx con\ufb01guration is available to the worker processes as they are forked from the\nmaster process. Con\ufb01guration structures are automatically shared by the usual virtual memory\nmanagement mechanisms. nginx con\ufb01guration has several di\ufb00erent contexts for main, http, server, upstream, location\n(and also mail for mail proxy) blocks of directives. Contexts never overlap. For instance, there is no\nsuch thing as putting a location block in the main block of directives. Also, to avoid unnecessary\nambiguity there isn\u2019t anything like a \u201cglobal web server\u201d con\ufb01guration. nginx con\ufb01guration is meant\nto be clean and logical, allowing users to maintain complicated con\ufb01guration \ufb01les that comprise\nthousands of directives. In a private conversation, Sysoev said, \u201cLocations, directories, and other\nblocks in the global server con\ufb01guration are the features I never liked in Apache, so this is the reason\nwhy they were never implemented in nginx.\u201d\nCon\ufb01guration syntax, formatting and de\ufb01nitions follow a so-called C-style convention. This\nparticular approach to making con\ufb01guration \ufb01les is already being used by a variety of open source\nand commercial software applications. By design, C-style con\ufb01guration is well-suited for nested\ndescriptions, being logical and easy to create, read and maintain, and liked by many engineers. C-style con\ufb01guration of nginx can also be easily automated. While some of the nginx directives resemble certain parts of Apache con\ufb01guration, setting up an\nnginx instance is quite a di\ufb00erent experience. For instance, rewrite rules are supported by nginx,\nthough it would require an administrator to manually adapt a legacy Apache rewrite con\ufb01guration to\nmatch nginx style. The implementation of the rewrite engine di\ufb00ers too. In general, nginx settings also provide support for several original mechanisms that can be very\nuseful as part of a lean web server con\ufb01guration. It makes sense to brie\ufb02y mention variables and the\ntry_files directive, which are somewhat unique to nginx. Variables in nginx were developed to\nprovide an additional even-more-powerful mechanism to control run-time con\ufb01guration of a web\nserver. Variables are optimized for quick evaluation and are internally pre-compiled to indices. Evaluation is done on demand; i.e., the value of a variable is typically calculated only once and\ncached for the lifetime of a particular request.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 232", "position": 232, "chunk_type": "semantic", "token_estimate": 400}
{"text": "Apache-style distributed con\ufb01gurations (i.e., .htaccess \ufb01les). All of the con\ufb01guration relevant to: Variables are optimized for quick evaluation and are internally pre-compiled to indices. Evaluation is done on demand; i.e., the value of a variable is typically calculated only once and\ncached for the lifetime of a particular request. Variables can be used with di\ufb00erent con\ufb01guration\ndirectives, providing additional \ufb02exibility for describing conditional request processing behavior. The try_files directive was initially meant to gradually replace conditional if con\ufb01guration\nstatements in a more proper way, and it was designed to quickly and e\ufb03ciently try/match against\ndi\ufb00erent URI-to-content mappings. Overall, the try_files directive works well and can be extremely\ne\ufb03cient and useful. It is recommended that the reader thoroughly check the try_files directive\nand adopt its use whenever applicable2. 14.4\nnginx Internals\nAs was mentioned before, the nginx codebase consists of a core and a number of modules. The core\nof nginx is responsible for providing the foundation of the web server, web and mail reverse proxy\nfunctionalities; it enables the use of underlying network protocols, builds the necessary run-time\nenvironment, and ensures seamless interaction between di\ufb00erent modules. However, most of the\nprotocol- and application-speci\ufb01c features are done by nginx modules, not the core. 2See http://nginx.org/en/docs/http/ngx_http_core_module.html#try_files for more details. 218\nnginx", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 232", "position": 232, "chunk_type": "semantic", "token_estimate": 209}
{"text": "Internally, nginx processes connections through a pipeline, or chain, of modules. In other words,: for every operation there\u2019s a module which is doing the relevant work; e.g., compression, modifying\ncontent, executing server-side includes, communicating to the upstream application servers through\nFastCGI or uwsgi protocols, or talking to memcached. There are a couple of nginx modules that sit somewhere between the core and the real \u201cfunctional\u201d\nmodules. These modules are http and mail. These two modules provide an additional level of\nabstraction between the core and lower-level components. In these modules, the handling of the\nsequence of events associated with a respective application layer protocol like HTTP, SMTP or IMAP\nis implemented. In combination with the nginx core, these upper-level modules are responsible for\nmaintaining the right order of calls to the respective functional modules. While the HTTP protocol\nis currently implemented as part of the http module, there are plans to separate it into a functional\nmodule in the future, due to the need to support other protocols like SPDY3. The functional modules can be divided into event modules, phase handlers, output \ufb01lters, variable\nhandlers, protocols, upstreams and load balancers. Most of these modules complement the HTTP\nfunctionality of nginx, though event modules and protocols are also used for mail. Event modules\nprovide a particular OS-dependent event noti\ufb01cation mechanism like kqueue or epoll. The event\nmodule that nginx uses depends on the operating system capabilities and build con\ufb01guration. Protocol\nmodules allow nginx to communicate through HTTPS, TLS/SSL, SMTP, POP3 and IMAP. A typical HTTP request processing cycle looks like the following:\n1. Client sends HTTP request\n2. nginx core chooses the appropriate phase handler based on the con\ufb01gured location matching\nthe request\n3. If con\ufb01gured to do so, a load balancer picks an upstream server for proxying\n4. Phase handler does its job and passes each output bu\ufb00er to the \ufb01rst \ufb01lter\n5. First \ufb01lter passes the output to the second \ufb01lter\n6. Second \ufb01lter passes the output to third (and so on)\n7. Final response is sent to the client\nnginx module invocation is extremely customizable. It is performed through a series of callbacks\nusing pointers to the executable functions. However, the downside of this is that it may place a big\nburden on programmers who would like to write their own modules, because they must de\ufb01ne exactly\nhow and when the module should run. Both the nginx API and developers\u2019 documentation are being\nimproved and made more available to alleviate this.", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 233", "position": 233, "chunk_type": "semantic", "token_estimate": 413}
{"text": "Internally, nginx processes connections through a pipeline, or chain, of modules. In other words,: However, the downside of this is that it may place a big\nburden on programmers who would like to write their own modules, because they must de\ufb01ne exactly\nhow and when the module should run. Both the nginx API and developers\u2019 documentation are being\nimproved and made more available to alleviate this. Some examples of where a module can attach are:\n\u2022 Before the con\ufb01guration \ufb01le is read and processed\n\u2022 For each con\ufb01guration directive for the location and the server where it appears\n\u2022 When the main con\ufb01guration is initialized\n\u2022 When the server (i.e., host/port) is initialized\n\u2022 When the server con\ufb01guration is merged with the main con\ufb01guration\n\u2022 When the location con\ufb01guration is initialized or merged with its parent server con\ufb01guration\n\u2022 When the master process starts or exits\n\u2022 When a new worker process starts or exits\n\u2022 When handling a request\n\u2022 When \ufb01ltering the response header and the body\n\u2022 When picking, initiating and re-initiating a request to an upstream server\n3See \"SPDY: An experimental protocol for a faster web\" at http://www.chromium.org/spdy/spdy-whitepaper\nAndrew Alexeev\n219", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 233", "position": 233, "chunk_type": "semantic", "token_estimate": 195}
{"text": "\u2022 When processing the response from an upstream server: \u2022 When \ufb01nishing an interaction with an upstream server\nInside a worker, the sequence of actions leading to the run-loop where the response is generated\nlooks like the following:\n1. Begin ngx_worker_process_cycle()\n2. Process events with OS speci\ufb01c mechanisms (such as epoll or kqueue)\n3. Accept events and dispatch the relevant actions\n4. Process/proxy request header and body\n5. Generate response content (header, body) and stream it to the client\n6. Finalize request\n7. Re-initialize timers and events\nThe run-loop itself (steps 5 and 6) ensures incremental generation of a response and streaming it\nto the client. A more detailed view of processing an HTTP request might look like this:\n1. Initialize request processing\n2. Process header\n3. Process body\n4. Call the associated handler\n5. Run through the processing phases\nWhich brings us to the phases. When nginx handles an HTTP request, it passes it through a\nnumber of processing phases. At each phase there are handlers to call. In general, phase handlers\nprocess a request and produce the relevant output. Phase handlers are attached to the locations\nde\ufb01ned in the con\ufb01guration \ufb01le. Phase handlers typically do four things: get the location con\ufb01guration, generate an appropriate\nresponse, send the header, and send the body. A handler has one argument: a speci\ufb01c structure\ndescribing the request. A request structure has a lot of useful information about the client request,\nsuch as the request method, URI, and header. When the HTTP request header is read, nginx does a lookup of the associated virtual server\ncon\ufb01guration. If the virtual server is found, the request goes through six phases:\n1. Server rewrite phase\n2. Location phase\n3. Location rewrite phase (which can bring the request back to the previous phase)\n4. Access control phase\n5. try_files phase\n6. Log phase\nIn an attempt to generate the necessary content in response to the request, nginx passes the\nrequest to a suitable content handler. Depending on the exact location con\ufb01guration, nginx may try\nso-called unconditional handlers \ufb01rst, like perl, proxy_pass, flv, mp4, etc. If the request does not\nmatch any of the above content handlers, it is picked by one of the following handlers, in this exact\norder: random index, index, autoindex, gzip_static, static. Indexing module details can be found in the nginx documentation, but these are the modules\nwhich handle requests with a trailing slash.", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 234", "position": 234, "chunk_type": "semantic", "token_estimate": 400}
{"text": "\u2022 When processing the response from an upstream server: If the request does not\nmatch any of the above content handlers, it is picked by one of the following handlers, in this exact\norder: random index, index, autoindex, gzip_static, static. Indexing module details can be found in the nginx documentation, but these are the modules\nwhich handle requests with a trailing slash. If a specialized module like mp4 or autoindex isn\u2019t\nappropriate, the content is considered to be just a \ufb01le or directory on disk (that is, static) and is served\nby the static content handler. For a directory it would automatically rewrite the URI so that the\ntrailing slash is always there (and then issue an HTTP redirect). 220\nnginx", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 234", "position": 234, "chunk_type": "semantic", "token_estimate": 121}
{"text": "The content handlers\u2019 content is then passed to the \ufb01lters. Filters are also attached to locations,: and there can be several \ufb01lters con\ufb01gured for a location. Filters do the task of manipulating the\noutput produced by a handler. The order of \ufb01lter execution is determined at compile time. For the\nout-of-the-box \ufb01lters it\u2019s prede\ufb01ned, and for a third-party \ufb01lter it can be con\ufb01gured at the build stage. In the existing nginx implementation, \ufb01lters can only do outbound changes and there is currently no\nmechanism to write and attach \ufb01lters to do input content transformation. Input \ufb01ltering will appear\nin future versions of nginx. Filters follow a particular design pattern. A \ufb01lter gets called, starts working, and calls the next\n\ufb01lter until the \ufb01nal \ufb01lter in the chain is called. After that, nginx \ufb01nalizes the response. Filters don\u2019t\nhave to wait for the previous \ufb01lter to \ufb01nish. The next \ufb01lter in a chain can start its own work as soon\nas the input from the previous one is available (functionally much like the Unix pipeline). In turn,\nthe output response being generated can be passed to the client before the entire response from the\nupstream server is received. There are header \ufb01lters and body \ufb01lters; nginx feeds the header and the body of the response to\nthe associated \ufb01lters separately. A header \ufb01lter consists of three basic steps:\n1. Decide whether to operate on this response\n2. Operate on the response\n3. Call the next \ufb01lter\nBody \ufb01lters transform the generated content. Examples of body \ufb01lters include:\n\u2022 Server-side includes\n\u2022 XSLT \ufb01ltering\n\u2022 Image \ufb01ltering (for instance, resizing images on the \ufb02y)\n\u2022 Charset modi\ufb01cation\n\u2022 gzip compression\n\u2022 Chunked encoding\nAfter the \ufb01lter chain, the response is passed to the writer. Along with the writer there are a couple\nof additional special purpose \ufb01lters, namely the copy \ufb01lter, and the postpone \ufb01lter. The copy \ufb01lter\nis responsible for \ufb01lling memory bu\ufb00ers with the relevant response content which might be stored in\na proxy temporary directory. The postpone \ufb01lter is used for subrequests. Subrequests are a very important mechanism for request/response processing. Subrequests are\nalso one of the most powerful aspects of nginx. With subrequests nginx can return the results from\na di\ufb00erent URL than the one the client originally requested. Some web frameworks call this an\ninternal redirect. However, nginx goes further\u2014not only can \ufb01lters perform multiple subrequests and\ncombine the outputs into a single response, but subrequests can also be nested and hierarchical.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 235", "position": 235, "chunk_type": "semantic", "token_estimate": 415}
{"text": "The content handlers\u2019 content is then passed to the \ufb01lters. Filters are also attached to locations,: Some web frameworks call this an\ninternal redirect. However, nginx goes further\u2014not only can \ufb01lters perform multiple subrequests and\ncombine the outputs into a single response, but subrequests can also be nested and hierarchical. A\nsubrequest can perform its own sub-subrequest, and a sub-subrequest can initiate sub-sub-subrequests. Subrequests can map to \ufb01les on the hard disk, other handlers, or upstream servers. Subrequests are\nmost useful for inserting additional content based on data from the original response. For example,\nthe SSI (server-side include) module uses a \ufb01lter to parse the contents of the returned document,\nand then replaces include directives with the contents of speci\ufb01ed URLs. Or, it can be an example\nof making a \ufb01lter that treats the entire contents of a document as a URL to be retrieved, and then\nappends the new document to the URL itself. Upstream and load balancers are also worth describing brie\ufb02y. Upstreams are used to implement\nwhat can be identi\ufb01ed as a content handler which is a reverse proxy (proxy_pass handler). Upstream\nmodules mostly prepare the request to be sent to an upstream server (or \u201cbackend\u201d) and receive\nthe response from the upstream server. There are no calls to output \ufb01lters here. What an upstream\nAndrew Alexeev\n221", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 235", "position": 235, "chunk_type": "semantic", "token_estimate": 221}
{"text": "module does exactly is set callbacks to be invoked when the upstream server is ready to be written to: and read from. Callbacks implementing the following functionality exist:\n\u2022 Crafting a request bu\ufb00er (or a chain of them) to be sent to the upstream server\n\u2022 Re-initializing/resetting the connection to the upstream server (which happens right before\ncreating the request again)\n\u2022 Processing the \ufb01rst bits of an upstream response and saving pointers to the payload received\nfrom the upstream server\n\u2022 Aborting requests (which happens when the client terminates prematurely)\n\u2022 Finalizing the request when nginx \ufb01nishes reading from the upstream server\n\u2022 Trimming the response body (e.g. removing a trailer)\nLoad balancer modules attach to the proxy_pass handler to provide the ability to choose an\nupstream server when more than one upstream server is eligible. A load balancer registers an enabling\ncon\ufb01guration \ufb01le directive, provides additional upstream initialization functions (to resolve upstream\nnames in DNS, etc. ), initializes the connection structures, decides where to route the requests, updates\nstats information. Currently nginx supports two standard disciplines for load balancing to upstream\nservers: round-robin and ip-hash. Upstream and load balancing handling mechanisms include algorithms to detect failed upstream\nservers and to re-route new requests to the remaining ones\u2014though a lot of additional work is\nplanned to enhance this functionality. In general, more work on load balancers is planned, and in the\nnext versions of nginx the mechanisms for distributing the load across di\ufb00erent upstream servers as\nwell as health checks will be greatly improved. There are also a couple of other interesting modules which provide an additional set of variables\nfor use in the con\ufb01guration \ufb01le. While the variables in nginx are created and updated across di\ufb00erent\nmodules, there are two modules that are entirely dedicated to variables: geo and map. The geo\nmodule is used to facilitate tracking of clients based on their IP addresses. This module can create\narbitrary variables that depend on the client\u2019s IP address. The other module, map, allows for the\ncreation of variables from other variables, essentially providing the ability to do \ufb02exible mappings\nof hostnames and other run-time variables. This kind of module may be called the variable handler. Memory allocation mechanisms implemented inside a single nginx worker were, to some extent,\ninspired by Apache.", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 236", "position": 236, "chunk_type": "semantic", "token_estimate": 383}
{"text": "module does exactly is set callbacks to be invoked when the upstream server is ready to be written to: This kind of module may be called the variable handler. Memory allocation mechanisms implemented inside a single nginx worker were, to some extent,\ninspired by Apache. A high-level description of nginx memory management would be the following:\nFor each connection, the necessary memory bu\ufb00ers are dynamically allocated, linked, used for\nstoring and manipulating the header and body of the request and the response, and then freed upon\nconnection release. It is very important to note that nginx tries to avoid copying data in memory as\nmuch as possible and most of the data is passed along by pointer values, not by calling memcpy. Going a bit deeper, when the response is generated by a module, the retrieved content is put in a\nmemory bu\ufb00er which is then added to a bu\ufb00er chain link. Subsequent processing works with this\nbu\ufb00er chain link as well. Bu\ufb00er chains are quite complicated in nginx because there are several\nprocessing scenarios which di\ufb00er depending on the module type. For instance, it can be quite tricky\nto manage the bu\ufb00ers precisely while implementing a body \ufb01lter module. Such a module can only\noperate on one bu\ufb00er (chain link) at a time and it must decide whether to overwrite the input bu\ufb00er,\nreplace the bu\ufb00er with a newly allocated bu\ufb00er, or insert a new bu\ufb00er before or after the bu\ufb00er in\nquestion. To complicate things, sometimes a module will receive several bu\ufb00ers so that it has an\nincomplete bu\ufb00er chain that it must operate on. However, at this time nginx provides only a low-level\nAPI for manipulating bu\ufb00er chains, so before doing any actual implementation a third-party module\ndeveloper should become really \ufb02uent with this arcane part of nginx. A note on the above approach is that there are memory bu\ufb00ers allocated for the entire life of a\nconnection, thus for long-lived connections some extra memory is kept. At the same time, on an idle\n222\nnginx", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 236", "position": 236, "chunk_type": "semantic", "token_estimate": 339}
{"text": "keepalive connection, nginx spends just 550 bytes of memory. A possible optimization for future: releases of nginx would be to reuse and share memory bu\ufb00ers for long-lived connections. The task of managing memory allocation is done by the nginx pool allocator. Shared memory\nareas are used to accept mutex, cache metadata, the SSL session cache and the information associated\nwith bandwidth policing and management (limits). There is a slab allocator implemented in nginx to\nmanage shared memory allocation. To allow simultaneous safe use of shared memory, a number\nof locking mechanisms are available (mutexes and semaphores). In order to organize complex data\nstructures, nginx also provides a red-black tree implementation. Red-black trees are used to keep\ncache metadata in shared memory, track non-regex location de\ufb01nitions and for a couple of other\ntasks. Unfortunately, all of the above was never described in a consistent and simple manner, making\nthe job of developing third-party extensions for nginx quite complicated. Although some good\ndocuments on nginx internals exist\u2014for instance, those produced by Evan Miller\u2014such documents\nrequired a huge reverse engineering e\ufb00ort, and the implementation of nginx modules is still a black\nart for many. Despite certain di\ufb03culties associated with third-party module development, the nginx user\ncommunity recently saw a lot of useful third-party modules. There is, for instance, an embedded Lua\ninterpreter module for nginx, additional modules for load balancing, full WebDAV support, advanced\ncache control and other interesting third-party work that the authors of this chapter encourage and\nwill support in the future. 14.5\nLessons Learned\nWhen Igor Sysoev started to write nginx, most of the software enabling the Internet already existed,\nand the architecture of such software typically followed de\ufb01nitions of legacy server and network\nhardware, operating systems, and old Internet architecture in general. However, this didn\u2019t prevent\nIgor from thinking he might be able to improve things in the web servers area. So, while the \ufb01rst\nlesson might seem obvious, it is this: there is always room for improvement. With the idea of better web software in mind, Igor spent a lot of time developing the initial code\nstructure and studying di\ufb00erent ways of optimizing the code for a variety of operating systems. Ten\nyears later he is developing a prototype of nginx version 2.0, taking into account the years of active\ndevelopment on version 1. It is clear that the initial prototype of a new architecture, and the initial\ncode structure, are vitally important for the future of a software product.", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 237", "position": 237, "chunk_type": "semantic", "token_estimate": 412}
{"text": "keepalive connection, nginx spends just 550 bytes of memory. A possible optimization for future: Ten\nyears later he is developing a prototype of nginx version 2.0, taking into account the years of active\ndevelopment on version 1. It is clear that the initial prototype of a new architecture, and the initial\ncode structure, are vitally important for the future of a software product. Another point worth mentioning is that development should be focused. The Windows version\nof nginx is probably a good example of how it is worth avoiding the dilution of development e\ufb00orts\non something that is neither the developer\u2019s core competence or the target application. It is equally\napplicable to the rewrite engine that appeared during several attempts to enhance nginx with more\nfeatures for backward compatibility with the existing legacy setups. Last but not least, it is worth mentioning that despite the fact that the nginx developer community\nis not very large, third-party modules and extensions for nginx have always been a very important\npart of its popularity. The work done by Evan Miller, Piotr Sikora, Valery Kholodkov, Zhang\nYichun (agentzh) and other talented software engineers has been much appreciated by the nginx user\ncommunity and its original developers. 223", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 237", "position": 237, "chunk_type": "semantic", "token_estimate": 203}
{"text": "[chapter15]: Open MPI\nJe\ufb00rey M. Squyres\n15.1\nBackground\nOpen MPI [GFB+04] is an open source software implementation of The Message Passing Interface\n(MPI) standard. Before the architecture and innards of Open MPI will make any sense, a little\nbackground on the MPI standard must be discussed. The Message Passing Interface (MPI)\nThe MPI standard is created and maintained by the MPI Forum1, an open group consisting of\nparallel computing experts from both industry and academia. MPI de\ufb01nes an API that is used for a\nspeci\ufb01c type of portable, high-performance inter-process communication (IPC): message passing. Speci\ufb01cally, the MPI document describes the reliable transfer of discrete, typed messages between\nMPI processes. Although the de\ufb01nition of an \u201cMPI process\u201d is subject to interpretation on a given\nplatform, it usually corresponds to the operating system\u2019s concept of a process (e.g., a POSIX\nprocess). MPI is speci\ufb01cally intended to be implemented as middleware, meaning that upper-level\napplications call MPI functions to perform message passing. MPI de\ufb01nes a high-level API, meaning that it abstracts away whatever underlying transport is\nactually used to pass messages between processes. The idea is that sending-process X can e\ufb00ectively\nsay \u201ctake this array of 1,073 double precision values and send them to process Y \u201d. The corresponding\nreceiving-process Y e\ufb00ectively says \u201creceive an array of 1,073 double precision values from process\nX.\u201d A miracle occurs, and the array of 1,073 double precision values arrives in Y \u2019s waiting bu\ufb00er. Notice what is absent in this exchange: there is no concept of a connection occurring, no stream\nof bytes to interpret, and no network addresses exchanged. MPI abstracts all of that away, not only\nto hide such complexity from the upper-level application, but also to make the application portable\nacross di\ufb00erent environments and underlying message passing transports. Speci\ufb01cally, a correct\nMPI application is source-compatible across a wide variety of platforms and network types. MPI de\ufb01nes not only point-to-point communication (e.g., send and receive), it also de\ufb01nes other\ncommunication patterns, such as collective communication. Collective operations are where multiple\nprocesses are involved in a single communication action. Reliable broadcast, for example, is where\none process has a message at the beginning of the operation, and at the end of the operation, all\n1http://www.mpi-forum.org/", "domains": ["Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 239", "position": 239, "chunk_type": "semantic", "token_estimate": 370}
{"text": "processes in a group have the message. MPI also de\ufb01nes other concepts and communications patterns: that are not described here.2\nUses of MPI\nThere are many implementations of the MPI standard that support a wide variety of platforms,\noperating systems, and network types. Some implementations are open source, some are closed\nsource. Open MPI, as its name implies, is one of the open source implementations. Typical MPI\ntransport networks include (but are not limited to): various protocols over Ethernet (e.g., TCP,\niWARP, UDP, raw Ethernet frames, etc. ), shared memory, and In\ufb01niBand. MPI implementations are typically used in so-called \u201chigh-performance computing\u201d (HPC)\nenvironments. MPI essentially provides the IPC for simulation codes, computational algorithms,\nand other \u201cbig number crunching\u201d types of applications. The input data sets on which these codes\noperate typically represent too much computational work for just one server; MPI jobs are spread out\nacross tens, hundreds, or even thousands of servers, all working in concert to solve one computational\nproblem. That is, the applications using MPI are both parallel in nature and highly compute-intensive. It is not unusual for all the processor cores in an MPI job to run at 100% utilization. To be clear,\nMPI jobs typically run in dedicated environments where the MPI processes are the only application\nrunning on the machine (in addition to bare-bones operating system functionality, of course). As such, MPI implementations are typically focused on providing extremely high performance,\nmeasured by metrics such as:\n\u2022 Extremely low latency for short message passing. As an example, a 1-byte message can be\nsent from a user-level Linux process on one server, through an In\ufb01niBand switch, and received\nat the target user-level Linux process on a di\ufb00erent server in a little over 1 microsecond (i.e.,\n0.000001 second). \u2022 Extremely high message network injection rate for short messages. Some vendors have MPI\nimplementations (paired with speci\ufb01ed hardware) that can inject up to 28 million messages\nper second into the network. \u2022 Quick ramp-up (as a function of message size) to the maximum bandwidth supported by the\nunderlying transport. \u2022 Low resource utilization. All resources used by MPI (e.g., memory, cache, and bus bandwidth)\ncannot be used by the application. MPI implementations therefore try to maintain a balance of\nlow resource utilization while still providing high performance. Open MPI\nThe \ufb01rst version of the MPI standard, MPI-1.0, was published in 1994 [Mes93]. MPI-2.0, a set of\nadditions on top of MPI-1, was completed in 1996 [GGHL+96].", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 240", "position": 240, "chunk_type": "semantic", "token_estimate": 409}
{"text": "processes in a group have the message. MPI also de\ufb01nes other concepts and communications patterns: Open MPI\nThe \ufb01rst version of the MPI standard, MPI-1.0, was published in 1994 [Mes93]. MPI-2.0, a set of\nadditions on top of MPI-1, was completed in 1996 [GGHL+96]. In the \ufb01rst decade after MPI-1 was published, a variety of MPI implementations sprung up. Many\nwere provided by vendors for their proprietary network interconnects. Many other implementations\narose from the research and academic communities. Such implementations were typically \u201cresearch-\nquality,\u201d meaning that their purpose was to investigate various high-performance networking concepts\nand provide proofs-of-concept of their work. However, some were high enough quality that they\ngained popularity and a number of users. 2As of this writing, the most recent version of the MPI standard is MPI-2.2 [For09]. Draft versions of the upcoming MPI-3\nstandard have been published; it may be \ufb01nalized as early as late 2012. 226\nOpen MPI", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 240", "position": 240, "chunk_type": "semantic", "token_estimate": 154}
{"text": "Open MPI represents the union of four research/academic, open source MPI implementations:: LAM/MPI, LA/MPI (Los Alamos MPI), and FT-MPI (Fault-Tolerant MPI). The members of the\nPACX-MPI team joined the Open MPI group shortly after its inception. The members of these four development teams decided to collaborate when we had the collective\nrealization that, aside from minor di\ufb00erences in optimizations and features, our software code bases\nwere quite similar. Each of the four code bases had their own strengths and weaknesses, but on the\nwhole, they more-or-less did the same things. So why compete? Why not pool our resources, work\ntogether, and make an even better MPI implementation? After much discussion, the decision was made to abandon our four existing code bases and take\nonly the best ideas from the prior projects. This decision was mainly predicated upon the following\npremises:\n\u2022 Even though many of the underlying algorithms and techniques were similar among the four\ncode bases, they each had radically di\ufb00erent implementation architectures, and would be\nincredible di\ufb03cult (if not impossible) to merge. \u2022 Each of the four also had their own (signi\ufb01cant) strengths and (signi\ufb01cant) weaknesses. Specif-\nically, there were features and architecture decisions from each of the four that were desirable\nto carry forward. Likewise, there were poorly optimized and badly designed code in each of\nthe four that were desirable to leave behind. \u2022 The members of the four developer groups had not worked directly together before. Starting\nwith an entirely new code base (rather than advancing one of the existing code bases) put all\ndevelopers on equal ground. Thus, Open MPI was born. Its \ufb01rst Subversion commit was on November 22, 2003. 15.2\nArchitecture\nFor a variety of reasons (mostly related to either performance or portability), C and C++ were the only\ntwo possibilities for the primary implementation language. C++ was eventually discarded because\ndi\ufb00erent C++ compilers tend to lay out structs/classes in memory according to di\ufb00erent optimization\nalgorithms, leading to di\ufb00erent on-the-wire network representations. C was therefore chosen as the\nprimary implementation language, which in\ufb02uenced several architectural design decisions. When Open MPI was started, we knew that it would be a large, complex code base:\n\u2022 In 2003, the current version of the MPI standard, MPI-2.0, de\ufb01ned over 300 API functions. \u2022 Each of the four prior projects were large in themselves. For example, LAM/MPI had over\n1,900 \ufb01les of source code, comprising over 300,000 lines of code (including comments and\nblanks).", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 241", "position": 241, "chunk_type": "semantic", "token_estimate": 408}
{"text": "Open MPI represents the union of four research/academic, open source MPI implementations:: \u2022 Each of the four prior projects were large in themselves. For example, LAM/MPI had over\n1,900 \ufb01les of source code, comprising over 300,000 lines of code (including comments and\nblanks). \u2022 We wanted Open MPI to support more features, environments, and networks than all four prior\nprojects put together. We therefore spent a good deal of time designing an architecture that focused on three things:\n1. Grouping similar functionality together in distinct abstraction layers\n2. Using run-time loadable plugins and run-time parameters to choose between multiple di\ufb00erent\nimplementations of the same behavior\n3. Not allowing abstraction to get in the way of performance\nJe\ufb00rey M. Squyres\n227", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 241", "position": 241, "chunk_type": "semantic", "token_estimate": 120}
{"text": "!\"#$%: !&'(%\n! )*+,-./%0123*4%\n5,+67,+*%\n&'(%8))9:;,-<.%\n! '8=%\nFigure 15.1: Abstraction layer architectural view of Open MPI showing its three main layers: OPAL, ORTE,\nand OMPI\nAbstraction Layer Architecture\nOpen MPI has three main abstraction layers, shown in Figure 15.1:\n\u2022 Open, Portable Access Layer (OPAL): OPAL is the bottom layer of Open MPI\u2019s abstractions. Its abstractions are focused on individual processes (versus parallel jobs). It provides utility\nand glue code such as generic linked lists, string manipulation, debugging controls, and other\nmundane\u2014yet necessary\u2014functionality. OPAL also provides Open MPI\u2019s core portability between di\ufb00erent operating systems, such as\ndiscovering IP interfaces, sharing memory between processes on the same server, processor\nand memory a\ufb03nity, high-precision timers, etc. \u2022 Open MPI Run-Time Environment (ORTE)3: An MPI implementation must provide not only the\nrequired message passing API, but also an accompanying run-time system to launch, monitor,\nand kill parallel jobs. In Open MPI\u2019s case, a parallel job is comprised of one or more processes\nthat may span multiple operating system instances, and are bound together to act as a single,\ncohesive unit. In simple environments with little or no distributed computational support, ORTE uses rsh\nor ssh to launch the individual processes in parallel jobs. More advanced, HPC-dedicated\nenvironments typically have schedulers and resource managers for fairly sharing computational\nresources between many users. Such environments usually provide specialized APIs to launch\nand regulate processes on compute servers. ORTE supports a wide variety of such managed\nenvironments, such as (but not limited to): Torque/PBS Pro, SLURM, Oracle Grid Engine,\nand LSF. \u2022 Open MPI (OMPI): The MPI layer is the highest abstraction layer, and is the only one exposed\nto applications. The MPI API is implemented in this layer, as are all the message passing\nsemantics de\ufb01ned by the MPI standard. Since portability is a primary requirement, the MPI layer supports a wide variety of network\ntypes and underlying protocols. Some networks are similar in their underlying characteristics\nand abstractions; some are not. Although each abstraction is layered on top of the one below it, for performance reasons the\nORTE and OMPI layers can bypass the underlying abstraction layers and interact directly with the\n3Pronounced \u201cor-tay\u201d. 228\nOpen MPI", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 242", "position": 242, "chunk_type": "semantic", "token_estimate": 364}
{"text": "operating system and/or hardware when needed (as depicted in Figure 15.1). For example, the: OMPI layer uses OS-bypass methods to communicate with certain types of NIC hardware to obtain\nmaximum networking performance. Each layer is built into a standalone library. The ORTE library depends on the OPAL library; the\nOMPI library depends on the ORTE library. Separating the layers into their own libraries has acted\nas a wonderful tool for preventing abstraction violations. Speci\ufb01cally, applications will fail to link\nif one layer incorrectly attempts to use a symbol in a higher layer. Over the years, this abstraction\nenforcement mechanism has saved many developers from inadvertently blurring the lines between\nthe three layers. Plugin Architecture\nAlthough the initial members of the Open MPI collaboration shared a similar core goal (produce a\nportable, high-performance implementation of the MPI standard), our organizational backgrounds,\nopinions, and agendas were\u2014and still are\u2014wildly di\ufb00erent. We therefore spent a considerable\namount of time designing an architecture that would allow us to be di\ufb00erent, even while sharing a\ncommon code base. Run-time loadable components were a natural choice (a.k.a., dynamic shared objects, or \u201cDSOs\u201d,\nor \u201cplugins\u201d). Components enforce a common API but place few limitations on the implementation of\nthat API. Speci\ufb01cally: the same interface behavior can be implemented multiple di\ufb00erent ways. Users\ncan then choose, at run time, which plugin(s) to use. This even allows third parties to independently\ndevelop and distribute their own Open MPI plugins outside of the core Open MPI package. Allowing\narbitrary extensibility is quite a liberating policy, both within the immediate set of Open MPI\ndevelopers and in the greater Open MPI community. This run-time \ufb02exibility is a key component of the Open MPI design philosophy and is deeply\nintegrated throughout the entire architecture. Case in point: the Open MPI v1.5 series includes\n155 plugins. To list just a few examples, there are plugins for di\ufb00erent memcpy() implementations,\nplugins for how to launch processes on remote servers, and plugins for how to communicate on\ndi\ufb00erent types of underlying networks. One of the major bene\ufb01ts of using plugins is that multiple groups of developers have freedom\nto experiment with alternate implementations without a\ufb00ecting the core of Open MPI. This was a\ncritical feature, particularly in the early days of the Open MPI project. Sometimes the developers\ndidn\u2019t always know what was the right way to implement something, or sometimes they just disagreed.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 243", "position": 243, "chunk_type": "semantic", "token_estimate": 398}
{"text": "operating system and/or hardware when needed (as depicted in Figure 15.1). For example, the: This was a\ncritical feature, particularly in the early days of the Open MPI project. Sometimes the developers\ndidn\u2019t always know what was the right way to implement something, or sometimes they just disagreed. In both cases, each party would implement their solution in a component, allowing the rest of the\ndeveloper community to easily compare and contrast. Code comparisons can be done without\ncomponents, of course, but the component concept helps guarantee that all implementations expose\nexactly the same external API, and therefore provide exactly the same required semantics. As a direct result of the \ufb02exibility that it provides, the component concept is utilized heavily\nthroughout all three layers of Open MPI; in each layer there are many di\ufb00erent types of components. Each type of component is enclosed in a framework. A component belongs to exactly one framework,\nand a framework supports exactly one kind of component. Figure 15.2 is a template of Open MPI\u2019s\narchitectural layout; it shows a few of Open MPI\u2019s frameworks and some of the components that\nthey contain. (The rest of Open MPI\u2019s frameworks and components are laid out in the same manner.) Open MPI\u2019s set of layers, frameworks, and components is referred to as the Modular Component\nArchitecture (MCA). Finally, another major advantage of using frameworks and components is their inherent compos-\nability. With over 40 frameworks in Open MPI v1.5, giving users the ability to mix-n-match di\ufb00erent\nJe\ufb00rey M. Squyres\n229", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 243", "position": 243, "chunk_type": "semantic", "token_estimate": 253}
{"text": "!\"#$%&'(%)*+#%,!'-./%!012/%3$4%!&'(%536#+78%: &'(%96:#%\n:+3$7;#+%536#+%\n,9:58%\n&'(%)*55#)<=#%\n*\"#+3<*$7%\n,)*558%\n>? @A%\n+#7*5B<*$%\n<C#+7%,<C#+8%\nD37#%\n1E'%\nFA3+#4%C#C*+6%\n! \"#$G39+? )7%\nH%\nD37#%\n1B$#4%\nFA3+#4%C#C*+6%\n. **\"93)I%\nH%\nD37#%\n+7A%%J%77A%\nF.K0&%\n-. 'F%,E+368%\nH%\nD37#%\n. ?$BL%\nM3+N?$%\nO?$4*N7%\nH%\nH%\nG+3C#N*+I7%\nD37#%3$4%\n)*C\"*$#$:7%\n,?P#P/%\"5B@?$78%\n'+*)#77%53B$)A?$@%\n3$4%C*$? :*+?$@%\n,\"5C8%\nFigure 15.2: Framework architectural view of Open MPI, showing just a few of Open MPI\u2019s frameworks and\ncomponents (i.e., plugins). Each framework contains a base and one or more components. This structure is\nreplicated in each of the layers shown in Figure 15.1. The sample frameworks listed in this \ufb01gure are spread\nacross all three layers: btl and coll are in the OMPI layer, plm is in the ORTE layer, and timer is in the OPAL\nlayer. plugins of di\ufb00erent types allows them to create a software stack that is e\ufb00ectively tailored to their\nindividual system. Plugin Frameworks\nEach framework is fully self-contained in its own subdirectory in the Open MPI source code tree. The\nname of the subdirectory is the same name as the framework; for example, the memory framework is\nin the memory directory. Framework directories contain at least the following three items:\n1. Component interface de\ufb01nition: A header \ufb01le named <framework>.h will be located in the\ntop-level framework directory (e.g., the Memory framework contains memory/memory.h). This well-known header \ufb01le de\ufb01nes the interfaces that each component in the framework must\nsupport. This header includes function pointer typedefs for the interface functions, structs for\nmarshaling these function pointers, and any other necessary types, attribute \ufb01elds, macros,\ndeclarations, etc. 2. Base code: The base subdirectory contains the glue code that provides the core functionality\nof the framework. For example, the memory framework\u2019s base directory is memory/base. The\nbase is typically comprised of logistical grunt work such as \ufb01nding and opening components\nat run-time, common utility functionality that may be utilized by multiple components, etc. 3. Components: All other subdirectories in the framework directory are assumed to be compo-\nnents. Just like the framework, the names of the components are the same names as their\nsubdirectories (e.g., the memory/posix subdirectory contains the POSIX component in the\nMemory framework). Similar to how each framework de\ufb01nes the interfaces to which its components must adhere,\nframeworks also de\ufb01ne other operational aspects, such as how they bootstrap themselves, how they\npick components to use, and how they are shut down.", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 244", "position": 244, "chunk_type": "semantic", "token_estimate": 390}
{"text": "!\"#$%&'(%)*+#%,!'-./%!012/%3$4%!&'(%536#+78%: Just like the framework, the names of the components are the same names as their\nsubdirectories (e.g., the memory/posix subdirectory contains the POSIX component in the\nMemory framework). Similar to how each framework de\ufb01nes the interfaces to which its components must adhere,\nframeworks also de\ufb01ne other operational aspects, such as how they bootstrap themselves, how they\npick components to use, and how they are shut down. Two common examples of how frameworks\ndi\ufb00er in their setup are many-of-many versus one-of-many frameworks, and static versus dynamic\nframeworks. 230\nOpen MPI", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 244", "position": 244, "chunk_type": "semantic", "token_estimate": 90}
{"text": "Many-of-many frameworks.: Some frameworks have functionality that can be implemented\nmultiple di\ufb00erent ways in the same process. For example, Open MPI\u2019s point-to-point network\nframework will load multiple driver plugins to allow a single process to send and receive messages\non multiple network types. Such frameworks will typically open all components that they can \ufb01nd and then query each\ncomponent, e\ufb00ectively asking, \u201cDo you want to run?\u201d The components determine whether they want\nto run by examining the system on which they are running. For example, a point-to-point network\ncomponent will look to see if the network type it supports is both available and active on the system. If it is not, the component will reply \u201cNo, I do not want to run\u201d, causing the framework to close and\nunload that component. If that network type is available, the component will reply \u201cYes, I want to\nrun\u201d, causing the framework to keep the component open for further use. One-of-many frameworks. Other frameworks provide functionality for which it does not make\nsense to have more than one implementation available at run-time. For example, the creation of\na consistent checkpoint of a parallel job\u2014meaning that the job is e\ufb00ectively \u201cfrozen\u201d and can be\narbitrarily resumed later\u2014must be performed using the same back-end checkpointing system for\neach process in the job. The plugin that interfaces to the desired back-end checkpointing system is\nthe only checkpoint plugin that must be loaded in each process\u2014all others are unnecessary. Dynamic frameworks. Most frameworks allow their components to be loaded at run-time via\nDSOs. This is the most \ufb02exible method of \ufb01nding and loading components; it allows features such\nas explicitly not loading certain components, loading third-party components that were not included\nin the main-line Open MPI distribution, etc. Static frameworks. Some one-of-many frameworks have additional constraints that force their\none-and-only-one component to be selected at compile time (versus run time). Statically linking one-\nof-many components allows direct invocation of its member functions (versus invocation via function\npointer), which may be important in highly performance-sensitive functionality. One example is the\nmemcpy framework, which provides platform-optimized memcpy() implementations. Additionally, some frameworks provide functionality that may need to be utilized before Open\nMPI is fully initialized. For example, the use of some network stacks require complicated memory\nregistration models, which, in turn, require replacing the C library\u2019s default memory management\nroutines.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 245", "position": 245, "chunk_type": "semantic", "token_estimate": 391}
{"text": "Many-of-many frameworks.: Additionally, some frameworks provide functionality that may need to be utilized before Open\nMPI is fully initialized. For example, the use of some network stacks require complicated memory\nregistration models, which, in turn, require replacing the C library\u2019s default memory management\nroutines. Since memory management is intrinsic to an entire process, replacing the default scheme\ncan only be done pre-main. Therefore, such components must be statically linked into Open MPI\nprocesses so that they can be available for pre-main hooks, long before MPI has even been initialized. Plugin Components\nOpen MPI plugins are divided into two parts: a component struct and a module struct. The component\nstruct and the functions to which it refers are typically collectively referred to as \u201cthe component.\u201d\nSimilarly, \u201cthe module\u201d collectively refers to the module struct and its functions. The division\nis somewhat analogous to C++ classes and objects. There is only one component per process; it\ndescribes the overall plugin with some \ufb01elds that are common to all components (regardless of\nframework). If the component elects to run, it is used to generate one or more modules, which\ntypically perform the bulk of the functionality required by the framework. Je\ufb00rey M. Squyres\n231", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 245", "position": 245, "chunk_type": "semantic", "token_estimate": 201}
{"text": "Throughout the next few sections, we\u2019ll build up the structures necessary for the TCP component: in the BTL (byte transfer layer) framework. The BTL framework e\ufb00ects point-to-point message\ntransfers; the TCP component, not surprisingly, uses TCP as its underlying transport for message\npassing. Component struct. Regardless of framework, each component contains a well-known, statically\nallocated and initialized component struct. The struct must be named according to the template\nmca_<framework>_<component>_component. For example, the TCP network driver component\u2019s\nstruct in the BTL framework is named mca_btl_tcp_component. Having templated component symbols both guarantees that there will be no name collisions\nbetween components, and allows the MCA core to \ufb01nd any arbitrary component struct via dlsym(2)\n(or the appropriate equivalent in each supported operating system). The base component struct contains some logistical information, such as the component\u2019s formal\nname, version, framework version adherence, etc. This data is used for debugging purposes, inventory\nlisting, and run-time compliance and compatibility checking. struct mca_base_component_2_0_0_t {\n/* Component struct version number */\nint mca_major_version, mca_minor_version, mca_release_version;\n/* The string name of the framework that this component belongs to,\nand the framework\u2019s API version that this component adheres to */\nchar mca_type_name[MCA_BASE_MAX_TYPE_NAME_LEN + 1];\nint mca_type_major_version, mca_type_minor_version,\nmca_type_release_version;\n/* This component\u2019s name and version number */\nchar mca_component_name[MCA_BASE_MAX_COMPONENT_NAME_LEN + 1];\nint mca_component_major_version, mca_component_minor_version,\nmca_component_release_version;\n/* Function pointers */\nmca_base_open_component_1_0_0_fn_t mca_open_component;\nmca_base_close_component_1_0_0_fn_t mca_close_component;\nmca_base_query_component_2_0_0_fn_t mca_query_component;\nmca_base_register_component_params_2_0_0_fn_t\nmca_register_component_params;\n};\nThe base component struct is the core of the TCP BTL component; it contains the following\nfunction pointers:\n\u2022 Open. The open call is the initial query function invoked on a component. It allows a component\nto initialize itself, look around the system where it is running, and determine whether it wants\nto run. If a component can always be run, it can provide a NULL open function pointer. The TCP BTL component open function mainly initializes some data structures and ensures\nthat invalid parameters were not set by the user. \u2022 Close. When a framework decides that a component is no longer needed, it calls the close\nfunction to allow the component to release any resources that it has allocated. The close\nfunction is invoked on all remaining components when processes are shutting down. However,\n232\nOpen MPI", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 246", "position": 246, "chunk_type": "semantic", "token_estimate": 367}
{"text": "close can also be invoked on components that are rejected at run time so that they can be closed: and ignored for the duration of the process. The TCP BTL component close function closes listening sockets and frees resources (e.g.,\nreceiving bu\ufb00ers). \u2022 Query. This call is a generalized \u201cDo you want to run?\u201d function. Not all frameworks utilize\nthis speci\ufb01c call\u2014some need more specialized query functions. The BTL framework does not use the generic query function (it de\ufb01nes its own; see below), so\nthe TCP BTL does not \ufb01ll it in. \u2022 Parameter registration. This function is typically the \ufb01rst function called on a component. It\nallows the component to register any relevant run-time, user-settable parameters. Run-time\nparameters are discussed further below. The TCP BTL component register function creates a variety of user-settable run-time parame-\nters, such as one which allows the user to specify which IP interface(s) to use. The component structure can also be extended on a per-framework and/or per-component basis. Frameworks typically create a new component struct with the component base struct as the \ufb01rst\nmember. This nesting allows frameworks to add their own attributes and function pointers. For\nexample, a framework that needs a more specialized query function (as compared to the query\nfunction provided on the basic component) can add a function pointer in its framework-speci\ufb01c\ncomponent struct. The MPI btl framework, which provides point-to-point MPI messaging functionality, uses this\ntechnique. struct mca_btl_base_component_2_0_0_t {\n/* Base component struct */\nmca_base_component_t btl_version;\n/* Base component data block */\nmca_base_component_data_t btl_data;\n/* btl-framework specific query functions */\nmca_btl_base_component_init_fn_t btl_init;\nmca_btl_base_component_progress_fn_t btl_progress;\n};\nAs an example of the TCP BTL framework query functions, the TCP BTL component btl_init\nfunction does several things:\n\u2022 Creates a listening socket for each \u201cup\u201d IPv4 and IPv6 interface\n\u2022 Creates a module for each \u201cup\u201d IP interface\n\u2022 Registers the tuple (IP address, port) for each \u201cup\u201d IP interface with a central repository\nso that other MPI processes know how to contact it\nSimilarly, plugins can extend the framework-speci\ufb01c component struct with their own members. The tcp component in the btl framework does this; it caches many data members in its component\nstruct. struct mca_btl_tcp_component_t {\n/* btl framework-specific component struct */\nmca_btl_base_component_2_0_0_t super;\nJe\ufb00rey M. Squyres\n233", "domains": ["Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 247", "position": 247, "chunk_type": "semantic", "token_estimate": 377}
{"text": "/* Some of the TCP BTL component\u2019s specific data members */: /* Number of TCP interfaces on this server */\nuint32_t tcp_addr_count;\n/* IPv4 listening socket descriptor */\nint tcp_listen_sd;\n/* ...and many more not shown here */\n};\nThis struct-nesting technique is e\ufb00ectively a simple emulation of C++ single inheritance: a\npointer to an instance of a struct mca_btl_tcp_component_t can be cast to any of the three types\nsuch that it can be used by an abstraction layer than does not understand the \u201cderived\u201d types. That being said, casting is generally frowned upon in Open MPI because it can lead to incredibly\nsubtle, di\ufb03cult-to-\ufb01nd bugs. An exception was made for this C++-emulation technique because it\nhas well-de\ufb01ned behaviors and helps enforce abstraction barriers. Module struct. Module structs are individually de\ufb01ned by each framework; there is little com-\nmonality between them. Depending on the framework, components generate one or more module\nstruct instances to indicate that they want to be used. For example, in the BTL framework, one module usually corresponds to a single network device. If an MPI process is running on a Linux server with three \u201cup\u201d Ethernet devices, the TCP BTL\ncomponent will generate three TCP BTL modules; one corresponding to each Linux Ethernet device. Each module will then be wholly responsible for all sending and receiving to and from its Ethernet\ndevice. Tying it all together. Figure 15.3 shows the nesting of the structures in the TCP BTL component,\nand how it generates one module for each of the three Ethernet devices. ! \"#$%\"&'%()*\"+)\"%,)%-',-./. \")\"&\n! \"#$%\"&'%()*\"+)*(!/)%-',-./. \")0)1)1)\"&&\n! \"#$%\"&'%()*(!/)%-',-./. \")0)1)1)\"&&\n! \"#$%\"&'%()*\"+)\"%,)'-2$+/)\"&\n3-$.2&\"-&/\"41&\n5-',-./. \"&\n%#/(\"/!&\n&\n'-2$+/&\n! \"#$%\"!&\n! \"#$%\"&'%()*\"+)\"%,)'-2$+/)\"&\n3-$.2&\"-&/\"46&\n! \"#$%\"&'%()*\"+)\"%,)'-2$+/)\"&\n3-$.2&\"-&/\"40&\nFigure 15.3: The left side shows the nesting of structures in the TCP BTL component. The right side shows\nhow the component generates one module struct for each \u201cup\u201d Ethernet interface. Composing BTL modules this way allows the upper-layer MPI progression engine both to treat\nall network devices equally, and to perform user-level channel bonding. For example, consider sending a large message across the three-device con\ufb01guration described\nabove. Assume that each of the three Ethernet devices can be used to reach the intended receiver\n(reachability is determined by TCP networks and netmasks, and some well-de\ufb01ned heuristics). In\nthis case, the sender will split the large message into multiple fragments. Each fragment will be\n234\nOpen MPI", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 248", "position": 248, "chunk_type": "semantic", "token_estimate": 397}
{"text": "assigned\u2014in a round-robin fashion\u2014to one of the TCP BTL modules (each module will therefore: be assigned roughly one third of the fragments). Each module then sends its fragments over its\ncorresponding Ethernet device. This may seem like a complex scheme, but it is surprisingly e\ufb00ective. By pipelining the sends\nof a large message across the multiple TCP BTL modules, typical HPC environments (e.g., where\neach Ethernet device is on a separate PCI bus) can sustain nearly maximum bandwidth speeds across\nmultiple Ethernet devices. Run-Time Parameters\nDevelopers commonly make decisions when writing code, such as:\n\u2022 Should I use algorithm A or algorithm B? \u2022 How large of a bu\ufb00er should I preallocate? \u2022 How long should the timeout be? \u2022 At what message size should I change network protocols? \u2022 ...and so on. Users tend to assume that the developers will answer such questions in a way that is generally\nsuitable for most types of systems. However, the HPC community is full of scientist and engineer\npower users who want to aggressively tweak their hardware and software stacks to eke out every\npossible compute cycle. Although these users typically do not want to tinker with the actual code\nof their MPI implementation, they do want to tinker by selecting di\ufb00erent internal algorithms,\nchoosing di\ufb00erent resource consumption patterns, or forcing speci\ufb01c network protocols in di\ufb00erent\ncircumstances. Therefore, the MCA parameter system was included when designing Open MPI; the system is\na \ufb02exible mechanism that allows users to change internal Open MPI parameter values at run time. Speci\ufb01cally, developers register string and integer MCA parameters throughout the Open MPI code\nbase, along with an associated default value and descriptive string de\ufb01ning what the parameter is\nand how it is used. The general rule of thumb is that rather than hard-coding constants, developers\nuse run-time-settable MCA parameters, thereby allowing power users to tweak run-time behavior. There are a number of MCA parameters in the base code of the three abstraction layers, but the\nbulk of Open MPI\u2019s MCA parameters are located in individual components. For example, the TCL\nBTL plugin has a parameter that speci\ufb01es whether only TCPv4 interfaces, only TCPv6 interfaces, or\nboth types of interfaces should be used. Alternatively, another TCP BTL parameter can be set to\nspecify exactly which Ethernet devices to use. Users can discover what parameters are available via a user-level command line tool (ompi_info).", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 249", "position": 249, "chunk_type": "semantic", "token_estimate": 397}
{"text": "assigned\u2014in a round-robin fashion\u2014to one of the TCP BTL modules (each module will therefore: Alternatively, another TCP BTL parameter can be set to\nspecify exactly which Ethernet devices to use. Users can discover what parameters are available via a user-level command line tool (ompi_info). Parameter values can be set in multiple ways: on the command line, via environment variables, via\nthe Windows registry, or in system- or user-level INI-style \ufb01les. The MCA parameter system complements the idea of run-time plugin selection \ufb02exibility, and has\nproved to be quite valuable to users. Although Open MPI developers try hard to choose reasonable\ndefaults for a wide variety of situations, every HPC environment is di\ufb00erent. There are inevitably\nenvironments where Open MPI\u2019s default parameter values will be unsuitable\u2014and possibly even\ndetrimental to performance. The MCA parameter system allows users to be proactive and tweak\nOpen MPI\u2019s behavior for their environment. Not only does this alleviate many upstream requests for\nchanges and/or bug reports, it allows users to experiment with the parameter space to \ufb01nd the best\ncon\ufb01guration for their speci\ufb01c system. Je\ufb00rey M. Squyres\n235", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 249", "position": 249, "chunk_type": "semantic", "token_estimate": 182}
{"text": "15.3: Lessons Learned\nWith such a varied group of core Open MPI members, it is inevitable that we would each learn\nsomething, and that as a group, we would learn many things. The following list describes just a few\nof these lessons. Performance\nMessage-passing performance and resource utilization are the king and queen of high-performance\ncomputing. Open MPI was speci\ufb01cally designed in such a way that it could operate at the very\nbleeding edge of high performance: incredibly low latencies for sending short messages, extremely\nhigh short message injection rates on supported networks, fast ramp-ups to maximum bandwidth for\nlarge messages, etc. Abstraction is good (for many reasons), but it must be designed with care so\nthat it does not get in the way of performance. Or, put di\ufb00erently: carefully choose abstractions that\nlend themselves to shallow, performant call stacks (versus deep, feature-rich API call stacks). That being said, we also had to accept that in some cases, abstraction\u2014not architecture\u2014must\nbe thrown out the window. Case in point: Open MPI has hand-coded assembly for some of its most\nperformance-critical operations, such as shared memory locking and atomic operations. It is worth noting that Figures 15.1 and 15.2 show two di\ufb00erent architectural views of Open MPI. They do not represent the run-time call stacks or calling invocation layering for the high performance\ncode sections. Lesson learned:\nIt is acceptable (albeit undesirable) and unfortunately sometimes necessary to\nhave gross, complex code in the name of performance (e.g., the aforementioned assembly code). However, it is always preferable to spend time trying to \ufb01gure out how to have good abstractions to\ndiscretize and hide complexity whenever possible. A few weeks of design can save literally hundreds\nor thousands of developer-hours of maintenance on tangled, subtle, spaghetti code. Standing on the Shoulders of Giants\nWe actively tried to avoid re-inventing code in Open MPI that someone else has already written (when\nsuch code is compatible with Open MPI\u2019s BSD licensing). Speci\ufb01cally, we have no compunctions\nabout either directly re-using or interfacing to someone else\u2019s code. There is no place for the \u201cnot invented here\u201d religion when trying to solve highly complex\nengineering problems; it only makes good logistical sense to re-use external code whenever possible. Such re-use frees developers to focus on the problems unique to Open MPI; there is no sense\nre-solving a problem that someone else has solved already.", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 250", "position": 250, "chunk_type": "semantic", "token_estimate": 396}
{"text": "15.3: There is no place for the \u201cnot invented here\u201d religion when trying to solve highly complex\nengineering problems; it only makes good logistical sense to re-use external code whenever possible. Such re-use frees developers to focus on the problems unique to Open MPI; there is no sense\nre-solving a problem that someone else has solved already. A good example of this kind of code re-use is the GNU Libtool Libltdl package. Libltdl is a\nsmall library that provides a portable API for opening DSOs and \ufb01nding symbols in them. Libltdl is\nsupported on a wide variety of operating systems and environments, including Microsoft Windows. Open MPI could have provided this functionality itself\u2014but why? Libltdl is a \ufb01ne piece of\nsoftware, is actively maintained, is compatible with Open MPI\u2019s license, and provides exactly the\nfunctionality that was needed. Given these points, there is no realistic gain for Open MPI developers\nto re-write this functionality. Lesson learned:\nWhen a suitable solution exists elsewhere, do not hesitate to integrate it and stop\nwasting time trying to re-invent it. 236\nOpen MPI", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 250", "position": 250, "chunk_type": "semantic", "token_estimate": 179}
{"text": "Jennifer Ruttan: Since their initial adoption, EMR (electronic medical record) systems have attempted to bridge the\ngap between the physical and digital worlds of patient care. Governments in countries around the\nworld have attempted to come up with a solution that enables better care for patients at a lower cost\nwhile reducing the paper trail that medicine typically generates. Many governments have been very\nsuccessful in their attempts to create such a system\u2014some, like that of the Canadian province of\nOntario, have not (some may remember the so-called \u201ceHealth Scandal\u201d in Ontario that, according\nto the Auditor General, cost taxpayers $1 billion CAD). An EMR permits the digitization of a patient chart, and when used properly should make it\neasier for a physician to deliver care. A good system should provide a physician a bird\u2019s eye view\nof a patient\u2019s current and ongoing conditions, their prescription history, their recent lab results,\nhistory of their previous visits, and so on. OSCAR (Open Source Clinical Application Resource), an\napproximately ten-year-old project of McMaster University in Hamilton, Ontario, Canada, is the\nopen source community\u2019s attempt to provide such a system to physicians at low or no cost. OSCAR has many subsystems that provide functionality on a component-by-component basis. For example, oscarEncounter provides an interface for interacting with a patient\u2019s chart directly; Rx3\nis a prescription module that checks for allergies and drug interactions automatically and allows a\nphysician to directly fax a prescription to a pharmacy from the UI; the Integrator is a component to\nenable data sharing between multiple compatible EMRs. All of these separate components come\ntogether to build the typical OSCAR user experience. OSCAR won\u2019t be for every physician; for example, a specialist may not \ufb01nd all the features of\nthe system useful, and it is not easily customizable. However, it o\ufb00ers a complete set of features for\na general physician interacting with patients on a day-to-day basis. In addition, OSCAR is CMS 3.0 certi\ufb01ed (and has applied for CMS 4.0 certi\ufb01cation)\u2014which\nallows physicians to receive funding for installing it in their clinic1. Receiving CMS certi\ufb01cation\ninvolves passing a set of requirements from the Government of Ontario and paying a fee. This chapter will discuss the architecture of OSCAR in fairly general terms, describing the\nhierarchy, major components, and most importantly the impact that past decisions have made on the\nproject.", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 253", "position": 253, "chunk_type": "semantic", "token_estimate": 391}
{"text": "16.1: System Hierarchy\nAs a Tomcat web application, OSCAR generally follows the typical model-view-controller design\npattern. This means that the model code (Data Access Objects, or DAOs) is separate from the\ncontroller code (servlets) and those are separated from the views (Java Server Pages, or JSPs). The\nmost signi\ufb01cant di\ufb00erence between the two is that servlets are classes and JSPs are HTML pages\nmarked up with Java code. Data gets placed into memory when a servlet executes and the JSP reads\nthat same data, usually done via reads and writes to the attributes of the request object. Just about\nevery JSP page in OSCAR has this kind of design. 16.2\nPast Decision Making\nI mentioned that OSCAR is a fairly old project. This has implications for how e\ufb00ectively the MVC\npattern has been applied. In short, there are sections of the code that completely disregard the\npattern as they were written before tighter enforcement of the MVC pattern began. Some of the most\ncommon features are written this way; for example, performing many actions related to demographics\n(patient records) are done via the demographiccontrol.jsp \ufb01le\u2014this includes creating patients\nand updating their data. OSCAR\u2019s age is a hurdle for tackling many of the problems that are facing the source tree today. Indeed, there has been signi\ufb01cant e\ufb00ort made to improve the situation, including enforcing design\nrules via a code review process. This is an approach that the community at present has decided will\nallow better collaboration in the future, and will prevent poor code from becoming part of the code\nbase, which has been a problem in the past. This is by no means a restriction on how we could design parts of the system now; it does,\nhowever, make it more complicated when deciding to \ufb01x bugs in a dated part of OSCAR. Do you, as\nsomebody tasked to \ufb01x a bug in the Demographic Creation function, \ufb01x the bug with code in the\nsame style as it currently exists? Or do you re-write the module completely so that it closely follows\nthe MVC design pattern? As developers we must carefully weigh our options in situations like those. There is no guarantee\nthat if you re-architect a part of the system you will not create new bugs, and when patient data is on\nthe line, we must make the decision carefully.", "domains": ["Architectural Patterns and Styles", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 254", "position": 254, "chunk_type": "semantic", "token_estimate": 391}
{"text": "16.1: As developers we must carefully weigh our options in situations like those. There is no guarantee\nthat if you re-architect a part of the system you will not create new bugs, and when patient data is on\nthe line, we must make the decision carefully. 16.3\nVersion Control\nA CVS repository was used for much of OSCAR\u2019s life. Commits weren\u2019t often checked for consistency\nand it was possible to commit code that could break the build. It was tough for developers to keep up\nwith changes\u2014especially new developers joining the project late in its lifecycle. A new developer\ncould see something that they would want to change, make the change, and get it into the source\nbranch several weeks before anybody would notice that something signi\ufb01cant had been modi\ufb01ed\n(this was especially prevalent during long holidays, such as Christmas break, when not many people\nwere watching the source tree). Things have changed; OSCAR\u2019s source tree is now controlled by git. Any commits to the main\nbranch have to pass code-style checking and unit testing, successfully compile, and be code reviewed\nby the developers (much of this is handled by the combination of Hudson2 and Gerrit3). The project\n2A continuous integration server: http://hudson-ci.org/\n3A code review tool: http://code.google.com/p/gerrit/\n240", "domains": ["Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 254", "position": 254, "chunk_type": "semantic", "token_estimate": 209}
{"text": "has become much more tightly controlled. Many or all of the issues caused by poor handling of the: source tree have been solved. 16.4\nData Models/DAOs\nWhen looking through the OSCAR source, you may notice that there are many di\ufb00erent ways to\naccess the database: you can use a direct connection to the database via a class called DBHandler,\nuse a legacy Hibernate model, or use a generic JPA model. As new and easier database access models\nbecame available, they were integrated into OSCAR. The result is that there is now a slightly noisy\npicture of how OSCAR interacts with data in MySQL, and the di\ufb00erences between the three types of\ndata access methods are best described with examples. EForms (DBHandler)\nThe EForm system allows users to create their own forms to attach to patient records\u2014this feature is\nusually used to replace a paper-based form with a digital version. On each creation of a form of a\nparticular type, the form\u2019s template \ufb01le is loaded; then the data in the form is stored in the database\nfor each instance. Each instance is attached to a patient record. EForms allow you to pull in certain types of data from a patient chart or other area of the system\nvia free-form SQL queries (which are de\ufb01ned in a \ufb01le called apconfig.xml). This can be extremely\nuseful, as a form can load and then immediately be populated with demographic or other relevant\ninformation without intervention from the user; for example, you wouldn\u2019t have to type in a patient\u2019s\nname, age, date of birth, hometown, phone number, or the last note that was recorded for that patient. A design decision was made, when originally developing the EForm module, to use raw database\nqueries to populate a POJO (plain-old Java object) called EForm in the controller that is then passed\nto the view layer to display data on the screen, sort of like a JavaBean. Using a POJO in this case is\nactually closer in design to the Hibernate or JPA architecture, as I\u2019ll discuss in the next sections. All of the functionality regarding saving EForm instances and templates is done via raw SQL\nqueries run through the DBHandler class. Ultimately, DBHandler is a wrapper for a simple JDBC\nobject and does not scrutinize a query before sending it to the SQL server. It should be added here\nthat DBHandler is a potential security \ufb02aw as it allows unchecked SQL to be sent to the server.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 255", "position": 255, "chunk_type": "semantic", "token_estimate": 411}
{"text": "has become much more tightly controlled. Many or all of the issues caused by poor handling of the: Ultimately, DBHandler is a wrapper for a simple JDBC\nobject and does not scrutinize a query before sending it to the SQL server. It should be added here\nthat DBHandler is a potential security \ufb02aw as it allows unchecked SQL to be sent to the server. Any\nclass that uses DBHandler must implement its own checking to make sure that SQL injection doesn\u2019t\noccur. Depending on the type of application you\u2019re writing, direct access of a database is sometimes\n\ufb01ne. In certain cases, it can even speed development up. Using this method to access the database\ndoesn\u2019t conform to the model-view-controller design pattern, though: if you\u2019re going to change your\ndatabase structure (the model), you have to change the SQL query elsewhere (in the controller). Sometimes, adding certain columns or changing their type in OSCAR\u2019s database tables requires this\nkind of invasive procedure just to implement small features. It may not surprise you to \ufb01nd out that the DBHandler object is one of the oldest pieces of code\nstill intact in the source. I personally don\u2019t know where it originated from but I consider it to be the\nmost \u201cprimitive\u201d of database access types that exist in the OSCAR source. No new code is permitted\nto use this class, and if code is committed that uses it, the commit will be rejected automatically. Jennifer Ruttan\n241", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 255", "position": 255, "chunk_type": "semantic", "token_estimate": 244}
{"text": "Demographic Records (Hibernate): A demographic record contains general metadata about a patient; for example, their name, age,\naddress, language, and sex; consider it to be the result of an intake form that a patient \ufb01lls out during\ntheir \ufb01rst visit to a doctor. All of this data is retrieved and displayed as part of OSCAR\u2019s Master\nRecord for a speci\ufb01c demographic. Using Hibernate to access the database is far safer than using DBHandler. For one, you have\nto explicitly de\ufb01ne which columns match to which \ufb01elds in your model object (in this case, the\nDemographic class). If you want to perform complex joins, they have to be done as prepared\nstatements. Finally, you will only ever receive an object of the type you ask for when performing a\nquery, which is very convenient. The process of working with a Hibernate-style DAO and Model pair is quite simple. In the case\nof the Demographic object, there\u2019s a \ufb01le called Demographic.hbm.xml that describes the mapping\nbetween object \ufb01eld and database column. The \ufb01le describes which table to look at and what type of\nobject to return. When OSCAR starts, this \ufb01le will be read and a sanity check occurs to make sure\nthat this kind of mapping can actually be made (server startup fails if it can\u2019t). Once running, you\ngrab an instance of the DemographicDao object and run queries against it. The best part about using Hibernate over DBHandler is that all of the queries to the server are\nprepared statements. This restricts you from running free-form SQL during runtime, but it also\nprevents any type of SQL injection attack. Hibernate will often build large queries to grab the data,\nand it doesn\u2019t always perform in an extremely e\ufb03cient way. In the previous section I mentioned an example of the EForm module using DBHandler to\npopulate a POJO. This is the next logical step to preventing that kind of code from being written. If\nthe model has to change, only the .hbm.xml \ufb01le and the model class have to change (a new \ufb01eld and\ngetter/setter for the new column), and doing so won\u2019t impact the rest of the application. While newer than DBHandler, the Hibernate method is also starting to show its age. It\u2019s not\nalways convenient to use and requires a big con\ufb01guration \ufb01le for each table you want to access.", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 256", "position": 256, "chunk_type": "semantic", "token_estimate": 391}
{"text": "performance of the notes retrieval operation (down to about 0.1 seconds to perform the same select: statement). The moral of the story is simply that while Hibernate does a fairly good job, unless the join is\nvery explicitly de\ufb01ned and controlled (either in the .hbm.xml \ufb01le or a join annotation in the object\nclass for a JPA model), it can very quickly get out of control. Dealing with objects instead of SQL\nqueries requires you to leave the actual implementation of the query up to the database access library\nand only really allows you to control de\ufb01nition. Unless you\u2019re careful with how you de\ufb01ne things, it\ncan all fall apart under extreme conditions. Furthermore, if you\u2019re a database programmer with lots\nof SQL knowledge, it won\u2019t really help much when designing a JPA-enabled class, and it removes\nsome of the control that you would have if you were writing an SQL statement manually. Ultimately,\na good knowledge of both SQL and JPA annotations and how they a\ufb00ect queries is required. 16.5\nPermissions\nCAISI (Client Access to Integrated Services and Information) was originally a standalone product\u2014a\nfork of OSCAR\u2014to help manage homeless shelters in Toronto. A decision was eventually made to\nmerge the code from CAISI into the main source branch. The original CAISI project may no longer\nexist, but what it gave to OSCAR is very important: its permission model. The permissions model in OSCAR is extremely powerful and can be used to create just about\nas many roles and permission sets as possible. Providers belong to programs (as sta\ufb00) where\nthey have a speci\ufb01c role. Each program takes place at a facility. Each role has a description (for\nexample, \u201cdoctor\u201d, \u201cnurse\u201d, \u201csocial worker\u201d, and so on) and a set of attached global permissions. The permissions are written in a format that makes them very easy to understand: \u201cread nurse notes\u201d\nmay be a permission that a doctor role may have, but the nurse role may not have the \u201cread doctor\nnotes\u201d permission. This format may be easy to understand, but under the hood it requires quite a bit of heavy lifting\nto actually check for these types of permissions. The name of the role that the current provider has is\nchecked against its list of permissions for a match with the action that they are trying to perform. For example, a provider attempting to read a doctor\u2019s notes would cause \u201cread doctor notes\u201d to be\nchecked for each and every note written by a doctor.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 258", "position": 258, "chunk_type": "semantic", "token_estimate": 416}
{"text": "performance of the notes retrieval operation (down to about 0.1 seconds to perform the same select: The name of the role that the current provider has is\nchecked against its list of permissions for a match with the action that they are trying to perform. For example, a provider attempting to read a doctor\u2019s notes would cause \u201cread doctor notes\u201d to be\nchecked for each and every note written by a doctor. Another problem is the reliance on English for permission de\ufb01nition. Anybody using OSCAR in\na language other than English would still need to write their permissions in a format such as \u201cread\n[role] notes\u201d, using the English words \u201cread\u201d, \u201cwrite\u201d, \u201cnotes\u201d, and so on. CAISI\u2019s permission model is a signi\ufb01cant part of OSCAR, but it\u2019s not the only model in place. Before CAISI was implemented, another role-based (but not program-based) system was developed\nand is still in use in many parts of the system today. For this second system, providers are assigned one or many roles (for example, \u201cdoctor\u201d, \u201cnurse\u201d,\n\u201cadmin\u201d, and so on). They can be assigned as many roles as necessary\u2014the roles\u2019 permissions\nstack on top of each other. These permissions are generally used for restricting access to parts of\nthe system, as opposed to CAISI\u2019s permissions which restrict access to certain pieces of data on\na patient\u2019s chart. For example, a user has to have the \u201c_admin\u201d \u201cread\u201d permission on a role that\nthey have assigned to them to be able to access the Admin panel. Having the \u201cread\u201d permission will\nexempt them from being able to perform administrative tasks, however. They\u2019ll need the \u201cwrite\u201d\npermission as well for that. Both of these systems accomplish roughly the same goal; it\u2019s due to CAISI\u2019s merge later in the\nproject lifecycle that they both exist. They don\u2019t always exist happily together, so in reality it can be\n244", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 258", "position": 258, "chunk_type": "semantic", "token_estimate": 311}
{"text": "a lot easier to just focus on using one for day-to-day operations of OSCAR. You can generally date: code in OSCAR by knowing which permissions model preceded which other permissions model:\nProvider Type then Provider Roles then CAISI Programs/Roles\nThe oldest type of permissions model, \u201cProvider Type\u201d, is so dated that it\u2019s actually not used in\nmost parts of the system and is in fact defaulted to \u201cdoctor\u201d during new provider creation\u2014having it\nas any other value (such as \u201creceptionist\u201d) causes signi\ufb01cant issues throughout the system. It\u2019s easier\nand more \ufb01ne-grained to control permissions via Provider Roles instead. 16.6\nIntegrator\nOSCAR\u2019s Integrator component is a separate web application that independent OSCAR instances\nuse to exchange patient, program and provider information over a secure link. It can be optionally\ninstalled as a component for an installation in an environment such as a LHN (Local Health Network)\nor a hospital. The easiest way to describe the Integrator is as a temporary storage facility. Consider the following use case and argument for use of the Integrator: in Hospital X, there is\nan ENT (ear, nose, and throat) clinic as well as an endocrinology clinic. If an ENT doctor refers\ntheir patient to an endocrinologist upstairs, they may be required to send along patient history and\nrecords. This is inconvenient and generates more paper than is necessary\u2014perhaps the patient is\nonly seeing the endocrinologist once. By using the Integrator, the patient\u2019s data can be accessed on\nthe endocrinologist\u2019s EMR, and access to the contents of the patient\u2019s chart can be revoked after the\nvisit. A more extreme example: if an unconscious man shows up in an ER with nothing but his health\ncard, because the home clinic and the hospital\u2019s system are connected via the Integrator, the man\u2019s\nrecord can be pulled and it can be very quickly realized that he has been prescribed the blood thinner\nwarfarin. Ultimately, information retrieval like this is what an EMR like OSCAR paired with the\nIntegrator can achieve. Technical Details\nThe Integrator is available in source code form only, which requires the user to retrieve and build it\nmanually. Like OSCAR, it runs on a standard installation of Tomcat with MySQL. When the URL where the Integrator lives is accessed, it doesn\u2019t appear to display anything useful. This component is almost purely a web service; OSCAR communicates via POST and GET requests\nto the Integrator URL.", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 259", "position": 259, "chunk_type": "semantic", "token_estimate": 399}
{"text": "a lot easier to just focus on using one for day-to-day operations of OSCAR. You can generally date: When the URL where the Integrator lives is accessed, it doesn\u2019t appear to display anything useful. This component is almost purely a web service; OSCAR communicates via POST and GET requests\nto the Integrator URL. As an independently developed project (initially as part of the CAISI project), the Integrator is\nfairly strict in adhering to the MVC design pattern. The original developers have done an excellent\njob of setting it up with very clearly de\ufb01ned lines between the models, views, and controllers. The\nmost recently implemented type of database access layer that I mentioned earlier\u2014generic JPA\u2014is\nthe only such layer in the project. (As an interesting side note: because the entire project is properly\nset up with JPA annotations on all the model classes, an SQL script is created at build time that\ncan be used to initialize the structure of the database; the Integrator, therefore, doesn\u2019t ship with a\nstand-alone SQL script.) Communication is handled via web service calls described in WSDL XML \ufb01les that are available\non the server. A client could query the Integrator to \ufb01nd out what kind of functions are available and\nadapt to it. This really means that the Integrator is compatible with any kind of EMR that somebody\nJennifer Ruttan\n245", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 259", "position": 259, "chunk_type": "semantic", "token_estimate": 226}
{"text": "compatibility (changing package names is a common o\ufb00ense). There are often several libraries: included with OSCAR that all accomplish the same task\u2014this goes back to poor source control, but\nalso the fact that that there has been no list or documentation describing which library is required by\nwhich component. Additionally, OSCAR is a little in\ufb02exible when it comes to adding new features to existing\nsubsystems. For example, if you want to add a new box to the E-Chart, you\u2019ll have to create a new\nJSP page and a new servlet, modify the layout of the E-Chart (in a few places), and modify the\ncon\ufb01guration \ufb01le of the application so that your servlet can load. Next, due to the lack of documentation, sometimes it is nearly impossible to \ufb01gure out how a part\nof the system works\u2014the original contributor may not even be part of the project anymore\u2014and\noften the only tool you have to \ufb01gure it out is a debugger. As a project of this age, this is costing\nthe community the potential for new contributors to get involved. However, it\u2019s something that, as a\ncollaborative e\ufb00ort, the community is working on. Finally, OSCAR is a repository for medical information and its security is compromised by the\ninclusion of the DBHandler class (discussed in a previous section). I personally feel that freeform\ndatabase queries that accept parameters should never be acceptable in an EMR because it\u2019s so easy\nto perform SQL injection attacks. While it\u2019s good that no new code is permitted that uses this class,\nit should be a priority of the development team to remove all instances of its use. All of that may sound like some harsh criticism of the project. In the past, all of these problems\nhave been signi\ufb01cant and, like I said, prevent the community from growing as the barrier to entry\nis so high. This is something that is changing, so in the future, these issues won\u2019t be so much of a\nhindrance. In looking back over the project\u2019s history (and especially over the past few versions) we can come\nup with a better design for how the application would be built. The system still has to provide a base\nlevel of functionality (mandated by the Ontario government for certi\ufb01cation as an EMR), so that\nall has to be baked in by default.", "domains": ["Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 263", "position": 263, "chunk_type": "semantic", "token_estimate": 390}
{"text": "compatibility (changing package names is a common o\ufb00ense). There are often several libraries: In looking back over the project\u2019s history (and especially over the past few versions) we can come\nup with a better design for how the application would be built. The system still has to provide a base\nlevel of functionality (mandated by the Ontario government for certi\ufb01cation as an EMR), so that\nall has to be baked in by default. But if OSCAR were to be redesigned today, it should be designed\nin a truly modular fashion that would allow modules to be treated as plugins; if you didn\u2019t like the\ndefault E-Form module, you could write your own (or even another module entirely). It should be\nable to speak to more systems (or more systems should be able to speak to it), including the medical\nhardware that you see in increasing use throughout the industry, such as devices for measuring visual\nacuity. This also means that it would be easy to adapt OSCAR to the requirements of local and\nfederal governments around the world for storing medical data. Since every region has a di\ufb00erent set\nof laws and requirements, this kind of design would be crucial for making sure that OSCAR develops\na worldwide userbase. I also believe that security should be the most important feature of all. An EMR is only as secure\nas its least secure component, so there should be focus on abstracting away as much data access\nas possible from the application so that it stores and retrieves data in a sandbox-style environment\nthrough a main data access layer API that has been audited by a third-party and found to be adequate\nfor storing medical information. Other EMRs can hide behind obscurity and proprietary code as\na security measure (which isn\u2019t really a security measure at all), but being open source, OSCAR\nshould lead the charge with better data protection. I stand \ufb01rmly as a believer in the OSCAR project. We have hundreds of users that we know about\n(and the many hundreds that we don\u2019t), and we receive valuable feedback from the physicians who\nare interacting with our project on a daily basis. Through the development of new processes and\nnew features, we hope to grow the installed base and to support users from other regions.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 263", "position": 263, "chunk_type": "semantic", "token_estimate": 385}
{"text": "compatibility (changing package names is a common o\ufb00ense). There are often several libraries: We have hundreds of users that we know about\n(and the many hundreds that we don\u2019t), and we receive valuable feedback from the physicians who\nare interacting with our project on a daily basis. Through the development of new processes and\nnew features, we hope to grow the installed base and to support users from other regions. It is our\nintention to make sure that what we deliver is something that improves the lives of the physicians who\nuse OSCAR as well as the lives of their patients, by creating better tools to help manage healthcare. 249", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 263", "position": 263, "chunk_type": "semantic", "token_estimate": 110}
{"text": "And here is its Processing.js conversion:: function($p) {\nfunction setup() {\n$p.size(200, 200);\n$p.noCursor();\n$p.noStroke();\n$p.smooth(); }\n$p.setup = setup;\nfunction draw() {\n$p.fill(255, 10);\n$p.rect(-1, -1, $p.width + 1, $p.height + 1);\nvar f = $p.frameCount * $p.PI / $p.__frameRate;\nvar d = 10 + $p.abs(60 * $p.sin(f));\n$p.fill(0, 100, 0, 50);\n$p.ellipse($p.mouseX, $p.mouseY, d, d); }\n$p.draw = draw; }\nThis sounds like a great thing, but there are a few problems when converting Java syntax to\nJavaScript syntax:\n1. Java programs are isolated entities. JavaScript programs share the world with a web page. 2. Java is strongly typed. JavaScript is not. 3. Java is a class/instance based object-oriented language. JavaScript is not. 4. Java has distinct variables and methods. JavaScript does not. 5. Java allows method overloading. JavaScript does not. 6. Java allows importing compiled code. JavaScript has no idea what that even means. Dealing with these problems has been a tradeo\ufb00between what users need, and what we can do\ngiven web technologies. The following sections will discuss each of these issues in greater detail. 17.2\nSigni\ufb01cant Di\ufb00erences\nJava programs have their own threads; JavaScript can lock up your browser. Java programs are isolated entities, running in their own thread in the greater pool of applications\non your system. JavaScript programs, on the other hand, live inside a browser, and compete with\neach other in a way that desktop applications don\u2019t. When a Java program loads a \ufb01le, the program\nwaits until the resource is done loading, and operation resumes as intended. In a setting where the\nprogram is an isolated entity on its own, this is \ufb01ne. The operating system stays responsive because\nit\u2019s responsible for thread scheduling, and even if the program takes an hour to load all its data, you\ncan still use your computer. On a web page, this is not how things work. If you have a JavaScript\n\u201cprogram\u201d waiting for a resource to be done loading, it will lock its process until that resource is\navailable. If you\u2019re using a browser that uses one process per tab, it will lock up your tab, and the rest\nof the browser is still usable. If you\u2019re using a browser that doesn\u2019t, your entire browser will seem\nfrozen.", "domains": ["Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 267", "position": 267, "chunk_type": "semantic", "token_estimate": 374}
{"text": "if there is a known fallback font. Rather than relying on these events, we embed a tiny TrueType: This might sound like a big issue, and at \ufb01rst we were convinced it would be, but you can\u2019t argue\nwith real world feedback: it turns out this is almost never an issue for people who put their sketches\nonline using Processing.js. Rather than solving this in some cool and creative way, the resolution of\nthis problem was actually remarkably straightforward; we didn\u2019t solve it, and as a design choice, we\ndon\u2019t intend to ever revisit that decision. Short of adding a symbol table with strong typing so that\nwe can fake types in JavaScript and switch functionality based on type, this incompatibility cannot\nproperly be solved without leaving much harder to \ufb01nd edge case bugs, and so rather than adding\nbulk to the code and slowdown to execution, we left this quirk in. It is a well-documented quirk, and\n\u201cgood code\u201d won\u2019t try to take advantage of Java\u2019s implicit number type casting. That said, sometimes\nyou will forget, and the result can be quite interesting. Java is a class/instance-based object-oriented language, with separate variable and method\nspaces; JavaScript is not. JavaScript uses prototype objects, and the inheritance model that comes with it. This means all\nobjects are essentially key/value pairs where each key is a string, and values are either primitives,\narrays, objects, or functions. On the inheritance side, prototypes can extend other prototypes,\nbut there is no real concept of \u201csuperclass\u201d and \u201csubclass\u201d. In order to make \u201cproper\u201d Java-style\nobject-oriented code work, we had to implement classical inheritance for JavaScript in Processing.js,\nwithout making it super slow (we think we succeeded in that respect). We also had to come up\nwith a way to prevent variable names and function names from stepping on each other. Because of\nthe key/value nature of JavaScript objects, de\ufb01ning a variable called line, followed by a function\nlike line(x1,y1,x2,y2) will leave you with an object that uses whatever was declared last for a\nkey. JavaScript \ufb01rst sets object.line = \"some value\" for you, and then sets object.line =\nfunction(x1,y1,x2,y2){...}, overriding what you thought your variable line was. Mike Kamermans\n255", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 269", "position": 269, "chunk_type": "semantic", "token_estimate": 365}
{"text": "It would have slowed down the library a lot to create separate administration for variables and: methods/functions, so again the documentation explains that it\u2019s a bad idea to use variables and\nfunctions with the same name. If everyone wrote \u201cproper\u201d code, this wouldn\u2019t be much of a problem,\nas you want to name variables and functions based on what they\u2019re for, or what they do, but the\nreal world does things di\ufb00erently. Sometimes your code won\u2019t work, and it\u2019s because we decided\nthat having your code break due to a naming con\ufb02ict is preferable to your code always working, but\nalways being slow. A second reason for not implementing variable and function separation was that\nthis could break JavaScript code used inside Processing sketches. Closures and the scope chain for\nJavaScript rely on the key/value nature of objects, so driving a wedge in that by writing our own\nadministration would have also severely impacted performance in terms of Just-In-Time compilation\nand compression based on functional closures. Java allows method overloading; JavaScript does not. One of Java\u2019s more powerful features is that you can de\ufb01ne a function, let\u2019s say add(int,int),\nand then de\ufb01ne another function with the same name, but a di\ufb00erent number of arguments, e.g. add(int,int,int), or with di\ufb00erent argument types, e.g. add(ComplexNumber,ComplexNumber). Calling add with two or three integer arguments will automatically call the appropriate function,\nand calling add with \ufb02oats or Car objects will generate an error. JavaScript, on the other hand, does\nnot support this. In JavaScript, a function is a property, and you can dereference it (in which case\nJavaScript will give you a value based on type coercion, which in this case returns true when the\nproperty points to a function de\ufb01nition, or false when it doesn\u2019t), or you can call it as a function using\nthe execution operators (which you will know as parentheses with zero or more arguments between\nthem). If you de\ufb01ne a function as add(x,y) and then call it as add(1,2,3,4,5,6), JavaScript is\nokay with that. It will set x to 1 and y to 2 and simply ignore the rest of the arguments. In order\nto make overloading work, we rewrite functions with the same name but di\ufb00erent argument count\nto a numbered function, so that function(a,b,c) in the source becomes function$3(a,b,c) in\nthe rewritten code, and function(a,b,c,d) becomes function$4(a,b,c,d), ensuring the correct\ncode paths.", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 270", "position": 270, "chunk_type": "semantic", "token_estimate": 395}
{"text": "Java allows importing compiled code.: Sometimes, plain Processing is not enough, and additional functionality is introduced in the form\nof a Processing library. These take the form of a .jarchive with compiled Java code, and o\ufb00er\nthings like networking, audio, video, hardware interfacing and other exotic functions not covered by\nProcessing itself. This is a problem, because compiled Java code is Java byte code. This has given us many\nheadaches: how do we support library imports without writing a Java byte code decompiler? After\nabout a year of discussions, we settled on what may seem the simplest solution. Rather than trying\nto also cover Processing libraries, we decided to support the import keyword in sketches, and\ncreate a Processing.js Library API, so that library developers can write a JavaScript version of\ntheir library (where feasible, given the web\u2019s nature), so that if they write a package that is used\nvia import processing.video, native Processing will pick the .jarchive, and Processing.js will\ninstead pick processing.video.js, thus ensuring that things \u201cjust work\u201d. This functionality is slated for\nProcessing.js 1.4, and library imports is the last major feature that is still missing from Processing.js\n(we currently support the import keyword only in the sense that it is removed from the source code\nbefore conversion), and will be the last major step towards parity. Why Pick JavaScript if It Can\u2019t Do Java? This is not an unreasonable question, and it has multiple answers. The most obvious one is that\nJavaScript comes with the browser. You don\u2019t \u201cinstall\u201d JavaScript yourself, there\u2019s no plugin to\ndownload \ufb01rst; it\u2019s just there. If you want to port something to the web, you\u2019re stuck with JavaScript. Although, given the \ufb02exibility of JavaScript, \u201cstuck with\u201d is really not doing justice to how powerful\nthe language is. So, one reason to pick JavaScript is \u201cbecause it\u2019s already there\u201d. Pretty much every\ndevice that is of interest comes with a JavaScript-capable browser these days. The same cannot be\nsaid for Java, which is being o\ufb00ered less and less as a preinstalled technology, if it is available at all. However, the proper answer is that it\u2019s not really true that JavaScript \u201ccan\u2019t do\u201d the things that\nJava does; it can, it would just be slower.", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 271", "position": 271, "chunk_type": "semantic", "token_estimate": 372}
{"text": "Java allows importing compiled code.: The same cannot be\nsaid for Java, which is being o\ufb00ered less and less as a preinstalled technology, if it is available at all. However, the proper answer is that it\u2019s not really true that JavaScript \u201ccan\u2019t do\u201d the things that\nJava does; it can, it would just be slower. Even though out of the box JavaScript can\u2019t do some of the\nthings Java does, it\u2019s still a Turing-complete programming language and it can be made to emulate any\nother programming language, at the cost of speed. We could, technically, write a full Java interpreter,\nwith a String heap, separate variable and method models, class/instance object-orientation with\nrigid class hierarchies, and everything else under the Sun (or, these days, Oracle), but that\u2019s not what\nwe\u2019re in it for: Processing.js is about o\ufb00ering a Processing-to-the-web conversion, in as little code as\nis necessary for that. This means that even though we decided not to make it do certain Java things,\nour library has one huge bene\ufb01t: it can cope with embedded JavaScript really, really well. In fact, during a meeting between the Processing.js and Processing people at Bocoup in Boston,\nin 2010, Ben Fry asked John Resig why he used regular expression replacement and only partial\nconversion instead of doing a proper parser and compiler. John\u2019s response was that it was important\nto him that people be able to mix Processing syntax (Java) and JavaScript without having to choose\nbetween them. That initial choice has been crucial in shaping the philosophy of Processing.js ever\nsince. We\u2019ve worked hard to keep it true in our code, and we can see a clear payo\ufb00when we look\nat all the \u201cpurely web\u201d users of Processing.js, who never used Processing, and will happily mix\nProcessing and JavaScript syntax without a problem. Mike Kamermans\n257", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 271", "position": 271, "chunk_type": "semantic", "token_estimate": 304}
{"text": "The following example shows how JavaScript and Processing work together.: // JavaScript (would throw an error in native Processing)\nvar cs = { x: 50,\ny: 0,\nlabel: \"my label\",\nrotate: function(theta) {\nvar nx = this.x*cos(theta) - this.y*sin(theta);\nvar ny = this.x*sin(theta) + this.y*cos(theta);\nthis.x = nx; this.y = ny; }};\n// Processing\nfloat angle = 0;\nvoid setup() {\nsize(200,200);\nstrokeWeight(15); }\nvoid draw() {\ntranslate(width/2,height/2);\nangle += PI/frameRate;\nwhile(angle>2*PI) { angle-=2*PI; }\njQuery(\u2019#log\u2019).text(angle); // JavaScript (error in native Processing)\ncs.rotate(angle);\n// legal JavaScript as well as Processing\nstroke(random(255));\npoint(cs.x, cs.y); }\nA lot of things in Java are promises: strong typing is a content promise to the compiler, visibility\nis a promise on who will call methods and reference variables, interfaces are promises that instances\ncontain the methods the interface describes, etc. Break those promises and the compiler complains. But, if you don\u2019t\u2014and this is a one of the most important thoughts for Processing.js\u2014then you don\u2019t\nneed the additional code for those promises in order for a program to work. If you stick a number in\na variable, and your code treats that variable as if it has a number in it, then at the end of the day var\nvarname is just as good as int varname. Do you need typing? In Java, you do; in JavaScript, you\ndon\u2019t, so why force it in? The same goes for other code promises. If the Processing compiler doesn\u2019t\ncomplain about your code, then we can strip all the explicit syntax for your promises and it\u2019ll still\nwork the same. This has made Processing.js a ridiculously useful library for data visualisation, media presentation\nand even entertainment. Sketches in native Processing work, but sketches that mix Java and JavaScript\nalso work just \ufb01ne, as do sketches that use pure JavaScript by treating Processing.js as a glori\ufb01ed\ncanvas drawing framework. In an e\ufb00ort to reach parity with native Processing, without forcing\nJava-only syntax, the project has been taken in by an audience as wide as the web itself. We\u2019ve seen\nactivity all over the web using Processing.js. Everyone from IBM to Google has built visualisations,\npresentations and even games with Processing.js\u2014Processing.js is making a di\ufb00erence. Another great thing about converting Java syntax to JavaScript while leaving JavaScript untouched\nis that we\u2019ve enabled something we hadn\u2019t even thought about ourselves: Processing.js will work with\nanything that will work with JavaScript.", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 272", "position": 272, "chunk_type": "semantic", "token_estimate": 398}
{"text": "took what we did and used it with brand new syntaxes. They could never have done that if we had: made Processing.js simply be a Java interpreter. By sticking with code conversion rather than writing\na code interpreter, Processing.js has given Processing a reach on the web far beyond what it would\nhave had if it had stayed Java-only, or even if it had kept a Java-only syntax, with execution on the\nweb taken care of by JavaScript. The uptake of our code not just by end users, but also by people\nwho try to integrate it with their own technologies, has been both amazing and inspiring. Clearly\nwe\u2019re doing something right, and the web seems happy with what we\u2019re doing. The Result\nAs we are coming up to Processing.js 1.4.0, our work has resulted in a library that will run any\nsketch you give it, provided it does not rely on compiled Java library imports. If you can write it in\nProcessing, and it runs, you can put it on a webpage and it will just run. Due to the di\ufb00erences in\nhardware access and low level implementations of di\ufb00erent parts of the rendering pipeline there will\nbe timing di\ufb00erences, but in general a sketch that runs at 60 frames per seconds in the Processing\nIDE will run at 60 frames per second on a modern computer, with a modern browser. We have\nreached a point where bug reports have started to die down, and most work is no longer about adding\nfeature support, but more about bug \ufb01xing and code optimization. Thanks to the e\ufb00orts of many developers working to resolve over 1800 bug reports, Processing\nsketches run using Processing.js \u201cjust work\u201d. Even sketches that rely on library imports can be made\nto work, provided that the library code is at hand. Under favourable circumstances, the library is\nwritten in a way that lets you rewrite it to pure Processing code with a few search-replace operations. In this case the code can be made to work online virtually immediately. When the library does things\nthat cannot be implemented in pure Processing, but can be implemented using plain JavaScript, more\nwork is required to e\ufb00ectively emulate the library using JavaScript code, but porting is still possible.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 273", "position": 273, "chunk_type": "semantic", "token_estimate": 377}
{"text": "took what we did and used it with brand new syntaxes. They could never have done that if we had: In this case the code can be made to work online virtually immediately. When the library does things\nthat cannot be implemented in pure Processing, but can be implemented using plain JavaScript, more\nwork is required to e\ufb00ectively emulate the library using JavaScript code, but porting is still possible. The only instances of Processing code that cannot be ported are those that rely on functionality that\nis inherently unavailable to browsers, such as interfacing directly with hardware devices (such as\nwebcams or Arduino boards) or performing unattended disk writes, though even this is changing. Browsers are constantly adding functionality to allow for more elaborate applications, and limiting\nfactors today may disappear a year from now, so that hopefully in the not too distant future, even\nsketches that are currently impossible to run online will become portable. 17.3\nThe Code Components\nProcessing.js is presented and developed as a large, single \ufb01le, but architecturally it represents three\ndi\ufb00erent components: 1) the launcher, responsible for converting Processing source to Processing.js\n\ufb02avoured JavaScript and executing it, 2) static functionality that can be used by all sketches, and 3)\nsketch functionality that has to be tied to individual instances. The Launcher\nThe launcher component takes care of three things: code preprocessing, code conversion, and sketch\nexecution. Mike Kamermans\n259", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 273", "position": 273, "chunk_type": "semantic", "token_estimate": 234}
{"text": "Preprocessing: In the preprocessing step, Processing.js directives are split o\ufb00from the code, and acted upon. These\ndirectives come in two \ufb02avours: settings and load instructions. There is a small number of directives,\nkeeping with the \u201cit should just work\u201d philosophy, and the only settings that sketch authors can change\nare related to page interaction. By default a sketch will keep running if the page is not in focus, but the\npauseOnBlur = true directive sets up a sketch in such a way that it will halt execution when the page\nthe sketch is running on is not in focus, resuming execution when the page is in focus again. Also by\ndefault, keyboard input is only routed to a sketch when it is focussed. This is especially important\nwhen people run multiple sketches on the same page, as keyboard input intended for one sketch\nshould not be processed by another. However, this functionality can be disabled, routing keyboard\nevents to every sketch that is running on a page, using the globalKeyEvents = true directive. Load instructions take the form of the aforementioned image preloading and font preloading. Because images and fonts can be used by multiple sketches, they are loaded and tracked globally, so\nthat di\ufb00erent sketches don\u2019t attempt multiple loads for the same resource. Code Conversion\nThe code conversion component decomposes the source code into AST nodes, such as statements\nand expressions, methods, variables, classes, etc. This AST then expanded to JavaScript source code\nthat builds a sketch-equivalent program when executed. This converted source code makes heavy use\nof the Processing.js instance framework for setting up class relations, where classes in the Processing\nsource code become JavaScript prototypes with special functions for determining superclasses and\nbindings for superclass functions and variables. Sketch Execution\nThe \ufb01nal step in the launch process is sketch execution, which consists of determining whether or\nnot all preloading has \ufb01nished, and if it has, adding the sketch to the list of running instances and\ntriggering its JavaScript onLoad event so that any sketch listeners can take the appropriate action. After this the Processing chain is run through: setup, then draw, and if the sketch is a looping sketch,\nsetting up an interval call to draw with an interval length that gets closest to the desired framerate for\nthe sketch.", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 274", "position": 274, "chunk_type": "semantic", "token_estimate": 383}
{"text": "Preprocessing: Sketch Execution\nThe \ufb01nal step in the launch process is sketch execution, which consists of determining whether or\nnot all preloading has \ufb01nished, and if it has, adding the sketch to the list of running instances and\ntriggering its JavaScript onLoad event so that any sketch listeners can take the appropriate action. After this the Processing chain is run through: setup, then draw, and if the sketch is a looping sketch,\nsetting up an interval call to draw with an interval length that gets closest to the desired framerate for\nthe sketch. Static Library\nMuch of Processing.js falls under the \u201cstatic library\u201d heading, representing constants, universal\nfunctions, and universal data types. A lot of these actually do double duty, being de\ufb01ned as global\nproperties, but also getting aliased by instances for quicker code paths. Global constants such as key\ncodes and color mappings are housed in the Processing object itself, set up once, and then referenced\nwhen instances are built via the Processing constructor. The same applies to self-contained helper\nfunctions, which lets us keep the code as close to \u201cwrite once, run anywhere\u201d as we can without\nsacri\ufb01cing performance. Processing.js has to support a large number of complex data types, not just in order to support\nthe data types used in Processing, but also for its internal workings. These, too, are de\ufb01ned in the\nProcessing constructor:\nChar, an internal object used to overcome some of the behavioural quirks of Java\u2019s char datatype. PShape, which represents shape objects. 260\nProcessing.js", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 274", "position": 274, "chunk_type": "semantic", "token_estimate": 251}
{"text": "PShapeSVG, an extension for PShape objects, which is built from and represents SVG XML.: For PShapeSVG, we implemented our own SVG-to-<canvas>-instructions code. Since Pro-\ncessing does not implement full SVG support, the code we saved by not relying on an external\nSVG library means that we can account for every line of code relating to SVG imports. It\nonly parses what it has to, and doesn\u2019t waste space with code that follows the spec, but is\nunused because native Processing does not support it. XMLElement, an XML document object. For XMLElement, too, we implemented our own code, relying on the browser to \ufb01rst load the\nXML element into a Node-based structure, then traveling the node structure to build a leaner\nobject. Again, this means we don\u2019t have any dead code sitting in Processing.js, taking up\nspace and potentially causing bugs because a patch accidentally makes use of a function that\nshouldn\u2019t be there. PMatrix2D and PMatrix3D, which perform matrix operations in 2D and 3D mode. PImage, which represents an image resource. This is e\ufb00ectively a wrapper of the Image object, with some additional functions and properties\nso that its API matches the Processing API. PFont, which represents a font resource. There is no Font object de\ufb01ned for JavaScript (at least for now), so rather than actually storing\nthe font as an object, our PFont implementation loads a font via the browser, computes its\nmetrics based on how the browser renders text with it, and then caches the resultant PFont\nobject. For speed, PFonts have a reference to the canvas that was used to determine the font\nproperties, in case textWidth must be calculated, but because we track PFont objects based\non name/size pair, if a sketch uses a lot of distinct text sizes, or fonts in general, this will\nconsume too much memory. As such, PFonts will clear their cached canvas and instead call\na generic textWidth computation function when the cache grows too large. As a secondary\nmemory preservation strategy, if the font cache continues to grow after clearing the cached\ncanvas for each PFont, font caching is disabled entirely, and font changes in the sketch simply\nbuild new throwaway PFont objects for every change in font name, text size or text leading. DrawingShared, Drawing2D, and Drawing3D, which house all the graphics functions. The DrawingShared object is actually the biggest speed trap in Processing.js.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 275", "position": 275, "chunk_type": "semantic", "token_estimate": 395}
{"text": "side of things. This function takes two objects as arguments, checks whether either of them: implements their own equals function, and if so, falls through to that function. If they don\u2019t,\nand the passed objects are primitives, primitive equality is checked. If they\u2019re not, then there\nis no equality. For hashing containers, things are even more interesting, as hashing containers act as shortcut\ntrees. The container actually wraps a variable number of lists, each tied to a speci\ufb01c hash\ncode. Objects are found based on \ufb01rst \ufb01nding the container that matches their hash code,\nin which the object is then searched for based on equality evaluation. As all objects in Java\nhave a hashCode method, we also wrote a \u201cvirtual hashcode\u201d function, which takes a single\nobject as an argument. The function checks whether the object implements its own hashCode\nfunction, and if so falls through to that function. If it doesn\u2019t, the hash code is computed\nbased on the same hashing algorithm that is used in Java. Administration\nThe \ufb01nal piece of functionality in the static code library is the instance list of all sketches that are\ncurrently running on the page. This instance list stores sketches based on the canvas they have been\nloaded in, so that users can call Processing.getInstanceById(\u2019canvasid\u2019) and get a reference\nto their sketch for page interaction purposes. Instance Code\nInstance code takes the form of p.functor = function(arg, ...) de\ufb01nitions for the Processing\nAPI, and p.constant = ... for sketch state variables (where p is our reference to the sketch being\nset up). Neither of these are located in dedicated code blocks. Rather, the code is organized based on\nfunction, so that instance code relating to PShape operations is de\ufb01ned near the PShape object, and\ninstance code for graphics functions are de\ufb01ned near, or in, the Drawing2D and Drawing3D objects. In order to keep things fast, a lot of code that could be written as static code with an instance wrap-\nper is actually implemented as purely instance code. For instance, the lerpColor(c1, c2, ratio)\nfunction, which determines the color corresponding to the linear interpolation of two colors, is\nde\ufb01ned as an instance function. Rather than having p.lerpColor(c1, c2, ratio) acting as a wrap-\nper for some static function Processing.lerpColor(c1, c2, ratio), the fact that nothing else\nin Processing.js relies on lerpColor means that code execution is faster if we write it as a pure\ninstance function.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 276", "position": 276, "chunk_type": "semantic", "token_estimate": 404}
{"text": "Make It Work: Writing code that works sounds like a tautological premise; you write code, and by the time you\u2019re\ndone your code either works, because that\u2019s what you set out to do, or it doesn\u2019t, and you\u2019re not done\nyet. However, \u201cmake it work\u201d comes with a corollary: Make it work, and when you\u2019re done, prove it. If there is one thing above all other things that has allowed Processing.js to grow at the pace it\nhas, it is the presence of tests. Any ticket that requires touching the code, be it either by writing new\ncode or rewriting old code, cannot be marked as resolved until there is a unit or reference test that\nallows others to verify not only that the code works the way it should, but also that it breaks when it\nshould. For most code, this typically involves a unit test\u2014a short bit of code that calls a function and\nsimply tests whether the function returns the correct values, for both legal and illegal function calls. Not only does this allow us to test code contributions, it also lets us perform regression tests. Before any code is accepted and merged into our stable development branch, the modi\ufb01ed Pro-\ncessing.js library is validated against an ever-growing battery of unit tests. Big \ufb01xes and performance\ntests in particular are prone to passing their own unit tests, but breaking parts that worked \ufb01ne before\nthe rewrite. Having tests for every function in the API, as well as internal functions, means that\nas Processing.js grows, we don\u2019t accidentally break compatibility with previous versions. Barring\ndestructive API changes, if none of the tests failed before a code contribution or modi\ufb01cation, none\nof the tests are allowed to fail with the new code in. The following is an example of a unit test verifying inline object creation. interface I {\nint getX();\nvoid test(); }\nI i = new I() {\nint x = 5;\npublic int getX() {\nreturn x; }\npublic void test() {\nx++; }};\ni.test();\n_checkEqual(i.getX(), 6);\n_checkEqual(i instanceof I, true);\n_checkEqual(i instanceof Object, true);\nIn addition to regular code unit tests, we also have visual reference (or \u201cref\u201d) tests. As Processing.js\nis a port of a visual programming language, some tests cannot be performed using just unit tests.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 277", "position": 277, "chunk_type": "semantic", "token_estimate": 383}
{"text": "Make It Work: interface I {\nint getX();\nvoid test(); }\nI i = new I() {\nint x = 5;\npublic int getX() {\nreturn x; }\npublic void test() {\nx++; }};\ni.test();\n_checkEqual(i.getX(), 6);\n_checkEqual(i instanceof I, true);\n_checkEqual(i instanceof Object, true);\nIn addition to regular code unit tests, we also have visual reference (or \u201cref\u201d) tests. As Processing.js\nis a port of a visual programming language, some tests cannot be performed using just unit tests. Testing to see whether an ellipse gets drawn on the correct pixels, or whether a single-pixel-wide\nvertical line is drawn crisp or smoothed cannot be determined without a visual reference. Because all\nmainstream browsers implement the <canvas> element and Canvas2D API with subtle di\ufb00erences,\nthese things can only be tested by running code in a browser and verifying that the resulting sketch\nlooks the same as what native Processing generates. To make life easier for developers, we use an\nautomated test suite for this, where new test cases are run through Processing, generating \u201cwhat it\nshould look like\u201d data to be used for pixel comparison. This data is then stored as a comment inside\nthe sketch that generated it, forming a test, and these tests are then run by Processing.js on a visual\nreference test page which executes each test and performs pixel comparisons between \u201cwhat it should\nMike Kamermans\n263", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 277", "position": 277, "chunk_type": "semantic", "token_estimate": 230}
{"text": "look like\u201d and \u201cwhat it looks like\u201d. If the pixels are o\ufb00, the test fails, and the developer is presented: with three images: what it should look like, how Processing.js rendered it, and the di\ufb00erence between\nthe two, marking problem areas as red pixels, and correct areas as white. Much like unit tests, these\ntests must pass before any code contribution can be accepted. Make It Fast\nIn an open source project, making things work is only the \ufb01rst step in the life of a function. Once\nthings work, you want to make sure things work fast. Based on the \u201cif you can\u2019t measure it, you can\u2019t\nimprove it\u201d principle, most functions in Processing.js don\u2019t just come with unit or ref tests, but also\nwith performance (or \u201cperf\u201d) tests. Small bits of code that simply call a function, without testing the\ncorrectness of the function, are run several hundred times in a row, and their run time is recorded on\na special performance test web page. This lets us quantify how well (or not!) Processing.js performs\nin browsers that support HTML5\u2019s <canvas> element. Every time an optimization patch passes\nunit and ref testing, it is run through our performance test page. JavaScript is a curious beast, and\nbeautiful code can, in fact, run several orders of magnitude slower than code that contains the same\nlines several times over, with inline code rather than function calls. This makes performance testing\ncrucial. We have been able to speed up certain parts of the library by three orders of magnitude\nsimply by discovering hot loops during perf testing, reducing the number of function calls by inlining\ncode, and by making functions return the moment they know what their return value should be, rather\nthan having only a single return at the very end of the function. Another way in which we try to make Processing.js fast is by looking at what runs it. As\nProcessing.js is highly dependent on the e\ufb03ciency of JavaScript engines, it makes sense to also look\nat which features various engines o\ufb00er to speed things up. Especially now that browsers are starting to\nsupport hardware accelerated graphics, instant speed boosts are possible when engines o\ufb00er new and\nmore e\ufb03cient data types and functions to perform the low level operations that Processing.js depends\non. For instance, JavaScript technically has no static typing, but graphics hardware programming\nenvironments do.", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 278", "position": 278, "chunk_type": "semantic", "token_estimate": 398}
{"text": "There is also another form of small code, and that\u2019s in terms of runtime code. Because JavaScript: lets you change function bindings on the \ufb02y, running code becomes much smaller if you can say\n\u201cbind the function for line2D to the function call for line\u201d once you know that a program runs in 2D\nrather than 3D mode, so that you don\u2019t have to perform:\nif(mode==2D) { line2D() } else { line3D() }\nfor every function call that might be either in 2D or 3D mode. Finally, there is the process of mini\ufb01cation. There are a number of good systems that let you\ncompress your JavaScript code by renaming variables, stripping whitespace, and applying certain\ncode optimisations that are hard to do by hand while still keeping the code readable. Examples of\nthese are the YUI mini\ufb01er and Google\u2019s closure compiler. We use these technologies in Processing.js\nto o\ufb00er end users bandwidth convenience\u2014mini\ufb01cation after stripping comments can shrink the\nlibrary by as much as 50%, and taking advantage of modern browser/server interaction for gzipped\ncontent, we can o\ufb00er the entire Processing.js library in gzipped form in 65 KB. If All Else Fails, Tell People\nNot everything that can currently be done in Processing can be done in the browser. Security models\nprevent certain things like saving \ufb01les to the hard disk and performing USB or serial port I/O, and\na lack of typing in JavaScript can have unexpected consequences (such as all math being \ufb02oating\npoint math). Sometimes we\u2019re faced with the choice between adding an incredible amount of code\nto enable an edge case, or mark the ticket as a \u201cwont\ufb01x\u201d issue. In such cases, a new ticket gets \ufb01led,\ntypically titled \u201cAdd documentation that explains why...\u201d. In order to make sure these things aren\u2019t lost, we have documentation for people who start\nusing Processing.js with a Processing background, and for people who start using Processing.js\nwith a JavaScript background, covering the di\ufb00erences between what is expected, and what actually\nhappens. Certain things just deserve special mention, because no matter how much work we put\ninto Processing.js, there are certain things we cannot add without sacri\ufb01cing usability. A good\narchitecture doesn\u2019t just cover the way things are, it also covers why; without that, you\u2019ll just end up\nhaving the same discussions about what the code looks like and whether it should be di\ufb00erent every\ntime the team changes.", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 279", "position": 279, "chunk_type": "semantic", "token_estimate": 399}
{"text": "There is also another form of small code, and that\u2019s in terms of runtime code. Because JavaScript: Certain things just deserve special mention, because no matter how much work we put\ninto Processing.js, there are certain things we cannot add without sacri\ufb01cing usability. A good\narchitecture doesn\u2019t just cover the way things are, it also covers why; without that, you\u2019ll just end up\nhaving the same discussions about what the code looks like and whether it should be di\ufb00erent every\ntime the team changes. 17.5\nLessons Learned\nThe most important lesson we learned while writing Processing.js is that when porting a language,\nwhat matters is that the result is correct, not whether or not the code used in your port is similar to\nthe original. Even though Java and JavaScript syntax are fairly similar, and modifying Java code\nto legal JavaScript code is fairly easy, it often pays to look at what JavaScript can natively do and\nexploit that to get the same functional result. Taking advantage of the lack of typing by recycling\nvariables, using certain built-in functions that are fast in JavaScript but slow in Java, or avoiding\npatterns that are fast in Java but slow in JavaScript means your code may look radically di\ufb00erent, but\nhas the exact same e\ufb00ect. You often hear people say not to reinvent the wheel, but that only applies\nto working with a single programming language. When you\u2019re porting, reinvent as many wheels as\nyou need to obtain the performance you require. Another important lesson is to return early, return often, and branch as little as possible. An if/then\nstatement followed by a return can be made (sometimes drastically) faster by using an if-return/return\nMike Kamermans\n265", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 279", "position": 279, "chunk_type": "semantic", "token_estimate": 285}
{"text": "construction instead, using the return statement as a conditional shortcut. While it\u2019s conceptually: pretty to aggregate your entire function state before calling the ultimate return statement for that\nfunction, it also means your code path may traverse code that is entirely unrelated to what you will\nbe returning. Don\u2019t waste cycles; return when you have all the information you need. A third lesson concerns testing your code. In Processing.js we had the bene\ufb01t of starting with\nvery good documentation outlining how Processing was \u201csupposed\u201d to work, and a large set of test\ncases, most of which started out as \u201cknown fail\u201d. This allowed us to do two things: 1) write code\nagainst tests, and 2) create tests before writing code. The usual process, in which code is written and\nthen test cases are written for that code, actually creates biased tests. Rather than testing whether or\nnot your code does what it should do, according to the speci\ufb01cation, you are only testing whether\nyour code is bug-free. In Processing.js, we instead start by creating test cases based on what the\nfunctional requirements for some function or set of functions is, based on the documentation for it. With these unbiased tests, we can then write code that is functionally complete, rather than simply\nbug-free but possibly de\ufb01cient. The last lesson is also the most general one: apply the rules of agile development to individual\n\ufb01xes as well. No one bene\ufb01ts from you retreating into dev mode and not being heard from for three\ndays straight while you write the perfect solution. Rather, get your solutions to the point where they\nwork, and not even necessarily for all test cases, then ask for feedback. Working alone, with a test\nsuite for catching errors, is no guarantee of good or complete code. No amount of automated testing\nis going to point out that you forgot to write tests for certain edge cases, or that there is a better\nalgorithm than the one you picked, or that you could have reordered your statements to make the\ncode better suited for JIT compilation. Treat \ufb01xes like releases: present \ufb01xes early, update often, and\nwork feedback into your improvements. 266\nProcessing.js", "domains": ["Design Patterns", "Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 280", "position": 280, "chunk_type": "semantic", "token_estimate": 365}
{"text": "[chapter18]: Puppet\nLuke Kanies\n18.1\nIntroduction\nPuppet is an open source IT management tool written in Ruby, used for datacenter automation\nand server management at Google, Twitter, the New York Stock Exchange, and many others. It is\nprimarily maintained by Puppet Labs, which also founded the project. Puppet can manage as few as\n2 machines and as many as 50,000, on teams with one system administrator or hundreds. Puppet is a tool for con\ufb01guring and maintaining your computers; in its simple con\ufb01guration\nlanguage, you explain to Puppet how you want your machines con\ufb01gured, and it changes them as\nneeded to match your speci\ufb01cation. As you change that speci\ufb01cation over time\u2014such as with package\nupdates, new users, or con\ufb01guration updates\u2014Puppet will automatically update your machines to\nmatch. If they are already con\ufb01gured as desired, then Puppet does nothing. In general, Puppet does everything it can to use existing system features to do its work; e.g., on\nRed Hat it will use yum for packages and init.d for services, but on OS X it will use dmg for packages\nand launchd for services. One of the guiding goals in Puppet is to have the work it does make sense\nwhether you are looking at Puppet code or the system itself, so following system standards is critical. Puppet comes from multiple traditions of other tools. In the open source world, it is most\nin\ufb02uenced by CFEngine, which was the \ufb01rst open source general-purpose con\ufb01guration tool, and\nISconf, whose use of make for all work inspired the focus on explicit dependencies throughout the\nsystem. In the commercial world, Puppet is a response to BladeLogic and Opsware (both since\nacquired by larger companies), each of which was successful in the market when Puppet was begun,\nbut each of which was focused on selling to executives at large companies rather than building great\ntools directly for system administrators. Puppet is meant to solve similar problems to these tools, but\nit is focused on a very di\ufb00erent user. For a simple example of how to use Puppet, here is a snippet of code that will make sure the\nsecure shell service (SSH) is installed and con\ufb01gured properly:\nclass ssh {\npackage { ssh: ensure => installed }\nfile { \"/etc/ssh/sshd_config\":\nsource => \u2019puppet:///modules/ssh/sshd_config\u2019,\nensure => present,\nrequire => Package[ssh]\n}\nservice { sshd:", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 281", "position": 281, "chunk_type": "semantic", "token_estimate": 386}
{"text": "ensure => running,: require => [File[\"/etc/ssh/sshd_config\"], Package[ssh]]\n}\n}\nThis makes sure the package is installed, the \ufb01le is in place, and the service is running. Note that\nwe\u2019ve speci\ufb01ed dependencies between the resources, so that we always perform any work in the right\norder. This class could then be associated with any host to apply this con\ufb01guration to it. Notice that\nthe building blocks of a Puppet con\ufb01guration are structured objects, in this case package, file, and\nservice. We call these objects resources in Puppet, and everything in a Puppet con\ufb01guration comes\ndown to these resources and the dependencies between them. A normal Puppet site will have tens or even hundreds of these code snippets, which we call\nclasses; we store these classes on disk in \ufb01les called manifests, and collect them in related groups\ncalled modules. For instance, you might have an ssh module with this ssh class plus any other\nrelated classes, along with modules for mysql, apache, and sudo. Most Puppet interactions are via the command line or long-running HTTP services, but there\nare graphical interfaces for some things such as report processing. Puppet Labs also produces\ncommercial products around Puppet, which tend more toward graphical web-based interfaces. Puppet\u2019s \ufb01rst prototype was written in the summer of 2004, and it was turned into a full-time\nfocus in February of 2005. It was initially designed and written by Luke Kanies, a sysadmin who\nhad a lot of experience writing small tools, but none writing tools greater than 10,000 lines of code. In essence, Luke learned to be a programmer while writing Puppet, and that shows in its architecture\nin both positive and negative ways. Puppet was \ufb01rst and foremost built to be a tool for sysadmins, to make their lives easier and\nallow them to work faster, more e\ufb03ciently, and with fewer errors. The \ufb01rst key innovation meant to\ndeliver on this was the resources mentioned above, which are Puppet\u2019s primitives; they would both\nbe portable across most operating systems and also abstract away implementation detail, allowing the\nuser to focus on outcomes rather than how to achieve them. This set of primitives was implemented\nin Puppet\u2019s Resource Abstraction Layer. Puppet resources must be unique on a given host. You can only have one package named \u201cssh\u201d,\none service named \u201csshd\u201d, and one \ufb01le named \u201c/etc/ssh/sshd_con\ufb01g\u201d.", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 282", "position": 282, "chunk_type": "semantic", "token_estimate": 389}
{"text": "ensure => running,: Puppet resources must be unique on a given host. You can only have one package named \u201cssh\u201d,\none service named \u201csshd\u201d, and one \ufb01le named \u201c/etc/ssh/sshd_con\ufb01g\u201d. This prevents di\ufb00erent parts of\nyour con\ufb01gurations from con\ufb02icting with each other, and you \ufb01nd out about those con\ufb02icts very early\nin the con\ufb01guration process. We refer to these resources by their type and title; e.g., Package[ssh]\nand Service[sshd]. You can have a package and a service with the same name because they are\ndi\ufb00erent types, but not two packages or services with the same name. The second key innovation in Puppet provides the ability to directly specify dependencies between\nresources. Previous tools focused on the individual work to be done, rather than how the various bits\nof work were related; Puppet was the \ufb01rst tool to explicitly say that dependencies are a \ufb01rst-class\npart of your con\ufb01gurations and must be modeled that way. It builds a graph of resources and their\ndependencies as one of the core data types, and essentially everything in Puppet hangs o\ufb00of this\ngraph (called a Catalog) and its vertices and edges. The last major component in Puppet is its con\ufb01guration language. This language is declarative,\nand is meant to be more con\ufb01guration data than full programming\u2014it most resembles Nagios\u2019s\ncon\ufb01guration format, but is also heavily in\ufb02uenced by CFEngine and Ruby. Beyond the functional components, Puppet has had two guiding principles throughout its develop-\nment: it should be as simple as possible, always preferring usability even at the expense of capability;\nand it should be built as a framework \ufb01rst and application second, so that others could build their\nown applications on Puppet\u2019s internals as desired. It was understood that Puppet\u2019s framework needed\n268\nPuppet", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 282", "position": 282, "chunk_type": "semantic", "token_estimate": 289}
{"text": "a killer application to be adopted widely, but the framework was always the focus, not the application.: Most people think of Puppet as being that application, rather than the framework behind it. When Puppet\u2019s prototype was \ufb01rst built, Luke was essentially a decent Perl programmer with a\nlot of shell experience and some C experience, mostly working in CFEngine. The odd thing is he\nhad experience building parsers for simple languages, having built two as part of smaller tools and\nalso having rewritten CFEngine\u2019s parser from scratch in an e\ufb00ort to make it more maintainable (this\ncode was never submitted to the project, because of small incompatibilities). A dynamic language was easily decided on for Puppet\u2019s implementation, based on much higher\ndeveloper productivity and time to market, but choosing the language proved di\ufb03cult. Initial\nprototypes in Perl went nowhere, so other languages were sought for experimentation. Python was\ntried, but Luke found the language quite at odds with how he thought about the world. Based on\nwhat amounted to a rumor of utility heard from a friend, Luke tried Ruby, and in four hours had built\na usable prototype. When Puppet became a full-time e\ufb00ort in 2005 Ruby was a complete unknown,\nso the decision to stick with it was a big risk, but again programmer productivity was deemed the\nprimary driver in language choice. The major distinguishing feature in Ruby, at least as opposed to\nPerl, was how easy it was to build non-hierarchical class relationships, but it also mapped very well\nto Luke\u2019s brain, which turned out to be critical. 18.2\nArchitectural Overview\nThis chapter is primarily about the architecture of Puppet\u2019s implementation (that is, the code that\nwe\u2019ve used to make Puppet do the things it\u2019s supposed to do) but it\u2019s worth brie\ufb02y discussing its\napplication architecture (that is, how the parts communicate), so that the implementation makes\nsome sense. Puppet has been built with two modes in mind: A client/server mode with a central server\nand agents running on separate hosts, or a serverless mode where a single process does all of the\nwork. To ensure consistency between these modes, Puppet has always had network transparency\ninternally, so that the two modes used the same code paths whether they went over the network or\nnot. Each executable can con\ufb01gure local or remote service access as appropriate, but otherwise they\nbehave identically.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 283", "position": 283, "chunk_type": "semantic", "token_estimate": 395}
{"text": "2. The parser uses that system information and Puppet modules on local disk to compile a: con\ufb01guration for that particular host and returns it to the agent. 3. The agent applies that con\ufb01guration locally, thus a\ufb00ecting the local state of the host, and \ufb01les\nthe resulting report with the server. Figure 18.1: Puppet data\ufb02ow\nThus, the agent has access to its own system information, its con\ufb01guration, and each report it\ngenerates. The server has copies of all of this data, plus access to all of the Puppet modules, and any\nback-end databases and services that might be needed to compile the con\ufb01guration. Beyond the components that go into this work\ufb02ow, which we\u2019ll address next, there are many data\ntypes that Puppet uses for internal communication. These data types are critical, because they\u2019re how\nall communication is done and they\u2019re public types which any other tools can consume or produce. The most important data types are:\nFacts: System data collected on each machine and used to compile con\ufb01gurations. Manifest: Files containing Puppet code, generally organized into collections called \u201cmodules\u201d. Catalog: A graph of a given host\u2019s resources to be managed and the dependencies between them. Report: The collection of all events generated during application of a given Catalog. Beyond Facts, Manifests, Catalogs, and Reports, Puppet supports data types for \ufb01les, certi\ufb01cates\n(which it uses for authentication), and others. 270\nPuppet", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 284", "position": 284, "chunk_type": "semantic", "token_estimate": 229}
{"text": "Agent: puppet master\nConnect\nPlugins\nFacter\n(facts)\nENC? puppet console\n(or other data source)\nFinish\nReport? \ufb01le? Database\nclasses:\n- ssh:\n  ssh_key: AAAA\u2026\nparameters:\nconf: LISA11\n./enc.script $certname\nnode $certname {\n  $conf = LISA11\n  class { 'ssh':\n    ssh_key => 'AAA. ',\n  }\n}\nCommon\nProd\nDev\nCompile Catalog\n(functions)\nApply Catalog\npuppet \ufb01leserver\npuppet report\npuppet:///server/\nReport Procesor\nFigure 18.2: Orchestration of data \ufb02ow between Puppet processes and components\n18.3\nComponent Analysis\nAgent\nThe \ufb01rst component encountered in a Puppet run is the agent process. This was traditionally a\nseparate executable called puppetd, but in version 2.6 we reduced down to one executable so now it\nis invoked with puppet agent, akin to how Git works. The agent has little functionality of its own;\nit is primarily con\ufb01guration and code that implements the client-side aspects of the above-described\nwork\ufb02ow. Facter\nThe next component after the agent is an external tool called Facter, which is a very simple tool used\nto discover information about the host it is running on. This is data like the operating system, IP\naddress, and host name, but Facter is easily extensible so many organizations add their own plugins\nto discover custom data. The agent sends the data discovered by Facter to the server, at which point\nit takes over the work\ufb02ow. External Node Classi\ufb01er\nOn the server, the \ufb01rst component encountered is what we call the External Node Classi\ufb01er, or\nENC. The ENC accepts the host name and returns a simple data structure containing the high-level\ncon\ufb01guration for that host. The ENC is generally a separate service or application: either another\nopen source project, such as Puppet Dashboard or Foreman, or integration with existing data stores,\nsuch as LDAP. The purpose of the ENC is to specify what functional classes a given host belongs to,\nLuke Kanies\n271", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 285", "position": 285, "chunk_type": "semantic", "token_estimate": 303}
{"text": "and what parameters should be used to con\ufb01gure those classes. For example, a given host might be: in the debian and webserver classes, and have the parameter datacenter set to atlanta. Note that as of Puppet 2.7, the ENC is not a required component; users can instead directly\nspecify node con\ufb01gurations in Puppet code. Support for an ENC was added about 2 years after\nPuppet was launched because we realized that classifying hosts is fundamentally di\ufb00erent than\ncon\ufb01guring them, and it made more sense to split these problems into separate tools than to extend\nthe language to support both facilities. The ENC is always recommended, and at some point soon\nwill become a required component (at which point Puppet will ship with a su\ufb03ciently useful one\nthat that requirement will not be a burden). Once the server receives classi\ufb01cation information from the ENC and system information from\nFacter (via the agent), it bundles all of the information into a Node object and passes it on to the\nCompiler. Compiler\nAs mentioned above, Puppet has a custom language built for specifying system con\ufb01gurations. Its\ncompiler is really three chunks: A Yacc-style parser generator and a custom lexer; a group of classes\nused to create our Abstract Syntax Tree (AST); and the Compiler class that handles the interactions\nof all of these classes and also functions as the API to this part of the system. The most complicated thing about the compiler is the fact that most Puppet con\ufb01guration code\nis lazily loaded on \ufb01rst reference (to reduce both load times and irrelevant logging about missing-\nbut-unneeded dependencies), which means there aren\u2019t really explicit calls to load and parse the\ncode. Puppet\u2019s parser uses a normal Yacc1-style parser generator (built using the open source Racc2\ntool). Unfortunately, there were no open source lexer generators when Puppet was begun, so it uses a\ncustom lexer. Because we use an AST in Puppet, every statement in the Puppet grammar evaluates to an instance\nof a Puppet AST class (e.g., Puppet::Parser::AST::Statement), rather than taking action directly,\nand these AST instances are collected into a tree as the grammar tree is reduced. This AST provides\na performance bene\ufb01t when a single server is compiling con\ufb01gurations for many di\ufb00erent nodes,\nbecause we can parse once but compile many times. It also gives us the opportunity to perform some\nintrospection of the AST, which provides us information and capability we wouldn\u2019t have if parsing\noperated directly.", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 286", "position": 286, "chunk_type": "semantic", "token_estimate": 408}
{"text": "and what parameters should be used to con\ufb01gure those classes. For example, a given host might be: This AST provides\na performance bene\ufb01t when a single server is compiling con\ufb01gurations for many di\ufb00erent nodes,\nbecause we can parse once but compile many times. It also gives us the opportunity to perform some\nintrospection of the AST, which provides us information and capability we wouldn\u2019t have if parsing\noperated directly. Very few approachable AST examples were available when Puppet was begun, so there has been\na lot of evolution in it, and we\u2019ve arrived at what seems a relatively unique formulation. Rather than\ncreating a single AST for the entire con\ufb01guration, we create many small ASTs, keyed o\ufb00their name. For instance, this code:\nclass ssh {\npackage { ssh: ensure => present }\n}\ncreates a new AST containing a single Puppet::Parser::AST::Resource instance, and stores that\nAST by the name \u201cssh\u201d in the hash of all classes for this particular environment. (I\u2019ve left out details\nabout other constructs akin to classes, but they are unnecessary for this discussion.) 1http://dinosaur.compilertools.net/\n2https://github.com/tenderlove/racc\n272\nPuppet", "domains": ["Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 286", "position": 286, "chunk_type": "semantic", "token_estimate": 181}
{"text": "Given the AST and a Node object (from the ENC), the compiler takes the classes speci\ufb01ed in the: node object (if there are any), looks them up and evaluates them. In the course of this evaluation, the\ncompiler is building up a tree of variable scopes; every class gets its own scope which is attached to\nthe creating scope. This amounts to dynamic scoping in Puppet: if one class includes another class,\nthen the included class can look up variables directly in the including class. This has always been a\nnightmare, and we have been on the path to getting rid of this capability. The Scope tree is temporary and is discarded once compiling is done, but the artifact of compiling\nis also built up gradually over the course of the compilation. We call this artifact a Catalog, but it\nis just a graph of resources and their relationships. Nothing of the variables, control structures, or\nfunction calls survive into the catalog; it\u2019s plain data, and can be trivially converted to JSON, YAML,\nor just about anything else. During compilation, we create containment relationships; a class \u201ccontains\u201d all of the resources\nthat come with that class (e.g., the ssh package above is contained by the ssh class). A class might\ncontain a de\ufb01nition, which itself contains either yet more de\ufb01nitions, or individual resources. A\ncatalog tends to be a very horizontal, disconnected graph: many classes, each no more than a couple\nof levels deep. One of the awkward aspects of this graph is that it also contains \u201cdependency\u201d relationships, such\nas a service requiring a package (maybe because the package installation actually creates the service),\nbut these dependency relationships are actually speci\ufb01ed as parameter values on the resources, rather\nthan as edges in the structure of the graph. Our graph class (called SimpleGraph, for historical\nreasons) does not support having both containment and dependency edges in the same graph, so we\nhave to convert between them for various purposes. Transaction\nOnce the catalog is entirely constructed (assuming there is no failure), it is passed on to the Transaction. In a system with a separate client and server, the Transaction runs on the client, which pulls the\nCatalog down via HTTP as in Figure 18.2. Puppet\u2019s transaction class provides the framework for actually a\ufb00ecting the system, whereas\neverything else we\u2019ve discussed just builds up and passes around objects. Unlike transactions in\nmore common systems such as databases, Puppet transactions do not have behaviors like atomicity.", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 287", "position": 287, "chunk_type": "semantic", "token_estimate": 414}
{"text": "Given the AST and a Node object (from the ENC), the compiler takes the classes speci\ufb01ed in the: Puppet\u2019s transaction class provides the framework for actually a\ufb00ecting the system, whereas\neverything else we\u2019ve discussed just builds up and passes around objects. Unlike transactions in\nmore common systems such as databases, Puppet transactions do not have behaviors like atomicity. The transaction performs a relatively straightforward task: walk the graph in the order speci\ufb01ed\nby the various relationships, and make sure each resource is in sync. As mentioned above, it\nhas to convert the graph from containment edges (e.g., Class[ssh] contains Package[ssh] and\nService[sshd]) to dependency edges (e.g., Service[sshd] depends on Package[ssh]), and then\nit does a standard topological sort of the graph, selecting each resource in turn. For a given resource, we perform a simple three-step process: retrieve the current state of that\nresource, compare it to the desired state, and make any changes necessary to \ufb01x discrepancies. For\ninstance, given this code:\nfile { \"/etc/motd\":\nensure => file,\ncontent => \"Welcome to the machine\",\nmode => 644\n}\nthe transaction checks the content and mode of /etc/motd, and if they don\u2019t match the speci\ufb01ed state,\nit will \ufb01x either or both of them. If /etc/motd is somehow a directory, then it will back up all of the\n\ufb01les in that directory, remove it, and replace it with a \ufb01le that has the appropriate content and mode. Luke Kanies\n273", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 287", "position": 287, "chunk_type": "semantic", "token_estimate": 239}
{"text": "This process of making changes is actually handled by a simple ResourceHarness class that de\ufb01nes: the entire interface between Transaction and Resource. This reduces the number of connections\nbetween the classes, and makes it easier to make changes to either independently. Resource Abstraction Layer\nThe Transaction class is the heart of getting work done with Puppet, but all of the work is actually\ndone by the Resource Abstraction Layer (RAL), which also happens to be the most interesting\ncomponent in Puppet, architecturally speaking. The RAL was the \ufb01rst component created in Puppet and, other than the language, it most clearly\nde\ufb01nes what the user can do. The job of the RAL is to de\ufb01ne what it means to be a resource and how\nresources can get work done on the system, and Puppet\u2019s language is speci\ufb01cally built to specify\nresources as modeled by the RAL. Because of this, it\u2019s also the most important component in the\nsystem, and the hardest to change. There are plenty of things we would like to \ufb01x in the RAL, and\nwe\u2019ve made a lot of critical improvements to it over the years (the most crucial being the addition of\nProviders), but there is still a lot of work to do to the RAL in the long term. In the Compiler subsystem, we model resources and resource types with separate classes (named,\nconveniently, Puppet::Resource and Puppet::Resource::Type). Our goal is to have these classes\nalso form the heart of the RAL, but for now these two behaviors (resource and type) are modeled\nwithin a single class, Puppet::Type. (The class is named poorly because it signi\ufb01cantly predates\nour use of the term Resource, and at the time we were directly serializing memory structures when\ncommunicating between hosts, so it was actually quite complicated to change class names.) When Puppet::Type was \ufb01rst created, it seemed reasonable to put resource and resource type\nbehaviors in the same class; after all, resources are just instances of resource types. Over time,\nhowever, it became clear that the relationship between a resource and its resource type aren\u2019t modeled\nwell in a traditional inheritance structure. Resource types de\ufb01ne what parameters a resource can\nhave, but not whether it accepts parameters (they all do), for instance. Thus, our base class of\nPuppet::Type has class-level behaviors that determine how resource types behave, and instance-\nlevel behaviors that determine how resource instances behave.", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 288", "position": 288, "chunk_type": "semantic", "token_estimate": 397}
{"text": "def sync: File.open(@resource[:name], \"w\") { |f| f.print @resource[:content] }\nend\nend\nend\nThis example is simpli\ufb01ed considerably (e.g., we use checksums internally, rather than the full\ncontent strings), but you get the idea. This became impossible to manage as we needed to support multiple varieties of a given resource\ntype. Puppet now supports more than 30 kinds of package management, and it would have been\nimpossible to support all of those within a single Package resource type. Instead, we provide a clean\ninterface between the de\ufb01nition of the resource type\u2014essentially, what the name of the resource\ntype is and what properties it supports\u2014from how you manage that type of resource. Providers\nde\ufb01ne getter and setter methods for all of a resource type\u2019s properties, named in obvious ways. For\nexample, this is how a provider of the above property would look:\nPuppet::Type.newtype(:file) do\nnewproperty(:content)\nend\nPuppet::Type.type(:file).provide(:posix) do\ndef content\nFile.read(@resource[:name])\nend\ndef content=(str)\nFile.open(@resource[:name], \"w\") { |f| f.print(str) }\nend\nend\nThis is a touch more code in the simplest cases, but is much easier to understand and maintain,\nespecially as either the number of properties or number of providers increases. I said at the beginning of this section that the Transaction doesn\u2019t actually a\ufb00ect the system\ndirectly, and it instead relies on the RAL for that. Now it\u2019s clear that it\u2019s the providers that do the\nactual work. In fact, in general the providers are the only part of Puppet that actually touch the\nsystem. The transaction asks for a \ufb01le\u2019s content, and the provider collects it; the transaction speci\ufb01es\nthat a \ufb01le\u2019s content should be changed, and the provider changes it. Note, however, that the provider\nnever decides to a\ufb00ect the system\u2014the Transaction owns the decisions, and the provider does the\nwork. This gives the Transaction complete control without requiring that it understand anything\nabout \ufb01les, users, or packages, and this separation is what enables Puppet to have a full simulation\nmode where we can largely guarantee the system won\u2019t be a\ufb00ected. The second major class type in the RAL is responsible for the parameters themselves.", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 289", "position": 289, "chunk_type": "semantic", "token_estimate": 347}
{"text": "Reporting: As the transaction walks the graph and uses the RAL to change the system\u2019s con\ufb01guration, it\nprogressively builds a report. This report largely consists of the events generated by changes to the\nsystem. These events, in turn, are comprehensive re\ufb02ections of what work was done: they retain\na timestamp the resource changed, the previous value, the new value, any message generated, and\nwhether the change succeeded or failed (or was in simulation mode). The events are wrapped in a ResourceStatus object that maps to each resource. Thus, for a given\nTransaction, you know all of the resources that are run, and you know any changes that happen, along\nwith all of the metadata you might need about those changes. Once the transaction is complete, some basic metrics are calculated and stored in the report, and\nthen it is sent o\ufb00to the server (if con\ufb01gured). With the report sent, the con\ufb01guration process is\ncomplete, and the agent goes back to sleep or the process just ends. 18.4\nInfrastructure\nNow that we have a thorough understanding of what Puppet does and how, it\u2019s worth spending a\nlittle time on the pieces that don\u2019t show up as capabilities but are still critical to getting the job done. Plugins\nOne of the great things about Puppet is that it is very extensible. There are at least 12 di\ufb00erent\nkinds of extensibility in Puppet, and most of these are meant to be usable by just about anyone. For\nexample, you can create custom plugins for these areas:\n\u2022 resource types and custom providers\n\u2022 report handlers, such as for storing reports in a custom database\n\u2022 Indirector plugins for interacting with existing data stores\n\u2022 facts for discovering extra information about your hosts\nHowever, Puppet\u2019s distributed nature means that agents need a way to retrieve and load new\nplugins. Thus, at the start of every Puppet run, the \ufb01rst thing we do is download all plugins that the\nserver has available. These might include new resource types or providers, new facts, or even new\nreport processors. This makes it possible to heavily upgrade Puppet agents without ever changing the core Puppet\npackages. This is especially useful for highly customized Puppet installations. Indirector\nYou\u2019ve probably detected by now that we have a tradition of bad class names in Puppet, and according\nto most people, this one takes the cake.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 290", "position": 290, "chunk_type": "semantic", "token_estimate": 394}
{"text": "Reporting: This is especially useful for highly customized Puppet installations. Indirector\nYou\u2019ve probably detected by now that we have a tradition of bad class names in Puppet, and according\nto most people, this one takes the cake. The Indirector is a relatively standard Inversion of Control\nframework with signi\ufb01cant extensibility. Inversion of Control systems allow you to separate develop-\nment of functionality from how you control which functionality you use. In Puppet\u2019s case, this allows\nus to have many plugins that provide very di\ufb00erent functionality, such as reaching the compiler via\nHTTP or loading it in-process, and switch between them with a small con\ufb01guration change rather\nthan a code change. In other words, Puppet\u2019s Indirector is basically an implementation of a service\nlocator, as described on the Wikipedia page for \u201cInversion of Control\u201d. All of the hand-o\ufb00s from\none class to another go through the Indirector, via a standard REST-like interface (e.g., we support\n276\nPuppet", "domains": ["Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 290", "position": 290, "chunk_type": "semantic", "token_estimate": 156}
{"text": "\ufb01nd, search, save, and destroy as methods), and switching Puppet from serverless to client/server is: largely a question of con\ufb01guring the agent to use an HTTP endpoint for retrieving catalogs, rather\nthan using a compiler endpoint. Because it is an Inversion of Control framework where con\ufb01guration is stringently separated\nfrom the code paths, this class can also be di\ufb03cult to understand, especially when you\u2019re debugging\nwhy a given code path was used. Networking\nPuppet\u2019s prototype was written in the summer of 2004, when the big networking question was\nwhether to use XMLRPC or SOAP. We chose XMLRPC, and it worked \ufb01ne but had most of the\nproblems everyone else had: it didn\u2019t encourage standard interfaces between components, and it\ntended to get overcomplicated very quickly as a result. We also had signi\ufb01cant memory problems,\nbecause the encoding needed for XMLRPC resulted in every object appearing at least twice in\nmemory, which quickly gets expensive for large \ufb01les. For our 0.25 release (begun in 2008), we began the process of switching all networking to a\nREST-like model, but we chose a much more complicated route than just changing out the networking. We developed the Indirector as the standard framework for inter-component communication, and\nbuilt REST endpoints as just one option. It took two releases to fully support REST, and we have\nnot quite \ufb01nished converting to using JSON (instead of YAML) for all serialization. We undertook\nswitching to JSON for two major reasons: \ufb01rst, YAML processing Ruby is painfully slow, and pure\nRuby processing of JSON is a lot faster; second, most of the web seems to be moving to JSON, and\nit tends to be implemented more portably than YAML. Certainly in the case of Puppet, the \ufb01rst use\nof YAML was not portable across languages, and was often not portable across di\ufb00erent versions of\nPuppet, because it was essentially serialization of internal Ruby objects. Our next major release of Puppet will \ufb01nally remove all of the XMLRPC support. 18.5\nLessons Learned\nIn terms of implementation, we\u2019re proudest of the various kinds of separation that exist in Puppet:\nthe language is completely separate from the RAL, the Transaction cannot directly touch the system,\nand the RAL can\u2019t decide to do work on its own. This gives the application developer a lot of control\nover application work\ufb02ow, along with a lot of access to information about what is happening and\nwhy.", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 291", "position": 291, "chunk_type": "semantic", "token_estimate": 400}
{"text": "\ufb01nd, search, save, and destroy as methods), and switching Puppet from serverless to client/server is: 18.5\nLessons Learned\nIn terms of implementation, we\u2019re proudest of the various kinds of separation that exist in Puppet:\nthe language is completely separate from the RAL, the Transaction cannot directly touch the system,\nand the RAL can\u2019t decide to do work on its own. This gives the application developer a lot of control\nover application work\ufb02ow, along with a lot of access to information about what is happening and\nwhy. Puppet\u2019s extensibility and con\ufb01gurability are also major assets, because anyone can build on top\nof Puppet quite easily without having to hack the core. We\u2019ve always built our own capabilities on\nthe same interfaces we recommend our users use. Puppet\u2019s simplicity and ease of use have always been its major draw. It\u2019s still too di\ufb03cult to get\nrunning, but it\u2019s miles easier than any of the other tools on the market. This simplicity comes with a\nlot of engineering costs, especially in the form of maintenance and extra design work, but it\u2019s worth\nit to allow users to focus on their problems instead of the tool. Puppet\u2019s con\ufb01gurability is a real feature, but we took it a bit too far. There are too many ways\nyou can wire Puppet together, and it\u2019s too easy to build a work\ufb02ow on top of Puppet that will make\nyou miserable. One of our major near-term goals is to dramatically reduce the knobs you can turn in\na Puppet con\ufb01guration, so the user cannot so easily con\ufb01gure it poorly, and so we can more easily\nupgrade it over time without worrying about obscure edge cases. Luke Kanies\n277", "domains": ["Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 291", "position": 291, "chunk_type": "semantic", "token_estimate": 280}
{"text": "We also just generally changed too slowly. There are major refactors we\u2019ve been wanting to do: for years but have never quite tackled. This has meant a more stable system for our users in the short\nterm, but also a more di\ufb03cult-to-maintain system, and one that\u2019s much harder to contribute to. Lastly, it took us too long to realize that our goals of simplicity were best expressed in the\nlanguage of design. Once we began speaking about design rather than just simplicity, we acquired a\nmuch better framework for making decisions about adding or removing features, with a better means\nof communicating the reasoning behind those decisions. 18.6\nConclusion\nPuppet is both a simple system and a complex one. It has many moving parts, but they\u2019re wired\ntogether quite loosely, and each of them has changed pretty dramatically since its founding in 2005. It is a framework that can be used for all manner of con\ufb01guration problems, but as an application it\nis simple and approachable. Our future success rests on that framework becoming more solid and more simple, and that\napplication staying approachable while it gains capability. 278\nPuppet", "domains": ["Design Principles"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 292", "position": 292, "chunk_type": "semantic", "token_estimate": 190}
{"text": "[chapter19]: PyPy\nBenjamin Peterson\nPyPy is a Python implementation and a dynamic language implementation framework. This chapter assumes familiarity with some basic interpreter and compiler concepts like bytecode\nand constant folding. 19.1\nA Little History\nPython is a high-level, dynamic programming language. It was invented by the Dutch programmer\nGuido van Rossum in the late 1980s. Guido\u2019s original implementation is a traditional bytecode\ninterpreter written in C, and consequently known as CPython. There are now many other Python\nimplementations. Among the most notable are Jython, which is written in Java and allows for\ninterfacing with Java code, IronPython, which is written in C# and interfaces with Microsoft\u2019s\n.NET framework, and PyPy, the subject of this chapter. CPython is still the most widely used\nimplementation and currently the only one to support Python 3, the next generation of the Python\nlanguage. This chapter will explain the design decisions in PyPy that make it di\ufb00erent from other\nPython implementations and indeed from any other dynamic language implementation. 19.2\nOverview of PyPy\nPyPy, except for a negligible number of C stubs, is written completely in Python. The PyPy source\ntree contains two major components: the Python interpreter and the RPython translation toolchain. The Python interpreter is the programmer-facing runtime that people using PyPy as a Python\nimplementation invoke. It is actually written in a subset of Python called Restricted Python (usually\nabbreviated RPython). The purpose of writing the Python interpreter in RPython is so the interpreter\ncan be fed to the second major part of PyPy, the RPython translation toolchain. The RPython\ntranslator takes RPython code and converts it to a chosen lower-level language, most commonly\nC. This allows PyPy to be a self-hosting implementation, meaning it is written in the language it\nimplements. As we shall see throughout this chapter, the RPython translator also makes PyPy a\ngeneral dynamic language implementation framework. PyPy\u2019s powerful abstractions make it the most \ufb02exible Python implementation. It has nearly\n200 con\ufb01guration options, which vary from selecting di\ufb00erent garbage collector implementations to\naltering parameters of various translation optimizations.", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 293", "position": 293, "chunk_type": "semantic", "token_estimate": 341}
{"text": "19.3: The Python Interpreter\nSince RPython is a strict subset of Python, the PyPy Python interpreter can be run on top of another\nPython implementation untranslated. This is, of course, extremely slow but it makes it possible to\nquickly test changes in the interpreter. It also enables normal Python debugging tools to be used to\ndebug the interpreter. Most of PyPy\u2019s interpreter tests can be run both on the untranslated interpreter\nand the translated interpreter. This allows quick testing during development as well as assurance that\nthe translated interpreter behaves the same as the untranslated one. For the most part, the details of the PyPy Python interpreter are quite similiar to that of CPython;\nPyPy and CPython use nearly identical bytecode and data structures during interpretation. The\nprimary di\ufb00erence between the two is PyPy has a clever abstraction called object spaces (or objspaces\nfor short). An objspace encapsulates all the knowledge needed to represent and manipulate Python\ndata types. For example, performing a binary operation on two Python objects or fetching an attribute\nof an object is handled completely by the objspace. This frees the interpreter from having to know\nanything about the implementation details of Python objects. The bytecode interpreter treats Python\nobjects as black boxes and calls objspace methods whenever it needs to manipulate them. For example,\nhere is a rough implementation of the BINARY_ADD opcode, which is called when two objects are\ncombined with the + operator. Notice how the operands are not inspected by the interpreter; all\nhandling is delegated immediately to the objspace. def BINARY_ADD(space, frame):\nobject1 = frame.pop() # pop left operand off stack\nobject2 = frame.pop() # pop right operand off stack\nresult = space.add(object1, object2) # perform operation\nframe.push(result) # record result on stack\nThe objspace abstraction has numerous advantages. It allows new data type implementations to\nbe swapped in and out without modifying the interpreter. Also, since the sole way to manipulate\nobjects is through the objspace, the objspace can intercept, proxy, or record operations on objects. Using the powerful abstraction of objspaces, PyPy has experimented with thunking, where results\ncan be lazily but completely transparently computed on demand, and tainting, where any operation\non an object will raise an exception (useful for passing sensitive data through untrusted code). The\nmost important application of objspaces, however, will be discussed in Section 19.4.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 294", "position": 294, "chunk_type": "semantic", "token_estimate": 391}
{"text": "19.3: Using the powerful abstraction of objspaces, PyPy has experimented with thunking, where results\ncan be lazily but completely transparently computed on demand, and tainting, where any operation\non an object will raise an exception (useful for passing sensitive data through untrusted code). The\nmost important application of objspaces, however, will be discussed in Section 19.4. The objspace used in a vanilla PyPy interpreter is called the standard objspace (std objspace for\nshort). In addition to the abstraction provided by the objspace system, the standard objspace provides\nanother level of indirection; a single data type may have multiple implementations. Operations on data\ntypes are then dispatched using multimethods. This allows picking the most e\ufb03cient representation\nfor a given piece of data. For example, the Python long type (ostensibly a bigint data type) can\nbe represented as a standard machine-word-sized integer when it is small enough. The memory\nand computationally more expensive arbitrary-precision long implementation need only be used\nwhen necessary. There\u2019s even an implementation of Python integers available using tagged pointers. Container types can also be specialized to certain data types. For example, PyPy has a dictionary\n(Python\u2019s hash table data type) implementation specialized for string keys. The fact that the same data\ntype can be represented by di\ufb00erent implementations is completely transparent to application-level\ncode; a dictionary specialized to strings is identical to a generic dictionary and will degenerate\ngracefully if non-string keys are put into it. PyPy distinguishes between interpreter-level (interp-level) and application-level (app-level) code. Interp-level code, which most of the interpreter is written in, must be in RPython and is translated. 280\nPyPy", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 294", "position": 294, "chunk_type": "semantic", "token_estimate": 266}
{"text": "It directly works with the objspace and wrapped Python objects. App-level code is always run by: the PyPy bytecode interpreter. As simple as interp-level RPython code is, compared to C or Java,\nPyPy developers have found it easiest to use pure app-level code for some parts of the interpreter. Consequently, PyPy has support for embedding app-level code in the interpreter. For example, the\nfunctionality of the Python print statement, which writes objects to standard output, is implemented\nin app-level Python. Builtin modules can also be written partially in interp-level code and partially\nin app-level code. 19.4\nThe RPython Translator\nThe RPython translator is a toolchain of several lowering phases that rewrite RPython to a target\nlanguage, typically C. The higher-level phases of translation are shown in Figure 19.1. The translator\nis itself written in (unrestricted) Python and intimately linked to the PyPy Python interpreter for\nreasons that will be illuminated shortly. Figure 19.1: Translation steps\nThe \ufb01rst thing the translator does is load the RPython program into its process. (This is done\nwith the normal Python module loading support.) RPython imposes a set of restrictions on normal,\ndynamic Python. For example, functions cannot be created at runtime, and a single variable cannot\nhave the possibility of holding incompatible types, such as an integer and a object instance. When\nthe program is initially loaded by the translator, though, it is running on a normal Python interpreter\nand can use all of Python\u2019s dynamic features. PyPy\u2019s Python interpreter, a huge RPython program,\nmakes heavy use of this feature for metaprogramming. For example, it generates code for standard\nobjspace multimethod dispatch. The only requirement is that the program is valid RPython by the\ntime the translator starts the next phase of translation. The translator builds \ufb02ow graphs of the RPython program through a process called abstract\ninterpretation. Abstract interpretation reuses the PyPy Python interpreter to interpret RPython\nprograms with a special objspace called the \ufb02ow objspace. Recall that the Python interpreter treats\nobjects in a program like black boxes, calling out to the objspace to perform any operation. The \ufb02ow\nobjspace, instead of the standard set of Python objects, has only two objects: variables and constants. Variables represent values not known during translation, and constants, not surprisingly, represent\nimmutable values that are known. The \ufb02ow objspace has a basic facility for constant folding; if it is\nasked to do an operation where all the arguments are constants, it will statically evaluate it. What\nBenjamin Peterson\n281", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 295", "position": 295, "chunk_type": "semantic", "token_estimate": 413}
{"text": "is immutable and must be constant in RPython is broader than in standard Python. For example,: modules, which are emphatically mutable in Python, are constants in the \ufb02ow objspace because\nthey don\u2019t exist in RPython and must be constant-folded out by the \ufb02ow objspace. As the Python\ninterpreter interprets the bytecode of RPython functions, the \ufb02ow objspace records the operations it\nis asked to perform. It takes care to record all branches of conditional control \ufb02ow constructs. The\nend result of abstract interpretation for a function is a \ufb02ow-graph consisting of linked blocks, where\neach block has one or more operations. An example of the \ufb02ow-graph generating process is in order. Consider a simple factorial function:\ndef factorial(n):\nif n == 1:\nreturn 1\nreturn n * factorial(n - 1)\nThe \ufb02ow-graph for the function looks like Figure 19.2. Figure 19.2: Flow-graph of factorial\nThe factorial function has been divided into blocks containing the operations the \ufb02owspace\nrecorded. Each block has input arguments and a list of operations on the variables and constants. The \ufb01rst block has an exit switch at the end, which determines which block control-\ufb02ow will pass to\nafter the \ufb01rst block is run. The exit switch can be based on the value of some variable or whether\nan exception occurred in the last operation of the block. Control-\ufb02ow follows the lines between the\nblocks. The \ufb02ow-graph generated in the \ufb02ow objspace is in static single assignment form, or SSA, an\nintermediate representation commonly used in compilers. The key feature of SSA is that every\n282\nPyPy", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 296", "position": 296, "chunk_type": "semantic", "token_estimate": 260}
{"text": "variable is only assigned once. This property simpli\ufb01es the implementation of many compiler: transformations and optimizations. After a function graph is generated, the annotation phase begins. The annotator assigns a type\nto the results and arguments of each operation. For example, the factorial function above will be\nannotated to accept and return an integer. The next phase is called RTyping. RTyping uses type information from the annotator to expand\neach high-level \ufb02ow-graph operation into low-level ones. It is the \ufb01rst part of translation where the\ntarget backend matters. The backend chooses a type system for the RTyper to specialize the program\nto. The RTyper currently has two type systems: A low-level typesystem for backends like C and one\nfor higher-level typesystems with classes. High-level Python operations and types are transformed\ninto the level of the type system. For example, an add operation with operands annotated as integers\nwill generate a int_add operation with the low-level type system. More complicated operations like\nhash table lookups generate function calls. After RTyping, some optimizations on the low-level \ufb02ow-graph are performed. They are mostly\nof the traditional compiler variety like constant folding, store sinking, and dead code removal. Python code typically has frequent dynamic memory allocations. RPython, being a Python\nderivative, inherits this allocation intensive pattern. In many cases, though, allocations are temporary\nand local to a function. Malloc removal is an optimization that addresses these cases. Malloc removal\nremoves these allocations by \u201c\ufb02attening\u201d the previously dynamically allocated object into component\nscalars when possible. To see how malloc removals works, consider the following function that computes the Euclidean\ndistance between two points on the plane in a roundabout fashion:\ndef distance(x1, y1, x2, y2):\np1 = (x1, y1)\np2 = (x2, y2)\nreturn math.hypot(p1[0] - p2[0], p1[1] - p2[1])\nWhen initially RTyped, the body of the function has the following operations:\nv60 = malloc((GcStruct tuple2))\nv61 = setfield(v60, (\u2019item0\u2019), x1_1)\nv62 = setfield(v60, (\u2019item1\u2019), y1_1)\nv63 = malloc((GcStruct tuple2))\nv64 = setfield(v63, (\u2019item0\u2019), x2_1)\nv65 = setfield(v63, (\u2019item1\u2019), y2_1)\nv66 = getfield(v60, (\u2019item0\u2019))\nv67 = getfield(v63, (\u2019item0\u2019))\nv68 = int_sub(v66, v67)\nv69 = getfield(v60, (\u2019item1\u2019))\nv70 = getfield(v63, (\u2019item1\u2019))\nv71 = int_sub(v69, v70)\nv72 = cast_int_to_float(v68)\nv73 = cast_int_to_float(v71)\nv74 = direct_call(math_hypot, v72, v73)\nThis code is suboptimal in several ways. Two tuples that never escape the function are allocated. Additionally, there is unnecessary indirection accessing the tuple \ufb01elds. Running malloc removal produces the following concise code:\nBenjamin Peterson\n283", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 297", "position": 297, "chunk_type": "semantic", "token_estimate": 406}
{"text": "v53 = int_sub(x1_0, x2_0): v56 = int_sub(y1_0, y2_0)\nv57 = cast_int_to_float(v53)\nv58 = cast_int_to_float(v56)\nv59 = direct_call(math_hypot, v57, v58)\nThe tuple allocations have been completely removed and the indirections \ufb02attened out. Later, we\nwill see how a technique similar to malloc removal is used on application-level Python in the PyPy\nJIT (Section 19.5). PyPy also does function inlining. As in lower-level languages, inlining improves performance\nin RPython. Somewhat surprisingly, it also reduces the size of the \ufb01nal binary. This is because it\nallows more constant folding and malloc removal to take place, which reduces overall code size. The program, now in optimized, low-level \ufb02ow-graphs, is passed to the backend to generate\nsources. Before it can generate C code, the C backend must perform some additional transformations. One of these is exception transformation, where exception handling is rewritten to use manual stack\nunwinding. Another is the insertion of stack depth checks. These raise an exception at runtime if the\nrecursion is too deep. Places where stack depth checks are needed are found by computing cycles in\nthe call graph of the program. Another one of the transformations performed by the C backend is adding garbage collection\n(GC). RPython, like Python, is a garbage-collected language, but C is not, so a garbage collector has\nto be added. To do this, a garbage collection transformer converts the \ufb02ow-graphs of the program into\na garbage-collected program. PyPy\u2019s GC transformers provide an excellent demonstration of how\ntranslation abstracts away mundane details. In CPython, which uses reference counting, the C code\nof the interpreter must carefully keep track of references to Python objects it is manipulating. This\nnot only hardcodes the garbage collection scheme in the entire codebase but is prone to subtle human\nerrors. PyPy\u2019s GC transformer solves both problems; it allows di\ufb00erent garbage collection schemes\nto be swapped in and out seamlessly. It is trivial to evaluate a garbage collector implementation (of\nwhich PyPy has many), simply by tweaking a con\ufb01guration option at translation. Modulo transformer\nbugs, the GC transformer also never makes reference mistakes or forgets to inform the GC when\nan object is no longer in use. The power of the GC abstraction allows GC implementations that\nwould be practically impossible to hardcode in an interpreter. For example, several of PyPy\u2019s GC\nimplementations require a write barrier.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 298", "position": 298, "chunk_type": "semantic", "token_estimate": 385}
{"text": "v53 = int_sub(x1_0, x2_0): The power of the GC abstraction allows GC implementations that\nwould be practically impossible to hardcode in an interpreter. For example, several of PyPy\u2019s GC\nimplementations require a write barrier. A write barrier is a check which must be performed every\ntime a GC-managed object is placed in another GC-managed array or structure. The process of\ninserting write barriers would be laborious and fraught with mistakes if done manually, but is trivial\nwhen done automatically by the GC transformer. The C backend can \ufb01nally emit C source code. The generated C code, being generated from\nlow-level \ufb02ow-graphs, is an ugly mess of gotos and obscurely named variables. An advantage of\nwriting C is that the C compiler can do most of the complicated static transformation work required\nto make a \ufb01nal binary-like loop optimizations and register allocation. 19.5\nThe PyPy JIT\nPython, like most dynamic languages, has traditionally traded e\ufb03ciency for \ufb02exibility. The archi-\ntecture of PyPy, being especially rich in \ufb02exibility and abstraction, makes very fast interpretation\ndi\ufb03cult. The powerful objspace and multimethod abstractions in the std objspace do not come with-\nout a cost. Consequently, the vanilla PyPy interpreter performs up to 4 times slower than CPython. To remedy not only this but Python\u2019s reputation as a sluggish language, PyPy has a just-in-time\n284\nPyPy", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 298", "position": 298, "chunk_type": "semantic", "token_estimate": 221}
{"text": "compiler (commonly written JIT). The JIT compiles frequently used codepaths into assembly during: the runtime of the program. The PyPy JIT takes advantage of PyPy\u2019s unique translation architecture described in Section 19.4. PyPy actually has no Python-speci\ufb01c JIT; it has a JIT generator. JIT generation is implemented as\nsimply another optional pass during translation. A interpreter desiring JIT generation need only\nmake two special function calls called jit hints. PyPy\u2019s JIT is a tracing JIT. This means it detects \u201chot\u201d (meaning frequently run) loops to\noptimize by compiling to assembly. When the JIT has decided it is going to compile a loop, it records\noperations in one iteration of the loop, a process called tracing. These operations are subsequently\ncompiled to machine code. As mentioned above, the JIT generator requires only two hints in the interpreter to generate a JIT:\nmerge_point and can_enter_jit. can_enter_jit tells the JIT where in the interpreter a loop\nstarts. In the Python interpreter, this is the end of the JUMP_ABSOLUTE bytecode. (JUMP_ABSOLUTE\nmakes the interpreter jump to the head of the app-level loop.) merge_point tells the JIT where it is\nsafe to return to the interpreter from the JIT. This is the beginning of the bytecode dispatch loop in\nthe Python interpreter. The JIT generator is invoked after the RTyping phase of translation. Recall that at this point,\nthe program\u2019s \ufb02ow-graphs consist of low-level operations nearly ready for target code generation. The JIT generator locates the hints mentioned above in the interpreter and replaces them with calls\nto invoke the JIT during runtime. The JIT generator then writes a serialized representation of the\n\ufb02ow-graphs of every function that the interpreter wants jitted. These serialized \ufb02ow-graphs are called\njitcodes. The entire interpreter is now described in terms of low-level RPython operations. The\njitcodes are saved in the \ufb01nal binary for use at runtime. At runtime, the JIT maintains a counter for every loop that is executed in the program. When a\nloop\u2019s counter exceeds a con\ufb01gurable threshold, the JIT is invoked and tracing begins. The key object\nin tracing is the meta-interpreter. The meta-interpreter executes the jitcodes created in translation. It is thus interpreting the main interpreter, hence the name. As it traces the loop, it creates a list\nof the operations it is executing and records them in JIT intermediate representation (IR), another\noperation format. This list is called the trace of the loop.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 299", "position": 299, "chunk_type": "semantic", "token_estimate": 399}
{"text": "compiler (commonly written JIT). The JIT compiles frequently used codepaths into assembly during: As it traces the loop, it creates a list\nof the operations it is executing and records them in JIT intermediate representation (IR), another\noperation format. This list is called the trace of the loop. When the meta-interpreter encounters a\ncall to a jitted function (one for which jitcode exists), the meta-interpreter enters it and records its\noperations to original trace. Thus, the tracing has the e\ufb00ect of \ufb02attening out the call stack; the only\ncalls in the trace are to interpreter functions that are outside the knowledge of jit. The meta-interpreter is forced to specialize the trace to properties of the loop iteration it is\ntracing. For example, when the meta-interpreter encounters a conditional in the jitcode, it naturally\nmust choose one path based on the state of the program. When it makes a choice based on runtime\ninformation, the meta-interpreter records an IR operation called a guard. In the case of a conditional,\nthis will be a guard_true or guard_false operation on the condition variable. Most arithmetic\noperations also have guards, which ensure the operation did not over\ufb02ow. Essentially, guards codify\nassumptions the meta-interpreter is making as it traces. When assembly is generated, the guards\nwill protect assembly from being run in a context it is not specialized for. Tracing ends when the\nmeta-interpreter reaches the same can_enter_jit operation with which it started tracing. The loop\nIR can now be passed to the optimizer. The JIT optimizer features a few classical compiler optimizations and many optimizations special-\nized for dynamic languages. Among the most important of the latter are virtuals and virtualizables. Virtuals are objects which are known not to escape the trace, meaning they are not passed as\narguments to external, non-jitted function calls. Structures and constant length arrays can be virtuals. Virtuals do not have to be allocated, and their data can be stored directly in registers and on the\nBenjamin Peterson\n285", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 299", "position": 299, "chunk_type": "semantic", "token_estimate": 330}
{"text": "stack. (This is much like the static malloc removal phase described in the section about translation: backend optimizations.) The virtuals optimization strips away the indirection and memory allocation\nine\ufb03ciencies in the Python interpreter. For example, by becoming virtual, boxed Python integer\nobjects are unboxed into simple word-sized integers and can be stored directly in machine registers. A virtualizable acts much like a virtual but may escape the trace (that is, be passed to non-\njitted functions). In the Python interpreter the frame object, which holds variable values and the\ninstruction pointer, is marked virtualizable. This allows stack manipulations and other operations on\nthe frame to be optimized out. Although virtuals and virtualizables are similar, they share nothing in\nimplementation. Virtualizables are handled during tracing by the meta-interpreter. This is unlike\nvirtuals, which are handled during trace optimization. The reason for this is virtualizables require\nspecial treatment, since they may escape the trace. Speci\ufb01cally, the meta-interpreter has to ensure\nthat non-jitted functions that may use the virtualizable don\u2019t actually try to fetch its \ufb01elds. This is\nbecause in jitted code, the \ufb01elds of virtualizable are stored in the stack and registers, so the actual\nvirtualizable may be out of date with respect to its current values in the jitted code. During JIT\ngeneration, code which accesses a virtualizable is rewritten to check if jitted assembly is running. If\nit is, the JIT is asked to update the \ufb01elds from data in assembly. Additionally when the external call\nreturns to jitted code, execution bails back to the interpreter. After optimization, the trace is ready to be assembled. Since the JIT IR is already quite low-level,\nassembly generation is not too di\ufb03cult. Most IR operations correspond to only a few x86 assembly\noperations. The register allocator is a simple linear algorithm. At the moment, the increased time that\nwould be spent in the backend with a more sophisticated register allocation algorithm in exchange for\ngenerating slightly better code has not been justi\ufb01ed. The trickiest portions of assembly generation\nare garbage collector integration and guard recovery. The GC has to be made aware of stack roots in\nthe generated JIT code. This is accomplished by special support in the GC for dynamic root maps. Figure 19.3: Bailing back to the interpreter on guard failure\nWhen a guard fails, the compiled assembly is no longer valid and control must return to the\nbytecode interpreter.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 300", "position": 300, "chunk_type": "semantic", "token_estimate": 399}
{"text": "stack. (This is much like the static malloc removal phase described in the section about translation: This is accomplished by special support in the GC for dynamic root maps. Figure 19.3: Bailing back to the interpreter on guard failure\nWhen a guard fails, the compiled assembly is no longer valid and control must return to the\nbytecode interpreter. This bailing out is one of the most di\ufb03cult parts of JIT implementation, since\nthe interpreter state has to be reconstructed from the register and stack state at the point the guard\nfailed. For each guard, the assembler writes a compact description of where all the values needed to\nreconstruct the interpreter state are. At guard failure, execution jumps to a function which decodes\nthis description and passes the recovery values to a higher level be reconstructed. The failing guard\nmay be in the middle of the execution of a complicated opcode, so the interpreter can not just start\nwith the next opcode. To solve this, PyPy uses a blackhole interpreter. The blackhole interpreter\nexecutes jitcodes starting from the point of guard failure until the next merge point is reached. There, the real interpreter can resume. The blackhole interpreter is so named because unlike the\nmeta-interpreter, it doesn\u2019t record any of the operations it executes. The process of guard failure is\ndepicted in Figure 19.3. 286\nPyPy", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 300", "position": 300, "chunk_type": "semantic", "token_estimate": 225}
{"text": "As described up to this point, the JIT would be essentially useless on any loop with a frequently: changing condition, because a guard failure would prevent assembly from running very many\niterations. Every guard has a failure counter. After the failure count has passed a certain threshold,\nthe JIT starts tracing from the point of guard failure instead of bailing back to the interpreter. This\nnew sub-trace is called a bridge. When the tracing reaches the end of the loop, the bridge is optimized\nand compiled and the original loop is patched at the guard to jump to the new bridge instead of the\nfailure code. This way, loops with dynamic conditions can be jitted. How successful have the techniques used in the PyPy JIT proven? At the time of this writing, PyPy\nis a geometric average of \ufb01ve times faster than CPython on a comprehensive suite of benchmarks. With the JIT, app-level Python has the possibility of being faster than interp-level code. PyPy\ndevelopers have recently had the excellent problem of having to write interp-level loops in app-level\nPython for performance. Most importantly, the fact that the JIT is not speci\ufb01c to Python means it can be applied to any\ninterpreter written within the PyPy framework. This need not necessarily be a language interpreter. For example, the JIT is used for Python\u2019s regular expression engine. NumPy is a powerful array\nmodule for Python used in numerical computing and scienti\ufb01c research. PyPy has an experimental\nreimplementation of NumPy. It harnesses the power of the PyPy JIT to speed up operations on\narrays. While the NumPy implementation is still in its early stages, initial performance results look\npromising. 19.6\nDesign Drawbacks\nWhile it beats C any day, writing in RPython can be a frustrating experience. Its implicit typing\nis di\ufb03cult to get used to at \ufb01rst. Not all Python language features are supported and others are\narbitrarily restricted. RPython is not speci\ufb01ed formally anywhere and what the translator accepts\ncan vary from day to day as RPython is adapted to PyPy\u2019s needs. The author of this chapter often\nmanages to create programs that churn in the translator for half an hour, only to fail with an obscure\nerror. The fact that the RPython translator is a whole-program analyzer creates some practical problems. The smallest change anywhere in translated code requires retranslating the entire interpreter. That\ncurrently takes about 40 minutes on a fast, modern system.", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 301", "position": 301, "chunk_type": "semantic", "token_estimate": 404}
{"text": "As described up to this point, the JIT would be essentially useless on any loop with a frequently: The smallest change anywhere in translated code requires retranslating the entire interpreter. That\ncurrently takes about 40 minutes on a fast, modern system. The delay is especially annoying for\ntesting how changes a\ufb00ect the JIT, since measuring performance requires a translated interpreter. The\nrequirement that the whole program be present at translation means modules containing RPython\ncannot be built and loaded separately from the core interpreter. The levels of abstraction in PyPy are not always as clear cut as in theory. While technically the\nJIT generator should be able to produce an excellent JIT for a language given only the two hints\nmentioned above, the reality is that it behaves better on some code than others. The Python interpreter\nhas seen a lot of work towards making it more \u201cjit-friendly\u201d, including many more JIT hints and\neven new data structures optimized for the JIT. The many layers of PyPy can make tracking down bugs a laborious process. A Python interpreter\nbug could be directly in the interpreter source or buried somewhere in the semantics of RPython and\nthe translation toolchain. Especially when a bug cannot be reproduced on the untranslated interpreter,\ndebugging is di\ufb03cult. It typically involves running GDB on the nearly unreadable generated C\nsources. Translating even a restricted subset of Python to a much lower-level language like C is not an\neasy task. The lowering passes described in Section 19.4 are not really independent. Functions\nBenjamin Peterson\n287", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 301", "position": 301, "chunk_type": "semantic", "token_estimate": 258}
{"text": "are being annotated and rtyped throughout translation, and the annotator has some knowledge of: low-level types. The RPython translator is thus a tangled web of cross-dependencies. The translator\ncould do with cleaning up in several places, but doing it is neither easy nor much fun. 19.7\nA Note on Process\nIn part to combat its own complexity (see Section 19.6), PyPy has adopted several so-called \u201cagile\u201d\ndevelopment methodologies. By far the most important of these is test-driven development. All\nnew features and bug \ufb01xes are required to have tests to verify their correctness. The PyPy Python\ninterpreter is also run against CPython\u2019s regression test suite. PyPy\u2019s test driver, py.test, was spun o\ufb00\nand is now used in many other projects. PyPy also has a continuous integration system that runs\nthe test suite and translates the interpreter on a variety of platforms. Binaries for all platforms are\nproduced daily and the benchmark suite is run. All these tests ensure that the various components\nare behaving, no matter what change is made in the complicated architecture. There is a strong culture of experimentation in the PyPy project. Developers are encouraged\nto make branches in the Mercurial repository. There, ideas in development can be re\ufb01ned without\ndestabilizing the main branch. Branches are not always successful, and some are abandoned. If\nanything though, PyPy developers are tenacious. Most famously, the current PyPy JIT is the \ufb01fth\nattempt to add a JIT to PyPy! Figure 19.4: The jitviewer showing Python bytecode and associated JIT IR operations\n288\nPyPy", "domains": ["Design Patterns", "Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 302", "position": 302, "chunk_type": "semantic", "token_estimate": 253}
{"text": "The PyPy project also prides itself on its visualization tools. The \ufb02ow-graph charts in Section 19.4: are one example. PyPy also has tools to show invocation of the garbage collector over time and view\nthe parse trees of regular expressions. Of special interest is jitviewer, a program that allows one to\nvisually peel back the layers of a jitted function, from Python bytecode to JIT IR to assembly. (The\njitviewer is shown in Figure 19.4.) Visualization tools help developers understand how PyPy\u2019s many\nlayers interact with each other. 19.8\nSummary\nThe Python interpreter treats Python objects as black boxes and leaves all behavior to be de\ufb01ned by\nthe objspace. Individual objspaces can provide special extended behavior to Python objects. The\nobjspace approach also enables the abstract interpretation technique used in translation. The RPython translator allows details like garbage collection and exception handling to be\nabstracted from the language interpreter. It also opens up the possibly of running PyPy on many\ndi\ufb00erent runtime platforms by using di\ufb00erent backends. One of the most important uses of the translation architecture is the JIT generator. The generality\nof the JIT generator allows JITs for new languages and sub-languages like regular expressions to be\nadded. PyPy is the fastest Python implementation today because of its JIT generator. While most of PyPy\u2019s development e\ufb00ort has gone into the Python interpreter, PyPy can be used\nfor the implementation of any dynamic language. Over the years, partial interpreters for JavaScript,\nProlog, Scheme, and IO have been written with PyPy. 19.9\nLessons Learned\nFinally, some of lessons to take away from the PyPy project:\nRepeated refactoring is often a necessary process. For example, it was originally envisioned that\nthe C backend for the translator would be able to work o\ufb00the high-level \ufb02ow graphs! It took several\niterations for the current multi-phase translation process to be born. The most important lesson of PyPy is the power of abstraction. In PyPy, abstractions separate\nimplementation concerns. For example, RPython\u2019s automatic garbage collection allows a developer\nworking the interpreter to not worry about memory management. At the same time, abstractions have\na mental cost. Working on the translation chain involves juggling the various phases of translation\nat once in one\u2019s head. What layer a bug resides in can also be clouded by abstractions; abstraction\nleakage, where swapping low-level components that should be interchangeable breaks higher-level\ncode, is perennial problem.", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 303", "position": 303, "chunk_type": "semantic", "token_estimate": 396}
{"text": "just on what data is desired, but also on the structure of information. If this utility is concealed,: there\u2019s little point in using a relational database in the \ufb01rst place. The issue of reconciling applications that seek concealment of an underlying relational database\nwith the fact that relational databases require great speci\ufb01city is often referred to as the \u201cobject-\nrelational impedance mismatch\u201d problem. SQLAlchemy takes a somewhat novel approach to this\nproblem. SQLAlchemy\u2019s Approach to Database Abstraction\nSQLAlchemy takes the position that the developer must be willing to consider the relational form of\nhis or her data. A system which pre-determines and conceals schema and query design decisions\nmarginalizes the usefulness of using a relational database, leading to all of the classic problems of\nimpedance mismatch. At the same time, the implementation of these decisions can and should be executed through\nhigh-level patterns as much as possible. Relating an object model to a schema and persisting it via\nSQL queries is a highly repetitive task. Allowing tools to automate these tasks allows the development\nof an application that\u2019s more succinct, capable, and e\ufb03cient, and can be created in a fraction of the\ntime it would take to develop these operations manually. To this end, SQLAlchemy refers to itself as a toolkit, to emphasize the role of the developer\nas the designer/builder of all relational structures and linkages between those structures and the\napplication, not as a passive consumer of decisions made by a library. By exposing relational\nconcepts, SQLAlchemy embraces the idea of \u201cleaky abstraction\u201d, encouraging the developer to tailor\na custom, yet fully automated, interaction layer between the application and the relational database. SQLAlchemy\u2019s innovation is the extent to which it allows a high degree of automation with little to\nno sacri\ufb01ce in control over the relational database. 20.2\nThe Core/ORM Dichotomy\nCentral to SQLAlchemy\u2019s goal of providing a toolkit approach is that it exposes every layer of\ndatabase interaction as a rich API, dividing the task into two main categories known as Core and\nORM. The Core includes Python Database API (DBAPI) interaction, rendering of textual SQL\nstatements understood by the database, and schema management. These features are all presented\nas public APIs. The ORM, or object-relational mapper, is then a speci\ufb01c library built on top of\nthe Core. The ORM provided with SQLAlchemy is only one of any number of possible object\nabstraction layers that could be built upon the Core, and many developers and organizations build\ntheir applications on top of the Core directly.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 306", "position": 306, "chunk_type": "semantic", "token_estimate": 418}
{"text": "just on what data is desired, but also on the structure of information. If this utility is concealed,: The ORM, or object-relational mapper, is then a speci\ufb01c library built on top of\nthe Core. The ORM provided with SQLAlchemy is only one of any number of possible object\nabstraction layers that could be built upon the Core, and many developers and organizations build\ntheir applications on top of the Core directly. The Core/ORM separation has always been SQLAlchemy\u2019s most de\ufb01ning feature, and it has both\npros and cons. The explicit Core present in SQLAlchemy leads the ORM to relate database-mapped\nclass attributes to a structure known as a Table, rather than directly to their string column names as\nexpressed in the database; to produce a SELECT query using a structure called select, rather than\npiecing together object attributes directly into a string statement; and to receive result rows through a\nfacade called ResultProxy, which transparently maps the select to each result row, rather than\ntransferring data directly from a database cursor to a user-de\ufb01ned object. Core elements may not be visible in a very simple ORM-centric application. However, as the\nCore is carefully integrated into the ORM to allow \ufb02uid transition between ORM and Core constructs,\na more complex ORM-centric application can \u201cmove down\u201d a level or two in order to deal with the\n292\nSQLAlchemy", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 306", "position": 306, "chunk_type": "semantic", "token_estimate": 226}
{"text": "Schema / Types: Engine\nObject Relational Mapper (ORM)\nDatabase\nFigure 20.1: SQLAlchemy layer diagram\ndatabase in a more speci\ufb01c and \ufb01nely tuned manner, as the situation requires. As SQLAlchemy has\nmatured, the Core API has become less explicit in regular use as the ORM continues to provide\nmore sophisticated and comprehensive patterns. However, the availability of the Core was also a\ncontributor to SQLAlchemy\u2019s early success, as it allowed early users to accomplish much more than\nwould have been possible when the ORM was still being developed. The downside to the ORM/Core approach is that instructions must travel through more steps. Python\u2019s traditional C implementation has a signi\ufb01cant overhead penalty for individual function\ncalls, which are the primary cause of slowness in the runtime. Traditional methods of ameliorating\nthis include shortening call chains through rearrangement and inlining, and replacing performance-\ncritical areas with C code. SQLAlchemy has spent many years using both of these methods to\nimprove performance. However, the growing acceptance of the PyPy interpreter for Python may\npromise to squash the remaining performance problems without the need to replace the majority of\nSQLAlchemy\u2019s internals with C code, as PyPy vastly reduces the impact of long call chains through\njust-in-time inlining and compilation. 20.3\nTaming the DBAPI\nAt the base of SQLAlchemy is a system for interacting with the database via the DBAPI. The\nDBAPI itself is not an actual library, only a speci\ufb01cation. Therefore, implementations of the DBAPI\nare available for a particular target database, such as MySQL or PostgreSQL, or alternatively for\nparticular non-DBAPI database adapters, such as ODBC and JDBC. Michael Bayer\n293", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 307", "position": 307, "chunk_type": "semantic", "token_estimate": 267}
{"text": "The DBAPI presents two challenges. The \ufb01rst is to provide an easy-to-use yet full-featured facade: around the DBAPI\u2019s rudimentary usage patterns. The second is to handle the extremely variable\nnature of speci\ufb01c DBAPI implementations as well as the underlying database engines. The Dialect System\nThe interface described by the DBAPI is extremely simple. Its core components are the DBAPI\nmodule itself, the connection object, and the cursor object\u2014a \u201ccursor\u201d in database parlance represents\nthe context of a particular statement and its associated results. A simple interaction with these objects\nto connect and retrieve data from a database is as follows:\nconnection = dbapi.connect(user=\"user\", pw=\"pw\", host=\"host\")\ncursor = connection.cursor()\ncursor.execute(\"select * from user_table where name=? \", (\"jack\",))\nprint \"Columns in result:\", [desc[0] for desc in cursor.description]\nfor row in cursor.fetchall():\nprint \"Row:\", row\ncursor.close()\nconnection.close()\nSQLAlchemy creates a facade around the classical DBAPI conversation. The point of entry\nto this facade is the create_engine call, from which connection and con\ufb01guration information is\nassembled. An instance of Engine is produced as the result. This object then represents the gateway\nto the DBAPI, which itself is never exposed directly. For simple statement executions, Engine o\ufb00ers what\u2019s known as an implicit execution interface. The work of acquiring and closing both a DBAPI connection and cursor are handled behind the\nscenes:\nengine = create_engine(\"postgresql://user:pw&#64;host/dbname\")\nresult = engine.execute(\"select * from table\")\nprint result.fetchall()\nWhen SQLAlchemy 0.2 was introduced the Connection object was added, providing the ability\nto explicitly maintain the scope of the DBAPI connection:\nconn = engine.connect()\nresult = conn.execute(\"select * from table\")\nprint result.fetchall()\nconn.close()\nThe result returned by the execute method of Engine or Connection is called a ResultProxy,\nwhich o\ufb00ers an interface similar to the DBAPI cursor but with richer behavior. The Engine,\nConnection, and ResultProxy correspond to the DBAPI module, an instance of a speci\ufb01c DBAPI\nconnection, and an instance of a speci\ufb01c DBAPI cursor, respectively. Behind the scenes, the Engine references an object called a Dialect. The Dialect is an\nabstract class for which many implementations exist, each one targeted at a speci\ufb01c DBAPI/database\ncombination. A Connection created on behalf of the Engine will refer to this Dialect for all\ndecisions, which may have varied behaviors depending on the target DBAPI and database in use. The Connection, when created, will procure and maintain an actual DBAPI connection from\na repository known as a Pool that\u2019s also associated with the Engine. The Pool is responsible for\n294\nSQLAlchemy", "domains": ["Design Patterns", "Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 308", "position": 308, "chunk_type": "semantic", "token_estimate": 407}
{"text": "ExecutionContext when used with the psycopg2 dialect. The PGDialect class provides behaviors: that are speci\ufb01c to the usage of the PostgreSQL database, such as the ARRAY datatype and schema\ncatalogs; the PGDialect_psycopg2 class then provides behaviors speci\ufb01c to the psycopg2 DBAPI,\nincluding unicode data handlers and server-side cursor behavior. Dialect\nDefaultDialect\nPGDialect_psycopg2\nPGDialect\nExecutionContext\nDefaultExecutionContext\nPGExecutionContext\nPGExecutionContext_psycopg2\nsqlalchemy.dialects.postgresql\nsqlalchemy.engine\n<<uses>>\nFigure 20.3: Simple Dialect/ExecutionContext hierarchy\nA variant on the above pattern presents itself when dealing with a DBAPI that supports multiple\ndatabases. Examples of this include pyodbc, which deals with any number of database backends via\nODBC, and zxjdbc, a Jython-only driver which deals with JDBC. The above relationship is augmented\nby the use of a mixin class from the sqlalchemy.connectors package which provides DBAPI\nbehavior that is common to multiple backends. Figure 20.4 illustrates the common functionality of\nsqlalchemy.connectors.pyodbc shared among pyodbc-speci\ufb01c dialects for MySQL and Microsoft\nSQL Server. The Dialect and ExecutionContext objects provide a means to de\ufb01ne every interaction with\nthe database and DBAPI, including how connection arguments are formatted and how special quirks\nduring statement execution are handled. The Dialect is also a factory for SQL compilation constructs\nthat render SQL correctly for the target database, and type objects which de\ufb01ne how Python data\nshould be marshaled to and from the target DBAPI and database. 20.4\nSchema De\ufb01nition\nWith database connectivity and interactivity established, the next task is to provide for the creation\nand manipulation of backend-agnostic SQL statements. To achieve this, we need to de\ufb01ne \ufb01rst\nhow we will refer to the tables and columns present in a database\u2014the so-called \u201cschema\u201d. Tables\nand columns represent how data is organized, and most SQL statements consist of expressions and\ncommands referring to these structures. 296\nSQLAlchemy", "domains": ["Design Patterns", "Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 310", "position": 310, "chunk_type": "semantic", "token_estimate": 289}
{"text": "Dialect: DefaultDialect\nMSDialect_pyodbc\nMSDialect\nsqlalchemy.dialects.mssql\nsqlalchemy.engine\nPyODBCConnector\nsqlalchemy.connectors\nMySQLDialect\nMySQLDialect_pyodbc\nsqlalchemy.dialects.mysql\nFigure 20.4: Common DBAPI behavior shared among dialect hierarchies\nAn ORM or data access layer needs to provide programmatic access to the SQL language; at the\nbase is a programmatic system of describing tables and columns. This is where SQLAlchemy o\ufb00ers\nthe \ufb01rst strong division of Core and ORM, by o\ufb00ering the Table and Column constructs that describe\nthe structure of the database independently of a user\u2019s model class de\ufb01nition. The rationale behind\nthe division of schema de\ufb01nition from object relational mapping is that the relational schema can be\ndesigned unambiguously in terms of the relational database, including platform-speci\ufb01c details if\nnecessary, without being muddled by object-relational concepts\u2014these remain a separate concern. Being independent of the ORM component also means the schema description system is just as\nuseful for any other kind of object-relational system which may be built on the Core. The Table and Column model falls under the scope of what\u2019s referred to as metadata, o\ufb00ering\na collection object called MetaData to represent a collection of Table objects. The structure is\nderived mostly from Martin Fowler\u2019s description of \u201cMetadata Mapping\u201d in Patterns of Enterprise\nApplication Architecture. Figure 20.5 illustrates some key elements of the sqlalchemy.schema\npackage. Table represents the name and other attributes of an actual table present in a target schema. Its collection of Column objects represents naming and typing information about individual table\ncolumns. A full array of objects describing constraints, indexes, and sequences is provided to \ufb01ll in\nmany more details, some of which impact the behavior of the engine and SQL construction system. In particular, ForeignKeyConstraint is central to determining how two tables should be joined. Table and Column in the schema package are unique versus the rest of the package in that they are\ndual-inheriting, both from the sqlalchemy.schema package and the sqlalchemy.sql.expression\npackage, serving not just as schema-level constructs, but also as core syntactical units in the SQL\nexpression language. This relationship is illustrated in Figure 20.6. In Figure 20.6 we can see that Table and Column inherit from the SQL world as speci\ufb01c forms of\n\u201cthings you can select from\u201d, known as a FromClause, and \u201cthings you can use in a SQL expression\u201d,\nknown as a ColumnElement. Michael Bayer\n297", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 311", "position": 311, "chunk_type": "semantic", "token_estimate": 380}
{"text": "SQLObject would represent these on an ORM-mapped class using a namespace accessed via the .q: attribute; SQLAlchemy named the attribute .c. The .c attribute remains today on Core selectable\nelements, such as those representing tables and select statements. Expression Trees\nA SQLAlchemy SQL expression construct is very much the kind of structure you\u2019d create if you were\nparsing a SQL statement\u2014it\u2019s a parse tree, except the developer creates the parse tree directly, rather\nthan deriving it from a string. The core type of node in this parse tree is called ClauseElement, and\nFigure 20.7 illustrates the relationship of ClauseElement to some key classes. ClauseElement\nFromClause\nColumnClause\nColumnElement\nSelect\nCompoundSelect\nSelectBase\nJoin\nAlias\nTableClause\n_Label\n_BinaryExpression\nClauseList\n_BindParam\n_UnaryExpression\nUpdateBase\nValuesBase\nInsert\nUpdate\nDelete\nFigure 20.7: Basic expression hierarchy\nThrough the use of constructor functions, methods, and overloaded Python operator functions, a\nstructure for a statement like:\nSELECT id FROM user WHERE name = ? might be constructed in Python like:\nfrom sqlalchemy.sql import table, column, select\nuser = table(\u2019user\u2019, column(\u2019id\u2019), column(\u2019name\u2019))\nstmt = select([user.c.id]).where(user.c.name==\u2019ed\u2019)\nThe structure of the above select construct is shown in Figure 20.8. Note the representation of\nthe literal value \u2019ed\u2019 is contained within the _BindParam construct, thus causing it to be rendered\nas a bound parameter marker in the SQL string using a question mark. Michael Bayer\n299", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 313", "position": 313, "chunk_type": "semantic", "token_estimate": 222}
{"text": "Compilation: The central class responsible for rendering SQL expression trees into textual SQL is the Compiled\nclass. This class has two primary subclasses, SQLCompiler and DDLCompiler. SQLCompiler handles\nSQL rendering operations for SELECT, INSERT, UPDATE, and DELETE statements, collectively\nclassi\ufb01ed as DQL (data query language) and DML (data manipulation language), while DDLCompiler\nhandles various CREATE and DROP statements, classi\ufb01ed as DDL (data de\ufb01nition language). There is an additional class hierarchy focused around string representations of types, starting at\nTypeCompiler. Individual dialects then provide their own subclasses of all three compiler types to\nde\ufb01ne SQL language aspects speci\ufb01c to the target database. Figure 20.9 provides an overview of this\nclass hierarchy with respect to the PostgreSQL dialect. Compiled\nDDLCompiler\nSQLCompiler\nPGCompiler\nGenericType\nCompiler\nPGDDLCompiler\nPGTypeCompiler\nTypeCompiler\nsqlalchemy.engine\nsqlalchemy.sql.compiler\nsqlalchemy.dialects.postgresql\nFigure 20.9: Compiler hierarchy, including PostgreSQL-speci\ufb01c implementation\nThe Compiled subclasses de\ufb01ne a series of visit methods, each one referred to by a particular\nsubclass of ClauseElement. A hierarchy of ClauseElement nodes is walked and a statement is\nconstructed by recursively concatenating the string output of each visit function. As this proceeds,\nthe Compiled object maintains state regarding anonymous identi\ufb01er names, bound parameter names,\nand nesting of subqueries, among other things, all of which aim for the production of a string SQL\nstatement as well as a \ufb01nal collection of bound parameters with default values. Figure 20.10 illustrates\nthe process of visit methods resulting in textual units. label_select_columns()\nSELECT   id   FROM  user  WHERE       name      =            ? visit_table()\nvisit_select()\nvisit_binary()\nvisit_column()\nvisit_bind_param()\nFigure 20.10: Call hierarchy of a statement compilation\nMichael Bayer\n301", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 315", "position": 315, "chunk_type": "semantic", "token_estimate": 259}
{"text": "It may appear, above, that the class instrumentation is being achieved directly by our placement of: SQLAlchemy originally followed\nSQLObject\u2019s behavior of using a special attribute as the source of SQL column expressions, referred\nto by SQLAlchemy as .c, as in this example:\nresult = session.query(User).filter(User.c.username == \u2019ed\u2019).all()\nIn version 0.4, however, SQLAlchemy moved the functionality into the mapped attributes them-\nselves:\nresult = session.query(User).filter(User.username == \u2019ed\u2019).all()\nThis change in attribute access proved to be a great improvement, as it allowed the column-\nlike objects present on the class to gain additional class-speci\ufb01c capabilities not present on those\noriginating directly from the underlying Table object. It also allowed usage integration between\ndi\ufb00erent kinds of class attributes, such as attributes which refer to table columns directly, attributes\nthat refer to SQL expressions derived from those columns, and attributes that refer to a related class. Finally, it provided a symmetry between a mapped class, and an instance of that mapped class, in\nthat the same attribute could take on di\ufb00erent behavior depending on the type of parent. Class-bound\nattributes return SQL expressions while instance-bound attributes return actual data. Anatomy of a Mapping\nThe id attribute that\u2019s been attached to our User class is a type of object known in Python as a\ndescriptor, an object that has __get__, __set__, and __del__ methods, which the Python runtime\ndefers to for all class and instance operations involving this attribute. SQLAlchemy\u2019s implementation\nis known as an InstrumentedAttribute, and we\u2019ll illustrate the world behind this facade with\nanother example. Starting with a Table and a user de\ufb01ned class, we set up a mapping that has just\none mapped column, as well as a relationship, which de\ufb01nes a reference to a related class:\nuser_table = Table(\"user\", metadata,\nColumn(\u2019id\u2019, Integer, primary_key=True),\nMichael Bayer\n303", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 317", "position": 317, "chunk_type": "semantic", "token_estimate": 296}
{"text": "): class User(object):\npass\nmapper(User, user_table, properties={\n\u2019related\u2019:relationship(Address)\n})\nWhen the mapping is complete, the structure of objects related to the class is detailed in Fig-\nure 20.11. Mapper\nInstrumented\nAttribute\nScalar\nAttributeImpl\nColumnProperty\nColumnLoader\nTable\nColumn\n__get__()\n__set__()\n__del__()\nRelationship\nProperty\nLazyLoader\nOneToManyDP\nInstrumented\nAttribute\nCollection\nAttributeImpl\n_sa_class_state/\nclass_\n__get__()\n__set__()\n__del__()\nmanager/\nmapper\nmapped_table\n_props\ncolumns\nproperty\nproperty\ncolumns\ntarget\nrelated mapper\nid\nrelated\n(dict)\nsqlalchemy.orm.instrumentation\nsqlalchemy.orm.attributes\nsqlalchemy.orm.mapper\nsqlalchemy.orm.properties\nsqlalchemy.schema\nUser\n(user-de\ufb01ned \nclass)\nClassManager\nsqlalchemy.orm.strategies\nsqlalchemy.orm.dependency\nFigure 20.11: Anatomy of a mapping\nThe \ufb01gure illustrates a SQLAlchemy mapping de\ufb01ned as two separate layers of interaction\nbetween the user-de\ufb01ned class and the table metadata to which it is mapped. Class instrumentation\nis pictured towards the left, while SQL and database functionality is pictured towards the right. The general pattern at play is that object composition is used to isolate behavioral roles, and object\ninheritance is used to distinguish amongst behavioral variances within a particular role. Within the realm of class instrumentation, the ClassManager is linked to the mapped class,\nwhile its collection of InstrumentedAttribute objects are linked to each attribute mapped on the\nclass. InstrumentedAttribute is also the public-facing Python descriptor mentioned previously,\nand produces SQL expressions when used in a class-based expression (e.g., User.id==5). When\ndealing with an instance of User, InstrumentedAttribute delegates the behavior of the attribute\nto an AttributeImpl object, which is one of several varieties tailored towards the type of data being\nrepresented. 304\nSQLAlchemy", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 318", "position": 318, "chunk_type": "semantic", "token_estimate": 240}
{"text": "Towards the mapping side, the Mapper represents the linkage of a user-de\ufb01ned class and a: selectable unit, most typically Table. Mapper maintains a collection of per-attribute objects known\nas MapperProperty, which deals with the SQL representation of a particular attribute. The most\ncommon variants of MapperProperty are ColumnProperty, representing a mapped column or SQL\nexpression, and RelationshipProperty, representing a linkage to another mapper. MapperProperty delegates attribute loading behavior\u2014including how the attribute renders in a\nSQL statement and how it is populated from a result row\u2014to a LoaderStrategy object, of which\nthere are several varieties. Di\ufb00erent LoaderStrategies determine if the loading behavior of an\nattribute is deferred, eager, or immediate. A default version is chosen at mapper con\ufb01guration\ntime, with the option to use an alternate strategy at query time. RelationshipProperty also\nreferences a DependencyProcessor, which handles how inter-mapper dependencies and attribute\nsynchronization should proceed at \ufb02ush time. The choice of DependencyProcessor is based on the\nrelational geometry of the parent and target selectables linked to the relationship. The Mapper/RelationshipProperty structure forms a graph, where Mapper objects are nodes\nand RelationshipProperty objects are directed edges. Once the full set of mappers have been\ndeclared by an application, a deferred \u201cinitialization\u201d step known as the con\ufb01guration proceeds. It is\nused mainly by each RelationshipProperty to solidify the details between its parent and target\nmappers, including choice of AttributeImpl as well as DependencyProcessor. This graph is a\nkey data structure used throughout the operation of the ORM. It participates in operations such as\nthe so-called \u201ccascade\u201d behavior that de\ufb01nes how operations should propagate along object paths, in\nquery operations where related objects and collections are \u201ceagerly\u201d loaded at once, as well as on\nthe object \ufb02ushing side where a dependency graph of all objects is established before \ufb01ring o\ufb00a\nseries of persistence steps. 20.7\nQuery and Loading Behavior\nSQLAlchemy initiates all object loading behavior via an object called Query. The basic state Query\nstarts with includes the entities, which is the list of mapped classes and/or individual SQL expressions\nto be queried. It also has a reference to the Session, which represents connectivity to one or more\ndatabases, as well as a cache of data that\u2019s been accumulated with respect to transactions on those\nconnections. Below is a rudimentary usage example:\nfrom sqlalchemy.orm import Session\nsession = Session(engine)\nquery = session.query(User)\nWe create a Query that will yield instances of User, relative to a new Session we\u2019ve created.", "domains": ["Design Patterns", "Design Principles"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 319", "position": 319, "chunk_type": "semantic", "token_estimate": 405}
{"text": "Towards the mapping side, the Mapper represents the linkage of a user-de\ufb01ned class and a: It also has a reference to the Session, which represents connectivity to one or more\ndatabases, as well as a cache of data that\u2019s been accumulated with respect to transactions on those\nconnections. Below is a rudimentary usage example:\nfrom sqlalchemy.orm import Session\nsession = Session(engine)\nquery = session.query(User)\nWe create a Query that will yield instances of User, relative to a new Session we\u2019ve created. Query provides a generative builder pattern in the same way as the select construct discussed\npreviously, where additional criteria and modi\ufb01ers are associated with a statement construct one\nmethod call at a time. When an iterative operation is called on the Query, it constructs a SQL\nexpression construct representing a SELECT, emits it to the database, and then interprets the result\nset rows as ORM-oriented results corresponding to the initial set of entities being requested. Query makes a hard distinction between the SQL rendering and the data loading portions of the\noperation. The former refers to the construction of a SELECT statement, the latter to the interpretation\nof SQL result rows into ORM-mapped constructs. Data loading can, in fact, proceed without a SQL\nrendering step, as the Query may be asked to interpret results from a textual query hand-composed\nby the user. Michael Bayer\n305", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 319", "position": 319, "chunk_type": "semantic", "token_estimate": 226}
{"text": "Both SQL rendering and data loading utilize a recursive descent through the graph formed by the: series of lead Mapper objects, considering each column- or SQL-expression-holding ColumnProperty\nas a leaf node and each RelationshipProperty which is to be included in the query via a so-called\n\u201ceager-load\u201d as an edge leading to another Mapper node. The traversal and action to take at each\nnode is ultimately the job of each LoaderStrategy associated with every MapperProperty, adding\ncolumns and joins to the SELECT statement being built in the SQL rendering phase, and producing\nPython functions that process result rows in the data loading phase. The Python functions produced in the data loading phase each receive a database row as they\nare fetched, and produce a possible change in the state of a mapped attribute in memory as a result. They are produced for a particular attribute conditionally, based on examination of the \ufb01rst incoming\nrow in the result set, as well as on loading options. If a load of the attribute is not to proceed, no\ncallable function is produced. Figure 20.12 illustrates the traversal of several LoaderStrategy objects in a joined eager load-\ning scenario, illustrating their connection to a rendered SQL statement which occurs during the\n_compile_context method of Query. It also shows generation of row population functions which\nreceive result rows and populate individual object attributes, a process which occurs within the\ninstances method of Query.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 320", "position": 320, "chunk_type": "semantic", "token_estimate": 237}
{"text": "SQLAlchemy\u2019s early approach to populating results used a traditional traversal of \ufb01xed object: methods associated with each strategy to receive each row and act accordingly. The loader callable\nsystem, \ufb01rst introduced in version 0.5, represented a dramatic leap in performance, as many decisions\nregarding row handling could be made just once up front instead of for each row, and a signi\ufb01cant\nnumber of function calls with no net e\ufb00ect could be eliminated. 20.8\nSession/Identity Map\nIn SQLAlchemy, the Session object presents the public interface for the actual usage of the ORM\u2014\nthat is, loading and persisting data. It provides the starting point for queries and persistence operations\nfor a given database connection. The Session, in addition to serving as the gateway for database connectivity, maintains an active\nreference to the set of all mapped entities which are present in memory relative to that Session. It\u2019s\nin this way that the Session implements a facade for the identity map and unit of work patterns,\nboth identi\ufb01ed by Fowler. The identity map maintains a database-identity-unique mapping of all\nobjects for a particular Session, eliminating the problems introduced by duplicate identities. The\nunit of work builds on the identity map to provide a system of automating the process of persisting\nall changes in state to the database in the most e\ufb00ective manner possible. The actual persistence step\nis known as a \u201c\ufb02ush\u201d, and in modern SQLAlchemy this step is usually automatic. Development History\nThe Session started out as a mostly concealed system responsible for the single task of emitting a\n\ufb02ush. The \ufb02ush process involves emitting SQL statements to the database, corresponding to changes\nin the state of objects tracked by the unit of work system and thereby synchronizing the current\nstate of the database with what\u2019s in memory. The \ufb02ush has always been one of the most complex\noperations performed by SQLAlchemy. The invocation of \ufb02ush started out in very early versions behind a method called commit, and\nit was a method present on an implicit, thread-local object called objectstore. When one used\nSQLAlchemy 0.1, there was no need to call Session.add, nor was there any concept of an explicit\nSession at all. The only user-facing steps were to create mappers, create new objects, modify\nexisting objects loaded through queries (where the queries themselves were invoked directly from\neach Mapper object), and then persist all changes via the objectstore.commit command. The pool\nof objects for a set of operations was unconditionally module-global and unconditionally thread-local.", "domains": ["Design Patterns", "Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 321", "position": 321, "chunk_type": "semantic", "token_estimate": 413}
{"text": "SQLAlchemy\u2019s early approach to populating results used a traditional traversal of \ufb01xed object: The only user-facing steps were to create mappers, create new objects, modify\nexisting objects loaded through queries (where the queries themselves were invoked directly from\neach Mapper object), and then persist all changes via the objectstore.commit command. The pool\nof objects for a set of operations was unconditionally module-global and unconditionally thread-local. The objectstore.commit model was an immediate hit with the \ufb01rst group of users, but the\nrigidity of this model quickly ran into a wall. Users new to modern SQLAlchemy sometimes lament\nthe need to de\ufb01ne a factory, and possibly a registry, for Session objects, as well as the need to keep\ntheir objects organized into just one Session at a time, but this is far preferable to the early days\nwhen the entire system was completely implicit. The convenience of the 0.1 usage pattern is still\nlargely present in modern SQLAlchemy, which features a session registry normally con\ufb01gured to\nuse thread local scoping. The Session itself was only introduced in version 0.2 of SQLAlchemy, modeled loosely after\nthe Session object present in Hibernate. This version featured integrated transactional control,\nwhere the Session could be placed into a transaction via the begin method, and completed via\nthe commit method. The objectstore.commit method was renamed to objectstore.flush, and\nnew Session objects could be created at any time. The Session itself was broken o\ufb00from another\nMichael Bayer\n307", "domains": ["Design Patterns", "Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 321", "position": 321, "chunk_type": "semantic", "token_estimate": 240}
{"text": "object called UnitOfWork, which remains as a private object responsible for executing the actual: \ufb02ush operation. While the \ufb02ush process started as a method explicitly invoked by the user, the 0.4 series of\nSQLAlchemy introduced the concept of auto\ufb02ush, which meant that a \ufb02ush was emitted immediately\nbefore each query. The advantage of auto\ufb02ush is that the SQL statement emitted by a query always\nhas access on the relational side to the exact state that is present in memory, as all changes have been\nsent over. Early versions of SQLAlchemy couldn\u2019t include this feature, because the most common\npattern of usage was that the \ufb02ush statement would also commit the changes permanently. But when\nauto\ufb02ush was introduced, it was accompanied by another feature called the transactional Session,\nwhich provided a Session that would start out automatically in a transaction that remained until\nthe user called commit explicitly. With the introduction of this feature, the flush method no longer\ncommitted the data that it \ufb02ushed, and could safely be called on an automated basis. The Session\ncould now provide a step-by-step synchronization between in-memory state and SQL query state by\n\ufb02ushing as needed, with nothing permanently persisted until the explicit commit step. This behavior\nis, in fact, exactly the same in Hibernate for Java. However, SQLAlchemy embraced this style of\nusage based on the same behavior in the Storm ORM for Python, introduced when SQLAlchemy\nwas in version 0.3. Version 0.5 brought more transaction integration when post-transaction expiration was intro-\nduced; after each commit or rollback, by default all states within the Session are expired (erased),\nto be populated again when subsequent SQL statements re-select the data, or when the attributes on\nthe remaining set of expired objects are accessed in the context of the new transaction. Originally,\nSQLAlchemy was constructed around the assumption that SELECT statements should be emitted as\nlittle as possible, unconditionally. The expire-on-commit behavior was slow in coming for this reason;\nhowever, it entirely solved the issue of the Session which contained stale data post-transaction with\nno simple way to load newer data without rebuilding the full set of objects already loaded. Early on,\nit seemed that this problem couldn\u2019t be reasonably solved, as it wasn\u2019t apparent when the Session\nshould consider the current state to be stale, and thus produce an expensive new set of SELECT\nstatements on the next access.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 322", "position": 322, "chunk_type": "semantic", "token_estimate": 395}
{"text": "object called UnitOfWork, which remains as a private object responsible for executing the actual: The expire-on-commit behavior was slow in coming for this reason;\nhowever, it entirely solved the issue of the Session which contained stale data post-transaction with\nno simple way to load newer data without rebuilding the full set of objects already loaded. Early on,\nit seemed that this problem couldn\u2019t be reasonably solved, as it wasn\u2019t apparent when the Session\nshould consider the current state to be stale, and thus produce an expensive new set of SELECT\nstatements on the next access. However, once the Session moved to an always-in-a-transaction\nmodel, the point of transaction end became apparent as the natural point of data expiration, as\nthe nature of a transaction with a high degree of isolation is that it cannot see new data until it\u2019s\ncommitted or rolled back anyway. Di\ufb00erent databases and con\ufb01gurations, of course, have varied\ndegrees of transaction isolation, including no transactions at all. These modes of usage are entirely\nacceptable with SQLAlchemy\u2019s expiration model; the developer only needs to be aware that a lower\nisolation level may expose un-isolated changes within a Session if multiple Sessions share the same\nrows. This is not at all di\ufb00erent from what can occur when using two database connections directly. Session Overview\nFigure 20.13 illustrates a Session and the primary structures it deals with. The public-facing portions above are the Session itself and the collection of user objects, each\nof which is an instance of a mapped class. Here we see that mapped objects keep a reference to a\nSQLAlchemy construct called InstanceState, which tracks ORM state for an individual instance\nincluding pending attribute changes and attribute expiration status. InstanceState is the instance-\nlevel side of the attribute instrumentation discussed in the preceding section, Anatomy of a Mapping,\ncorresponding to the ClassManager at the class level, and maintaining the state of the mapped\nobject\u2019s dictionary (i.e., the Python __dict__ attribute) on behalf of the AttributeImpl objects\nassociated with the class. 308\nSQLAlchemy", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 322", "position": 322, "chunk_type": "semantic", "token_estimate": 336}
{"text": "Session: IdentityMap\n(dict of key->state)\nSessionTransaction\nConnection\ndatabase\nvia", "domains": ["Design Patterns", "Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 323", "position": 323, "chunk_type": "semantic", "token_estimate": 9}
{"text": "InstanceState: (persistent)\nuser object\nInstanceState\n(persistent+ \ndeleted)\nInstanceState\n(pending)\nInstanceState\n(transient)\nuser object\nuser object\nuser object\nnew\ndeleted\nidentity_map\ntransaction\n1..n\nweakref\nweakref\nweakref\nweakref\nmodi\ufb01ed\nFigure 20.13: Session overview\nState Tracking\nThe IdentityMap is a mapping of database identities to InstanceState objects, for those objects\nwhich have a database identity, which are referred to as persistent. The default implementation\nof IdentityMap works with InstanceState to self-manage its size by removing user-mapped\ninstances once all strong references to them have been removed\u2014in this way it works in the same\nway as Python\u2019s WeakValueDictionary. The Session protects the set of all objects marked as\ndirty or deleted, as well as pending objects marked new, from garbage collection, by creating strong\nreferences to those objects with pending changes. All strong references are then discarded after the\n\ufb02ush. InstanceState also performs the critical task of maintaining \u201cwhat\u2019s changed\u201d for the attributes\nof a particular object, using a move-on-change system that stores the \u201cprevious\u201d value of a particular\nattribute in a dictionary called committed_state before assigning the incoming value to the object\u2019s\ncurrent dictionary. At \ufb02ush time, the contents of committed_state and the __dict__ associated\nwith the object are compared to produce the set of net changes on each object. In the case of collections,\na separate collections package coordinates with the\nInstrumentedAttribute/InstanceState system to maintain a collection of net changes to a par-\nticular mapped collection of objects. Common Python classes such as set, list and dict are\nsubclassed before use and augmented with history-tracking mutator methods. The collection system\nwas reworked in 0.4 to be open ended and usable for any collection-like object. Transactional Control\nSession, in its default state of usage, maintains an open transaction for all operations which is\ncompleted when commit or rollback is called. The SessionTransaction maintains a set of\nzero or more Connection objects, each representing an open transaction on a particular database. SessionTransaction is a lazy-initializing object that begins with no database state present. As a\nparticular backend is required to participate in a statement execution, a Connection corresponding\nto that database is added to SessionTransaction\u2019s list of connections. While a single connection\nat a time is common, the multiple connection scenario is supported where the speci\ufb01c connection\nused for a particular operation is determined based on con\ufb01gurations associated with the Table,\nMichael Bayer\n309", "domains": ["Design Patterns", "Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 323", "position": 323, "chunk_type": "semantic", "token_estimate": 390}
{"text": "Mapper, or SQL construct itself involved in the operation. Multiple connections can also coordinate: the transaction using two-phase behavior, for those DBAPIs which provide it. 20.9\nUnit of Work\nThe flush method provided by Session turns over its work to a separate module called unitofwork. As mentioned earlier, the \ufb02ush process is probably the most complex function of SQLAlchemy. The job of the unit of work is to move all of the pending state present in a particular Session out\nto the database, emptying out the new, dirty, and deleted collections maintained by the Session. Once completed, the in-memory state of the Session and what\u2019s present in the current transaction\nmatch. The primary challenge is to determine the correct series of persistence steps, and then to\nperform them in the correct order. This includes determining the list of INSERT, UPDATE, and\nDELETE statements, including those resulting from the cascade of a related row being deleted or\notherwise moved; ensuring that UPDATE statements contain only those columns which were actually\nmodi\ufb01ed; establishing \u201csynchronization\u201d operations that will copy the state of primary key columns\nover to referencing foreign key columns, at the point at which newly generated primary key identi\ufb01ers\nare available; ensuring that INSERTs occur in the order in which objects were added to the Session\nand as e\ufb03ciently as possible; and ensuring that UPDATE and DELETE statements occur within a\ndeterministic ordering so as to reduce the chance of deadlocks. History\nThe unit of work implementation began as a tangled system of structures that was written in an ad hoc\nway; its development can be compared to \ufb01nding the way out of a forest without a map. Early bugs\nand missing behaviors were solved with bolted-on \ufb01xes, and while several refactorings improved\nmatters through version 0.5, it was not until version 0.6 that the unit of work\u2014by that time stable,\nwell-understood, and covered by hundreds of tests\u2014could be rewritten entirely from scratch. After\nmany weeks of considering a new approach that would be driven by consistent data structures, the\nprocess of rewriting it to use this new model took only a few days, as the idea was by this time well\nunderstood. It was also greatly helped by the fact that the new implementation\u2019s behavior could be\ncarefully cross-checked against the existing version. This process shows how the \ufb01rst iteration of\nsomething, however awful, is still valuable as long as it provides a working model.", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 324", "position": 324, "chunk_type": "semantic", "token_estimate": 405}
{"text": "Mapper, or SQL construct itself involved in the operation. Multiple connections can also coordinate: It was also greatly helped by the fact that the new implementation\u2019s behavior could be\ncarefully cross-checked against the existing version. This process shows how the \ufb01rst iteration of\nsomething, however awful, is still valuable as long as it provides a working model. It further shows\nhow total rewrites of a subsystem is often not only appropriate, but an integral part of development\nfor hard-to-develop systems. Topological Sort\nThe key paradigm behind the unit of work is that of assembling the full list of actions to be taken into\na data structure, with each node representing a single step; this is known in design patterns parlance\nas the command pattern. The series of \u201ccommands\u201d within this structure is then organized into a\nspeci\ufb01c ordering using a topological sort. A topological sort is a process that sorts items based on\na partial ordering, that is, only certain elements must precede others. Figure 20.14 illustrates the\nbehavior of the topological sort. The unit of work constructs a partial ordering based on those persistence commands which must\nprecede others. The commands are then topologically sorted and invoked in order. The determination\nof which commands precede which is derived primarily from the presence of a relationship that\n310\nSQLAlchemy", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 324", "position": 324, "chunk_type": "semantic", "token_estimate": 219}
{"text": "SaveUpdateState: INSERT INTO user\nSaveUpdateState\nINSERT INTO user\nProcessState\n(User->User)\ncopy user.id to \nuser.contact_id\nProcessAll\n(User->Address)\ncopy user.id to \naddress.user_id\ncopy user.id to \naddress.user_id\nSaveUpdateAll\n(Address)\nINSERT INTO address\nINSERT INTO address\nFigure 20.16: Organizing reference cycles into individual steps\n20.10\nConclusion\nSQLAlchemy has aimed very high since its inception, with the goal of being the most feature-rich and\nversatile database product possible. It has done so while maintaining its focus on relational databases,\nrecognizing that supporting the usefulness of relational databases in a deep and comprehensive way\nis a major undertaking; and even now, the scope of the undertaking continues to reveal itself as larger\nthan previously perceived. The component-based approach is intended to extract the most value possible from each area of\nfunctionality, providing many di\ufb00erent units that applications can use alone or in combination. This\nsystem has been challenging to create, maintain, and deliver. The development course was intended to be slow, based on the theory that a methodical, broad-\nbased construction of solid functionality is ultimately more valuable than fast delivery of features\nwithout foundation. It has taken a long time for SQLAlchemy to construct a consistent and well-\ndocumented user story, but throughout the process, the underlying architecture was always a step\nahead, leading in some cases to the \u201ctime machine\u201d e\ufb00ect where features can be added almost before\nusers request them. Michael Bayer\n313", "domains": ["Design Patterns", "Design Principles", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 327", "position": 327, "chunk_type": "semantic", "token_estimate": 230}
{"text": "The Python language has been a reliable host (if a little \ufb01nicky, particularly in the area of: performance). The language\u2019s consistency and tremendously open run-time model has allowed\nSQLAlchemy to provide a nicer experience than that o\ufb00ered by similar products written in other\nlanguages. It is the hope of the SQLAlchemy project that Python gain ever-deeper acceptance into as wide a\nvariety of \ufb01elds and industries as possible, and that the use of relational databases remains vibrant\nand progressive. The goal of SQLAlchemy is to demonstrate that relational databases, Python, and\nwell-considered object models are all very much worthwhile development tools. 314\nSQLAlchemy", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 328", "position": 328, "chunk_type": "semantic", "token_estimate": 103}
{"text": "[chapter21]: Twisted\nJessica McKellar\nTwisted is an event-driven networking engine in Python. It was born in the early 2000s, when the\nwriters of networked games had few scalable and no cross-platform libraries, in any language, at\ntheir disposal. The authors of Twisted tried to develop games in the existing networking landscape,\nstruggled, saw a clear need for a scalable, event-driven, cross-platform networking framework and\ndecided to make one happen, learning from the mistakes and hardships of past game and networked\napplication writers. Twisted supports many common transport and application layer protocols, including TCP, UDP,\nSSL/TLS, HTTP, IMAP, SSH, IRC, and FTP. Like the language in which it is written, it is \u201cbatteries-\nincluded\u201d; Twisted comes with client and server implementations for all of its protocols, as well as\nutilities that make it easy to con\ufb01gure and deploy production-grade Twisted applications from the\ncommand line. 21.1\nWhy Twisted? In 2000, glyph, the creator of Twisted, was working on a text-based multiplayer game called Twisted\nReality. It was a big mess of threads, 3 per connection, in Java. There was a thread for input that\nwould block on reads, a thread for output that would block on some kind of write, and a \u201clogic\u201d thread\nthat would sleep while waiting for timers to expire or events to queue. As players moved through the\nvirtual landscape and interacted, threads were deadlocking, caches were getting corrupted, and the\nlocking logic was never quite right\u2014the use of threads had made the software complicated, buggy,\nand hard to scale. Seeking alternatives, he discovered Python, and in particular Python\u2019s select module for multi-\nplexing I/O from stream objects like sockets and pipes1; at the time, Java didn\u2019t expose the operating\nsystem\u2019s select interface or any other asynchronous I/O API2. A quick prototype of the game in\nPython using select immediately proved less complex and more reliable than the threaded version. An instant convert to Python, select, and event-driven programming, glyph wrote a client and\nserver for the game in Python using the select API. But then he wanted to do more. Fundamentally,\nhe wanted to be able to turn network activity into method calls on objects in the game. What if you\ncould receive email in the game, like the Nethack mailer daemon? What if every player in the game\n1The Single UNIX Speci\ufb01cation, Version 3 (SUSv3) describes the select API.", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 329", "position": 329, "chunk_type": "semantic", "token_estimate": 394}
{"text": "[chapter21]: What if you\ncould receive email in the game, like the Nethack mailer daemon? What if every player in the game\n1The Single UNIX Speci\ufb01cation, Version 3 (SUSv3) describes the select API. 2The java.nio package for non-blocking I/O was added in J2SE 1.4, released in 2002.", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 329", "position": 329, "chunk_type": "semantic", "token_estimate": 47}
{"text": "had a home page? Glyph found himself needing good Python IMAP and HTTP clients and servers: that used select. He \ufb01rst turned to Medusa, a platform developed in the mid-\u201990s for writing networking servers\nin Python based on the asyncore module3. asyncore is an asynchronous socket handler that builds\na dispatcher and callback interface on top of the operating system\u2019s select API. This was an inspiring \ufb01nd for glyph, but Medusa had two drawbacks:\n1. It was on its way towards being unmaintained by 2001 when glyph started working on Twisted\nReality. 2. asyncore is such a thin wrapper around sockets that application programmers are still re-\nquired to manipulate sockets directly. This means portability is still the responsibility of the\nprogrammer. Additionally, at the time, asyncore\u2019s Windows support was buggy, and glyph\nknew that he wanted to run a GUI client on Windows. Glyph was facing the prospect of implementing a networking platform himself and realized that\nTwisted Reality had opened the door to a problem that was just as interesting as his game. Over time, Twisted Reality the game became Twisted the networking platform, which would do\nwhat existing networking platforms in Python didn\u2019t:\n\u2022 Use event-driven programming instead of multi-threaded programming. \u2022 Be cross-platform: provide a uniform interface to the event noti\ufb01cation systems exposed by\nmajor operating systems. \u2022 Be \u201cbatteries-included\u201d: provide implementations of popular application-layer protocols out\nof the box, so that Twisted is immediately useful to developers. \u2022 Conform to RFCs, and prove conformance with a robust test suite. \u2022 Make it easy to use multiple networking protocols together. \u2022 Be extensible. 21.2\nThe Architecture of Twisted\nTwisted is an event-driven networking engine. Event-driven programming is so integral to Twisted\u2019s\ndesign philosophy that it is worth taking a moment to review what exactly event-driven programming\nmeans. Event-driven programming is a programming paradigm in which program \ufb02ow is determined by\nexternal events. It is characterized by an event loop and the use of callbacks to trigger actions when\nevents happen. Two other common programming paradigms are (single-threaded) synchronous and\nmulti-threaded programming. Let\u2019s compare and contrast single-threaded, multi-threaded, and event-driven programming\nmodels with an example. Figure 21.1 shows the work done by a program over time under these three\nmodels. The program has three tasks to complete, each of which blocks while waiting for I/O to\n\ufb01nish. Time spent blocking on I/O is greyed out. In the single-threaded synchronous version of the program, tasks are performed serially.", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 330", "position": 330, "chunk_type": "semantic", "token_estimate": 410}
{"text": "Figure 21.1: Threading models: concurrently on multiple processors or interleaved on a single processor. This allows progress to\nbe made by some threads while others are blocking on resources. This is often more time-e\ufb03cient\nthan the analogous synchronous program, but one has to write code to protect shared resources that\ncould be accessed concurrently from multiple threads. Multi-threaded programs can be harder to\nreason about because one now has to worry about thread safety via process serialization (locking),\nreentrancy, thread-local storage, or other mechanisms, which when implemented improperly can\nlead to subtle and painful bugs. The event-driven version of the program interleaves the execution of the three tasks, but in a\nsingle thread of control. When performing I/O or other expensive operations, a callback is registered\nwith an event loop, and then execution continues while the I/O completes. The callback describes\nhow to handle an event once it has completed. The event loop polls for events and dispatches them\nas they arrive, to the callbacks that are waiting for them. This allows the program to make progress\nwhen it can without the use of additional threads. Event-driven programs can be easier to reason\nabout than multi-threaded programs because the programmer doesn\u2019t have to worry about thread\nsafety. Jessica McKellar\n317", "domains": ["Architectural Patterns and Styles"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 331", "position": 331, "chunk_type": "semantic", "token_estimate": 210}
{"text": "The event-driven model is often a good choice when there are:: 1. many tasks, that are...\n2. largely independent (so they don\u2019t have to communicate with or wait on each other), and...\n3. some of these tasks block while waiting on events. It is also a good choice when an application has to share mutable data between tasks, because no\nsynchronization has to be performed. Networking applications often have exactly these properties, which is what makes them such a\ngood \ufb01t for the event-driven programming model. Reusing Existing Applications\nMany popular clients and servers for various networking protocols already existed when Twisted was\ncreated. Why did glyph not just use Apache, IRCd, BIND, OpenSSH, or any of the other pre-existing\napplications whose clients and servers would have to get re-implemented from scratch for Twisted? The problem is that all of these server implementations have networking code written from scratch,\ntypically in C, with application code coupled directly to the networking layer. This makes them very\ndi\ufb03cult to use as libraries. They have to be treated as black boxes when used together, giving a\ndeveloper no chance to reuse code if he or she wanted to expose the same data over multiple protocols. Additionally, the server and client implementations are often separate applications that don\u2019t share\ncode. Extending these applications and maintaining cross-platform client-server compatibility is\nharder than it needs to be. With Twisted, the clients and servers are written in Python using a consistent interface. This\nmakes it is easy to write new clients and servers, to share code between clients and servers, to share\napplication logic between protocols, and to test one\u2019s code. The Reactor\nTwisted implements the reactor design pattern, which describes demultiplexing and dispatching\nevents from multiple sources to their handlers in a single-threaded environment. The core of Twisted is the reactor event loop. The reactor knows about network, \ufb01le system, and\ntimer events. It waits on and then handles these events, abstracting away platform-speci\ufb01c behavior\nand presenting interfaces to make responding to events anywhere in the network stack easy. The reactor essentially accomplishes:\nwhile True:\ntimeout = time_until_next_timed_event()\nevents = wait_for_events(timeout)\nevents += timed_events_until(now())\nfor event in events:\nevent.process()\nA reactor based on the poll API4 is the current default on all platforms. Twisted additionally\nsupports a number of platform-speci\ufb01c high-volume multiplexing APIs.", "domains": ["Architectural Patterns and Styles", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 332", "position": 332, "chunk_type": "semantic", "token_estimate": 387}
{"text": "The event-driven model is often a good choice when there are:: The reactor essentially accomplishes:\nwhile True:\ntimeout = time_until_next_timed_event()\nevents = wait_for_events(timeout)\nevents += timed_events_until(now())\nfor event in events:\nevent.process()\nA reactor based on the poll API4 is the current default on all platforms. Twisted additionally\nsupports a number of platform-speci\ufb01c high-volume multiplexing APIs. Platform-speci\ufb01c reactors\ninclude the KQueue reactor based on FreeBSD\u2019s kqueue mechanism, an epoll-based reactor for\nsystems supporting the epoll interface (currently Linux 2.6), and an IOCP reactor based on Windows\nInput/Output Completion Ports. Examples of polling implementation-dependent details that Twisted takes care of include:\n4The Single UNIX Speci\ufb01cation, Version 3 (SUSv3) describes the poll API. 318\nTwisted", "domains": ["Architectural Patterns and Styles", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 332", "position": 332, "chunk_type": "semantic", "token_estimate": 112}
{"text": "\u2022 Network and \ufb01lesystem limits.: \u2022 Bu\ufb00ering behavior. \u2022 How to detect a dropped connection. \u2022 The values returned in error cases. Twisted\u2019s reactor implementation also takes care of using the underlying non-blocking APIs\ncorrectly and handling obscure edge cases correctly. Python doesn\u2019t expose the IOCP API at all, so\nTwisted maintains its own implementation. Managing Callback Chains\nCallbacks are a fundamental part of event-driven programming and are the way that the reactor\nindicates to an application that events have completed. As event-driven programs grow, handling\nboth the success and error cases for the events in one\u2019s application becomes increasingly complex. Failing to register an appropriate callback can leave a program blocking on event processing that\nwill never happen, and errors might have to propagate up a chain of callbacks from the networking\nstack through the layers of an application. Let\u2019s examine some of the pitfalls of event-driven programs by comparing synchronous and\nasynchronous versions of a toy URL fetching utility in Python-like pseudo-code:\nSynchronous URL fetcher:\nimport getPage\ndef processPage(page):\nprint page\ndef logError(error):\nprint error\ndef finishProcessing(value):\nprint \"Shutting down...\"\nexit(0)\nurl = \"http://google.com\"\ntry:\npage = getPage(url)\nprocessPage(page)\nexcept Error, e:\nlogError(error)\nfinally:\nfinishProcessing()\nAsynchronous URL fetcher:\nfrom twisted.internet import reactor\nimport getPage\ndef processPage(page):\nprint page\nfinishProcessing()\ndef logError(error):\nprint error\nfinishProcessing()\ndef finishProcessing(value):\nprint \"Shutting down...\"\nreactor.stop()\nurl = \"http://google.com\"", "domains": ["Architectural Patterns and Styles"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 333", "position": 333, "chunk_type": "semantic", "token_estimate": 225}
{"text": "Figure 21.2: Callback chains: Deferreds can only be \ufb01red once; attempting to re-\ufb01re them will raise an Exception. This gives\nDeferreds semantics closer to those of the try/except blocks of their synchronous cousins, which\nmakes processing the asynchronous events easier to reason about and avoids subtle bugs caused by\ncallbacks being invoked more or less than once for a single event. Understanding Deferreds is an important part of understanding the \ufb02ow of Twisted programs. However, when using the high-level abstractions Twisted provides for networking protocols, one\noften doesn\u2019t have to use Deferreds directly at all. The Deferred abstraction is powerful and has been borrowed by many other event-driven\nplatforms, including jQuery, Dojo, and Mochikit. Transports\nTransports represent the connection between two endpoints communicating over a network. Trans-\nports are responsible for describing connection details, like being stream- or datagram-oriented, \ufb02ow\ncontrol, and reliability. TCP, UDP, and Unix sockets are examples of transports. They are designed\nto be \u201cminimally functional units that are maximally reusable\u201d and are decoupled from protocol\nimplementations, allowing for many protocols to utilize the same type of transport. Transports\nimplement the ITransport interface, which has the following methods:\nJessica McKellar\n321", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 335", "position": 335, "chunk_type": "semantic", "token_estimate": 194}
{"text": "write: Write some data to the physical connection, in sequence, in a\nnon-blocking fashion. writeSequence\nWrite a list of strings to the physical connection. loseConnection\nWrite all pending data and then close the connection. getPeer\nGet the remote address of this connection. getHost\nGet the address of this side of the connection. Decoupling transports from procotols also makes testing the two layers easier. A mock transport\ncan simply write data to a string for inspection. Protocols\nProcotols describe how to process network events asynchronously. HTTP, DNS, and IMAP are\nexamples of application protocols. Protocols implement the IProtocol interface, which has the\nfollowing methods:\nmakeConnection\nMake a connection to a transport and a server. connectionMade\nCalled when a connection is made. dataReceived\nCalled whenever data is received. connectionLost\nCalled when the connection is shut down. The relationship between the reactor, protocols, and transports is best illustrated with an example. Here are complete implementations of an echo server and client, \ufb01rst the server:\nfrom twisted.internet import protocol, reactor\nclass Echo(protocol.Protocol):\ndef dataReceived(self, data):", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 336", "position": 336, "chunk_type": "semantic", "token_estimate": 171}
{"text": "As soon as any data is received, write it back: self.transport.write(data)\nclass EchoFactory(protocol.Factory):\ndef buildProtocol(self, addr):\nreturn Echo()\nreactor.listenTCP(8000, EchoFactory())\nreactor.run()\nAnd the client:\nfrom twisted.internet import reactor, protocol\nclass EchoClient(protocol.Protocol):\ndef connectionMade(self):\nself.transport.write(\"hello, world!\") def dataReceived(self, data):\nprint \"Server said:\", data\nself.transport.loseConnection()\ndef connectionLost(self, reason):\n322\nTwisted", "domains": ["Design Patterns", "Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 336", "position": 336, "chunk_type": "semantic", "token_estimate": 48}
{"text": "print \"connection lost\": class EchoFactory(protocol.ClientFactory):\ndef buildProtocol(self, addr):\nreturn EchoClient()\ndef clientConnectionFailed(self, connector, reason):\nprint \"Connection failed - goodbye!\" reactor.stop()\ndef clientConnectionLost(self, connector, reason):\nprint \"Connection lost - goodbye!\" reactor.stop()\nreactor.connectTCP(\"localhost\", 8000, EchoFactory())\nreactor.run()\nRunning the server script starts a TCP server listening for connections on port 8000. The server\nuses the Echo protocol, and data is written out over a TCP transport. Running the client makes a TCP\nconnection to the server, echoes the server response, and then terminates the connection and stops\nthe reactor. Factories are used to produce instances of protocols for both sides of the connection. The communication is asynchronous on both sides; connectTCP takes care of registering callbacks\nwith the reactor to get noti\ufb01ed when data is available to read from a socket. Applications\nTwisted is an engine for producing scalable, cross-platform network servers and clients. Making it\neasy to deploy these applications in a standardized fashion in production environments is an important\npart of a platform like this getting wide-scale adoption. To that end, Twisted developed the Twisted application infrastructure, a re-usable and con\ufb01gurable\nway to deploy a Twisted application. It allows a programmer to avoid boilerplate code by hooking an\napplication into existing tools for customizing the way it is run, including daemonization, logging,\nusing a custom reactor, pro\ufb01ling code, and more. The application infrastructure has four main parts: Services, Applications, con\ufb01guration man-\nagement (via TAC \ufb01les and plugins), and the twistd command-line utility. To illustrate this infras-\ntructure, we\u2019ll turn the echo server from the previous section into an Application. Service\nA Service is anything that can be started and stopped and which adheres to the IService interface. Twisted comes with service implementations for TCP, FTP, HTTP, SSH, DNS, and many other\nprotocols. Many Services can register with a single application. The core of the IService interface is:\nstartService\nStart the service. This might include loading con\ufb01guration data,\nsetting up database connections, or listening on a port. stopService\nShut down the service. This might include saving state to disk,\nclosing database connections, or stopping listening on a port. Jessica McKellar\n323", "domains": ["Design Patterns", "Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 337", "position": 337, "chunk_type": "semantic", "token_estimate": 349}
{"text": "Our echo service uses TCP, so we can use Twisted\u2019s default TCPServer implementation of this: IService interface. Application\nAn Application is the top-level service that represents the entire Twisted application. Services register\nthemselves with an Application, and the twistd deployment utility described below searches for\nand runs Applications. We\u2019ll create an echo Application with which the echo Service can register. TAC Files\nWhen managing Twisted applications in a regular Python \ufb01le, the developer is responsible for\nwriting code to start and stop the reactor and to con\ufb01gure the application. Under the Twisted\napplication infrastructure, protocol implementations live in a module, Services using those protocols\nare registered in a Twisted Application Con\ufb01guration (TAC) \ufb01le, and the reactor and con\ufb01guration\nare managed by an external utility. To turn our echo server into an echo application, we can follow a simple algorithm:\n1. Move the Protocol parts of the echo server into their own module. 2. Inside a TAC \ufb01le:\n(a) Create an echo Application. (b) Create an instance of the TCPServer Service which will use our EchoFactory, and\nregister it with the Application. The code for managing the reactor will be taken care of by twistd, discussed below. The\napplication code ends up looking like this:\nThe echo.py \ufb01le:\nfrom twisted.internet import protocol, reactor\nclass Echo(protocol.Protocol):\ndef dataReceived(self, data):\nself.transport.write(data)\nclass EchoFactory(protocol.Factory):\ndef buildProtocol(self, addr):\nreturn Echo()\nThe echo_server.tac \ufb01le:\nfrom twisted.application import internet, service\nfrom echo import EchoFactory\napplication = service.Application(\"echo\")\nechoService = internet.TCPServer(8000, EchoFactory())\nechoService.setServiceParent(application)\n324\nTwisted", "domains": ["Design Patterns", "Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 338", "position": 338, "chunk_type": "semantic", "token_estimate": 247}
{"text": "twistd: twistd (pronounced \u201ctwist-dee\u201d) is a cross-platform utility for deploying Twisted applications. It\nruns TAC \ufb01les and handles starting and stopping an application. As part of Twisted\u2019s batteries-\nincluded approach to network programming, twistd comes with a number of useful con\ufb01guration\n\ufb02ags, including daemonizing the application, the location of log \ufb01les, dropping privileges, running\nin a chroot, running under a non-default reactor, or even running the application under a pro\ufb01ler. We can run our echo server Application with:\n$ twistd -y echo_server.tac\nIn this simplest case, twistd starts a daemonized instance of the application, logging to\ntwistd.log. After starting and stopping the application, the log looks like this:\n2011-11-19 22:23:07-0500 [-] Log opened. 2011-11-19 22:23:07-0500 [-] twistd 11.0.0 (/usr/bin/python 2.7.1) starting up. 2011-11-19 22:23:07-0500 [-] reactor class: twisted.internet.selectreactor.SelectReactor. 2011-11-19 22:23:07-0500 [-] echo.EchoFactory starting on 8000\n2011-11-19 22:23:07-0500 [-] Starting factory <echo.EchoFactory instance at 0x12d8670>\n2011-11-19 22:23:20-0500 [-] Received SIGTERM, shutting down. 2011-11-19 22:23:20-0500 [-] (TCP Port 8000 Closed)\n2011-11-19 22:23:20-0500 [-] Stopping factory <echo.EchoFactory instance at 0x12d8670>\n2011-11-19 22:23:20-0500 [-] Main loop terminated. 2011-11-19 22:23:20-0500 [-] Server Shut Down. Running a service using the Twisted application infrastructure allows developers to skip writ-\ning boilerplate code for common service functionalities like logging and daemonization. It also\nestablishes a standard command line interface for deploying applications. Plugins\nAn alternative to the TAC-based system for running Twisted applications is the plugin system. While the TAC system makes it easy to register simple hierarchies of pre-de\ufb01ned services within\nan application con\ufb01guration \ufb01le, the plugin system makes it easy to register custom services as\nsubcommands of the twistd utility, and to extend the command-line interface to an application. Using this system:\n1. Only the plugin API is required to remain stable, which makes it easy for third-party developers\nto extend the software. 2. Plugin discoverability is codi\ufb01ed. Plugins can be loaded and saved when a program is \ufb01rst run,\nre-discovered each time the program starts up, or polled for repeatedly at runtime, allowing\nthe discovery of new plugins installed after the program has started. To extend a program using the Twisted plugin system, all one has to do is create objects which\nimplement the IPlugin interface and put them in a particular location where the plugin system\nknows to look for them. Having already converted our echo server to a Twisted application, transformation into a Twisted\nplugin is straightforward.", "domains": ["Design Patterns", "Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 339", "position": 339, "chunk_type": "semantic", "token_estimate": 394}
{"text": "twistd: To extend a program using the Twisted plugin system, all one has to do is create objects which\nimplement the IPlugin interface and put them in a particular location where the plugin system\nknows to look for them. Having already converted our echo server to a Twisted application, transformation into a Twisted\nplugin is straightforward. Alongside the echo module from before, which contains the Echo protocol\nand EchoFactory de\ufb01nitions, we add a directory called twisted, containing a subdirectory called\nplugins, containing our echo plugin de\ufb01nition. This plugin will allow us to start an echo server and\nspecify the port to use as arguments to the twistd utility:\nJessica McKellar\n325", "domains": ["Design Patterns", "Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 339", "position": 339, "chunk_type": "semantic", "token_estimate": 111}
{"text": "from zope.interface import implements: from twisted.python import usage\nfrom twisted.plugin import IPlugin\nfrom twisted.application.service import IServiceMaker\nfrom twisted.application import internet\nfrom echo import EchoFactory\nclass Options(usage.Options):\noptParameters = [[\"port\", \"p\", 8000, \"The port number to listen on.\"]] class EchoServiceMaker(object):\nimplements(IServiceMaker, IPlugin)\ntapname = \"echo\"\ndescription = \"A TCP-based echo server.\" options = Options\ndef makeService(self, options):\n\"\"\"\nConstruct a TCPServer from a factory defined in myproject.\n\"\"\" return internet.TCPServer(int(options[\"port\"]), EchoFactory())\nserviceMaker = EchoServiceMaker()\nOur echo server will now show up as a server option in the output of twistd --help, and running\ntwistd echo --port=1235 will start an echo server on port 1235. Twisted comes with a pluggable authentication system for servers called twisted.cred, and a\ncommon use of the plugin system is to add an authentication pattern to an application. One can use\ntwisted.cred AuthOptionMixin to add command-line support for various kinds of authentication\no\ufb00the shelf, or to add a new kind. For example, one could add authentication via a local Unix\npassword database or an LDAP server using the plugin system. twistd comes with plugins for many of Twisted\u2019s supported protocols, which turns the work of\nspinning up a server into a single command. Here are some examples of twistd servers that ship\nwith Twisted:\ntwistd web --port 8080 --path . Run an HTTP server on port 8080, serving both static and dynamic content out of the current\nworking directory. twistd dns -p 5553 --hosts-file=hosts\nRun a DNS server on port 5553, resolving domains out of a \ufb01le called hosts in the format of\n/etc/hosts. sudo twistd conch -p tcp:2222\nRun an ssh server on port 2222. ssh keys must be set up independently. twistd mail -E -H localhost -d localhost=emails\nRun an ESMTP POP3 server, accepting email for localhost and saving it to the emails\ndirectory. twistd makes it easy to spin up a server for testing clients, but it is also pluggable, production-\ngrade code. 326\nTwisted", "domains": ["Design Patterns", "Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 340", "position": 340, "chunk_type": "semantic", "token_estimate": 321}
{"text": "In that respect, Twisted\u2019s application deployment mechanisms via TAC \ufb01les, plugins, and twistd: have been a success. However, anecdotally, most large Twisted deployments end up having to rewrite\nsome of these management and monitoring facilities; the architecture does not quite expose what\nsystem administrators need. This is a re\ufb02ection of the fact that Twisted has not historically had\nmuch architectural input from system administrators\u2014the people who are experts at deploying and\nmaintaining applications. Twisted would be well-served to more aggressively solicit feedback from expert end users when\nmaking future architectural decisions in this space. 21.3\nRetrospective and Lessons Learned\nTwisted recently celebrated its 10th anniversary. Since its inception, inspired by the networked game\nlandscape of the early 2000s, it has largely achieved its goal of being an extensible, cross-platform,\nevent-driven networking engine. Twisted is used in production environments at companies from\nGoogle and Lucas\ufb01lm to Justin.TV and the Launchpad software collaboration platform. Server\nimplementations in Twisted are the core of numerous other open source applications, including\nBuildBot, BitTorrent, and Tahoe-LAFS. Twisted has had few major architectural changes since its initial development. The one crucial\naddition was Deferred, as discussed above, for managing pending results and their callback chains. There was one important removal, which has almost no footprint in the current implementation:\nTwisted Application Persistence. Twisted Application Persistence\nTwisted Application Persistence (TAP) was a way of keeping an application\u2019s con\ufb01guration and state\nin a pickle. Running an application using this scheme was a two-step process:\n1. Create the pickle that represents an Application, using the now defunct mktap utility. 2. Use twistd to unpickle and run the Application. This process was inspired by Smalltalk images, an aversion to the proliferation of seemingly ad\nhoc con\ufb01guration languages that were hard to script, and a desire to express con\ufb01guration details in\nPython. TAP \ufb01les immediately introduced unwanted complexity. Classes would change in Twisted without\ninstances of those classes getting changed in the pickle. Trying to use class methods or attributes\nfrom a newer version of Twisted on the pickled object would crash the application. The notion of\n\u201cupgraders\u201d that would upgrade pickles to new API versions was introduced, but then a matrix of\nupgraders, pickle versions, and unit tests had to be maintained to cover all possible upgrade paths,\nand comprehensively accounting for all interface changes was still hard and error-prone. TAPs and their associated utilities were abandoned and then eventually removed from Twisted\nand replaced with TAC \ufb01les and plugins.", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 341", "position": 341, "chunk_type": "semantic", "token_estimate": 409}
{"text": "In that respect, Twisted\u2019s application deployment mechanisms via TAC \ufb01les, plugins, and twistd: The notion of\n\u201cupgraders\u201d that would upgrade pickles to new API versions was introduced, but then a matrix of\nupgraders, pickle versions, and unit tests had to be maintained to cover all possible upgrade paths,\nand comprehensively accounting for all interface changes was still hard and error-prone. TAPs and their associated utilities were abandoned and then eventually removed from Twisted\nand replaced with TAC \ufb01les and plugins. TAP was backronymed to Twisted Application Plugin, and\nfew traces of the failed pickling system exist in Twisted today. The lesson learned from the TAP \ufb01asco was that to have reasonable maintainability, persistent\ndata needs an explicit schema. More generally, it was a lesson about adding complexity to a project:\nwhen considering introducing a novel system for solving a problem, make sure the complexity of that\nsolution is well understood and tested and that the bene\ufb01ts are clearly worth the added complexity\nbefore committing the project to it. Jessica McKellar\n327", "domains": ["Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 341", "position": 341, "chunk_type": "semantic", "token_estimate": 171}
{"text": "made to add WebSockets to Twisted as browsers began to adopt support for the new protocol in 2009,: but development moved to external projects after a decision not to include the protocol until it moved\nfrom an IETF draft to a standard. All of this being said, the proliferation of libraries and add-ons is a testament to Twisted\u2019s \ufb02exibility\nand extensibility. A strict test-driven development policy and accompanying documentation and\ncoding standards help the project avoid regressions and preserve backwards compatibility while\nmaintaining a large matrix of supported protocols and platforms. It is a mature, stable project that\ncontinues to have very active development and adoption. Twisted looks forward to being the engine of your Internet for another ten years. 329", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 343", "position": 343, "chunk_type": "semantic", "token_estimate": 121}
{"text": "[chapter22]: It is an e\ufb03cient, developer-friendly language which provides many compile-time\nchecks of program correctness. The goal of Yesod is to extend Haskell\u2019s strengths into web development. Yesod strives to\nmake your code as concise as possible. As much as possible, every line of your code is checked for\ncorrectness at compile time. Instead of requiring large libraries of unit tests to test basic properties,\nthe compiler does it all for you. Under the surface, Yesod uses as many advanced performance\ntechniques as we can muster to make your high-level code \ufb02y.", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 345", "position": 345, "chunk_type": "semantic", "token_estimate": 92}
{"text": "22.1: Compared to Other Frameworks\nIn general terms, Yesod is more similar to than di\ufb00erent than the leading frameworks such as Rails\nand Django. It generally follows the Model-View-Controller (MVC) paradigm, has a templating\nsystem that separates view from logic, provides an Object-Relational Mapping (ORM) system, and\nhas a front controller approach to routing. The devil is in the details. Yesod strives to push as much error catching to the compile phase\ninstead of runtime, and to automatically catch both bugs and security \ufb02aws through the type system. While Yesod tries to maintain a user-friendly, high-level API, it uses a number of newer techniques\nfrom the functional programming world to achieve high performance, and is not afraid to expose\nthese internals to developers. The main architectural challenge in Yesod is balancing these two seemingly con\ufb02icting goals. For\nexample, there is nothing revolutionary about Yesod\u2019s approach to routing (called type-safe URLs1. Historically, implementing such a solution was a tedious, error-prone process. Yesod\u2019s innovation is\nto use Template Haskell (a form of code generation) to automate the boilerplate required to bootstrap\nthe process. Similarly, type-safe HTML has been around for a long while; Yesod tries to keep the\ndeveloper-friendly aspect of common template languages while keeping the power of type safety. 22.2\nWeb Application Interface\nA web application needs some way to communicate with a server. One possible approach is to bake\nthe server directly into the framework, but doing so necessarily limits your options for deployment\nand leads to poor interfaces. Many languages have created standard interfaces to address this issue:\nPython has WSGI and Ruby has Rack. In Haskell, we have WAI: Web Application Interface. WAI is not intended to be a high-level interface. It has two speci\ufb01c goals: generality and\nperformance. By staying general, WAI has been able to support backends for everything from\nstandalone servers to old school CGI and even works directly with Webkit to produce faux desktop\napplications. The performance side will introduce us to a number of the cool features of Haskell. Datatypes\nOne of the biggest advantages of Haskell\u2014and one of the things we make the most use of in Yesod\u2014\nis strong static typing. Before we begin to write the code for how to solve something, we need to\nthink about what the data will look like. WAI is a perfect example of this paradigm.", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 346", "position": 346, "chunk_type": "semantic", "token_estimate": 392}
{"text": "Figure 22.1: Overall structure of a Yesod application: highly optimized datatypes that provide a high-level, safe API. In the case of a query string we store\nthe raw bytes transferred over the wire as a ByteString and the parsed, decoded values as Text. Streaming\nA ByteString represents a single memory bu\ufb00er. If we were to naively use a plain ByteString for\nholding the entire request or response bodies, our applications could never scale to large requests\nor responses. Instead, we use a technique called enumerators, very similar in concept to generators\nin Python. Our Application becomes a consumer of a stream of ByteStrings representing the\nincoming request body, and a producer of a separate stream for the response. We now need to slightly revise our de\ufb01nition of an Application. An Application will take a\nRequest value, containing headers, query string, etc., and will consume a stream of ByteStrings,\nproducing a Response. So the revised de\ufb01nition of an Application is:\ntype Application = Request -> Iteratee ByteString IO Response\nThe IO simply explains what types of side e\ufb00ects an application can perform. In the case of IO, it\ncan perform any kind of interaction with the outside world, an obvious necessity for the vast majority\nof web applications. Builder\nThe trick in our arsenal is how we produce our response bu\ufb00ers. We have two competing desires here:\nminimizing system calls, and minimizing bu\ufb00er copies. On the one hand, we want to minimize system\ncalls for sending data over the socket. To do this we need to store outgoing data in a bu\ufb00er. However,\nif we make this bu\ufb00er too large, we will exhaust our memory and slow down the application\u2019s\nresponse time. On the other hand, we want to minimize the number of times data is copied between\nbu\ufb00ers, preferably copying just once from the source to destination bu\ufb00er. Michael Snoyman\n333", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 347", "position": 347, "chunk_type": "semantic", "token_estimate": 311}
{"text": "Haskell\u2019s solution is the builder. A builder is an instruction for how to \ufb01ll a memory bu\ufb00er, such: as: place the \ufb01ve bytes \"hello\" in the next open position. Instead of passing a stream of memory\nbu\ufb00ers to the server, a WAI application passes a stream of these instructions. The server takes the\nstream and uses it to \ufb01ll up optimally sized memory bu\ufb00ers. As each bu\ufb00er is \ufb01lled, the server makes\na system call to send the data over over the wire and then starts \ufb01lling up the next bu\ufb00er. (The optimal size for a bu\ufb00er will depend on many factors such as cache size. The underlying\nblaze-builder library underwent signi\ufb01cant performance testing to determine the best trade-o\ufb00.) In theory, this kind of optimization could be performed in the application itself. However, by\nencoding this approach in the interface, we are able to simply prepend the response headers to the\nresponse body. The result is that, for small to medium-sized responses, the entire response can be\nsent with a single system call and memory is copied only once. Handlers\nNow that we have an application, we need some way to run it. In WAI parlance, this is a handler. WAI\nhas some basic, standard handlers, such as the standalone server Warp (discussed below), FastCGI,\nSCGI and CGI. This spectrum allows WAI applications to be run on anything from dedicated servers\nto shared hosting. But in addition to these, WAI has some more interesting backends:\nWebkit: This backend embeds a Warp server and calls out to QtWebkit. By launching a server, then\nlaunching a new standalone browser window, we have faux desktop applications. Launch: This is a slight variant on Webkit. Having to deploy the Qt and Webkit libraries can be a\nbit burdensome, so instead we just launch the user\u2019s default browser. Test: Even testing counts as a handler. After all, testing is simply the act of running an application\nand inspecting the responses. Most developers will likely use Warp. It is lightweight enough to be used for testing. It requires\nno con\ufb01g \ufb01les, no folder hierarchy and no long-running, administrator-owned process. It\u2019s a simple\nlibrary that gets compiled into your application or run via the Haskell interpreter. Warp is an\nincredibly fast server, with protection from all kinds of attack vectors, such as Slowloris and in\ufb01nite\nheaders. Warp can be the only web server you need, though it is also quite happy to sit behind a\nreverse HTTP proxy.", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 348", "position": 348, "chunk_type": "semantic", "token_estimate": 411}
{"text": "Haskell\u2019s solution is the builder. A builder is an instruction for how to \ufb01ll a memory bu\ufb00er, such: Warp is an\nincredibly fast server, with protection from all kinds of attack vectors, such as Slowloris and in\ufb01nite\nheaders. Warp can be the only web server you need, though it is also quite happy to sit behind a\nreverse HTTP proxy. The PONG benchmark measures the requests per second of various servers for the 4-byte response\nbody \"PONG\". In the graph shown in Figure 22.2, Yesod is measured as a framework on top of\nWarp. As can be seen, the Haskell servers (Warp, Happstack and Snap) lead the pack. Most of the reasons for Warp\u2019s speed have already been spelled out in the overall description of\nWAI: enumerators, builders and packed datatypes. The last piece in the puzzle is from the Glasgow\nHaskell Compiler\u2019s (GHC\u2019s) multithreaded runtime. GHC, Haskell\u2019s \ufb02agship compiler, has light-\nweight green threads. Unlike system threads, it is possible to spin up thousands of these without\nserious performance hits. Therefore, in Warp each connection is handled by its own green thread. The next trick is asynchronous I/O. Any web server hoping to scale to tens of thousands of\nrequests per second will need some type of asynchronous communication. In most languages, this\ninvolves complicated programming involving callbacks. GHC lets us cheat: we program as if we\u2019re\nusing a synchronous API, and GHC automatically switches between di\ufb00erent green threads waiting\nfor activity. Under the surface, GHC uses whatever system is provided by the host operating system, such as\nkqueue, epoll and select. This gives us all the performance of an event-based I/O system, without\nworrying about cross-platform issues or writing in a callback-oriented way. 334\nYesod", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 348", "position": 348, "chunk_type": "semantic", "token_estimate": 287}
{"text": "pass it to an application, and test properties about the response. wai-test simply provides some: convenience functions for testing common properties like the presence of a header or a status code. 22.3\nTemplates\nIn the typical Model-View-Controller (MVC) paradigm, one of the goals is to separate logic from the\nview. Part of this separation is achieved through the use of a template language. However, there are\nmany di\ufb00erent ways to approach this issue. At one end of the spectrum, for example, PHP/ASP/JSP\nwill allow you to embed any arbitrary code within your template. At the other end, you have systems\nlike StringTemplate and QuickSilver, which are passed some arguments and have no other way of\ninteracting with the rest of the program. Each system has its pros and cons. Having a more powerful template system can be a huge\nconvenience. Need to show the contents of a database table? No problem, pull it in with the template. However, such an approach can quickly lead to convoluted code, interspersing database cursor\nupdates with HTML generation. This can be commonly seen in a poorly written ASP project. While weak template systems make for simple code, they also tend towards a lot of redundant work. You will often need to not only keep your original values in datatypes, but also create dictionaries of\nvalues to pass to the template. Maintaining such code is not easy, and usually there is no way for a\ncompiler to help you out. Yesod\u2019s family of template languages, the Shakespearean languages, strive for a middle ground. By leveraging Haskell\u2019s standard referential transparency, we can be assured that our templates\nproduce no side e\ufb00ects. However, they still have full access to all the variables and functions available\nin your Haskell code. Also, since they are fully checked for both well-formedness, variable resolution\nand type safety at compile time, typos are much less likely to have you searching through your code\ntrying to pin down a bug. Why the Name Shakespeare? The HTML language, Hamlet, was the \ufb01rst language written, and originally based its syntax on\nHaml. Since it was at the time a \"reduced\" Haml, Hamlet seemed appropriate. As we added\nCSS and Javascript options, we decided to keep the naming theme with Cassius and Julius. At\nthis point, Hamlet looks nothing like Haml, but the name stuck anyway. Types\nOne of the overarching themes in Yesod is proper use of types to make developers\u2019 lives easier.", "domains": ["Architectural Patterns and Styles"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 350", "position": 350, "chunk_type": "semantic", "token_estimate": 408}
{"text": "The Other Languages: In addition to Hamlet, there are three other languages: Julius, Cassius and Lucius. Julius is used for\nJavascript; however, it\u2019s a simple pass-through language, just allowing for interpolation. In other\nwords, barring accidental use of the interpolation syntax, any piece of Javascript could be dropped\ninto Julius and be valid. For example, to test the performance of Julius, jQuery was run through the\nlanguage without an issue. The other two languages are alternate CSS syntaxes. Those familiar with the di\ufb00erence between\nSass and Less will recognize this immediately: Cassius is whitespace delimited, while Lucius uses\nbraces. Lucius is in fact a superset of CSS, meaning all valid CSS \ufb01les are valid Lucius \ufb01les. In\naddition to allowing text interpolation, there are some helper datatypes provided to model unit sizes\nand colors. Also, type-safe URLs work in these languages, making it convenient for specifying\nbackground images. Aside from the type safety and compile-time checks mentioned above, having specialized lan-\nguages for CSS and Javascript give us a few other advantages:\n\u2022 For production, all the CSS and Javascript is compiled into the \ufb01nal executable, increasing\nperformance (by avoiding \ufb01le I/O) and simplifying deployment. \u2022 By being based around the e\ufb03cient builder construct described earlier, the templates can be\nrendered very quickly. \u2022 There is built-in support for automatically including these in \ufb01nal webpages. We will get into\nthis in more detail when describing widgets below. 22.4\nPersistent\nMost web applications will want to store information in a database. Traditionally, this has meant\nsome kind of SQL database. In that regard, Yesod continues a long tradition, with PostgreSQL as\nour most commonly used backend. But as we have been seeing in recent years, SQL isn\u2019t always\nthe answer to the persistence question. Therefore, Yesod was designed to work well with NoSQL\ndatabases as well, and ships with a MongoDB backend as a \ufb01rst-class citizen. The result of this design decision is Persistent, Yesod\u2019s preferred storage option. There are really\ntwo guiding lights for Persistent: make it as back-end-agnostic as possible, and let user code be\ncompletely type-checked. At the same time, we fully recognize that it is impossible to completely shield the user from all\ndetails of the backend. Therefore, we provide two types of escape routes:\n\u2022 Back-end-speci\ufb01c functionality as necessary. For example, Persistent provides features for\nSQL joins and MongoDB lists and hashes.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 352", "position": 352, "chunk_type": "semantic", "token_estimate": 396}
{"text": "The Other Languages: Therefore, we provide two types of escape routes:\n\u2022 Back-end-speci\ufb01c functionality as necessary. For example, Persistent provides features for\nSQL joins and MongoDB lists and hashes. Proper portability warnings will apply, but if you\nwant this functionality, it\u2019s there. \u2022 Easy access to performing raw queries. We don\u2019t believe it\u2019s possible for any abstraction to\ncover every use case of the underlying library. If you just have to write a 5-table, correlated\nsubquery in SQL, go right ahead. Terminology\nThe most primitive datatype in Persistent is the PersistValue. This represents any raw data that can\nappear within the database, such as a number, a date, or a string. Of course, sometimes you\u2019ll have\n338\nYesod", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 352", "position": 352, "chunk_type": "semantic", "token_estimate": 118}
{"text": "some more user-friendly datatypes you want to store, like HTML. For that, we have the PersistField: class. Internally, a PersistField expresses itself to the database in terms of a PersistValue. All of this is very nice, but we will want to combine di\ufb00erent \ufb01elds together into a larger picture. For this, we have a PersistEntity, which is basically a collection of PersistFields. And \ufb01nally,\nwe have a PersistBackend that describes how to create, read, update and delete these entities. As a practical example, consider storing a person in a database. We want to store the person\u2019s\nname, birthday, and a pro\ufb01le image (a PNG \ufb01le). We create a new entity Person with three \ufb01elds:\na Text, a Day and a PNG. Each of those gets stored in the database using a di\ufb00erent PersistValue\nconstructor: PersistText, PersistDay and PersistByteString, respectively. There is nothing surprising about the \ufb01rst two mappings, but the last one is interesting. There is\nno speci\ufb01c constructor for storing PNG content in a database, so instead we use a more generic type\n(a ByteString, which is just a sequence of bytes). We could use the same mechanism to store other\ntypes of arbitrary data. (The commonly held best practice for storing images is to keep the data on the \ufb01lesystem and\njust keep a path to the image in the database. We do not advocate against using that approach, but\nare rather using database-stored images as an illustrative example.) How is all this represented in the database? Consider SQL as an example: the Person entity\nbecomes a table with three columns (name, birthday, and picture). Each \ufb01eld is stored as a di\ufb00erent\nSQL type: Text becomes a VARCHAR, Day becomes a Date and PNG becomes a BLOB (or BYTEA). The story for MongoDB is very similar. Person becomes its own document, and its three \ufb01elds\neach become a MongoDB \ufb01eld. There is no need for datatypes or creation of a schema in MongoDB. Persistent", "domains": ["Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 353", "position": 353, "chunk_type": "semantic", "token_estimate": 326}
{"text": "\u2022 Even 100% test coverage doesn\u2019t guarantee that you really have tested every case. All it\u2019s done: is proven you\u2019ve tested every line of code. Cross-Database Syntax\nCreating an SQL schema that works for multiple SQL engines can be tricky enough. How do you\ncreate a schema that will also work with a non-SQL database like MongoDB? Persistent allows you to de\ufb01ne your entities in a high-level syntax, and will automatically create\nthe SQL schema for you. In the case of MongoDB, we currently use a schema-less approach. This\nalso allows Persistent to ensure that your Haskell datatypes match perfectly with the database\u2019s\nde\ufb01nitions. Additionally, having all this information gives Persistent the ability to perform more advanced\nfunctions, such as migrations, for you automatically. Migrations\nPersistent not only creates schema \ufb01les as necessary, but will also automatically apply database\nmigrations if possible. Database modi\ufb01cation is one of the less-developed pieces of the SQL standard,\nand thus each engine has a di\ufb00erent take on the process. As such, each Persistent backend de\ufb01nes its\nown set of migration rules. In PostgreSQL, which has a rich set of ALTER TABLE rules, we use those\nextensively. Since SQLite lacks much of that functionality, we are reduced to creating temporary\ntables and copying rows. MongoDB\u2019s schema-less approach means no migration support is required. This feature is purposely limited to prevent any kind of data loss. It will not remove any columns\nautomatically; instead, it will give you an error message, telling you the unsafe operations that are\nnecessary in order to continue. You will then have the option of either manually running the SQL it\nprovides you, or changing your data model to avoid the dangerous behavior. Relations\nPersistent is non-relational in nature, meaning it has no requirement for backends to support relations. However, in many use cases, we may want to use relations. In those cases, developers will have full\naccess to them. Assume we want to now store a list of skills with each user. If we were writing a MongoDB-\nspeci\ufb01c app, we could go ahead and just store that list as a new \ufb01eld in the original Person entity. But that approach would not work in SQL. In SQL, we call this kind of relationship a one-to-many\nrelationship. The idea is to store a reference to the \"one\" entity (person) with each \"many\" entity (skill). Then\nif we want to \ufb01nd all the skills a person has, we simply \ufb01nd all skills that reference that person.", "domains": ["Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 354", "position": 354, "chunk_type": "semantic", "token_estimate": 415}
{"text": "\u2022 Even 100% test coverage doesn\u2019t guarantee that you really have tested every case. All it\u2019s done: The idea is to store a reference to the \"one\" entity (person) with each \"many\" entity (skill). Then\nif we want to \ufb01nd all the skills a person has, we simply \ufb01nd all skills that reference that person. For\nthis reference, every entity has an ID. And as you might expect by now, these IDs are completely\ntype-safe. The datatype for a Person ID is PersonId. So to add our new skill, we would just add the\nfollowing to our entity de\ufb01nition:\nSkill\nperson PersonId\nname Text\ndescription Text\nUniqueSkill person name\n340\nYesod", "domains": ["Domain-Driven Design"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 354", "position": 354, "chunk_type": "semantic", "token_estimate": 111}
{"text": "This ID datatype concept comes up throughout Persistent and Yesod. You can dispatch based on: an ID. In such a case, Yesod will automatically marshal the textual representation of the ID to the\ninternal one, catching any parse errors along the way. These IDs are used for lookup and deletion\nwith the get and delete functions, and are returned by the insertion and query functions insert\nand selectList. 22.5\nYesod\nIf we are looking at the typical Model-View-Controller (MVC) paradigm, Persistent is the model and\nShakespeare is the view. This would leave Yesod as the controller. The most basic feature of Yesod is routing. It features a declarative syntax and type-safe dispatch. Layered on top of this, Yesod provides many other features: streaming content generation, widgets,\ni18n, static \ufb01les, forms and authentication. But the core feature added by Yesod is really routing. This layered approach makes it simpler for users to swap di\ufb00erent components of the system. Some people are not interested in using Persistent. For them, nothing in the core system even mentions\nPersistent. Likewise, while they are commonly used features, not everyone needs authentication or\nstatic \ufb01le serving. On the other hand, many users will want to integrate all of these features. And doing so, while\nenabling all the optimizations available in Yesod, is not always straightforward. To simplify the\nprocess, Yesod also provides a sca\ufb00olding tool that sets up a basic site with the most commonly used\nfeatures. Routes\nGiven that routing is really the main function of Yesod, let\u2019s start there. The routing syntax is very\nsimple: a resource pattern, a name, and request methods. For example, a simple blog site might look\nlike:\n/ HomepageR GET\n/add-entry AddEntryR GET POST\n/entry/#EntryId EntryR GET\nThe \ufb01rst line de\ufb01nes the homepage. This says \"I respond to the root path of the domain, I\u2019m\ncalled HomepageR, and I answer GET requests.\" (The trailing \"R\" on the resource names is simply a\nconvention, it doesn\u2019t hold any special meaning besides giving a cue to the developer that something\nis a route.) The second line de\ufb01nes the add-entry page. This time, we answer both GET and POST requests. You might be wondering why Yesod, as opposed to most frameworks, requires you to explicitly state\nyour request methods. The reason is that Yesod tries to adhere to RESTful principles as much as\npossible, and GET and POST requests really have very di\ufb00erent meanings.", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 355", "position": 355, "chunk_type": "semantic", "token_estimate": 402}
{"text": "This ID datatype concept comes up throughout Persistent and Yesod. You can dispatch based on: You might be wondering why Yesod, as opposed to most frameworks, requires you to explicitly state\nyour request methods. The reason is that Yesod tries to adhere to RESTful principles as much as\npossible, and GET and POST requests really have very di\ufb00erent meanings. Not only do you state\nthese two methods separately, but later you will de\ufb01ne their handler functions separately. (This is\nactually an optional feature in Yesod. If you want, you can leave o\ufb00the list of methods and your\nhandler function will deal with all methods.) The third line is a bit more interesting. After the second slash we have #EntryId. This de\ufb01nes\na parameter of type EntryId. We already alluded to this feature in the Persistent section: Yesod\nwill now automatically marshal the path component into the relevant ID value. Assuming an SQL\nbackend (Mongo is addressed later), if a user requests /entry/5, the handler function will get called\nMichael Snoyman\n341", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 355", "position": 355, "chunk_type": "semantic", "token_estimate": 171}
{"text": "Handlers: Once you de\ufb01ne your routes, you need to tell Yesod how you want to respond to requests. This is\nwhere handler functions come into play. The setup is simple: for each resource (e.g., HomepageR)\nand request method, create a function named methodResourceR. For our previous example, we\nwould need four functions: getHomepageR, getAddEntryR, postAddEntryR, and getEntryR. All of the parameters collected from the route are passed in as arguments to the handler function. getEntryR will take a \ufb01rst argument of type EntryId, while all the other functions will take no\narguments. The handler functions live in a Handler monad, which provides a great deal of functionality,\nsuch as redirecting, accessing sessions, and running database queries. For the last one, a typical way\nto start o\ufb00the getEntryR function would be:\ngetEntryR entryId = do\nentry <- runDB $ get404 entryId\nThis will run a database action that will get the entry associated with the given ID from the\ndatabase. If there is no such entry, it will return a 404 response. Each handler function will return some value, which must be an instance of HasReps. This is\nanother RESTful feature at play: instead of just returning some HTML or some JSON, you can return\na value that will return either one, depending on the HTTP Accept request header. In other words, in\nYesod, a resource is a speci\ufb01c piece of data, and it can be returned in one of many representations. Widgets\nAssume you want to include a navbar on a few di\ufb00erent pages of your site. This navbar will load up\nthe \ufb01ve most recent blog posts (stored in your database), generate some HTML, and then need some\nCSS and Javascript to style and enhance. Without a higher-level interface to tie these components together, this could be a pain to implement. You could add the CSS to the site-wide CSS \ufb01le, but that\u2019s adding extra declarations you don\u2019t always\nneed. Likewise with the Javascript, though a bit worse: having that extra Javascript might cause\nproblems on a page it was not intended to live on. You will also be breaking modularity by having to\ngenerate the database results from multiple handler functions. In Yesod, we have a very simple solution: widgets.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 357", "position": 357, "chunk_type": "semantic", "token_estimate": 372}
{"text": "Handlers: You will also be breaking modularity by having to\ngenerate the database results from multiple handler functions. In Yesod, we have a very simple solution: widgets. A widget is a piece of code that ties together\nHTML, CSS and Javascript, allowing you to add content to both the head and body, and can run any\narbitrary code that belongs in a handler. For example, to implement our navbar:\n-- Get last five blog posts. The \"lift\" says to run this code like we\u2019re in the handler. entries <- lift $ runDB $ selectList [] [LimitTo 5, Desc EntryPosted]\ntoWidget [hamlet|\n<ul .navbar>\n$forall entry <- entries\n<li>#{entryTitle entry}\n|]\ntoWidget [lucius| .navbar { color: red } |]\ntoWidget [julius|alert(\"Some special Javascript to play with my navbar\");|]\nBut there is even more power at work here. When you produce a page in Yesod, the standard\napproach is to combine a number of widgets together into a single widget containing all your page\nMichael Snoyman\n343", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 357", "position": 357, "chunk_type": "semantic", "token_estimate": 165}
{"text": "As this was my \ufb01rst major piece of Haskell code, I learned a lot about the language during: Yesod\u2019s development. I\u2019m sure others can relate to the feeling of \"How did I ever write code like\nthis?\" Even though that initial code was not of the same caliber as the code we have in Yesod at this\npoint, it was solid enough to kick-start the project. The second lesson is:\nDon\u2019t be deterred by supposed lack of mastery of the tools at hand. Write the best code you can,\nand keep improving it. One of the most di\ufb03cult steps in Yesod\u2019s development was moving from a single-person team\u2014\nme\u2014to collaborating with others. It started o\ufb00simply, with merging pull requests on GitHub,\nand eventually moved to having a number of core maintainers. I had established some of my own\ndevelopment patterns, which were nowhere explained or documented. As a result, contributors found\nit di\ufb03cult to pull my latest unreleased changes and play around with them. This hindered others both\nwhen contributing and testing. When Greg Weber came aboard as another lead on Yesod, he put in place a lot of the coding\nstandards that were sorely lacking. To compound the problems, there were some inherent di\ufb03culties\nplaying with the Haskell development toolchain; speci\ufb01cally in dealing with Yesod\u2019s large number\nof packages. One of the goals of the entire Yesod team has since been to create standard scripts and\ntools to automate building. Many of these tools are making their way back into the general Haskell\ncommunity. The \ufb01nal lesson is:\nConsider early on how to make your project approachable for others. 345", "domains": ["Design Principles", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 359", "position": 359, "chunk_type": "semantic", "token_estimate": 271}
{"text": "[chapter23]: Yocto\nElizabeth Flanagan\nThe Yocto Project is an open source project that provides a common starting point for developers of\nembedded Linux systems to create customized distributions for embedded products in a hardware-\nagnostic setting. Sponsored by the Linux Foundation, Yocto is more than a build system. It provides\ntools, processes, templates and methods so developers can rapidly create and deploy products for the\nembedded market. One of the core components of Yocto is the Poky Build system. As Poky is a\nlarge and complex system, we will be focusing on one of its core components, BitBake. BitBake is a\nGentoo-Portage-inspired build tool, used by both the Yocto Project and OpenEmbedded communities\nto utilize metadata to create Linux images from source. In 2001, Sharp Corporation introduced the SL-5000 PDA, named Zaurus, which ran an embedded\nLinux distribution, Lineo. Not long after the Zaurus\u2019s introduction, Chris Larson founded the\nOpenZaurus Project, a replacement Linux distribution for the SharpROM, based on a build system\ncalled buildroot. With the founding of the project, people began contributing many more software\npackages, as well as targets for other devices, and it wasn\u2019t long before the build system for OpenZaurus\nbegan to show fragility. In January 2003, the community began discussing a new build system to\nincorporate the community usage model of a generic build system for embedded Linux distributions. This would eventually become OpenEmbedded. Chris Larson, Michael Lauer, and Holger Schurig\nbegan work on OpenEmbedded by porting hundreds of OpenZaurus packages over to the new build\nsystem. The Yocto Project springs from this work. At the project\u2019s core is the Poky build system, created\nby Richard Purdie. It began as a stabilized branch of OpenEmbedded using a core subset of the\nthousands of OpenEmbedded recipes, across a limited set of architectures. Over time, it slowly\ncoalesced into more than just an embedded build system, but into a complete software development\nplatform, with an Eclipse plugin, a fakeroot replacement and QEMU based images. Around November\n2010, the Linux Foundation announced that this work would all continue under the heading of the\nYocto Project as a Linux Foundation-sponsored project. It was then established that Yocto and\nOpenEmbedded would coordinate on a core set of package metadata called OE-Core, combining the\nbest of both Poky and OpenEmbedded with an increased use of layering for additional components.", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 361", "position": 361, "chunk_type": "semantic", "token_estimate": 389}
{"text": "[chapter23]: Around November\n2010, the Linux Foundation announced that this work would all continue under the heading of the\nYocto Project as a Linux Foundation-sponsored project. It was then established that Yocto and\nOpenEmbedded would coordinate on a core set of package metadata called OE-Core, combining the\nbest of both Poky and OpenEmbedded with an increased use of layering for additional components. 23.1\nIntroduction to the Poky Build System\nThe Poky build system is the core of the Yocto Project. In Poky\u2019s default con\ufb01guration, it can provide\na starting image footprint that ranges from a shell-accessible minimal image all the way up to a Linux\nStandard Base-compliant image with a GNOME Mobile and Embedded (GMAE) based reference", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 361", "position": 361, "chunk_type": "semantic", "token_estimate": 117}
{"text": "user interface called Sato. From these base image types, metadata layers can be added to extend: functionality; layers can provide an additional software stack for an image type, add a board support\npackage (BSP) for additional hardware or even represent a new image type. Using the 1.1 release\nof Poky, named \u201cedison\u201d, we will show how BitBake uses these recipes and con\ufb01guration \ufb01les to\ngenerate an embedded image. From a very high level, the build process starts out by setting up the shell environment for the\nbuild run. This is done by sourcing a \ufb01le, oe-init-build-env, that exists in the root of the Poky\nsource tree. This sets up the shell environment, creates an initial customizable set of con\ufb01guration\n\ufb01les and wraps the BitBake runtime with a shell script that Poky uses to determine if the minimal\nsystem requirements have been met. For example, one of the things it will look for is the existence of Pseudo, a fakeroot replacement\ncontributed to the Yocto Project by Wind River Systems. At this point, bitbake core-image-minimal,\nfor example, should be able to create a fully functional cross-compilation environment and then\ncreate a Linux image based on the image de\ufb01nition for core-image-minimal from source as de\ufb01ned\nin the Yocto metadata layer. Figure 23.1: High-level overview of Poky task execution\nDuring the creation of our image, BitBake will parse its con\ufb01guration, include any additional\nlayers, classes, tasks or recipes de\ufb01ned, and begin by creating a weighted dependency chain. This\nprocess provides an ordered and weighted task priority map. BitBake then uses this map to determine\nwhat packages must be built in which order so as to most e\ufb03ciently ful\ufb01ll compilation dependencies. Tasks needed by the most other tasks are weighted higher, and thus run earlier during the build process. The task execution queue for our build is created. BitBake also stores the parsed metadata summaries\nand if, on subsequent runs, it determines that the metadata has changed, it can re-parse only what\n348\nYocto", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 362", "position": 362, "chunk_type": "semantic", "token_estimate": 330}
{"text": "has changed. The BitBake scheduler and parser are some of the more interesting architectural: designs of BitBake and some of the decisions surrounding them and their implementation by BitBake\ncontributors will be discussed later. BitBake then runs through its weighted task queue, spawning threads (up to the number de\ufb01ned\nby BB_NUMBER_THREADS in conf/local.conf) that begin executing those tasks in the predetermined\norder. The tasks executed during a package\u2019s build may be modi\ufb01ed, prepended- or appended-to\nthrough its recipe. The basic, default package task order of execution starts by fetching and unpacking\npackage source and then con\ufb01guring and cross-compiling the unpacked source. The compiled source\nis then split up into packages and various calculations are made on the compilation result such as\nthe creation of debug package information. The split packages are then packaged into a supported\npackage format; RPM, ipk and deb are supported. BitBake will then use these packages to build the\nroot \ufb01le system. Poky Build System Concepts\nOne of the most powerful properties of the Poky build system is that every aspect of a build is\ncontrolled by metadata. Metadata can be loosely grouped into con\ufb01guration \ufb01les or package recipes. A recipe is a collection of non-executable metadata used by BitBake to set variables or de\ufb01ne\nadditional build-time tasks. A recipe contains \ufb01elds such as the recipe description, the recipe version,\nthe license of the package and the upstream source repository. It may also indicate that the build\nprocess uses autotools, make, distutils or any other build process, in which case the basic\nfunctionality can be de\ufb01ned by classes it inherits from the OE-Core layer\u2019s class de\ufb01nitions in\n./meta/classes. Additional tasks can also be de\ufb01ned, as well as task prerequisites. BitBake also\nsupports both _prepend and _append as a method of extending task functionality by injecting code\nindicated by using prepend or append su\ufb03x into the beginning or end of a task. Con\ufb01guration \ufb01les can be broken down into two types. There are those that con\ufb01gure BitBake\nand the overall build run, and those that con\ufb01gure the various layers Poky uses to create di\ufb00erent\ncon\ufb01gurations of a target image. A layer is any grouping of metadata that provides some sort of\nadditional functionality. These can be BSP for new devices, additional image types or additional\nsoftware outside of the core layers.", "domains": ["Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 364", "position": 364, "chunk_type": "semantic", "token_estimate": 384}
{"text": "has changed. The BitBake scheduler and parser are some of the more interesting architectural: A layer is any grouping of metadata that provides some sort of\nadditional functionality. These can be BSP for new devices, additional image types or additional\nsoftware outside of the core layers. In fact, the core Yocto metadata, meta-yocto, is itself a layer\napplied on top of the OE-Core metadata layer, meta which adds additional software and image types\nto the OE-Core layer. An example of how one would use layering is by creating a NAS device for the Intel n660\n(Crownbay), using x32, the new 32-bit native ABI for x86-64, with a custom software layer that adds\na user interface. Given the task at hand, we could split this functionality out into layers. At the lowest level we\nwould utilize a BSP layer for Crownbay that would enable Crownbay-speci\ufb01c hardware functionality,\nsuch as video drivers. As we want x32, we would use the experimental meta-x32 layer. The NAS\nfunctionality would be layered on top of this by adding the Yocto Project\u2019s example NAS layer,\nmeta-baryon. And lastly, we\u2019ll use an imaginary layer called meta-myproject, to provide the\nsoftware and con\ufb01guration to create a graphical user interface for con\ufb01guration of the NAS. During the setup of the BitBake environment, some initial con\ufb01guration \ufb01les are generated by\nsourcing oe-build-init-env. These con\ufb01guration \ufb01les allow us quite a bit of control over how\nand what Poky generates. The \ufb01rst of these con\ufb01guration \ufb01les is bblayers.conf. This \ufb01le is what\nwe will use to add additional layers in order to build our example project. 350\nYocto", "domains": ["Architectural Patterns and Styles", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 364", "position": 364, "chunk_type": "semantic", "token_estimate": 266}
{"text": "All of the BitBake con\ufb01guration \ufb01les help generate BitBake\u2019s datastore which is used during: Given the number of recipes and the number of variables\nand tasks needed to accomplish a build, BitBake 1.0 was a memory hog. At a time when memory\nwas expensive and systems had much less, builds could be painful a\ufb00airs. It was not unheard of for\na system to run out of memory (writing to swap!) as it slugged through a long running build. In\nits \ufb01rst incarnation, while it did the job (sometimes), it did it slowly while consuming an enormous\namount of resources. Worse, as BitBake 1.0 had no concept of a data persistence cache or shared\nstate, it also had no ability to do incremental builds. If a build failed, one would have to restart it\nfrom scratch. A quick di\ufb00between the current BitBake version used in Poky \u201cedison\u201d 1.13.3 and 1.0 shows\nthe implementation of BitBake\u2019s client-server architecture, the data persistence cache, its datastore, a\ncopy-on-write improvement for the datastore, shared state implementation and drastic improvements\nin how it determines task and package dependency chains. This evolution has made it more reliable,\nmore e\ufb03cient and more dynamic. Much of this functionality came out of necessity for quicker, more\nreliable builds that used fewer resources. Three improvements to BitBake that we will examine are\n352\nYocto", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 366", "position": 366, "chunk_type": "semantic", "token_estimate": 223}
{"text": "the implementation of a client-server architecture, optimizations around BitBake\u2019s data storage and: work done on how BitBake determines its build and task dependency chain. BitBake IPC\nSince we now know a good deal about how the Poky build system uses con\ufb01gurations, recipes and\nlayers to create embedded images, we\u2019re prepared to begin to look under the hood of BitBake and\nexamine how this is all combined. Starting with the core BitBake executable, bitbake/bin/bake,\nwe can begin to see the process BitBake follows as it begins to set up the infrastructure needed to\nbegin a build. The \ufb01rst item of interest is BitBake\u2019s Interprocess Communications (IPC). Initially,\nBitBake had no concept of a client-server. This functionality was factored into the BitBake design\nover a period of time in order to allow BitBake to run multiple processes during a build, as it was\ninitially single-threaded, and to allow di\ufb00erent user experiences. Figure 23.3: Overview of BitBake IPC\nAll Poky builds are begun by starting a user interface instance. The user interface provides a\nmechanism for logging of build output, build status and build progress, as well as for receiving\nevents from build tasks through the BitBake event module. The default user interface used is knotty,\nBitBake\u2019s command line interface. Called knotty, or \u201c(no) tty\u201d, since it handles both ttys and non-ttys,\nit is one of a few interfaces that are supported. One of these additional user interfaces is Hob. Hob\nis the graphical interface to BitBake, a kind of \u201cBitBake commander\u201d. In addition to the typical\nfunctions you would see in the knotty user interface, Hob (written by Joshua Lock) brings the ability\nto modify con\ufb01guration \ufb01les, add additional layers and packages, and fully customize a build. BitBake user interfaces have the ability to send commands to the next module brought up by\nthe BitBake executable, the BitBake server. Like the user interface, BitBake also supports multiple\nElizabeth Flanagan\n353", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 367", "position": 367, "chunk_type": "semantic", "token_estimate": 318}
{"text": "di\ufb00erent server types, such as XMLRPC. The default server that most users use when executing: BitBake from the knotty user interface is BitBake\u2019s process server. After bringing up the server, the\nBitBake executable brings up the cooker. The cooker is a core portion of BitBake and is where most of the particularly interesting things\nthat occur during a Poky build are called from. The cooker is what manages the parsing of metadata,\ninitiates the generation of the dependency and task trees, and manages the build. One of the functions\nof BitBake\u2019s server architecture is allowing multiple ways of exposing the command API, indirectly,\nto the user interface. The command module is the worker of BitBake, running build commands and\ntriggering events that get passed up to the user interface through BitBake\u2019s event handler. Once the\ncooker is brought up from the BitBake executable, it initializes the BitBake datastore and then begins\nto parse all of Poky\u2019s con\ufb01guration \ufb01les. It then creates the runqueue object, and triggers the build. BitBake DataSmart Copy-on-Write Data Storage\nIn BitBake 1.0, all BitBake variables were parsed and stored in one very large dictionary during\nthe initialization of that version\u2019s data class. As previously mentioned, this was problematic in\nthat very large Python dictionaries are slow on writes and member access, and if the build host\nruns out of physical memory during the build, we end up using swap. While this is less likely in\nmost systems in late 2011, when OpenEmbedded and BitBake were \ufb01rst starting up, the average\ncomputer\u2019s speci\ufb01cation usually had less than one or two gigabytes of memory. This was one of the major pain points in early BitBake. Two major issues needed to be worked\nout in order to help increase performance: one was precomputation of the build dependency chain;\nthe other was to reduce the size of data being stored in memory. Much of the data being stored\nfor a recipe doesn\u2019t change from recipe to recipe; for example, with TMPDIR, BB_NUMBER_THREADS\nand other global BitBake variables, having a copy of the entire data environment per recipe stored\nin memory was ine\ufb03cient. The solution was Tom Ansell\u2019s copy-on-write dictionary that \u201cabuses\nclasses to be nice and fast\u201d. BitBake\u2019s COW module is both an especially fearless and clever hack.", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 368", "position": 368, "chunk_type": "semantic", "token_estimate": 378}
{"text": "di\ufb00erent server types, such as XMLRPC. The default server that most users use when executing: The solution was Tom Ansell\u2019s copy-on-write dictionary that \u201cabuses\nclasses to be nice and fast\u201d. BitBake\u2019s COW module is both an especially fearless and clever hack. Running python BitBake/lib/bb/COW.py and examining the module will give you an idea of how\nthis copy-on-write implementation works and how BitBake uses it to store data e\ufb03ciently\nThe DataSmart module, which uses the COW dictionary, stores the data from the initial Poky\ncon\ufb01guration, data from .conf \ufb01les and .bbclass \ufb01les, in a dict as a data object. Each of these\nobjects can contain another data object of just the di\ufb00of the data. So, if a recipe changes something\nfrom the initial data con\ufb01guration, instead of copying the entire con\ufb01guration in order to localize it,\na di\ufb00of the parent data object is stored at the next layer down in the COW stack. When an attempt is\nmade to access a variable, the data module will use DataSmart to look into the top level of the stack. If the variable is not found it will defer to a lower level of the stack until it does \ufb01nd the variable or\nthrows an error. One of the other interesting things about the DataSmart module centers around variable expansion. As BitBake variables can contain executable Python code, one of the things that needs to be done\nis run the variable through BitBake\u2019s bb.codeparser to ensure that it\u2019s valid Python and that it\ncontains no circular references. An example of a variable containing Python code is this example\ntaken from ./meta/conf/distro/include/tclibc-eglibc.inc:\nLIBCEXTENSION = \"${@[\u2019\u2019, \u2019-gnu\u2019][(d.getVar(\u2019ABIEXTENSION\u2019, True) or \u2019\u2019) != \u2019\u2019]}\"\nThis\nvariable\nis\nincluded\nfrom\none\nof\nthe\nOE-Core\ncon\ufb01guration\n\ufb01les,\n./meta/conf/distro/include/defaultsetup.conf, and is used to provide a set of default op-\ntions across di\ufb00erent distro con\ufb01gurations that one would want to lay on top of Poky or OpenEmbed-\n354\nYocto", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 368", "position": 368, "chunk_type": "semantic", "token_estimate": 317}
{"text": "ded. This \ufb01le imports some eglibc-speci\ufb01c variables that are set dependent on the value of another: BitBake variable ABIEXTENSION. During the creation of the datastore, the Python code within this\nvariable needs to be parsed and validated to ensure tasks that use this variable will not fail. BitBake Scheduler\nOnce BitBake has parsed the con\ufb01guration and created its datastore, it needs to parse the recipes\nrequired for the image and produce a build chain. This is one of the more substantial improvements\nto BitBake. Originally, BitBake took its build priorities from a recipe. If a recipe had a DEPENDS, it\nwould try to \ufb01gure out what to build in order to satisfy that dependency. If a task failed because it\nlacked a prerequisite needed for its buildout, it was simply put to the side and attempted later. This\nhad obvious drawbacks, both in e\ufb03ciency and reliability. As no precomputed dependency chain was established, task execution order was \ufb01gured out\nduring the build run. This limited BitBake to being single-threaded. To give an idea of how painful\nsingle-threaded BitBake builds can be, the smallest image \u201ccore-image-minimal\u201d on a standard\ndeveloper machine in 2011 (Intel Core i7, 16 gigabytes of DDR3 memory) takes about three or four\nhours to build a complete cross-compilation toolchain and use it to produce packages that are then\nused to create an image. For reference, a build on the same machine with BB_NUMBER_THREADS at\n14 and PARALLEL_MAKE set to \u201c-j 12\u201d takes about 30 to 40 minutes. As one could imagine, running\nsingle-threaded with no precomputed order of task execution on slower hardware that had less\nmemory with a large portion wasted by duplicate copies of the entire datastore took much longer. Dependencies\nWhen we talk of build dependencies, we need to make a distinction between the various types. A\nbuild dependency, or DEPENDS, is something we require as a prerequisite so that Poky can build the\nrequired package, whereas a runtime dependency, RDEPENDS, requires that the image the package is\nto be installed on also contains the package listed as an RDEPENDS. Take, for example, the package\ntask-core-boot. If we look at the recipe for it in\nmeta/recipes-core/tasks/task-core-boot.bb\nwe will see two BitBake variables set: RDEPENDS and DEPENDS. BitBake uses these two \ufb01elds\nduring the creation of its dependency chain. Here is a portion of task-core-boot.bb showing DEPENDS and RDEPENDS:\nDEPENDS = \"virtual/kernel\"\n...\nRDEPENDS_task-core-boot = \"\\\nbase-files \\\nbase-passwd \\\nbusybox \\\ninitscripts \\\n...", "domains": ["Software Quality Attributes", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 369", "position": 369, "chunk_type": "semantic", "token_estimate": 410}
{"text": "Internally dependent tasks are set within a recipe and add a task before and/or after another: task. For example, in a recipe, we could add a task called do_deploy by adding the line addtask\ndeploy before do_build after do_compile. This would add a dependency for running the\ndo_deploy task prior to do_build being started, but after do_compile is completed. DEPENDS and\nRDEPENDS dependent tasks are tasks that run after a denoted task. For example, if we wanted to run\ndo_deploy of a package after the do_install of its DEPENDS or RDEPENDS, our recipe would include\ndo_deploy[deptask] = \u2019do_install\u2019 or do_deploy[rdeptask] = \u2019do_install\u2019. For inter-\ntask dependencies, if we wanted a task to be dependent on a di\ufb00erent package\u2019s task we would add, us-\ning the above example of do_deploy, do_deploy[depends] = \u201c<target\u2019s name>:do_install\u201d. RunQueue\nAs an image build can have hundreds of recipes, each with multiple packages and task, each with its\nown dependency, BitBake is now tasked with trying to sort this out into something it can use as an\norder of execution. After the cooker has gotten the entire list of packages needed to be built from the\ninitialization of the bb.data object, it will begin to create a weighted task map from this data in order\nto produce an ordered list of tasks it needs to run, called the runqueue. Once the runqueue is created,\nBitBake can begin executing it in order of priority, tasking out each portion to a di\ufb00erent thread. Within the provider module, BitBake will \ufb01rst look to see if there is a PREFERRED_PROVIDER\nfor a given package or image. As more than one recipe can provide a given package and as tasks are\nde\ufb01ned in recipes, BitBake needs to decide which provider of a package it will use. It will sort all\nthe providers of the package, weighting each provider by various criteria. For example, preferred\nversions of software will get a higher priority than others. However, BitBake also takes into account\npackage version as well as the dependencies of other packages. Once it has selected the recipe from\nwhich it will derive its package, BitBake will iterate over the DEPENDS and RDEPENDS of that\nrecipe and proceed to compute the providers for those packages. This chain reaction will produce a\nlist of packages needed for image generation as well as providers for those packages. Runqueue now has a full list of all packages that need to be built and a dependency chain.", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 370", "position": 370, "chunk_type": "semantic", "token_estimate": 410}
{"text": "Internally dependent tasks are set within a recipe and add a task before and/or after another: This chain reaction will produce a\nlist of packages needed for image generation as well as providers for those packages. Runqueue now has a full list of all packages that need to be built and a dependency chain. In\norder to begin execution of the build, the runqueue module now needs to create the TaskData object\nso it can begin to sort out a weighted task map. It begins by taking each buildable package it has\nfound, splitting the tasks needed to generate that package and weighing each of those tasks based\non the number of packages that require it. Tasks with a higher weight have more dependents, and\ntherefore are generally run earlier in the build. Once this is complete, the runqueue module then\nprepares to convert the TaskData object into a runqueue. The creation of the runqueue is somewhat complex. BitBake \ufb01rst iterates through the list of task\nnames within the TaskData object in order to determine task dependencies. As it iterates through\nTaskData, it begins to build a weighted task map. When it is complete, if it has found no circular\ndependencies, unbuildable tasks or any such problems, it will then order the task map by weight and\nreturn a complete runqueue object to the cooker. The cooker will begin to attempt to execute the\nrunqueue, task by task. Depending upon image size and computing resources, Poky may take from\na half-hour to hours to generate a cross-compilation toolchain, a package feed and the embedded\nLinux image speci\ufb01ed. It is worth noting that from the time of executing bitbake <image_name>\nfrom the command line, the entire process up to right before the execution of the task execution\nqueue has taken less than a few seconds. 356\nYocto", "domains": ["Design Patterns", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 370", "position": 370, "chunk_type": "semantic", "token_estimate": 306}
{"text": "reference implementation for it and participating in several large-scale projects heavily based on: messaging technology\u2014to realise that there\u2019s something wrong with the classic client/server model\nof smart messaging server (broker) and dumb messaging clients. Our primary concern at the time was with the performance: If there\u2019s a server in the middle,\neach message has to pass the network twice (from the sender to the broker and from the broker to the\nreceiver) inducing a penalty in terms of both latency and throughput. Moreover, if all the messages\nare passed through the broker, at some point it\u2019s bound to become the bottleneck. A secondary concern was related to large-scale deployments: when the deployment crosses\norganisational boundaries the concept of a central authority managing the whole message \ufb02ow\ndoesn\u2019t apply any more. No company is willing to cede control to a server in di\ufb00erent company; there\nare trade secrets and there\u2019s legal liability. The result in practice is that there\u2019s one messaging server\nper company, with hand-written bridges to connect it to messaging systems in other companies. The\nwhole ecosystem is thus heavily fragmented, and maintaining a large number of bridges for every\ncompany involved doesn\u2019t make the situation better. To solve this problem, we need a fully distributed\narchitecture, an architecture where every component can be possibly governed by a di\ufb00erent business\nentity. Given that the unit of management in server-based architecture is the server, we can solve the\nproblem by installing a separate server for each component. In such a case we can further optimize\nthe design by making the server and the component share the same processes. What we end up with\nis a messaging library. \u00d8MQ was started when we got an idea about how to make messaging work without a central\nserver. It required turning the whole concept of messaging upside down and replacing the model of\nan autonomous centralised store of messages in the center of the network with a \u201csmart endpoint,\ndumb network\u201d architecture based on the end-to-end principle. The technical consequence of that\ndecision was that \u00d8MQ, from the very beginning, was a library, not an application. In the meantime we\u2019ve been able to prove that this architecture is both more e\ufb03cient (lower\nlatency, higher throughput) and more \ufb02exible (it\u2019s easy to build arbitrary complex topologies instead\nof being tied to classic hub-and-spoke model).", "domains": ["Design Patterns", "Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 374", "position": 374, "chunk_type": "semantic", "token_estimate": 390}
{"text": "program; however, it\u2019s almost impossible to create a library from an existing executable. A library: o\ufb00ers much more \ufb02exibility to the users, at the same time sparing them non-trivial administrative\ne\ufb00ort. 24.2\nGlobal State\nGlobal variables don\u2019t play well with libraries. A library may be loaded several times in the process\nbut even then there\u2019s only a single set of global variables. Figure 24.1 shows a \u00d8MQ library being\nused from two di\ufb00erent and independent libraries. The application then uses both of those libraries. When such a situation occurs, both instances of \u00d8MQ access the same variables, resulting in\nrace conditions, strange failures and unde\ufb01ned behaviour. To prevent this problem, the \u00d8MQ library has no global variables. Instead, a user of the library\nis responsible for creating the global state explicitly. The object containing the global state is called\ncontext. While from the user\u2019s perspective context looks more or less like a pool of worker threads,\nfrom \u00d8MQ\u2019s perspective it\u2019s just an object to store any global state that we happen to need. In the\npicture above, libA would have its own context and libB would have its own as well. There would\nbe no way for one of them to break or subvert the other one. The lesson here is pretty obvious: Don\u2019t use global state in libraries. If you do, the library is\nlikely to break when it happens to be instantiated twice in the same process. 24.3\nPerformance\nWhen \u00d8MQ was started, its primary goal was to optimize performance. Performance of messaging\nsystems is expressed using two metrics: throughput\u2014how many messages can be passed during a\ngiven amount of time; and latency\u2014how long it takes for a message to get from one endpoint to the\nother. Which metric should we focus on? What\u2019s the relationship between the two? Isn\u2019t it obvious? Run the test, divide the overall time of the test by number of messages passed and what you get is\nlatency. Divide the number of messages by time and what you get is throughput. In other words,\nlatency is the inverse value of throughput. Trivial, right? Instead of starting coding straight away we spent some weeks investigating the performance\nmetrics in detail and we found out that the relationship between throughput and latency is much\nmore subtle than that, and often the metrics are quite counter-intuitive. Imagine A sending messages to B. (See Figure 24.2.) The overall time of the test is 6 seconds. There are 5 messages passed.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 375", "position": 375, "chunk_type": "semantic", "token_estimate": 413}
{"text": "program; however, it\u2019s almost impossible to create a library from an existing executable. A library: The overall time of the test is 6 seconds. There are 5 messages passed. Therefore the throughput is 0.83 msgs/sec ( 5\n6) and the latency is 1.2\nsec ( 6\n5), right? Have a look at the diagram again. It takes a di\ufb00erent time for each message to get from A to\nB: 2 sec, 2.5 sec, 3 sec, 3.5 sec, 4 sec. The average is 3 seconds, which is pretty far away from our\noriginal calculation of 1.2 second. This example shows the misconceptions people are intuitively\ninclined to make about performance metrics. Now have a look at the throughput. The overall time of the test is 6 seconds. However, at A it\ntakes just 2 seconds to send all the messages. From A\u2019s perspective the throughput is 2.5 msgs/sec\n( 5\n2). At B it takes 4 seconds to receive all messages. So from B\u2019s perspective the throughput is 1.25\nmsgs/sec ( 5\n4). Neither of these numbers matches our original calculation of 1.2 msgs/sec. To make a long story short, latency and throughput are two di\ufb00erent metrics; that much is obvious. The important thing is to understand the di\ufb00erence between the two and their mutual relationship. Martin S\u00fastrik\n361", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 375", "position": 375, "chunk_type": "semantic", "token_estimate": 218}
{"text": "Figure 24.2: Sending messages from A to B: Latency can be measured only between two di\ufb00erent points in the system; There\u2019s no such thing\nas latency at point A. Each message has its own latency. You can average the latencies of multiple\nmessages; however, there\u2019s no such thing as latency of a stream of messages. Throughput, on the other hand, can be measured only at a single point of the system. There\u2019s a\nthroughput at the sender, there\u2019s a throughput at the receiver, there\u2019s a throughput at any intermediate\npoint between the two, but there\u2019s no such thing as overall throughput of the whole system. And\nthroughput make sense only for a set of messages; there\u2019s no such thing as throughput of a single\nmessage. As for the relationship between throughput and latency, it turns out there really is a relationship;\nhowever, the formula involves integrals and we won\u2019t discuss it here. For more information, read the\nliterature on queueing theory. There are many more pitfalls in benchmarking the messaging systems that we won\u2019t go further\ninto. The stress should rather be placed on the lesson learned: Make sure you understand the problem\nyou are solving. Even a problem as simple as \u201cmake it fast\u201d can take lot of work to understand properly. What\u2019s more, if you don\u2019t understand the problem, you are likely to build implicit assumptions and\npopular myths into your code, making the solution either \ufb02awed or at least much more complex or\nmuch less useful than it could possibly be. 24.4\nCritical Path\nWe discovered during the optimization process that three factors have a crucial impact on performance:\n\u2022 Number of memory allocations\n\u2022 Number of system calls\n\u2022 Concurrency model\nHowever, not every memory allocation or every system call has the same e\ufb00ect on performance. The performance we are interested in in messaging systems is the number of messages we can transfer\nbetween two endpoints during a given amount of time. Alternatively, we may be interested in how\nlong it takes for a message to get from one endpoint to another. 362\nZeroMQ", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 376", "position": 376, "chunk_type": "semantic", "token_estimate": 348}
{"text": "However, given that \u00d8MQ is designed for scenarios with long-lived connections, the time it: takes to establish a connection or the time needed to handle a connection error is basically irrelevant. These events happen very rarely and so their impact on overall performance is negligible. The part of a codebase that gets used very frequently, over and over again, is called the critical\npath; optimization should focus on the critical path. Let\u2019s have a look at an example: \u00d8MQ is not extremely optimized with respect to memory alloca-\ntions. For example, when manipulating strings, it often allocates a new string for each intermediate\nphase of the transformation. However, if we look strictly at the critical path\u2014the actual message\npassing\u2014we\u2019ll \ufb01nd out that it uses almost no memory allocations. If messages are small, it\u2019s just one\nmemory allocation per 256 messages (these messages are held in a single large allocated memory\nchunk). If, in addition, the stream of messages is steady, without huge tra\ufb03c peaks, the number of\nmemory allocations on the critical path drops to zero (the allocated memory chunks are not returned\nto the system, but re-used over and over again). Lesson learned: optimize where it makes di\ufb00erence. Optimizing pieces of code that are not on\nthe critical path is wasted e\ufb00ort. 24.5\nAllocating Memory\nAssuming that all the infrastructure was initialised and a connection between two endpoints has been\nestablished, there\u2019s only one thing to allocate when sending a message: the message itself. Thus, to\noptimize the critical path we had to look into how messages are allocated and passed up and down\nthe stack. It\u2019s common knowledge in the high-performance networking \ufb01eld that the best performance is\nachieved by carefully balancing the cost of message allocation and the cost of message copying4. For\nsmall messages, copying is much cheaper than allocating memory. It makes sense to allocate no new\nmemory chunks at all and instead to copy the message to preallocated memory whenever needed. For large messages, on the other hand, copying is much more expensive than memory allocation. It makes sense to allocate the message once and pass a pointer to the allocated block, instead of\ncopying the data. This approach is called \u201czero-copy\u201d. Figure 24.3: Message copying (or not)\n\u00d8MQ handles both cases in a transparent manner. A \u00d8MQ message is represented by an opaque\nhandle. The content of very small messages is encoded directly in the handle. So making a copy of\n4For example, http://hal.inria.fr/docs/00/29/28/31/PDF/Open-MX-IOAT.pdf.", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 377", "position": 377, "chunk_type": "semantic", "token_estimate": 411}
{"text": "the handle actually copies the message data. When the message is larger, it\u2019s allocated in a separate: bu\ufb00er and the handle contains just a pointer to the bu\ufb00er. Making a copy of the handle doesn\u2019t result\nin copying the message data, which makes sense when the message is megabytes long (Figure 24.3). It should be noted that in the latter case the bu\ufb00er is reference-counted so that it can be referenced by\nmultiple handles without the need to copy the data. Lesson learned: When thinking about performance, don\u2019t assume there\u2019s a single best solution. It\nmay happen that there are several subclasses of the problem (e.g., small messages vs. large messages),\neach having its own optimal algorithm. 24.6\nBatching\nIt has already been mentioned that the sheer number of system calls in a messaging system can\nresult in a performance bottleneck. Actually, the problem is much more generic than that. There\u2019s a\nnon-trivial performance penalty associated with traversing the call stack and thus, when creating\nhigh-performance applications, it\u2019s wise to avoid as much stack traversing as possible. Figure 24.4: Sending four messages\nConsider Figure 24.4. To send four messages, you have to traverse the entire network stack\nfour times (i.e., \u00d8MQ, glibc, user/kernel space boundary, TCP implementation, IP implementation,\nEthernet layer, the NIC itself and back up the stack again). However, if you decide to join those messages into a single batch, there would be only one\ntraversal of the stack (Figure 24.5). The impact on message throughput can be overwhelming: up to\ntwo orders of magnitude, especially if the messages are small and hundreds of them can be packed\ninto a single batch. Figure 24.5: Batching messages\nOn the other hand, batching can have negative impact on latency. Let\u2019s take, for example, the\nwell-known Nagle\u2019s algorithm, as implemented in TCP. It delays the outbound messages for a certain\n364\nZeroMQ", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 378", "position": 378, "chunk_type": "semantic", "token_estimate": 311}
{"text": "amount of time and merges all the accumulated data into a single packet. Obviously, the end-to-end: latency of the \ufb01rst message in the packet is much worse than the latency of the last one. Thus, it\u2019s\ncommon for applications that need consistently low latency to switch Nagle\u2019s algorithm o\ufb00. It\u2019s even\ncommon to switch o\ufb00batching on all levels of the stack (e.g., NIC\u2019s interrupt coalescing feature). But again, no batching means extensive traversing of the stack and results in low message\nthroughput. We seem to be caught in a throughput versus latency dilemma. \u00d8MQ tries to deliver consistently low latencies combined with high throughput using the following\nstrategy: when message \ufb02ow is sparse and doesn\u2019t exceed the network stack\u2019s bandwidth, \u00d8MQ turns\nall the batching o\ufb00to improve latency. The trade-o\ufb00here is somewhat higher CPU usage\u2014we still\nhave to traverse the stack frequently. However, that isn\u2019t considered to be a problem in most cases. When the message rate exceeds the bandwidth of the network stack, the messages have to be\nqueued\u2014stored in memory till the stack is ready to accept them. Queueing means the latency is\ngoing to grow. If the message spends one second in the queue, end-to-end latency will be at least\none second. What\u2019s even worse, as the size of the queue grows, latencies will increase gradually. If\nthe size of the queue is not bound, the latency can exceed any limit. It has been observed that even though the network stack is tuned for lowest possible latency\n(Nagle\u2019s algorithm switched o\ufb00, NIC interrupt coalescing turned o\ufb00, etc.) latencies can still be\ndismal because of the queueing e\ufb00ect, as described above. In such situations it makes sense to start batching aggressively. There\u2019s nothing to lose as the\nlatencies are already high anyway. On the other hand, aggressive batching improves throughput and\ncan empty the queue of pending messages\u2014which in turn means the latency will gradually drop as\nthe queueing delay decreases. Once there are no outstanding messages in the queue, the batching\ncan be turned o\ufb00to improve the latency even further. One additional observation is that the batching should only be done on the topmost level. If the\nmessages are batched there, the lower layers have nothing to batch anyway, and so all the batching\nalgorithms underneath do nothing except introduce additional latency.", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 379", "position": 379, "chunk_type": "semantic", "token_estimate": 385}
{"text": "Finally, there\u2019s a context object (discussed in the previous sections but not shown on the diagram): that holds the global state and is accessible by all the sockets and all the asynchronous objects. 24.8\nConcurrency Model\nOne of the requirements for \u00d8MQ was to take advantage of multi-core boxes; in other words, to scale\nthe throughput linearly with the number of available CPU cores. Our previous experience with messaging systems showed that using multiple threads in a classic\nway (critical sections, semaphores, etc.) doesn\u2019t yield much performance improvement. In fact,\na multi-threaded version of a messaging system can be slower than a single-threaded one, even if\nmeasured on a multi-core box. Individual threads are simply spending too much time waiting for\neach other while, at the same time, eliciting a lot of context switching that slows the system down. Given these problems, we\u2019ve decided to go for a di\ufb00erent model. The goal was to avoid locking\nentirely and let each thread run at full speed. The communication between threads was to be provided\nvia asynchronous messages (events) passed between the threads. This, as insiders know, is the classic\nactor model. The idea was to launch one worker thread per CPU core\u2014having two threads sharing the same\ncore would only mean a lot of context switching for no particular advantage. Each internal \u00d8MQ\nobject, such as say, a TCP engine, would be tightly bound to a particular worker thread. That, in turn,\nmeans that there\u2019s no need for critical sections, mutexes, semaphores and the like. Additionally, these\n\u00d8MQ objects won\u2019t be migrated between CPU cores so would thus avoid the negative performance\nimpact of cache pollution (Figure 24.7). Figure 24.7: Multiple worker threads\nThis design makes a lot of traditional multi-threading problems disappear. Nevertheless, there\u2019s\na need to share the worker thread among many objects, which in turn means there has to be some\nkind of cooperative multitasking. This means we need a scheduler; objects need to be event-driven\nrather than being in control of the entire event loop; we have to take care of arbitrary sequences of\nevents, even very rare ones; we have to make sure that no object holds the CPU for too long; etc. In short, the whole system has to become fully asynchronous. No object can a\ufb00ord to do a\nblocking operation, because it would not only block itself but also all the other objects sharing the\nsame worker thread. All objects have to become, whether explicitly or implicitly, state machines.", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 381", "position": 381, "chunk_type": "semantic", "token_estimate": 415}
{"text": "Finally, there\u2019s a context object (discussed in the previous sections but not shown on the diagram): No object can a\ufb00ord to do a\nblocking operation, because it would not only block itself but also all the other objects sharing the\nsame worker thread. All objects have to become, whether explicitly or implicitly, state machines. Martin S\u00fastrik\n367", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 381", "position": 381, "chunk_type": "semantic", "token_estimate": 57}
{"text": "With hundreds or thousands of state machines running in parallel you have to take care of all the: possible interactions between them and\u2014most importantly\u2014of the shutdown process. It turns out that shutting down a fully asynchronous system in a clean way is a dauntingly complex\ntask. Trying to shut down a thousand moving parts, some of them working, some idle, some in the\nprocess of being initiated, some of them already shutting down by themselves, is prone to all kinds of\nrace conditions, resource leaks and similar. The shutdown subsystem is de\ufb01nitely the most complex\npart of \u00d8MQ. A quick check of the bug tracker indicates that some 30\u201350% of reported bugs are\nrelated to shutdown in one way or another. Lesson learned: When striving for extreme performance and scalability, consider the actor model;\nit\u2019s almost the only game in town in such cases. However, if you are not using a specialised system\nlike Erlang or \u00d8MQ itself, you\u2019ll have to write and debug a lot of infrastructure by hand. Additionally,\nthink, from the very beginning, about the procedure to shut down the system. It\u2019s going to be the\nmost complex part of the codebase and if you have no clear idea how to implement it, you should\nprobably reconsider using the actor model in the \ufb01rst place. 24.9\nLock-Free Algorithms\nLock-free algorithms have been in vogue lately. They are simple mechanisms for inter-thread\ncommunication that don\u2019t rely on the kernel-provided synchronisation primitives, such as mutexes\nor semaphores; rather, they do the synchronisation using atomic CPU operations, such as atomic\ncompare-and-swap (CAS). It should be understood that they are not literally lock-free\u2014instead,\nlocking is done behind the scenes on the hardware level. \u00d8MQ uses a lock-free queue in pipe objects to pass messages between the user\u2019s threads and\n\u00d8MQ\u2019s worker threads. There are two interesting aspects to how \u00d8MQ uses the lock-free queue. First, each queue has exactly one writer thread and exactly one reader thread. If there\u2019s a need\nfor 1-to-N communication, multiple queues are created (Figure 24.8). Given that this way the queue\ndoesn\u2019t have to take care of synchronising the writers (there\u2019s only one writer) or readers (there\u2019s\nonly one reader) it can be implemented in an extra-e\ufb03cient way.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 382", "position": 382, "chunk_type": "semantic", "token_estimate": 372}
{"text": "With hundreds or thousands of state machines running in parallel you have to take care of all the: If there\u2019s a need\nfor 1-to-N communication, multiple queues are created (Figure 24.8). Given that this way the queue\ndoesn\u2019t have to take care of synchronising the writers (there\u2019s only one writer) or readers (there\u2019s\nonly one reader) it can be implemented in an extra-e\ufb03cient way. Figure 24.8: Queues\nSecond, we realised that while lock-free algorithms were more e\ufb03cient than classic mutex-based\nalgorithms, atomic CPU operations are still rather expensive (especially when there\u2019s contention\nbetween CPU cores) and doing an atomic operation for each message written and/or each message\nread was slower than we were willing to accept. The way to speed it up\u2014once again\u2014was batching. Imagine you had 10 messages to be written\nto the queue. It can happen, for example, when you received a network packet containing 10 small\nmessages. Receiving a packet is an atomic event; you cannot get half of it. This atomic event results\nin the need to write 10 messages to the lock-free queue. There\u2019s not much point in doing an atomic\n368\nZeroMQ", "domains": ["Design Patterns"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 382", "position": 382, "chunk_type": "semantic", "token_estimate": 188}
{"text": "operation for each message. Instead, you can accumulate the messages in a \u201cpre-write\u201d portion of the: queue that\u2019s accessed solely by the writer thread, and then \ufb02ush it using a single atomic operation. The same applies to reading from the queue. Imagine the 10 messages above were already\n\ufb02ushed to the queue. The reader thread can extract each message from the queue using an atomic\noperation. However, it\u2019s overkill; instead, it can move all the pending messages to a \u201cpre-read\u201d\nportion of the queue using a single atomic operation. Afterwards, it can retrieve the messages from\nthe \u201cpre-read\u201d bu\ufb00er one by one. \u201cPre-read\u201d is owned and accessed solely by the reader thread and\nthus no synchronisation whatsoever is needed in that phase. Figure 24.9: Lock-free queue\nThe arrow on the left of Figure 24.9 shows how the pre-write bu\ufb00er can be \ufb02ushed to the queue\nsimply by modifying a single pointer. The arrow on the right shows how the whole content of the\nqueue can be shifted to the pre-read by doing nothing but modifying another pointer. Lesson learned: Lock-free algorithms are hard to invent, troublesome to implement and almost\nimpossible to debug. If at all possible, use an existing proven algorithm rather than inventing your\nown. When extreme performance is required, don\u2019t rely solely on lock-free algorithms. While they\nare fast, the performance can be signi\ufb01cantly improved by doing smart batching on top of them. 24.10", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 383", "position": 383, "chunk_type": "semantic", "token_estimate": 237}
{"text": "The user interface is the most important part of any product. It\u2019s the only part of your program visible: to the outside world and if you get it wrong the world will hate you. In end-user products it\u2019s either\nthe GUI or the command line interface. In libraries it\u2019s the API. In early versions of \u00d8MQ the API was based on AMQP\u2019s model of exchanges and queues6. I\nspent the end of 2009 rewriting it almost from scratch to use the BSD Socket API instead. That was\nthe turning point; \u00d8MQ adoption soared from that point on. While before it was a niche product\nused by a bunch of messaging experts, afterwards it became a handy commonplace tool for anybody. In a year or so the size of the community increased tenfold, some 20 bindings to di\ufb00erent languages\nwere implemented, etc. The user interface de\ufb01nes the perception of a product. With basically no change to the functionality\u2014\njust by changing the API\u2014\u00d8MQ changed from an \u201centerprise messaging\u201d product to a \u201cnetworking\u201d\nproduct. In other words, the perception changed from \u201ca complex piece of infrastructure for big\nbanks\u201d to \u201chey, this helps me to send my 10-byte-long message from application A to application B\u201d. 6See the AMQP speci\ufb01cation at http://www.amqp.org/specification/1.0/amqp-org-download. From a historical\nperspective it\u2019s interesting to have a look at the white paper from 2007 that tries to reconcile AMQP with a brokerless model\nof messaging. The white paper is at http://www.zeromq.org/whitepapers:messaging-enabled-network. Martin S\u00fastrik\n369", "domains": ["Design Patterns", "Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 383", "position": 383, "chunk_type": "semantic", "token_estimate": 245}
{"text": "Lesson learned: Understand what you want your project to be and design the user interface: accordingly. Having a user interface that doesn\u2019t align with the vision of the project is a 100%\nguaranteed way to fail. One of the important aspects of the move to the BSD Sockets API was that it wasn\u2019t a revolutionary\nfreshly invented API, but an existing and well-known one. Actually, the BSD Sockets API is one of\nthe oldest APIs still in active use today; it dates back to 1983 and 4.2BSD Unix. It\u2019s been widely\nused and stable for literally decades. The above fact brings a lot of advantages. Firstly, it\u2019s an API that everybody knows, so the\nlearning curve is ludicrously \ufb02at. Even if you\u2019ve never heard of \u00d8MQ, you can build your \ufb01rst\napplication in couple of minutes thanks to the fact that you are able to reuse your BSD Sockets\nknowledge. Secondly, using a widely implemented API enables integration of \u00d8MQ with existing technologies. For example, exposing \u00d8MQ objects as \u201csockets\u201d or \u201c\ufb01le descriptors\u201d allows for processing TCP,\nUDP, pipe, \ufb01le and \u00d8MQ events in the same event loop. Another example: the experimental project\nto bring \u00d8MQ-like functionality to the Linux kernel7 turned out to be pretty simple to implement. By sharing the same conceptual framework it can re-use a lot of infrastructure already in place. Thirdly and probably most importantly, the fact that the BSD Sockets API survived almost three\ndecades despite numerous attempts to replace it means that there is something inherently right in\nthe design. BSD Sockets API designers have\u2014whether deliberately or by chance\u2014made the right\ndesign decisions. By adopting the API we can automatically share those design decisions without\neven knowing what they were and what problem they were solving. Lesson learned: While code reuse has been promoted from time immemorial and pattern reuse\njoined in later on, it\u2019s important to think of reuse in an even more generic way. When designing a\nproduct, have a look at similar products. Check which have failed and which have succeeded; learn\nfrom the successful projects. Don\u2019t succumb to Not Invented Here syndrome. Reuse the ideas, the\nAPIs, the conceptual frameworks, whatever you \ufb01nd appropriate. By doing so you are allowing users\nto reuse their existing knowledge. At the same time you may be avoiding technical pitfalls you are\nnot even aware of at the moment.", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 384", "position": 384, "chunk_type": "semantic", "token_estimate": 396}
{"text": "Lesson learned: Understand what you want your project to be and design the user interface: By doing so you are allowing users\nto reuse their existing knowledge. At the same time you may be avoiding technical pitfalls you are\nnot even aware of at the moment. 24.11\nMessaging Patterns\nIn any messaging system, the most important design problem is that of how to provide a way for the\nuser to specify which messages are routed to which destinations. There are two main approaches,\nand I believe this dichotomy is quite generic and applicable to basically any problem encountered in\nthe domain of software. One approach is to adopt the Unix philosophy of \u201cdo one thing and do it well\u201d. What this means\nis that the problem domain should be arti\ufb01cially restricted to a small and well-understood area. The\nprogram should then solve this restricted problem in a correct and exhaustive way. An example of\nsuch approach in the messaging area is MQTT8. It\u2019s a protocol for distributing messages to a set of\nconsumers. It can\u2019t be used for anything else (say for RPC) but it is easy to use and does message\ndistribution well. The other approach is to focus on generality and provide a powerful and highly con\ufb01gurable\nsystem. AMQP is an example of such a system. Its model of queues and exchanges provides the\n7https://github.com/250bpm/linux-2.6\n8http://mqtt.org/\n370\nZeroMQ", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 384", "position": 384, "chunk_type": "semantic", "token_estimate": 230}
{"text": "user with the means to programmatically de\ufb01ne almost any routing algorithm they can think of. The: trade-o\ufb00, of course, is a lot of options to take care of. \u00d8MQ opts for the former model because it allows the resulting product to be used by basically\nanyone, while the generic model requires messaging experts to use it. To demonstrate the point, let\u2019s\nhave a look how the model a\ufb00ects the complexity of the API. What follows is implementation of\nRPC client on top of a generic system (AMQP):\nconnect (\"192.168.0.111\")\nexchange.declare (exchange=\"requests\", type=\"direct\", passive=false,\ndurable=true, no-wait=true, arguments={})\nexchange.declare (exchange=\"replies\", type=\"direct\", passive=false,\ndurable=true, no-wait=true, arguments={})\nreply-queue = queue.declare (queue=\"\", passive=false, durable=false,\nexclusive=true, auto-delete=true, no-wait=false, arguments={})\nqueue.bind (queue=reply-queue, exchange=\"replies\",\nrouting-key=reply-queue)\nqueue.consume (queue=reply-queue, consumer-tag=\"\", no-local=false,\nno-ack=false, exclusive=true, no-wait=true, arguments={})\nrequest = new-message (\"Hello World!\") request.reply-to = reply-queue\nrequest.correlation-id = generate-unique-id ()\nbasic.publish (exchange=\"requests\", routing-key=\"my-service\",\nmandatory=true, immediate=false)\nreply = get-message ()\nOn the other hand, \u00d8MQ splits the messaging landscape into so-called \u201cmessaging patterns\u201d. Examples of the patterns are \u201cpublish/subscribe\u201d, \u201crequest/reply\u201d or \u201cparallelised pipeline\u201d. Each\nmessaging pattern is completely orthogonal to other patterns and can be thought of as a separate tool. What follows is the re-implementation of the above application using \u00d8MQ\u2019s request/reply pattern. Note how all the option tweaking is reduced to the single step of choosing the right messaging pattern\n(\u201cREQ\u201d):\ns = socket (REQ)\ns.connect (\"tcp://192.168.0.111:5555\")\ns.send (\"Hello World!\") reply = s.recv ()\nUp to this point we\u2019ve argued that speci\ufb01c solutions are better than generic solutions. We want\nour solution to be as speci\ufb01c as possible. However, at the same time we want to provide our customers\nwith as wide a range of functionality as possible. How can we solve this apparent contradiction? The answer consists of two steps:\n1. De\ufb01ne a layer of the stack to deal with a particular problem area (e.g. transport, routing,\npresentation, etc.). 2. Provide multiple implementations of the layer. There should be a separate non-intersecting\nimplementation for each use case. Let\u2019s have a look at the example of the transport layer in the Internet stack. It\u2019s meant to provide\nservices such as transferring data streams, applying \ufb02ow control, providing reliability, etc., on the\ntop of the network layer (IP). It does so by de\ufb01ning multiple non-intersecting solutions: TCP for\nconnection-oriented reliable stream transfer, UDP for connectionless unreliable packet transfer, SCTP\nfor transfer of multiple streams, DCCP for unreliable connections and so on. Martin S\u00fastrik\n371", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 385", "position": 385, "chunk_type": "semantic", "token_estimate": 401}
{"text": "Note that each implementation is completely orthogonal: a UDP endpoint cannot speak to: a TCP endpoint. Neither can a SCTP endpoint speak to a DCCP endpoint. It means that new\nimplementations can be added to the stack at any moment without a\ufb00ecting the existing portions of\nthe stack. Conversely, failed implementations can be forgotten and discarded without compromising\nthe viability of the transport layer as a whole. The same principle applies to messaging patterns as de\ufb01ned by \u00d8MQ. Messaging patterns\nform a layer (the so-called \u201cscalability layer\u201d) on top of the transport layer (TCP and friends). Individual messaging patterns are implementations of this layer. They are strictly orthogonal\u2014the\npublish/subscribe endpoint can\u2019t speak to the request/reply endpoint, etc. Strict separation between\nthe patterns in turn means that new patterns can be added as needed and that failed experiments with\nnew patterns won\u2019t hurt the existing patterns. Lesson learned: When solving a complex and multi-faceted problem it may turn out that a\nmonolithic general-purpose solution may not be the best way to go. Instead, we can think of the\nproblem area as an abstract layer and provide multiple implementations of this layer, each focused on\na speci\ufb01c well-de\ufb01ned use case. When doing so, delineate the use case carefully. Be sure about what\nis in the scope and what is not. By restricting the use case too aggressively the application of your\nsoftware may be limited. If you de\ufb01ne the problem too broadly, however, the product may become\ntoo complex, blurry and confusing for the users. 24.12\nConclusion\nAs our world becomes populated with lots of small computers connected via the Internet\u2014mobile\nphones, RFID readers, tablets and laptops, GPS devices, etc.\u2014the problem of distributed computing\nceases to be the domain of academic science and becomes a common everyday problem for every\ndeveloper to tackle. The solutions, unfortunately, are mostly domain-speci\ufb01c hacks. This article\nsummarises our experience with building a large-scale distributed system in a systematic manner. It\nfocuses on problems that are interesting from a software architecture point of view, and we hope that\ndesigners and programmers in the open source community will \ufb01nd it useful. 372\nZeroMQ", "domains": ["Software Quality Attributes"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 386", "position": 386, "chunk_type": "semantic", "token_estimate": 355}
{"text": "Bibliography: [aut12]\nAutoconf, 2012. [CKP05]\nManuel M. T. Chakravarty, Gabriele Keller, and Simon Peyton Jones. Associated type\nsynonyms. In Proceedings of the Tenth ACM SIGPLAN International Conference on\nFunctional Programming, ICFP \u201905, pages 241\u2013253. ACM, 2005. [ecl12]\nEclipse home page, 2012. [Ent11a]\nEnthought. Kiva source code, 2011. [Ent11b]\nEnthought. PDF reference and Adobe extensions to the PDF speci\ufb01cation, 2011. [For09]\nMessage Passing Interface Forum. MPI: A Message-Passing Interface Standard,\nVersion 2.2, September 2009. [GFB+04]\nEdgar Gabriel, Graham E. Fagg, George Bosilca, Thara Angskun, Jack J. Dongarra,\nJe\ufb00rey M. Squyres, Vishal Sahay, Prabhanjan Kambadur, Brian Barrett, Andrew Lums-\ndaine, Ralph H. Castain, David J. Daniel, Richard L. Graham, and Timothy S. Woodall. Open MPI: Goals, concept, and design of a next generation MPI implementation. In\nProc. 11th European PVM/MPI Users\u2019 Group Meeting, pages 97\u2013104, September\n2004. [GGHL+96] Al Geist, William Gropp, Steve Huss-Lederman, Andrew Lumsdaine, Ewin Lusk,\nWilliam Saphir, Tony Skjellum, and Mark Snir. MPI-2: Extending the Message-\nPassing Interface. In Euro-Par \u201996 Parallel Processing, pages 128\u2013135. Springer\nVerlag, 1996. [GLP93]\nA. Gill, J. Launchbury, and S. L. Peyton Jones. A Short Cut to Deforestation. In\nACM Conference on Functional Programming and Computer Architecture (FPCA\u201993). ACM, 1993. [HHPW07]\nPaul Hudak, John Hughes, Simon Peyton Jones, and Philip Wadler. A History of\nHaskell: being lazy with class. In Proceedings of the Third ACM SIGPLAN Conference\non the History of Programming Languages, pages 12\u20131\u201312\u201355. ACM, 2007. [Knu86]\nDonald E. Knuth. Computers & Typesetting B: TeX: The Program. Addison Wesley,\n1986. [Mar10]\nThe Haskell 2010 Report, 2010.\nhttp://www.haskell.org/haskellwiki/\nLanguage_and_library_specification#The_Haskell_2010_report.", "domains": ["Domain-Driven Design", "Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 387", "position": 387, "chunk_type": "semantic", "token_estimate": 255}
{"text": "[McG07]: Paul McGuire. Getting Started with Pyparsing. O\u2019Reilly Media, 2007. [Mes93]\nMessage Passing Interface Forum. MPI: A Message Passing Interface. In Proc. Supercomputing \u201993, pages 878\u2013883. IEEE Computer Society Press, November 1993. [MHJP08]\nSimon Marlow, Tim Harris, Roshan P. James, and Simon Peyton Jones. Parallel\ngenerational-copying garbage collection with a block-structured heap. In Proceedings\nof the 7th International Symposium on Memory Management, ISMM \u201908, pages 11\u201320. ACM, 2008. [MPT04]\nSimon Marlow, Simon Peyton Jones, and Wolfgang Thaller. Extending the Haskell\nForeign Function Interface with Concurrency. In Proceedings of the ACM SIGPLAN\nWorkshop on Haskell, pages 57\u201368, September 2004. [PM02]\nSimon Peyton Jones and Simon Marlow. Secrets of the Glasgow Haskell Compiler\ninliner. Journal of Functional Programming, 12:393\u2013434, July 2002. [PTH01]\nSimon L. Peyton Jones, Andrew Tolmach, and Tony Hoare. Playing by the rules:\nrewriting as a practical optimisation technique in GHC. In Ralf Hinze, editor, 2001\nHaskell Workshop. ACM SIGPLAN, September 2001. [PVWW06] Simon Peyton Jones, Dimitrios Vytiniotis, Stephanie Weirich, and Geo\ufb00rey Washburn. Simple uni\ufb01cation-based type inference for GADTs. In Proceedings of the Eleventh\nACM SIGPLAN International Conference on Functional Programming, pages 50\u201361. ACM, 2006. [Ray]\nWhy GPSes suck, and what to do about it. http://esr.ibiblio.org/?p=801. [SCPD07]\nMartin Sulzmann, Manuel Chakravarty, Simon Peyton Jones, and Kevin Donnelly. System F with type equality coercions. In ACM SIGPLAN International Workshop on\nTypes in Language Design and Implementation (TLDI\u201907). ACM, 2007. [She06]\nMaxim Shemanarev. Anti-Grain Geometry: A high quality rendering engine for C++,\n2002-2006. [SPS+00]\nRichard M. Stallman, Roland Pesch, Stan Shebs, et al. Debugging with GDB: The\nGNU Source-Level Debugger. Free Software Foundation, 2000. 374", "domains": ["Code Organization"], "source": "ArchitectureOfOpenSourceApp2.pdf", "section": "Page 388", "position": 388, "chunk_type": "semantic", "token_estimate": 263}
