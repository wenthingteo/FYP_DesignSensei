{"text": "Praise for Software Architecture: The Hard Parts: \u201cThis book provides the missing manual around building microservices and analyzing\nthe nuances of architectural decisions throughout the whole tech stack. In this book, you\nget a catalog of architectural decisions you can make when building your distributed\nsystem and what are the pros and cons associated with each decision. This book is a must\nfor every architect that is building modern distributed systems.\u201d\n\u2014Aleksandar Serafimoski, Lead Consultant, Thoughtworks\n\u201cIt\u2019s a must-read for technologists who are passionate about architecture. Great articulation of patterns.\u201d\n\u2014Vanya Seth, Head Of Tech, Thoughtworks India\n\u201cWhether you\u2019re an aspiring architect or an experienced one leading a team, no\nhandwaving, this book will guide you through the specifics of how to succeed in your\njourney to create enterprise applications and microservices.\u201d\n\u2014Dr. Venkat Subramaniam,\nAward-winning Author and Founder of Agile Developer, Inc.\n\u201cSoftware Architecture: The Hard Parts provides the reader with valuable insight, practices,\nand real-world examples on pulling apart highly coupled systems and building them back\nup again. By gaining effective trade-off analysis skills, you will start\nto make better architecture decisions.\u201d\n\u2014Joost van Wenen,\nManaging Partner & Cofounder, Infuze Consulting", "domains": ["Architectural Patterns and Styles"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 3", "position": 3, "chunk_type": "semantic", "token_estimate": 193}
{"text": "\u201cI loved reading this comprehensive body of work on distributed architectures! A great: mix of solid discussions on fundamental concepts, together with tons of practical advice.\u201d\n\u2014David Kloet, Independent Software Architect\n\u201cSplitting a big ball of mud is no easy work. Starting from the code and getting to the data,\nthis book will help you see the services that should be extracted and the\nservices that should remain together.\u201d\n\u2014Rub\u00e9n D\u00edaz-Mart\u00ednez, Software Developer at Codesai\n\u201cThis book will equip you with the theoretical background and with a practical\nframework to help answer the most difficult questions faced in\nmodern software architecture.\u201d\n\u2014James Lewis, Technical Director, Thoughtworks", "domains": ["Design Principles"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 4", "position": 4, "chunk_type": "semantic", "token_estimate": 106}
{"text": "3. Architectural Modularity. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  45: Modularity Drivers                                                                                                        49\nMaintainability                                                                                                           50\nTestability                                                                                                                    54\nDeployability                                                                                                               55\nScalability                                                                                                                     56\nAvailability/Fault Tolerance                                                                                      58\nSysops Squad Saga: Creating a Business Case                                                           59\n4. Architectural Decomposition. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\nIs the Codebase Decomposable? 65\nAfferent and Efferent Coupling                                                                               66\nAbstractness and Instability                                                                                      67\nDistance from the Main Sequence                                                                           69\nComponent-Based Decomposition                                                                             71\nTactical Forking                                                                                                              73\nTrade-Offs                                                                                                                   77\nSysops Squad Saga: Choosing a Decomposition Approach                                     78\n5. Component-Based Decomposition Patterns. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\nIdentify and Size Components Pattern                                                                       84\nPattern Description                                                                                                    84\nFitness Functions for Governance                                                                           87\nSysops Squad Saga: Sizing Components                                                                 90\nGather Common Domain Components Pattern                                                       94\nPattern Description                                                                                                    94\nFitness Functions for Governance                                                                           95\nSysops Squad Saga: Gathering Common Components                                        97\nFlatten Components Pattern                                                                                      101\nPattern Description                                                                                                  102\nFitness Functions for Governance                                                                         107\nSysops Squad Saga: Flattening Components                                                        107\nDetermine Component Dependencies Pattern                                                       111\nPattern Description                                                                                                  112\nFitness Functions for Governance                                                                         117\nSysops Squad Saga: Identifying Component Dependencies                              118\nCreate Component Domains Pattern                                                                       120\nPattern Description                                                                                                  121\nFitness Functions for Governance                                                                         122\nSysops Squad Saga: Creating Component Domains                                           123\nCreate Domain Services Pattern                                                                                126\nPattern Description                                                                                                  126\nvi \n| \nTable of Contents", "domains": ["Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 8", "position": 8, "chunk_type": "semantic", "token_estimate": 314}
{"text": "Fitness Functions for Governance                                                                         129: Sysops Squad Saga: Creating Domain Services                                                   129\nSummary                                                                                                                       130\n6. Pulling Apart Operational Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131\nData Decomposition Drivers                                                                                     132\nData Disintegrators                                                                                                  133\nData Integrators                                                                                                        146\nSysops Squad Saga: Justifying Database Decomposition                                   150\nDecomposing Monolithic Data                                                                                 151\nStep 1: Analyze Database and Create Data Domains                                          156\nStep 2: Assign Tables to Data Domains                                                                156\nStep 3: Separate Database Connections to Data Domains                                 158\nStep 4: Move Schemas to Separate Database Servers                                          159\nStep 5: Switch Over to Independent Database Servers                                       161\nSelecting a Database Type                                                                                           161\nRelational Databases                                                                                                163\nKey-Value Databases                                                                                                165\nDocument Databases                                                                                               167\nColumn Family Databases                                                                                      169\nGraph Databases                                                                                                      171\nNewSQL Databases                                                                                                  173\nCloud Native Databases                                                                                          175\nTime-Series Databases                                                                                             177\nSysops Squad Saga: Polyglot Databases                                                                    179\n7. Service Granularity. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185\nGranularity Disintegrators                                                                                         188\nService Scope and Function                                                                                    189\nCode Volatility                                                                                                          191\nScalability and Throughput                                                                                    192\nFault Tolerance                                                                                                         193\nSecurity                                                                                                                      195\nExtensibility                                                                                                              196\nGranularity Integrators                                                                                               197\nDatabase Transactions                                                                                             198\nWorkflow and Choreography                                                                                 200\nShared Code                                                                                                              203\nData Relationships                                                                                                   205\nFinding the Right Balance                                                                                          208\nSysops Squad Saga: Ticket Assignment Granularity                                               209\nSysops Squad Saga: Customer Registration Granularity                                        212\nTable of Contents \n| \nvii", "domains": ["Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 9", "position": 9, "chunk_type": "semantic", "token_estimate": 286}
{"text": "Part II.: Putting Things Back Together\n8. Reuse Patterns. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219\nCode Replication                                                                                                          221\nWhen to Use                                                                                                             223\nShared Library                                                                                                              223\nDependency Management and Change Control                                                 224\nVersioning Strategies                                                                                               225\nWhen To Use                                                                                                            227\nShared Service                                                                                                              228\nChange Risk                                                                                                              229\nPerformance                                                                                                              231\nScalability                                                                                                                  232\nFault Tolerance                                                                                                         232\nWhen to Use                                                                                                             234\nSidecars and Service Mesh                                                                                          234\nWhen to Use                                                                                                             239\nSysops Squad Saga: Common Infrastructure Logic                                                239\nCode Reuse: When Does It Add Value? 242\nReuse via Platforms                                                                                                  244\nSysops Squad Saga: Shared Domain Functionality                                                 244\n9. Data Ownership and Distributed Transactions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249\nAssigning Data Ownership                                                                                        250\nSingle Ownership Scenario                                                                                        251\nCommon Ownership Scenario                                                                                  252\nJoint Ownership Scenario                                                                                           253\nTable Split Technique                                                                                              254\nData Domain Technique                                                                                         256\nDelegate Technique                                                                                                  258\nService Consolidation Technique                                                                              261\nData Ownership Summary                                                                                         262\nDistributed Transactions                                                                                            263\nEventual Consistency Patterns                                                                                   267\nBackground Synchronization Pattern                                                                   269\nOrchestrated Request-Based Pattern                                                                     272\nEvent-Based Pattern                                                                                                 277\nSysops Squad Saga: Data Ownership for Ticket Processing                                  279\n10. Distributed Data Access. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283\nInterservice Communication Pattern                                                                       285\nColumn Schema Replication Pattern                                                                        287\nviii \n| \nTable of Contents", "domains": ["Design Principles", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 10", "position": 10, "chunk_type": "semantic", "token_estimate": 311}
{"text": "Replicated Caching Pattern                                                                                        288: Data Domain Pattern                                                                                                  293\nSysops Squad Saga: Data Access for Ticket Assignment                                        295\n11. Managing Distributed Workflows. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299\nOrchestration Communication Style                                                                        301\nChoreography Communication Style                                                                       306\nWorkflow State Management                                                                                 311\nTrade-Offs Between Orchestration and Choreography                                         315\nState Owner and Coupling                                                                                     315\nSysops Squad Saga: Managing Workflows                                                               317\n12. Transactional Sagas. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323\nTransactional Saga Patterns                                                                                        324\nEpic Saga(sao) Pattern                                                                                                325\nPhone Tag Saga(sac) Pattern                                                                                      330\nFairy Tale Saga(seo) Pattern                                                                                       333\nTime Travel Saga(sec) Pattern                                                                                   336\nFantasy Fiction Saga(aao) Pattern                                                                             340\nHorror Story(aac) Pattern                                                                                          343\nParallel Saga(aeo) Pattern                                                                                           346\nAnthology Saga(aec) Pattern                                                                                     349\nState Management and Eventual Consistency                                                         351\nSaga State Machines                                                                                                 352\nTechniques for Managing Sagas                                                                                356\nSysops Squad Saga: Atomic Transactions and Compensating Updates               358\n13. Contracts. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365\nStrict Versus Loose Contracts                                                                                    367\nTrade-Offs Between Strict and Loose Contracts                                                 370\nContracts in Microservices                                                                                     372\nStamp Coupling                                                                                                           376\nOver-Coupling via Stamp Coupling                                                                      376\nBandwidth                                                                                                                 377\nStamp Coupling for Workflow Management                                                       378\nSysops Squad Saga: Managing Ticketing Contracts                                               379\n14. Managing Analytical Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381\nPrevious Approaches                                                                                                   382\nThe Data Warehouse                                                                                               382\nThe Data Lake                                                                                                           386\nThe Data Mesh                                                                                                             389\nTable of Contents \n| \nix", "domains": ["Architectural Patterns and Styles", "Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 11", "position": 11, "chunk_type": "semantic", "token_estimate": 391}
{"text": "Architects shouldn\u2019t constantly seek out silver-bullet solutions to their problems; they: are as rare now as in 1986, when Fred Brooks coined the term:\nThere is no single development, in either technology or management technique, which\nby itself promises even one order of magnitude [tenfold] improvement within a decade\nin productivity, in reliability, in simplicity. \u2014Fred Brooks from \u201cNo Silver Bullet\u201d\nBecause virtually every problem presents novel challenges, the real job of an architect\nlies in their ability to objectively determine and assess the set of trade-offs on either\nside of a consequential decision to resolve it as well as possible. The authors don\u2019t talk\nabout \u201cbest solutions\u201d (in this book or in the real world) because \u201cbest\u201d implies that\nan architect has managed to maximize all the possible competing factors within the\ndesign. Instead, our tongue-in-cheek advice is as follows:\nDon\u2019t try to find the best design in software architecture; instead,\nstrive for the least worst combination of trade-offs. Often, the best design an architect can create is the least worst collection of trade-offs\n\u2014no single architecture characteristics excels as it would alone, but the balance of all\nthe competing architecture characteristics promote project success. Which begs the question: \u201cHow can an architect find the least worst combination of\ntrade-offs (and document them effectively)?\u201d This book is primarily about decision\nmaking, enabling architects to make better decisions when confronted with novel\nsituations. Why \u201cThe Hard Parts\u201d? Why did we call this book Software Architecture: The Hard Parts? Actually, the \u201chard\u201d\nin the title performs double duty. First, hard connotes difficult, and architects con\u2010\nstantly face difficult problems that literally (and figuratively) no one has faced before,\ninvolving numerous technology decisions with long-term implications layered on top\nof the interpersonal and political environment where the decision must take place. Second, hard connotes solidity\u2014just as in the separation of hardware and software,\nthe hard one should change much less because it provides the foundation for the soft\nstuff. Similarly, architects discuss the distinction between architecture and design,\nwhere the former is structural and the latter is more easily changed. Thus, in this\nbook, we talk about the foundational parts of architecture. 2 \n| \nChapter 1: What Happens When There Are No \u201cBest Practices\u201d?", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Design Principles", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 20", "position": 20, "chunk_type": "semantic", "token_estimate": 372}
{"text": "The definition of software architecture itself has provided many hours of non-: productive conversation among its practitioners. One favorite tongue-in-cheek defi\u2010\nnition is that \u201csoftware architecture is the stuff that\u2019s hard to change later.\u201d That stuff is\nwhat our book is about. Giving Timeless Advice About Software Architecture\nThe software development ecosystem constantly and chaotically shifts and grows. Topics that were all the rage a few years ago have either been subsumed by the ecosys\u2010\ntem and disappeared or replaced by something different/better. For example, 10 years\nago, the predominant architecture style for large enterprises was orchestration-\ndriven, service-oriented architecture. Now, virtually no one builds in that architecture\nstyle anymore (for reasons we\u2019ll uncover along the way); the current favored style for\nmany distributed systems is microservices. How and why did that transition happen? When architects look at a particular style (especially a historical one), they must con\u2010\nsider the constraints in place that lead to that architecture becoming dominant. At the\ntime, many companies were merging to become enterprises, with all the attendant\nintegration woes that come with that transition. Additionally, open source wasn\u2019t a\nviable option (often for political rather than technical reasons) for large companies. Thus, architects emphasized shared resources and centralized orchestration as a\nsolution. However, in the intervening years, open source and Linux became viable alternatives,\nmaking operating systems commercially free. However, the real tipping point occur\u2010\nred when Linux became operationally free with the advent of tools like Puppet and\nChef, which allowed development teams to programmatically spin up their environ\u2010\nments as part of an automated build. Once that capability arrived, it fostered an archi\u2010\ntectural revolution with microservices and the quickly emerging infrastructure of\ncontainers and orchestration tools like Kubernetes. This illustrates that the software development ecosystem expands and evolves in\ncompletely unexpected ways. One new capability leads to another one, which unex\u2010\npectedly creates new capabilities. Over the course of time, the ecosystem completely\nreplaces itself, one piece at a time. This presents an age-old problem for authors of books about technology generally\nand software architecture specifically\u2014how can we write something that isn\u2019t old\nimmediately? We don\u2019t focus on technology or other implementation details in this book. Rather,\nwe focus on how architects make decisions, and how to objectively weigh trade-offs\nwhen presented with novel situations.", "domains": ["Architectural Patterns and Styles"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 21", "position": 21, "chunk_type": "semantic", "token_estimate": 385}
{"text": "The Importance of Data in Architecture: Data is a precious thing and will last longer than the systems themselves. \u2014Tim Berners-Lee\nFor many in architecture, data is everything. Every enterprise building any system\nmust deal with data, as it tends to live much longer than systems or architecture,\nrequiring diligent thought and design. However, many of the instincts of data archi\u2010\ntects to build tightly coupled systems create conflicts within modern distributed\narchitectures. For example, architects and DBAs must ensure that business data sur\u2010\nvives the breaking apart of monolith systems and that the business can still derive\nvalue from its data regardless of architecture undulations. It has been said that data is the most important asset in a company. Businesses want to\nextract value from the data that they have and are finding new ways to deploy data in\ndecision making. Every part of the enterprise is now data driven, from servicing exist\u2010\ning customers, to acquiring new customers, increasing customer retention, improv\u2010\ning products, predicting sales, and other trends. This reliance on data means that all\nsoftware architecture is in the service of data, ensuring the right data is available and\nusable by all parts of the enterprise. The authors built many distributed systems a few decades ago when they first became\npopular, yet decision making in modern microservices seems more difficult, and we\nwanted to figure out why. We eventually realized that, back in the early days of dis\u2010\ntributed architecture, we mostly still persisted data in a single relational database. However, in microservices and the philosophical adherence to a bounded context\nfrom Domain-Driven Design, as a way of limiting the scope of implementation detail\ncoupling, data has moved to an architectural concern, along with transactionality. Many of the hard parts of modern architecture derive from tensions between data\nand architecture concerns, which we untangle in both Part I and Part II. One important distinction that we cover in a variety of chapters is the separation\nbetween operational versus analytical data:\nOperational data\nData used for the operation of the business, including sales, transactional data,\ninventory, and so on. This data is what the company runs on\u2014if something inter\u2010\nrupts this data, the organization cannot function for very long. This type of data\nis defined as Online Transactional Processing (OLTP), which typically involves\ninserting, updating, and deleting data in a database. 4 \n| \nChapter 1: What Happens When There Are No \u201cBest Practices\u201d?", "domains": ["Architectural Patterns and Styles", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 22", "position": 22, "chunk_type": "semantic", "token_estimate": 406}
{"text": "Analytical data: Data used by data scientists and other business analysts for predictions, trending,\nand other business intelligence. This data is typically not transactional and often\nnot relational\u2014it may be in a graph database or snapshots in a different format\nthan its original transactional form. This data isn\u2019t critical for the day-to-day\noperation but rather for the long-term strategic direction and decisions. We cover the impact of both operational and analytical data throughout the book. Architectural Decision Records\nOne of the most effective ways of documenting architecture decisions is through\nArchitectural Decision Records (ADRs). ADRs were first evangelized by Michael\nNygard in a blog post and later marked as \u201cadopt\u201d in the Thoughtworks Technology\nRadar. An ADR consists of a short text file (usually one to two pages long) describing\na specific architecture decision. While ADRs can be written using plain text, they are\nusually written in some sort of text document format like AsciiDoc or Markdown. Alternatively, an ADR can also be written using a wiki page template. We devoted an\nentire chapter to ADRs in our previous book, Fundamentals of Software Architecture\n(O\u2019Reilly). We will be leveraging ADRs as a way of documenting various architecture decisions\nmade throughout the book. For each architecture decision, we will be using the fol\u2010\nlowing ADR format with the assumption that each ADR is approved:\nADR: A short noun phrase containing the architecture decision\nContext\nIn this section of the ADR we will add a short one- or two-sentence description of the\nproblem, and list the alternative solutions. Decision\nIn this section we will state the architecture decision and provide a detailed justifica\u2010\ntion of the decision. Consequences\nIn this section of the ADR we will describe any consequences after the decision is\napplied, and also discuss the trade-offs that were considered. A list of all the Architectural Decision Records created in this book can be found in\nAppendix B.\nDocumenting a decision is important for an architect, but governing the proper use\nof the decision is a separate topic. Fortunately, modern engineering practices allow\nautomating many common governance concerns by using architecture fitness\nfunctions. Architectural Decision Records \n| \n5", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 23", "position": 23, "chunk_type": "semantic", "token_estimate": 358}
{"text": "Architecture Fitness Functions: The DevOps revolution followed a similar course. As Linux and other open source\nsoftware became \u201cgood enough\u201d for enterprises, combined with the advent of tools\nthat allowed programmatic definition of (eventually) virtual machines, operations\npersonnel realized they could automate machine definitions and many other repeti\u2010\ntive tasks. In both cases, advances in technology and insights led to automating a recurring job\nthat was handled by an expensive role\u2014which describes the current state of architec\u2010\nture governance in most organizations. For example, if an architect chooses a particu\u2010\nlar architecture style or communication medium, how can they make sure that a\n6 \n| \nChapter 1: What Happens When There Are No \u201cBest Practices\u201d?", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 24", "position": 24, "chunk_type": "semantic", "token_estimate": 115}
{"text": "developer implements it correctly? When done manually, architects perform code: reviews or perhaps hold architecture review boards to assess the state of governance. However, just as in manually configuring computers in operations, important details\ncan easily fall through superficial reviews. Using Fitness Functions\nIn the 2017 book Building Evolutionary Architectures (O\u2019Reilly), the authors (Neal\nFord, Rebecca Parsons, and Patrick Kua) defined the concept of an architectural fit\u2010\nness function: any mechanism that performs an objective integrity assessment of some\narchitecture characteristic or combination of architecture characteristics. Here is a\npoint-by-point breakdown of that definition:\nAny mechanism\nArchitects can use a wide variety of tools to implement fitness functions; we will\nshow numerous examples throughout the book. For example, dedicated testing\nlibraries exist to test architecture structure, architects can use monitors to test\noperational architecture characteristics such as performance or scalability, and\nchaos engineering frameworks test reliability and resiliency. Objective integrity assessment\nOne key enabler for automated governance lies with objective definitions for\narchitecture characteristics. For example, an architect can\u2019t specify that they want\na \u201chigh performance\u201d website; they must provide an object value that can be\nmeasured by a test, monitor, or other fitness function. Architects must watch out for composite architecture characteristics\u2014ones that\naren\u2019t objectively measurable but are really composites of other measurable\nthings. For example, \u201cagility\u201d isn\u2019t measurable, but if an architect starts pulling\nthe broad term agility apart, the goal is for teams to be able to respond quickly\nand confidently to change, either in ecosystem or domain. Thus, an architect can\nfind measurable characteristics that contribute to agility: deployability, testability,\ncycle time, and so on. Often, the lack of ability to measure an architecture char\u2010\nacteristic indicates too vague a definition. If architects strive toward measurable\nproperties, it allows them to automate fitness function application. Some architecture characteristic or combination of architecture characteristics\nThis characteristic describes the two scopes for fitness functions:\nAtomic\nThese fitness functions handle a single architecture characteristic in isola\u2010\ntion. For example, a fitness function that checks for component cycles within\na codebase is atomic in scope. Architecture Fitness Functions \n| \n7", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 25", "position": 25, "chunk_type": "semantic", "token_estimate": 350}
{"text": "Holistic: Holistic fitness functions validate a combination of architecture characteris\u2010\ntics. A complicating feature of architecture characteristics is the synergy they\nsometimes exhibit with other architecture characteristics. For example, if an\narchitect wants to improve security, a good chance exists that it will affect\nperformance. Similarly, scalability and elasticity are sometimes at odds\u2014\nsupporting a large number of concurrent users can make handling sudden\nbursts more difficult. Holistic fitness functions exercise a combination of\ninterlocking architecture characteristics to ensure that the combined effect\nwon\u2019t negatively affect the architecture. An architect implements fitness functions to build protections around unexpected\nchange in architecture characteristics. In the Agile software development world,\ndevelopers implement unit, functional, and user acceptance tests to validate different\ndimensions of the domain design. However, until now, no similar mechanism existed\nto validate the architecture characteristics part of the design. In fact, the separation\nbetween fitness functions and unit tests provides a good scoping guideline for archi\u2010\ntects. Fitness functions validate architecture characteristics, not domain criteria; unit\ntests are the opposite. Thus, an architect can decide whether a fitness function or unit\ntest is needed by asking the question: \u201cIs any domain knowledge required to execute\nthis test?\u201d If the answer is \u201cyes,\u201d then a unit/function/user acceptance test is appropri\u2010\nate; if \u201cno,\u201d then a fitness function is needed. For example, when architects talk about elasticity, it\u2019s the ability of the application to\nwithstand a sudden burst of users. Notice that the architect doesn\u2019t need to know any\ndetails about the domain\u2014this could be an ecommerce site, an online game, or some\u2010\nthing else. Thus, elasticity is an architectural concern and within the scope of a fitness\nfunction. If on the other hand the architect wanted to validate the proper parts of a\nmailing address, that is covered via a traditional test. Of course, this separation isn\u2019t\npurely binary\u2014some fitness functions will touch on the domain and vice versa, but\nthe differing goals provide a good way to mentally separate them. Here are a couple of examples to make the concept less abstract. One common architect goal is to maintain good internal structural integrity in the\ncodebase. However, malevolent forces work against the architect\u2019s good intentions on\nmany platforms.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 26", "position": 26, "chunk_type": "semantic", "token_estimate": 369}
{"text": "Holistic: One common architect goal is to maintain good internal structural integrity in the\ncodebase. However, malevolent forces work against the architect\u2019s good intentions on\nmany platforms. For example, when coding in any popular Java or .NET development\nenvironment, as soon as a developer references a class not already imported, the IDE\nhelpfully presents a dialog asking the developer if they would like to auto-import the\nreference. This occurs so often that most programmers develop the habit of swatting\nthe auto-import dialog away like a reflex action. However, arbitrarily importing classes or components among one another spells dis\u2010\naster for modularity. For example, Figure 1-1 illustrates a particularly damaging anti-\npattern that architects aspire to avoid. 8 \n| \nChapter 1: What Happens When There Are No \u201cBest Practices\u201d?", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 26", "position": 26, "chunk_type": "semantic", "token_estimate": 128}
{"text": "Figure 1-1. Cyclic dependencies between components: In this anti-pattern, each component references something in the others. Having a\nnetwork of components such as this damages modularity because a developer cannot\nreuse a single component without also bringing the others along. And, of course, if\nthe other components are coupled to other components, the architecture tends more\nand more toward the Big Ball of Mud anti-pattern. How can architects govern this\nbehavior without constantly looking over the shoulders of trigger-happy developers? Code reviews help but happen too late in the development cycle to be effective. If an\narchitect allows a development team to rampantly import across the codebase for a\nweek until the code review, serious damage has already occurred in the codebase. The solution to this problem is to write a fitness function to avoid component cycles,\nas shown in Example 1-1. Example 1-1. Fitness function to detect component cycles\npublic class CycleTest {\n    private JDepend jdepend;\n    @BeforeEach\n    void init() {\n \n  jdepend = new JDepend();\n \n  jdepend.addDirectory(\"/path/to/project/persistence/classes\");\n \n  jdepend.addDirectory(\"/path/to/project/web/classes\");\n \n  jdepend.addDirectory(\"/path/to/project/thirdpartyjars\");\n    }\n    @Test\n    void testAllPackages() {\n \n  Collection packages = jdepend.analyze();\n \n  assertEquals(\"Cycles exist\", false, jdepend.containsCycles());\n    }\n}\nIn the code, an architect uses the metrics tool JDepend to check the dependencies\nbetween packages. The tool understands the structure of Java packages and fails the\ntest if any cycles exist. An architect can wire this test into the continuous build on a\nproject and stop worrying about the accidental introduction of cycles by trigger-\nhappy developers. This is a great example of a fitness function guarding the impor\u2010\nArchitecture Fitness Functions \n| \n9", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 27", "position": 27, "chunk_type": "semantic", "token_estimate": 258}
{"text": "tant rather than urgent practices of software development: it\u2019s an important concern: for architects, yet has little impact on day-to-day coding. Example 1-1 shows a very low-level, code-centric fitness function. Many popular\ncode hygiene tools (such as SonarQube) implement many common fitness functions\nin a turnkey manner. However, architects may also want to validate the macro struc\u2010\nture of the architecture as well as the micro. When designing a layered architecture\nsuch as the one in Figure 1-2, the architect defines the layers to ensure separation of\nconcerns. Figure 1-2. Traditional layered architecture\nHowever, how can the architect ensure that developers will respect these layers? Some\ndevelopers may not understand the importance of the patterns, while others may\nadopt a \u201cbetter to ask forgiveness than permission\u201d attitude because of some overrid\u2010\ning local concern, such as performance. But allowing implementers to erode the rea\u2010\nsons for the architecture hurts the long-term health of the architecture. ArchUnit allows architects to address this problem via a fitness function, shown in\nExample 1-2. Example 1-2. ArchUnit fitness function to govern layers\nlayeredArchitecture()\n    .layer(\"Controller\").definedBy(\"..controller..\")\n    .layer(\"Service\").definedBy(\"..service..\")\n    .layer(\"Persistence\").definedBy(\"..persistence..\")\n    .whereLayer(\"Controller\").mayNotBeAccessedByAnyLayer()\n    .whereLayer(\"Service\").mayOnlyBeAccessedByLayers(\"Controller\")\n    .whereLayer(\"Persistence\").mayOnlyBeAccessedByLayers(\"Service\")\n10 \n| \nChapter 1: What Happens When There Are No \u201cBest Practices\u201d?", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 28", "position": 28, "chunk_type": "semantic", "token_estimate": 197}
{"text": "In Example 1-2, the architect defines the desirable relationship between layers and: writes a verification fitness function to govern it. This allows an architect to establish\narchitecture principles outside the diagrams and other informational artifacts, and\nverify them on an ongoing basis. A similar tool in the .NET space, NetArchTest, allows similar tests for that platform. A layer verification in C# appears in Example 1-3. Example 1-3. NetArchTest for layer dependencies\n// Classes in the presentation should not directly reference repositories\nvar result = Types.InCurrentDomain()\n    .That()\n    .ResideInNamespace(\"NetArchTest.SampleLibrary.Presentation\")\n    .ShouldNot()\n    .HaveDependencyOn(\"NetArchTest.SampleLibrary.Data\")\n    .GetResult()\n    .IsSuccessful;\nTools continue to appear in this space with increasing degrees of sophistication. We\nwill continue to highlight many of these techniques as we illustrate fitness functions\nalongside many of our solutions. Finding an objective outcome for a fitness function is critical. However, objective\ndoesn\u2019t imply static. Some fitness functions will have noncontextual return values,\nsuch as true/false or a numeric value such as a performance threshold. However,\nother fitness functions (deemed dynamic) return a value based on some context. For\nexample, when measuring scalability, architects measure the number of concurrent\nusers and also generally measure the performance for each user. Often, architects\ndesign systems so that as the number of users goes up, performance per user declines\nslightly\u2014but doesn\u2019t fall off a cliff. Thus, for these systems, architects design perfor\u2010\nmance fitness functions that take into account the number of concurrent users. As\nlong as the measure of an architecture characteristic is objective, architects can test it. While most fitness functions should be automated and run continually, some will\nnecessarily be manual. A manual fitness function requires a person to handle the vali\u2010\ndation. For example, for systems with sensitive legal information, a lawyer may need\nto review changes to critical parts to ensure legality, which cannot be automated. Most deployment pipelines support manual stages, allowing teams to accommodate\nmanual fitness functions. Ideally, these are run as often as reasonably possible\u2014a vali\u2010\ndation that doesn\u2019t run can\u2019t validate anything. Teams execute fitness functions either\non demand (rarely) or as part of a continuous integration work stream (most\ncommon). To fully achieve the benefit of validations such as fitness functions, they\nshould be run continually. Architecture Fitness Functions \n| \n11", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 29", "position": 29, "chunk_type": "semantic", "token_estimate": 370}
{"text": "Continuity is important, as illustrated in this example of enterprise-level governance: using fitness functions. Consider the following scenario: what does a company do\nwhen a zero-day exploit is discovered in one of the development frameworks or libra\u2010\nries the enterprise uses? If it\u2019s like most companies, security experts scour projects to\nfind the offending version of the framework and make sure it\u2019s updated, but that pro\u2010\ncess is rarely automated, relying on many manual steps. This isn\u2019t an abstract ques\u2010\ntion; this exact scenario affected a major financial institution described in The\nEquifax Data Breach. Like the architecture governance described previously, manual\nprocesses are error prone and allow details to escape. The Equifax Data Breach\nOn September 7, 2017, Equifax, a major credit scoring agency in the US, announced\nthat a data breach had occurred. Ultimately, the problem was traced to a hacking\nexploit of the popular Struts web framework in the Java ecosystem (Apache Struts\nvCVE-2017-5638). The foundation issued a statement announcing the vulnerability\nand released a patch on March 7, 2017. The Department of Homeland Security con\u2010\ntacted Equifax and similar companies the next day, warning them of this problem,\nand they ran scans on March 15, 2017, which didn\u2019t reveal all of the affected systems. Thus, the critical patch wasn\u2019t applied to many older systems until July 29, 2017, when\nEquifax\u2019s security experts identified the hacking behavior that lead to the data breach. Imagine an alternative world in which every project runs a deployment pipeline, and\nthe security team has a \u201cslot\u201d in each team\u2019s deployment pipeline where they can\ndeploy fitness functions. Most of the time, these will be mundane checks for safe\u2010\nguards like preventing developers from storing passwords in databases and similar\nregular governance chores. However, when a zero-day exploit appears, having the\nsame mechanism in place everywhere allows the security team to insert a test in every\nproject that checks for a certain framework and version number; if it finds the dan\u2010\ngerous version, it fails the build and notifies the security team. Teams configure\ndeployment pipelines to awaken for any change to the ecosystem: code, database\nschema, deployment configuration, and fitness functions. This allows enterprises to\nuniversally automate important governance tasks. Fitness functions provide many benefits for architects, not the least of which is the\nchance to do some coding again! One of the universal complaints among architects is\nthat they don\u2019t get to code much anymore\u2014but fitness functions are often code!", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 30", "position": 30, "chunk_type": "semantic", "token_estimate": 410}
{"text": "Continuity is important, as illustrated in this example of enterprise-level governance: Fitness functions provide many benefits for architects, not the least of which is the\nchance to do some coding again! One of the universal complaints among architects is\nthat they don\u2019t get to code much anymore\u2014but fitness functions are often code! By\nbuilding an executable specification of the architecture, which anyone can validate\nanytime by running the project\u2019s build, architects must understand the system and its\nongoing evolution well, which overlaps with the core goal of keeping up with the\ncode of the project as it grows. 12 \n| \nChapter 1: What Happens When There Are No \u201cBest Practices\u201d?", "domains": ["Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 30", "position": 30, "chunk_type": "semantic", "token_estimate": 110}
{"text": "Third, if we start down the path of implementing all the varieties of options we show,: this would be the longest book ever written. Focus on architecture principles allows\nus to keep things as generic as they can be. To keep subjects as grounded in architecture as possible, we use the simplest defini\u2010\ntions possible for key concepts. For example, coupling in architecture can fill entire\nbooks (and it has). To that end, we use the following simple, verging on simplistic,\ndefinitions:\nService\nIn colloquial terms, a service is a cohesive collection of functionality deployed as\nan independent executable. Most of the concepts we discuss with regard to serv\u2010\nices apply broadly to distributed architectures, and specifically microservices\narchitectures. In the terms we define in Chapter 2, a service is part of an architecture quantum,\nwhich includes further definitions of both static and dynamic coupling between\nservices and other quanta. Coupling\nTwo artifacts (including services) are coupled if a change in one might require a\nchange in the other to maintain proper functionality. Component\nAn architectural building block of the application that does some sort of business\nor infrastructure function, usually manifested through a package structure (Java),\nnamespace (C#), or a physical grouping of source code files within some sort of\ndirectory structure. For example, the component Order History might be imple\u2010\nmented through a set of class files located in the namespace app.busi\nness.order.history. Synchronous communication\nTwo artifacts communicate synchronously if the caller must wait for the response\nbefore proceeding. Asynchronous communication\nTwo artifacts communicate asynchronously if the caller does not wait for the\nresponse before proceeding. Optionally, the caller can be notified by the receiver\nthrough a separate channel when the request has completed. Orchestrated coordination\nA workflow is orchestrated if it includes a service whose primary responsibility is\nto coordinate the workflow. 14 \n| \nChapter 1: What Happens When There Are No \u201cBest Practices\u201d?", "domains": ["Architectural Patterns and Styles", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 32", "position": 32, "chunk_type": "semantic", "token_estimate": 317}
{"text": "Choreographed coordination: A workflow is choreographed when it lacks an orchestrator; rather, the services\nin the workflow share the coordination responsibilities of the workflow. Atomicty\nA workflow is atomic if all parts of the workflow maintain a consistent state at all\ntimes; the opposite is represented by the spectrum of eventual consistency, cov\u2010\nered in Chapter 6. Contract\nWe use the term contract broadly to define the interface between two software\nparts, which may encompass method or function calls, integration architecture\nremote calls, dependencies, and so on. Anywhere two pieces of software join, a\ncontract is involved. Software architecture is by its nature abstract: we cannot know what unique combi\u2010\nnation of platforms, technologies, commercial software, and the other dizzying array\nof possibilities our readers might have, except that no two are exactly alike. We cover\nmany abstract ideas, but must ground them with some implementation details to\nmake them concrete. To that end, we need a problem to illustrate architecture con\u2010\ncepts against\u2014which leads us to the Sysops Squad. Introducing the Sysops Squad Saga\nsaga\nA long story of heroic achievement. \u2014Oxford English Dictionary\nWe discuss a number of sagas in this book, both literal and figurative. Architects have\nco-opted the term saga to describe transactional behavior in distributed architectures\n(which we cover in detail in Chapter 12). However, discussions about architecture\ntend to become abstract, especially when considering abstract problems such as the\nhard parts of architecture. To help solve this problem and provide some real-world\ncontext for the solutions we discuss, we kick off a literal saga about the Sysops Squad. We use the Sysops Squad saga within each chapter to illustrate the techniques and\ntrade-offs described in this book. While many books on software architecture cover\nnew development efforts, many real-world problems exist within existing systems. Therefore, our story starts with the existing Sysops Squad architecture highlighted\nhere. Penultimate Electronics is a large electronics giant that has numerous retail stores\nthroughout the country. When customers buy computers, TVs, stereos, and other\nelectronic equipment, they can choose to purchase a support plan. When problems\nIntroducing the Sysops Squad Saga \n| \n15", "domains": ["Design Patterns", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 33", "position": 33, "chunk_type": "semantic", "token_estimate": 354}
{"text": "occur, customer-facing technology experts (the Sysops Squad) come to the customer\u2019s: residence (or work office) to fix problems with the electronic device. The four main users of the Sysops Squad ticketing application are as follows:\nAdministrator\nThe administrator maintains the internal users of the system, including the list of\nexperts and their corresponding skill set, location, and availability. The adminis\u2010\ntrator also manages all of the billing processing for customers using the system,\nand maintains static reference data (such as supported products, name-value\npairs in the system, and so on). Customer\nThe customer registers for the Sysops Squad service and maintains their cus\u2010\ntomer profile, support contracts, and billing information. Customers enter prob\u2010\nlem tickets into the system, and also fill out surveys after the work has been\ncompleted. Sysops Squad expert\nExperts are assigned problem tickets and fix problems based on the ticket. They\nalso interact with the knowledge base to search for solutions to customer prob\u2010\nlems and enter notes about repairs. Manager\nThe manager keeps track of problem ticket operations and receives operational\nand analytical reports about the overall Sysops Squad problem ticket system. Nonticketing Workflow\nThe nonticketing workflows include those actions that administrators, managers, and\ncustomers perform that do not relate to a problem ticket. These workflows are out\u2010\nlined as follows:\n1. Sysops Squad experts are added and maintained in the system through an\nadministrator, who enters in their locale, availability, and skills. 2. Customers register with the Sysops Squad system and have multiple support\nplans based on the products they purchased. 3. Customers are automatically billed monthly based on credit card information\ncontained in their profile. Customers can view billing history and statements\nthrough the system. 4. Managers request and receive various operational and analytical reports, includ\u2010\ning financial reports, expert performance reports, and ticketing reports. 16 \n| \nChapter 1: What Happens When There Are No \u201cBest Practices\u201d?", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 34", "position": 34, "chunk_type": "semantic", "token_estimate": 314}
{"text": "Ticketing Workflow: The ticketing workflow starts when a customer enters a problem ticket into the sys\u2010\ntem, and ends when the customer completes the survey after the repair is done. This\nworkflow is outlined as follows:\n1. Customers who have purchased the support plan enter a problem ticket by using\nthe Sysops Squad website. 2. Once a problem ticket is entered in the system, the system then determines\nwhich Sysops Squad expert would be the best fit for the job based on skills, cur\u2010\nrent location, service area, and availability. 3. Once assigned, the problem ticket is uploaded to a dedicated custom mobile app\non the Sysops Squad expert\u2019s mobile device. The expert is also notified via a text\nmessage that they have a new problem ticket. 4. The customer is notified through an SMS text message or email (based on their\nprofile preference) that the expert is on their way. 5. The expert uses the custom mobile application on their phone to retrieve the\nticket information and location. The Sysops Squad expert can also access a\nknowledge base through the mobile app to find out what has been done in the\npast to fix the problem. 6. Once the expert fixes the problem, they mark the ticket as \u201ccomplete.\u201d The sysops\nsquad expert can then add information about the problem and repair the knowl\u2010\nedge base. 7. After the system receives notification that the ticket is complete, it sends an email\nto the customer with a link to a survey, which the customer then fills out. 8. The system receives the completed survey from the customer and records the\nsurvey information. A Bad Scenario\nThings have not been good with the Sysops Squad problem ticket application lately. The current trouble ticket system is a large monolithic application that was developed\nmany years ago. Customers are complaining that consultants are never showing up\nbecause of lost tickets, and often the wrong consultant shows up to fix something\nthey know nothing about. Customers have also been complaining that the system is\nnot always available to enter new problem tickets. Change is also difficult and risky in this large monolith. Whenever a change is made,\nit usually takes too long and something else usually breaks.", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 35", "position": 35, "chunk_type": "semantic", "token_estimate": 373}
{"text": "Ticketing Workflow: Change is also difficult and risky in this large monolith. Whenever a change is made,\nit usually takes too long and something else usually breaks. Because of reliability\nissues, the Sysops Squad system frequently \u201cfreezes up,\u201d or crashes, resulting in all\napplication functionality not being available anywhere from five minutes to two\nhours while the problem is identified and the application restarted. Introducing the Sysops Squad Saga \n| \n17", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 35", "position": 35, "chunk_type": "semantic", "token_estimate": 71}
{"text": "If something isn\u2019t done soon, Penultimate Electronics will be forced to abandon the: very lucrative support contract business line and lay off all the Sysops Squad adminis\u2010\ntrators, experts, managers, and IT development staff\u2014including the architects. Sysops Squad Architectural Components\nThe monolithic system for the Sysops Squad application handles ticket management,\noperational reporting, customer registration, and billing, as well as general adminis\u2010\ntrative functions such as user maintenance, login, and expert skills and profile main\u2010\ntenance. Figure 1-3 and the corresponding Table 1-1 illustrate and describe the\ncomponents of the existing monolithic application (the ss. part of the namespace\nspecifies the Sysops Squad application context). Figure 1-3. Components within the existing Sysops Squad application\n18 \n| \nChapter 1: What Happens When There Are No \u201cBest Practices\u201d?", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 36", "position": 36, "chunk_type": "semantic", "token_estimate": 126}
{"text": "Table 1-1. Existing Sysops Squad components: Component\nNamespace\nResponsibility\nLogin\nss.login\nInternal user and customer login and security logic\nBilling payment\nss.billing.payment\nCustomer monthly billing and customer credit card info\nBilling history\nss.billing.history\nPayment history and prior billing statements\nCustomer notification\nss.customer.notifica\ntion\nNotify customer of billing, general info\nCustomer profile\nss.customer.profile\nMaintain customer profile, customer registration\nExpert profile\nss.expert.profile\nMaintain expert profile (name, location, skills, etc.) KB maint\nss.kb.maintenance\nMaintain and view items in the knowledge base\nKB search\nss.kb.search\nQuery engine for searching the knowledge base\nReporting\nss.reporting\nAll reporting (experts, tickets, financial)\nTicket\nss.ticket\nTicket creation, maintenance, completion, common code\nTicket assign\nss.ticket.assign\nFind an expert and assign the ticket\nTicket notify\nss.ticket.notify\nNotify customer that the expert is on their way\nTicket route\nss.ticket.route\nSend the ticket to the expert\u2019s mobile device app\nSupport contract\nss.supportcontract\nSupport contracts for customers, products in the plan\nSurvey\nss.survey\nMaintain surveys, capture and record survey results\nSurvey notify\nss.survey.notify\nSend survey email to customer\nSurvey templates\nss.survey.templates\nMaintain various surveys based on type of service\nUser maintenance\nss.users\nMaintain internal users and roles\nThese components will be used in subsequent chapters to illustrate various techni\u2010\nques and trade-offs when dealing with breaking applications into distributed\narchitectures. Sysops Squad Data Model\nThe Sysops Squad application with its various components listed in Table 1-1 uses a\nsingle schema in the database to host all its tables and related database code. The\ndatabase is used to persist customers, users, contracts, billing, payments, knowledge\nbase, and customer surveys; the tables are listed in Table 1-2, and the ER model is\nillustrated in Figure 1-4. Introducing the Sysops Squad Saga \n| \n19", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 37", "position": 37, "chunk_type": "semantic", "token_estimate": 278}
{"text": "Figure 1-4. Data model within the existing Sysops Squad application: Table 1-2. Existing Sysops Squad database tables\nTable\nResponsibility\nCustomer\nEntities needing Sysops support\nCustomer_Notification\nNotification preferences for customers\nSurvey\nA survey for after-support customer satisfaction\nQuestion\nQuestions in a survey\nSurvey_Question\nA question is assigned to the survey\nSurvey_Administered\nSurvey question is assigned to customer\nSurvey_Response\nA customer\u2019s response to the survey\nBilling\nBilling information for support contract\nContract\nA contract between an entity and Sysops for support\nPayment_Method\nPayment methods supported for making payment\nPayment\nPayments processed for billings\n20 \n| \nChapter 1: What Happens When There Are No \u201cBest Practices\u201d?", "domains": ["Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 38", "position": 38, "chunk_type": "semantic", "token_estimate": 103}
{"text": "Table: Responsibility\nSysOps_User\nThe various users in Sysops\nProfile\nProfile information for Sysops users\nExpert_Profile\nProfiles of experts\nExpertise\nVarious expertise within Sysops\nLocation\nLocations served by the expert\nArticle\nArticles for the knowledge base\nTag\nTags on articles\nKeyword\nKeyword for an article\nArticle_Tag\nTags associated to articles\nArticle_Keyword\nJoin table for keywords and articles\nTicket\nSupport tickets raised by customers\nTicket_Type\nDifferent types of tickets\nTicket_History\nThe history of support tickets\nThe Sysops data model is a standard third normal form data model with only a few\nstored procedures or triggers. However, a fair number of views exist that are mainly\nused by the Reporting component. As the architecture team tries to break up the\napplication and move toward distributed architecture, it will have to work with the\ndatabase team to accomplish the tasks at the database level. This setup of database\ntables and views will be used throughout the book to discuss various techniques and\ntrade-offs to accomplish the task of breaking apart the database. Introducing the Sysops Squad Saga \n| \n21", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 39", "position": 39, "chunk_type": "semantic", "token_estimate": 175}
{"text": "PART I: Pulling Things Apart\nAs many of us discovered when we were children, a great way to understand how\nsomething fits together is to first pull it apart. To understand complex subjects (such\nas trade-offs in distributed architectures), an architect must figure out where to start\nuntangling. In the book What Every Programmer Should Know About Object-Oriented Design\n(Dorset House), Meilir Page-Jones made the astute observation that coupling in\narchitecture may be split into static and dynamic coupling. Static coupling refers to\nthe way architectural parts (classes, components, services, and so on) are wired\ntogether: dependencies, coupling degree, connection points, and so on. An architect\ncan often measure static coupling at compile time as it represents the static depen\u2010\ndencies within the architecture. Dynamic coupling refers to how architecture parts call one another: what kind of\ncommunication, what information is passed, strictness of contracts, and so on. Our goal is to investigate how to do trade-off analysis in distributed architectures; to\ndo that, we must pull the moving pieces apart so that we can discuss them in isolation\nto understand them fully before putting them back together. Part I primarily deals with architectural structure, how things are statically coupled\ntogether. In Chapter 2, we tackle the problem of defining the scope of static and\ndynamic coupling in architectures, and present the entire picture that we must pull\napart to understand. Chapter 3 begins that process, defining modularity and separa\u2010\ntion in architecture. Chapter 4 provides tools to evaluate and deconstruct codebases,\nand Chapter 5 supplies patterns to assist the process.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 41", "position": 41, "chunk_type": "semantic", "token_estimate": 261}
{"text": "CHAPTER 2: Discerning Coupling in\nSoftware Architecture\nWednesday, November 3, 13:00\nLogan, the lead architect for Penultimate Electronics, interrupted a small group of archi-\ntects in the cafeteria, discussing distributed architectures. \u201cAusten, are you wearing a cast\nagain?\u201d\n\u201cNo, it\u2019s just a splint,\u201d replied Austen. \u201cI sprained my wrist playing extreme disc golf over\nthe weekend\u2014it\u2019s almost healed.\u201d\n\u201cWhat is\u2026never mind. What is this impassioned conversation I barged in on?\u201d\n\u201cWhy wouldn\u2019t someone always choose the saga pattern in microservices to wire together transac-\ntions?\u201d asked Austen. \u201cThat way, architects can make the services as small as they want.\u201d\n\u201cBut don\u2019t you have to use orchestration with sagas?\u201d asked Addison. \u201cWhat about times when we\nneed asynchronous communication? And, how complex will the transactions get? If we break things\ndown too much, can we really guarantee data fidelity?\u201d\n\u201cYou know,\u201d said Austen, \u201cif we use an enterprise service bus, we can get it to manage most of that\nstuff for us.\u201d\n\u201cI thought no one used ESBs anymore\u2014shouldn\u2019t we use Kafka for stuff like that?\u201d\n\u201cThey aren\u2019t even the same thing!\u201d said Austen. Logan interrupted the increasingly heated conversation. \u201cIt is an apples-to-oranges comparison, but\nnone of these tools or approaches is a silver bullet. Distributed architectures like microservices are\ndifficult, especially if architects cannot untangle all the forces at play. What we need is an approach\nor framework that helps us figure out the hard problems in our architecture.\u201d\n25", "domains": ["Architectural Patterns and Styles"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 43", "position": 43, "chunk_type": "semantic", "token_estimate": 241}
{"text": "\u201cWell,\u201d said Addison, \u201cwhatever we do, it has to be as decoupled as possible\u2014everything I\u2019ve read: says that architects should embrace decoupling as much as possible.\u201d\n\u201cIf you follow that advice,\u201d said Logan, \u201cEverything will be so decoupled that nothing can communi-\ncate with anything else\u2014it\u2019s hard to build software that way! Like a lot of things, coupling isn\u2019t inher-\nently bad; architects just have to know how to apply it appropriately. In fact, I remember a famous\nquote about that from a Greek philosopher\u2026.\u201d\nAll things are poison, and nothing is without poison; the dosage alone makes it so a\nthing is not a poison. \u2014Paracelsus\nOne of the most difficult tasks an architect will face is untangling the various forces\nand trade-offs at play in distibuted architectures. People who provide advice con\u2010\nstantly extol the benefits of \u201cloosely coupled\u201d systems, but how can architects design\nsystems where nothing connects to anything else? Architects design fine-grained\nmicroservices to achieve decoupling, but then orchestration, transactionality, and\nasynchronicity become huge problems. Generic advice says \u201cdecouple,\u201d but provides\nno guidelines for how to achieve that goal while still constructing useful systems. Architects struggle with granularity and communication decisions because there are\nno clear universal guides for making decisions\u2014no best practices exist that can apply\nto real-world complex systems. Until now, architects lacked the correct perspective\nand terminology to allow a careful analysis that could determine the best (or least\nworst) set of trade-offs on a case-by-case basis. Why have architects struggled with decisions in distributed architectures? After all,\nwe\u2019ve been building distributed systems since the last century, using many of the\nsame mechanisms (message queues, events, and so on). Why has the complexity ram\u2010\nped up so much with microservices? The answer lies with the fundamental philosophy of microservices, inspired by the\nidea of a bounded context. Building services that model bounded contexts required a\nsubtle but important change to the way architects designed distributed systems\nbecause now transactionality is a first-class architectural concern. In many of the dis\u2010\ntributed systems architects designed prior to microservices, event handlers typically\nconnected to a single relational database, allowing it to handle details such as integ\u2010\nrity and transactions. Moving the database within the service boundary moves data\nconcerns into architecture concerns. As we\u2019ve said before, \u201cSoftware architecture\u201d is the stuff you can\u2019t Google answers for. A\nskill that modern architects must build is the ability to do trade-off analysis.", "domains": ["Architectural Patterns and Styles", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 44", "position": 44, "chunk_type": "semantic", "token_estimate": 405}
{"text": "This book focuses on how architects can perform trade-off analysis for any number: of scenarios unique to their situation. As in many things in architecture, the advice is\nsimple; the hard parts lie in the details, particularly how difficult parts become entan\u2010\ngled, making it difficult to see and understand the individual parts, as illustrated in\nFigure 2-1. Figure 2-1. A braid entangles hair, making the individual strands hard to identify\nWhen architects look at entangled problems, they struggle with performing trade-off\nanalysis because of the difficulties separating the concerns, so that they may consider\nthem independently. Thus, the first step in trade-off analysis is untangle the dimen\u2010\nsions of the problem, analyzing what parts are coupled to one another and what\nimpact that coupling has on change. For this purpose, we use the simplest definition\nof the word coupling:\nCoupling\nTwo parts of a software system are coupled if a change in one might cause a\nchange in the other. Often, software architecture creates multidimensional problems, where multiple\nforces all interact in interdependent ways. To analyze trade-offs, an architect must\nfirst determine what forces need to trade off with each other. Thus, here\u2019s our advice for modern trade-off analysis in software architecture:\n1. Find what parts are entangled together. 2. Analyze how they are coupled to one another. 3. Assess trade-offs by determining the impact of change on interdependent\nsystems. While the steps are simple, the hard parts lurk in the details. Thus, to illustrate this\nframework in practice, we take one of the most difficult (and probably the closest to\ngeneric) problems in distributed architectures, which is related to microservices:\nDiscerning Coupling in Software Architecture \n| \n27", "domains": ["Architectural Patterns and Styles"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 45", "position": 45, "chunk_type": "semantic", "token_estimate": 279}
{"text": "How do architects determine the size and communication styles for microservices?: Determining the proper size for microservices seems a pervasive problem\u2014too-\nsmall services create transactional and orchestration issues, and too-large serv\u2010\nices create scale and distribution issues. To that end, the remainder of this book untangles the many aspects to consider when\nanswering the preceding question. We provide new terminology to differentiate simi\u2010\nlar but distinct patterns and show practical examples of applying these and other\npatterns. However, the overarching goal of this book is to provide you with example-driven\ntechniques to learn how to construct your own trade-off analysis for the unique prob\u2010\nlems within your realm. We start with our first great untangling of forces in dis\u2010\ntributed architectures: defining architecture quantum along with the two types of\ncoupling, static and dynamic. Architecture (Quantum | Quanta)\nThe term quantum is, of course, used heavily in the field of physics known as quan\u2010\ntum mechanics. However, the authors chose the word for the same reasons physicists\ndid. Quantum originated from the Latin word quantus, meaning \u201chow great\u201d or \u201chow\nmany.\u201d Before physics co-opted it, the legal profession used it to represent the\n\u201crequired or allowed amount\u201d (for example, in damages paid). The term also appears\nin the mathematics field of topology, concerning the properties of families of shapes. Because of its Latin roots, the singular is quantum, and the plural is quanta, similar to\nthe datum/data symmetry. An architecture quantum measures several aspects of both topology and behavior in\nsoftware architecture related to how parts connect and communicate with one\nanother:\nArchitecture quantum\nAn architecture quantum is an independently deployable artifact with high func\u2010\ntional cohesion, high static coupling, and synchronous dynamic coupling. A\ncommon example of an architecture quantum is a well-formed microservice\nwithin a workflow. Static coupling\nRepresents how static dependencies resolve within the architecture via contracts. These dependencies include operating system, frameworks, and/or libraries\ndelivered via transitive dependency management, and any other operational\nrequirement to allow the quantum to operate. 28 \n| \nChapter 2: Discerning Coupling in Software Architecture", "domains": ["Architectural Patterns and Styles"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 46", "position": 46, "chunk_type": "semantic", "token_estimate": 343}
{"text": "Dynamic coupling: Represents how quanta communicate at runtime, either synchronously or asyn\u2010\nchronously. Thus, fitness functions for these characteristics must be continuous,\ntypically utilizing monitors. Even though both static and dynamic coupling seem similar, architects must distin\u2010\nguish two important differences. An easy way to think about the difference is that\nstatic coupling describes how services are wired together, whereas dynamic coupling\ndescribes how services call one another at runtime. For example, in a microservices\narchitecture, a service must contain dependent components such as a database, repre\u2010\nsenting static coupling\u2014the service isn\u2019t operational without the necessary data. That\nservice may call other services during the course of a workflow, which represents\ndynamic coupling. Neither service requires the other to be present to function, except\nfor this runtime workflow. Thus, static coupling analyzes operational dependencies,\nand dynamic coupling analyzes communication dependencies. These definitions include important characteristics; let\u2019s cover each in detail as they\ninform most of the examples in the book. Independently Deployable\nIndependently deployable implies several aspects of an architecture quantum\u2014each\nquantum represents a separate deployable unit within a particular architecture. Thus,\na monolithic architecture\u2014one that is deployed as a single unit\u2014is by definition a\nsingle architecture quantum. Within a distributed architecture such as microservices,\ndevelopers tend toward the ability to deploy services independently, often in a highly\nautomated way. Thus, from an independently deployable standpoint, a service within\na microservices architecture represents an architecture quantum (contingent on cou\u2010\npling\u2014as discussed next). Making each architecture quantum represent a deployable asset within the architec\u2010\nture serves several useful purposes. First, the boundary represented by an architec\u2010\nture quantum serves as a useful common language among architects, developers, and\noperations. Each understands the common scope under question: architects under\u2010\nstand the coupling characteristics, developers understand the scope of behavior, and\nthe operations team understands the deployable characteristics. Second, the architecture quantum represents one of the forces (static coupling) archi\u2010\ntects must consider when striving for proper granularity of services within a dis\u2010\ntributed architecture. Often, in microservices architectures, developers face the\ndifficult question of what service granularity offers the optimum set of trade-offs. Some of those trade-offs revolve around deployability: what release cadence does this\nservice require, what other services might be affected, what engineering practices are\ninvolved, and so on. Architects benefit from a firm understanding of exactly where\nArchitecture (Quantum | Quanta) \n| \n29", "domains": ["Architectural Patterns and Styles", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 47", "position": 47, "chunk_type": "semantic", "token_estimate": 394}
{"text": "deployment boundaries lie in distributed architectures. We discuss service granularity: and its attendant trade-offs in Chapter 7. Third, independent deployability forces the architecture quantum to include common\ncoupling points such as databases. Most discussions about architecture conveniently\nignore issues such as databases and user interfaces, but real-world systems must com\u2010\nmonly deal with those problems. Thus, any system that uses a shared database fails\nthe architecture quantum criteria for independent deployment unless the database\ndeployment is in lockstep with the application. Many distributed systems that would\notherwise qualify for multiple quanta fail the independently deployable part if they\nshare a common database that has its own deployment cadence. Thus, merely consid\u2010\nering the deployment boundaries doesn\u2019t solely provide a useful measure. Architects\nshould also consider the second criteria for an architecture quantum, high functional\ncohesion, to limit the architecture quantum to a useful scope. High Functional Cohesion\nHigh functional cohesion refers structurally to the proximity of related elements:\nclasses, components, services, and so on. Throughout history, computer scientists\ndefined a variety of cohesion types, scoped in this case to the generic module, which\nmay be represented as classes or components, depending on platform. From a domain\nstandpoint, the technical definition of high functional cohesion overlaps with the goals\nof the bounded context in domain-driven design: behavior and data that implements a\nparticular domain workflow. From a purely independent deployability standpoint, a giant monolithic architecture\nqualifies as an architecture quantum. However, it almost certainly isn\u2019t highly func\u2010\ntionally cohesive, but rather includes the functionality of the entire system. The larger\nthe monolith, the less likely it is singularly functionally cohesive. Ideally, in a microservices architecture, each service models a single domain or work\u2010\nflow, and therefore exhibits high functional cohesion. Cohesion in this context isn\u2019t\nabout how services interact to perform work, but rather how independent and cou\u2010\npled one service is to another service. High Static Coupling\nHigh static coupling implies that the elements inside the architecture quantum are\ntightly wired together, which is really an aspect of contracts. Architects recognize\nthings like REST or SOAP as contract formats, but method signatures and opera\u2010\ntional dependencies (via coupling points such as IP addresses or URLs) also represent\ncontracts. Thus, contracts are an architecture hard part; we cover coupling issues\ninvolving all types of contracts, including how to choose appropriate ones, in\nChapter 13. 30 \n| \nChapter 2: Discerning Coupling in Software Architecture", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 48", "position": 48, "chunk_type": "semantic", "token_estimate": 402}
{"text": "the software components under development. Most monolithic architectures contain: a single coupling point (typically, a database) that makes its quantum measure one. Distributed architectures often feature decoupling at the component level; consider\nthe next set of architecture styles, starting with the service-based architecture shown\nin Figure 2-3. Figure 2-3. Architecture quantum for a service-based architecture\nWhile this individual services model shows the isolation common in microservices,\nthe architecture still utilizes a single relational database, rendering its architecture\nquantum score to one. Service-Based Architecture\nWhen we refer to service-based architecture, we don\u2019t mean a generic architecture\nbased on services, but rather a specific hybrid architecture style that follows a dis\u2010\ntributed macro-layered structure consisting of a separately deployed user interface,\nseparately deployed remote coarse-grained services, and a monolithic database. This\narchitecture addresses one of the complexities of microservices\u2014separation at the\ndatabase level. Services in a service-based architecture follow the same principles as\nmicroservices (based on domain-driven design\u2019s bounded context) but rely on a single\nrelational database because the architects didn\u2019t see value in separation (or saw too\nmany negative trade-offs). Service-based architectures are common targets when restructuring monolithic archi\u2010\ntectures, allowing for decomposition without disrupting existing database schemas\nand integration points. We cover decomposition patterns in Chapter 5. 32 \n| \nChapter 2: Discerning Coupling in Software Architecture", "domains": ["Architectural Patterns and Styles", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 50", "position": 50, "chunk_type": "semantic", "token_estimate": 216}
{"text": "So far, the static coupling measurement of architecture quantum has evaluated all the: topologies to one. However, distributed architectures create the possibility of multiple\nquanta but don\u2019t necessarily guarantee it. For example, the mediator style of event-\ndriven architecture will always be evaluated to an single architecture quantum, as\nillustrated in Figure 2-4. Even though this style represents a distributed architecture, two coupling points push\nit toward a single architecture quantum: the database, as common with the previous\nmonolithic architectures, but also the Request Orchestrator itself\u2014any holistic\ncoupling point necessary for the architecture to function forms an architecture quan\u2010\ntum around it. Figure 2-4. A mediated EDA has a single architecture quantum\nBroker event-driven architectures (without a central mediator) are less coupled, but\nthat doesn\u2019t guarantee complete decoupling. Consider the event-driven architecture\nillustrated in Figure 2-5. This broker-style event driven architecture (without a central mediator) is neverthe\u2010\nless a single architecture quantum because all the services utilize a single relational\ndatabase, which acts as a common coupling point. The question answered by the\nstatic analysis for an architecture quantum is, \u201cIs this dependent of the architecture\nnecessary to bootstrap this service?\u201d Even in the case of an event-driven architecture\nwhere some of the services don\u2019t access the database, if they rely on services that do\naccess the database, then they become part of the static coupling of the architecture\nquantum. Architecture (Quantum | Quanta) \n| \n33", "domains": ["Architectural Patterns and Styles", "Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 51", "position": 51, "chunk_type": "semantic", "token_estimate": 236}
{"text": "Figure 2-5. Even a distributed architecture such as broker-style event-driven architecture: can be a single quantum\nHowever, what about situations in distributed architectures where common coupling\npoints don\u2019t exist? Consider the event-driven architecture illustrated in Figure 2-6. The architects designed this event-driven system with two data stores, and no static\ndependencies between the sets of services. Note that either architecture quantum can\nrun in a production-like ecosystem. It may not be able to participate in all workflows\nrequired by the system, but it runs successfully and operates\u2014sends requests and\nreceives them within the architecture. The static coupling measure of an architecture quantum assesses the coupling depen\u2010\ndencies between architectural and operational components. Thus, the operating sys\u2010\ntem, data store, message broker, container orchestration, and all other operational\ndependencies form the static coupling points of an architecture quantum, using the\nstrictest possible contracts, operational dependencies (more about the role of con\u2010\ntracts in architecture quanta in Chapter 13). 34 \n| \nChapter 2: Discerning Coupling in Software Architecture", "domains": ["Architectural Patterns and Styles", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 52", "position": 52, "chunk_type": "semantic", "token_estimate": 166}
{"text": "Figure 2-6. An event-driven architecture with multiple quanta: The microservices architecture style features highly decoupled services, including\ndata dependencies. Architects in these architectures favor high degrees of decoupling\nand take care not to create coupling points between services, allowing each individual\nservice to each form its own quanta, as shown in Figure 2-7. Figure 2-7. Microservices may form their own quanta\nArchitecture (Quantum | Quanta) \n| \n35", "domains": ["Architectural Patterns and Styles"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 53", "position": 53, "chunk_type": "semantic", "token_estimate": 67}
{"text": "Each service (acting as a bounded context) may have its own set of architecture char\u2010: acteristics\u2014one service might have higher levels of scalability or security than\nanother. This granular level of architecture characteristics scoping represents one of\nthe advantages of the microservices architecture style. High degrees of decoupling\nallow teams working on a service to move as quickly as possible, without worrying\nabout breaking other dependencies. However, if the system is tightly coupled to a user interface, the architecture forms a\nsingle architecture quantum, as illustrated in Figure 2-8. Figure 2-8. A tightly coupled user interface can reduce a microservices architecture\nquantum to one\nUser interfaces create coupling points between the front and back end, and most user\ninterfaces won\u2019t operate if portions of the backend aren\u2019t available. Additionally, it will be difficult for an architect to design different levels of opera\u2010\ntional architecture characteristics (performance, scale, elasticity, reliability, and so on)\nfor each service if they all must cooperate together in a single user interface (particu\u2010\nlarly in the case of synchronous calls, covered in \u201cDynamic Quantum Coupling\u201d on\npage 38). Architects design user interfaces utilizing asynchronicity that doesn\u2019t create coupling\nbetween front and back. A trend on many microservices projects is to use a micro\nfrontend framework for user interface elements in a microservices architecture. In\nsuch an architecture, the user interface elements that interact on behalf of the services\nare emitted from the services themselves. The user interface surface acts as a canvas\n36 \n| \nChapter 2: Discerning Coupling in Software Architecture", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 54", "position": 54, "chunk_type": "semantic", "token_estimate": 255}
{"text": "where the user interface elements can appear, and also facilitates loosely coupled: communication between components, typically using events. Such an architecture is\nillustrated in Figure 2-9. Figure 2-9. In a micro-frontend architecture, each service + user interface component\nforms an architecture quantum\nIn this example, the four tinted services along with their corresponding micro-\nfrontends form architecture quanta: each of these services may have different archi\u2010\ntecture characteristics. Any coupling point in an architecture can create static coupling points from a quan\u2010\ntum standpoint. Consider the impact of a shared database between two systems, as\nillustrated in Figure 2-10. The static coupling of a system provides valuable insight, even in complex systems\ninvolving integration architecture. Increasingly, a common architect technique for\nunderstanding legacy architecture involves creating a static quantum diagram of how\nthings are \u201cwired\u201d together, which helps determine what systems will be impacted by\nchange and offers a way of understanding (and potentially decoupling) the\narchitecture. Static coupling is only one-half of the forces at play in distributed architectures. The\nother is dynamic coupling. Architecture (Quantum | Quanta) \n| \n37", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 55", "position": 55, "chunk_type": "semantic", "token_estimate": 181}
{"text": "Synchronous communication requires the requestor to wait for the response from the: receiver, as shown in Figure 2-11. Figure 2-11. A synchronous call waits for a result from the receiver\nThe calling service makes a call (using one of a number of protocols that support syn\u2010\nchronous calls, such as gRPC) and blocks (does no further processing) until the\nreceiver returns a value (or status indicating a state change or error condition). Asynchronous communication occurs between two services when the caller posts a\nmessage to the receiver (usually via a mechanism such as a message queue) and, once\nthe caller gets acknowledgment that the message will be processed, it returns to work. If the request required a response value, the receiver can use a reply queue to (asyn\u2010\nchronously) notify the caller of the result, which is illustrated in Figure 2-12. Figure 2-12. Asynchronous communication allows parallel processing\nThe caller posts a message to a message queue and continues processing until notified\nby the receiver that the requested information is available via return call. Generally,\narchitects use message queues (illustrated via the gray cylindrical tube in the top dia\u2010\ngram in Figure 2-12) to implement asynchronous communication, but queues are\ncommon and create noise on diagrams, so many architects leave them off, as shown\nin the lower diagram. And, of course, architects can implement asynchronous com\u2010\nmunication without message queues by using a variety of libraries or frameworks. Each diagram variety implies asynchronous messaging; the second provides visual\nshorthand and less implementation detail. Architecture (Quantum | Quanta) \n| \n39", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 57", "position": 57, "chunk_type": "semantic", "token_estimate": 259}
{"text": "Architects must consider significant trade-offs when choosing how services will com\u2010: municate. Decisions around communication affect synchronization, error handling,\ntransactionality, scalability, and performance. The remainder of this book delves into\nmany of these issues. Consistency\nConsistency refers to the strictness of transactional integrity that communication calls\nmust adhere to. Atomic transactions (all-or-nothing transactions requiring consis\u2010\ntency during the processing of a request) lie on one side of the spectrum, whereas dif\u2010\nferent degrees of eventual consistency lie on the other side. Transactionality\u2014having several services participate in an all-or-nothing transaction\n\u2014is one of the most difficult problems to model in distibuted architectures, resulting\nin the general advice to try to avoid cross-service transactions. We discuss consis\u2010\ntency and the intersection of data and architecture in Chapters 6, 9, 10, and 12. Coordination\nCoordination refers to how much coordination the workflow modeled by the commu\u2010\nnication requires. The two common generic patterns for microservices are orchestra\u2010\ntion and choreography, which we describe in Chapter 11. Simple workflows\u2014a single\nservice replying to a request\u2014don\u2019t require special consideration from this dimen\u2010\nsion. However, as the complexity of the workflow grows, the greater the need for\ncoordination. These three factors\u2014communication, consistency, and coordination\u2014all inform the\nimportant decision an architect must make. Critically, however, architects cannot\nmake these choices in isolation; each option has a gravitation effect on the others. For\nexample, transactionality is easier in synchronous architectures with mediation,\nwhereas higher levels of scale are possible with eventually consistent asynchronous\nchoreographed systems. Thinking about these forces as related to each other forms a three-dimensional space,\nillustrated in Figure 2-13. Each force in play during service communication appears as a dimension. For a par\u2010\nticular decision, an architect could graph the position in space representing the\nstrength of these forces. 40 \n| \nChapter 2: Discerning Coupling in Software Architecture", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 58", "position": 58, "chunk_type": "semantic", "token_estimate": 303}
{"text": "transactionality, and service granularity. In Part II, Putting Things Back Together, we: focus on dynamic coupling and understanding communication patterns in\nmicroservices. Sysops Squad Saga: Understanding Quanta\nTuesday, November 23, 14:32\nAusten came to Addison\u2019s office wearing an uncharacteristic cross expression. \u201cHey,\nAddison, can I bother you for a minute?\u201d\n\u201cSure, what\u2019s up?\u201d\n\u201cI\u2019ve been reading about this architecture quantum stuff, and I just\u2026don\u2019t\u2026get\u2026it!\u201d\nAddison laughed, \u201cI know what you mean. I struggled with it when it was purely\nabstract, but when you ground it in practical things, it turns out to be a useful set of\nperspectives.\u201d\n\u201cWhat do you mean?\u201d\n\u201cWell,\u201d said Addison, \u201cthe architecture quantum basically defines a DDD bounded context in archi-\ntectural terms.\u201d\n\u201cWhy not just use bounded context, then?\u201d asked Austen. \u201cBounded context has a specific definition in DDD, and overloading it with stuff about architecture\njust makes people constantly have to differentiate. They are similar, but not the same thing. The first\npart about functional cohesion and independent deployment certainly matches a service based on\nbounded context. But the architecture quantum definition goes further by identifying types of cou-\npling\u2014that\u2019s where the static and dynamic stuff comes in.\u201d\n\u201cWhat is that all about? Isn\u2019t coupling just coupling? Why make the distinction?\u201d\n\u201cIt turns out that a bunch of different concerns revolve around the different types,\u201d said Addison. \u201cLet\u2019s take the static one first, which I like to think of as how things are wired together. Another way\nto think about it: consider one of the services we\u2019re building in our target architecture. What is all the\nwiring required to bootstrap that service?\u201d\n\u201cWell, it\u2019s written in Java, using a Postgres database, and running in Docker\u2014that\u2019s it, right?\u201d\n\u201cYou\u2019re missing a lot.\u201d said Addison. \u201cWhat if you had to build that service from scratch, assuming we\nhad nothing in place? It\u2019s Java, but also using SpringBoot and, what, about 15 or 20 different frame-\nworks and libraries?\u201d\n\u201cThat\u2019s right, we can look in the Maven POM file to figure out all those dependencies. What else?\u201d\n\u201cThe idea behind static quantum coupling is the wiring required to function. We\u2019re using events to\ncommunicate between services\u2014what about the event broker?\u201d\n42 \n| \nChapter 2: Discerning Coupling in Software Architecture", "domains": ["Architectural Patterns and Styles", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 60", "position": 60, "chunk_type": "semantic", "token_estimate": 373}
{"text": "\u201cBut isn\u2019t that the dynamic part?\u201d: \u201cNot the presence of the broker. If the service (or, more broadly, architecture quantum) I want to\nbootstrap utilizes a message broker to function, the broker must be present. When the service calls\nanother service via the broker, we get into the dynamic side.\u201d\n\u201cOK, that makes sense,\u201d said Austen. \u201cIf I think about what it would take to bootstrap it from scratch,\nthat\u2019s the static quantum coupling.\u201d\n\u201cThat\u2019s right. And just that information is super useful. We recently built a diagram of the static quan-\ntum coupling for each of our services defensively.\u201d\nAusten laughed. \u201cDefensively? What do you\u2026\u201d\n\u201cWe were performing a reliability analysis to determine if I change this thing, what might break,\nwhere thing could be anything in our architecture or operations. They\u2019re trying to do risk mitigation\n\u2014if we change a service, they want to know what must be tested.\u201d\n\u201cI see\u2014that\u2019s the static quantum coupling. I can see how that\u2019s a useful view. It also shows how\nteams might impact one another. That seems really useful. Is there a tool we can download that\nfigures that out for us?\u201d\n\u201cWouldn\u2019t that be nice!\u201d laughed Addison. \u201cUnfortunately, no one with our unique mix of architec-\nture has built and open sourced exactly the tool we want. However, some of the platform team is\nworking on a tool to automate it, necessarily customized to our architecture. They\u2019re using the con-\ntainer manifests, POM files, NPM dependencies, and other dependency tools to build and maintain a\nlist of build dependencies. We have also instituted observability for all our services, so we now have\nconsistent log files about what systems call each other, when, and how often. They\u2019re using that to\nbuild a call graph to see how things are connected.\u201d\n\u201cOK, so static coupling is how things are wired together. What about dynamic coupling?\u201d\n\u201cDynamic coupling concerns how quanta communicate with each other, particularly synchronous\nversus asynchronous calls and their impact on operational architecture characteristics\u2014things like\nperformance, scale, elasticity, reliability, and so on. Consider elasticity for a moment\u2014remember the\ndifference between scalability and elasticity?\u201d\nAusten smirked. \u201cI didn\u2019t know there was going to be a test. Let\u2019s see\u2026scalability is the ability to sup-\nport a large number of concurrent users; elasticity is the ability to support a burst of user requests in\na short time frame.\u201d\n\u201cCorrect! Gold star for you. OK, let\u2019s think about elasticity.", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 61", "position": 61, "chunk_type": "semantic", "token_estimate": 405}
{"text": "\u201cBut isn\u2019t that the dynamic part?\u201d: Gold star for you. OK, let\u2019s think about elasticity. Suppose in our future state architecture we\nhave two services like Ticketing and Assignment, and the two types of calls. We carefully designed\nour services to be highly statically decoupled from one another, so that they can be independently\nelastic. That\u2019s the other side effect of static coupling, by the way\u2014it identifies the scope of things\nlike operational architecture characteristics. Let\u2019s say that Ticketing is operating at 10 times the elas-\ntic scale of Assignment, and we need to make a call between them. If we make a synchronous call,\nSysops Squad Saga: Understanding Quanta \n| \n43", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 61", "position": 61, "chunk_type": "semantic", "token_estimate": 112}
{"text": "the whole workflow will bog down, as the caller waits for the slower service to process and return. If: on the other hand we make an asynchronous call, using the message queue as a buffer, we can\nallow the two services to execute operationally independently, allowing the caller to add messages\nto the queue and continue working, receiving notification when the workflow is complete.\u201d\n\u201cOh, I see, I see! The architecture quantum defines the scope of architecture characteristics\u2014it\u2019s\nobvious how the static coupling can affect that. But I see now that, depending on the type of call\nyou make, you might temporarily couple two services together.\u201d\n\u201cThat\u2019s right,\u201d said Addison. \u201cThe architecture quanta can entangle one another temporarily, during\nthe course of a call, if the nature of the call ties things like performance, responsiveness, scale, and a\nbunch of others.\u201d\n\u201cOK, I think I understand what an architecture quantum is, and how the coupling definitions work. But I\u2019m never going to get that quantum/quanta thing straight!\u201d\n\u201cSame for datum/data, but no one ever uses datum!\u201d laughed Addison. \u201cYou\u2019ll see a lot more of the\nimpact of dynamic coupling on workflows and transactional sagas as you keep digging into our\narchitecture.\u201d\n\u201cI can\u2019t wait!\u201d\n44 \n| \nChapter 2: Discerning Coupling in Software Architecture", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 62", "position": 62, "chunk_type": "semantic", "token_estimate": 213}
{"text": "CHAPTER 3: Architectural Modularity\nTuesday, September 21 09:33\nIt was the same conference room they had been in a hundred times before, but today\nthe atmosphere was different. Very different. As people gathered, no small talk was\nexchanged. Only silence. The sort of dead silence that you could cut with a knife. Yes,\nthat was indeed an appropriate cliche given the topic of the meeting. The business leaders and sponsors of the failing Sysops Squad ticketing application met\nwith the application architects, Addison and Austen, with the purpose of voicing their\nconcern and frustration about the inability of the IT department to fix the never-ending\nissues associated with the trouble ticket application. \u201cWithout a working application,\u201d they had said,\n\u201cwe cannot possibly continue to support this business line.\u201d\nAs the tense meeting ended, the business sponsors quietly filed out one by one, leaving Addison\nand Austen alone in the conference room. \u201cThat was a bad meeting,\u201d said Addison. \u201cI can\u2019t believe they\u2019re actually blaming us for all the issues\nwe\u2019re currently facing with the trouble ticket application. This is a really bad situation.\u201d\n\u201cYeah, I know,\u201d said Austen. \u201cEspecially the part about possibly closing down the product support\nbusiness line. We\u2019ll be assigned to other projects, or worse, maybe even let go. Although I\u2019d rather be\nspending all of my time on the soccer field or on the slopes skiing in the winter, I really can\u2019t afford\nto lose this job.\u201d\n\u201cNeither can I,\u201d said Addison. \u201cBesides, I really like the development team we have in place, and I\u2019d\nhate to see it broken up.\u201d\n\u201cMe too,\u201d said Austen. \u201cI still think breaking apart the application would solve most of these issues.\u201d\n45", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 63", "position": 63, "chunk_type": "semantic", "token_estimate": 283}
{"text": "\u201cI agree with you,\u201d said Addison, \u201cbut how do we convince the business to spend more money and: \u201cWe already told you,\u201d said Austen, \u201cbecause nothing else we try seems to work!\u201d\n\u201cSorry,\u201d said Logan, \u201cbut you know as well as I do that\u2019s not a reasonable justification for the business. You\u2019ll never get the funding you need with that kind of reason.\u201d\n\u201cSo, what would be a good business justification?\u201d asked Addison. \u201cHow do we sell this approach to\nthe business and get the additional funding approved?\u201d\n\u201cWell,\u201d said Logan, \u201cto build a good business case for something of this magnitude, you first need to\nunderstand the benefits of architectural modularity, match those benefits to the issues you are fac-\ning with the current system, and finally analyze and document the trade-offs involved with breaking\napart the application.\u201d\n46 \n| \nChapter 3: Architectural Modularity", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 64", "position": 64, "chunk_type": "semantic", "token_estimate": 145}
{"text": "Businesses today face a torrent of change; market evolution seems to keep accelerat\u2010: ing at a blistering pace. Business drivers (such as mergers and acquisitions), increased\ncompetition in the marketplace, increased consumer demand, and increased innova\u2010\ntion (such as automation through machine learning and artificial intelligence) neces\u2010\nsarily require changes to underlying computer systems. In many cases, those changes\nin computer systems consequently necessitate changes to the underlying architectures\nsupporting them. However, it\u2019s not only business that\u2019s undergoing constant and rapid change\u2014it\u2019s also\nthe technical environment in which those computer systems reside. Containerization,\nthe move to cloud-based infrastructure, the adoption of DevOps, and even new\nadvancements in continuous delivery pipelines all impact the underlying architecture\nof those computer systems. It\u2019s difficult in today\u2019s world to manage all of this constant and rapid change with\nrespect to software architecture. Software architecture is the foundational structure of\na system, and is therefore generally thought of as something that should remain sta\u2010\nble and not undergo frequent change, similar to the underlying structural aspects of a\nlarge building or skyscraper. However, unlike the structural architecture of a building,\nsoftware architecture must constantly change and adapt to meet the new demands of\ntoday\u2019s business and technology environment. Consider the increased number of mergers and acquisitions happening in today\u2019s\nmarketplace. When one company acquires another, not only does it acquire the phys\u2010\nical aspects of a company (such as people, buildings, inventory, and so on) but also\nmore customers. Can the existing systems in either company scale to meet the\nincrease in user volume as a result of the merger or acquisition? Scalability is a big\npart of mergers and acquisitions, as is agility and extensibility, all of which are archi\u2010\ntectural concerns. Large monolithic (single deployment) systems generally do not provide the level of\nscalability, agility, and extensibility required to support most mergers and acquisi\u2010\ntions. The capacity for additional machine resources (threads, memory, and CPU)\nfills up very quickly. To illustrate this point, consider the water glass shown in Figure\n3-1. The glass represents the server (or virtual machine), and the water represents the\napplication. As monolithic applications grow to handle increased consumer demand\nand user load (whether from mergers, acquisitions, or company growth), they begin\nto consume more and more resources. As more water is added to the glass (represent\u2010\ning the growing monolithic application), the glass begins to fill up.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 65", "position": 65, "chunk_type": "semantic", "token_estimate": 399}
{"text": "Businesses today face a torrent of change; market evolution seems to keep accelerat\u2010: As monolithic applications grow to handle increased consumer demand\nand user load (whether from mergers, acquisitions, or company growth), they begin\nto consume more and more resources. As more water is added to the glass (represent\u2010\ning the growing monolithic application), the glass begins to fill up. Adding another\nglass (represented as another server or virtual machine) does nothing, because the\nnew glass would contain the same amount of water as the first one. Architectural Modularity \n| \n47", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 65", "position": 65, "chunk_type": "semantic", "token_estimate": 91}
{"text": "Figure 3-1. A full glass representing a large monolithic application close to capacity: One aspect of architectural modularity is breaking large monolithic applications into\nseparate and smaller parts to provide more capacity for further scalability and\ngrowth, while at the same time facilitating constant and rapid change. In turn, these\ncapabilities can help achieve a company\u2019s strategic goals. By adding another empty glass to our water glass example and breaking the water\n(application) into two separate parts, half the water can now be poured into the new\nempty glass, providing 50% more capacity, as shown in Figure 3-2. The water glass\nanalogy is a great way of explaining architectural modularity (the breaking up of\nmonolithic applications) to business stakeholders and C-level executives, who will\ninevitably be paying for the architecture-refactoring effort. Figure 3-2. Two half-full glasses representing an application broken apart with plenty of\ncapacity for growth\n48 \n| \nChapter 3: Architectural Modularity", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 66", "position": 66, "chunk_type": "semantic", "token_estimate": 153}
{"text": "Increased scalability is only one benefit of architectural modularity. Another impor\u2010: tant benefit is agility, the ability to respond quickly to change. An article from Forbes\nin January 2020 by David Benjamin and David Komlos stated the following:\nThere is one thing that will separate the pack into winners and losers: the on-demand\ncapability to make bold and decisive course-corrections that are executed effectively\nand with urgency. Businesses must be agile in order to survive in today\u2019s world. However, while business\nstakeholders may be able to make quick decisions and change direction quickly, the\ncompany\u2019s technology staff may not be able to implement those new directives fast\nenough to make a difference. Enabling technology to move as fast as the business (or,\nconversely, preventing technology from slowing the business) requires a certain level\nof architectural agility. Modularity Drivers\nArchitects shouldn\u2019t break a system into smaller parts unless clear business drivers\nexist. The primary business drivers for breaking applications into smaller parts\ninclude speed-to-market (sometimes called time-to-market) and achieving a level of\ncompetitive advantage in the marketplace. Speed-to-market is achieved through architectural agility\u2014the ability to respond\nquickly to change. Agility is a compound architectural characteristic made up of\nmany other architecture characteristics, including maintainability, testability, and\ndeployability. Competitive advantage is achieved through speed-to-market combined with scalabil\u2010\nity and overall application availability and fault tolerance. The better a company does,\nthe more it grows, hence the need for more scalability to support increased user activ\u2010\nity. Fault tolerance, the ability of an application to fail and continue to operate, is nec\u2010\nessary to ensure that as parts of the application fail, other parts are still able to\nfunction as normal, minimizing the overall impact to the end user. Figure 3-3 illus\u2010\ntrates the relationship between the technical drivers and the resulting business drivers\nfor modularity (enclosed within boxes). Businesses must be agile to survive in today\u2019s fast-paced and ever-changing volatile\nmarket, meaning the underlying architectures must be agile as well. As illustrated in\nFigure 3-3, the five key architectural characteristics to support agility, speed-to-\nmarket, and, ultimately, competitive advantage in today\u2019s marketplace are availability\n(fault tolerance), scalability, deployability, testability, maintainability. Modularity Drivers \n| \n49", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 67", "position": 67, "chunk_type": "semantic", "token_estimate": 362}
{"text": "Figure 3-3. The drivers for modularity and the relationships among them: Note that architectural modularity does not always have to translate to a distributed\narchitecture. Maintainability, testability, and deployability (defined in the following\nsections) can also be achieved through monolithic architectures such as a modular\nmonolith or even a microkernel architecture (see Appendix B for a list of references\nproviding more information about these architecture styles). Both of these architec\u2010\nture styles offer a level of architectural modularity based on the way the components\nare structured. For example, with a modular monolith, components are grouped into\nwell-formed domains, providing for what is known as a domain partitioned architec\u2010\nture (see Fundamentals of Software Architecture, Chapter 8, page 103). With the\nmicrokernel architecture, functionality is partitioned into separate plug-in compo\u2010\nnents, allowing for a much smaller testing and deployment scope. Maintainability\nMaintainability is about the ease of adding, changing, or removing features, as well as\napplying internal changes such as maintenance patches, framework upgrades, third-\nparty upgrades, and so on. As with most composite architecture characteristics,\nmaintainability is hard to define objectively. Alexander von Zitzewitz, software archi\u2010\ntect and founder of hello2morrow, wrote an article about a new metric for objectively\ndefining the maintainability level of an application. While von Zitzewitz\u2019s maintaina\u2010\nbility metric is fairly complicated and involves lots of factors, its initial form is as\nfollows:\n50 \n| \nChapter 3: Architectural Modularity", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 68", "position": 68, "chunk_type": "semantic", "token_estimate": 233}
{"text": "ML = 100 * \u2211: i = 1\nk\nci\nwhere ML is the maintainability level of the overall system (percentage from 0% to\n100%), k is the total number of logical components in the system, and ci is the cou\u2010\npling level for any given component, with a special focus on incoming coupling lev\u2010\nels. This equation basically demonstrates that the higher the incoming coupling level\nbetween components, the lower the overall maintainability level of the codebase. Putting aside complicated mathematics, some of the typical metrics used for deter\u2010\nmining the relative maintainability of an application based on components (the archi\u2010\ntectural building blocks of an application) include the following:\nComponent coupling\nThe degree and manner to which components know about one another\nComponent cohesion\nThe degree and manner to which the operations of a component interrelate\nCyclomatic complexity\nThe overall level of indirection and nesting within a component\nComponent size\nThe number of aggregated statements of code within a component\nTechnical versus domain partitioning\nComponents aligned by technical usage or by domain purpose\u2014see Appendix A\nWithin the context of architecture, we are defining a component as an architectural\nbuilding block of the application that does some sort of business or infrastructure\nfunction, usually manifested through a package structure (Java), namespace (C#), or\nphysical grouping of files (classes) within some sort of directory structure. For exam\u2010\nple, the component Order History might be implemented through a set of class files\nlocated in the namespace app.business.order.history. Large monolithic architectures generally have low levels of maintainability due to the\ntechnical partitioning of functionality into layers, the tight coupling between compo\u2010\nnents, and weak component cohesion from a domain perspective. For example, con\u2010\nsider a new requirement within a traditional monolithic layered architecture to add\nan expiration date to items contained in a customer\u2019s wish list (items in a list to\nmaybe purchase at a later time). Notice in Figure 3-4 that the change scope of the new\nrequirement is at an application level since the change is propagated to all of the layers\nwithin the application. Modularity Drivers \n| \n51", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 69", "position": 69, "chunk_type": "semantic", "token_estimate": 349}
{"text": "Figure 3-4. With monolithic layered architectures, change is at an application level: Depending on the team structure, implementing this simple change to add an expira\u2010\ntion date to wish list items in a monolithic layered architecture could possibly require\nthe coordination of at least three teams:\n\u2022 A member from the user interface team would be needed to add the new expiry\nfield to the screen. \u2022 A member from the backend team would be needed to add business rules associ\u2010\nated with the expiry date and change contracts to add the new expiry field. \u2022 A member from the database team would be needed to change the table schema\nto add the new expiry column in the Wishlist table. Since the Wishlist domain is spread throughout the entire architecture, it becomes\nharder to maintain a particular domain or subdomain (such as Wishlist). Modular\narchitectures, on the other hand, partition domains and subdomains into smaller,\nseparately deployed units of software, thereby making it easier to modify a domain or\nsubdomain. Notice that with a distributed service-based architecture, as shown in\nFigure 3-5, the change scope of the new requirement is at a domain level within a par\u2010\nticular domain service, making it easier to isolate the specific deployment unit requir\u2010\ning the change. Moving to even more architectural modularity such as a microservices architecture,\nas illustrated in Figure 3-6, places the new requirement at a function-level change\n52 \n| \nChapter 3: Architectural Modularity", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 70", "position": 70, "chunk_type": "semantic", "token_estimate": 243}
{"text": "scope, isolating the change to a specific service responsible for the wish list: functionality. Figure 3-5. With service-based architectures, change is at a domain level\nFigure 3-6. With microservices architectures, change is at a function level\nModularity Drivers \n| \n53", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 71", "position": 71, "chunk_type": "semantic", "token_estimate": 40}
{"text": "These three progressions toward modularity demonstrate that as the level of architec\u2010: tural modularity increases, so does maintainability, making it easier to add, change,\nor remove functionality. Testability\nTestability is defined as the ease of testing (usually implemented through automated\ntests) as well as the completeness of testing. Testability is an essential ingredient for\narchitectural agility. Large monolithic architecture styles like the layered architecture\nsupport relatively low levels of testability (and hence agility) due to the difficulty in\nachieving full and complete regression testing of all features within the large deploy\u2010\nment unit. Even if a monolithic application did have a suite of full regression tests,\nimagine the frustration of having to execute hundreds or even thousands of unit tests\nfor a simple code change. Not only would it take a long time to execute all of the tests,\nbut the poor developer would be stuck researching why dozens of tests failed when in\nfact the failed tests have nothing to do with the change. Architectural modularity\u2014the breaking apart of applications into smaller deploy\u2010\nment units\u2014significantly reduces the overall testing scope for changes made to a ser\u2010\nvice, allowing for better completeness of testing as well as ease of testing. Not only\ndoes modularity result in smaller, more targeted test suites, but maintaining the unit\ntests becomes easier as well. While architectural modularity generally improves testability, it can sometimes lead\nto the same problems that exist with monolithic, single-deployment applications. For\nexample, consider an application that was broken into three smaller self-contained\ndeployment units (services), as depicted in Figure 3-7. Making a change to Service A limits the testing scope to only that service, since Ser\u2010\nvice B and Service C are not coupled to Service A. However, as communication\nincreases among these services, as shown at the bottom of Figure 3-7, testability\ndeclines rapidly because the testing scope for a change to Service A now includes Ser\u2010\nvice B and Service C, therefore impacting both the ease of testing and the complete\u2010\nness of testing. 54 \n| \nChapter 3: Architectural Modularity", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 72", "position": 72, "chunk_type": "semantic", "token_estimate": 342}
{"text": "Figure 3-7. Testing scope is increased as services communicate with one another: Deployability\nDeployability is not only about the ease of deployment\u2014it is also about the frequency\nof deployment and the overall risk of deployment. To support agility and respond\nquickly to change, applications must support all three of these factors. Deploying\nsoftware every two weeks (or more) not only increases the overall risk of deployment\n(due to grouping multiple changes together), but in most cases unnecessarily delays\nnew features or bug fixes that are ready to be pushed out to customers. Of course,\ndeployment frequency must be balanced with the customer\u2019s (or end user\u2019s) ability to\nbe able to absorb changes quickly. Monolithic architectures generally support low levels of deployability due to the\namount of ceremony involved in deploying the application (such as code freezes,\nmock deployments, and so on), the increased risk that something else might break\nonce new features or bug fixes are deployed, and a long time frame between deploy\u2010\nments (weeks to months). Applications having a certain level of architectural modu\u2010\nlarity in terms of separately deployed units of software have less deployment\nceremony, less risk of deployment, and can be deployed more frequently than a large,\nsingle monolithic application. Modularity Drivers \n| \n55", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 73", "position": 73, "chunk_type": "semantic", "token_estimate": 209}
{"text": "Like testability, deployability is also negatively impacted as services become smaller: and communicate more with each other to complete a business transaction. Deploy\u2010\nment risk is increased, and it becomes more difficult to deploy a simple change for\nfear of breaking other services. To quote software architect Matt Stine in his article on\norchestrating microservices:\nIf your microservices must be deployed as a complete set in a specific order, please put\nthem back in a monolith and save yourself some pain. This scenario leads to what is commonly referred to as the \u201cbig ball of distributed\nmud,\u201d where very few (if any) of the benefits of architectural modularity are realized. Scalability\nScalability is defined as the ability of a system to remain responsive as user load grad\u2010\nually increases over time. Related to scalability is elasticity, which is defined as the\nability of a system to remain responsive during significantly high instantaneous and\nerratic spikes in user load. Figure 3-8 illustrates the differences between scalability\nand elasticity. Figure 3-8. Scalability is different from elasticity\nWhile both of these architectural characteristics include responsiveness as a function\nof the number of concurrent requests (or users in the system), they are handled dif\u2010\nferently from an architectural and implementation standpoint. Scalability generally\noccurs over a longer period of time as a function of normal company growth, whereas\nelasticity is the immediate response to a spike in user load. A great example to further illustrate the difference is that of a concert-ticketing\nsystem. Between major concert events, there is usually a fairly light concurrent user\nload. However, the minute tickets go on sale for a popular concert, concurrent user\nload significantly spikes. The system may go from 20 concurrent users to 3,000 con\u2010\ncurrent users in a matter of seconds. To maintain responsiveness, the system must\nhave the capacity to handle the high peaks in user load, and also have the ability to\ninstantaneously start up additional services to handle the spike in traffic. Elasticity\nrelies on services having a very small mean time to startup (MTTS), which is achieved\n56 \n| \nChapter 3: Architectural Modularity", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 74", "position": 74, "chunk_type": "semantic", "token_estimate": 352}
{"text": "architecturally by having very small, fine-grained services. With an appropriate archi\u2010: tectural solution in place, MTTS (and hence elasticity) can then be further managed\nthrough design-time techniques such as small lightweight platforms and runtime\nenvironments. Although both scalability and elasticity improve with finer-grained services, elasticity\nis more a function of granularity (the size of a deployment unit), whereas scalability is\nmore a function of modularity (the breaking apart of applications into separate\ndeployment units). Consider the traditional layered architecture, service-based archi\u2010\ntecture, and microservices architecture styles and their corresponding star ratings for\nscalability and elasticity, as illustrated in Figure 3-9 (the details of these architecture\nstyles and their corresponding star ratings can be found in our previous book, Funda\u2010\nmentals of Software Architecture. Note that one star means that the capability is not\nwell supported by the architecture style, whereas five stars means that capability is a\nmajor feature of the architecture style and is well supported. Notice that scalability and elasticity rate relatively low with the monolithic layered\narchitecture. Large monolithic layered architectures are both difficult and expensive\nto scale because all of the application functionality must scale to the same degree\n(application-level scalability and poor MTTS). This can become particularly costly in\ncloud-based infrastructures. However, with service-based architecture, notice that\nscalability improves, but not as much as elasticity. This is because domain services in\na service-based architecture are coarse grained and usually contain the entire domain\nin one deployment unit (such as order processing or warehouse management), and\ngenerally have too long of a mean time to startup (MTTS) to respond fast enough to\nimmediate demand for elasticity due to their large size (domain-level scalability and\nfair MTTS). Notice that with microservices, both scalability and elasticity are maxi\u2010\nmized because of the small, single-purpose, fine-grained nature of each separately\ndeployed service (function-level scalability and excellent MTTS). Like testability and deployability, the more services communicate with one other to\ncomplete a single business transaction, the greater the negative impact on scalability\nand elasticity. For this reason, it is important to keep synchronous communication\namong services to a minimum when requiring high levels of scalability and elasticity. Modularity Drivers \n| \n57", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 75", "position": 75, "chunk_type": "semantic", "token_estimate": 361}
{"text": "Figure 3-9. Scalability and elasticity improve with modularity: Availability/Fault Tolerance\nLike many architecture characteristics, fault tolerance has varying definitions. Within\nthe context of architectural modularity, we define fault tolerance as the ability for\nsome parts of the system to remain responsive and available as other parts of the sys\u2010\ntem fail. For example, if a fatal error (such as an out-of-memory condition) in the\npayment-processing portion of a retail application occurs, the users of the system\nshould still be able to search for items and place orders, even though the payment\nprocessing is unavailable. All monolithic systems suffer from low levels of fault tolerance. While fault tolerance\ncan be somewhat mitigated in a monolithic system by having multiple instances of\nthe entire application load balanced, this technique is both expensive and ineffective. If the fault is due to a programming bug, that bug will exist in both instances, there\u2010\nfore potentially bringing down both instances. Architectural modularity is essential to achieving domain-level and function-level\nfault tolerance in a system. By breaking apart the system into multiple deployment\n58 \n| \nChapter 3: Architectural Modularity", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 76", "position": 76, "chunk_type": "semantic", "token_estimate": 183}
{"text": "units, catastrophic failure is isolated to only that deployment unit, thereby allowing: the rest of the system to function normally. There is a caveat to this, however: if other\nservices are synchronously dependent on a service that is failing, fault tolerance is not\nachieved. This is one of the reasons asynchronous communication between services\nis essential for maintaining a good level of fault tolerance in a distributed system. Sysops Squad Saga: Creating a Business Case\nThursday, September 30, 12:01\nArmed with a better understanding of what is meant by architectural modularity and the\ncorresponding drivers for breaking apart a system, Addison and Austen met to discuss\nthe Sysops Squad issues and try to match them to modularity drivers in order to build a\nsolid business justification to present to the business sponsors. \u201cLet\u2019s take each of the issues we are facing and see if we can match them to some of the\nmodularity drivers,\u201d said Addison. \u201cThat way, we can demonstrate to the business that\nbreaking apart the application will in fact address the issues we are facing.\u201d\n\u201cGood idea,\u201d said Austen. \u201cLet\u2019s start with the first issue they talked about in the meeting\u2014change. We cannot seem to effectively apply changes to the existing monolithic system without something\nelse breaking. Also, changes take way too long, and testing the changes is a real pain.\u201d\n\u201cAnd the developers are constantly complaining that the codebase is too large, and it\u2019s difficult to\nfind the right place to apply changes to new features or bug fixes,\u201d said Addison. \u201cOK,\u201d said Austen, \u201cso clearly, overall maintainability is a key issue here.\u201d\n\u201cRight,\u201d said Addison. \u201cSo, by breaking apart the application, it would not only decouple the code,\nbut it would isolate and partition the functionality into separately deployed services, making it easier\nfor developers to apply changes.\u201d\n\u201cTestability is another key characteristic related to this problem, but we have that covered already\nbecause of all our automated unit tests,\u201d said Austen. \u201cActually, it\u2019s not,\u201d replied Addison. \u201cTake a look at this.\u201d\nAddison showed Austen that over 30% of the test cases are commented out or obsolete, and there\nare missing test cases for some of the critical workflow parts of the system.", "domains": ["Design Principles", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 77", "position": 77, "chunk_type": "semantic", "token_estimate": 368}
{"text": "units, catastrophic failure is isolated to only that deployment unit, thereby allowing: \u201cActually, it\u2019s not,\u201d replied Addison. \u201cTake a look at this.\u201d\nAddison showed Austen that over 30% of the test cases are commented out or obsolete, and there\nare missing test cases for some of the critical workflow parts of the system. Addison also explained\nthat the developers were continually complaining that the entire unit test suite had to be run for any\nchange (big or small), which not only took a long time, but developers were faced with having to fix\nissues not related to their change. This was one of the reasons it was taking so long to apply even\nthe simplest of changes. \u201cTestability is about the ease of testing, but also the completeness of testing,\u201d said Addison. \u201cWe\nhave neither. By breaking apart the application, we can significantly reduce the scope of testing for\nSysops Squad Saga: Creating a Business Case \n| \n59", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 77", "position": 77, "chunk_type": "semantic", "token_estimate": 158}
{"text": "changes made to the application, group relevant automated unit tests together, and get better: \u201cI have some latest metrics here that show it\u2019s not the core ticketing function-\nality that keeps bringing the system down, but the customer survey functionality and reporting.\u201d\n\u201cThis is excellent news,\u201d said Addison. \u201cSo by breaking apart that functionality of the system into sep-\narate services, we can isolate those faults, keeping the core ticketing functionality operational. That\u2019s\na good justification in and of itself!\u201d\n\u201cExactly,\u201d said Austen. \u201cSo, we are in agreement then that overall availability through fault tolerance\nwill address the application not always being available for the customers since they only interact\nwith the ticketing portion of the system.\u201d\n\u201cBut what about the system freezing up?\u201d asked Addison. \u201cHow do we justify that part with breaking\nup the application?\u201d\n60 \n| \nChapter 3: Architectural Modularity", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 78", "position": 78, "chunk_type": "semantic", "token_estimate": 143}
{"text": "\u201cIt just so happens I asked Sydney from the Sysops Squad development team to run some analysis: for me regarding exactly that issue,\u201d said Austen. \u201cIt turns out that it is a combination of two things. First, whenever we have more than 25 customers creating tickets at the same time, the system\nfreezes. But, check this out\u2014whenever they run the operational reports during the day when cus-\ntomers are entering problem tickets, the system also freezes up.\u201d\n\u201cSo,\u201d said Addison, \u201cit appears we have both a scalability and a database load issue here.\u201d\n\u201cExactly!\u201d Austen said. \u201cAnd get this\u2014by breaking up the application and the monolithic database,\nwe can segregate reporting into its own system and also provide the added scalability for the\ncustomer-facing ticketing functionality.\u201d\nSatisfied that they had a good business case to present to the business sponsors and confident that\nthis was the right approach for saving this business line, Addison created an Architecture Decision\nRecord (ADR) for the decision to break apart the system and create a corresponding business case\npresentation for the business sponsors. ADR: Migrate Sysops Squad Application to a Distributed Architecture\nContext\nThe Sysops Squad is currently a monolithic problem ticket application that supports many\ndifferent business functions related to problem tickets, including customer registration,\nproblem ticket entry and processing, operations and analytical reporting, billing and pay-\nment processing, and various administrative maintenance functions. The current applica-\ntion has numerous issues involving scalability, availability, and maintainability. Decision\nWe will migrate the existing monolithic Sysops Squad application to a distributed architec-\nture. Moving to a distributed architecture will accomplish the following:\n\u2022 Make the core ticketing functionality more available for our external customers,\ntherefore providing better fault tolerance\n\u2022 Provide better scalability for customer growth and ticket creation, resolving the fre-\nquent application freeze-ups we\u2019ve been experiencing\n\u2022 Separate the reporting functionality and reporting load on the database, resolving\nthe frequent application freeze-ups we\u2019ve been experiencing\n\u2022 Allow teams to implement new features and fix bugs much faster than with the cur-\nrent monolithic application, therefore providing for better overall agility\n\u2022 Reduce the amount of bugs introduced into the system when changes occur, there-\nfore providing better testability\n\u2022 Allow us to deploy new features and bug fixes at a much faster rate (weekly or even\ndaily), therefore providing better deployability\nSysops Squad Saga: Creating a Business Case \n| \n61", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 79", "position": 79, "chunk_type": "semantic", "token_estimate": 397}
{"text": "Consequences: The migration effort will cause delays for new features being introduced since most of the\ndevelopers will be needed for the architecture migration. The migration effort will incur additional cost (cost estimates to be determined). Until the existing deployment pipeline is modified, release engineers will have to manage\nthe release and monitoring of multiple deployment units. The migration effort will require us to break apart the monolithic database. Addison and Austen met with the business sponsors for the Sysops Squad problem ticketing system\nand presented their case in a clear and concise manner. The business sponsors were pleased with\nthe presentation and agreed with the approach, informing Addison and Austen to move forward\nwith the migration. 62 \n| \nChapter 3: Architectural Modularity", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 80", "position": 80, "chunk_type": "semantic", "token_estimate": 123}
{"text": "Addison and Austen met with Logan to discuss some of the approaches they were considering for: how to break apart the application. They explained to Logan that they wanted to start with the\nknowledge base and survey functionality but weren\u2019t sure what to do after that. \u201cThe approach you\u2019re suggesting,\u201d said Logan, \u201cis what is known as the Elephant Migration Anti-\nPattern. Eating the elephant one bite at a time may seem like a good approach at the start, but in\nmost cases it leads to an unstructured approach that results in a big ball of distributed mud, what\nsome people also call a distributed monolith. I would not recommend that approach.\u201d\n\u201cSo, what other approaches exist? Are there patterns we can use to break apart the application?\u201d\nasked Addison. \u201cYou need to take a holistic view of the application and apply either tactical forking or component-\nbased decomposition,\u201d said Logan. \u201cThose are the two most effective approaches I know of.\u201d\nAddison and Austen looked at Logan. \u201cBut how do we know which one to use?\u201d\nWhereas architectural modularity describes the why for breaking apart a monolithic\napplication, architectural decomposition describes the how. Breaking apart large,\ncomplex monolithic applications can be a complex and time-consuming undertaking,\nand it\u2019s important to know whether it is even feasible to begin such an effort and how\nto approach it. Component-based decomposition and tactical forking are two common approaches\nfor breaking apart monolithic applications. Component-based decomposition is an\nextraction approach that applies various refactoring patterns for refining and extract\u2010\ning components (the logical building blocks of an application) to form a distributed\narchitecture in an incremental and controlled fashion. The tactical forking approach\ninvolves making replicas of an application and chipping away at the unwanted parts\nto form services, similar to the way a sculptor creates a beautiful work of art from a\nblock of granite or marble. Which approach is most effective? The answer to this question is, of course, it\ndepends. One of the main factors in selecting a decomposition approach is how well\nthe existing monolithic application code is structured. Do clear components and\ncomponent boundaries exist within the codebase, or is the codebase largely an\nunstructured big ball of mud? As the flowchart in Figure 4-1 illustrates, the first step in an architecture decomposi\u2010\ntion effort is to first determine whether the codebase is even decomposable. We cover\nthis topic in detail in the next section.", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 82", "position": 82, "chunk_type": "semantic", "token_estimate": 406}
{"text": "loosely defined) components, then a component-based decomposition approach (see: \u201cComponent-Based Decomposition\u201d on page 71) is the way to go. Figure 4-1. The decision tree for selecting a decomposition approach\nWe describe both of these approaches in this chapter, and then devote an entire chap\u2010\nter (Chapter 5) to describing each of the component-based decomposition patterns in\ndetail. Is the Codebase Decomposable? What happens when a codebase lacks internal structure? Can it even be decomposed? Such software has a colloquial name\u2014the Big Ball of Mud Anti-Pattern, coined by\nBrian Foote in a same-named essay in 1999. For example, a complex web application\nwith event handlers wired directly to database calls and no modularity can be consid\u2010\nered a Big Ball of Mud architecture. Generally, architects don\u2019t spend much time cre\u2010\nating patterns for these kinds of systems; software architecture concerns internal\nstructure, and these systems lack that defining feature. Is the Codebase Decomposable? | \n65", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 83", "position": 83, "chunk_type": "semantic", "token_estimate": 154}
{"text": "Unfortunately, without careful governance, many software systems degrade into big: balls of mud, leaving it to subsequent architects (or perhaps a despised former self) to\nrepair. Step one in any architecture restructuring exercise requires an architect to\ndetermine a plan for the restructuring, which in turn requires the architect to under\u2010\nstand the internal structure. The key question the architect must answer becomes is\nthis codebase salvageable? In other words, is it a candidate for decomposition pat\u2010\nterns, or is another approach more appropriate? No single measure will determine whether a codebase has reasonable internal struc\u2010\nture\u2014that evaluation falls to one or more architects to determine. However, archi\u2010\ntects do have tools to help determine macro characteristics of a codebase, particularly\ncoupling metrics, to help evaluate internal structure. Afferent and Efferent Coupling\nIn 1979, Edward Yourdon and Larry Constantine published Structured Design: Fun\u2010\ndamentals of a Discipline of Computer Program and Systems Design (Yourdon), defin\u2010\ning many core concepts, including the metrics afferent and efferent coupling. Afferent\ncoupling measures the number of incoming connections to a code artifact (compo\u2010\nnent, class, function, and so on). Efferent coupling measures the outgoing connections\nto other code artifacts. Note the value of just these two measures when changing the structure of a system. For example, when deconstructing a monolith into a distributed architecture, an\narchitect will find shared classes such as Address. When building a monolith, it is\ncommon and encouraged for developers to reuse core concepts such as Address, but\nwhen pulling the monolith apart, now the architect must determine how many other\nparts of the system use this shared asset. Virtually every platform has tools that allow architects to analyze the coupling char\u2010\nacteristics of code in order to assist in restructuring, migrating, or understanding a\ncodebase. Many tools exist for various platforms that provide a matrix view of class\nand/or component relationships, as illustrated in Figure 4-2. In this example, the Eclipse plug-in provides a visualization of the output of JDepend,\nwhich includes coupling analysis per package, along with some aggregate metrics\nhighlighted in the next section. 66 \n| \nChapter 4: Architectural Decomposition", "domains": ["Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 84", "position": 84, "chunk_type": "semantic", "token_estimate": 353}
{"text": "In the equation, ma represents abstract elements (interfaces or abstract classes) within: the codebase, and mc represents concrete elements. Architects calculate abstractness\nby calculating the ratio of the sum of abstract artifacts to the sum of the concrete\nones. Another derived metric, instability, is the ratio of efferent coupling to the sum of both\nefferent and afferent coupling, shown in Equation 4-2. Equation 4-2. Instability\nI =\nCe\nCe + Ca\nIn the equation, Ce represents efferent (or outgoing) coupling, and Ca represents\nafferent (or incoming) coupling. The instability metric determines the volatility of a codebase. A codebase that exhibits\nhigh degrees of instability breaks more easily when changed because of high cou\u2010\npling. Consider two scenarios, each with Ca of 2. For the first scenario, Ce = 0, yield\u2010\ning an instability score of zero. In the other scenario, Ce = 3, yielding an instability\nscore of 3/5. Thus, the measure of instability for a component reflects how many\npotential changes might be forced by changes to related components. A component\nwith an instability value near one is highly unstable, a value close to zero may be\neither stable or rigid: it is stable if the module or component contains mostly abstract\nelements, and rigid if it comprises mostly concrete elements. However, the trade-off\nfor high stability is lack of reuse\u2014if every component is self contained, duplication is\nlikely. A component with an I value close to 1, we can agree, is highly instable. However, a\ncomponent with a value of I close to 0 may be either stable or rigid. However, if it\ncontains mostly concrete elements, then it is rigid. Thus, in general, it is important to look at the value of I and A together rather than in\nisolation. Hence the reason to consider the main sequence presented on the next\npage. 68 \n| \nChapter 4: Architectural Decomposition", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 86", "position": 86, "chunk_type": "semantic", "token_estimate": 312}
{"text": "Distance from the Main Sequence: One of the few holistic metrics architects have for architectural structure is distance\nfrom the main sequence, a derived metric based on instability and abstractness, shown\nin Equation 4-3. Equation 4-3. Distance from the main sequence\nD = A + I \u22121\nIn the equation, A = abstractness and I = instability. The distance-from-the-main-sequence metric imagines an ideal relationship between\nabstractness and instability; components that fall near this idealized line exhibit a\nhealthy mixture of these two competing concerns. For example, graphing a particular\ncomponent allows developers to calculate the distance-from-the-main-sequence met\u2010\nric, illustrated in Figure 4-3. Figure 4-3. Normalized distance from the main sequence for a particular component\nDevelopers graph the candidate component, then measure the distance from the ide\u2010\nalized line. The closer to the line, the better balanced the component. Components\nthat fall too far into the upper-right corner enter into what architects call the zone of\nuselessness: code that is too abstract becomes difficult to use. Conversely, code that\nfalls into the lower-left corner enter the zone of pain: code with too much implemen\u2010\ntation and not enough abstraction becomes brittle and hard to maintain, illustrated in\nFigure 4-4. Is the Codebase Decomposable? | \n69", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 87", "position": 87, "chunk_type": "semantic", "token_estimate": 204}
{"text": "Tools exist in many platforms to provide these measures, which assist architects when: analyzing codebases because of unfamiliarity, migration, or technical debt\nassessment. What does the distance-from-the-main-sequence metric tell architects looking to\nrestructure applications? Just as in construction projects, moving a large structure\nthat has a poor foundation presents risks. Similarly, if an architect aspires to restruc\u2010\nture an application, improving the internal structure will make it easier to move the\nentity. Figure 4-4. Zones of uselessness and pain\nThis metric also provides a good clue as to the balance of the internal structure. If an\narchitect evaluates a codebase where many of the components fall into either the\nzones of uselessness or pain, perhaps it is not a good use of time to try to shore up the\ninternal structure to the point where it can be repaired. Following the flowchart in Figure 4-1, once an architect decides that the codebase is\ndecomposable, the next step is to determine what approach to take to decompose the\napplication. The following sections describe the two approaches for decomposing an\napplication: component-based decomposition and tactical forking. 70 \n| \nChapter 4: Architectural Decomposition", "domains": ["Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 88", "position": 88, "chunk_type": "semantic", "token_estimate": 190}
{"text": "Component-Based Decomposition: It has been our experience that most of the difficulty and complexity involved with\nmigrating monolithic applications to highly distributed architecture like microservi\u2010\nces comes from poorly defined architectural components. Here we define a compo\u2010\nnent as a building block of the application that has a well-defined role and\nresponsibility in the system and a well-defined set of operations. Components in\nmost applications are manifested through namespaces or directory structures and are\nimplemented through component files (or source files). For example, in Figure 4-5,\nthe directory structure penultimate/ss/ticket/assign would represent a component\ncalled Ticket Assign with the namespace penultimate.ss.ticket.assign. Figure 4-5. The directory structure of a codebase becomes the namespace of the\ncomponent\nWhen breaking monolithic applications into distributed architec\u2010\ntures, build services from components, not individual classes. Throughout many collective years of migrating monolithic applications to distributed\narchitectures (such as microservices), we\u2019ve developed a set of component-based\ndecomposition patterns described in Chapter 5 that help prepare a monolithic appli\u2010\ncation for migration. These patterns involve the refactoring of source code to arrive at\na set of well-defined components that can eventually become services, easing the\neffort needed to migrate applications to distributed architectures. Component-Based Decomposition \n| \n71", "domains": ["Architectural Patterns and Styles", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 89", "position": 89, "chunk_type": "semantic", "token_estimate": 200}
{"text": "These component-based decomposition patterns essentially enable the migration of a: monolithic architecture to a service-based architecture, which is defined in Chapter 2\nand described in more detail in Fundamentals of Software Architecture. Service-based\narchitecture is a hybrid of the microservices architecture style where an application is\nbroken into domain services, which are coarse-grained, separately deployed services\ncontaining all of the business logic for a particular domain. Moving to a service-based architecture is suitable as a final target or as a stepping-\nstone to microservices:\n\u2022 As a stepping-stone, it allows an architect to determine which domains require\nfurther levels of granularity into microservices and which ones can remain as\ncoarse-grained domain services (this decision is discussed in detail in Chapter 7). \u2022 Service-based architecture does not require the database to be broken apart,\ntherefore allowing architects to focus on the domain and functional partitioning\nprior to tackling database decomposition (discussed in detail in Chapter 6). \u2022 Service-based architecture does not require any operational automation or con\u2010\ntainerization. Each domain service can be deployed using the same deployment\nartifact as the original application (such as an EAR file, WAR file, Assembly, and\nso on). \u2022 The move to service-based architecture is a technical one, meaning it generally\ndoesn\u2019t involve business stakeholders and doesn\u2019t require any change to the orga\u2010\nnization structure of the IT department nor the testing and deployment\nenvironments. When migrating monolithic applications to microservices, con\u2010\nsider moving to a service-based architecture first as a stepping-\nstone to microservices. But what if the codebase is an unstructured big ball of mud and doesn\u2019t contain very\nmany observable components? That\u2019s where tactical forking comes in. 72 \n| \nChapter 4: Architectural Decomposition", "domains": ["Architectural Patterns and Styles", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 90", "position": 90, "chunk_type": "semantic", "token_estimate": 281}
{"text": "The first step in tactical forking involves cloning the entire monolith, and giving each: team a copy of the entire codebase, as illustrated in Figure 4-9. Figure 4-9. Step one clones the monolith\nEach team receives a copy of the entire codebase, and they start deleting (as illustrated\npreviously in Figure 4-7) the code they don\u2019t need rather than extract the desirable\ncode. Developers often find this easier in a tightly coupled codebase because they\ndon\u2019t have to worry about extracting the large number of dependencies that high cou\u2010\npling creates. Rather, in the deletion strategy, once functionality has been isolated,\ndelete any code that doesn\u2019t break anything. Tactical Forking \n| \n75", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 93", "position": 93, "chunk_type": "semantic", "token_estimate": 112}
{"text": "At the completion of the tactical forking pattern, teams have split the original mono\u2010: lithic application into two parts, preserving the coarse-grained structure of the behav\u2010\nior in each part, as illustrated in Figure 4-11. Figure 4-11. The end state of tactical forking features two services\nNow the restructuring is complete, leaving two coarse-grained services as the result. Trade-Offs\nTactical forking is a viable alternative to a more formal decomposition approach,\nmost suited to codebases that have little or no internal structure. Like all practices in\narchitecture, it has its share of trade-offs:\nBenefits\n\u2022 Teams can start working right away with virtually no up-front analysis. \u2022 Developers find it easier to delete code rather than extract it. Extracting code\nfrom a chaotic codebase presents difficulties because of high coupling, whereas\ncode not needed can be verified by compilation or simple testing. Tactical Forking \n| \n77", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 95", "position": 95, "chunk_type": "semantic", "token_estimate": 146}
{"text": "Shortcomings: \u2022 The resulting services will likely still contain a large amount of mostly latent code\nleft over from the monolith. \u2022 Unless developers undertake additional efforts, the code inside the newly derived\nservices won\u2019t be better than the chaotic code from the monolith\u2014there\u2019s just less\nof it. \u2022 Inconsistencies may occur between the naming of shared code and shared com\u2010\nponent files, resulting in difficultly identifying common code and keeping it\nconsistent. The name of this pattern is apt (as all good pattern names should be)\u2014it provides a\ntactical rather than strategic approach for restructuring architectures, allowing teams\nto quickly migrate important or critical systems to the next generation (albeit in an\nunstructured way). Sysops Squad Saga: Choosing a Decomposition Approach\nFriday, October 29, 10:01\nNow that Addison and Austen understood both approaches, they met in the main con-\nference room to analyze the Sysops Squad application using the abstractness and insta-\nbility metrics to determine which approach would be the most appropriate given their\nsituation. \u201cLook at this,\u201d said Addison. \u201cMost of the code lies along the main sequence. There are a\nfew outliers of course, but I think we can conclude that it\u2019s feasible to break apart this\napplication. So the next step is to determine which approach to use.\u201d\n\u201cI really like the tactical forking approach,\u201d said Austen. \u201cIt reminds me of famous sculptors, when\nasked how they were able to carve such beautiful works out of solid marble, who replied that they\nwere merely removing the marble that wasn\u2019t supposed to be there. I feel like the Sysops Squad\napplication could be my sculpture!\u201d\n\u201cHold on there, Michelangelo,\u201d said Addison. \u201cFirst sports, and now sculpting? You need to make up\nyour mind about what you like to spend your nonworking time on. The thing I don\u2019t like about the\ntactical forking approach is all the duplicate code and shared functionality within each service. Most\nof our problems have to do with maintainability, testability, and overall reliability. Can you imagine\nhaving to apply the same change to several different services at the same time? That would be a\nnightmare!\u201d\n\u201cBut how much shared functionality is there, really?\u201d asked Austen.", "domains": ["Design Principles", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 96", "position": 96, "chunk_type": "semantic", "token_estimate": 363}
{"text": "Shortcomings: Can you imagine\nhaving to apply the same change to several different services at the same time? That would be a\nnightmare!\u201d\n\u201cBut how much shared functionality is there, really?\u201d asked Austen. \u201cI\u2019m not sure,\u201d said Addison, \u201cbut I do know there\u2019s quite a bit of shared code for the infrastructure\nstuff like logging and security, and I know a lot of the database calls are shared from the persistence\nlayer of the application.\u201d\n78 \n| \nChapter 4: Architectural Decomposition", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 96", "position": 96, "chunk_type": "semantic", "token_estimate": 81}
{"text": "Austen paused and thought about Addison\u2019s argument for a bit. \u201cMaybe you\u2019re right. Since we have: good component boundaries already defined, I\u2019m OK with doing the slower component-based\ndecomposition approach and giving up my sculpting career. But I\u2019m not giving up sports!\u201d\nAddison and Austen came to an agreement that the component decomposition approach would\nbe the appropriate one for the Sysops Squad application. Addison wrote an ADR for this decision,\noutlining the trade-offs and justification for the component-based decomposition approach. ADR: Migration Using the Component-Based Decomposition Approach\nContext\nWe will be breaking apart the monolithic Sysops Squad application into separately\ndeployed services. The two approaches we considered for the migration to a distributed\narchitecture were tactical forking and component-based decomposition. Decision\nWe will use the component-based decomposition approach to migrate the existing mon-\nolithic Sysops Squad application to a distributed architecture. The application has well-defined component boundaries, lending itself to the\ncomponent-based decomposition approach. This approach reduces the chance of having to maintain duplicate code within each\nservice. With the tactical forking approach, we would have to define the service boundaries up\nfront to know how many forked applications to create. With the component-based\ndecomposition approach, the service definitions will naturally emerge through compo-\nnent grouping. Given the nature of the problems we are facing with the current application with regard to\nreliability, availability, scalability, and workflow, using the component-based decomposi-\ntion approach provides a safer and more controlled incremental migration than the tacti-\ncal forking approach does. Consequences\nThe migration effort will likely take longer with the component-based decomposition\napproach than with tactical forking. However, we feel the justifications in the previous sec-\ntion outweigh this trade-off. This approach allows the developers on the team to work collaboratively to identify\nshared functionality, component boundaries, and domain boundaries. Tactical forking\nwould require us to break apart the team into smaller, separate teams for each forked\napplication and increase the amount of coordination needed between the smaller teams. Sysops Squad Saga: Choosing a Decomposition Approach \n| \n79", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 97", "position": 97, "chunk_type": "semantic", "token_estimate": 337}
{"text": "CHAPTER 5: Component-Based Decomposition Patterns\nMonday, November 1, 11:53\nAddison and Austen chose to use the component-based decomposition approach, but\nwere unsure about the details of each decomposition pattern. They tried to research this\napproach, but did not find much on the internet about it. Once again, they met with\nLogan in the conference room for advice on what these patterns are all about and how\nto use them. \u201cListen, Logan,\u201d said Addison, \u201cI want to start out by saying we both really appreciate the\namount of time you have been spending with us to get this migration process started. I\nknow you\u2019re super busy on your own firefights.\u201d\n\u201cNo problem,\u201d said Logan. \u201cUs firefighters have to stick together. I\u2019ve been in your shoes before, so I\nknow what it\u2019s like flying blind on these sort of things. Besides, this is a highly visible migration effort,\nand it\u2019s important you both get this thing right the first time. Because there won\u2019t be a second time.\u201d\n\u201cThanks, Logan,\u201d said Austen. \u201cI\u2019ve got a game in about two hours, so we\u2019ll try to make this short. You\ntalked earlier about component-based decomposition, and we chose that approach, but we aren\u2019t\nable to find much about it on the internet.\u201d\n\u201cI\u2019m not surprised,\u201d said Logan. \u201cNot much has been written about them yet, but I know a book is\ncoming out describing these patterns in detail sometime later this year. I first learned about these\ndecomposition patterns at a conference about four years ago in a session with an experienced soft-\nware architect. I was really impressed with the iterative and methodical approach to safely move\nfrom a monolithic architecture to a distributed one like service-based architecture and microservi-\nces. Since then I\u2019ve been using these patterns with quite a bit of success.\u201d\n\u201cCan you show us how these patterns work?\u201d asked Addison. \u201cSure,\u201d said Logan. \u201cLet\u2019s take it one pattern at a time.\u201d\n81", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 99", "position": 99, "chunk_type": "semantic", "token_estimate": 322}
{"text": "Component-based decomposition (introduced in Chapter 4) is a highly effective: technique for breaking apart a monolithic application when the codebase is some\u2010\nwhat structured and grouped by namespaces (or directories). This chapter introduces\na set of patterns, known as component-based decomposition patterns, that describe the\nrefactoring of monolithic source code to arrive at a set of well-defined components\nthat can eventually become services. These decomposition patterns significantly ease\nthe effort of migrating monolithic applications to distributed architectures. Figure 5-1 shows the road map for the component-based decomposition patterns\ndescribed in this chapter and how they are used together to break apart a monolithic\napplication. Initially, these patterns are used together in sequence when moving a\nmonolithic application to a distributed one, and then individually as maintenance is\napplied to the monolithic application during migration. These decomposition pat\u2010\nterns are summarized as follows:\n\u201cIdentify and Size Components Pattern\u201d on page 84\nTypically the first pattern applied when breaking apart a monolithic application. This pattern is used to identify, manage, and properly size components. \u201cGather Common Domain Components Pattern\u201d on page 94\nUsed to consolidate common business domain logic that might be duplicated\nacross the application, reducing the number of potentially duplicate services in\nthe resulting distributed architecture. \u201cFlatten Components Pattern\u201d on page 101\nUsed to collapse or expand domains, subdomains, and components, thus ensur\u2010\ning that source code files reside only within well-defined components. \u201cDetermine Component Dependencies Pattern\u201d on page 111\nUsed to identify component dependencies, refine those dependencies, and deter\u2010\nmine the feasibility and overall level of effort for a migration from a monolithic\narchitecture to a distributed one. \u201cCreate Component Domains Pattern\u201d on page 120\nUsed to group components into logical domains within the application and to\nrefactor component namespaces and/or directories to align with a particular\ndomain. \u201cCreate Domain Services Pattern\u201d on page 126\nUsed to physically break apart a monolithic architecture by moving logical\ndomains within the monolithic application to separately deployed domain\nservices. 82 \n| \nChapter 5: Component-Based Decomposition Patterns", "domains": ["Design Principles", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 100", "position": 100, "chunk_type": "semantic", "token_estimate": 334}
{"text": "Figure 5-1. Component-based decomposition pattern flow and usage: Each pattern described in this chapter is divided into three sections. The first section,\n\u201cPattern Description,\u201d describes how the pattern works, why the pattern is important,\nand what the outcome is of applying the pattern. Knowing that most systems are\nmoving targets during a migration, the second section, \u201cFitness Functions for Gover\u2010\nnance,\u201d describes the automated governance that can be used after applying the pat\u2010\ntern to continually analyze and verify the correctness of the codebase during ongoing\nmaintenance. The third section uses the real-world Sysops Squad application (see\n\u201cIntroducing the Sysops Squad Saga\u201d on page 15) to illustrate the use of the pattern\nand illustrate the transformations of the application after the pattern has been\napplied. Component-Based Decomposition Patterns \n| \n83", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 101", "position": 101, "chunk_type": "semantic", "token_estimate": 130}
{"text": "Architecture Stories: Throughout this chapter, we will be using architecture stories as a way of recording\nand describing code refactoring that impacts the structural aspect of the application\nfor each of the Sysops Squad sagas. Unlike user stories, which describe a feature that\nneeds to be implemented or changed, an architecture story describes particular code\nrefactoring that impacts the overall structure of an application and satisfies some sort\nof business driver (such as increased scalability, better time-to-market, etc.). For\nexample, if an architect sees the need to break apart a payment service to support bet\u2010\nter overall extensibility for adding additional payment types, a new architecture story\nwould be created and read as follows:\nAs an architect, I need to decouple the payment service to support better extensibility\nand agility when adding additional payment types. We view architecture stories as separate from technical debt stories. Technical debt\nstories usually capture things a developer needs to do in a later iteration to \u201cclean up\nthe code,\u201d whereas an architecture story captures something that needs to change\nquickly to support a particular architectural characteristic or business need. Identify and Size Components Pattern\nThe first step in any monolithic migration is to apply the Identify and Size Compo\u2010\nnents pattern. The purpose of this pattern is to identify and catalog the architectural\ncomponents (logical building blocks) of the application and then properly size the\ncomponents. Pattern Description\nBecause services are built from components, it is critical to not only identify the com\u2010\nponents within an application, but to properly size them as well. This pattern is used\nto identify components that are either too big (doing too much) or too small (not\ndoing enough). Components that are too large relative to other components are gen\u2010\nerally more coupled to other components, are harder to break into separate services,\nand lead to a less modular architecture. Unfortunately, it is difficult to determine the size of a component. The number of\nsource files, classes, and total lines of code are not good metrics because every pro\u2010\ngrammer designs classes, methods, and functions differently. One metric we\u2019ve found\nuseful for component sizing is calculating the total number of statements within a\ngiven component (the sum of statements within all source files contained within a\nnamespace or directory).", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 102", "position": 102, "chunk_type": "semantic", "token_estimate": 381}
{"text": "Architecture Stories: The number of\nsource files, classes, and total lines of code are not good metrics because every pro\u2010\ngrammer designs classes, methods, and functions differently. One metric we\u2019ve found\nuseful for component sizing is calculating the total number of statements within a\ngiven component (the sum of statements within all source files contained within a\nnamespace or directory). A statement is a single complete action performed in the\nsource code, usually terminated by a special character (such as a semicolon in lan\u2010\nguages such as Java, C, C++, C#, Go, and JavaScript; or a newline in languages such as\n84 \n| \nChapter 5: Component-Based Decomposition Patterns", "domains": ["Design Patterns", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 102", "position": 102, "chunk_type": "semantic", "token_estimate": 108}
{"text": "F#, Python, and Ruby). While not a perfect metric, at least it\u2019s a good indicator of how: much the component is doing and how complex the component is. Having a relatively consistent component size within an application is important. Generally speaking, the size of components in an application should fall between one\nto two standard deviations from the average (or mean) component size. In addition,\nthe percentage of code represented by each component should be somewhat evenly\ndistributed between application components and not vary significantly. While many static code analysis tools can show the number of statements within a\nsource file, many of them don\u2019t accumulate total statement by component. Because of\nthis, the architect usually must perform manual or automated post-processing to\naccumulate total statements by component and then calculate the percentage of code\nthat component represents. Regardless of the tools or algorithms used, the important information and metrics to\ngather and calculate for this pattern are shown in Table 5-1 and are defined in the\nfollowing list. Table 5-1. Component inventory and component size analysis example\nComponent name\nComponent namespace\nPercent\nStatements\nFiles\nBilling Payment\nss.billing.payment\n5\n4,312\n23\nBilling History\nss.billing.history\n4\n3,209\n17\nCustomer Notification\nss.customer.notification\n2\n1,433\n7\nComponent name\nA descriptive name and identifier of the component that is consistent throughout\napplication diagrams and documentation. The component name should be clear\nenough to be as self-describing as possible. For example, the component Billing\nHistory shown in Table 5-1 is clearly a component that contains source code files\nused to manage a customer\u2019s billing history. If the distinct role and responsibility\nof the component isn\u2019t immediately identifiable, consider changing the compo\u2010\nnent (and potentially the corresponding namespace) to a more descriptive one. For example, a component named Ticket Manager leaves too many unanswered\nquestions about its role and responsibility in the system, and should be renamed\nto better describe its role. Component namespace\nThe physical (or logical) identification of the component representing where the \nsource code files implementing that component are grouped and stored. This\nidentifier is usually denoted through a namespace, package structure (Java), or\ndirectory structure. When a directory structure is used to denote the component,\nwe usually convert the file separator to a dot (.) and create a corresponding\nIdentify and Size Components Pattern \n| \n85", "domains": ["Design Patterns", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 103", "position": 103, "chunk_type": "semantic", "token_estimate": 384}
{"text": "logical namespace. For example, the component namespace for source code files: in the ss/customer/notification directory structure would have the namespace\nvalue ss.customer.notification. Some languages require that the namespace\nmatch the directory structure (such as Java with a package), whereas other lan\u2010\nguages (such as C# with a namespace) do not enforce this constraint. Whatever\nnamespace identifier is used, make sure the type of identifier is consistent across\nall of the components in the application. Percent\nThe relative size of the component based on its percentage of the overall source\ncode containing that component. The percent metric is helpful in identifying\ncomponents that appear too large or too small in the overall application. This\nmetric is calculated by taking the total number of statements within the source\ncode files representing that component and dividing that number by the total\nnumber of statements in the entire codebase of the application. For example, the\npercent value of 5 for the ss.billing.payment component in Table 5-1 means\nthat this component constitutes 5% of the overall codebase. Statements\nThe sum of the total number of source code statements in all source files con\u2010\ntained within that component. This metric is useful for determining not only the\nrelative size of the components within an application, but also for determining\nthe overall complexity of the component. For example, a seemingly simple single-\npurpose component named Customer Wishlist might have a total of 12,000 state\u2010\nments, indicating that the processing of wish list items is perhaps more complex\nthan it looks. This metric is also necessary for calculating the percent metric pre\u2010\nviously described. Files\nThe total number of source code files (such as classes, interfaces, types, and so\non) that are contained within the component. While this metric has little to do\nwith the size of a component, it does provide additional information about the\ncomponent from a class structure standpoint. For example, a component with\n18,409 statements and only 2 files is a good candidate for refactoring into smaller,\nmore contextual classes. When resizing a large component, we recommend using a functional decomposition\napproach or a domain-driven approach to identify subdomains that might exist\nwithin the large component. For example, assume the Sysops Squad application has a\nTrouble Ticket component containing 22% of the codebase that is responsible for\nticket creation, assignment, routing, and completion.", "domains": ["Design Patterns", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 104", "position": 104, "chunk_type": "semantic", "token_estimate": 389}
{"text": "logical namespace. For example, the component namespace for source code files: When resizing a large component, we recommend using a functional decomposition\napproach or a domain-driven approach to identify subdomains that might exist\nwithin the large component. For example, assume the Sysops Squad application has a\nTrouble Ticket component containing 22% of the codebase that is responsible for\nticket creation, assignment, routing, and completion. In this case, it might make sense\nto break the single Trouble Ticket component into four separate components (Ticket\nCreation, Ticket Assignment, Ticket Routing, and Ticket Completion), reducing the\npercentage of code each component represents, therefore creating a more modular\n86 \n| \nChapter 5: Component-Based Decomposition Patterns", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 104", "position": 104, "chunk_type": "semantic", "token_estimate": 111}
{"text": "application. If no clear subdomains exist within a large component, then leave the: component as is. Fitness Functions for Governance\nOnce this decomposition pattern has been applied and components have been identi\u2010\nfied and sized correctly, it\u2019s important to apply some sort of automated governance to\nidentify new components and to ensure components don\u2019t get too large during nor\u2010\nmal application maintenance and create unwanted or unintended dependencies. Automated holistic fitness functions can be triggered during deployment to alert the\narchitect if specified constraints are exceeded (such as the percent metric discussed\npreviously or use of standard deviations to identify outliers). Fitness functions can be implemented through custom-written code or through the\nuse of open source or COTS tools as part of a CI/CD pipeline. Some of the automated\nfitness functions that can be used to help govern this decomposition pattern are as\nfollows. Fitness function: Maintain component inventory\nThis automated holistic fitness function, usually triggered on deployment through a\nCI/CD pipeline, helps keep the inventory of components current. It\u2019s used to alert an\narchitect of components that might have been added or removed by the development\nteam. Identifying new or removed components is not only critical for this pattern, but\nfor the other decomposition patterns as well. Example 5-1 shows the pseudocode and\nalgorithm for one possible implementation of this fitness function. Example 5-1. Pseudocode for maintaining component inventory", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 105", "position": 105, "chunk_type": "semantic", "token_estimate": 230}
{"text": "Get prior component namespaces that are stored in a datastore: LIST prior_list = read_from_datastore()", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 105", "position": 105, "chunk_type": "semantic", "token_estimate": 14}
{"text": "Fitness function: No component shall exceed <some percent> of the overall codebase: This automated holistic fitness function, usually triggered on deployment through a\nCI/CD pipeline, identifies components that exceed a given threshold in terms of the\npercentage of overall source code represented by that component, and alerts the\narchitect if any component exceeds that threshold. As mentioned earlier in this chap\u2010\nter, the threshold percentage value will vary depending on the size of the application,\nbut should be set so as to identify significant outliers. For example, for a relatively\nsmall application with only 10 components, setting the percentage threshold to some\u2010\nthing like 30% would sufficiently identify a component that is too large, whereas for a\nlarge application with 50 components, a threshold of 10% would be more appropri\u2010\nate. Example 5-2 shows the pseudocode and algorithm for one possible implementa\u2010\ntion of this fitness function. Example 5-2. Pseudocode for maintaining component size based on percent of code", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 106", "position": 106, "chunk_type": "semantic", "token_estimate": 159}
{"text": "Section: Walk through the source code for each component, accumulating statements", "domains": ["Design Patterns", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 106", "position": 106, "chunk_type": "semantic", "token_estimate": 11}
{"text": "Section: and calculating the percentage of code each component represents. Send", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 106", "position": 106, "chunk_type": "semantic", "token_estimate": 11}
{"text": "an alert if greater than 10%: FOREACH component IN component_list {\n  component_statements = accumulate_statements(component)\n  percent = component_statements / total_statements\n  IF percent > .10 {\n    send_alert(component, percent)\n  }\n}\nFitness function: No component shall exceed <some number of standard deviations> from the\nmean component size\nThis automated holistic fitness function, usually triggered on deployment through a\nCI/CD pipeline, identifies components that exceed a given threshold in terms of the\nnumber of standard deviations from the mean of all component sizes (based on the\ntotal number of statements in the component), and alerts the architect if any compo\u2010\nnent exceeds that threshold. Standard deviation is a useful means of determining outliers in terms of component\nsize. Standard deviation is calculated as follows:\ns =\n1\nN \u22121\u2211i = 1\nN\nxi \u2212x 2\n88 \n| \nChapter 5: Component-Based Decomposition Patterns", "domains": ["Design Patterns", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 106", "position": 106, "chunk_type": "semantic", "token_estimate": 138}
{"text": "where N is the number of observed values, xi is the observed values, and x is the mean: of the observed values. The mean of observed values (x) is calculated as follows:\nx = 1\nN \u2211\ni = 1\nN\nxi\nThe standard deviation can then be used along with the difference from the mean to\ndetermine the number of standard deviations the component size is from the mean. Example 5-3 shows the pseudocode for this fitness function, using three standard\ndeviations from the mean as a threshold. Example 5-3. Pseudocode for maintaining component size based on number of standard\ndeviations", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 107", "position": 107, "chunk_type": "semantic", "token_estimate": 102}
{"text": "of statements per component: SET total_statements TO 0\nMAP component_size_map\nFOREACH component IN component_list {\n  num_statements = accumulate_statements(component)\n  ADD num_statements TO total_statements\n  ADD component,num_statements TO component_size_map\n}", "domains": ["Design Patterns", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 107", "position": 107, "chunk_type": "semantic", "token_estimate": 27}
{"text": "Calculate the standard deviation: SET square_diff_sum TO 0\nnum_components = get_num_entries(component_list)\nmean = total_statements / num_components\nFOREACH component,size IN component_size_map {\n  diff = size - mean\n  ADD square(diff) TO square_diff_sum\n}\nstd_dev = square_root(square_diff_sum / (num_components - 1))", "domains": ["Design Patterns", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 107", "position": 107, "chunk_type": "semantic", "token_estimate": 38}
{"text": "Section: For each component calculate the number of standard deviations from the", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 107", "position": 107, "chunk_type": "semantic", "token_estimate": 12}
{"text": "mean. Send an alert if greater than 3: FOREACH component,size IN component_size_map {\n  diff_from_mean = absolute_value(size - mean);\n  num_std_devs = diff_from_mean / std_dev\n  IF num_std_devs > 3 {\n    send_alert(component, num_std_devs)\n  }\n}\nIdentify and Size Components Pattern \n| \n89", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 107", "position": 107, "chunk_type": "semantic", "token_estimate": 39}
{"text": "Sysops Squad Saga: Sizing Components: Tuesday, November 2, 09:12\nAfter the discussion with Logan (the lead architect) about component-based decompo-\nsition patterns, Addison decided to apply the Identify and Size Components pattern to\nidentify all of the components in the Sysops Squad ticketing application and calculate\nthe size of each component based on the total number of statements in each\ncomponent. Addison gathered all the necessary component information and put this information\ninto Table 5-2, calculating the percentage of code for each component based on the\ntotal number of statements in the entire application (in this case, 82,931 statements). Table 5-2. Component size analysis for the Sysops Squad application\nComponent name\nComponent namespace\nPercent\nStatements\nFiles\nLogin\nss.login\n2\n1865\n3\nBilling Payment\nss.billing.payment\n5\n4,312\n23\nBilling History\nss.billing.history\n4\n3,209\n17\nCustomer Notification\nss.customer.notification\n2\n1,433\n7\nCustomer Profile\nss.customer.profile\n5\n4,012\n16\nExpert Profile\nss.expert.profile\n6\n5,099\n32\nKB Maint\nss.kb.maintenance\n2\n1,701\n14\nKB Search\nss.kb.search\n3\n2,871\n4\nReporting\nss.reporting\n33\n27,765\n162\nTicket\nss.ticket\n8\n7,009\n45\nTicket Assign\nss.ticket.assign\n9\n7,845\n14\nTicket Notify\nss.ticket.notify\n2\n1,765\n3\nTicket Route\nss.ticket.route\n2\n1,468\n4\nSupport Contract\nss.supportcontract\n5\n4,104\n24\nSurvey\nss.survey\n3\n2,204\n5\nSurvey Notify\nss.survey.notify\n2\n1,299\n3\nSurvey Templates\nss.survey.templates\n2\n1,672\n7\nUser Maintenance\nss.users\n4\n3,298\n12\n90 \n| \nChapter 5: Component-Based Decomposition Patterns", "domains": ["Design Patterns", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 108", "position": 108, "chunk_type": "semantic", "token_estimate": 227}
{"text": "Addison noticed that most of the components listed in Table 5-2 are about the same size, with the: exception of the Reporting component (ss.reporting) which consisted of 33% of the codebase. Since the Reporting component was significantly larger than the other components (illustrated in\nFigure 5-2), Addison chose to break this component apart to reduce its overall size. Figure 5-2. The Reporting component is too big and should be broken apart\nAfter doing some analysis, Addison found that the reporting component contained source code\nthat implemented three categories of reports:\n\u2022 Ticketing reports (ticket demographics reports, tickets per day/week/month reports, ticket res-\nolution time reports, and so on)\n\u2022 Expert reports (expert utilization reports, expert distribution reports, and so on)\nIdentify and Size Components Pattern \n| \n91", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 109", "position": 109, "chunk_type": "semantic", "token_estimate": 127}
{"text": "\u2022 Financial reports (repair cost reports, expert cost reports, profit reports, and so on): Addison also identified common (shared) code that all reporting categories used, such as common\nutilities, calculators, shared data queries, report distribution, and shared data formatters. Addison cre-\nated an architecture story (see \u201cArchitecture Stories\u201d on page 84) for this refactoring and explained it\nto the development team. Sydney, one of the Sysops Squad developers assigned the architecture\nstory, refactored the code to break apart the single Reporting component into four separate compo-\nnents\u2014a Reporting Shared component containing the common code and three other components\n(Ticket Reports, Expert Reports, and Financial Reports), each representing a functional reporting\narea, as illustrated in Figure 5-3. Figure 5-3. The large Reporting component broken into smaller reporting components\n92 \n| \nChapter 5: Component-Based Decomposition Patterns", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 110", "position": 110, "chunk_type": "semantic", "token_estimate": 134}
{"text": "After Sydney committed the changes, Addison reanalyzed the code and verified that all of the com-: ponents were now fairly equally distributed in size. Addison recorded the results of applying this\ndecomposition pattern in Table 5-3. Table 5-3. Component size after applying the Identify and Size Components pattern\nComponent name\nComponent namespace\nPercent\nStatements\nFiles\nLogin\nss.login\n2\n1865\n3\nBilling Payment\nss.billing.payment\n5\n4,312\n23\nBilling History\nss.billing.history\n4\n3,209\n17\nCustomer Notification\nss.customer.notification\n2\n1,433\n7\nCustomer Profile\nss.customer.profile\n5\n4,012\n16\nExpert Profile\nss.expert.profile\n6\n5,099\n32\nKB Maint\nss.kb.maintenance\n2\n1,701\n14\nKB Search\nss.kb.search\n3\n2,871\n4\nReporting Shared\nss.reporting.shared\n7\n5,309\n20\nTicket Reports\nss.reporting.tickets\n8\n6,955\n58\nExpert Reports\nss.reporting.experts\n9\n7,734\n48\nFinancial Reports\nss.reporting.financial\n9\n7,767\n36\nTicket\nss.ticket\n8\n7,009\n45\nTicket Assign\nss.ticket.assign\n9\n7,845\n14\nTicket Notify\nss.ticket.notify\n2\n1,765\n3\nTicket Route\nss.ticket.route\n2\n1,468\n4\nSupport Contract\nss.supportcontract\n5\n4,104\n24\nSurvey\nss.survey\n3\n2,204\n5\nSurvey Notify\nss.survey.notify\n2\n1,299\n3\nSurvey Templates\nss.survey.templates\n2\n1,672\n7\nUser Maintenance\nss.users\n4\n3,298\n12\nNotice in the preceding Sysops Squad Saga that Reporting no longer exists as a com\u2010\nponent in Table 5-3 or Figure 5-3. Although the namespace still exists (ss.report\ning), it is no longer considered a component, but rather a subdomain. The refactored\ncomponents listed in Table 5-3 will be used when applying the next decomposition\npattern, Gather Common Domain Components. Identify and Size Components Pattern \n| \n93", "domains": ["Design Patterns", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 111", "position": 111, "chunk_type": "semantic", "token_estimate": 245}
{"text": "Gather Common Domain Components Pattern: When moving from a monolithic architecture to a distributed one, it is often benefi\u2010\ncial to identify and consolidate common domain functionality to make common\nservices easier to identify and create. The Gather Common Domain Components pat\u2010\ntern is used to identify and collect common domain logic and centralize it into a sin\u2010\ngle component. Pattern Description\nShared domain functionality is distinguished from shared infrastructure functionality\nin that domain functionality is part of the business processing logic of an application\n(such as notification, data formatting, and data validation) and is common to only\nsome processes, whereas infrastructure functionality is operational in nature (such as\nlogging, metrics gathering, and security) and is common to all processes. Consolidating common domain functionality helps eliminate duplicate services when\nbreaking apart a monolithic system. Often there are only very subtle differences\namong common domain functionality that is duplicated throughout the application,\nand these differences can be easily resolved within a single common service (or\nshared library). Finding common domain functionality is mostly a manual process, but some auto\u2010\nmation can be used to assist in this effort (see \u201cFitness Functions for Governance\u201d on\npage 95). One hint that common domain processing exists in the application is the\nuse of shared classes across components or a common inheritance structure used by\nmultiple components. Take, for example, a class file named SMTPConnection in a\nlarge codebase that is used by five classes, all contained within different namespaces\n(components). This scenario is a good indication that common email notification\nfunctionality is spread throughout the application and might be a good candidate for\nconsolidation. Another way of identifying common domain functionality is through the name of a\nlogical component or its corresponding namespace. Consider the following compo\u2010\nnents (represented as namespaces) in a large codebase:\n\u2022 Ticket Auditing (penultimate.ss.ticket.audit)\n\u2022 Billing Auditing (penultimate.ss.billing.audit)\n\u2022 Survey Auditing (penultimate.ss.survey.audit)\n94 \n| \nChapter 5: Component-Based Decomposition Patterns", "domains": ["Design Principles", "Software Quality Attributes", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 112", "position": 112, "chunk_type": "semantic", "token_estimate": 320}
{"text": "Notice how each of these components (Ticket Auditing, Billing Auditing, and Survey: Auditing) all have the same thing in common\u2014writing the action performed and the\nuser requesting the action to an audit table. While the context may be different, the\nfinal outcome is the same\u2014inserting a row in an audit table. This common domain\nfunctionality can be consolidated into a new component called penulti\nmate.ss.shared.audit, resulting in less duplication of code and also fewer services\nin the resulting distributed architecture. Not all common domain functionality necessarily becomes a shared service. Alterna\u2010\ntively, common code could be gathered into a shared library that is bound to the code\nduring compile time. The pros and cons of using a shared service rather than a\nshared library are discussed in detail in Chapter 8. Fitness Functions for Governance\nAutomating the governance of shared domain functionality is rather difficult because\nof the subjectiveness of identifying shared functionality and classifying it as domain\nfunctionality versus infrastructure functionality. For the most part, the fitness func\u2010\ntions used to govern this pattern are therefore somewhat manual. That said, there are\nsome ways to automate the governance to assist in the manual interpretation of com\u2010\nmon domain functionality. The following fitness functions can assist in finding com\u2010\nmon domain functionality. Fitness function: Find common names in leaf nodes of component namespace\nThis automated holistic fitness function can be triggered on deployment through a\nCI/CD pipeline to locate common names within the namespace of a component. When a common ending namespace node name is found between two or more com\u2010\nponents, the architect is alerted and can analyze the functionality to determine if it is\ncommon domain logic. So that the same alert isn\u2019t continuously sent as a \u201cfalse posi\u2010\ntive,\u201d an exclusion file can be used to store those namespaces that have common end\u2010\ning node names but are not deemed common domain logic (such as multiple\nnamespaces ending in .calculate or .validate). Example 5-4 shows the pseudo\u2010\ncode for this fitness function. Example 5-4. Pseudocode for finding common namespace leaf node names", "domains": ["Design Principles", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 113", "position": 113, "chunk_type": "semantic", "token_estimate": 345}
{"text": "Section: Locate possible duplicate component node names that are not in the exclusion", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 113", "position": 113, "chunk_type": "semantic", "token_estimate": 13}
{"text": "FOREACH component IN component_list {: leaf_name = get_last_node(component)\n  IF leaf_name IN leaf_node_list AND\n     leaf_name NOT IN excluded_leaf_node_list {\n    ADD component TO common_component_list\n  } ELSE {\n    ADD leaf_name TO leaf_node_list\n  }\n}", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 114", "position": 114, "chunk_type": "semantic", "token_estimate": 31}
{"text": "of source file names for each component: LIST component_list = identify_components(root_directory)\nLIST source_file_list = get_source_files(root_directory)\nMAP component_source_file_map\nFOREACH component IN component_list {\n  LIST component_source_file_list = get_source_files(component)\n  ADD component, component_source_file_list TO component_source_file_map\n}", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 114", "position": 114, "chunk_type": "semantic", "token_estimate": 32}
{"text": "the exclusion list stored in a datastore: LIST excluded_source_file_list = read_datastore()\nLIST common_source_file_list\nFOREACH source_file IN source_file_list {\n  SET count TO 0\n  FOREACH component,component_source_file_list IN component_source_file_map {\n    IF source_file IN component_source_file_list {\n      ADD 1 TO count\n    }\n  }\n  IF count > 1 AND source_file NOT IN excluded_source_file_list {\n    ADD source_file TO common_source_file_list\n  }\n96 \n| \nChapter 5: Component-Based Decomposition Patterns", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 114", "position": 114, "chunk_type": "semantic", "token_estimate": 60}
{"text": "Send an alert if any source files are used in multiple components: IF common_source_file_list NOT EMPTY {\n \nsend_alert(common_source_file_list)\n}\nSysops Squad Saga: Gathering Common Components\nFriday, November 5, 10:34\nHaving identified and sized the components in the Sysops Squad application, Addison\napplied the Gather Common Domain Components pattern to see if any common func-\ntionality existed between components. From the list of components in Table 5-3, Addi-\nson noticed there were three components all related to notifying a Sysops Squad\ncustomer, and listed these in Table 5-4. Table 5-4. Sysops Squad components with common domain functionality\nComponent\nNamespace\nResponsibility\nCustomer Notification\nss.customer.notification\nGeneral notification\nTicket Notify\nss.ticket.notify\nNotify that expert is en route\nSurvey Notify\nss.survey.notify\nSend survey email\nWhile each of these notification components had a different context for notifying a customer, Addi-\nson realized they all have one thing in common\u2014they all sent information to a customer. Figure 5-4\nillustrates these common notification components within the Sysops Squad application. Noticing that the source code contained in these components was also very similar, Addison consul-\nted with Austen (the other Sysops Squad architect). Austen liked the idea of a single notification\ncomponent, but was concerned about impacting the overall level of coupling between compo-\nnents. Addison agreed that this might be an issue and investigated this trade-off further. Gather Common Domain Components Pattern \n| \n97", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 115", "position": 115, "chunk_type": "semantic", "token_estimate": 226}
{"text": "Figure 5-4. Notification functionality is duplicated throughout the application: Addison analyzed the incoming (afferent) coupling level for the existing Sysops Squad notification\ncomponents and came up with the resulting coupling metrics listed in Table 5-5, with \u201cCA\u201d repre-\nsenting the number of other components requiring that component (afferent coupling). 98 \n| \nChapter 5: Component-Based Decomposition Patterns", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 116", "position": 116, "chunk_type": "semantic", "token_estimate": 56}
{"text": "Table 5-5. Sysops Squad coupling analysis before component consolidation: Component", "domains": ["Design Principles", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 117", "position": 117, "chunk_type": "semantic", "token_estimate": 10}
{"text": "Used by: Customer Notification\n2\nBilling Payment, Support Contract\nTicket Notify\n2\nTicket, Ticket Route\nSurvey Notify\n1\nSurvey\nAddison then found that if the customer notification functionality was consolidated into a single\ncomponent, the coupling level for the resulting single component increased to an incoming cou-\npling level of 5, as shown in Table 5-6. Table 5-6. Sysops Squad coupling analysis after component consolidation\nComponent", "domains": ["Design Principles", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 117", "position": 117, "chunk_type": "semantic", "token_estimate": 66}
{"text": "Used by: Notification\n5\nBilling Payment, Support Contract, Ticket, Ticket Route, Survey\nAddison brought these findings to Austen, and they discussed the results. What they found is that,\nwhile the new consolidated component had a fairly high level of incoming coupling, it didn\u2019t affect\nthe overall afferent (incoming) coupling level for notifying a customer. In other words, the three sep-\narate components had a total incoming coupling level of 5, but so did the single consolidated\ncomponent. Addison and Austen both realized how important it was to analyze the coupling level after consoli-\ndating common domain functionality. In some cases, combining common domain functionality into\na single consolidated component increased the incoming coupling level of that component, thus\nresulting in too many dependencies on a single shared component within the application. However,\nin this case both Addison and Austen were comfortable with the coupling analysis, and agreed to\nconsolidate the notification functionality to reduce the duplication of both code and functionality. Addison wrote an architecture story to combine all of the notification functionality into a single\nnamespace representing a common Notification component. Sydney, assigned to the architecture\nstory, refactored the source code, creating a single component for customer notification, as illustra-\nted in Figure 5-5. Gather Common Domain Components Pattern \n| \n99", "domains": ["Design Principles", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 117", "position": 117, "chunk_type": "semantic", "token_estimate": 212}
{"text": "Figure 5-5. Notification functionality is consolidated into a new single component called: Notification\nTable 5-7 shows the resulting components after Sydney implemented the architecture story Addi-\nson created. Notice that the Customer Notification component (ss.customer.notification), Ticket\nNotify component (ss.ticket.notify), and Survey Notify components (ss.survey.notify) were\nremoved, and the source code moved to the new consolidated Notification component\n( ss.notification). 100 \n| \nChapter 5: Component-Based Decomposition Patterns", "domains": ["Design Principles", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 118", "position": 118, "chunk_type": "semantic", "token_estimate": 66}
{"text": "Table 5-7. Sysops Squad components after applying the Gather Common Domain: Components pattern\nComponent\nNamespace\nResponsibility\nLogin\nss.login\nUser and customer login\nBilling Payment\nss.billing.payment\nCustomer monthly billing\nBilling History\nss.billing.history\nPayment history\nCustomer Profile\nss.customer.profile\nMaintain customer profile\nExpert Profile\nss.expert.profile\nMaintain expert profile\nKB Maint\nss.kb.maintenance\nMaintain & view knowledge base\nKB Search\nss.kb.search\nSearch knowledge base\nNotification\nss.notification\nAll customer notification\nReporting Shared\nss.reporting.shared\nShared functionality\nTicket Reports\nss.reporting.tickets\nCreate ticketing reports\nExpert Reports\nss.reporting.experts\nCreate expert reports\nFinancial Reports\nss.reporting.financial\nCreate financial reports\nTicket\nss.ticket\nTicket creation & maintenance\nTicket Assign\nss.ticket.assign\nAssign expert to ticket\nTicket Route\nss.ticket.route\nSend ticket to expert\nSupport Contract\nss.supportcontract\nSupport contract maintenance\nSurvey\nss.survey\nSend and receive surveys\nSurvey Templates\nss.survey.templates\nMaintain survey templates\nUser Maintenance\nss.users\nMaintain internal users\nFlatten Components Pattern\nAs mentioned previously, components\u2014the building blocks of an application\u2014are\nusually identified through namespaces, package structures, or directory structures\nand are implemented through class files (or source code files) contained within these\nstructures. However, when components are built on top of other components, which\nare in turn built on top of other components, they start to lose their identity and stop\nbecoming components as per our definition. The Flatten Components pattern is used\nto ensure that components are not built on top of one another, but rather flattened\nand represented as leaf nodes in a directory structure or namespace. Flatten Components Pattern \n| \n101", "domains": ["Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 119", "position": 119, "chunk_type": "semantic", "token_estimate": 237}
{"text": "Pattern Description: When the namespace representing a particular component gets extended (in other\nwords, another node is added to the namespace or directory structure), the prior\nnamespace or directory no longer represents a component, but rather a subdomain. To illustrate this point, consider the customer survey functionality within the Sysops\nSquad application represented by two components: Survey (ss.survey) and Survey\nTemplates (ss.survey.templates). Notice in Table 5-8 how the ss.survey name\u2010\nspace, which contains five class files used to manage and collect the surveys, is exten\u2010\nded with the ss.survey.templates namespace to include seven classes representing\neach survey type send out to customers. Table 5-8. The Survey component contains orphaned classes and should be flattened\nComponent name\nComponent namespace\nFiles\n\u2192 Survey\nss.survey\n5\nSurvey Templates\nss.survey.templates\n7\nWhile this structure might seem to make sense from a developer\u2019s standpoint in order\nto keep the template code separate from survey processing, it does create some prob\u2010\nlems because Survey Templates, as a component, would be considered part of the Sur\u2010\nvey component. One might be tempted to consider Survey Templates as a\nsubcomponent of Survey, but then issues arise when trying to form services from these\ncomponents\u2014should both components reside in a single service called Survey, or\nshould the Survey Templates be a separate service from the Survey service? We\u2019ve resolved this dilemma by defining a component as the last node (or leaf node)\nof the namespace or directory structure. With this definition, ss.survey.templates\nis a component, whereas ss.survey would be considered a subdomain, not a compo\u2010\nnent. We further define namespaces such as ss.survey as root namespaces because\nthey are extended with other namespace nodes (in this case, .templates). Notice how the ss.survey root namespace in Table 5-8 contains five class files. We\ncall these class files orphaned classes because they do not belong to any definable com\u2010\nponent. Recall that a component is identified by a leaf node namespace containing\nsource code. Because the ss.survey namespace was extended to include .templates,\nss.survey is no longer considered a component and therefore should not contain\nany class files. 102 \n| \nChapter 5: Component-Based Decomposition Patterns", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 120", "position": 120, "chunk_type": "semantic", "token_estimate": 354}
{"text": "The following terms and corresponding definitions are important for understanding: and applying the Flatten Components decomposition pattern:\nComponent\nA collection of classes grouped within a leaf node namespace that performs some\nsort of specific functionality in the application (such as payment processing or\ncustomer survey functionality). Root namespace\nA namespace node that has been extended by another namespace node. For\nexample, given the namespaces ss.survey and ss.survey.templates, ss.sur\nvey would be considered a root namespace because it is extended by .templates. Root namespaces are also sometimes referred to as subdomains. Orphaned classes\nClasses contained within a root namespace, and hence have no definable compo\u2010\nnent associated with them. These definitions are illustrated in Figure 5-6, where the box with a C represents\nsource code contained within that namespace. This diagram (and all others like it) are\npurposely drawn from the bottom up to emphasize the notion of hills in the applica\u2010\ntion, as well as emphasize the notion of namespaces building upon each other. Figure 5-6. Components, root namespaces, and orphaned classes (C box denotes source\ncode)\nNotice that since both ss.survey and ss.ticket are extended through other name\u2010\nspace nodes, those namespaces are considered root namespaces, and the classes con\u2010\ntained in those root namespaces are hence orphaned classes (belonging to no defined\nFlatten Components Pattern \n| \n103", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 121", "position": 121, "chunk_type": "semantic", "token_estimate": 219}
{"text": "component). Thus, the only components denoted in Figure 5-6 are ss.survey.tem: plates, ss.login, ss.ticket.assign, and ss.ticket.route. The Flatten Components decomposition pattern is used to move orphaned classes to\ncreate well-defined components that exist only as leaf nodes of a directory or name\u2010\nspace, creating well-defined subdomains (root namespaces) in the process. We refer\nto the flattening of components as the breaking down (or building up) of namespaces\nwithin an application to remove orphaned classes. For example, one way of flattening\nthe ss.survey root namespace in Figure 5-6 and remove orphaned classes is to move\nthe source code contained in the ss.survey.templates namespace down to the\nss.survey namespace, thereby making ss.survey a single component (.survey is\nnow the leaf node of that namespace). This flattening option is illustrated in Figure\n5-7. Figure 5-7. Survey is flattened by moving the survey template code into the .survey\nnamespace\nAlternatively, flattening could also be applied by taking the source code in ss.survey\nand applying functional decomposition or domain-driven design to identify separate\nfunctional areas within the root namespace, thus forming components from those\nfunctional areas. For example, suppose the functionality within the ss.survey name\u2010\nspace creates and sends a survey to a customer, and then processes a completed sur\u2010\nvey received from the customer. Two components could be created from the\nss.survey namespace: ss.survey.create, which creates and sends the survey, and\nss.survey.process, which processes a survey received from a customer. This form\nof flattening is illustrated in Figure 5-8. 104 \n| \nChapter 5: Component-Based Decomposition Patterns", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 122", "position": 122, "chunk_type": "semantic", "token_estimate": 252}
{"text": "Figure 5-8. Survey is flattened by moving the orphaned classes to new leaf nodes: ( components)\nRegardless of the direction of flattening, make sure source code\nfiles reside only in leaf node namespaces or directories so that\nsource code can always be identified within a specific component. Another common scenario where orphaned source code might reside in a root name\u2010\nspace is when code is shared by other components within that namespace. Consider\nthe example in Figure 5-9 where customer survey functionality resides in three com\u2010\nponents (ss.survey.templates, ss.survey.create, and ss.survey.process), but\ncommon code (such as interfaces, abstract classes, common utilities) resides in the\nroot namespace ss.survey. Figure 5-9. Shared code in .survey is considered orphaned classes and should be moved\nFlatten Components Pattern \n| \n105", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 123", "position": 123, "chunk_type": "semantic", "token_estimate": 126}
{"text": "The shared classes in ss.survey would still be considered orphaned classes, even: though they represent shared code. Applying the Flatten Components pattern would\nmove those shared orphaned classes to a new component called ss.survey.shared,\ntherefore removing all orphaned classes from the ss.survey subdomain, as illustra\u2010\nted in Figure 5-10. Figure 5-10. Shared survey code is moved into its own component\nOur advice when moving shared code to a separate component (leaf node name\u2010\nspace) is to pick a word that is not used in any existing codebase in the domain, such\nas .sharedcode, .commoncode, or some such unique name. This allows the architect\nto generate metrics based on the number of shared components in the codebase, as\nwell as the percentage of source code that is shared in the application. This is a good\nindicator as to the feasibility of breaking up the monolithic application. For example,\nif the sum of all the statements in all namespaces ending with .sharedcode consti\u2010\ntutes 45% of the overall source code, chances are moving to a distributed architecture\nwill result in too many shared libraries and end up becoming a nightmare to maintain\nbecause of shared library dependencies. Another good metric involving the analysis of shared code is the number of compo\u2010\nnents ending in .sharedcode (or whatever common shared namespace node is used). This metric gives the architect insight into how many shared libraries (JAR, DLL, and\nso on) or shared services will result from breaking up the monolithic application. 106 \n| \nChapter 5: Component-Based Decomposition Patterns", "domains": ["Design Patterns", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 124", "position": 124, "chunk_type": "semantic", "token_estimate": 255}
{"text": "Fitness Functions for Governance: Applying the Flatten Components decomposition pattern involves a fair amount of\nsubjectivity. For example, should code from leaf nodes be consolidated into the root\nnamespace, or should code in a root namespace be moved into leaf nodes? That said,\nthe following fitness function can assist in automating the governance of keeping\ncomponents flat (only in leaf nodes). Fitness function: No source code should reside in a root namespace\nThis automated holistic fitness function can be triggered on deployment through a\nCI/CD pipeline to locate orphaned classes\u2014classes that reside in a root namespace. Use of this fitness function helps keep components flat when undergoing a mono\u2010\nlithic migration, especially when performing ongoing maintenance to the monolithic\napplication during the migration effort. Example 5-6 shows the pseudocode that\nalerts an architect when orphaned classes appear anywhere in the codebase. Example 5-6. Pseudocode for finding code in root namespaces", "domains": ["Design Principles", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 125", "position": 125, "chunk_type": "semantic", "token_estimate": 150}
{"text": "Send an alert if a non-leaf node in any component contains source files: FOREACH component IN component_list {\n  LIST component_node_list = get_nodes(component)\n  FOREACH node IN component_node_list {\n    IF contains_code(node) AND NOT last_node(component_node_list) {\n      send_alert(component)\n    }\n  }\n}\nSysops Squad Saga: Flattening Components\nWednesday, November 10, 11:10\nAfter applying the \u201cGather Common Domain Components Pattern\u201d on page 94, Addison\nanalyzed the results in Table 5-7 and observed that the Survey and Ticket components\ncontained orphaned classes. Addison highlighted these components in Table 5-9 and in\nFigure 5-11. Flatten Components Pattern \n| \n107", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 125", "position": 125, "chunk_type": "semantic", "token_estimate": 91}
{"text": "Figure 5-11. The Survey and Ticket components contain orphaned classes and should be: flattened\nTable 5-9. Sysops Squad Ticket and Survey components should be flattened\nComponent name\nComponent namespace\nStatements\nFiles\nTicket\nss.ticket\n7,009\n45\nTicket Assign\nss.ticket.assign\n7,845\n14\nTicket Route\nss.ticket.route\n1,468\n4\nSurvey\nss.survey\n2,204\n5\nSurvey Templates\nss.survey.templates\n1,672\n7\n108 \n| \nChapter 5: Component-Based Decomposition Patterns", "domains": ["Design Patterns", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 126", "position": 126, "chunk_type": "semantic", "token_estimate": 61}
{"text": "Addison decided to address the ticketing components first. Knowing that flattening components: meant getting rid of source code in nonleaf nodes, Addison had two choices: consolidate the code\ncontained in the ticket assignment and ticket routing components into the ss.ticket component,\nor break up the 45 classes in the ss.ticket component into separate components, thus making\nss.ticket a subdomain. Addison discussed these options with Sydney (one of the Sysops Squad\ndevelopers), and based on the complexity and frequent changes in the ticket assignment function-\nality, decided to keep those components separate and move the orphaned code from the\nss.ticket root namespace into other namespaces, thus forming new components. With help from Sydney, Addison found that the 45 orphaned classes contained in the ss.ticket\nnamespace implemented the following ticketing functionality:\n\u2022 Ticket creation and maintenance (creating a ticket, updating a ticket, canceling a ticket, etc.) \u2022 Ticket completion logic\n\u2022 Shared code common to most of the ticketing functionality\nSince ticket assignment and ticket routing functionality were already in their own components\n(ss.ticket.assign and ss.ticket.route, respectively), Addison created an architecture story to\nmove the source code contained in the ss.ticket namespace to three new components, as shown\nin Table 5-10. Table 5-10. The prior Sysops Squad Ticket component broken into three new components\nComponent\nNamespace\nResponsibility\nTicket Shared\nss.ticket.shared\nCommon code and utilities\nTicket Maintenance\nss.ticket.maintenance\nAdd and maintain tickets\nTicket Completion\nss.ticket.completion\nComplete ticket and initiate survey\nTicket Assign\nss.ticket.assign\nAssign expert to ticket\nTicket Route\nss.ticket.route\nSend ticket to expert\nAddison then considered the survey functionality. Working with Sydney, Addison found that the sur-\nvey functionality rarely changed and was not overly complicated. Sydney talked with Skyler, the\nSysops Squad developer who originally created the ss.survey.templates namespace, and found\nthere was no compelling reason to separate the survey templates into their own namespace (\u201cIt just\nseemed like a good idea at the time,\u201d said Skyler). With this information, Addison created an architec-\nture story to move the seven class files from ss.survey.templates into the ss.survey namespace\nand removed the ss.survey.template component, as shown in Table 5-11. Flatten Components Pattern \n| \n109", "domains": ["Design Principles", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 127", "position": 127, "chunk_type": "semantic", "token_estimate": 352}
{"text": "Table 5-11. The prior Sysops Squad Survey components flattened into a single component: Component\nNamespace\nResponsibility\nSurvey\nss.survey\nSend and seceive surveys\nAfter applying the Flatten Components pattern (illustrated in Figure 5-12), Addison observed that\nthere were no \u201chills\u201d (component upon component) or orphaned classes and that all of the compo-\nnents were contained only in the leaf nodes of the corresponding namespace. Figure 5-12. The Survey component was flattened into a single component, whereas the\nTicket component was raised up and flattened, creating a Ticket subdomain\nAddison recorded the results of the refactoring efforts thus far in applying these decomposition pat-\nterns and listed them in Table 5-12. 110 \n| \nChapter 5: Component-Based Decomposition Patterns", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 128", "position": 128, "chunk_type": "semantic", "token_estimate": 116}
{"text": "Table 5-12. Sysops Squad components after applying the Flatten Components pattern: Component\nNamespace\nLogin\nss.login\nBilling Payment\nss.billing.payment\nBilling History\nss.billing.history\nCustomer Profile\nss.customer.profile\nExpert Profile\nss.expert.profile\nKB Maint\nss.kb.maintenance\nKB Search\nss.kb.search\nNotification\nss.notification\nReporting Shared\nss.reporting.shared\nTicket Reports\nss.reporting.tickets\nExpert Reports\nss.reporting.experts\nFinancial Reports\nss.reporting.financial\nTicket Shared\nss.ticket.shared\nTicket Maintenance\nss.ticket.maintenance\nTicket Completion\nss.ticket.completion\nTicket Assign\nss.ticket.assign\nTicket Route\nss.ticket.route\nSupport Contract\nss.supportcontract\nSurvey\nss.survey\nUser Maintenance\nss.users\nDetermine Component Dependencies Pattern\nThree of the most common questions asked when considering a migration from a\nmonolithic application to a distributed architecture are as follows:\n1. Is it feasible to break apart the existing monolithic application? 2. What is the rough overall level of effort for this migration? 3. Is this going to require a rewrite of the code or a refactoring of the code? Determine Component Dependencies Pattern \n| \n111", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 129", "position": 129, "chunk_type": "semantic", "token_estimate": 142}
{"text": "One of your authors was engaged several years ago in a large migration effort to move: a complex monolithic application to microservices. On the first day of the project, the\nCIO wanted to know only one thing\u2014was this migration effort a golfball, basketball,\nor an airliner? Your author was curious about the sizing comparisons, but the CIO\ninsisted that the answer to this simple question shouldn\u2019t be that difficult given that\nkind of coarse-grained sizing. As it turned out, applying the Determine Component\nDependencies pattern quickly and easily answered this question for the CIO\u2014the\neffort was unfortunately an airliner, but only a small Embraer 190 migration rather\nthan a large Boeing 787 Dreamliner migration. Pattern Description\nThe purpose of the Determine Component Dependencies pattern is to analyze the\nincoming and outgoing dependencies (coupling) between components to determine\nwhat the resulting service dependency graph might look like after breaking up the\nmonolithic application. While there are many factors in determining the right level of\ngranularity for a service (see Chapter 7), each component in the monolithic applica\u2010\ntion is potentially a service candidate (depending on the target distributed architec\u2010\nture style). For this reason, it is critical to understand the interactions and\ndependencies between components. It\u2019s important to note that this pattern is about component dependencies, not individ\u2010\nual class dependencies within a component. A component dependency is formed when\na class from one component (namespace) interacts with a class from another compo\u2010\nnent (namespace). For example, suppose the CustomerSurvey class in the ss.survey\ncomponent invokes a method in the CustomerNotification class in the ss.notifica\ntion component to send out the customer survey, as illustrated in the pseudocode in\nExample 5-7. Example 5-7. Pseudocode showing a dependency between the Survey and Notification\ncomponents\nnamespace ss.survey\nclass CustomerSurvey {\n   function createSurvey {\n      ...\n   }\n   function sendSurvey {\n      ...\n      ss.notification.CustomerNotification.send(customer_id, survey)\n   }\n}\n112 \n| \nChapter 5: Component-Based Decomposition Patterns", "domains": ["Architectural Patterns and Styles", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 130", "position": 130, "chunk_type": "semantic", "token_estimate": 317}
{"text": "Notice the dependency between the Survey and Notification components, because the: CustomerNotification class used by the CustomerSurvey class resides outside the\nss.survey namespace. Specifically, the Survey component would have an efferent (or\noutgoing) dependency on the Notification component, and the Notification compo\u2010\nnent would have an afferent (or incoming) dependency on the Survey component. Note that the classes within a particular component may be a highly coupled mess of\nnumerous dependencies, but that doesn\u2019t matter when applying this pattern\u2014what\nmatters is only those dependencies between components. Several tools are available that can assist in applying this pattern and visualizing com\u2010\nponent dependencies. In addition, many modern IDEs have plug-ins that will pro\u2010\nduce dependency diagrams of the components, or namespaces, within a particular\ncodebase. These visualizations can be useful in answering the three key questions\nposed at the start of this section. For example, consider the dependency diagram shown in Figure 5-13, where the\nboxes represent components (not classes), and the lines represent coupling points\nbetween the components. Notice there is only a single dependency between the com\u2010\nponents in this diagram, making this application a good candidate for breaking apart\nsince the components are functionally independent from one another. Figure 5-13. A monolithic application with minimal component dependencies takes less\neffort to break apart (golf ball sizing)\nDetermine Component Dependencies Pattern \n| \n113", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 131", "position": 131, "chunk_type": "semantic", "token_estimate": 224}
{"text": "With a dependency diagram like Figure 5-13, the answers to the three key questions: are as follows:\n1. Is it feasible to break apart the existing monolithic application? Yes\n2. What is the rough overall level of effort for this migration? A golf ball (relatively\nstraightforward)\n3. Is this going to be a rewrite of the code or a refactoring of the code? Refactoring\n(moving existing code into separately deployed services)\nNow look at the dependency diagram shown in Figure 5-14. Unfortunately, this dia\u2010\ngram is typical of the dependencies between components in most business applica\u2010\ntions. Notice in particular how the lefthand side of this diagram has the highest level\nof coupling, whereas the righthand side looks much more feasible to break apart. Figure 5-14. A monolithic application with a high number of component dependencies\ntakes more effort to break apart (basketball sizing)\n114 \n| \nChapter 5: Component-Based Decomposition Patterns", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 132", "position": 132, "chunk_type": "semantic", "token_estimate": 151}
{"text": "With this level of tight coupling between components, the answers to the three key: questions are not very encouraging:\n1. Is it feasible to break apart the existing monolithic application? Maybe\u2026\n2. What is the rough overall level of effort for this migration? A basketball (much\nharder)\n3. Is this going to be a rewrite of the code or a refactoring of the code? Likely a com\u2010\nbination of some refactoring and some rewriting of the existing code\nFinally, consider the dependency diagram illustrated in Figure 5-15. In this case, the\narchitect should turn around and run in the opposite direction as fast as they can! Figure 5-15. A monolithic application with too many component dependencies is not\nfeasible to break apart (airliner sizing)\nDetermine Component Dependencies Pattern \n| \n115", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 133", "position": 133, "chunk_type": "semantic", "token_estimate": 130}
{"text": "The answers to the three key questions for applications with this sort of component: dependency matrix are not surprising:\n1. Is it feasible to break apart the existing monolithic application? No\n2. What is the rough overall level of effort for this migration? An airliner\n3. Is this going to be a rewrite of the code or a refactoring of the code? Total rewrite\nof the application\nWe cannot stress enough the importance of these kinds of visual diagrams when\nbreaking apart a monolithic application. In essence these diagrams form a radar from\nwhich to determine where the enemy (high component coupling) is located, and also\npaint a picture of what the resulting service dependency matrix might look like if the\nmonolithic application were to be broken into a highly distributed architecture. It has been our experience that component coupling is one of the most significant\nfactors in determining the overall success (and feasibility) of a monolithic migration\neffort. Identifying and understanding the level of component coupling not only\nallows the architect to determine the feasibility of the migration effort, but also what\nto expect in terms of the overall level of effort. Unfortunately, all too often we see\nteams jump straight into breaking a monolithic application into microservices\nwithout having any analysis or visuals into what the monolithic application even\nlooks like. And not surprisingly, those teams struggle to break apart their monolithic\napplications. This pattern is useful not only for identifying the overall level of component coupling\nin an application, but also for determining dependency refactoring opportunities\nprior to breaking apart the application. When analyzing the coupling level between\ncomponents, it is important to analyze both afferent (incoming) coupling (denoted in\nmost tools as CA), and efferent (outgoing) coupling (denoted in most tools as CE). CT, or total coupling, is the sum of both afferent and efferent coupling. Many times, breaking apart a component can reduce the level of coupling of that\ncomponent. For example, assume component A has an afferent coupling level of 20\n(meaning, 20 other components are dependent on the functionality of the compo\u2010\nnent). This does not necessarily mean that all 20 of the other components require all\nof the functionality from component A. Maybe 14 of the other components require\nonly a small part of the functionality contained in component A.", "domains": ["Architectural Patterns and Styles", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 134", "position": 134, "chunk_type": "semantic", "token_estimate": 388}
{"text": "The answers to the three key questions for applications with this sort of component: This does not necessarily mean that all 20 of the other components require all\nof the functionality from component A. Maybe 14 of the other components require\nonly a small part of the functionality contained in component A. Breaking compo\u2010\nnent A into two different components (component A1 containing the smaller, cou\u2010\npled functionality, and component A2 containing the majority of the functionality)\nreduces the afferent coupling in component A2 to 6, with component A1 having an\nafferent coupling level of 14. 116 \n| \nChapter 5: Component-Based Decomposition Patterns", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 134", "position": 134, "chunk_type": "semantic", "token_estimate": 103}
{"text": "Fitness Functions for Governance: Two ways to automate the governance for component dependencies are to make sure\nno component has \u201ctoo many\u201d dependencies, and to restrict certain components\nfrom being coupled to other components. The fitness functions described next are\nsome ways of governing these type of dependencies. Fitness function: No component shall have more than <some number> of total dependencies\nThis automated holistic fitness function can be triggered on deployment through a\nCI/CD pipeline to make sure that the coupling level of any given component doesn\u2019t\nexceed a certain threshold. It is up to the architect to determine that this maximum\nthreshold should be based on the overall level of coupling within the application and\nthe number of components. An alert generated from this fitness function allows the\narchitect to discuss any sort of increase in coupling with the development team, pos\u2010\nsibly promoting action to break apart components to reduce coupling. This fitness\nfunction could also be modified to generate an alert for a threshold limit of incoming\nonly, outgoing only, or both (as separate fitness functions). Example 5-8 shows the\npseudocode for sending an alert if the total coupling (incoming and outgoing)\nexceeds a combined level of 15, which for most applications would be considered rel\u2010\natively high. Example 5-8. Pseudocode for limiting the total number of dependencies of any given\ncomponent", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 135", "position": 135, "chunk_type": "semantic", "token_estimate": 225}
{"text": "contained within those components: LIST component_list = identify_components(root_directory)\nMAP component_source_file_map\nFOREACH component IN component_list {\n  LIST component_source_file_list = get_source_files(component)\n  ADD component, component_source_file_list TO component_source_file_map\n}", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 135", "position": 135, "chunk_type": "semantic", "token_estimate": 25}
{"text": "the total dependency count is greater than 15: FOREACH component,component_source_file_list IN component_source_file_map {\n  FOREACH source_file IN component_source_file_list {\n    incoming count = used_by_other_components(source_file, component_source_file_map) {\n    outgoing_count = uses_other_components(source_file) {\n    total_count = incoming count + outgoing count\n  }\n  IF total_count > 15 {\n    send_alert(component, total_count)\n  }\n}\nDetermine Component Dependencies Pattern \n| \n117", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 135", "position": 135, "chunk_type": "semantic", "token_estimate": 51}
{"text": "Fitness function: <some component> should not have a dependency on <another: component>\nThis automated holistic fitness function can be triggered on deployment through a\nCI/CD pipeline to restrict certain components from having a dependency on other\nones. In most cases, there will be one fitness function for each dependency restriction\nso that, if there were 10 different component restrictions, there would be 10 different\nfitness functions, one for each component in question. Example 5-9 shows an exam\u2010\nple using ArchUnit for ensuring that the Ticket Maintenance component\n(ss.ticket.maintenance) does not have a dependency on the Expert Profile compo\u2010\nnent (ss.expert.profile). Example 5-9. ArchUnit code for governing dependency restrictions between components\npublic void ticket_maintenance_cannot_access_expert_profile() {\n   noClasses().that()\n   .resideInAPackage(\"..ss.ticket.maintenance..\")\n   .should().accessClassesThat()\n   .resideInAPackage(\"..ss.expert.profile..\")\n   .check(myClasses);\n}\nSysops Squad Saga: Identifying Component Dependencies\nMonday, November 15, 09:45\nAfter reading about the Determine Component Dependencies pattern, Addison won-\ndered what the Sysops Squad application dependency matrix looked like and whether it\nwas feasible to even break the application apart. Addison used an IDE plug-in to gener-\nate a component dependency diagram of the current Sysops Squad application. Initially,\nAddison felt a bit discouraged because Figure 5-16 showed a lot of dependencies\nbetween the Sysops Squad application components. 118 \n| \nChapter 5: Component-Based Decomposition Patterns", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 136", "position": 136, "chunk_type": "semantic", "token_estimate": 206}
{"text": "Figure 5-16. Component dependencies in the Sysops Squad application: However, after further analysis, Addison saw that the Notification component had the most depen-\ndencies, which was not surprising given that it\u2019s a shared component. However, Addison also saw\nlots of dependencies within the Ticketing and Reporting components. Both of these domain areas\nhave a specific component for shared code (interfaces, helper classes, entity classes, and so on). Real-\nizing that both the ticketing and reporting shared code contains mostly compile-based class refer-\nences and would likely be implemented as shared libraries rather than services, Addison filtered out\nthese components to get a better view of the dependencies between the core functionality of the\napplication, which is illustrated in Figure 5-17. Determine Component Dependencies Pattern \n| \n119", "domains": ["Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 137", "position": 137, "chunk_type": "semantic", "token_estimate": 126}
{"text": "Figure 5-17. Component dependencies in the Sysops Squad application without shared: library dependencies\nWith the shared components filtered out, Addison saw that the dependencies were fairly minimal. Addison showed these results to Austen, and they both agreed that most of the components were\nrelatively self-contained and it appeared that the Sysops Squad application was a good candidate\nfor breaking apart into a distributed architecture. Create Component Domains Pattern\nWhile each component identified within a monolithic application can be considered\na possible candidate for a separate service, in most cases the relationship between a\nservice and components is a one-to-many relationship\u2014that is, a single service may\ncontain one or more components. The purpose of the Create Component Domains\npattern is to logically group components together so that more coarse-grained\ndomain services can be created when breaking up an application. 120 \n| \nChapter 5: Component-Based Decomposition Patterns", "domains": ["Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 138", "position": 138, "chunk_type": "semantic", "token_estimate": 145}
{"text": "Pattern Description: Identifying component domains\u2014the grouping of components that perform some\nsort of related functionality\u2014is a critical part of breaking apart any monolithic appli\u2010\ncation. Recall the advice from Chapter 4:\nWhen breaking apart monolithic applications, consider first moving to service-based\narchitecture as a stepping-stone to other distributed architectures. Creating component domains is an effective way of determining what will eventually\nbecome domain services in a service-based architecture. Component domains are physically manifested in an application through component\nnamespaces (or directories). Because namespace nodes are hierarchical in nature,\nthey become an excellent way of representing the domains and subdomains of func\u2010\ntionality. This technique is illustrated in Figure 5-18, where the second node in the\nnamespace (.customer) refers to the domain, the third node represents a subdomain\nunder the customer domain (.billing), and the leaf node (.payment) refers to the\ncomponent. The .MonthlyBilling at the end of this namespace refers to a class file\ncontained within the Payment component. Figure 5-18. Component domains are identified through the namespace nodes\nSince many older monolithic applications were implemented prior to the widespread\nuse of domain-driven design, in many cases refactoring of the namespaces is needed\nto structurally identify domains within the application. For example, consider the\ncomponents listed in Table 5-13 that make up the Customer domain within the\nSysops Squad application. Table 5-13. Components related to the Customer domain before refactoring\nComponent\nNamespace\nBilling Payment\nss.billing.payment\nBilling History\nss.billing.history\nCustomer Profile\nss.customer.profile\nSupport Contract\nss.supportcontract\nCreate Component Domains Pattern \n| \n121", "domains": ["Design Patterns", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 139", "position": 139, "chunk_type": "semantic", "token_estimate": 250}
{"text": "Notice how each component is related to customer functionality, but the correspond\u2010: ing namespaces don\u2019t reflect that association. To properly identify the Customer\ndomain (manifested through the namespace ss.customer), the namespaces for the\nBilling Payment, Billing History, and Support Contract components would have to be\nmodified to add the .customer node at the beginning of the namespace, as shown in\nTable 5-14. Table 5-14. Components related to the Customer domain after refactoring\nComponent\nNamespace\nBilling Payment\nss.customer.billing.payment\nBilling History\nss.customer.billing.history\nCustomer Profile\nss.customer.profile\nSupport Contract\nss.customer.supportcontract\nNotice in the prior table that all of the customer-related functionality (billing, profile\nmaintenance, and support contract maintenance) is now grouped under .customer,\naligning each component with that particular domain. Fitness Functions for Governance\nOnce refactored, it\u2019s important to govern the component domains to ensure that\nnamespace rules are enforced and that no code exists outside the context of a compo\u2010\nnent domain or subdomain. The following automated fitness function can be used to\nhelp govern component domains once they are established within the monolithic\napplication. Fitness function: All namespaces under <root namespace node> should be restricted to <list\nof domains>\nThis automated holistic fitness function can be triggered on deployment through a\nCI/CD pipeline to restrict the domains contained within an application. This fitness\nfunction helps prevent additional domains from being inadvertently created by devel\u2010\nopment teams and alerts the architect if any new namespaces (or directories) are cre\u2010\nated outside the approved list of domains. Example 5-10 shows an example using\nArchUnit for ensuring that only the ticket, customer, and admin domains exist within\nan application. 122 \n| \nChapter 5: Component-Based Decomposition Patterns", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 140", "position": 140, "chunk_type": "semantic", "token_estimate": 271}
{"text": "Example 5-10. ArchUnit code for governing domains within an application: public void restrict_domains() {\n   classes()\n     .should().resideInAPackage(\"..ss.ticket..\")\n     .orShould().resideInAPackage(\"..ss.customer..\")\n     .orShould().resideInAPackage(\"..ss.admin..\")\n     .check(myClasses);\n}\nSysops Squad Saga: Creating Component Domains\nThursday, November 18, 13:15\nAddison and Austen consulted with Parker, the Sysops Squad product owner, and\ntogether they identified five main domains within the application: a Ticketing domain\n(ss.ticket) containing all ticket-related functionality, including ticket processing, cus-\ntomer surveys, and knowledge base (KB) functionality; a Reporting domain (ss.report\ning) containing all reporting functionality; a Customer domain (ss.customer)\ncontaining customer profile, billing, and support contracts; an Admin domain\n(ss.admin) containing maintenance of users and Sysops Squad experts; and finally, a\nShared domain (ss.shared) containing login and notification functionality used by the other\ndomains. Addison created a domain diagram (see Figure 5-19) showing the various domains and correspond-\ning groups of components within each domain, and was satisfied with this grouping as no compo-\nnent was left out, and there was good cohesion between the components within each domain. The exercise Addison did in diagramming and grouping the components was an important one as it\nvalidated the identified domain candidates and also demonstrated the need for collaboration with\nbusiness stakeholders (such as the product owner or business application sponsor). Had the compo-\nnents not lined up properly or Addison was left with components that didn\u2019t belong anywhere,\nmore collaboration with Parker (the product owner) would have been necessary. Satisfied that all of the components fit nicely into these domains, Addison then looked at the various\ncomponent namespaces in Table 5-12 after applying the \u201cFlatten Components Pattern\u201d on page 101\nand identified the component domain refactoring that needed to take place. Create Component Domains Pattern \n| \n123", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 141", "position": 141, "chunk_type": "semantic", "token_estimate": 280}
{"text": "Figure 5-19. The five domains identified (with darkened borders) within the Sysops: Squad application\nAddison started with the Ticket domain and saw that while the core ticket functionality started with\nthe namespace ss.ticket, the survey and knowledge base components did not. Therefore, Addi-\nson wrote an architecture story to refactor the components listed in Table 5-15 to align with the\nticketing domain. 124 \n| \nChapter 5: Component-Based Decomposition Patterns", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 142", "position": 142, "chunk_type": "semantic", "token_estimate": 69}
{"text": "Table 5-15. Sysops Squad component refactoring for the Ticket domain: Component\nDomain\nCurrent namespace\nTarget namespace\nKB Maint\nTicket\nss.kb.maintenance\nss.ticket.kb.maintenance\nKB Search\nTicket\nss.kb.search\nss.ticket.kb.search\nTicket Shared\nTicket\nss.ticket.shared\nSame (no change)\nTicket Maintenance\nTicket\nss.ticket.maintenance\nSame (no change)\nTicket Completion\nTicket\nss.ticket.completion\nSame (no change)\nTicket Assign\nTicket\nss.ticket.assign\nSame (no change)\nTicket Route\nTicket\nss.ticket.route\nSame (no change)\nSurvey\nTicket\nss.survey\nss.ticket.survey\nNext Addison considered the customer-related components, and found that the billing and survey\ncomponents needed to be refactored to include them under the Customer domain, creating a Bill-\ning subdomain in the process. Addison wrote an architecture story for the refactoring of the Cus-\ntomer domain functionality, shown in Table 5-16. Table 5-16. Sysops Squad component refactoring for the Customer domain\nComponent\nDomain\nCurrent namespace\nTarget namespace\nBilling Payment\nCustomer\nss.billing.payment\nss.customer.billing.payment\nBilling History\nCustomer\nss.billing.history\nss.customer.billing.history\nCustomer Profile\nCustomer\nss.customer.profile\nSame (no change)\nSupport Contract\nCustomer\nss.supportcontract\nss.customer.supportcontract\nBy applying the \u201cIdentify and Size Components Pattern\u201d on page 84, Addison found that the report-\ning domain was already aligned, and no further action was needed with the reporting components\nlisted in Table 5-17. Table 5-17. Sysops Squad Reporting components are already aligned with the Reporting\ndomain\nComponent\nDomain\nCurrent namespace\nTarget namespace\nReporting Shared\nReporting\nss.reporting.shared\nSame (no change)\nTicket Reports\nReporting\nss.reporting.tickets\nSame (no change)\nExpert Reports\nReporting\nss.reporting.experts\nSame (no change)\nFinancial Reports\nReporting\nss.reporting.financial\nSame (no change)\nCreate Component Domains Pattern \n| \n125", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 143", "position": 143, "chunk_type": "semantic", "token_estimate": 242}
{"text": "Addison saw that both the Admin and Shared domains needed alignment as well, and decided to: create a single architecture story for this refactoring effort and listed these components in\nTable 5-18. Addison also decided to rename the ss.expert.profile namespace to ss.experts to\navoid an unnecessary Expert subdomain under the Admin domain. Table 5-18. Sysops Squad component refactoring for the Admin and Shared domains\nComponent\nDomain\nCurrent namespace\nTarget namespace\nLogin\nShared\nss.login\naa.shared.login\nNotification\nShared\nss.notification\nss.shared.notification\nExpert Profile\nAdmin\nss.expert.profile\nss.admin.experts\nUser Maintenance\nAdmin\nss.users\nss.admin.users\nWith this pattern complete, Addison realized they were now prepared to structurally break apart the\nmonolithic application and move to the first stage of a distributed architecture by applying the Cre-\nate Domain Services pattern (described in the next section). Create Domain Services Pattern\nOnce components have been properly sized, flattened, and grouped into domains, \nthose domains can then be moved to separately deployed domain services, creating\nwhat is known as a service-based architecture (see Appendix A). Domain services are\ncoarse-grained, separately deployed units of software containing all of the functional\u2010\nity for a particular domain (such as Ticketing, Customer, Reporting, and so on). Pattern Description\nThe previous \u201cCreate Component Domains Pattern\u201d on page 120 forms well-defined\ncomponent domains within a monolithic application and manifests those domains\nthrough the component namespaces (or directory structures). This pattern takes\nthose well-defined component domains and extracts those component groups into\nseparately deployed services, known as a domain services, thus creating a service-\nbased architecture. In its simplest form, service-based architecture consists of a user interface that\nremotely accesses coarse-grained domain services, all sharing a single monolithic\ndatabase. Although there are many topologies within service-based architecture (such\nas breaking up the user interface, breaking up the database, adding an API gateway,\nand so on), the basic topology shown in Figure 5-20 is a good starting point for\nmigrating a monolithic application. 126 \n| \nChapter 5: Component-Based Decomposition Patterns", "domains": ["Design Patterns", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 144", "position": 144, "chunk_type": "semantic", "token_estimate": 322}
{"text": "Figure 5-20. The basic topology for a service-based architecture: In addition to the benefits mentioned in \u201cComponent-Based Decomposition\u201d on page\n71, moving to service-based architecture first allows the architect and development\nteam to learn more about each domain service to determine whether it should be\nbroken into smaller services within a microservices architecture or left as a larger\ndomain service. Too many teams make the mistake of starting out too fine-grained,\nand as a result must embrace all of the trappings of microservices (such as data\ndecomposition, distributed workflows, distributed transactions, operational automa\u2010\ntion, containerization, and so on) without the need for all of those fine-grained\nmicroservices. Figure 5-21 illustrates how the Create Domain Services pattern works. Notice in the\ndiagram how the Reporting component domain defined in the \u201cCreate Component\nDomains Pattern\u201d on page 120 is extracted from of the monolithic application, form\u2010\ning its own separately deployed Reporting service. Create Domain Services Pattern \n| \n127", "domains": ["Architectural Patterns and Styles", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 145", "position": 145, "chunk_type": "semantic", "token_estimate": 157}
{"text": "Figure 5-21. Component domains are moved to external domain services: A word of advice, however: don\u2019t apply this pattern until all of the component\ndomains have been identified and refactored. This helps reduce the amount of modi\u2010\nfication needed to each domain service when moving components (and hence source\ncode) around. For example, suppose all of the ticketing and knowledge base function\u2010\nality in the Sysops Squad application was grouped and refactored into a Ticket\ndomain, and a new Ticket service created from that domain. Now suppose that the\ncustomer survey component (identified through the ss.customer.survey name\u2010\nspace) was deemed part of the Ticket domain. Since the Ticket domain had already\nbeen migrated, the Ticket service would now have to be modified to include the Sur\u2010\nvey component. Better to align and refactor all of the components into component\ndomains first, then start migrating those component domains to domain services. 128 \n| \nChapter 5: Component-Based Decomposition Patterns", "domains": ["Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 146", "position": 146, "chunk_type": "semantic", "token_estimate": 157}
{"text": "Fitness Functions for Governance: It is important to keep the components within each domain service aligned with the\ndomain, particularly if the domain service will be broken into smaller microservices. This type of governance helps keep domain services from becoming their own\nunstructured monolithic service. The following fitness function ensures that the\nnamespace (and hence components) are kept consistent within a domain service. Fitness function: All components in <some domain service> should start with the same\nnamespace\nThis automated holistic fitness function can be triggered on deployment through a\nCI/CD pipeline to make sure the namespaces for components within a domain ser\u2010\nvice remain consistent. For example, all components within the Ticket domain ser\u2010\nvice should start with ss.ticket. Example 5-11 uses ArchUnit for ensuring this\nconstraint. Each domain service would have its own corresponding fitness function\nbased on its particular domain. Example 5-11. ArchUnit code for governing components within the Ticket domain\nservice\npublic void restrict_domain_within_ticket_service() {\n   classes().should().resideInAPackage(\"..ss.ticket..\")\n   .check(myClasses);\n}\nSysops Squad Saga: Creating Domain Services\nTuesday, November 23, 09:04\nAddison and Austen worked closely with the Sysops Squad development team to\ndevelop a migration plan to stage the migration from component domains to domain\nservices. They realized this effort not only required the code within each component\ndomain to be extracted from the monolith and moved to a new project workspace, but\nalso for the user interface to now remotely access the functionality within that domain. Working from the component domains identified previously in Figure 5-19, the team\nmigrated each component, one at a time, eventually arriving at a service-based architec-\nture, as shown in Figure 5-22. Notice how each domain area identified in the previous pattern now\nbecomes a separately deployed service. Create Domain Services Pattern \n| \n129", "domains": ["Architectural Patterns and Styles", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 147", "position": 147, "chunk_type": "semantic", "token_estimate": 291}
{"text": "Figure 5-22. Separately deployed domain services result in a distributed Sysops Squad: application\nSummary\nIt has been our experience that \u201cseat-of-the-pants\u201d migration efforts rarely produce\npositive results. Applying these component-based decomposition patterns provides a\nstructured, controlled, and incremental approach for breaking apart monolithic\narchitectures. Once these patterns are applied, teams can now work to decompose\nmonolithic data (see Chapter 6) and begin breaking apart domain services into more\nfine-grained microservices (see Chapter 7) as needed. 130 \n| \nChapter 5: Component-Based Decomposition Patterns", "domains": ["Architectural Patterns and Styles", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 148", "position": 148, "chunk_type": "semantic", "token_estimate": 82}
{"text": "Addison quickly realized things were spiraling out of control, and quickly put some key negotiation: and facilitation skills to use. \u201cOK,\u201d said Addison, \u201cwe should have included you in our initial discus-\nsions, and for that I apologize. I should have known better. What can we do to bring you on board\nand help us decompose the Sysops Squad database?\u201d\n\u201cThat\u2019s easy,\u201d said Dana. \u201cConvince me that the Sysops Squad database really does need to be broken\napart. Provide me with a solid justification. If you can do that, then we\u2019ll talk about Devon\u2019s five-step\nprocess. Otherwise, it stays as it is.\u201d\nBreaking apart a database is hard\u2014much harder, in fact, than breaking apart applica\u2010\ntion functionality. Because data is generally the most important asset in the company,\nthere is greater risk of business and application disruption when breaking apart or\nrestructuring data. Also, data tends to be highly coupled to application functionality,\nmaking it harder to identify well-defined seams within a large data model. In the same way a monolithic application is broken into separate deployment units,\nthere are times when it is desirable (or even necessary) to break up a monolithic data\u2010\nbase as well. Some architecture styles, such as microservices, require data to be broken\napart to form well-defined bounded contexts (where each service owns its own data), \nwhereas other distributed architectures, such as service-based architecture, allow\nservices to share a single database. Interestingly enough, some of the same techniques used to break apart application\nfunctionality can be applied to breaking apart data as well. For example, components\ntranslate to data domains, class files translate to database tables, and coupling points\nbetween classes translate to database artifacts such as foreign keys, views, triggers, or\neven stored procedures. In this chapter, we explore some of the drivers for decomposing data and show tech\u2010\nniques for how to effectively break apart monolithic data into separate data domains,\nschemas, and even separate databases in an iterative and controlled fashion. Knowing\nthat the database world is not all relational, we also discuss various types of databases\n(relational, graph, document, key-value, columnar, NewSQL, and cloud native) and\noutline the various trade-offs associated with each of these database types. Data Decomposition Drivers\nBreaking apart a monolithic database can be a daunting task, and as such it\u2019s impor\u2010\ntant to understand if (and when) a database should be decomposed, as illustrated in\nFigure 6-1.", "domains": ["Architectural Patterns and Styles", "Design Principles", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 150", "position": 150, "chunk_type": "semantic", "token_estimate": 400}
{"text": "these two driving forces and analyzing the trade-offs of each is the key to getting data: granularity right. Figure 6-1. Under what circumstances should a monolithic database be decomposed? In this section, we will explore the data disintegrators and data integrators used to\nhelp make the right choice when considering breaking apart monolithic data. Data Disintegrators\nData disintegration drivers provide answers and justifications for the question \u201cwhen\nshould I consider breaking apart my data?\u201d The six main disintegration drivers for\nbreaking apart data include the following:\nChange control\nHow many services are impacted by a database table change? Connection management\nCan my database handle the connections needed from multiple distributed serv\u2010\nices? Scalability\nCan the database scale to meet the demands of the services accessing it? Fault tolerance\nHow many services are impacted by a database crash or maintenance downtime? Architectural quanta\nIs a single shared database forcing me into an undesirable single architecture\nquantum? Database type optimization\nCan I optimize my data by using multiple database types? Data Decomposition Drivers \n| \n133", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 151", "position": 151, "chunk_type": "semantic", "token_estimate": 173}
{"text": "Figure 6-3. Services impacted by a database change but forgotten will continue to fail: until redeployed\nIn most applications, the danger of forgotten services is mitigated by diligent impact\nanalysis and agressive regression testing. However, consider a microservices ecosys\u2010\ntem with 400 services, all sharing the same monolithic highly available clustered rela\u2010\ntional database. Imagine running around to all the development teams in many\ndomain areas, trying to find out which services use the table being changed. Also\nimagine having to then coordinate, test, and deploy all of these services together as a\nsingle unit, along with the database. Thinking about this scenario starts to become a\nmind-numbing exercise, usually leading to some degree of insanity. Breaking apart a database into well-defined bounded contexts significantly helps con\u2010\ntrol breaking database changes. The bounded context concept comes from the semi\u2010\nnal book Domain-Driven Design by Eric Evans (Addison-Wesley) and describes the\nsource code, business logic, data structures, and data all bound together\u2014encapsula\u2010\nted\u2014within a specific context. As illustrated in Figure 6-4, well-formed bounded con\u2010\ntexts around services and their corresponding data helps control change, because\nchange is isolated to just those services within that bounded context. Most typically, bounded contexts are formed around services and the data the serv\u2010\nices owns. By \u201cown\u201d we mean a service that writes to the database (as opposed to hav\u2010\ning read-only access to the data). We discuss distributed data ownership in more\ndetail in Chapter 9. Data Decomposition Drivers \n| \n135", "domains": ["Architectural Patterns and Styles", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 153", "position": 153, "chunk_type": "semantic", "token_estimate": 247}
{"text": "Figure 6-4. Database changes are isolated to only those services within the associated: bounded context\nNotice in Figure 6-4 that Service C needs access to some of the data in Database D\nthat is contained in a bounded context with Service D. Since Database D is in a differ\u2010\nent bounded context, Service C cannot directly access the data. This would not only\nviolate the bounded context rule, but also create a mess with regard to change con\u2010\ntrol. Therefore, Service C must ask Service D for the data. There are many ways of\naccessing data a service doesn\u2019t own while still maintaining a bounded context. These\ntechniques are discussed in detail in Chapter 10. One important aspect of a bounded context related to the scenario between Service C\nneeding data and Service D owning that data within its bounded context is that of\ndatabase abstraction. Notice in Figure 6-5 that Service D is sending data that was\nrequested by Service C through some sort of contract (such as JSON, XML, or maybe\neven an object). The advantage of the bounded context is that the data sent to Service C can be a dif\u2010\nferent contract than the schema for Database D. This means that a breaking change to\nsome table in Database D impacts only Service D and not necessarily the contract of\nthe data sent to Service C. In other words, Service C is abstracted from the actual\nschema structure of Database D.\n136 \n| \nChapter 6: Pulling Apart Operational Data", "domains": ["Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 154", "position": 154, "chunk_type": "semantic", "token_estimate": 253}
{"text": "Figure 6-5. The contract from a service call abstracts the caller from the underlying data\u2010: base schema\nTo illustrate the power of this bounded context abstraction within a distributed archi\u2010\ntecture, assume Database D has a Wishlist table with the following structure:\nCREATE TABLE Wishlist\n(\nCUSTOMER_ID VARCHAR(10),\nITEM_ID VARCHAR(20),\nQUANTITY INT,\nEXPIRATION_DT DATE\n);\nThe corresponding JSON contract that Service D sends to Service C requesting wish\nlist items is as follows:\n{\n  \"$schema\": \"http://json-schema.org/draft-04/schema#\",\n  \"properties\": {\n    \"cust_id\": {\"type\": \"string\"},\n    \"item_id\": {\"type\": \"string\"},\n    \"qty\": {\"type\": \"number\"},\n    \"exp_dt\": {\"type\": \"number\"}\n  },\n}\nNotice how the expiration data field (exp_dt) in the JSON schema is named differ\u2010\nently than the database column name and is specified as a number (a long value rep\u2010\nresenting the epoch time\u2014the number of milliseconds since midnight on 1 January\n1970), whereas in the database it is represented as a DATE field. Any column name\nData Decomposition Drivers \n| \n137", "domains": ["Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 155", "position": 155, "chunk_type": "semantic", "token_estimate": 154}
{"text": "change or column type change made in the database no longer impacts Service C: because of the separate JSON contract. To illustrate this point, suppose the business decides to no longer expire wish list\nitems. This would require a change in the table structure of the database:\nALTER TABLE Wishlist\nDROP COLUMN EXPIRATION_DT;\nService D would have to be modified to accommodate this change because it is within\nthe same bounded context as the database, but the corresponding contract would not\nhave to change at the same time. Until the contract is eventually changed, Service D\ncould either specify a date far into the future or set the value to zero indicating the\nitem doesn\u2019t expire. The bottom line is that Service C is abstracted from breaking\nchanges made to Database D due to the bounded context. Connection management\nEstablishing a connection to a database is an expensive operation. A database connec\u2010\ntion pool is often used not only to increase performance, but also to limit the number\nof concurrent connections an application is allowed to use. In monolithic applica\u2010\ntions, the database connection pool is usually owned by the application (or applica\u2010\ntion server). However, in distributed architectures, each service\u2014or more specifically,\neach service instance\u2014typically has its own connection pool. As illustrated in Figure\n6-6, when multiple services share the same database, the number of connections can\nquickly become saturated, particularly as the number of services or service instances\nincrease. Figure 6-6. Database connections can quickly get saturated with multiple service\ninstances\n138 \n| \nChapter 6: Pulling Apart Operational Data", "domains": ["Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 156", "position": 156, "chunk_type": "semantic", "token_estimate": 261}
{"text": "Reaching (or exceeding) the maximum number of available database connections is: yet another driver to consider when deciding whether to break apart a database. Fre\u2010\nquent connection waits (the amount of time it takes waiting for a connection to\nbecome available) is usually the first sign that the maximum number of database con\u2010\nnections has been reached. Since connection waits can also manifest themselves as\nrequest time-outs or tripped circuit breakers, looking for connection waits is usually\nthe first thing we recommend if these conditions frequently occur when using a\nshared database. To illustrate the issues associated with database connections and distributed architec\u2010\nture, consider the following example: a monolithic application with 200 database con\u2010\nnections is broken into a distributed architecture consisting of 50 services, each with\n10 database connections in its connection pool. Original monolithic application\n200 connections\nDistributed services\n50\nConnections per service\n10\nMinimum service instances\n2\nTotal service connections\n1,000\nNotice how the number of database connections within the same application context\ngrew from 200 to 1,000, and the services haven\u2019t even started scaling yet! Assuming\nhalf of the services scale to an average of 5 instances each, the number of database\nconnections quickly grows to 1,700. Without some sort of connection strategy or governance plan, services will try to use\nas many connections as possible, frequently starving other services from much\nneeded connections. For this reason, it\u2019s important to govern how database connec\u2010\ntions are used in a distributed architecture. One effective approach is to assign each\nservice a connection quota to govern the distribution of available database connec\u2010\ntions across services. A connection quota specifies the maximum number of database\nconnections a service is allowed to use or make available in its connection pool. By specifying a connection quota, services are not allowed to create more database\nconnections than are allocated to it. If a service reaches the maximum number of\ndatabase connections in its quota, it must wait for one of the connections it\u2019s using to\nbecome available. This method can be implemented using two approaches: evenly\ndistributing the same connection quota to every service, or assigning a different con\u2010\nnection quota to each service based on its needs. The even distribution approach is typically used when first deploying services, and it\nis not known yet how many connections each service will need during normal and\npeak operations. While simple, this approach is not overly efficient because some\nData Decomposition Drivers \n| \n139", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 157", "position": 157, "chunk_type": "semantic", "token_estimate": 411}
{"text": "services may need more connections than others, while some connections held by: other services may go unused. While more complex, the variable distribution approach is much more efficient for\nmanaging database connections to a shared database. With this approach, each ser\u2010\nvice is assigned a different connection quota based on its functionality and scalability\nrequirements. The advantage of this approach is that it optimizes the use of available\ndatabase connections across distributed services, making sure those services that\nrequire more database connections have them available for use. However, the disad\u2010\nvantage is that it requires knowledge about the nature of the functionality and the\nscalability requirements of each service. We usually recommend starting out with the even distribution approach and creating\nfitness functions to measure the concurrent connection usage for each service. We\nalso recommend keeping the connection quota values in an external configuration\nserver (or service) so that the values can be easily adjusted either manually or pro\u2010\ngrammatically through simple machine learning algorithms. This technique not only\nhelps mitigate connection saturation risk, but also properly balances available data\u2010\nbase connections between distributed services to ensure that no idle connections are\nwasted. Table 6-1 shows an example of starting out using the even distribution approach for a\ndatabase that can support a maximum of 100 concurrent connections. Notice that\nService A has only ever needed a maximum of 5 connections, Service C only 15 con\u2010\nnections, and Service E only 14 connections, whereas Service B and Service D have\nreached their max connection quota and have experienced connection waits. Table 6-1. Connection quota allocations evenly distributed\nService\nQuota\nMax used\nWaits", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 158", "position": 158, "chunk_type": "semantic", "token_estimate": 272}
{"text": "No: This analysis, which can be derived from continuous fitness functions that gather\nstreamed metrics data from each service, can also be used to determine how close the\nmaximum number of connections used is to the maximum number of connections\navailable, and also how much buffer exists for each service in terms of its quota and\nmaximum connections used. Scalability\nOne of the many advantages of a distributed architecture is scalability\u2014the ability for\nservices to handle increases in request volume while maintaining a consistent\nresponse time. Most cloud-based and on-prem infrastructure-related products do a\ngood job at ensuring that services, containers, HTTP servers, and virtual machines\nscale to satisfy increases in demand. But what about the database? As illustrated in Figure 6-7, service scalability can put a tremendous strain on the\ndatabase, not only in terms of database connections (as discussed in the prior sec\u2010\ntion), but also on throughput and database capacity. In order for a distributed system\nto scale, all parts of the system need to scale\u2014including the database. Data Decomposition Drivers \n| \n141", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 159", "position": 159, "chunk_type": "semantic", "token_estimate": 176}
{"text": "Figure 6-7. The database must also scale when services scale: Scalability is another data disintegration driver to consider when thinking about\nbreaking apart a database. Database connections, capacity, throughput, and perfor\u2010\nmance are all factors in determining whether a shared database can meet the\ndemands of multiple services within a distributed architecture. Consider the refined variable database connection quotas in Table 6-3 in the prior\nsection. When services scale by adding multiple instances, the picture changes dra\u2010\nmatically, as shown in Table 6-4, where the total number of database connections is\n100. Table 6-4. When services scale, more connection are used than are available\nService\nQuota\nMax used\nInstances\nTotal used", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 160", "position": 160, "chunk_type": "semantic", "token_estimate": 111}
{"text": "Notice that even though the connection quota is distributed to match the 100 data\u2010: base connections available, once services start to scale, the quota is no longer valid\nbecause the total number of connections used increases to 242, which is 142 more\nconnections than are available in the database. This will likely result in connection\nwaits, which in turn will result in overall performance degradation and request\ntime-outs. Breaking data into separate data domains or even a database-per-service, as illustra\u2010\nted in Figure 6-8, requires fewer connections to each database, hence providing better\ndatabase scalability and performance as the services scale. Figure 6-8. Breaking apart the database provides better database scalability\nIn addition to database connections, another factor to consider with respect to scala\u2010\nbility is the load placed on the database. By breaking apart a database, less load is\nplaced on each database, thereby also improving overall performance and scalability. Fault tolerance\nWhen multiple services share the same database, the overall system becomes less fault\ntolerant because the database becomes a single point of failure (SPOF). Here, we are\ndefining fault tolerance as the ability of some parts of the system to continue uninter\u2010\nrupted when a service or database fails. Notice in Figure 6-9 that when sharing a sin\u2010\ngle database, overall fault tolerance is low because if the database goes down, all\nservices become nonoperational. Data Decomposition Drivers \n| \n143", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 161", "position": 161, "chunk_type": "semantic", "token_estimate": 233}
{"text": "Figure 6-9. If the database goes down, all services become nonoperational: Fault tolerance is another driver for considering breaking apart data. If fault tolerance\nis required for certain parts of the system, breaking apart the data can remove the sin\u2010\ngle point of failure in the system, as shown in Figure 6-10. This ensures that some\nparts of the system are still operational in the event of a database crash. Figure 6-10. Breaking apart the database achieves better fault tolerance\nNotice that since the data is now broken apart, if Database B goes down, only Service\nB and Service C are impacted and become nonoperational, whereas the other services\ncontinue to operate uninterrupted. Architectural quantum\nRecall from Chapter 2 that an architectural quantum is defined as an independently\ndeployable artifact with high functional cohesion, high static coupling, and synchro\u2010\nnous dynamic coupling. The architecture quantum helps provide guidance in terms\nof when to break apart a database, making it another data disintegration driver. 144 \n| \nChapter 6: Pulling Apart Operational Data", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 162", "position": 162, "chunk_type": "semantic", "token_estimate": 171}
{"text": "The two main integration drivers for pulling data back together are the following:: Data relationships\nAre there foreign keys, triggers, or views that form close relationships between\nthe tables? Database transactions\nIs a single transactional unit of work necessary to ensure data integrity and con\u2010\nsistency? Each of these integration drivers is discussed in detail in the following sections. Data relationships\nLike components within an architecture, database tables can be coupled as well, par\u2010\nticularly with regard to relational databases. Artifacts like foreign keys, triggers,\nviews, and stored procedures tie tables together, making it difficult to pull data apart;\nsee Figure 6-13. Imagine walking up to your DBA or data architect and telling them that since the\ndatabase must be broken apart to support tightly formed bounded contexts within a\nmicroservices ecosystem, every foreign key and view in the database needs to be\nremoved! That\u2019s not a likely (or even feasible) scenario, yet that is precisely what\nwould need to happen to support a database-per-service pattern in microservices. Figure 6-13. Foreign keys (FK), triggers, and views create tightly coupled relationships\nbetween data\nData Decomposition Drivers \n| \n147", "domains": ["Architectural Patterns and Styles", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 165", "position": 165, "chunk_type": "semantic", "token_estimate": 187}
{"text": "These artifacts are necessary in most relational databases to support data consistency: and data integrity. In addition to these physical artifacts, data may also be logically\nrelated, such as a problem ticket table and its corresponding problem ticket status\ntable. However, as illustrated in Figure 6-14, these artifacts must be removed when\nmoving data to another schema or database to form bounded contexts. Figure 6-14. Data artifacts must be removed when breaking apart data\nNotice that the foreign key (FK) relationship between the tables in Service A can be\npreserved because the data is in the same bounded context, schema, or database. However, the foreign keys (FK) between the tables in Service B and Service C must be\nremoved (as well as the view that is used in Service C) because those tables are associ\u2010\nated with different databases or schemas. The relationship between data, either logical or physical, is a data integration driver,\nthus creating a trade-off between data disintegrators and data integrators. For exam\u2010\nple, is change control (a data disintegrator) more important than preserving the for\u2010\neign key relationships between the tables (a data integrator)? Is fault tolerance (a data\ndisintegrator) more important than preserving materialized views between tables (a\ndata integrator)? Identifying what is more important helps make the decision about\nwhether the data should be broken apart and what the resulting schema granularity\nshould be. Database transactions\nAnother data integrator is that of database transactions, something we discuss in\ndetail in \u201cDistributed Transactions\u201d on page 263. As shown in Figure 6-15, when a\nsingle service does multiple database write actions to separate tables in the same data\u2010\nbase or schema, those updates can be done within an Atomicity, Consistency, Isola\u2010\ntion, Durability (ACID) transaction and either committed or rolled back as a single\nunit of work. 148 \n| \nChapter 6: Pulling Apart Operational Data", "domains": ["Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 166", "position": 166, "chunk_type": "semantic", "token_estimate": 309}
{"text": "Sysops Squad Saga: Justifying Database Decomposition: Monday, November 15, 15:55\nArmed with their justifications, Addison and Devon met to convince Dana that it was\nnecessary to break apart the monolithic Sysops Squad database. \u201cHi, Dana,\u201d said Addison. \u201cWe think we have enough evidence to convince you that it\u2019s\nnecessary to break apart the Sysops Squad database.\u201d\n\u201cI\u2019m all ears,\u201d said Dana, arms crossed and ready to argue that the database should\nremain as is. \u201cI\u2019ll start,\u201d said Addison. \u201cNotice how these logs continuously show that whenever the operational\nreports run, the ticketing functionality in the application freezes up?\u201d\n\u201cYeah,\u201d said Dana, \u201cI\u2019ll admit that even I suspected that. It\u2019s clearly something wrong with the way the\nticketing functionality is accessing the database, not reporting.\u201d\n\u201cActually,\u201d said Addison, \u201cit\u2019s a combination of both ticketing and reporting. Look here.\u201d\nAddison showed Dana metrics and logs that demonstrated some of the queries were necessarily\nwrapped in threads, and that the queries from the ticketing functionality were timing out because of\na wait state when the reporting queries were run. Addison also showed how the reporting part of\nthe system used parallel threads to query parts of the more complex reports concurrently, essen-\ntially taking up all of the database connections. \u201cOK, I can see how having a separate reporting database would help the situation from a database\nconnection perspective. But that still doesn\u2019t convince me that the nonreporting data should be\nbroken apart,\u201d said Dana. \u201cSpeaking of database connections,\u201d said Devon, \u201clook at this connection pool estimate as we start\nbreaking apart the domain services.\u201d\nDevon showed Dana the number of estimated services in the final planned Sysops Squad dis-\ntributed application, including the projected number of instances for each of the services as the\napplication scales. Dana explained to Devon that the connection pool was contained within each\nseparate service instance, not like in the current phase of the migration where the application server\nowned the connection pool. \u201cSo you see, Dana,\u201d said Devon, \u201cwith these projected estimates, we will need an additional 2,000\nconnections to the database to provide the scalability we need to handle the ticket load, and we\nsimply do not have them with a single database.\u201d\nDana took a moment to look over the numbers. \u201cDo you agree with these numbers, Addison?\u201d\n\u201cI do,\u201d said Addison.", "domains": ["Design Patterns", "Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 168", "position": 168, "chunk_type": "semantic", "token_estimate": 388}
{"text": "Sysops Squad Saga: Justifying Database Decomposition: \u201cSo you see, Dana,\u201d said Devon, \u201cwith these projected estimates, we will need an additional 2,000\nconnections to the database to provide the scalability we need to handle the ticket load, and we\nsimply do not have them with a single database.\u201d\nDana took a moment to look over the numbers. \u201cDo you agree with these numbers, Addison?\u201d\n\u201cI do,\u201d said Addison. \u201cDevon and I came up with them ourselves after a lot of analysis based on the\namount of HTTP traffic as well as the projected growth rates supplied by Parker.\u201d\n150 \n| \nChapter 6: Pulling Apart Operational Data", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 168", "position": 168, "chunk_type": "semantic", "token_estimate": 106}
{"text": "\u201cI must admit,\u201d said Dana, \u201cthis is good stuff you\u2019ve both prepared. I particularly like that you\u2019ve: already thought about not having services connect to multiple databases or schemas. As you know,\nin my book that\u2019s a no-go.\u201d\n\u201cUs, too. However, we have one more justification to talk to you about,\u201d said Addison. \u201cAs you may or\nmay not know, we\u2019ve been having lots of issues with regard to the system not being available for our\ncustomers. While breaking apart the services provides us with some level of fault tolerance, if a mon-\nolithic database should go down for either maintenance or a server crash, all services would become\nnonoperational.\u201d\n\u201cWhat Addison is saying,\u201d added Devon, \u201cis that by breaking apart the database, we can provide bet-\nter fault tolerance by creating domain silos for the data. In other words, if the survey database were\nto go down, ticketing functionality would still be available.\u201d\n\u201cWe call that an architectural quantum,\u201d said Addison. \u201cIn other words, since the database is part of\nthe static coupling of a system, breaking it apart would make the core ticketing functionality stand-\nalone and not synchronously dependent on other parts of the system.\u201d\n\u201cListen,\u201d said Dana, \u201cyou\u2019ve convinced me that there\u2019s good reasons to break apart the Sysops Squad\ndatabase, but explain to me how you can even think about doing that. Do you realize how many\nforeign keys and views there are in that database? There\u2019s no way you\u2019re going to be able remove all\nof those things.\u201d\n\u201cWe don\u2019t necessarily have to remove all of those artifacts. That\u2019s where data domains and the five-\nstep process come into play,\u201d said Devon. \u201cHere, let me explain\u2026\u201d\nDecomposing Monolithic Data\nDecomposing a monolithic database is hard, and requires an architect to collaborate\nclosely with the database team to safely and effectively break apart the data. One par\u2010\nticularly effective technique for breaking apart data is to leverage what is known as\nthe five-step process. As illustrated in Figure 6-17, this evolutionary and iterative pro\u2010\ncess leverages the concept of a data domain as a vehicle for methodically migrating\ndata into separate schemas, and consequently different physical databases. Figure 6-17. Five-step process for decomposing a monolithic database\nDecomposing Monolithic Data \n| \n151", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 169", "position": 169, "chunk_type": "semantic", "token_estimate": 376}
{"text": "One way to conceptually think about data domains is to think about the database as a: soccer ball, where each white hexagon represents a separate data domain. As illustra\u2010\nted in Figure 6-18, each white hexagon of the soccer ball contains a collection of\ndomain-related tables along with all of the coupling artifacts (such as foreign keys,\nviews, stored procedures, and so on). Figure 6-18. Database objects in a hexagon belong in a data domain\nVisualizing the database this way allows the architect and database team to clearly see\ndata domain boundaries and also the cross-domain dependencies (such as foreign\nkeys, views, stored procedures, and so on) that need to be broken. Notice in Figure\n6-18 that within each white hexagon, all data table dependencies and relationships\ncan be preserved, but not between each white hexagon. For example, in the diagram\nnotice that solid lines represent dependencies that are self-contained to the data\ndomain, while the dotted lines cross data domains and must be removed when the\ndata domains are extracted into separate schemas. Decomposing Monolithic Data \n| \n153", "domains": ["Design Principles"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 171", "position": 171, "chunk_type": "semantic", "token_estimate": 179}
{"text": "billing.customer_id, contract.contract_start_date,: ate,\n    contract.contract_duration, billing.billing_date,\n    billing.billing_amount\nFROM payment.contract AS contract\nINNER JOIN payment.billing AS billing\n    ON ( contract.contract_id = billing.contract_id )\nWHERE contract.auto_renewal = 0\nThe bounded context rules for data domains apply just the same as individual tables\n\u2014a service cannot talk to multiple data domains. Therefore, by removing this table\nfrom the view, the Payment service must now call the Customer service to get the\ncustomer name that it originally had from the view. Once architects and database teams understand the concept of a data domain, they\ncan apply the five-step process for decomposing a monolithic database. Those five\nsteps are outlined in the following sections. Decomposing Monolithic Data \n| \n155", "domains": ["Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 173", "position": 173, "chunk_type": "semantic", "token_estimate": 113}
{"text": "Step 1: Analyze Database and Create Data Domains: As illustrated in Figure 6-20, all services have access to all data in the database. This\npractice, known as the shared database integration style described by Gregor Hohpe\nand Bobby Woolf in their book Enterprise Integration Patterns: Designing, Building,\nand Deploying Messaging Solutions (Addison-Wesley), creates a tight coupling\nbetween data and the services accessing that data. As discussed in \u201cData Decomposi\u2010\ntion Drivers\u201d on page 132, this tight coupling in the database makes change manage\u2010\nment very difficult. Figure 6-20. Multiple services use the same database, accessing all the tables necessary\nfor read or write purposes\nThe first step in breaking apart a database is to identify specific domain groupings\nwithin the database. For example, as shown in Table 6-5, related tables are grouped\ntogether to help identify possible data domains. Step 2: Assign Tables to Data Domains\nThe next step is to group tables along a specific bounded context, assigning tables\nthat belong to a specific data domain into their own schema. A schema is a logical\nconstruct in database servers. A schema contain objects such as tables, views, func\u2010\ntions, and so on. In some database servers, like Oracle, the schema is same as the user,\nwhile in other databases, like SQL Server, a schema is logical space for database\nobjects where users have access to these schemas. As illustrated in Figure 6-21, we have created schemas for each data domain and\nmoved tables to the schemas to which they belong. 156 \n| \nChapter 6: Pulling Apart Operational Data", "domains": ["Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 174", "position": 174, "chunk_type": "semantic", "token_estimate": 259}
{"text": "Figure 6-22. Move the cross-schema object access to the services, away from direct cross-: schema access\nWhen data from other domains is needed, do not reach into their\ndatabases. Instead, access it using the service that owns the data\ndomain. Upon completion of this step, the database is in a state of data sovereignty per service,\nwhich occurs when each service owns its own data. Data sovereignty per service is the\nnirvana state for a distributed architecture. Like all practices in architecture, it\nincludes benefits and shortcomings:\nBenefits\n\u2022 Teams can change the database schema without worrying about affecting\nchanges in other domains. \u2022 Each service can use the database technology and database type best suitable for\ntheir use case. Shortcomings\n\u2022 Performance issues occur when services need access to large volumes of data. \u2022 Referential integrity cannot be maintained in the database, resulting in the possi\u2010\nbility of bad data quality. \u2022 All database code (stored procedures, functions) that access tables belonging to\nother domains must be moved to the service layer. Step 4: Move Schemas to Separate Database Servers\nOnce database teams have created and separated data domains, and have isolated\nservices so that they access their own data, they can now move the data domains to\nseparate physical databases. This is often a necessary step because even though\nDecomposing Monolithic Data \n| \n159", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 177", "position": 177, "chunk_type": "semantic", "token_estimate": 226}
{"text": "services access their own schemas, accessing a single database creates a single archi\u2010: tecture quantum, as discussed in Chapter 2, which might have adverse effects for\noperational characteristics, such as scalability, fault tolerance, and performance. When moving schemas to separate physical databases, database teams have two\noptions: backup and restore, or replication. These options are outlined as follows:\nBackup and restore\nWith this option, teams first back up each schema with data domains, then set up\ndatabase servers for each data domain. They then restore the schemas, connect\nservices to schemas in the new database servers, and finally remove schemas\nfrom the original database server. This approach usually requires downtime for\nthe migration. Replicate\nUsing the replicate option, teams first set up database servers for each data\ndomain. Next they replicate the schemas, switch connections over to the new\ndatabase servers, and then remove the schemas from the original database server. While this approach avoids downtime, it does require more work to set up the\nreplication and manage increased coordination. Figure 6-23 shows an example of the replication option, where the database team sets\nup multiple database servers so that there is one database server for each data\ndomain. Figure 6-23. Replicate schemas (data domains) to their own database servers\n160 \n| \nChapter 6: Pulling Apart Operational Data", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 178", "position": 178, "chunk_type": "semantic", "token_estimate": 218}
{"text": "Step 5: Switch Over to Independent Database Servers: Once the schemas are fully replicated, the service connections can be switched. The\nlast step in getting the data domains and services to act as their own independent\ndeployable units is to remove the connection to the old database servers and remove\nthe schemas from the old database servers as well. The final state is seen in Figure\n6-24. Figure 6-24. Independent database servers for each data domain\nOnce the database team has separated the data domains, isolated the database con\u2010\nnections, and finally moved the data domains to their own database servers, they can\noptimize the individual database servers for availability and scalability. Teams can\nalso analyze the data to determine the most appropriate database type to use, intro\u2010\nducing polyglot database usage within the ecosystem. Selecting a Database Type\nBeginning around 2005, a revolution has occurred in database technologies. Unfortu\u2010\nnately, the number of products that have emerged during this time have created a\nproblem known as The Paradox of Choice. Having such a large number of products\nand choices means having more trade-off decisions to make. Given that each product\nis optimized for certain trade-offs, it rests on both software and data architects to pick\nthe appropriate product with these trade-offs in mind as it relates to their problem\nspace. In this section, we introduce star ratings for the various database types, using the fol\u2010\nlowing characteristics in our analysis:\nEase-of-learning curve\nThis characteristic refers to the ease with which new developers, data architects,\ndata modelers, operational DBAs, and other users of the databases can learn and\nadopt. For example, it\u2019s assumed that most software developers understand SQL,\nSelecting a Database Type \n| \n161", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 179", "position": 179, "chunk_type": "semantic", "token_estimate": 285}
{"text": "whereas something like Gremlin (a graph query language) may be a niche skill.: The higher the star rating, the easier the learning curve. The lower the star rating,\nthe harder the learning curve. Ease of data modeling\nThis characteristic refers to the ease with which data modelers can represent the\ndomain in terms of a data model. A higher star rating means data modeling\nmatches many use cases, and once modeled, is easy to change and adopt. Scalability/throughput\nThis characteristic refers to the degree and ease with which a database can scale\nto handle increased throughput. Is it easy to scale the database? Can the database\nscale horizontally, vertically, or both? A higher star rating means it\u2019s easier to\nscale and get higher throughput. Availability/partition tolerance\nThis characteristic refers to whether the database supports high availability con\u2010\nfigurations (such as replica-sets in MongoDB or tunable consistency in Apache\nCassandra). Does it provide features to handle network partitions? The higher\nthe star rating, the better the database supports higher availability and/or better\npartition tolerance. Consistency\nThis characteristic refers to whether the database supports an \u201calways consistent\u201d\nparadigm. Does the database support ACID transactions, or does it lean toward\nBASE transactions with an eventual consistency model? Does it provide features\nto have tunable consistency models for different types of writes? The higher the\nstar rating, the more consistency the database supports. Programming language support, product maturity, SQL support, and community\nThis characteristic refers to which (and how many) programming languages the\ndatabase supports, how mature the database is, and the size of the database com\u2010\nmunity. Can an organization easily hire people who know how to work with the\ndatabase? Higher star ratings means there is better support, the product is\nmature, and it\u2019s easy to hire talent. Read/write priority\nThis characteristic refers to whether the database prioritizes reads over writes, or\nwrites over reads, or if it is balanced in its approach. This is not a binary choice\u2014\nrather, it\u2019s more of a scale toward which direction the database optimizes. 162 \n| \nChapter 6: Pulling Apart Operational Data", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 180", "position": 180, "chunk_type": "semantic", "token_estimate": 347}
{"text": "Relational Databases: Relational databases (also known as an RDBMS) have been the database of choice for\nmore than three decades. There is significant value in their usage and the stability\nthey provide, particularly within most business-related applications. These databases\nare known for the ubiquitous Structured Query Language (SQL) and the ACID prop\u2010\nerties they provide. The SQL interface they provide makes them a preferred choice\nfor implementing different read models on top of the same write model. The star\nratings for relational databases appear in Figure 6-25. Figure 6-25. Relational databases rated for various adoption characteristics\nEase-of-learning curve\nRelational databases have been around for many years. They are commonly\ntaught in schools, and mature documentation and tutorials exist. Therefore, they\nare much easier to learn than other database types. Ease of data modeling\nRelational databases allow for flexible data modeling. They allow the modeling of\nkey-value, document, graph-like structures, and they allow for changes in read\npatterns with addition of new indexes. Some models are really difficult to ach\u2010\nieve, such as graph structures with arbitrary depth. Relational databases organize\ndata into tables and rows (similar to spreadsheets), something that is natural for\nmost database modelers. Selecting a Database Type \n| \n163", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 181", "position": 181, "chunk_type": "semantic", "token_estimate": 202}
{"text": "Scalability/throughput: Relational databases are generally vertically scaled using large machines. How\u2010\never, setup with replications and automated switchover are complex, requiring\nhigher coordination and setup. Availability/partition tolerance\nRelational databases favor consistency over availability and partition tolerance,\ndiscussed in \u201cTable Split Technique\u201d on page 254. Consistency\nRelational databases have been dominant for years because of their support for\nACID properties. The ACID features handle many concerns in concurrent sys\u2010\ntems and allow for developing applications without being concerned about\nlower-level details of concurrency and how the databases handle them. Programming language support, product maturity, SQL support, and community\nSince relational databases have been around for many years, well-known design,\nimplementation, and operational patterns can be applied to them, thus making\nthem easy to adopt, develop, and integrate within an architecture. Many of the\nrelational databases lack support for reactive stream APIs and similar new con\u2010\ncepts; newer architectural concepts take longer to implement in well-established\nrelational databases. Numerous programming language interfaces work with\nrelational databases, and the community of users is large (although splintered\namong all the vendors). Read/write priority\nIn relational databases, the data model can be designed in such a way that either\nreads become more efficient or writes become more efficient. The same database\ncan handle different types of workloads, allowing for balanced read-write prior\u2010\nity. For example, not all use cases need ACID properties, especially in large data\nand traffic scenarios, or when really flexible schema is desired such as in survey\nadministration. In these cases, other database types may be a better option. MySQL, Oracle, Microsoft SQL Server, and PostgreSQL are the most popular rela\u2010\ntional databases and can be run as standalone installations or are available as Data\u2010\nbase as a Service on major cloud provider platforms. 164 \n| \nChapter 6: Pulling Apart Operational Data", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 182", "position": 182, "chunk_type": "semantic", "token_estimate": 300}
{"text": "Aggregate Orientation: Aggregate orientation is the preference to operate on data that is related and has a\ncomplex data structure. Aggregate is a term originated in Domain-Driven Design:\nTackling Complexity in the Heart of Software by Erik Evans. Think of ticket or cus\ntomer with all its dependent tables in the Sysops Squad\u2014they are aggregates. Like all\npractices in architecture, aggregate orientation includes benefits and shortcomings:\nBenefits\n\u2022 Enables easy distribution of data in clusters of servers, as the whole aggregate can\nbe copied over to different servers. \u2022 Improves read and write performance, as it reduces joins in the database. \u2022 Reduces impedance mismatch between the application model and storage model. Shortcomings\n\u2022 It\u2019s difficult to arrive at proper aggregates, and changing aggregate boundaries is\nhard. \u2022 Analyzing data across aggregates is difficult. Key-Value Databases\nKey-value databases are similar to a hash table data structure, something like tables in\nan RDBMS with an ID column as the key and a blob column as the value, which can\nconsequently store any type of data. Key-value databases are part of a family known\nas NoSQL databases. In the book NoSQL Distilled: A Brief Guide to the Emerging\nWorld of Polyglot Persistence (Addison-Wesley), Pramod Sadalage (one of your\nauthors) and Martin Fowler describe the rise of NoSQL databases and the motiva\u2010\ntions, usages, and trade-offs of using these types of databases, and is a good reference\nfor further information on this database type. Key-value databases are easiest to understand among the NoSQL databases. An appli\u2010\ncation client can insert a key and a value, get a value for a known key, or delete a\nknown key and its value. A key-value database does not know what\u2019s inside the value\npart, nor does it care what\u2019s inside, meaning that the database can query using the key\nand nothing else. Unlike relational databases, key-value databases should be picked based on needs. There are persistent key-value databases like Amazon DynamoDB or Riak KV, non\u2010\npersistent databases like MemcacheDB, and other databases like Redis that can be\nconfigured to be persistent or not. Other relational database constructs like joins,\nwhere, and order by are not supported, but rather the operations get, put, and\ndelete. The ratings for key-value databases appear in Figure 6-26. Selecting a Database Type \n| \n165", "domains": ["Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 183", "position": 183, "chunk_type": "semantic", "token_estimate": 383}
{"text": "Figure 6-26. Key-value databases rated for various adoption characteristics: Ease-of-learning curve\nKey-value databases are easy to understand. Since they use \u201c\nAggregate Orienta\u2010\ntion\u201d on page 165, it\u2019s important to design the aggregate properly because any\nchange in the aggregate means rewriting all the data. Moving from relational\ndatabases to any of the NoSQL databases takes practice and unlearning familiar\npractices. For example, a developer cannot simply query \u201cGet me all the keys.\u201d\nEase of data modeling\nSince key-value databases are aggregate oriented, they can use memory structures\nlike arrays, maps, or any other type of data, including big blob. The data can be\nqueried only by key or ID, which means the client should have access to the key\noutside of the database. Good examples of a key include session_id, user_id,\nand order_id. Scalability/throughput\nSince key-value databases are indexed by key or ID, key lookups are very fast as\nthere are no joins or order by operations. The value is fetched and returned to\nthe client, which allows for easier scaling and higher throughput. Availability/partition tolerance\nSince there are many types of key-value databases and each has different proper\u2010\nties, even the same database can be configured to act in different ways either for\nan installation or for each read. For example, in Riak users can use quorum prop\u2010\nerties such as all, one, quorum, and default. When we use one quorum, the\nquery can return success when any one node responds. When the all quorum\n166 \n| \nChapter 6: Pulling Apart Operational Data", "domains": ["Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 184", "position": 184, "chunk_type": "semantic", "token_estimate": 255}
{"text": "is used, all nodes have to respond for the query to return success. Each query: can tune the partition tolerance and availability. Hence, assuming that all key-\nvalue stores are the same is a mistake. Consistency\nDuring each write operation, we can apply configurations that are similar to\napplying quorum during read operations; these configurations provide what is\nknown as tunable consistency. Higher consistency can be achieved by trading off\nlatency. For a write to be highly consistent, all nodes have to respond, which\nreduces partition tolerance. Using a majority quorum is considered a good\ntrade-off. Programming language support, product maturity, SQL support, and community\nKey-value databases have good programming language support, and many open\nsource databases have an active community to help learn and understand them. Since most databases have an HTTP REST API, they are much easier to interface\nwith. Read/write priority\nSince key-value databases are aggregate oriented, access to data via a key or ID is\ngeared toward read priority. Key-value databases can be used for session storage,\nand can be used to cache user properties and preferences as well. Sharding in Databases\nThe concept of partitioning is well-known in relational databases: the table data is\npartitioned into sets based on a schema on the same database server. Sharding is simi\u2010\nlar to partitioning, but data resides on different servers or nodes. Nodes collaborate to\nfigure out where data exists or where data should be stored based on a sharding key. The word shard means horizontal partition of data in a database. Document Databases\nDocuments such as JSON or XML are the basis of document databases. Documents\nare human-readable, self-describing, hierarchical tree structures. Document data\u2010\nbases are another type of NoSQL database, whose ratings appear in Figure 6-27. These databases understand the structure of the data and can index multiple\nattributes of the documents, allowing for better query flexibility. Selecting a Database Type \n| \n167", "domains": ["Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 185", "position": 185, "chunk_type": "semantic", "token_estimate": 318}
{"text": "Figure 6-27. Document databases rated for various adoption characteristics: Ease-of-learning curve\nDocument databases are like key-value databases where the value is human read\u2010\nable. This makes learning the database much easier. Enterprises are used to deal\u2010\ning with documents, such as XML and JSON in different contexts, such as API\npayloads and JavaScript frontends. Ease of data modeling\nJust like key-value databases, data modeling involves modeling aggregates such as\norders, tickets, and other domain objects. Document databases are forgiving\nwhen it comes to aggregate design, as the parts of the aggregate are queryable and\ncan be indexed. Scalability/throughput\nDocument databases are aggregate oriented and easy to scale. Complex indexing\nreduces scalability, and increased data size leads to a need for partitioning or\nsharding. Once sharding is introduced, it increases the complexity and also\nforces the selection of a sharding key. Availability/partition tolerance\nLike key-value databases, document databases can be configured for higher avail\u2010\nability. The setup gets complicated when there are replicated clusters for sharded\ncollections. The cloud providers are trying to make these setups more usable. 168 \n| \nChapter 6: Pulling Apart Operational Data", "domains": ["Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 186", "position": 186, "chunk_type": "semantic", "token_estimate": 185}
{"text": "Consistency: Some document databases have started supporting ACID transactions within a\ncollection, but this may not work in some edge cases. Just like key-value data\u2010\nbases, document databases provide the ability to tune the read and write opera\u2010\ntions using the quorum mechanism. Programming language support, product maturity, SQL support, and community\nDocument databases are the most popular of the NoSQL databases, with an\nactive user community, numerous online learning tutorials, and many program\u2010\nming language drivers that allow for easier adoption. Read/write priority\nDocument databases are aggregate oriented and have secondary indexes to query,\nso these databases are favoring read priority. Schema-less Databases\nOne common theme in NoSQL databases is duplication of data and schema attribute\nnames. No two entries have to be the same in terms of schema or attribute names. This introduces interesting change control dynamics and provides flexibility. The\nschema-less nature of the database is powerful, but it\u2019s important to understand that\nthe data always has a schema even if it\u2019s implicit or defined elsewhere. The application\nneeds to handle multiple versions of the schema returned by a database. The claim\nthat NoSQL databases are entirely schema-less is misleading. Column Family Databases\nColumn family databases, also known as wide column databases or big table databases,\nhave rows with varying numbers of columns, where each column is a name-value\npair. With columnar databases, the name is known as a column-key, the value is\nknown as a column-value, and the primary key of a row is known as a row key. Col\u2010\numn family databases are another type of NoSQL database that group related data\nthat is accessed at the same time, and whose ratings appear in Figure 6-28. Selecting a Database Type \n| \n169", "domains": ["Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 187", "position": 187, "chunk_type": "semantic", "token_estimate": 287}
{"text": "Figure 6-28. Column family databases rated for various adoption characteristics: Ease-of-learning curve\nColumn family databases are difficult to understand. Since a collection of name-\nvalue pairs belong to a row, each row can have different name-value pairs. Some\nname-value pairs can have a map of columns and are known as super columns. Understanding how to use these takes practice and time. Ease of data modeling\nData modeling with column family databases takes some getting used to. Data\nneeds to be arranged in groups of name-value pairs that have a single row identi\u2010\nfier, and designing this row key takes multiple iterations. Some column family\ndatabases like Apache Cassandra have introduced a SQL-like query language\nknown as Cassandra Query Language (CQL) that makes data modeling\naccessible. Scalability/throughput\nAll column family databases are highly scalable and suit use cases where high\nwrite or read throughput is needed. Column family databases scale horizontally\nfor read and write operations. Availability/partition tolerance\nColumn family databases naturally operate in clusters, and when some nodes of\nthe cluster are down, it is transparent to the client. The default replication factor\nis three, which means at least three copies of data are made, improving availabil\u2010\nity and partition tolerance. Similar to key-value and document databases, column\nfamily databases can tune writes and reads based on quorum needs. 170 \n| \nChapter 6: Pulling Apart Operational Data", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 188", "position": 188, "chunk_type": "semantic", "token_estimate": 227}
{"text": "Consistency: Column family databases, like other NoSQL databases, follow the concept of tun\u2010\nable consistency. This means that, based on needs, each operation can decide\nhow much consistency is desired. For example, in high write scenarios where\nsome data loss can be tolerated, the write consistency level of ANY could be used,\nwhich means at least one node has accepted the write, while a consistency level of\nALL means all nodes have to accept the write and respond success. Similar consis\u2010\ntency levels can be applied to read operations. It\u2019s a trade-off\u2014higher consistency\nlevels reduce availability and partition tolerance. Programming language support, product maturity, SQL support, and community\nColumn family databases like Cassandra and Scylla have active communities, and\nthe development of SQL-like interfaces has made the adoption of these databases\neasier. Read/write priority\nColumn family databases use the concepts of SSTables, commit logs, and memta\u2010\nbles, and since the name-value pairs are populated when data is present, they can\nhandle sparse data much better than relational databases. They are ideal for high\nwrite-volume scenarios. All NoSQL databases are designed to understand aggregate orientation. Having\naggregates improves read and write performance, and also allows for higher availabil\u2010\nity and partition tolerance when the databases are run as a cluster. The notion of CAP\ntheorem is covered in \u201cTable Split Technique\u201d on page 254 at more length. Graph Databases\nUnlike relational databases, where relations are implied based on references, graph\ndatabases use nodes to store entities and their properties. These nodes are connected\nwith edges, also known as relationships, which are explicit objects. Nodes are organ\u2010\nized by relationships and allow for analysis of the connected data by traversing along\nspecific edges. Figure 6-29. In graph databases, direction of the edge has significance when querying\nSelecting a Database Type \n| \n171", "domains": ["Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 189", "position": 189, "chunk_type": "semantic", "token_estimate": 300}
{"text": "The edges in graph databases have directional significance. In Figure 6-29, an edge of: type TICKET_CREATED connecting a ticket node with ID 4235143 to a customer node\nwith ID Neal. We can traverse from the ticket node via the outgoing edge TICKET_CRE\nATED or the customer node via the incoming edge TICKET_CREATED. When the direc\u2010\ntions get mixed up, querying the graph becomes really difficult. The ratings for graph\ndatabases are illustrated in Figure 6-29. Figure 6-30. Graph databases rated for various adoption characteristics\nEase-of-learning curve\nGraph databases have a steep learning curve. Understanding how to use the\nnodes, relations, relation type, and properties takes time. Ease of data modeling\nUnderstanding how to model the domains and convert them into nodes and rela\u2010\ntions is hard. In the beginning, the tendency is to add properties to relations. As\nmodeling knowledge improves, increased usage of nodes and relations, and con\u2010\nverting some relation properties to nodes with additional relation type takes\nplace, which improves graph traversal. Scalability/throughput\nReplicated nodes improve read scaling, and throughput can be tuned for read\nloads. Since it\u2019s difficult to split or shard graphs, write throughput is constrained\nwith the type of graph database picked. Traversing the relationships is very fast,\nas the indexing and storage is persisted and not calculated at query time. 172 \n| \nChapter 6: Pulling Apart Operational Data", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 190", "position": 190, "chunk_type": "semantic", "token_estimate": 225}
{"text": "Availability/partition tolerance: Some of the graph databases that have high partition tolerance and availability\nare distributed. Graph database clusters can use nodes that can be promoted as\nleaders when current leaders are unavailable. Consistency\nMany graph databases support ACID transactions. Some graph databases, such as\nNeo4j, support transactions, so that data is always consistent. Programming language support, product maturity, SQL support, and community\nGraph databases have lots of support in the community. Many algorithms, like\nDijkstra\u2019s algorithm or node similarity, are implemented in the database, reducing\nthe need to write them from scratch. The language framework known as Gremlin\nworks across many different databases, helping in the ease of use. Neo4J supports\na query language known as Cypher, allowing developers to easily query the\ndatabase. Read/write priority\nIn graph databases, data storage is optimized for relationship traversal as\nopposed to relational databases, where we have to query the relationships and\nderive them at query time. Graph databases are better for read-heavy scenarios. Graph databases allow the same node to have various types of relationships. In the\nSysops Squad example, a sample graph might look as follows: a knowledge_base was\ncreated_by user sysops_user and knowledge_base used_by sysops_user. Thus, the\nrelationships created_by and used_by join the same nodes for different relationship\ntypes. Changing Relationship Types\nChanging relationship types is an expensive operation, since each relationship type\nhas to be re-created. When this happens, both nodes connected by the edge have to be\nvisited, the new edge created, and the old edge removed. Hence, edge type or relation\u2010\nship types have to be thought about carefully. NewSQL Databases\nMatthew Aslett first used the term NewSQL to define new databases that aimed to\nprovide the scalability of NoSQL databases while supporting the features of relational\ndatabases like ACID. NewSQL databases use different types of storage mechanisms,\nand all of them support SQL. NewSQL databases, whose ratings appear in Figure 6-31, improve upon relational\ndatabases by providing automated data partitioning or sharding, allowing for\nSelecting a Database Type \n| \n173", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 191", "position": 191, "chunk_type": "semantic", "token_estimate": 335}
{"text": "horizontal scaling and improved availability, while at the same time allowing an easy: transition for developers to use the known paradigm of SQL and ACID. Figure 6-31. New SQL databases rated for various adoption characteristics\nEase-of-learning curve\nSince NewSQL databases are just like relational databases (with SQL interface,\nadded features of horizontal scaling, ACID compliant), the learning curve is\nmuch easier. Some of them are available as only Database as a Service (DBaaS),\nwhich may make learning them more difficult. Ease of data modeling\nSince NewSQL databases are like relational databases, data modeling is familiar\nto many and easier to pick up. The extra wrinkle is sharding design, allowing\nsharded data placement in geographically different locations. Scalability/throughput\nNewSQL databases are designed to support horizontal scaling for distributed sys\u2010\ntems, allowing for multiple active nodes, unlike relational databases that have\nonly one active leader, and the rest of the nodes are followers. The multiple active\nnodes allow NewSQL databases to be highly scalable and to have better\nthroughput. Availability/partition tolerance\nBecause of the multiple active nodes design, the benefits to availability can be\nreally high with greater partition tolerance. CockroachDB is a popular NewSQL\ndatabase that survives disk, machine, and data center failures. 174 \n| \nChapter 6: Pulling Apart Operational Data", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 192", "position": 192, "chunk_type": "semantic", "token_estimate": 210}
{"text": "Consistency: NewSQL databases support strongly consistent ACID transactions. The data is\nalways consistent, and this allows for relational database users to easily transition\nto NewSQL databases. Programming language support, product maturity, SQL support, and community\nThere are many open source NewSQL databases, so learning them is accessible. Some of the databases also support wire-compatible protocols with existing rela\u2010\ntional databases, which allows them to replace relational databases without any\ncompatibility problems. Read/write priority\nNewSQL databases are used just like relational databases, with added benefits of\nindexing and distributing geographically either to improve read performance or\nwrite performance. Cloud Native Databases\nWith increased cloud usage, cloud databases such as Snowflake, Amazon Redshift,\nDatomic, and Azure CosmosDB have gained in popularity. These databases reduce\noperational burden, provide cost transparency, and are an easy way to experiment\nsince no up-front investments are needed. Ratings for cloud native databases appear\nin Figure 6-32. Figure 6-32. Cloud native databases rated for various adoption characteristics\nSelecting a Database Type \n| \n175", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 193", "position": 193, "chunk_type": "semantic", "token_estimate": 165}
{"text": "Ease-of-learning curve: Some cloud databases like AWS Redshift are like relational databases and there\u2010\nfore are easier to understand. Databases like Snowflake, which have a SQL inter\u2010\nface but have different storage and compute mechanisms, require some practice. Datomic is totally different in terms of models and uses immutable atomic facts. Thus, the learning curve varies with each database offering. Ease of data modeling\nDatomic does not have the concept of tables or the need to define attributes in\nadvance. It is necessary to define properties of individual attributes, and entities\ncan have any attribute. Snowflake and Redshift are used more for data warehous\u2010\ning type workloads. Understanding the type of modeling provided by the data\u2010\nbase is critical in selecting the database to use. Scalability/throughput\nSince all these databases are cloud only, scaling them is relatively simple since\nresources can be allocated automatically for a price. In these decisions, the trade-\noff typically relates to price. Availability/partition tolerance\nDatabases in this category (such as Datomic) are highly available when deployed\nusing Production Topology. They have no single point of failure and are sup\u2010\nported by extensive caching. Snowflake, for example, replicates its databases\nacross regions and accounts. Other databases in this category support higher\navailability with various options to configure. For example, Redshift runs in a\nsingle availability zone and would need to be run in multiple clusters to support\nhigher availability. Consistency\nDatomic supports ACID transactions using storage engines to store blocks in\nblock storage. Other databases, like Snowflake and Redshift, support ACID\ntransactions. Programming language support, product maturity, SQL support, and community\nMany of these databases are new, and finding experienced help can be difficult. Experimenting with these databases requires a cloud account, which can create\nanother barrier. While cloud native databases reduce operational workload on\nthe operational DBAs, they do have a higher learning curve for developers. Datomic uses Clojure in all its examples, and stored procedures are written with\nClojure, so not knowing Clojure maybe a barrier to usage. Read/write priority\nThese databases can be used for both read-heavy or write-heavy loads. Snowflake\nand Redshift are geared more toward data warehouse type workloads, lending\n176 \n| \nChapter 6: Pulling Apart Operational Data", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 194", "position": 194, "chunk_type": "semantic", "token_estimate": 367}
{"text": "them toward read priority, while Datomic can support both type of loads with: different indexes such as EAVT (Entity, Attribute, Value, then Transaction) first. Time-Series Databases\nGiven the trends, we see increased usage of IoT, microservices, self-driving cars, and\nobservability, all of which have driven a phenomenal increase in time-series analytics. This trend has given rise to databases optimized for storing sequences of data points\ncollected during a time window, enabling users to track changes over any duration of\ntime. The ratings for this database type appear in Figure 6-33. Figure 6-33. Time-series databases rated for various adoption characteristics\nEase-of-learning curve\nUnderstanding time-series data is often easy\u2014every data point is attached to a\ntimestamp, and data is almost always inserted and never updated or deleted. Understanding append-only operations takes some unlearning from other data\u2010\nbase usage, where errors in the data can be corrected with an update. InfluxDB,\nKx, and TimeScale are some of the popular time-series databases. Ease of data modeling\nThe underlying concept with time-series databases is to analyze changes in data\nover time. For example, with the Sysops Squad example, changes done to a ticket\nobject can be stored in a time-series database, where the timestamp of change\nand ticket_id are tagged. It\u2019s considered bad practice to add more than one\npiece of information in one tag. For example, ticket_status=Open,\nticket_id=374737 is better than ticket_info=Open.374737. Selecting a Database Type \n| \n177", "domains": ["Architectural Patterns and Styles", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 195", "position": 195, "chunk_type": "semantic", "token_estimate": 234}
{"text": "Scalability/throughput: Timescale is based on PostgreSQL and allows for standard scaling and through\u2010\nput improvement patterns. Running InfluxDB in cluster mode by using meta\nnodes that manage metadata and data nodes that store actual data provides scal\u2010\ning and throughput improvements. Availability/partition tolerance\nSome databases like InfluxDB have better availability and partition tolerance\noptions with configurations for meta and data nodes, along with replication\nfactors. Consistency\nTime-series databases that use relational databases as their storage engine get\nACID properties for consistency, while other databases can tune consistency\nusing consistency-level of any, one, or quorum. Higher consistency-level con\u2010\nfiguration generally results in higher consistency and lower availability, so it\u2019s a\ntrade-off that needs to be considered. Programming language support, product maturity, SQL support, and community\nTime-series databases have become popular lately, and there are many resources\nto learn from. Some of these databases, such as InfluxDB, provide a SQL-like\nquery language known as InfluxQL. Read/write priority\nTime-series databases are append only and tend to be better suited for read-\nheavy workloads. When using time-series databases, the database automatically attaches a timestamp to\nevery datum creation, and the data contains tags or attributes of information. The\ndata is queried based on some fact between specific time windows. Therefore, time-\nseries databases are not general-purpose databases. In summary of all the database types discussed in this section, Table 6-6 shows some\npopular database products for the database type. Table 6-6. Summary of database types and products in the database type\nDatabase type\nProducts\nRelational\nPostgreSQL, Oracle, Microsoft SQL\nKey-value\nRiak KV, Amazon DynamoDB, Redis\nDocument\nMongoDB, Couchbase, AWS DocumentDB\nColumn family\nCassandra, Scylla, Amazon SimpleDB\nGraph\nNeo4j, Infinite Graph, Tiger Graph\nNewSQL\nVoltDB, ClustrixDB, SimpleStore (aka MemSQL)\n178 \n| \nChapter 6: Pulling Apart Operational Data", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 196", "position": 196, "chunk_type": "semantic", "token_estimate": 292}
{"text": "Database type: Products\nCloud native\nSnowflake, Datomic, Redshift\nTime-series\nInfluxDB, kdb+, Amazon Timestream\nSysops Squad Saga: Polyglot Databases\nThursday, December 16, 16:05\nNow that the team had formed data domains from the monolithic Sysops Squad data-\nbase, Devon noticed that the Survey data domain would be a great candidate for\nmigrating from a traditional relational database to a document database using JSON. However, Dana, the head of data architecture, didn\u2019t agree and wanted to keep the\ntables as relational. \u201cI simply don\u2019t agree,\u201d said Dana. \u201cThe survey tables have always worked in the past as\nrelational tables, so I see no reason to change things around. \" \u201cActually,\u201d said Skyler, \u201cif you had originally talked with us about this when the system was first being\ndeveloped, you would understand that from a user interface perspective, it\u2019s really hard to deal with\nrelational data for something like a customer survey. So I disagree. It may work out good for you, but\nfrom a user interface development standpoint, dealing with relational data for the survey stuff has\nbeen a major pain point.\u201d\n\u201cSee, so there you are,\u201d said Devon. \u201cThis is why we need to change it to a document database.\u201d\n\u201cYou seem to forget that as the data architect for this company, I am the one who has ultimate\nresponsibility for all these different databases. You can\u2019t just start adding different database types to\nthe system,\u201d said Dana. \u201cBut it would be a much better solution,\u201d said Devon. \u201cSorry, but I\u2019m not going to cause a disruptor on the database teams just so Skyler can have an easier\njob maintaining the user interface. Things don\u2019t work that way.\u201d\n\u201cWait,\u201d said Skyler, \u201cdidn\u2019t we all agree that part of the problem of the current monolithic Sysops\nSquad application was that the development teams didn\u2019t work close enough with the database\nteams?\u201d\n\u201cYes,\u201d said Dana. \u201cWell then,\u201d said Skyler, \u201clet\u2019s do that. Let\u2019s work together to figure this out.\u201d\n\u201cOK,\u201d said Dana, \u201cbut what I\u2019m going to need from you and Devon is a good solid justification for\nintroducing another type of database into the mix.\u201d\n\u201cYou got it,\u201d said Devon. \u201cWe\u2019ll start working on that right away.\u201d\nSysops Squad Saga: Polyglot Databases \n| \n179", "domains": ["Design Principles", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 197", "position": 197, "chunk_type": "semantic", "token_estimate": 372}
{"text": "Devon and Skyler knew that a document database would be a much better solution for the cus-: tomer survey data, but they weren\u2019t sure how to build the right justifications for Dana to agree to\nmigrate the data. Skyler suggested that they meet with Addison to get some help, since both\nagreed that this was somewhat an architectural concern. Addison agreed to help, and set up a\nmeeting with Parker (the Sysops Squad product owner) to validate whether there was any business\njustification to migrating the customer survey tables to a document database. \u201cThanks for meeting with us, Parker,\u201d said Addison. \u201cAs I mentioned to you before, we are thinking of\nchanging the way the customer survey data is stored, and have a few questions for you.\u201d\n\u201cWell,\u201d said Parker, \u201cthat was one of the reasons why I agreed to this meeting. You see, the customer\nsurvey part of the system has been a major pain point for the marketing department, as well as for\nme.\u201d\n\u201cHuh?\u201d asked Skyler. \u201cWhat do you mean?\u201d\n\u201cHow long does it take you to apply even the smallest of change requests to the customer surveys?\u201d\nasked Parker. \u201cWell,\u201d said Devon, \u201cit\u2019s not too bad from the database side. I mean, it\u2019s a matter of adding a new\ncolumn for a new question or changing the answer type.\u201d\n\u201cHold on,\u201d said Skyler. \u201cSorry, but for me it\u2019s a major change, even when you add an additional ques-\ntion. You have no idea how hard it is to query all of that relational data and render a customer survey\nin the user interface. So, my answer is, a very long time.\u201d\n\u201cListen,\u201d said Parker. \u201cWe on the business side of things get very frustrated ourselves when even the\nsimplest of changes take you literally days to do. It\u2019s simply not acceptable.\u201d\n\u201cI think I can help here,\u201d said Addison. \u201cSo Parker, what you\u2019re saying is that the customer survey\nchanges frequently, and it is taking too long to make the changes?\u201d\n\u201cCorrect,\u201d said Parker. \u201cThe marketing department not only wants better flexibility in the customer\nsurveys, but better response from the IT department as well.", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 198", "position": 198, "chunk_type": "semantic", "token_estimate": 359}
{"text": "consisted of two primary tables\u2014a Survey table and a Question table, with a one-to-many relation-: ship between the two tables. Figure 6-34. Tables and relationships in the sysops survey data domain\nAn example of the data contained in each table is shown Figure 6-35, where the Question table con-\ntains the question, the answer options, and the data type for the answer. Figure 6-35. Relational data in tables for survey and question in the survey data domain\n\u201cSo, essentially we have two options for modeling the survey questions in a document database,\u201d\nsaid Devon. \u201cA single aggregate document or one that is split.\u201d\n\u201cHow to we know which one to use?\u201d asked Skyler, happy that the development teams were now\nfinally working with the database teams to arrive at a unified solution. \u201cI know,\u201d said Addison, \u201clet\u2019s model both so we can visually see the trade-offs with each approach.\u201d\nDevon showed the team that with the single aggregate option, as shown in Figure 6-36, with the\ncorresponding source code listing in Example 6-3, both the survey data and all related question data\nwere stored as one document. Therefore, the entire customer survey could be retrieved from the\ndatabase by using a single get operation, making it easy for Skyler and others on the development\nteam to work with the data. Sysops Squad Saga: Polyglot Databases \n| \n181", "domains": ["Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 199", "position": 199, "chunk_type": "semantic", "token_estimate": 227}
{"text": "Figure 6-36. Survey model with single aggregate: Example 6-3. JSON document for single aggregate design with children embedded", "domains": ["Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 200", "position": 200, "chunk_type": "semantic", "token_estimate": 18}
{"text": "Survey aggregate with embedded questions: {\n    \"survey_id\": \"19999\",\n    \"created_date\": \"Dec 28 2021\",\n    \"description\": \"Survey to gauge customer...\",\n    \"questions\": [\n        {\n            \"question_id\": \"50001\",\n            \"question\": \"Rate the expert\",\n            \"answer_type\": \"Option\",\n            \"answer_options\": \"1,2,3,4,5\",\n            \"order\": \"2\"\n        },\n        {\n            \"question_id\": \"50000\",\n            \"question\": \"Did the expert fix the problem? \",\n            \"answer_type\": \"Boolean\",\n            \"answer_options\": \"Yes,No\",\n            \"order\": \"1\"\n        }\n    ]\n}\n182 \n| \nChapter 6: Pulling Apart Operational Data", "domains": ["Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 200", "position": 200, "chunk_type": "semantic", "token_estimate": 61}
{"text": "\u201cI really like that approach,\u201d said Skyler. \u201cEssentially, I wouldn\u2019t have to worry so much about aggre-: gating things myself in the user interface, meaning I could simply render the document I retrieve on\nthe web page.\u201d\n\u201cYeah,\u201d said Devon, \u201cbut it would require additional work on the database side as questions would be\nreplicated in each survey document. You know, the whole reuse argument. Here, let me show you\nthe other approach.\u201d\nSkyler explained that another way to think about aggregates was to split the survey and question\nmodel so that the questions could be operated on in an independent fashion, as shown in Figure\n6-37, with the corresponding source code listing in Example 6-4. This would allow the same ques-\ntion to be used in multiple surveys, but would be harder to render and retrieve than the single\naggregate. Figure 6-37. Survey model with multiple aggregates with references\nExample 6-4. JSON document with aggregates split and parent document showing\nreferences to children", "domains": ["Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 201", "position": 201, "chunk_type": "semantic", "token_estimate": 164}
{"text": "Survey aggregate with references to Questions: {\n    \"survey_id\": \"19999\",\n    \"created_date\": \"Dec 28\",\n    \"description\": \"Survey to gauge customer...\",\n    \"questions\": [\n        {\"question_id\": \"50001\", \"order\": \"2\"},\n        {\"question_id\": \"50000\", \"order\": \"1\"}\n    ]\nSysops Squad Saga: Polyglot Databases \n| \n183", "domains": ["Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 201", "position": 201, "chunk_type": "semantic", "token_estimate": 35}
{"text": "Question aggregate: {\n    \"question_id\": \"50001\",\n    \"question\": \"Rate the expert\",\n    \"answer_type\": \"Option\",\n    \"answer_options\": \"1,2,3,4,5\"\n}\n{\n    \"question_id\": \"50000\",\n    \"question\": \"Did the expert fix the problem? \",\n    \"answer_type\": \"Boolean\",\n    \"answer_options\": \"Yes,No\"\n}\nBecause most of the complexity and change issues were in the user interface, Skyler liked the single\naggregate model better. Devon liked the multiple aggregate to avoid duplication of question data in\neach survey. However, Addison pointed out that there were only five survey types (one for each\nproduct category), and that most of the changes involved adding or removing questions. The team\ndiscussed the trade-offs, and all agreed that they were willing to trade off some duplication of ques-\ntion data for the ease of changes and rendering on the user interface side. Because of the difficulty\nof this decision and the structural nature of changing the data, Addison created an ADR to record\nthe justifications of this decision: \nADR: Use of Document Database for Customer Survey\nContext\nCustomers receive a survey after the work has been completed by the customer, which is\nrendered on a web page for the customer to fill out and submit. The customer receives\none of five survey types based on the type of electronic product fixed or installed. The\nsurvey is currently stored in a relational database, but the team wants to migrate the sur-\nvey to a document database using JSON. Decision\nWe will use a document database for the customer survey. The Marketing Department requires more flexibility and timeliness for changes to the cus-\ntomer surveys. Moving to a document database would not only provide better flexibility,\nbut also better timeliness for changes needed to the customer surveys. Using a document database would simplify the customer survey user interface and better\nfacilitate changes to the surveys. Consequences\nSince we will be using a single aggregate, multiple documents would need to be changed\nwhen a common survey question is updated, added, or removed. Survey functionality will need to be shut down during the data migration from the rela-\ntional database to the document database. 184 \n| \nChapter 6: Pulling Apart Operational Data", "domains": ["Design Patterns", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 202", "position": 202, "chunk_type": "semantic", "token_estimate": 350}
{"text": "\u201cI\u2019m telling you,\u201d said Taylen, \u201cwe need to break up the domain services into smaller services. They are: simply too coarse-grained for microservices. From what I remember, micro means small. We are, after\nall, moving to microservices. What Addison and Austen are suggesting simply doesn\u2019t fit with the\nmicroservices model.\u201d\n\u201cNot every portion of an application has to be microservices,\u201d said Logan. \u201cThat\u2019s one of the biggest\npitfalls of the microservices architecture style.\u201d\n\u201cIf that\u2019s the case, then how do you determine what services should and shouldn\u2019t be broken apart?\u201d\nasked Taylen. \u201cLet me ask you something, Taylen,\u201d said Logan. \u201cWhat is your reason for wanting to make all of the\nservices so small?\u201d\n\u201cSingle-responsibility principle,\u201d answered Taylen. \u201cLook it up. That\u2019s what microservices is based on.\u201d\n\u201cI know what the single-responsibility principle is,\u201d said Logan. \u201cAnd I also know how subjective it can\nbe. Let\u2019s take our customer notification service as an example. We can notify our customers through\nSMS, email, and we even send out postal letters. So tell me everyone, one service or three services?\u201d\n\u201cThree,\u201d immediately answered Taylen. \u201cEach notification method is its own thing. That\u2019s what micro-\nservices is all about.\u201d\n\u201cOne,\u201d answered Addison. \u201cNotification itself is clearly a single responsibility.\u201d\n\u201cI\u2019m not sure,\u201d answered Austen. \u201cI can see it both ways. Should we just toss a coin?\u201d\n\u201cThis is exactly why we need help,\u201d sighed Addison. \u201cThe key to getting service granularity right,\u201d said Logan, \u201cis to remove opinion and gut feeling, and\nuse granularity disintegrators and integrators to objectively analyze the trade-offs and form solid\njustifications for whether or not to break apart a service.\u201d\n\u201cWhat are granularity disintegrators and integrators?\u201d asked Austen. \u201cLet me show you,\u201d said Logan. Architects and developers frequently confuse the terms modularity and granularity, \nand in some cases even treat them to mean the same thing. Consider the following\ndictionary definitions of each of these terms:\nModularity\nConstructed with standardized units or dimensions for flexibility and variety in\nuse. Granularity\nConsisting of or appearing to consist of one of numerous particles forming a\nlarger unit. 186 \n| \nChapter 7: Service Granularity", "domains": ["Architectural Patterns and Styles", "Design Principles", "Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 204", "position": 204, "chunk_type": "semantic", "token_estimate": 353}
{"text": "It\u2019s no wonder so much confusion exists between these terms! Although the terms: have similar dictionary definitions, we want to distinguish between them because\nthey mean different things within the context of software architecture. In our usage,\nmodularity concerns breaking up systems into separate parts (see Chapter 3), whereas\ngranularity deals with the size of those separate parts. Interestingly enough, most\nissues and challenges within distributed systems are typically not related to modular\u2010\nity, but rather granularity. Determining the right level of granularity\u2014the size of a service\u2014is one of the many\nhard parts of software architecture that architects and development teams continually\nstruggle with. Granularity is not defined by the number of classes or lines of code in a\nservice, but rather what the service does\u2014hence why it is so hard to get service gran\u2010\nularity right. Architects can leverage metrics to monitor and measure various aspects of a service\nto determine the appropriate level of service granularity. One such metric used to\nobjectively measure the size of a service is to calculate the number of statements in a\nservice. Every developer has a different coding style and technique, which is why the\nnumber of classes and number of lines of code are poor metrics to use to measure\ngranularity. The number of statements, on the other hand, at least allows an architect\nor development team to objectively measure what the service is doing. Recall from\nChapter 4 that a statement is a single complete action performed in the source code,\nusually terminated by a special character (such as a semicolon in languages such as\nJava, C, C++, C#, Go, JavaScript; or a newline in languages such as F#, Python, and\nRuby). Another metric to determine service granularity is to measure and track the number\nof public interfaces or operations exposed by a service. Granted, while there is still a\nbit of subjectiveness and variability with these two metrics, it\u2019s the closest thing we\u2019ve\ncome up with so far to objectively measure and assess service granularity. Two opposing forces for service granularity are granularity disintegrators and granu\u2010\nlarity integrators. These opposing forces are illustrated in Figure 7-1. Granularity dis\u2010\nintegrators address the question \u201cWhen should I consider breaking apart a service\ninto smaller parts?\u201d\n, whereas Granularity integrators address the question \u201cWhen\nshould I consider putting services back together?\u201d One common mistake many devel\u2010\nopment teams make is focusing too much on granularity disintegrators while ignor\u2010\ning granularity integrators.", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 205", "position": 205, "chunk_type": "semantic", "token_estimate": 409}
{"text": "Figure 7-1. Service granularity depends on a balance of disintegrators and integrators: Granularity Disintegrators\nGranularity disintegrators provide guidance and justification for when to break a ser\u2010\nvice into smaller pieces. While the justification for breaking up a service may involve\nonly a single driver, in most cases the justification will be based on multiple drivers. The six main drivers for granularity disintegration are as follows:\nService scope and function\nIs the service doing too many unrelated things? Code volatility\nAre changes isolated to only one part of the service? Scalability and throughput\nDo parts of the service need to scale differently? Fault tolerance\nAre there errors that cause critical functions to fail within the service? Security\nDo some parts of the service need higher security levels than others? Extensibility\nIs the service always expanding to add new contexts? 188 \n| \nChapter 7: Service Granularity", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 206", "position": 206, "chunk_type": "semantic", "token_estimate": 144}
{"text": "The following sections detail each of these granularity disintegration drivers.: Service Scope and Function\nThe service scope and function is the first and most common driver for breaking up a\nsingle service into smaller ones, particularly with regard to microservices. There are\ntwo dimensions to consider when analyzing the service scope and function. The first\ndimension is cohesion: the degree and manner to which the operations of a particular\nservice interrelate. The second dimension is the overall size of a component, meas\u2010\nured usually in terms of the total number of statements summed from the classes that\nmake up that service, the number of public entrypoints into the service, or both. Consider a typical Notification Service that does three things: notifies a customer\nthrough SMS (Short Message Service), email, or a printed postal letter that is mailed\nto the customer. Although it is very tempting to break this service into three separate\nsingle-purpose services (one for SMS, one for email, and one for postal letters) as\nillustrated in Figure 7-2, this alone is not enough to justify breaking the service apart\nbecause it already has relatively strong cohesion\u2014all of these functions relate to one\nthing, notifying the customer. Because \u201csingle purpose\u201d is left for individual opinion\nand interpretation, it is difficult to know whether to break apart this service or not. Figure 7-2. A service with relatively strong cohesion is not a good candidate for disinte\u2010\ngration based on functionality alone\nGranularity Disintegrators \n| \n189", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 207", "position": 207, "chunk_type": "semantic", "token_estimate": 245}
{"text": "Now consider a single service that manages the customer profile information, cus\u2010: tomer preferences, and also customer comments made on the website. Unlike the\nprevious Notification Service example, this service has relatively weak cohesion\nbecause these three functions relate to a broader scope\u2014customer. This service is\npossibly doing too much, and hence should probably be broken into three separate\nservices, as illustrated in Figure 7-3. Figure 7-3. A service with relatively weak cohesion is a good candidate for disintegration\nThis granularity disintegrator is related to the single-responsibility principle coined\nby Robert C. Martin as part of his SOLID principles, which states, \u201cevery class should\nhave responsibility over a single part of that program\u2019s functionality, which it should\nencapsulate. All of that module, class or function\u2019s services should be narrowly\naligned with that responsibility.\u201d While the single-responsibility principle was origi\u2010\nnally scoped within the context of classes, in later years it has expanded to include\ncomponents and services. 190 \n| \nChapter 7: Service Granularity", "domains": ["Design Patterns", "Design Principles", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 208", "position": 208, "chunk_type": "semantic", "token_estimate": 163}
{"text": "Within the microservices architecture style, a microservice is defined as a single-: purpose, separately deployed unit of software that does one thing really well. No won\u2010\nder developers are so tempted to make services as small as possible without\nconsidering why they are doing so! The subjectiveness related to what is and isn\u2019t a\nsingle responsibility is where most developers get into trouble with regard to service\ngranularity. While there are some metrics (such as LCOM) to measure cohesion, it is\nnevertheless highly subjective when it comes to services\u2014is notifying the customer\none single thing, or is notifying via email one single thing? For this reason, it is vital\nto understand other granularity disintegrators to determine the appropriate level of\ngranularity. Code Volatility\nCode volatility--the rate at which the source code changes\u2014is another good driver for\nbreaking a service into smaller ones. This is also known as volatility-based decomposi\u2010\ntion. Objectively measuring the frequency of code changes in a service (easily done\nthrough standard facilities in any source code version-control system) can sometimes\nlead to a good justification for breaking apart a service. Consider the notification ser\u2010\nvice example again from the prior section. Service scope (cohesion) alone was not\nenough to justify breaking the service apart. However, by applying change metrics,\nrelevant information is revealed about the service:\n\u2022 SMS notification functionality rate of change: every six months (avg)\n\u2022 Email notification functionality rate of change: every six months (avg)\n\u2022 Postal letter notification functionality rate of change: weekly (avg)\nNotice that the postal letter functionality changes weekly (on average), whereas the\nSMS and email functionality rarely changes. As a single service, any change to the\npostal letter code would require the developer to test and redeploy the entire service,\nincluding SMS and email functionality. Depending on the deployment environment,\nthis also might mean SMS and email functionality would not be available when the\npostal letter changes are deployed. Thus, as a single service, testing scope is increased\nand deployment risk is high. However, by breaking this service into two separate\nservices (Electronic Notification and Postal Letter Notification), as illustrated in Fig\u2010\nure 7-4, frequent changes are now isolated into a single, smaller service. This in turn\nmeans that the testing scope is significantly reduced, deployment risk is lower, and\nSMS and email functionality is not disrupted during a deployment of postal letter\nchanges. Granularity Disintegrators \n| \n191", "domains": ["Architectural Patterns and Styles", "Design Principles"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 209", "position": 209, "chunk_type": "semantic", "token_estimate": 399}
{"text": "Figure 7-4. An area of high code change in a service is a good candidate for: disintegration\nScalability and Throughput\nAnother driver for breaking up a service into separate smaller ones is scalability and\nthroughput. The scalability demands of different functions of a service can be objec\u2010\ntively measured to qualify whether a service should be broken apart. Consider once\nagain the Notification Service example, where a single service notifies customers\nthrough SMS, email, and printed postal letter. Measuring the scalability demands of\nthis single service reveals the following information:\n\u2022 SMS notification: 220,000/minute\n\u2022 Email notification: 500/minute\n\u2022 Postal letter notification: 1/minute\nNotice the extreme variation between sending out SMS notifications and postal letter\nnotifications. As a single service, email and postal letter functionality must unneces\u2010\nsarily scale to meet the demands of SMS notifications, impacting cost and also elastic\u2010\nity in terms of mean time to startup (MTTS). Breaking the Notification Service into\nthree separate services (SMS, Email, and Letter), as illustrated in Figure 7-5, allows\neach of these services to scale independently to meet their varying demands of\nthroughput. 192 \n| \nChapter 7: Service Granularity", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 210", "position": 210, "chunk_type": "semantic", "token_estimate": 188}
{"text": "Figure 7-5. Differing scalability and throughput needs is a good disintegration driver: Fault Tolerance\nFault tolerance describes the ability of an application or functionality within a particu\u2010\nlar domain to continue to operate, even though a fatal crash occurs (such as an out-\nof-memory condition). Fault Tolerance is another good driver for granularity\ndisintegration. Consider the same consolidated Notification Service example that notifies customers\nthrough SMS, email, and postal letter (Figure 7-6). If the email functionality contin\u2010\nues to have problems with out-of-memory conditions and fatally crashes, the entire\nservice comes down, including SMS and postal letter processing. Separating this single consolidated Notification Service into three separate services\nprovides a level of fault tolerance for the domain of customer notification. Now, a\nfatal error in the functionality of the email service doesn\u2019t impact SMS or postal\nletters. Notice in this example that the Notification Service is split into three separate services\n(SMS, Email, and Postal Letter), even though email functionality is the only issue\nwith regard to frequent crashes (the other two are very stable). Since email function\u2010\nality is the only issue, why not combine the SMS and postal letter functionality into a\nsingle service? Consider the code volatility example from the prior section. In this case Postal Letter\nchanges constantly, whereas the other two (SMS and Email) do not. Splitting this\nGranularity Disintegrators \n| \n193", "domains": ["Design Principles", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 211", "position": 211, "chunk_type": "semantic", "token_estimate": 227}
{"text": "service into only two services made sense because Postal Letter was the offending: functionality, but Email and SMS are related\u2014they both have to do with electronically\nnotifying the customer. Now consider the fault-tolerance example. What do SMS\nnotification and Postal Letter notification have in common other than a notification\nmeans to the customer? What would be an appropriate self-descriptive name of that\ncombined service? Figure 7-6. Fault tolerance and service availability are good disintegration drivers\nMoving the email functionality to a separate service disrupts the overall domain cohe\u2010\nsion because the resulting cohesion between SMS and postal letter functionality is\nweak. Consider what the likely service names would be: Email Service and\u2026Other\nNotification Service? Email Service and\u2026SMS-Letter Notification Service? Email Ser\u2010\nvice and\u2026Non-Email Service? This naming problem relates back to the service scope\nand function granularity disintegrator\u2014if a service is too hard to name because it\u2019s\ndoing multiple things, then consider breaking apart the service. The following disin\u2010\ntegrations help in visualizing this important point:\n\u2022 Notification Service \u2192 Email Service, Other Notification Service (poor name)\n\u2022 Notification Service \u2192 Email Service, Non-Email Service (poor name)\n\u2022 Notification Service \u2192 Email Service, SMS-Letter Service (poor name)\n\u2022 Notification Service \u2192 Email Service, SMS Service, Letter Service (good names)\n194 \n| \nChapter 7: Service Granularity", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 212", "position": 212, "chunk_type": "semantic", "token_estimate": 214}
{"text": "In this example, only the last disintegration makes sense, particularly considering the: addition of another social media notification\u2014where would that go? Whenever\nbreaking apart a service, regardless of the disintegration driver, always check to see if\nstrong cohesion can be formed with the \u201cleftover\u201d functionality. Security\nA common pitfall when securing sensitive data is to think only in terms of the storage\nof that data. For example, securing PCI (Payment Card Industry) data from non-PCI\ndata might be addressed through separate schemas or databases residing in different\nsecure regions. What is sometimes missing from this practice, however, is also secur\u2010\ning how that data is accessed. Consider the example illustrated in Figure 7-7 that describes a Customer Profile Ser\u2010\nvice containing two main functions: customer profile maintenance for adding, chang\u2010\ning, or deleting basic profile information (name, address, and so on); and customer\ncredit card maintenance for adding, removing, and updating credit card information. Figure 7-7. Security and data access are good disintegration drivers\nWhile the credit card data may be protected, access to that data is at risk because the\ncredit card functionality is joined together with the basic customer profile functional\u2010\nity. Although the API entry points into the consolidated customer profile service may\ndiffer, nevertheless there is risk that someone entering into the service to retrieve the\ncustomer name might also have access to credit card functionality. By breaking this\nservice into two separate services, access to the functionality used to maintain credit\nGranularity Disintegrators \n| \n195", "domains": ["Design Principles", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 213", "position": 213, "chunk_type": "semantic", "token_estimate": 250}
{"text": "card information can be made more secure because the set of credit card operations is: going into only a single-purpose service. Extensibility\nAnother primary driver for granularity disintegration is_ extensibility_\u2014the ability\nto add additional functionality as the service context grows. Consider a payment ser\u2010\nvice that manages payments and refunds through multiple payment methods, includ\u2010\ning credit cards, gift cards, and PayPal transactions. Suppose the company wants to\nstart supporting other managed payment methods, such as reward points, store credit\nfrom returns; and other third-party payment services, such as ApplePay, SamsungPay,\nand so on. How easy is it to extend the payment service to add these additional pay\u2010\nment methods? These additional payment methods could certainly be added to a single consolidated\npayment service. However, every time a new payment method is added, the entire\npayment service would need to be tested (including other payment types), and the\nfunctionality for all other payment methods unnecessarily redeployed into produc\u2010\ntion. Thus, with the single consolidated payment service, the testing scope is\nincreased and deployment risk is higher, making it more difficult to add additional\npayment types. Now consider breaking up the existing consolidated service into three separate serv\u2010\nices (Credit Card Processing, Gift Card Processing, and PayPal Transaction Process\u2010\ning), as illustrated in Figure 7-8. Now that the single payment service is broken into separate services by payment\nmethods, adding another payment method (such as reward points) is only a matter of\ndeveloping, testing, and deploying a single service separate from the others. As a\nresult, development is faster, testing scope is reduced, and deployment risk is lower. Our advice is to apply this driver only if it is known ahead of time that additional\nconsolidated contextual functionality is planned, desired, or part of the normal\ndomain. For example, with notification, it is doubtful the means of notification would\ncontinually expand beyond the basic notification means (SMS, email, or letter). How\u2010\never, with payment processing, it is highly likely that additional payment types would\nbe added in the future, and therefore separate services for each payment type would\nbe warranted. Since it is often difficult to sometimes \u201cguess\u201d whether (and when) con\u2010\ntextual functionality might expand (such as additional payment methods), our advice\nis to wait on this driver as a primary means of justifying a granularly disintegration\nuntil a pattern can be established or confirmation of continued extensibility can be\nconfirmed. 196 \n| \nChapter 7: Service Granularity", "domains": ["Design Principles", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 214", "position": 214, "chunk_type": "semantic", "token_estimate": 407}
{"text": "Figure 7-8. Planned extensibility is a good disintegration driver: Granularity Integrators\nWhereas granularity disintegrators provide guidance and justification for when to\nbreak a service into smaller pieces, granularity integrators work in the opposite way\u2014\nthey provide guidance and justification for putting services back together (or not\nbreaking apart a service in the first place). Analyzing the trade-offs between disinte\u2010\ngration drivers and integration drivers is the secret to getting service granularity\nright. The four main drivers for granularity integration are as follows:\nDatabase transactions\nIs an ACID transaction required between separate services? Workflow and choreography\nDo services need to talk to one another? Shared code: Do services need to share\ncodeamong one another? Database relationships: Although a service can be bro\u2010\nken apart, can the data it uses be broken apart as well? The following sections detail each of these granularity integration drivers. Granularity Integrators \n| \n197", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 215", "position": 215, "chunk_type": "semantic", "token_estimate": 147}
{"text": "Database Transactions: Most monolithic systems and course-grained domain services using relational data\u2010\nbases rely on single-unit-of-work database transactions to maintain data integrity and\nconsistency; see \u201cDistributed Transactions\u201d on page 263 for the details of ACID\n(database) transactions and how they differ from BASE (distributed) transactions. To\nunderstand how database transactions impact service granularity, consider the situa\u2010\ntion illustrated in Figure 7-9 where customer functionality has been split into a Cus\u2010\ntomer Profile Service that maintains customer profile information and a Password\nService that maintains password and other security-related information and\nfunctionality. Notice that having two separate services provides a good level of security access con\u2010\ntrol to password information since access is at a service level rather than at a request\nlevel. Access to operations such as changing a password, resetting a password, and\naccessing a customer\u2019s password for sign-in can all be restricted to a single service\n(and hence the access can be restricted to that single service). However, while this\nmay be a good disintegration driver, consider the operation of registering a new cus\u2010\ntomer, as illustrated in Figure 7-10. Figure 7-9. Separate services with atomic operations have better security access control\n198 \n| \nChapter 7: Service Granularity", "domains": ["Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 216", "position": 216, "chunk_type": "semantic", "token_estimate": 200}
{"text": "Figure 7-10. Separate services with combined operations do not support database: (ACID) transactions\nWhen registering a new customer, both profile and encrypted password information\nis passed into the Profile Service from a user interface screen. The Profile Service\ninserts the profile information into its corresponding database table, commits that\nwork, and then passes the encrypted password information to the Password Service,\nwhich in turn inserts the password information into its corresponding database table\nand commits its own work. While separating the services provides better security access control to the password\ninformation, the trade-off is that there is no ACID transaction for actions such as reg\u2010\nistering a new customer or unsubscribing (deleting) a customer from the system. If\nthe password service fails during either of these operations, data is left in an inconsis\u2010\ntent state, resulting in complex error handling (which is also error prone) to reverse\nthe original profile insert or take other corrective action (see \u201cTransactional Saga Pat\u2010\nterns\u201d on page 324 for the details of eventual consistency and error handling within\ndistributed transactions). Thus, if having a single-unit-of-work ACID transaction is\nrequired from a business perspective, these services should be consolidated into a sin\u2010\ngle service, as illustrated in Figure 7-11. Granularity Integrators \n| \n199", "domains": ["Design Patterns", "Design Principles", "Software Quality Attributes", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 217", "position": 217, "chunk_type": "semantic", "token_estimate": 208}
{"text": "Figure 7-11. A single service supports database (ACID) transactions: Workflow and Choreography\nAnother common granularity integrator is workflow and choreography--services talk\u2010\ning to one another (also sometimes referred to as interservice communication or east-\nwest communications). Communication between services is fairly common and in\nmany cases necessary in highly distributed architectures like microservices. However,\nas services move toward a finer level of granularity based on the disintegration factors\noutlined in the previous section, service communication can increase to a point\nwhere negative impacts start to occur. Issues with overall fault tolerance is the first impact of too much synchronous inter-\nservice communication. Consider the diagram in Figure 7-12: Service A communi\u2010\ncates with services B and C, Service B communicates with Service C, Service D\ncommunicates with Service E, and finally Service E communicates with Service C. In\nthis case, if Service C goes down, all other services become nonoperational because of\na transitive dependency with Service C, creating an issue with overall fault tolerance,\navailability, and reliability. 200 \n| \nChapter 7: Service Granularity", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 218", "position": 218, "chunk_type": "semantic", "token_estimate": 174}
{"text": "Figure 7-12. Too much workflow impacts fault tolerance: Interestingly enough, fault tolerance is one of the granularity disintegration drivers\nfrom the previous section\u2014yet when those services need to talk to one another, noth\u2010\ning is really gained from a fault-tolerance perspective. When breaking apart services,\nalways check to see if the functionalities are tightly coupled and dependent on one\nanother. If it is, then overall fault tolerance from a business request standpoint won\u2019t\nbe achieved, and it might be best to consider keeping the services together. Overall performance and responsiveness is another driver for granularity integration\n(putting services back together). Consider the scenario in Figure 7-13: a large cus\u2010\ntomer service is split into five separate services (services A through E). While each of\nthese services has its own collection of cohesive atomic requests, retrieving all of the\ncustomer information collectively from a single API request into a single user inter\u2010\nface screen involves five separate hops when using choreography (see Chapter 11 for\nan alternative solution to this problem using orchestration). Assuming 300 ms in net\u2010\nwork and security latency per request, this single request would incur an additional\n1500 ms just in latency alone! Consolidating all of these services into a single service\nwould remove the latency, therefore increasing overall performance and\nresponsiveness. Granularity Integrators \n| \n201", "domains": ["Design Principles", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 219", "position": 219, "chunk_type": "semantic", "token_estimate": 219}
{"text": "Figure 7-13. Too much workflow impacts overall performance and responsiveness: In terms of overall performance, the trade-off for this integration driver is balancing\nthe need to break apart a service with the corresponding performance loss if those\nservices need to communicate with one another. A good rule of thumb is to take into\nconsideration the number of requests that require multiple services to communicate\nwith one another, also taking into account the criticality of those requests requiring\ninterservice communication. For example, if 30% of the requests require a workflow\nbetween services to complete the request and 70% are purely atomic (dedicated to\nonly one service without the need for any additional communication), then it might\nbe OK to keep the services separate. However, if the percentages are reversed, then\nconsider putting them back together again. This assumes, of course, that overall per\u2010\nformance matters. There\u2019s more leeway in the case of backend functionality where an\nend user isn\u2019t waiting for the request to complete. The other performance consideration is with regard to the criticality of the request\nrequiring workflow. Consider the previous example, where 30% of the requests\nrequire a workflow between services to complete the request, and 70% are purely\natomic. If a critical request that requires extremely fast response time is part of that\n30%, then it might be wise to put the services back together, even though 70% of the\nrequests are purely atomic. Overall reliability and data integrity are also impacted with increased service commu\u2010\nnication. Consider the example in Figure 7-14: customer information is separated\ninto five separate customer services. In this case, adding a new customer to the sys\u2010\ntem involves the coordination of all five customer services. However, as explained in\na previous section, each of these services has its own database transaction. Notice in\n202 \n| \nChapter 7: Service Granularity", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 220", "position": 220, "chunk_type": "semantic", "token_estimate": 308}
{"text": "Figure 7-14 that services A, B, and C have all committed part of the customer data, but: Service D fails. Figure 7-14. Too much workflow impacts reliability and data integrity\nThis creates a data consistency and data integrity issue because part of the customer\ndata has already been committed, and may have already been acted upon through a\nretrieval of that information from another process or even a message sent out from\none of those services broadcasting an action based on that data. In either case, that\ndata would either have to be rolled back through compensating transactions or\nmarked with a specific state to know where the transaction left off in order to restart\nit. This is very messy situation, one we describe in detail in \u201cTransactional Saga Pat\u2010\nterns\u201d on page 324. If data integrity and data consistency are important or critical to\nan operation, it might be wise to consider putting those services back together. Shared Code\nShared source code is a common (and necessary) practice in software development. Functions like logging, security, utilities, formatters, converters, extractors, and so on\nare all good examples of shared code. However, things can get complicated when\ndealing with shared code in a distributed architecture and can sometimes influence\nservice granularity. Shared code is often contained in a shared library, such as a JAR file in the Java Eco\u2010\nsystem, a GEM in the Ruby environment, or a DLL in the .NET environment, and is\ntypically bound to a service at compile time. While we dive into code reuse patterns\nin detail in Chapter 8, here we illustrate only how shared code can sometimes\nGranularity Integrators \n| \n203", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 221", "position": 221, "chunk_type": "semantic", "token_estimate": 277}
{"text": "of the collective codebase. Breaking up the collective functionality into separate: services would mean that almost half of the source code is in a shared library\nused only by those three services. In this example it might be wise to consider\nkeeping the collective customer-related functionality in a single consolidated ser\u2010\nvice along with the shared code (particularly if the shared code changes fre\u2010\nquently, as discussed next). Frequent shared code changes\nRegardless of the size of the shared library, frequent changes to shared function\u2010\nality require frequent coordinated changes to the services using that shared\ndomain functionality. While versioning can sometimes be used to help mitigate\ncoordinated changes, eventually services using that shared functionality will need\nto adopt the latest version. If the shared code changes frequently, it might be wise\nto consider consolidating the services using that shared code to help mitigate the\ncomplex change coordination of multiple deployment units. Defects that cannot be versioned\nWhile versioning can help mitigate coordinated changes and allow for backward\ncompatibility and agility (the ability to respond quickly to change), at times cer\u2010\ntain business functionality must be applied to all services at the same time (such\nas a defect or a change in business rules). If this happens frequently, it might be\ntime to consider putting services back together to simplify the changes. Data Relationships\nAnother trade-off in the balance between granularity disintegrators and integrators is\nthe relationship between the data that a single consolidated service uses as opposed to\nthe data that separate services would use. This integrator driver assumes that the data\nresulting from breaking apart a service is not shared, but rather formed into tight\nbounded contexts within each service to facilitate change control and support overall\navailability and reliability. Consider the example in Figure 7-16: a single consolidated service has three functions\n(A, B, and C) and corresponding data table relationships. The solid lines pointing to\nthe tables represent writes to the tables (hence data ownership), and the dotted lines\npointing away from the tables represent read-only access to the table. Performing a\nmapping operation between the functions and the tables reveals the results shown in\nTable 7-1, where owner implies writes (and corresponding reads) and access implies\nread-only access to a table not owned by that function. Granularity Integrators \n| \n205", "domains": ["Design Principles", "Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 223", "position": 223, "chunk_type": "semantic", "token_estimate": 385}
{"text": "Figure 7-16. The database table relationships of a consolidated service: Table 7-1. Function-to-table mapping\nFunction Table 1\nTable 2\nTable 3\nTable 4\nTable 5\nTable 6\nA\nowner\nowner\nowner\nowner\nB\nowner\naccess\nC\naccess\nowner\nAssume that based on some of the disintegration drivers outlined in the prior section,\nthis service was broken into three separate services (one for each of the functions in\nthe consolidated service); see Figure 7-17. However, breaking apart the single consoli\u2010\ndated service into three separate services now requires the corresponding data tables\nto be associated with each service in a bounded context. Notice at the top of Figure 7-17 that Service A owns tables 1, 2, 4, and 6 as part of its\nbounded context; Service B owns table 3; and Service C owns table 5. However, notice\nin the diagram that every operation in Service B requires access to data in table 5\n(owned by Service C), and every operation in Service C requires access to data in\ntable 3 (owned by Service B). Because of the bounded context, Service B cannot sim\u2010\nply reach out and directly query table 5, nor can Service C directly query table 3. To better understand the bounded context and why Service C cannot simply access\ntable 3, say Service B (which owns table 3) decides to make a change to its business\nrules that requires a column to be removed from table 3. Doing so would break Ser\u2010\nvice C and any other services using table 3. This is why the bounded context concept\nis so important in highly distributed architectures like microservices. To resolve this\nissue, Service B would have to ask Service C for its data, and Service C would have to\nask Service B for its data, resulting in back-and-forth interservice communication\nbetween these services, as illustrated at the bottom of Figure 7-17. 206 \n| \nChapter 7: Service Granularity", "domains": ["Architectural Patterns and Styles", "Design Principles", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 224", "position": 224, "chunk_type": "semantic", "token_estimate": 319}
{"text": "Figure 7-17. Database table relationships impact service granularity: Based on the dependency of the data between services B and C, it would be wise to\nconsolidate those services into a single service to avoid the latency, fault tolerance,\nand scalability issues associated with the interservice communication between these\nservices, demonstrating that relationships between tables can influence service granu\u2010\nlarity. We\u2019ve saved this granularity integration driver for last because it is the one\ngranularity integration driver with the fewest number of trade-offs. While occasion\u2010\nally a migration from a monolithic system requires a refactoring of the way data is\norganized, in most cases it isn\u2019t feasible to reorganize database table entity relation\u2010\nships for the sake of breaking apart a service. We dive into the details about breaking\napart data in Chapter 6. Granularity Integrators \n| \n207", "domains": ["Design Principles", "Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 225", "position": 225, "chunk_type": "semantic", "token_estimate": 136}
{"text": "Finding the Right Balance: Finding the right level of service granularity is hard. The secret to getting granularity\nright is understanding both granularity disintegrators (when to break apart a service)\nand granularity integrators (when to put them back together), and analyze the corre\u2010\nsponding trade-offs between the two. As illustrated in the previous scenarios, this\nrequires an architect to not only identify the trade-offs, but also to collaborate closely\nwith business stakeholders to analyze those trade-offs and arrive at the appropriate\nsolution for service granularity. Tables 7-2 and 7-3 summarize the drivers for disintegrators and integrators. Table 7-2. Disintegrator drivers (breaking apart a service)\nDisintegrator driver Reason for applying driver\nService scope\nSingle-purpose services with tight cohesion\nCode volatility\nAgility (reduced testing scope and deployment risk)\nScalability\nLower costs and faster responsiveness\nFault tolerance\nBetter overall uptime\nSecurity access\nBetter security access control to certain functions\nExtensibility\nAgility (ease of adding new functionality)\nTable 7-3. Integrator drivers (putting services back together)\nIntegrator driver\nReason for applying driver\nDatabase transactions Data integrity and consistency\nWorkflow\nFault tolerance, performance, and reliability\nShared code\nMaintainability\nData relationships\nData integrity and correctness\nArchitects can use the drivers in these tables to form trade-off statements that can\nthen be discussed and resolved by collaborating with a product owner or business\nsponsor. Example 1:\nArchitect: \u201cWe want to break apart our service to isolate frequent code changes, but in\ndoing so we won\u2019t be able to maintain a database transaction. Which is more important\nbased on our business needs\u2014better overall agility (maintainability, testability, and\ndeployability), which translates to faster time-to-market, or stronger data integrity and\nconsistency?\u201d\nProject Sponsor: \u201cBased on our business needs, I\u2019d rather sacrifice a little bit slower\ntime-to-market to have better data integrity and consistency, so let\u2019s leave it as a single\nservice for right now.\u201d\n208 \n| \nChapter 7: Service Granularity", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 226", "position": 226, "chunk_type": "semantic", "token_estimate": 309}
{"text": "Example 2:: Architect: \u201cWe need to keep the service together to support a database transaction\nbetween two operations to ensure data consistency, but that means sensitive function\u2010\nality in the combined single service will be less secure. Which is more important based\non our business needs\u2014better data consistency or better security?\u201d\nProject Sponsor: \u201cOur CIO has been through some rough situations with regard to\nsecurity and protecting sensitive data, and it\u2019s on the forefront of their mind and part\nof almost every discussion. In this case, it\u2019s more important to secure sensitive data, so\nlet\u2019s keep the services separate and work out how we can mitigate some of the issues\nwith data consistency.\u201d\nExample 3:\nArchitect: \u201cWe need to break apart our payment service to provide better extensibility\nfor adding new payment methods, but that means we will have increased workflow\nthat will impact the responsiveness when multiple payment types are used for an order\n(which happens frequently). Which is more important based on our business needs\u2014\nbetter extensibility within the payment processing, hence better agility and overall\ntime-to-market, or better responsiveness for making a payment?\u201d\nProject Sponsor: \u201cGiven that I see us adding only two, maybe three more payment\ntypes over the next couple of years, I\u2019d rather have us focus on the overall responsive\u2010\nness since the customer must wait for payment processing to be complete before the\norder ID is issued.\u201d\nSysops Squad Saga: Ticket Assignment Granularity\nMonday, October 25 11:08\nOnce a trouble ticket has been created by a customer and accepted by the system, it\nmust be assigned to a Sysops Squad expert based on their skill set, location, and availa-\nbility. Ticket assignment involves two main components\u2014a Ticket Assignment compo-\nnent that determines which consultant should be assigned the job, and the Ticket\nRouting component that locates the Sysops Squad expert, forwards the ticket to the\nexpert\u2019s mobile device (via a custom Sysops Squad mobile app), and notifies the expert\nvia an SMS text message that a new ticket has been assigned. The Sysops Squad development team was having trouble deciding whether these two components\n(assignment and routing) should be implemented as a single consolidated service or two separate\nservices, as illustrated in Figure 7-18. The development team consulted with Addison (one of the\nSysops Squad architects) to help decide which option it should go with. Sysops Squad Saga: Ticket Assignment Granularity \n| \n209", "domains": ["Design Principles", "Software Quality Attributes", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 227", "position": 227, "chunk_type": "semantic", "token_estimate": 400}
{"text": "\u201cSee,\u201d said Skyler, \u201cyou cannot make a ticket assignment without routing it to the expert. So the two: functions are one.\u201d\n\u201cNo, no, no,\u201d said Taylen. \u201cYou don\u2019t understand. If an expert is seen to be available within a certain\namount of time, then that expert is assigned. Period. Routing is just a transport thing.\u201d\n\u201cWhat happens in the current functionality if a ticket can\u2019t be routed to the expert?\u201d asked Addison. \u201cThen another expert is selected,\u201d said Taylen. \u201cOK, so think about it a minute, Taylen,\u201d said Addison. \u201cIf assignment and routing are two separate\nservices, then the routing service would have to then communicate back to the assignment service,\nletting it know that the expert cannot be located and to pick another one. That\u2019s a lot of coordina-\ntion between the two services.\u201d\n\u201cYes, but they are still two separate functions, not one as Skyler is suggesting,\u201d said Taylen. \u201cI have an idea,\u201d said Addison. \u201cCan we all agree that the assignment and routing are two separate\nactivities, but are tightly bound synchronously to each other? Meaning, one function cannot exist\nwithout the other?\u201d\n\u201cYes,\u201d both Taylen and Skyler replied. \u201cIn that case,\u201d said Addison, \u201clet\u2019s analyze the trade-offs. Which is more important\u2014isolating the\nassignment functionality for change control purposes, or combining assignment and routing into a\nsingle service for better performance, error handling, and workflow control?\u201d\n\u201cWell,\u201d said Taylen, \u201cwhen you put it that way, obviously the single service. But I still want to isolate\nthe assignment code.\u201d\n\u201cOK,\u201d said Addison, \u201cin that case, how about we make three distinct architectural components in the\nsingle service. We can delineate assignment, routing, and shared code with separate namespaces in\nthe code. Would that help?\u201d\n\u201cYeah,\u201d said Taylen, \u201cthat would work. OK, you both win. Let\u2019s go with a single service then.\u201d\n\u201cTaylen,\u201d said Addison, \u201cit\u2019s not about winning, it\u2019s about analyzing the trade-offs to arrive at the most\nappropriate solution; that\u2019s all.\u201d\nWith everyone agreeing to a single service for assignment and routing, Addison wrote the following\narchitecture decision record (ADR) for this decision:\nADR: Consolidated Service for Ticket Assignment and Routing\nContext\nOnce a ticket is created and accepted by the system, it must be assigned to an expert and\nthen routed to that expert\u2019s mobile device. This can be done through a single consolida-\nted ticket assignment service or separate services for ticket assignment and ticket routing. Sysops Squad Saga: Ticket Assignment Granularity \n| \n211", "domains": ["Design Principles", "Software Quality Attributes", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 229", "position": 229, "chunk_type": "semantic", "token_estimate": 409}
{"text": "Decision: We will create a single consolidated ticket assignment service for the assignment and\nrouting functions of the ticket. Tickets are immediately routed to the Sysops Squad expert once they are assigned, so\nthese two operations are tightly bound and dependent each other. Both functions must scale the same, so there are no throughput differences between\nthese services, nor is back-pressure needed between these functions. Since both functions are fully dependent on each other, fault tolerance is not a driver for\nbreaking these functions apart. Making these functions separate services would require workflow between them, result-\ning in performance, fault tolerance, and possible reliability issues. Consequences\nChanges to the assignment algorithm (which occur on a regular basis) and changes to the\nrouting mechanism (infrequent change) would require testing and deployment of both\nfunctions, resulting in increased testing scope and deployment risk. Sysops Squad Saga: Customer Registration Granularity\nFriday January 14, 13:15\nCustomers must register with the system to gain access to the Sysops Squad support\nplan. During registration, customers must provide profile information (name, address,\nbusiness name if applicable, and so on), credit card information (which is billed on a\nmonthly basis), password and security question information, and a list of products pur-\nchased they would like to have covered under the Sysops Squad support plan. Some members of the development team insisted that this should be a single consoli-\ndated Customer Service containing all of the customer information, yet other members\nof the team disagreed and thought that there should be a separate service for each of these func-\ntions (a Profile service, Credit Card service, Password service, and a Supported Product service). Sky-\nler, having prior experience in PCI and PII data, thought that the credit card and password\ninformation should be a separate service from the rest, and hence only two services (a Profile service\ncontaining profile and product information and a separate Customer Secure service containing\ncredit card and password information). These three options are illustrated in Figure 7-19. 212 \n| \nChapter 7: Service Granularity", "domains": ["Design Principles", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 230", "position": 230, "chunk_type": "semantic", "token_estimate": 339}
{"text": "Figure 7-19. Options for customer registration: Because Addison was busy with the core ticketing functionality, the development team asked for\nAusten\u2019s help in resolving this granularity issue. Anticipating this will not be an easy decision, partic-\nularly since it involved security, Austen scheduled a meeting with Parker, (the product owner), and\nSam, the Penultimate Electronics security expert to discuss these options. \u201cOK, so what can we do for you?\u201d asked Parker. \u201cWell,\u201d said Austen, \u201cwe are struggling with how many services to create for registering customers\nand maintaining customer-related information, You see, there are four main pieces of data we are\ndealing with here: profile info, credit card info, password info, and purchased product info.\u201d\nSysops Squad Saga: Customer Registration Granularity \n| \n213", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 231", "position": 231, "chunk_type": "semantic", "token_estimate": 123}
{"text": "\u201cWhoa, hold on now,\u201d interrupted Sam. \u201cYou know that credit card and password information must: be secure, right?\u201d\n\u201cOf course we know it has to be secure,\u201d said Austen. \u201cWhat we\u2019re struggling with is the fact that\nthere\u2019s a single customer registration API to the backend, so if we have separate services they all\nhave to be coordinated together when registering a customer, which would require a distributed\ntransaction.\u201d\n\u201cWhat do you mean by that?\u201d asked Parker. \u201cWell,\u201d said Austen, \u201cwe wouldn\u2019t be able to synchronize all of the data together as one atomic unit\nof work.\u201d\n\u201cThat\u2019s not an option,\u201d said Parker. \u201cAll of the customer information is either saved in the database, or\nit\u2019s not. Let me put it another way. We absolutely cannot have the situation where we have a cus-\ntomer record without a corresponding credit card or password record. Ever.\u201d\n\u201cOK, but what about securing the credit card and password information?\u201d asked Sam. \u201cSeems to me,\nhaving separate services would allow much better security control access to that type of sensitive\ninformation.\u201d\n\u201cI think I may have an idea.\u201d said Austen. \u201cThe credit card information is tokenized in the database,\nright?\u201d\n\u201cTokenized and encrypted,\u201d said Sam. \u201cGreat. And the password information?\u201d asked Austen. \u201cThe same,\u201d said Sam. \u201cOK,\u201d said Austen, \u201cso it seems to me that what we really need to focus on here is controlling access\nto the password and credit card information separate from the other customer-related requests\u2014\nyou know, like getting and updating profile information, and so on.\u201d\n\u201cI think I see where you are coming from with your problem,\u201d said Parker. \u201cYou\u2019re telling me that if\nyou separate all of this functionality into separate services, you can better secure access to sensitive\ndata, but you cannot guarantee my all-or-nothing requirement. Am I right?\u201d\n\u201cExactly. That\u2019s the trade-off,\u201d said Austen. \u201cHold on,\u201d said Sam. \u201cAre you using the Tortoise security libraries to secure the API calls?\u201d\n\u201cYes. We use those libraries not only at the API layer, but also within each service to control access\nthrough the service mesh. So essentially it\u2019s a double-check,\u201d said Austen. \u201cHmmm,\u201d said Sam. \u201cOK, I\u2019m good with a single service providing you use the Tortoise security\nframework.\u201d\n\u201cMe too, providing we can still have the all-or-nothing customer registration process,\u201d said Parker. 214 \n| \nChapter 7: Service Granularity", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 232", "position": 232, "chunk_type": "semantic", "token_estimate": 391}
{"text": "\u201cThen I think we are all in agreement that the all-or-nothing customer registration is an absolute: requirement and we will maintain multilevel security access using Tortoise,\u201d said Austen. \u201cAgreed,\u201d said Parker. \u201cAgreed,\u201d said Sam. Parker noticed how Austen handled the meeting by facilitating the conversation rather than control-\nling it. This was an important lesson as an architect in identifying, understanding, and negotiating\ntrade-offs. Parker also better understood the difference between design versus architecture in that\nsecurity can be controlled through design (use of a custom library with special encryption) rather\nthan architecture (breaking up functionality into separate deployment units). Based on the conversation with Parker and Sam, Austen made the decision that customer-related\nfunctionality would be managed through a single consolidated domain service (rather than sepa-\nrately deployed services) and wrote the following ADR for this decision:\nADR: Consolidated Service for Customer-Related Functionality\nContext\nCustomers must register with the system to gain access to the Sysops Squad support plan. During registration, customers must provide profile information, credit card information,\npassword information, and products purchased. This can be done through a single con-\nsolidated customer service, a separate service for each of these functions, or a separate\nservice for sensitive and nonsensitive data. Decision\nWe will create a single consolidated customer service for profile, credit card, password, and\nproducts supported. Customer registration and unsubscribe functionality requires a single atomic unit of work. A single service would support ACID transactions to meet this requirement, whereas sepa-\nrate services would not. Use of the Tortoise security libraries in the API layer and the service mesh will mitigate\nsecurity access risk to sensitive information. Consequences\nWe will require the Tortoise security library to ensure security access in both the API gate-\nway and the service mesh. Because it\u2019s a single service, changes to source code for profile info, credit card, password,\nor products purchased will increase testing scope and increase deployment risk. The combined functionality (profile, credit card, password, and products purchased) will\nhave to scale as one unit. The trade-off discussed in a meeting with the product owner and security expert is trans-\nactionality versus security. Breaking the customer functionality into separate services\nSysops Squad Saga: Customer Registration Granularity \n| \n215", "domains": ["Design Principles", "Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 233", "position": 233, "chunk_type": "semantic", "token_estimate": 369}
{"text": "provides better security access, but doesn\u2019t support the \u201call-or-nothing\u201d database transac-: tion required for customer registration or unsubscribing. However, the security concerns\nare mitigated through the use the custom Tortoise security library. 216 \n| \nChapter 7: Service Granularity", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 234", "position": 234, "chunk_type": "semantic", "token_estimate": 38}
{"text": "PART II: Putting Things Back Together\nAttempting to divide a cohesive module would only result in increased coupling and\ndecreased readability. \u2014Larry Constantine\nOnce a system is broken apart, architects often find it necessary to stitch it back\ntogether to make it work as one cohesive unit. As Larry Constantine so eloquently\ninfers in the preceding quote, it\u2019s not quite as easy as it sounds, with lots of trade-offs\ninvolved when breaking things apart. In this second part of this book, we discuss various techniques for overcoming some\nof the hard challenges associated with distributed architectures, including managing\nservice communication, contracts, distributed workflows, distributed transactions,\ndata ownership, data access, and analytical data. Part I was about structure; Part II is about communication. Once an architect under\u2010\nstands the structure and the decisions that lead to it, it\u2019s time to think about how the\nstructural parts interact with each other.", "domains": ["Design Patterns", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 235", "position": 235, "chunk_type": "semantic", "token_estimate": 149}
{"text": "\u201cNo,\u201d said Skyler. \u201cThat code should be in a shared DLL.\u201d: \u201cWhat\u2019s all the shouting about over there?\u201d asked Addison. \u201cTaylen wants the authorization functionality to be in a shared service. That\u2019s just crazy. I think it\nshould go in the common shared DLL,\u201d said Skyler. \u201cNo way,\u201d said Taylen. \u201cIt\u2019s got to be in its own separate shared service.\u201d\n\u201cAnd,\u201d said Skyler, \u201cTaylen is insisting on having multiple shared libraries for the shared functionality\nrather than a single shared library.\u201d\n\u201cTell you what,\u201d said Addison. \u201cLet\u2019s go over the trade-offs of shared library granularity, and also go\nover the trade-offs between a shared library and a shared service to see if we can resolve these\nissues in a more reasonable and thoughtful manner.\u201d\nCode reuse is a normal part of software development. Common business domain\nfunctionality, such as formatters, calculators, validators, and auditing, are typically\nshared across multiple components, as is common infrastructure functionality, such\nas security, logging, and metrics gathering. In most monolithic architectures, code\nreuse is rarely given a second thought\u2014it\u2019s a matter of simply importing or auto-\ninjecting shared class files. However, in distributed architectures, as shown in Figure\n8-1, things get a bit more complicated, as questions arise about how to deal with\nshared functionality. Figure 8-1. Code reuse is a hard part of distributed architecture\nFrequently within highly distributed architectures like microservices and serverless\nenvironments, phrases like \u201creuse is abuse!\u201d and \u201cshare nothing!\u201d are touted by archi\u2010\ntects in an attempt to reduce the amount of shared code within these types of archi\u2010\ntectures. Architects in these environments have even been found to offer countering\nadvice to the famous DRY principle (Don\u2019t repeat yourself) by using an opposing\nacronym called WET (Write every time or Write everything twice). 220 \n| \nChapter 8: Reuse Patterns", "domains": ["Architectural Patterns and Styles", "Design Principles", "Software Quality Attributes", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 238", "position": 238, "chunk_type": "semantic", "token_estimate": 300}
{"text": "While developers should try to limit the amount of code reuse within distributed: architectures, it is nevertheless a fact of life in software development and must be\naddressed, particularly in distributed architectures. In this chapter, we introduce sev\u2010\neral techniques for managing code reuse within a distributed architecture, including\nreplicating code, shared libraries, shared services, and sidecars within a service mesh. For each of these options, we also discuss the pros, cons, and trade-offs of each\napproach. Code Replication\nIn code replication, shared code is copied into each service (or more specifically, each\nservice source code repository), as shown in Figure 8-2, thereby avoiding code shar\u2010\ning altogether. While it might sound crazy, this technique became popular in the early\ndays of microservices when a lot of confusion and misunderstanding arose about the\nbounded context concept, hence the drive to create a \u201cshare nothing architecture.\u201d In\ntheory, code replication seemed like a good approach at that time to reduce code\nsharing, but in practice it quickly fell apart. Figure 8-2. With replication, shared functionality is copied into each service\nWhile code replication isn\u2019t used much today, it nevertheless is still a valid technique\nfor addressing code reuse across multiple distributed services. This technique should\nbe approached with extreme caution for the obvious reason that if a bug is found in\nthe code or an important change to the code is needed, it would be very difficult and\ntime-consuming to update all services containing the replicated code. At times, however, this technique can prove useful, particularly for highly static one-\noff code that most (or all) services need. For example, consider the Java code in\nExample 8-1 and the corresponding C# code in Example 8-2 that identifies the class\nin the service that represents the service entry point (usually the restful API class\nwithin a service). Code Replication \n| \n221", "domains": ["Architectural Patterns and Styles", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 239", "position": 239, "chunk_type": "semantic", "token_estimate": 309}
{"text": "Example 8-1. Source code defining a service entry point annotation (Java): @Retention(RetentionPolicy.RUNTIME)\n@Target(ElementType.TYPE)\npublic @interface ServiceEntrypoint {}\n/* Usage:\n@ServiceEntrypoint\npublic class PaymentServiceAPI {\n   ...\n}\n*/\nExample 8-2. Source code defining a service entry point attribute (C#)\n[AttributeUsage(AttributeTargets.Class)]\nclass ServiceEntrypoint : Attribute {}\n/* Usage:\n[ServiceEntrypoint]\nclass PaymentServiceAPI {\n   ...\n}\n*/\nNote that the source code in Example 8-1 actually contains no functionality whatso\u2010\never. The annotation is simply a marker (or tag) used to identify a particular class as\nrepresenting the service entry point. However, this simple annotation is very useful\nfor placing other metadata annotations about a particular service, including the ser\u2010\nvice type, domain, bounded context, and so on; see Chapter 89 in 97 Things Every\nJava Programmer Should Know by Kevlin Henney and Trisha Gee (O\u2019Reilly) for a\ndescription of these metadata custom annotations. This kind of source code makes a good candidate for replication because it\u2019s static and\ndoesn\u2019t contain any bugs (and most likely will not in the future). If this were a unique\none-off class, it might be worth copying it into each service code repository rather\nthan creating a shared library for it. That said, we generally encourage investigating\nthe other code-sharing techniques presented in this chapter before opting for the\ncode replication technique. While the replication technique preserves the bounded context, it does make it diffi\u2010\ncult to apply changes if the code ever does need to be modified. Table 8-1 lists the\nvarious trade-offs associated with this technique. 222 \n| \nChapter 8: Reuse Patterns", "domains": ["Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 240", "position": 240, "chunk_type": "semantic", "token_estimate": 256}
{"text": "Trade-Offs: Table 8-1. Trade-offs for the code replication technique\nAdvantages\nDisadvantages\nPreserves the bounded context\nDifficult to apply code changes\nNo code sharing\nCode inconsistency across services\nNo versioning capabilities across services\nWhen to Use\nThe replication technique is a good approach when developers have simple static\ncode (like annotations, attributes, simple common utilities, and so on) that is either a\none-off class or code that is unlikely to ever change because of defects or functional\nchanges. However, as mentioned earlier, we encourage exploring other code-reuse\noptions before embracing the code replication technique. When migrating from a monolithic architecture to a distributed one, we\u2019ve also\nfound that the replication technique can sometimes work for common static utility\nclasses. For example, by replicating a Utility.cs C# class to all services, each service\ncan now remove (or enhance) the Utility.cs class to suit its particular needs, there\u2010\nfore eliminating unnecessary code and allowing the utility class to evolve for each\nspecific context (similar to the tactical forking technique described in Chapter 3). Again, the risk with this technique is that a defect or change is very difficult to propa\u2010\ngate to all services because the code is duplicated for each service. Shared Library\nOne of the most common techniques for sharing code is to use a shared library. A\nshared library is an external artifact (such as a JAR file, DLL, and so on) containing\nsource code that is used by multiple services which is typically bound to the service at\ncompile time (see Figure 8-3). Although the shared library technique seems simple\nand straightforward, it has its share of complexities and trade-offs, not the least of\nwhich is shared library granularity and versioning. Shared Library \n| \n223", "domains": ["Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 241", "position": 241, "chunk_type": "semantic", "token_estimate": 286}
{"text": "Breaking shared code into smaller functionality-based shared libraries (such as secu\u2010: rity, formatters, annotations, calculators, and so on) is better for change control and\noverall maintainability, but unfortunately creates a mess in terms of dependency\nmanagement. As shown in Figure 8-5, a change in shared class C7 impacts only Ser\u2010\nvice D and Service E, but managing the dependency matrix between shared libraries\nand services quickly starts looking like a big ball of distributed mud (or what some\npeople refer to as a distributed monolith). Figure 8-5. Changes to fine-grained shared libraries impact fewer services but increase\ndependencies\nThe choice of shared library granularity may not matter much with only a few serv\u2010\nices, but as the number of services increases, so do the issues associated with change\ncontrol and dependency management. Just imagine a system with 200 services and 40\nshared libraries\u2014it would quickly become overly complex and unmaintainable. Given these trade-offs of change control and dependency management, our advice is\nto generally avoid large, coarse-grained shared libraries and strive for smaller, func\u2010\ntionally partitioned libraries whenever possible, thus favoring change control over\ndependency management. For example, carving off relatively static functionality such\nas formatters and security (authentication and authorization) into their own shared\nlibraries isolates this static code, therefore reducing the testing scope and unnecessary\nversion deprecation deployments for other shared functionality. Versioning Strategies\nOur general advice about shared library versioning is to always use versioning! Ver\u2010\nsioning your shared libraries provides not only backward compatibility, but also a\nhigh level of agility\u2014the ability to respond quickly to change. To illustrate this point, consider a shared library containing common field validation\nrules called Validation.jar that is used by 10 services. Suppose one of those services\nShared Library \n| \n225", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 243", "position": 243, "chunk_type": "semantic", "token_estimate": 291}
{"text": "needs an immediate change to one of the validation rules. By versioning the Valida\u2010: tion.jar file, the service needing the change can immediately incorporate the new Val\u2010\nidation.jar version and be deployed to production right away, without any impact to\nthe other 9 services. Without versioning, all 10 services would have to be tested and\nredeployed when making the shared library change, thereby increasing the amount of\ntime and coordination for the shared library change (hence less agility). While the preceding advice may seem obvious, there are trade-offs and hidden com\u2010\nplexity in versioning. As a matter of fact, versioning can be so complex that your\nauthors often think of versioning as the ninth fallacy of distributed computing: \u201cver\u2010\nsioning is simple\u201d\nOne of the first complexities of shared library versioning is communicating a version\nchange. In a highly distributed architecture with multiple teams, it is often difficult to\ncommunicate a version change to a shared library. How do other teams know that\nValidation.jar just increased to version 1.5? What were the changes? What services\nare impacted? What teams are impacted? Even with the plethora of tools that manage\nshared libraries, versions, and change documentation (such as JFrog Artifactory),\nversion changes must nevertheless be coordinated and communicated to the right\npeople at the right time. Another complexity is the deprecation of older versions of a shared library\u2014remov\u2010\ning those versions no longer supported after a certain date. Deprecation strategies\nrange from custom (for individual shared libraries) all the way to global (for all shared\nlibraries). And, not surprisingly, trade-offs are involved with both approaches. Assigning a custom deprecation strategy to each shared library is usually the desired\napproach because libraries change at different rates. For example, if a Security.jar\nshared library doesn\u2019t change often, maintaining only two or three versions is a rea\u2010\nsonable strategy. However, if the Calculators.jar shared library changes weekly, main\u2010\ntaining only two or three versions means that all services using that shared library\nwill be incorporating a newer version on a monthly (or even weekly) basis\u2014causing a\nlot of unnecessary frequent retesting and redeployment. Therefore, maintaining 10\nversions of Calculators.jar would be a much more reasonable strategy because of the\nfrequency of change. The trade-off of this approach, however, is that someone must\nmaintain and track the deprecation for each shared library. This can sometimes be a\ndaunting task and is definitely not for the faint of heart.", "domains": ["Design Patterns", "Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 244", "position": 244, "chunk_type": "semantic", "token_estimate": 403}
{"text": "needs an immediate change to one of the validation rules. By versioning the Valida\u2010: The trade-off of this approach, however, is that someone must\nmaintain and track the deprecation for each shared library. This can sometimes be a\ndaunting task and is definitely not for the faint of heart. Because change is variable among the various shared libraries, the global deprecation\nstrategy, while simpler, is a less effective approach. The global deprecation strategy\ndictates that all shared libraries, regardless of the rate of change, will not support\nmore than a certain number of backward versions (for example, four). While this is\neasy to maintain and govern, it can cause significant churn--the constant retesting\nand redeploying of services\u2014just to maintain compatibility with the latest version of\n226 \n| \nChapter 8: Reuse Patterns", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 244", "position": 244, "chunk_type": "semantic", "token_estimate": 131}
{"text": "a frequently changed shared library. This can drive teams crazy and significantly: reduce overall team velocity and productivity. Regardless of the deprecation strategy used, serious defects or breaking changes to\nshared code invalidate any sort of deprecation strategy, causing all services to adopt\nthe latest version of a shared library at once (or within a very short period of time). This is another reason we recommend keeping shared libraries as fine-grained as\nappropriate and avoid the coarse-grained SharedStuff.jar type of libraries containing\nall the shared functionality in the system. One last word of advice regarding versioning: avoid the use of the LATEST version\nwhen specifying which version of a library a service requires. It has been our experi\u2010\nence that services using the LATEST version experience issues when doing quick fixes\nor emergency hot deployments into production, because something in the LATEST\nversion might be incompatible with the service, therefore causing additional develop\u2010\nment and testing effort for the team to release the service into production. While the shared library technique allows changes to be versioned (therefore provid\u2010\ning a good level of agility for shared code changes), dependency management can be\ndifficult and messy. Table 8-2 lists various trade-offs associated with this technique. Trade-Offs\nTable 8-2. Trade-offs for the shared library technique\nAdvantages\nDisadvantages\nAbility to version changes\nDependencies can be difficult to manage\nShared code is compile-based, reducing runtime\nerrors\nCode duplication in heterogeneous codebases\nGood agility for code shared code changes\nVersion deprecation can be difficult\nVersion communication can be difficult\nWhen To Use\nThe shared library technique is a good approach for homogeneous environments\nwhere shared code change is low to moderate. The ability to version (although some\u2010\ntimes complex) allows for good levels of agility when making shared code changes. Because shared libraries are usually bound to the service at compile time, operational\ncharacteristics such as performance, scalability, and fault tolerance are not impacted,\nand the risk of breaking other services with a change to common code is low because\nof versioning. Shared Library \n| \n227", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 245", "position": 245, "chunk_type": "semantic", "token_estimate": 341}
{"text": "Shared Service: The primary alternative to using a shared library for common functionality is to use a\nshared service instead. The shared service technique, illustrated in Figure 8-6, avoids\nreuse by placing shared functionality in a separately deployed service. Figure 8-6. With the shared service technique, common functionality is made available\nat runtime through separate services\nOne distinguishing factor about the shared service technique is that the shared code\nmust be in the form of composition, not inheritance. While there is a lot of debate\nabout the use of composition over inheritance from a source code design standpoint\n(see the Thoughtworks article \u201cComposition vs. Inheritance: How to Choose\u201d and\nMartin Fowler\u2019s article \u201cDesigned Inheritance\u201d), architecturally composition versus\ninheritance matters when choosing a code-reuse technique, particularly with the\nshared services technique. Back in the day, shared services were a common approach to address shared func\u2010\ntionality within a distributed architecture. Changes to shared functionality no longer\nrequire redeployment of services; rather, since changes are isolated to a separate ser\u2010\nvice, they can be deployed without redeploying other services needing the shared\nfunctionality. However, like everything in software architecture, many trade-offs are\nassociated with using shared services, including change risk, performance, scalability,\nand fault tolerance. 228 \n| \nChapter 8: Reuse Patterns", "domains": ["Design Principles", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 246", "position": 246, "chunk_type": "semantic", "token_estimate": 209}
{"text": "This necessarily brings to the forefront the topic of versioning. In the shared library: technique, versioning is managed through compile-time bindings, significantly\nreducing risk associated with a change in a shared library. However, how does one\nversion a simple shared service change? The immediate response, of course, is to use API endpoint versioning\u2014in other\nwords, create a new endpoint containing each shared service change, as shown in\nExample 8-3. Example 8-3. Discount calendar with versioning for shared service endpoint\napp/1.0/discountcalc?orderid=123\napp/1.1/discountcalc?orderid=123\napp/1.2/discountcalc?orderid=123\napp/1.3/discountcalc?orderid=123\nlatest change -> app/1.4/discountcalc?orderid=123\nUsing this approach, each time a shared service changes, the team would create a new\nAPI endpoint containing a new version of the URI. It\u2019s not difficult to see the issues\nthat arise with this practice. First of all, services accessing the discount calculator ser\u2010\nvice (or the corresponding configuration for each service) must change to point to\nthe correct version. Second, when should the team create a new API endpoint? What\nabout for a simple error message change? What about for a new calculation? Version\u2010\ning starts to become largely subjective at this point, and the services using the shared\nservice must still change to point to the correct endpoint. Another problem with API endpoint versioning is that it assumes all access to the\nshared service is through a RESTful API call going through a gateway or via point-to-\npoint communication. However, in some cases, access to a shared service through\ninterservice communication is commonly done through other types of protocols such\nas messaging and gRPC (in addition to a RESTful API call). This further complicates\nthe versioning strategy for a change, making it difficult to coordinate versions across\nmultiple protocols. The bottom line is that with the shared service technique, changes to a shared service\nare generally runtime in nature, and therefore carry much more risk than with shared\nlibraries. While versioning can help reduce this risk, it\u2019s much more complex to apply\nand manage than that of a shared library. 230 \n| \nChapter 8: Reuse Patterns", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 248", "position": 248, "chunk_type": "semantic", "token_estimate": 336}
{"text": "Performance: Because services requiring the shared functionality must make an interservice call to\na shared service, performance is impacted because of network latency (and security\nlatency, assuming the endpoints to the shared service are secure). This trade-off,\nshown in Figure 8-9, does not exist with the shared library technique when accessing\nshared code. Figure 8-9. Shared service introduces network and security latency\nUse of gRPC can help mitigate some of the performance issues by significantly reduc\u2010\ning network latency, as can the use of asynchronous protocols like messaging. With\nmessaging, the service needing the shared functionality can issue a request through a\nrequest queue, perform other work, and once needed, can retrieve the results through\na separate reply queue using a correlation ID (see Java Message Service, Second Edi\u2010\ntion by Mark Richards et al. (O\u2019Reilly) for more information about messaging\ntechniques). Shared Service \n| \n231", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 249", "position": 249, "chunk_type": "semantic", "token_estimate": 146}
{"text": "Scalability: Another drawback of the shared service technique is that the shared service must\nscale as services using the shared service scale. This can sometimes be a mess to man\u2010\nage, particularly with multiple services concurrently accessing the same shared ser\u2010\nvice. However, as illustrated in Figure 8-10, the shared library technique does not\nhave this issue because the shared functionality is contained within the service at\ncompile time. Figure 8-10. Shared services must scale as dependent services scale\nFault Tolerance\nWhile fault-tolerance issues can usually be mitigated through multiple instances of a\nservice, nevertheless it is a trade-off to consider when using the shared service techni\u2010\nque. As illustrated in Figure 8-11, if the shared service becomes unavailable, services\nrequiring the shared functionality are rendered nonoperational until the shared ser\u2010\nvice is available. The shared library technique does not have this issue since the\nshared functionality is contained in the service at compile time, and therefore\naccessed through standard method or function calls. 232 \n| \nChapter 8: Reuse Patterns", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 250", "position": 250, "chunk_type": "semantic", "token_estimate": 170}
{"text": "Figure 8-11. Shared services introduce fault-tolerance issues: While the shared service technique preserves the bounded context and is good for\nshared code that changes frequently, operational characteristics such as performance,\nscalability, and availability suffer. Table 8-3 lists the various trade-offs associated with\nthis technique. Trade-Offs\nTable 8-3. Trade-offs for the shared service technique\nAdvantages\nDisadvantages\nGood for high code volatility\nVersioning changes can be difficult\nNo code duplication in\nheterogeneous codebases\nPerformance is impacted due to latency\nPreserves the bounded context\nFault tolerance and availability issues due to service\ndependency\nNo static code sharing\nScalability and throughput issues due to service\ndependency\nIncreased risk due to runtime changes\nShared Service \n| \n233", "domains": ["Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 251", "position": 251, "chunk_type": "semantic", "token_estimate": 112}
{"text": "When to Use: The shared service technique is good to use in highly polyglot environments (those\nwith multiple heterogeneous languages and platforms), and also when shared func\u2010\ntionality tends to change often. While changes in a shared service tend to be much\nmore agile overall than with the shared library technique, be careful of runtime side-\neffects and risks to services needing the shared functionality. Sidecars and Service Mesh\nPerhaps the most common response to any question posed by an architect is \u201cIt\ndepends!\u201d No issue in distributed architectures better illustrates this ambiguity better\nthan operational coupling. One of the design goals of microservices architectures is a high degree of decoupling,\noften manifested in the advice \u201cDuplication is preferable to coupling.\u201d For example,\nlet\u2019s say that two Sysops Squad services need to pass customer information, yet the\ndomain-driven design bounded context insists that implementation details remain\nprivate to the service. Thus, a common solution allows each service its own internal\nrepresentation of entities such as Customer, passing that information in loosely cou\u2010\npled ways such as name-value pairs in JSON. Notice that this allows each service to\nchange its internal representation at will, including the technology stack, without\nbreaking the integration. Architects generally frown on duplicating code because it\ncauses synchronization issues, semantic drift, and a host of other issues, but some\u2010\ntimes forces exist that are worse than the problems of duplication, and coupling in\nmicroservices often fits that bill. Thus, in microservices architecture, the answer to\nthe question of \u201cshould we duplicate or couple to some capability?\u201d is likely duplicate,\nwhereas in another architecture style such as a service-based architecture, the correct\nanswer is likely couple. It depends! When designing microservices, architects have resigned themselves to the reality of\nimplementation duplication to preserve decoupling. But what about the type of capa\u2010\nbilities that benefit from high coupling? For example, consider common operational\ncapabilities such as monitoring, logging, authentication and authorization, circuit\nbreakers, and a host of other operational abilities that each service should have. But\nallowing each team to manage these dependencies often descends into chaos. For\nexample, consider a company like Penultimate Electronics trying to standardize on a\ncommon monitoring solution to make it easier to operationalize the various services. Yet if each team is responsible for implementing monitoring for their service, how\ncan the operations team be sure they did? Also, what about issues such as unified\nupgrades?", "domains": ["Architectural Patterns and Styles", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 252", "position": 252, "chunk_type": "semantic", "token_estimate": 402}
{"text": "The common solution that has emerged in the microservices ecosystem over the last: few years solves this problem in an elegant way, by using the Sidecar pattern. This\npattern is based on a much earlier architecture pattern defined by Alistair Cockburn,\nknown as the hexagonal architecture, illustrated in Figure 8-12. In this Hexagonal pattern, what we would now call the domain logic resides in the\ncenter of the hexagon, which is surrounded by ports and adaptors to other parts of\nthe ecosystem (in fact, this pattern is alternately known as the Ports and Adaptors Pat\u2010\ntern). While predating microservices by a number of years, this pattern has similari\u2010\nties to modern microservices, with one significant difference: data fidelity. The\nhexagonal architecture treated the database as just another adaptor that can be plug\u2010\nged in, but one of the insights from DDD suggests that data schemas and transaction\u2010\nality should be inside the interior\u2014like microservices. Figure 8-12. The Hexagonal pattern separated domain logic from technical coupling\nSidecars and Service Mesh \n| \n235", "domains": ["Architectural Patterns and Styles"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 253", "position": 253, "chunk_type": "semantic", "token_estimate": 171}
{"text": "The Sidecar pattern leverages the same concept as hexagonal architecture in that it: decouples the domain logic from the technical (infrastructure) logic. For example,\nconsider two microservices, as shown in Figure 8-13. Figure 8-13. Two microservices that share the same operational capabilities\nHere, each service includes a split between operational concerns (the larger compo\u2010\nnents toward the bottom of the service) and domain concerns, pictured in the boxes\ntoward the top of the service labeled \u201cdomain.\u201d If architects desire consistency in\noperational capabilities, the separable parts go into a sidecar component, metaphori\u2010\ncally named for the sidecar that attaches to motorcycles, whose implementation is\neither a shared responsibility across teams or managed by a centralized infrastructure\ngroup. If architects can assume that every service includes the sidecar, it forms a con\u2010\nsistent operational interface across services, typically attached via a service plane,\nshown in Figure 8-14. 236 \n| \nChapter 8: Reuse Patterns", "domains": ["Architectural Patterns and Styles", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 254", "position": 254, "chunk_type": "semantic", "token_estimate": 152}
{"text": "Figure 8-14. When each microservice includes a common component, architects can: establish links between them for consistent control\nIf architects and operations can safely assume that every service includes the sidecar\ncomponent (governed by fitness functions), it forms a service mesh, illustrated in Fig\u2010\nure 8-15. The boxes to the right of each service all interconnect, forming a \u201cmesh.\u201d\nHaving a mesh allows architects and DevOps to create dashboards, control opera\u2010\ntional characteristics such as scale, and a host of other capabilities. The Sidecar pattern allows governance groups like enterprise architects a reasonable\nrestraint over too many polyglot environments: one of the advantages of microservi\u2010\nces is a reliance on integration rather than a common platform, allowing teams to\nchoose the correct level of complexity and capabilities on a service-by-service basis. However, as the number of platforms proliferates, unified governance becomes more\ndifficult. Therefore, teams often use the consistency of the service mesh as a driver to\nsupport infrastructure and other cross-cutting concerns across multiple heterogene\u2010\nous platforms. For example, without a service mesh, if enterprise architects want to\nunify around a common monitoring solution, then teams must build a sidecar per\nplatform that supports that solution. Sidecars and Service Mesh \n| \n237", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 255", "position": 255, "chunk_type": "semantic", "token_estimate": 203}
{"text": "Figure 8-15. A service mesh is an operational link among services: The Sidecar pattern represents not only a way to decouple operational capabilities\nfrom domains\u2014it\u2019s an orthogonal reuse pattern to address a specific kind of coupling\n(see \u201cOrthogonal Coupling\u201d on page 238). Often, architectural solutions require sev\u2010\neral types of coupling, such as our current example of domain versus operational cou\u2010\npling. An orthogonal reuse pattern presents a way to reuse some aspect counter to\none or more seams in the architecture. For example, microservices architectures are\norganized around domains, but operational coupling requires cutting across those\ndomains. A sidecar allows an architect to isolate those concerns in a cross-cutting,\nbut consistent, layer through the architecture. Orthogonal Coupling\nIn mathematics, two lines are orthogonal if they intersect at right angles, which also\nimplies independence. In software architecture, two parts of an architecture may be\northogonally coupled: two distinct purposes that must still intersect to form a complete\nsolution. The obvious example from this chapter is an operational concern such as\nmonitoring, which is necessary but independent from domain behavior, like catalog\ncheckout. Recognizing orthogonal coupling allows architects to find intersection\npoints that cause the least entanglement between concerns. 238 \n| \nChapter 8: Reuse Patterns", "domains": ["Architectural Patterns and Styles"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 256", "position": 256, "chunk_type": "semantic", "token_estimate": 204}
{"text": "While the Sidecar pattern offers a nice abstraction, it has trade-offs like all other: architectural approaches, shown in Table 8-4. Trade-Offs\nTable 8-4. Trade-offs for the Sidecar pattern / service mesh technique\nAdvantages\nDisadvantages\nOffers a consistent way to create isolated coupling\nMust implement a sidecar per platform\nAllows consistent infrastructure coordination\nSidecar component may grow large/\ncomplex\nOwnership per team, centralized, or some\ncombination\nWhen to Use\nThe Sidecar pattern and service mesh offer a clean way to spread some sort of cross-\ncutting concern across a distributed architecture, and can be used by more than just\noperational coupling (see Chapter 14). It offers an architectural equivalent to the Dec\u2010\norator Design Pattern from the Gang of Four Design Patterns book (Addison Wesley)\n\u2014it allows an architect to \u201cdecorate\u201d behavior across a distributed architecture inde\u2010\npendent of the normal connectivity. Sysops Squad Saga: Common Infrastructure Logic\nThursday, February 10, 10:34\nSydney peeped into Taylen\u2019s office on a foggy morning. \u201cHey, are you using the shared\nMessage Dispatch library?\u201d\nTaylen replied, \u201cYes, we\u2019re trying to consolidate on that to get some consistency on mes-\nsage resolution.\u201d\nSydney said, \u201cOK, but now we\u2019re getting double log messages\u2014it looks like the library\nwrites to the logs, but our service also writes to the log. Is that as it should be?\u201d\n\u201cNo,\u201d Taylen replied. \u201cWe definitely don\u2019t want duplicate log entries. That just makes everything con-\nfusing. We should ask Addison about that.\u201d\nConsequently, Sydney and Taylen darkened Addison\u2019s door. \u201cHey, do you have a minute?\u201d\nAddison replied, \u201cAlways for you\u2014what\u2019s up?\u201d\nSysops Squad Saga: Common Infrastructure Logic \n| \n239", "domains": ["Design Principles", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 257", "position": 257, "chunk_type": "semantic", "token_estimate": 267}
{"text": "Sydney said, \u201cWe\u2019ve been consolidating a bunch of our duplicated code into shared libraries, and: that\u2019s working well\u2014we\u2019re getting better at identifying the parts that rarely change. But, now we\u2019ve\nhit the problem that brings us here\u2014who is supposed to be writing log messages? Libraries, serv-\nices, or something else? And, how can we make that consistent?\u201d\nAddison said, \u201cWe\u2019ve bumped into operational shared behavior. Logging is just one of them. What\nabout monitoring, service discovery, circuit breakers, even some of our utility functions, like the JSON\ntoXML library that a few teams are sharing? We need a better way to handle this to prevent issues. That\u2019s why we\u2019re in the process of implementing a service mesh with this common behavior in a\nsidecar component.\u201d\nSydney said, \u201cI\u2019ve read about sidecars and service mesh\u2014it\u2019s a way to share things across a bunch of\nmicroservices, right?\u201d\nAddison said, \u201cSort of, but not all kinds of things. The intent of the service mesh and sidecar is to\nconsolidate operational coupling, not domain coupling. For example, just like in our case, we want\nconsistency for logging and monitoring across all our services, but don\u2019t want each team to have to\nworry about that. If we consolidate logging code into the common sidecar that every service imple-\nments, we can enforce consistency.\u201d\nTaylen asked, \u201cWho owns the shared library? Shared responsibility across all the teams?\u201d\nAddison replied, \u201cWe thought about that, but we have enough teams now; we\u2019ve built a shared\ninfrastructure team that is going to manage and maintain the sidecar component. They have built\nthe deployment pipeline to automatically test the sidecar once it\u2019s been bound into the service with\na set of fitness functions.\u201d\nSydney said, \u201cSo if we need to share libraries between services, just ask them to put it in the sidecar?\u201d\nAddison said, \u201cBe careful\u2014the sidecar isn\u2019t meant to be used for just anything, only operational\ncoupling.\u201d\n\u201cI\u2019m not sure what that distinction is,\u201d Taylen said. \u201cOperational coupling includes the things we\u2019ve been discussing\u2014logging, monitoring, service dis-\ncovery, authentication and authorization, and so on. Basically, it covers all the plumbing parts of the\ninfrastructure that have no domain responsibility. But you should never put domain shared compo-\nnents, like the Address or Customer class, in the sidecar.\u201d\nSydney asked, \u201cBut why? What if I need the same class definition in two services?", "domains": ["Architectural Patterns and Styles", "Design Principles", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 258", "position": 258, "chunk_type": "semantic", "token_estimate": 394}
{"text": "Sydney said, \u201cWe\u2019ve been consolidating a bunch of our duplicated code into shared libraries, and: But you should never put domain shared compo-\nnents, like the Address or Customer class, in the sidecar.\u201d\nSydney asked, \u201cBut why? What if I need the same class definition in two services? Won\u2019t putting it in\nthe sidecar make it available to both?\u201d\nAddison replied, \u201cYes, but now you are increasing coupling in exactly the way we try to avoid in\nmicroservices. In most architectures, a single implementation of that service would be shared across\nthe teams that need it. However, in microservices, that creates a coupling point, tying several serv-\nices together in an undesirable way\u2014if one team changes the shared code, every team must coor-\ndinate with that change. However, the architects could decide to put the shared library in the\n240 \n| \nChapter 8: Reuse Patterns", "domains": ["Architectural Patterns and Styles", "Design Principles"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 258", "position": 258, "chunk_type": "semantic", "token_estimate": 145}
{"text": "sidecar\u2014it is, after all a technical capability. Neither answer is unambiguously correct, making this an: architect decision and worthy of trade-off analysis. For example, if the Address class changes and\nboth services rely on it, they must both change\u2014the definition of coupling. We handle those issues\nwith contracts. The other issue concerns size: we don\u2019t want the sidecar to become the biggest part\nof the architecture. For example, consider the JSONtoXML library we were discussing before. How\nmany teams use that?\u201d\nTaylen said, \u201cWell, any team that has to integrate with the mainframe system for anything\u2014probably\n5 out of, what, 16 or 17 teams?\u201d\nAddison said, \u201cPerfect. OK, what\u2019s the trade-off of putting the JSONtoXML in the sidecar?\u201d\nSydney answered, \u201cWell, that means that every team automatically has the library and doesn\u2019t have\nto wire it in through dependencies.\u201d\n\u201cAnd the bad side?\u201d asked Addison. \u201cWell, adding it to the sidecar makes it bigger, but not by much\u2014it\u2019s a small library.\u201d said Sydney. \u201cThat\u2019s the key trade-off for shared utility code\u2014how many teams need it versus how much over-\nhead does it add to every service, particularly ones that don\u2019t need it.\u201d\n\u201cAnd if less than one-half the teams use it, it\u2019s probably not worth the overhead,\u201d Sydney said. \u201cRight! So, for now, we\u2019ll leave that out of the sidecar and perhaps reassess in the future,\u201d said\nAddison. ADR: Using a Sidecar for Operational Coupling\nContext\nEach service in our microservices architecture requires common and consistent opera-\ntional behavior; leaving that responsibility to each team introduces inconsistencies and\ncoordination issues. Decision\nWe will use a sidecar component in conjunction with a service mesh to consolidate\nshared operational coupling. The shared infrastructure team will own and maintain the sidecar for service teams; service\nteams act as their customers. The following services will be provided by the sidecar:\n\u2022 Monitoring\n\u2022 Logging\n\u2022 Service discovery\n\u2022 Authentication\n\u2022 Authorization\nSysops Squad Saga: Common Infrastructure Logic \n| \n241", "domains": ["Architectural Patterns and Styles", "Design Principles", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 259", "position": 259, "chunk_type": "semantic", "token_estimate": 325}
{"text": "Consequences: Teams should not add domain classes to the sidecar, which encourages inappropriate\ncoupling. Teams work with the shared infrastructure team to place shared, operational libraries in the\nsidecar if enough teams require it. Code Reuse: When Does It Add Value? Many architects fail to properly assess trade-offs when they encounter some situa\u2010\ntions, which isn\u2019t necessarily a deficiency\u2014many trade-offs become obvious only\nafter the fact. Reuse is one of the most abused abstractions, because the general view in organiza\u2010\ntions is that reuse represents a laudable goal that teams should strive for. However,\nfailing to evaluate all the trade-offs associated with reuse can lead to serious problems\nwithin architecture. The danger of too much reuse was one of the lessons many architects learned from\nthe early 20th century trend of orchestration-driven service-oriented architecture,\nwhere one of the primary goals for many organizations was to maximize reuse. Consider the scenario from an insurance company, illustrated in Figure 8-16. Figure 8-16. Each domain within a large insurance company has a view of the customer\n242 \n| \nChapter 8: Reuse Patterns", "domains": ["Architectural Patterns and Styles"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 260", "position": 260, "chunk_type": "semantic", "token_estimate": 179}
{"text": "Each division in the company has some aspect of customers it cares about. Years ago,: architects were instructed to keep an eye out for this type of commonality; once dis\u2010\ncovered, the goal was to consolidate the organizational view of customer into a single\nservice, shown in Figure 8-17. Figure 8-17. Unifying on a centralized Customer service\nWhile the picture in Figure 8-17 may seem logical, it\u2019s an architectural disaster for\ntwo reasons. First, if all institutional information about a key entity like Customer\nmust reside in a single place, that entity must be complex enough to handle any\ndomain and scenario, making it difficult to use for simple things. Secondly, though, it creates brittleness within the architecture. If every domain that\nneeds customer information must get it from a single place, when that place changes,\neverything breaks. For example, in our example, what happens when CustomerSer\nvice needs to add new capabilities on behalf of one of the domains? That change\ncould potentially impact every other domain, requiring coordination and testing to\nensure that the change hasn\u2019t \u201crippled\u201d throughout the architecture. What architects failed to realize is that reuse has two important aspects; they got the\nfirst one correct: abstraction. The way architects and developers discover candidates\nfor reuse is via abstraction. However, the second consideration is the one that deter\u2010\nmines utility and value: rate of change. Observing that some reuse causes brittleness begs the question about how that kind\nof reuse differs from the kinds we clearly benefit from. Consider things that everyone\nsuccessfully reuses: operating systems, open source frameworks and libraries, and so\nCode Reuse: When Does It Add Value? | \n243", "domains": ["Design Principles", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 261", "position": 261, "chunk_type": "semantic", "token_estimate": 276}
{"text": "on. What distinguishes those from assets that project teams build? The answer is slow: rate of change. We benefit from technical coupling, like operating systems and exter\u2010\nnal frameworks, because they have a well-understood rate of change and update\ncadence. Internal domain capabilities or quick-changing technical frameworks make\nterrible coupling targets. Reuse is derived via abstraction but operationalized by slow rate of\nchange. Reuse via Platforms\nMuch press exists extolling the virtue of platforms within organizations, almost to the\npoint of semantic diffusion. However, most agree that the platform is the new target of\nreuse within organizations, meaning that for each distinguishable domain capability,\nthe organization builds a platform with a well-defined API to hide the implementa\u2010\ntion details. Slow rate of change drives this reasoning. As we discuss in Chapter 13, an API can be\ndesigned to be quite loosely coupled to callers, allowing for an aggressive internal rate\nof change of implementation details without breaking the API. This, of course,\ndoesn\u2019t protect the organization from changes to the semantics of the information it\nmust pass between domains, but by careful design of encapsulation and contracts,\narchitects can limit the amount of breaking change and brittleness in integration\narchitecture. Sysops Squad Saga: Shared Domain Functionality\nTuesday, February 8, 12:50\nWith Addison\u2019s approval, the development team had decided to split the core ticketing\nfunctionality into three separate services: a customer-facing Ticket Creation service, a\nTicket Assignment service, and a Ticket Completion service. However, all three services\nused common database logic (queries and updates) and shared a set of database tables\nin the ticketing data domain. Taylen wanted to create a shared data service that would contain the common database\nlogic, thus forming a database abstraction layer, as shown in Figure 8-18. 244 \n| \nChapter 8: Reuse Patterns", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 262", "position": 262, "chunk_type": "semantic", "token_estimate": 297}
{"text": "\u201cFirst of all,\u201d said Skyler, \u201call three services would need to make an interservice call to the shared data: service for every database query or update. We\u2019re going take a serious performance hit if we do that. Furthermore, if the shared data service goes down, all three of those services become\nnonoperational.\u201d\n\u201cSo?\u201d said Taylen. \u201cIt\u2019s all backend functionality, so who cares? The backend functionality doesn\u2019t\nhave to be that fast, and services come up fairly quickly if they fail.\u201d\n\u201cActually,\u201d said Addison, \u201cit\u2019s not all backend functionality. Don\u2019t forget, the Ticket Creation service is\ncustomer facing, and it would be using the same shared data service as the backend ticketing\nfunctionality.\u201d\n\u201cYeah, but most of the functionality is still backend,\u201d said Taylen, with a little less confidence than\nbefore. \u201cSo far,\u201d said Addison, \u201cit looks like the trade-off for using the shared data service is performance and\nfault tolerance for the ticketing services.\u201d\n\u201cLet\u2019s also not forget that any changes made to the shared data service are runtime changes. In\nother words,\u201d said Skyler, \u201cif we make a change and deploy the shared data service, we could possi-\nbly break something.\u201d\n\u201cThat\u2019s why we test,\u201d said Taylen. \u201cYeah, but if you want to reduce risk you would have to test all of the ticketing services for every\nchange to the shared data service, which increases testing time significantly. With a shared DLL, we\ncould version the shared library to provide backward compatibility,\u201d said Skyler. \u201cOK, we will add increased risk for changes and increased testing effort to the trade-offs as well,\u201d said\nAddison. \u201cAlso, let\u2019s not forget that we would have extra coordination from a scalability standpoint. Every time we create more instances of the ticket creation service, we would have to make sure we\ncreate more instances of the shared data service as well.\u201d\n\u201cLet\u2019s not keep focusing so much on the negatives.\u201d said Taylen. \u201cHow about the positives of using a\nshared data service?\u201d\n\u201cOK,\u201d said Addison, \u201clet\u2019s talk about the benefits of using a shared data service.\u201d\n\u201cData abstraction, of course,\u201d said Taylen. \u201cThe services wouldn\u2019t have to worry about any database\nlogic. All they would have to do is make a remote service call to the shared data service.\u201d\n\u201cAny other benefits?\u201d asked Addison. \u201cWell,\u201d said Taylen, \u201cI was going to say centralized connection pooling, but we would need multiple\ninstances anyway to support the customer ticket creation service.", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 264", "position": 264, "chunk_type": "semantic", "token_estimate": 404}
{"text": "\u201cLet\u2019s take a look at those shared class files in the repository and see historically how much change: there really is for that code,\u201d said Addison. Addison, Taylen, and Skyler all looked at the repository history for the shared data logic class files. \u201cHmm\u2026\u201d said Taylen, \u201cI thought there were a lot more changes to that code than what is showing\nup in the repo. OK, so I guess the changes are fairly minimal for the shared database logic after all.\u201d\nThrough the conversation of discussing trade-offs, Taylen started to realize that the negatives of a\nshared service seemed to outweigh the positives, and there was no real compelling justification for\nputting the shared database logic in a shared service. Taylen agreed to put the shared database logic\nin a shared DLL, and Addison wrote an ADR for this architecture decision:\nADR: Use of a Shared Library for Common Ticketing Database Logic\nContext\nThe ticketing functionality is broken into three services: Ticket Creation, Ticket Assignment,\nand Ticket Completion. All three services use common code for the bulk of the database\nqueries and update statements. The two options are to use a shared library or create a\nshared data service. Decision\nWe will use a shared library for the common ticketing database logic. Using a shared library will improve performance, scalability, and fault tolerance of the\ncustomer-facing Ticket Creation service, as well as for the Ticket Assignment service. We found that the common database logic code does not change much and is therefore\nfairly stable code. Furthermore, change is less risky for the common database logic\nbecause services would need to be tested and redeployed. If changes are needed, we will\napply versioning where appropriate so that not all services need to be redeployed when\nthe common database logic changes. Using a shared library reduces service coupling and eliminates additional service depen-\ndencies, HTTP traffic, and overall bandwidth. Consequences\nChanges to the common database logic in the shared DLL will require the ticketing serv-\nices to be tested and deployed, therefore reducing overall agility for common database\nlogic for the ticketing functionality. Service instances will need to manage their own database connection pool. Sysops Squad Saga: Shared Domain Functionality \n| \n247", "domains": ["Design Patterns", "Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 265", "position": 265, "chunk_type": "semantic", "token_estimate": 370}
{"text": "CHAPTER 9: Data Ownership and\nDistributed Transactions\nFriday, December 10 09:12\nWhile the database team worked on decomposing the monolithic Sysops Squad data-\nbase, the Sysops Squad development team, along with Addison, the Sysops Squad archi-\ntect, started to work on forming bounded contexts between the services and the data,\nassigning table ownership to services in the process. \u201cWhy did you add the expert profile table to the bounded context of the Ticket Assign-\nment service?\u201d asked Addison. \u201cBecause,\u201d said Sydney, \u201cthe ticket assignment relies on that table for the assignment algorithms. It\nconstantly queries that table to get the expert\u2019s location and skills information.\u201d\n\u201cBut it only does queries to the expert table,\u201d said Addison. \u201cThe User Maintenance service contains\nthe functionality to perform database updates to maintain that information. Therefore, it seems to\nme the expert profile table should be owned by the User Maintenance service and put within that\nbounded context.\u201d\n\u201cI disagree,\u201d said Sydney. \u201cWe simply cannot afford for the assignment service to make remote calls\nto the User Maintenance service for every query it needs. It simply won\u2019t work.\u201d\n\u201cIn that case, how to you see updates occurring to the table when an expert acquires a new skill or\nchanges their service location? And what about when we hire a new expert?\u201d asked Addison. \u201cHow\nwould that work?\u201d\n\u201cSimple,\u201d said Sydney. \u201cThe User Maintenance service can still access the expert table. All it would\nneed to do is connect to a different database. What\u2019s the big deal about that?\u201d\n249", "domains": ["Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 267", "position": 267, "chunk_type": "semantic", "token_estimate": 255}
{"text": "Figure 9-1. Once data is broken apart, tables must be assigned to services that own them: To further complicate matters, notice that the Wishlist Service writes to both the\nAudit table and the Wishlist table, the Catalog Service writes to the Audit table and\nthe Product table, and the Inventory Service writes to the Audit table and the Product\ntable. Suddenly, this simple real-world example makes assigning data ownership a\ncomplex and confusing task. In this chapter, we unravel this complexity by discussing the three scenarios encoun\u2010\ntered when assigning data ownership to services (single ownership, common owner\u2010\nship, and joint ownership), and exploring techniques for resolving these scenarios,\nusing Figure 9-1 as a common reference point. Single Ownership Scenario\nSingle table ownership occurs when only one service writes to a table. This is the most\nstraightforward of the data ownership scenarios and is relatively easy to resolve. Referring back to Figure 9-1, notice that the Wishlist table has only a single service\nthat writes to it\u2014the Wishlist Service. In this scenario, it is clear that the Wishlist Service should be the owner of the Wish\u2010\nlist table (regardless of other services that need read-only access to the Wishlist table),\nsee Figure 9-2. Notice that on the right side of this diagram, the Wishlist table\nbecomes part of the bounded context of the Wishlist Service. This diagramming tech\u2010\nnique is an effective way to indicate table ownership and the bounded context formed\nbetween the service and its corresponding data. Single Ownership Scenario \n| \n251", "domains": ["Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 269", "position": 269, "chunk_type": "semantic", "token_estimate": 254}
{"text": "Figure 9-2. With single ownership, the service that writes to the table becomes the table: owner\nBecause of the simplicity of this scenario, we recommend addressing single table\nownership relationships first to clear the playing field in order to better address the\nmore complicated scenarios that arise: common ownership and joint ownership. Common Ownership Scenario\nCommon table ownership occurs when most (or all) of the services need to write to\nthe same table. For example, Figure 9-1 shows that all services (Wishlist, Catalog, and\nInventory) need to write to the Audit table to record the action performed by the\nuser. Since all services need to write to the table, it\u2019s difficult to determine who should\nactually own the Audit table. While this simple example includes only three services,\nimagine a more realistic example where potentially hundreds (or even thousands) of\nservices must write to the same Audit table. The solution of simply putting the Audit table in a shared database or shared schema\nthat is used by all services unfortunately reintroduces all of the data-sharing issues\ndescribed at the beginning of Chapter 6, including change control, connection starva\u2010\ntion, scalability, and fault tolerance. Therefore, another solution is needed to solve\ncommon data ownership. A popular technique for addressing common table ownership is to assign a dedicated\nsingle service as the primary (and only) owner of that data, meaning only one service\nis responsible for writing data to the table. Other services needing to perform write\nactions would send information to the dedicated service, which would then perform\nthe actual write operation on the table. 252 \n| \nChapter 9: Data Ownership and Distributed Transactions", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 270", "position": 270, "chunk_type": "semantic", "token_estimate": 273}
{"text": "Example 9-1. DDL source code for splitting up the Product table and moving inventory: counts to a new Inventory table\nCREATE TABLE Inventory\n(\nproduct_id VARCHAR(10),\ninv_cnt INT\n);\nINSERT INTO Inventory VALUES (product_id, inv_cnt)\nAS SELECT product_id, inv_cnt FROM Product;\nCOMMIT;\nALTER TABLE Product DROP COLUMN inv_cnt;\nSplitting the database table moves the joint ownership to a single table ownership sce\u2010\nnario: the Catalog Service owns the data in the Product table, and the Inventory Ser\u2010\nvice owns the data in the Inventory table. However, as shown in Figure 9-5, this\ntechnique requires communication between the Catalog Service and Inventory Ser\u2010\nvice when products are created or removed to ensure the data remains consistent\nbetween the two tables. Figure 9-5. Joint ownership can be addressed by breaking apart the shared table\nFor example, if a new product is added, the Catalog Service generates a product ID\nand inserts the new product into the Product table. The Catalog Service then must\nsend that new product ID (and potentially the initial inventory counts) to the Inven\u2010\ntory Service. If a product is removed, the Catalog Service first removes the product\nfrom the Product table, then must notify the Inventory Service to remove the inven\u2010\ntory row from the Inventory table. Synchronizing data between split tables is not a trivial matter. Should communication\nbetween the Catalog Service and the Inventory Service be synchronous or asynchro\u2010\nnous? What should the Catalog Service do when adding or removing a product and\nfinding that the Inventory Service is not available? These are hard questions to\nanswer, and are usually driven by the traditional availability verses consistency\nJoint Ownership Scenario \n| \n255", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 273", "position": 273, "chunk_type": "semantic", "token_estimate": 276}
{"text": "trade-off commonly found in distributed architectures. Choosing availability means: that it\u2019s more important that the Catalog Service always be able to add or remove\nproducts, even though a corresponding inventory record may not be created in the\nInventory table. Choosing consistency means that it\u2019s more important that the two\ntables always remain in sync with each other, which would cause a product creation\nor removal operation to fail if the Inventory Service is not available. Because network\npartitioning is necessary in distributed architectures, the CAP theorem states that\nonly one of these choices (consistency or availability) is possible. The type of communication protocol (synchronous versus asynchronous) also mat\u2010\nters when splitting a table. Does the Catalog Service require a confirmation that the\ncorresponding Inventory record is added when creating a new product? If so, then\nsynchronous communication is required, providing better data consistency at the\nsacrifice of performance. If no confirmation is required, the Catalog Service can use\nasynchronous fire-and-forget communication, providing better performance at the\nsacrifice of data consistency. So many trade-offs to consider! Table 9-1 summarizes the trade-offs associated with the table split technique for joint\nownership. Trade-Offs\nTable 9-1. Joint ownership table split technique trade-offs\nAdvantages\nDisadvantages\nPreserves bounded context\nTables must be altered and restructured\nSingle data ownership\nPossible data consistency issues\nNo ACID transaction between table updates\nData synchronization is difficult\nData replication between tables may occur\nData Domain Technique\nAnother technique for joint ownership is to create a shared data domain. This is\nformed when data ownership is shared between the services, thus creating multiple\nowners for the table. With this technique, the tables shared by the same services are\nput into the same schema or database, therefore forming a broader bounded context\nbetween the services and the data. 256 \n| \nChapter 9: Data Ownership and Distributed Transactions", "domains": ["Design Patterns", "Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 274", "position": 274, "chunk_type": "semantic", "token_estimate": 304}
{"text": "Notice that Figure 9-6 looks close to the original diagram in Figure 9-4 with one: noticeable difference\u2014the data domain diagram has the Product table in a separate\nbox outside the context of each owning service. This diagramming technique makes\nit clear that the table is not owned by or part of the bounded context of either service,\nbut rather shared between them in a broader bounded context. Figure 9-6. With joint ownership, services can share data by using the data domain tech\u2010\nnique (shared schema)\nWhile data sharing is generally discouraged in distributed architectures (particularly\nwith microservices), it does resolve some of the performance, availability, and data\nconsistency issues found in other joint ownership techniques. Because the services\nare not dependent on each other, the Catalog Service can create or remove products\nwithout needing to coordinate with the Inventory Service, and the Inventory Service\ncan adjust inventory without needing the Catalog Service. Both services become\ncompletely independent from each other. When choosing the data domain technique, always reevaluate why\nseparate services are needed since the data is common to each of\nthe services. Justifications might include scalability differences,\nfault-tolerance needs, throughput differences, or isolating code vol\u2010\natility (see Chapter 7). Unfortunately, sharing data in a distributed architecture introduces a number of\nissues, the first of these being increased effort for changes made to the structure of the\ndata (such as changing the schema of a table). Because a broader bounded context is\nformed between the services and the data, changes to the shared table structures may\nrequire those changes to be coordinated among multiple services. This increases\ndevelopment effort, testing scope, and deployment risk. Joint Ownership Scenario \n| \n257", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 275", "position": 275, "chunk_type": "semantic", "token_estimate": 279}
{"text": "Another issue with the data domain technique with regard to data ownership is con\u2010: trolling which services have write responsibility to what data. In some cases, this\nmight not matter, but if it\u2019s important to control write operations to certain data,\nadditional effort is required to apply specific governance rules to maintain specific\ntable or column write ownership. Table 9-2 summarizes the trade-offs associated with the data domain technique for\nthe joint ownership scenario. Trade-Offs\nTable 9-2. Joint ownership data-domain technique trade-offs\nAdvantages\nDisadvantages\nGood data access performance\nData schema changes involve more services\nNo scalability and throughput issues\nIncreased testing scope for data schema changes\nData remains consistent\nData ownership governance (write responsibility)\nNo service dependency\nIncreased deployment risk for data schema changes\nDelegate Technique\nAn alternative method for addressing the joint ownership scenario is the delegate\ntechnique. With this technique, one service is assigned single ownership of the table\nand becomes the delegate, and the other service (or services) communicates with the\ndelegate to perform updates on its behalf. One of the challenges of the delegate technique is knowing which service to assign as\nthe delegate (the sole owner of the table). The first option, called primary domain pri\u2010\nority, assigns table ownership to the service that most closely represents the primary\ndomain of the data\u2014in other words, the service that does most of the primary entity\nCRUD operations for the particular entity within that domain. The second option,\ncalled operational characteristics priority, assigns table ownership to the service need\u2010\ning higher operational architecture characteristics, such as performance, scalability,\navailability, and throughput. To illustrate these two options and the corresponding trade-offs associated with each,\nconsider the Catalog Service and Inventory Service joint ownership scenario shown\nin Figure 9-4. In this example, the Catalog Service is responsible for creating, updat\u2010\ning, and removing products, as well as retrieving product information; the Inventory\nService is responsible for retrieving and updating product inventory count as well as\nfor knowing when to restock if inventory gets too low. 258 \n| \nChapter 9: Data Ownership and Distributed Transactions", "domains": ["Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 276", "position": 276, "chunk_type": "semantic", "token_estimate": 345}
{"text": "With the primary domain priority option, the service that performs most of the: CRUD operations on the main entity becomes the owner of the table. As illustrated in\nFigure 9-7, since the Catalog Service performs most of the CRUD operations on\nproduct information, the Catalog Service would be assigned as the single owner of\nthe table. This means that the Inventory service must communicate with the Catalog\nService to retrieve or update inventory counts since it doesn\u2019t own the table. Figure 9-7. Table ownership is assigned to the Catalog service because of domain priority\nLike the common ownership scenario described earlier, the delegate technique always\nforces interservice communication between the other services needing to update the\ndata. Notice in Figure 9-7 that the Inventory Service must send inventory updates\nthrough some sort of remote access protocol to the Catalog Service so that it can per\u2010\nform the inventory updates and reads on behalf of the Inventory Service. This com\u2010\nmunication can either be synchronous or asynchronous. As always in software\narchitecture, more trade-off analysis to consider. With synchronous communication, the Inventory Service must wait for the inventory\nto be updated by the Catalog Service, which impacts overall performance but ensures\ndata consistency. Using asynchronous communication to send inventory updates\nmakes the Inventory Service perform much faster, but the data is only eventually con\u2010\nsistent. Furthermore, with asynchronous communication, because an error can occur\nin the Catalog Service while trying to update inventory, the Inventory Service has no\nguarantee that the inventory was ever updated, impacting data integrity as well. With the operational characteristics priority option, the ownership roles would be\nreversed because inventory updates occur at a much faster rate than static product\ndata. In this case, table ownership would be assigned to the Inventory Service, the jus\u2010\ntification being that updating product inventory is a part of the frequent real-time\ntransactional processing of purchasing products as opposed to the more infrequent\nadministrative task of updating product information or adding and removing prod\u2010\nucts (see Figure 9-8). Joint Ownership Scenario \n| \n259", "domains": ["Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 277", "position": 277, "chunk_type": "semantic", "token_estimate": 342}
{"text": "With this option, frequent updates to inventory counts can use direct database calls: rather than remote access protocols, therefore making inventory operations much\nfaster and more reliable. In addition, the most volatile data (inventory count) is kept\nhighly consistent. Figure 9-8. Table ownership is assigned to the Inventory Service because of operational\ncharacteristics priority\nHowever, one major problem with the diagram illustrated in Figure 9-8 is that of\ndomain management responsibility. The Inventory Service is responsible for manag\u2010\ning product inventory, not the database activity (and corresponding error handling)\nfor adding, removing, and updating static product information. For this reason, we\nusually recommend the domain priority option, and leveraging things like a replica\u2010\nted in-memory cache or a distributed cache to help address performance and fault-\ntolerance issues. Regardless of which service is assigned as the delegate (sole table owner), the delegate\ntechnique has some disadvantages, the biggest being service coupling and the need\nfor interservice communication. This in turn leads to other issues for nondelegate\nservices, including the lack of an atomic transaction when performing write opera\u2010\ntions, low performance due to network and processing latency, and low fault toler\u2010\nance. Because of these issues, the delegate technique is generally better suited for\ndatabase write scenarios that do not require atomic transactions and that can tolerate\neventual consistency through asynchronous communications. 260 \n| \nChapter 9: Data Ownership and Distributed Transactions", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 278", "position": 278, "chunk_type": "semantic", "token_estimate": 231}
{"text": "Table 9-3 summarizes the overall trade-offs of the delegate technique.: Trade-Offs\nTable 9-3. Joint ownership delegate technique trade-offs\nAdvantages\nDisadvantages\nForms single table ownership\nHigh level of service coupling\nGood data schema change control\nLow performance for nonowner writes\nAbstracts data structures from other services\nNo atomic transaction for nonowner writes\nLow fault tolerance for nonowner services\nService Consolidation Technique\nThe delegate approach discussed in the prior section highlights the primary issue\nassociated with joint ownership\u2014service dependency. The service consolidation tech\u2010\nnique resolves service dependency and addresses joint ownership by combining mul\u2010\ntiple table owners (services) into a single consolidated service, thus moving joint\nownership into a single ownership scenario (see Figure 9-9). Figure 9-9. Table ownership is resolved by combining services\nLike the data domain technique, this technique resolves issues associated with service\ndependencies and performance, while at the same time addressing the joint owner\u2010\nship problem. However, like the other techniques, it has its share of trade-offs as well. Combining services creates a more coarse-grained service, thereby increasing the\noverall testing scope as well as overall deployment risk (the chance of breaking some\u2010\nthing else in the service when a new feature is added or a bug is fixed). Consolidating\nservices might also impact overall fault tolerance since all parts of the service fail\ntogether. Service Consolidation Technique \n| \n261", "domains": ["Design Principles", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 279", "position": 279, "chunk_type": "semantic", "token_estimate": 222}
{"text": "Overall scalability is also impacted when using the service consolidation technique: because all parts of the service must scale equally, even though some functionality\nmight not need to scale at the same level as other functionality. For example, in Figure\n9-9, the catalog maintenance functionality (what used to be in a separate Catalog ser\u2010\nvice) must unnecessarily scale to meet the high demands of the inventory retrieval\nand update functionality. Table 9-4 summarizes the overall trade-offs of the service consolidation technique. Trade-Offs\nTable 9-4. Joint ownership service consolidation technique trade-offs\nAdvantages\nDisadvantages\nPreserves atomic transactions\nMore coarse-grained scalability\nGood overall performance\nLess fault tolerance\nIncreased deployment risk\nIncreased testing scope\nData Ownership Summary\nFigure 9-10 shows the resulting table ownership assignments from Figure 9-1 after\napplying the techniques described in this section. For the single table scenario involv\u2010\ning the Wishlist Service, we simply assigned ownership to the Wishlist Service, form\u2010\ning a tight bounded context between the service and the table. For the common\nownership scenario involving the audit table, we created a new Audit Service, with all\nother services sending an asynchronous message to a persisted queue. Finally, for the\nmore complex joint ownership scenario involving the product table with the Catalog\nService and Inventory Service, we chose to use the delegate technique, assigning sin\u2010\ngle ownership of the product table to the Catalog Service, with the Inventory Service\nsending update requests to the Catalog Service. Once table ownership has been assigned to services, an architect must then validate\nthe table ownership assignments by analyzing business workflows and their corre\u2010\nsponding transaction requirements. 262 \n| \nChapter 9: Data Ownership and Distributed Transactions", "domains": ["Design Principles", "Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 280", "position": 280, "chunk_type": "semantic", "token_estimate": 274}
{"text": "Atomicity means a transaction must either commit or roll back all of its updates in a: single unit of work, regardless of the number of updates made during that transac\u2010\ntion. In other words, all updates are treated as a collective whole, so all changes either\nget committed or get rolled back as one unit. For example, assume registering a cus\u2010\ntomer involves inserting customer profile information into a Customer Profile table,\ninserting credit card information into a Wallet table, and inserting security-related\ninformation into a Security table. Suppose the profile and credit card information are\nsuccessfully inserted, but the security information insert fails. With atomicity, the\nprofile and credit card inserts would be rolled back, keeping the database tables in\nsync. Consistency means that during the course of a transaction, the database would never\nbe left in an inconsistent state or violate any of the integrity constraints specified in\nthe database. For example, during an ACID transaction, the system cannot add a\ndetail record (such as an item) without first adding the corresponding summary\nrecord (such as an order). Although some databases defer this check until commit\ntime, in general programmers cannot violate consistency constraints such as a\nforeign-key constraint during the course of a transaction. Isolation refers to the degree to which individual transactions interact with each\nother. Isolation protects uncommitted transaction data from being visible to other\ntransactions during the course of the business request. For example, during the\ncourse of an ACID transaction, when the customer profile information is inserted\ninto the Customer Profile table, no other services outside of the ACID transaction\nscope can access the newly inserted information until the entire transaction is\ncommitted. Durability means that once a successful response from a transaction commit occurs,\nit is guaranteed that all data updates are permanent, regardless of further system\nfailures. To illustrate an ACID transaction, suppose a customer registering for the Sysops\nSquad application enters all of their profile information, the electronic products they\nwant covered under the support plan, and their billing information on a single user\ninterface screen. This information is then sent to the single Customer Service, as\nshown in Figure 9-11, which then performs all of the database activity associated with\nthe customer registration business request. 264 \n| \nChapter 9: Data Ownership and Distributed Transactions", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 282", "position": 282, "chunk_type": "semantic", "token_estimate": 385}
{"text": "Instead of ACID, distributed transactions support something called BASE. In chemis\u2010: try, an acid substance and a base substance are exactly the opposite. The same is true\nwith atomic and distributed transactions\u2014ACID transactions are opposite of BASE\ntransactions. BASE describes the properties of a distributed transaction: basic availa\u2010\nbility, soft state, and eventual consistency. Basic availability (the \u201cBA\u201d part of BASE) means that all of the services or systems in\nthe distributed transaction are expected to be available to participate in the dis\u2010\ntributed transaction. While asynchronous communication can help decouple services\nand address availability issues associated with the distributed transaction participants,\nit unfortunately impacts how long it will take the data to become consistent for the\natomic business transaction (see eventual consistency later in this section). Soft state (the S part of BASE) describes the situation where a distributed transaction\nis in progress and the state of the atomic business request is not yet complete (or in\nsome cases not even known). In the customer registration example shown in Figure\n9-12, soft state occurs when the customer profile information is inserted (and com\u2010\nmitted) in the Profile table, but the support contract and billing information are not. The unknown part of soft state can occur if, using the same example, all three services\nwork in parallel to insert their corresponding data\u2014the exact state of the atomic busi\u2010\nness request is not known at any point in time until all three services report back that\nthe data has been successfully processed. In the case of a workflow using asynchro\u2010\nnous communication (see Chapter 11), the in-progress or final state of the distributed\ntransaction is usually difficult to determine. Eventual consistency (the E part of BASE) means that given enough time, all parts of\nthe distributed transaction will complete successfully and all of the data is in sync\nwith one another. The type of eventual consistency pattern used and the way errors\nare handled dictates how long it will take for all of the data sources involved in the\ndistributed transaction to become consistent. The next section describes the three types of eventual consistency patterns and the\ncorresponding trade-offs associated with each pattern. Eventual Consistency Patterns\nDistributed architectures rely heavily on eventual consistency as a trade-off for better\noperational architecture characteristics such as performance, scalability, elasticity,\nfault tolerance, and availability.", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 285", "position": 285, "chunk_type": "semantic", "token_estimate": 389}
{"text": "Instead of ACID, distributed transactions support something called BASE. In chemis\u2010: The next section describes the three types of eventual consistency patterns and the\ncorresponding trade-offs associated with each pattern. Eventual Consistency Patterns\nDistributed architectures rely heavily on eventual consistency as a trade-off for better\noperational architecture characteristics such as performance, scalability, elasticity,\nfault tolerance, and availability. While there are numerous ways to achieve eventual\nconsistency between data sources and systems, the three main patterns in use today\nare the background synchronization pattern, orchestrated request-based pattern, and\nthe event-based pattern. To better describe each pattern and illustrate how they work, consider again the cus\u2010\ntomer registration process from the Sysops Squad application we discussed earlier in\nEventual Consistency Patterns \n| \n267", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 285", "position": 285, "chunk_type": "semantic", "token_estimate": 121}
{"text": "Figure 9-13. In this example, three separate services are involved in the customer reg\u2010: istration process: a Customer Profile Service that maintains basic profile information,\na Support Contract Service that maintains products covered under the Sysops Squad\nrepair plan for each customer, and a Billing Payment Service that charges the cus\u2010\ntomer for the support plan. Notice in the figure that customer 123 is a subscriber to\nthe Sysops Squad service, and therefore has data in each of the corresponding tables\nowned by each service. Figure 9-13. Customer 123 is a subscriber in the Sysops Squad application\nCustomer 123 decides they are no longer interested in the Sysops Squad support plan,\nso they unsubscribe from the service. As shown in Figure 9-14, the Customer Profile\nService receives this request from the user interface, removes the customer from the\nProfile table, and returns a confirmation to the customer that they are successfully\nunsubscribed and will no longer be billed. However, data for that customer still exists\nin the Contract table owned by the Support Contract Service and the Billing table\nowned by the Billing Payment Service. Figure 9-14. Data is out of sync after the customer unsubscribes from the support plan\n268 \n| \nChapter 9: Data Ownership and Distributed Transactions", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 286", "position": 286, "chunk_type": "semantic", "token_estimate": 209}
{"text": "Figure 9-15. The background synchronization pattern uses an external process to ensure: data consistency\nThe biggest disadvantage of the background synchronization pattern is that it couples\nall of the data sources together, thus breaking every bounded context between the\ndata and the services. Notice in Figure 9-16 that the background batch synchroniza\u2010\ntion process must have write access to each of the tables owned by the corresponding\nservices, meaning that all of the tables effectively have shared ownership between the\nservices and the background synchronization process. This shared data ownership between the services and the background synchroniza\u2010\ntion process is riddled with issues, and emphasizes the need for tight bounded con\u2010\ntexts within a distributed architecture. Structural changes made to the tables owned\nby each service (changing a column name, dropping a column, and so on) must also\nbe coordinated with an external background process, making changes difficult and\ntime-consuming. In addition to difficulties with change control, problems occur with regard to dupli\u2010\ncated business logic as. In looking at Figure 9-15, it might seem fairly straightforward\nthat the background process would simply perform a DELETE operation on all rows in\nthe Contract and Billing tables containing customer 123. However, certain business\nrules may exist within these services for the particular operation. 270 \n| \nChapter 9: Data Ownership and Distributed Transactions", "domains": ["Design Patterns", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 288", "position": 288, "chunk_type": "semantic", "token_estimate": 221}
{"text": "Figure 9-16. The background synchronization pattern is coupled to the data sources,: therefore breaking the bounded context and data ownership\nFor example, when a customer unsubscribes, their existing support contracts and bill\u2010\ning history are kept for three months in the event the customer decides to resubscribe\nto the support plan. Therefore, rather than deleting the rows in those tables, a\nremove_date column is set with a long value representing the date the rows should be\nremoved (a zero value in this column indicates an active customer). Both services\ncheck the remove_date daily to determine which rows should be removed from their\nrespective tables. The question is, where is that business logic located? The answer, of\ncourse, is in the Support Contract and Billing Payment Services\u2014oh, and also the\nbackground batch process! The background synchronization eventual consistency pattern is not suitable for dis\u2010\ntributed architectures requiring tight bounded contexts (such as microservices)\nwhere the coupling between data ownership and functionality is a critical part of the\narchitecture. Situations where this pattern is useful are closed (self-contained) hetero\u2010\ngeneous systems that don\u2019t communicate with each other or share data. For example, consider a contractor order entry system that accepts orders for build\u2010\ning materials, and another separate system (implemented in a different platform) that\ndoes contractor invoicing. Once a contractor orders supplies, a background synchro\u2010\nnization process moves those orders to the invoicing system to generate invoices. When a contractor changes an order or cancels it, the background synchronization\nprocess moves those changes to the invoicing system to update the invoices. This is a\ngood example of systems becoming eventually consistent, with the contractor order\nalways in sync between the two systems. Eventual Consistency Patterns \n| \n271", "domains": ["Architectural Patterns and Styles", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 289", "position": 289, "chunk_type": "semantic", "token_estimate": 286}
{"text": "Table 9-6 summarizes the trade-offs for the orchestrated request-based pattern for: eventual consistency. Trade-Offs\nTable 9-6. Orchestrated request-based pattern trade-offs\nAdvantages\nDisadvantages\nServices are decoupled\nSlower responsiveness\nTimeliness of data consistency\nComplex error handling\nAtomic business request\nUsually requires compensating transactions\nEvent-Based Pattern\nThe event-based pattern is one of the most popular and reliable eventual consistency\npatterns for most modern distributed architectures, including microservices and\nevent-driven architectures. With this pattern, events are used in conjunction with an\nasynchronous publish-and-subscribe (pub/sub) messaging model to post events\n(such as customer unsubscribed) or command messages (such as unsubscribe cus\ntomer) to a topic or event stream. Services involved in the distributed transaction lis\u2010\nten for certain events and respond to those events. The eventual consistency time is usually short for achieving data consistency because\nof the parallel and decoupled nature of the asynchronous message processing. Serv\u2010\nices are highly decoupled from one another with this pattern, and responsiveness is\ngood because the service triggering the eventual consistency event doesn\u2019t have to\nwait for the data synchronization to occur before returning information to the\ncustomer. Figure 9-20 illustrates how the event-based pattern for eventual consistency works. Notice that the customer issues the unsubscribe request to the Customer Profile Ser\u2010\nvice at 11:23:00. The Customer Profile Service receives the request, removes the cus\u2010\ntomer from the Profile table, publishes a message to a message topic or event stream,\nand returns information one second later letting the customer know they were suc\u2010\ncessfully unsubscribed. At around the same time this happens, both the Support Con\u2010\ntract and Billing Payment Services receive the unsubscribe event and perform\nwhatever functionality is needed to unsubscribe the customer, making all the data\nsources eventually consistent. Eventual Consistency Patterns \n| \n277", "domains": ["Architectural Patterns and Styles", "Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 295", "position": 295, "chunk_type": "semantic", "token_estimate": 290}
{"text": "ADR: Single Table Ownership for Bounded Contexts: Context\nWhen forming bounded contexts between services and data, tables must be assigned\nownership to a particular service or group of services. Decision\nWhen only one service writes to a table, that table will be assigned ownership to that ser-\nvice. Furthermore, services requiring read-only access to a table in another bounded con-\ntext cannot directly access the database or schema containing that table. Per the database team, table ownership is defined as the service that performs write oper-\nations on a table. Therefore, for single table ownership scenarios, regardless of how many\nother services need to access the table, only one service is ever assigned an owner, and\nthat owner is the service that maintains the data. Consequences\nDepending on the technique used, services requiring read-only access to a table in\nanother bounded context may incur performance and fault-tolerance issues when access-\ning data in a different bounded context. Now that Sydney and Addison better understood table ownership and how to form bounded con-\ntexts between the service and the data, they started to work on the survey functionality. The Ticket\nCompletion Service would write the timestamp the ticket was completed and the expert who per-\nformed the job to the survey table. The Survey Service would write the timestamp the survey was\nsent to the customer, and also insert all of the survey results once the survey is received. \u201cThis isn\u2019t so hard now that I better understand bounded contexts and table ownership,\u201d said\nSydney. \u201cOK, let\u2019s move on to the survey functionality,\u201d said Addison. \u201cOops,\u201d said Sydney. \u201cBoth the Ticket Completion Service and the Survey Service write to the Survey\ntable.\u201d\n\u201cThat\u2019s what Dana called joint-table ownership,\u201d said Addison. \u201cSo, what are our options?\u201d asked Sydney. \u201cSince splitting up the table won\u2019t work, it really leaves us with only two options,\u201d said Addison. \u201cWe\ncan use a common data domain so that both services own the data, or we can use the delegate\ntechnique and assign only one service as the owner.\u201d\n\u201cI like the common data domain. Let both services write to the table and share a common schema,\u201d\nsaid Sydney. \u201cExcept that won\u2019t work in this scenario,\u201d said Addison. \u201cThe Ticket Completion Service is already talk-\ning to the common ticketing data domain. Remember, a service can\u2019t connect to multiple schemas.\u201d\n280 \n| \nChapter 9: Data Ownership and Distributed Transactions", "domains": ["Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 298", "position": 298, "chunk_type": "semantic", "token_estimate": 402}
{"text": "\u201cThat\u2019s the same thing,\u201d said Taylen. \u201cI still have to wait for the information to come back, whether we: use messaging, REST, or any other remote access protocol. That table simply needs to be in the same\ndata domain as the ticketing tables.\u201d\n\u201cThere\u2019s got to be another solution to access data we no longer own,\u201d said Addison. \u201cLet me check\nwith Logan.\u201d\nIn most monolithic systems using a single database, developers don\u2019t give a second\nthought to reading database tables. SQL table joins are commonplace, and with a sim\u2010\nple query all necessary data can be retrieved in a single database call. However, when\ndata is broken into separate databases or schemas owned by distinct services, data\naccess for read operations starts to become hard. This chapter describes the various ways services can gain read access to data they\ndon\u2019t own\u2014in other words, outside the bounded context of the services needing the\ndata. The four patterns of data access we discuss in this chapter include the Inter-\nservice Communication pattern, Column Schema Replication pattern, Replicated\nCache pattern, and the Data Domain pattern. Each of these data access patterns has its share of advantages and disadvantages. Yes,\nonce again, trade-offs. To better describe each of these patterns, we will return to our\nWishlist Service and a Catalog Service example from Chapter 9. The Wishlist Service\nshown in Figure 10-1 maintains a list of items a customer may want to eventually\npurchase, and contains the customer ID, item ID, and date the item was added in the\ncorresponding Wishlist table. The Catalog Service is responsible for maintaining all\nof the items the company sells, and includes the item ID, item description, and static\nproduct dimension information, such as the weight, height, length, and so on. In this example, when a request is made from a customer to display in their wish list,\nboth the item ID and and the item description (item_desc) are returned to the cus\u2010\ntomer. However, the Wishlist Service does not have the item description in its table;\nthat data is owned by the Catalog Service in a tightly formed bounded context provid\u2010\ning change control and data ownership. Therefore, the architect must use one of the\ndata access patterns outlined in this chapter to ensure the Wishlist Service can obtain\nthe product descriptions from the Catalog Service. 284 \n| \nChapter 10: Distributed Data Access", "domains": ["Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 302", "position": 302, "chunk_type": "semantic", "token_estimate": 397}
{"text": "Figure 10-1. Wishlist Service needs item descriptions but doesn\u2019t have access to the: product table containing the data\nInterservice Communication Pattern\nThe Interservice Communication pattern is by far the most common pattern for\naccessing data in a distributed system. If one service (or system) needs to read data\nthat it cannot access directly, it simply asks the owning service or system for it by\nusing some sort of remote access protocol. What can be more simple? As with most things in software architecture, all is not as it seems. While simple, this\ncommon data access technique is unfortunately riddled with disadvantages. Consider\nFigure 10-2: the Wishlist Service makes a synchronous remote access call to the Cata\u2010\nlog Service, passing in a list of item IDs in exchange for a list of corresponding item\ndescriptions. Notice that for every request to get a customer wish list, the Wishlist Service must\nmake a remote call to the Catalog Service to get the item descriptions. The first issue\nthat occurs with this pattern is slower performance due to network latency, security\nlatency, and data latency. Network latency is the packet transmission time to and from\na service (usually somewhere between 30 ms and 300 ms). Security latency occurs\nwhen the endpoint to the target service requires additional authorization to perform\nthe request. Security latency can vary greatly depending on the level of security on the\nendpoint being accessed, but could be anywhere between 20 ms and 400 ms for most\nsystems. Data latency describes the situation where multiple database calls need to be\nmade to retrieve the necessary information to pass back to the end user. In this case,\nrather than a single SQL table join statement, an additional database call must be\nmade by the Catalog Service to retrieve the item description. This might add\nInterservice Communication Pattern \n| \n285", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 303", "position": 303, "chunk_type": "semantic", "token_estimate": 308}
{"text": "anywhere from 10 ms to 50 ms additional processing time. Add all of that up, and the: latency could be up to one second just to get the item descriptions. Figure 10-2. Interservice communication data access pattern\nAnother big disadvantage of this pattern is service coupling. Because the Wishlist\nmust rely on the Catalog Service being available, the services are therefore both\nsemantically and statically coupled, meaning that if the Catalog Service is not avail\u2010\nable, neither is the Wishlist Service. Furthermore, because of the tight static coupling\nbetween the Wishlist Service and the Catalog Service, as the Wishlist Service scales to\nmeet additional demand volume, so must the Catalog Service. Table 10-1 summarizes the trade-offs associated with the interservice communication\ndata access pattern. Trade-Offs\nTable 10-1. Trade-offs for the Interservice Communication data access pattern\nAdvantages\nDisadvantages\nSimplicity\nNetwork, data, and security latency (performance)\nNo data volume issues\nScalability and throughput issues\nNo fault tolerance (availability issues)\nRequires contracts between services\n286 \n| \nChapter 10: Distributed Data Access", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 304", "position": 304, "chunk_type": "semantic", "token_estimate": 168}
{"text": "Column Schema Replication Pattern: With the Column Schema Replication pattern, columns are replicated across tables,\ntherefore replicating the data and making it available to other bounded contexts. As\nshown in Figure 10-3, the item_desc column is added to the Wishlist table, making\nthat data available to the Wishlist Service without having to ask the Catalog Service\nfor the data. Figure 10-3. With the Column Schema Replication data access pattern, data is replicated\nto other tables\nData synchronization and data consistency are the two biggest issues associated with\nthe Column Schema Replication data access pattern. Whenever a product is created,\nremoved from the catalog, or a product description changed, the Catalog Service\nmust somehow let the Wishlist Service (and any other services replicating the data)\nknow about the change. This is usually done through asynchronous communications\nusing queues, topics, or event streaming. Unless immediate transactional synchroni\u2010\nzation is required, asynchronous communication is a preferred choice over synchro\u2010\nnous communication because it increases responsiveness and reduces the availability\ndependency between the services. Another challenge with this pattern is that it is sometimes difficult to govern data\nownership. Because the data is replicated in tables belonging to other services, those\nservices can update the data, even though they don\u2019t officially own the data. This in\nturn creates even more data consistency issues. Column Schema Replication Pattern \n| \n287", "domains": ["Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 305", "position": 305, "chunk_type": "semantic", "token_estimate": 225}
{"text": "Even though the services are still coupled because of data synchronization, the service: requiring read access has immediate access to the data, and can do simple SQL joins\nor queries to its own table to get the data. This increases performance, fault tolerance,\nand scalability, all things that were disadvantages with the interservice communica\u2010\ntion pattern. While in general we caution against use of this data access pattern for scenarios such\nas the Wishlist Service and Catalog Service example, some situations where it might\nbe a consideration are data aggregation, reporting, or situations where the other data\naccess patterns are not a good fit because of large data volumes, high responsiveness\nrequirements, or high-fault tolerance requirements. Table 10-2 summarizes the trade-offs associated with the Column Schema Replica\u2010\ntion data access pattern. Trade-Offs\nTable 10-2. Trade-offs for the Column Schema Replication data access pattern\nAdvantages\nDisadvantages\nGood data access performance\nData consistency issues\nNo scalability and throughput issues\nData ownership issues\nNo fault-tolerance issues\nData synchronization is required\nNo service dependencies\nReplicated Caching Pattern\nMost developers and architects think of caching as a technique for increasing overall\nresponsiveness. By storing data within an in-memory cache, retrieving data goes\nfrom dozens of milliseconds to only a couple of nanoseconds. However, caching can\nalso be an effective tool for distributed data access and sharing. This pattern leverages\nreplicated in-memory caching so that data needed by other services is made available\nto each service without them having to ask for it. A replicated cache differs from\nother caching models in that data is held in-memory within each service and is con\u2010\ntinuously synchronized so that all services have the same exact data at all times. 288 \n| \nChapter 10: Distributed Data Access", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 306", "position": 306, "chunk_type": "semantic", "token_estimate": 288}
{"text": "To better understand the replicated caching model, it\u2019s useful to compare it to other: caching models to see the differences between them. The single in-memory caching\nmodel is the simplest form of caching, where each service has its own internal in-\nmemory cache. With this caching model (illustrated in Figure 10-4), in-memory data\nis not synchronized between the caches, meaning each service has its own unique\ndata specific to that service. While this caching model does help increase responsive\u2010\nness and scalability within each service, it\u2019s not useful for sharing data between serv\u2010\nices because of the lack of cache synchronization between the services. Figure 10-4. With a single in-memory cache, each service contains its own unique data\nThe other caching model used in distributed architectures is distributed caching. As\nillustrated in Figure 10-5, with this caching model, data is not held in-memory within\neach service, but rather held externally within a caching server. Services, using a pro\u2010\nprietary protocol, make requests to the caching server to retrieve or update shared\ndata. Note that unlike the single in-memory caching model, data can be shared\namong the services. The distributed cache model is not an effective caching model to use for the replica\u2010\nted caching data access pattern for several reasons. First, there\u2019s no benefit to the\nfault-tolerance issues found with the Interservice Communication pattern. Rather\nthan depending on a service to retrieve data, the dependency has merely shifted to\nthe caching server. Because the cache data is centralized and shared, the distributed cache model allows\nother services to update data, thereby breaking the bounded context regarding data\nownership. This can cause data inconsistencies between the cache and the owning\ndatabase. While this can sometimes be addressed through strict governance, it is nev\u2010\nertheless an issue with this caching model. Lastly, since access to the centralized distributed cache is through a remote call, net\u2010\nwork latency adds additional retrieval time for the data, thus impacting overall\nresponsiveness as compared to an in-memory replicated cache. Replicated Caching Pattern \n| \n289", "domains": ["Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 307", "position": 307, "chunk_type": "semantic", "token_estimate": 338}
{"text": "Figure 10-7. Replicated caching data access pattern: With this pattern, the Wishlist Service no longer needs to make calls to the Catalog\nService to retrieve product descriptions\u2014they\u2019re already in-memory within the\nWishlist Service. When updates are made to the product description by the Catalog\nService, the caching product will update the cache in the Wishlist Service to make the\ndata consistent. The clear advantages of the replicated caching pattern are responsiveness, fault toler\u2010\nance, and scalability. Because no explicit interservice communication is required\nbetween the services, data is readily available in-memory, providing the fastest possi\u2010\nble access to data a service doesn\u2019t own. Fault tolerance is also well supported with\nthis pattern. Even if the Catalog Service goes down, the Wishlist Service can continue\nto operate. Once the Catalog Service comes back up, the caches connect to one\nanother without any disruption to the Wishlist Service. Lastly, with this pattern, the\nWishlist Service can scale independently from the Catalog Service. With all these clear advantages, how could there possibly be a trade-off with this pat\u2010\ntern? As the first law of software architecture states in our book, The Fundamentals of\nSoftware Architecture, everything in software architecture is a trade-off, and if an\narchitect thinks they have discovered something that isn\u2019t a trade-off, it means they\njust haven\u2019t identified the trade-off yet. Replicated Caching Pattern \n| \n291", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 309", "position": 309, "chunk_type": "semantic", "token_estimate": 226}
{"text": "The first trade-off with this pattern is a service dependency with regard to the cache: data and startup timing. Since the Catalog Service owns the cache and is responsible\nfor populating the cache, it must be running when the initial Wishlist Service starts\nup. If the Catalog Service is unavailable, the initial Wishlist Service must go into a\nwait state until a connection with the Catalog Service is established. Notice that only\nthe initial Wishlist Service instance is impacted by this startup dependency; if the\nCatalog Service is down, other Wishlist instances can be started up, with the cache\ndata transferred from one of the other Wishlist instances. It\u2019s also important to note\nthat once the Wishlist Service starts and has the data in the cache, it is not necessary\nfor the Catalog Service to be available. Once the cache is made available in the Wish\u2010\nlist Service, the Catalog Service can come up and down without impacting the Wish\u2010\nlist Service (or any of its instances). The second trade-off with this pattern is that of data volumes. If the volume of data is\ntoo high (such as exceeding 500 MB), the feasibility of this pattern diminishes\nquickly, particularly with regard to multiple instances of services needing the data. Each service instance has its own replicated cache, meaning that if the cache size of\n500 MB and 5 instances of a service are required, the total memory used is 2.5 GB. Architects must analyze both the size of the cache and the total number of services\ninstances needing the cached data to determine the total memory requirements for\nthe replicated cache. A third trade-off is that the replicated caching model usually cannot keep the data\nfully in sync between services if the rate of change of the data (update rate) is too\nhigh. This varies based on the size of the data and the replication latency, but in gen\u2010\neral this pattern is not well suited for highly volatile data (such as product inventory\ncounts). However, for relatively static data (such as a product description), this pat\u2010\ntern works well. The last trade-off associated with this pattern is that of configuration and setup man\u2010\nagement. Services know about each other in the replicated caching model through\nTCP/IP broadcasts and lookups. If the TCI/IP broadcast and lookup range is too\nbroad, it can take a long time to establish the socket-level handshake between serv\u2010\nices.", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 310", "position": 310, "chunk_type": "semantic", "token_estimate": 404}
{"text": "Table 10-3 lists the trade-offs associated with the replicated cache data access pattern.: Trade-Offs\nTable 10-3. Trade-offs associated with the replicated caching data access pattern\nAdvantages\nDisadvantages\nGood data access performance\nCloud and containerized configuration can be hard\nNo scalability and throughput issues\nNot good for high data volumes\nGood level of fault tolerance\nNot good for high update rates\nData remains consistent\nInitial service startup dependency\nData ownership is preserved\nData Domain Pattern\nIn the previous chapter, we discussed the use of a data domain to resolve joint owner\u2010\nship, where multiple services both need to write data to the same table. Tables that are\nshared between services are put into a single schema that is then shared by both serv\u2010\nices. That same pattern can be used for data access as well. Consider the Wishlist Service and Catalog Service problem again, where the Wishlist\nService needs access to the product descriptions but does not have access to the table\ncontaining those descriptions. Suppose the Interservice Communication pattern is\nnot a feasible solution because of reliability issues with the Catalog Service as well as\nthe performance issues with network latency and the additional data retrieval. Also\nsuppose using the Column Schema Replication pattern is not feasible because of the\nneed for high levels of data consistency. Finally, suppose that the Replicated Cache\npattern isn\u2019t an option because of the high data volumes. The only other solution is to\ncreate a data domain, combining the Wishlist and Product tables in the same shared\nschema, accessible to both the Wishlist Service and the Catalog Service. Figure 10-8 illustrates the use of this data access pattern. Notice that the Wishlist and\nProduct tables are no longer owned by either service, but rather shared between\nthem, forming a broader bounded context. With this pattern, gaining access to the\nproduct descriptions in the Wishlist Service is a matter of a simple SQL join between\nthe two tables. Data Domain Pattern \n| \n293", "domains": ["Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 311", "position": 311, "chunk_type": "semantic", "token_estimate": 328}
{"text": "Figure 10-8. Data domain data access pattern: While the sharing of data is generally discouraged in a distributed architecture, this\npattern has huge benefits over the other data access patterns. First of all, the services\nare completely decoupled from each other, thereby resolving any availability depend\u2010\nency, responsiveness, throughput, and scalability issues. Responsiveness is very good\nwith this pattern because the data is available using a normal SQL call, removing the\nneed to do additional data aggregations within the functionality of the service (as is\nrequired with the Replicated Cache pattern). Both data consistency and data integrity rate very high with the Data Domain pat\u2010\ntern. Since multiple services access the same data tables, data does not need to be\ntransferred, replicated, or synchronized. Data integrity is preserved in this pattern in\nthe sense that foreign-key constraints can now be enforced between the tables. In\naddition, other database artifacts, such as views, stored procedures, and triggers, can\nexist within the data domain. As a matter of fact, the preservation of these integrity\nconstraints and database artifacts is another driver for the use of the Data Domain\npattern. With this pattern, no additional contracts are needed to transfer data between serv\u2010\nices\u2014the table schema becomes the contract. While this is an advantage for this pat\u2010\ntern, it\u2019s a trade-off as well. The contracts used with the interservice communication\npattern and the Replicated Cache pattern form an abstraction layer over the table\nschema, allowing changes to the table structures to remain within a tight bounded\ncontext and not impact other services. However, this pattern forms a broader boun\u2010\nded context, requiring multiple services to possibly change when the structure to any\nof the tables in the data domain changes. 294 \n| \nChapter 10: Distributed Data Access", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 312", "position": 312, "chunk_type": "semantic", "token_estimate": 294}
{"text": "Another disadvantage of this pattern is that it can possibly open up security issues: associated with data access. For example, in Figure 10-8 the Wishlist Service has com\u2010\nplete access to all the data within the data domain. While this is OK in the Wishlist\nand Catalog Service example, there might be times when services accessing the data\ndomain shouldn\u2019t have access to certain data. A tighter bounded context with strict\nservice ownership can prevent other services from accessing certain data through the\ncontracts used to pass the data back and forth. Table 10-4 lists trade-offs associated with the data domain data access pattern. Trade-Offs\nTable 10-4. Trade-offs associated with the data domain data access pattern\nAdvantages\nDisadvantages\nGood data access performance\nBroader bounded context to manage data changes\nNo scalability and throughput issues\nData ownership governance\nNo fault tolerance issues\nData access security\nNo service dependency\nData remains consistent\nSysops Squad Saga: Data Access for Ticket Assignment\nThursday, March 3, 14:59\nLogan explained the various methods for data access within a distributed architecture,\nand also outlined the corresponding trade-offs of each technique. Addison, Sydney, and\nTaylen then had to come to a decision about which technique to use. \u201cUnless we start consolidating all of these services, I guess we are stuck with the fact that\nthe Ticket Assignment needs to somehow get to the expert profile data, and fast,\u201d said\nTaylen. \u201cOK,\u201d said Addison. \u201cSo service consolidation is out because these services are in entirely different\ndomains, and the shared data domain option is out for the same reasons we talked about before\u2014\nwe cannot have the Ticket Assignment Service connecting to two different databases.\u201d\n\u201cSo, that leaves us with one of two choices.\u201d said Sydney. \u201cEither we use interservice communication\nor replicated caching.\u201d\n\u201cWait. Let\u2019s explore the replicated caching option for a minute,\u201d said Taylen. \u201cHow much data are we\ntalking about here?\u201d\nSysops Squad Saga: Data Access for Ticket Assignment \n| \n295", "domains": ["Design Principles", "Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 313", "position": 313, "chunk_type": "semantic", "token_estimate": 325}
{"text": "\u201cWell,\u201d said Sydney, \u201cwe have 900 experts in the database. What data does the Ticket Assignment: Service need from the expert profile table?\u201d\n\u201cIt\u2019s mostly static information as we get the current expert location feeds from elsewhere. So, it\nwould be the expert\u2019s skill, their service location zones, and their standard scheduled availability,\u201d\nsaid Taylen. \u201cOK, so that\u2019s about 1.3 KB of data per expert. And since we have 900 experts total, that would be\u2026\nabout 1200 KB of data total. And the data is relatively static,\u201d said Sydney. \u201cHmm, that isn\u2019t much data to store in memory,\u201d said Taylen. \u201cLet\u2019s not forget that if we used a replicated cache, we would have to take into account how many\ninstances we would have for the User Management Service as well as the Ticket Assignment Ser-\nvice,\u201d said Addison. \u201cJust to be on the safe side, we should use the maximum number of instances of\neach we expect.\u201d\n\u201cI\u2019ve got that information,\u201d said Taylen. \u201cWe expect to have only a maximum of two instances of the\nUser Management Service, and a maximum of four at our highest peak for the Ticket Assignment\nService.\u201d\n\u201cThat\u2019s not much total in-memory data,\u201d observed Sydney. \u201cNo, it\u2019s not,\u201d said Addison. \u201cOK, let\u2019s analyze the trade-offs using the hypothesis-based approach we\ntried earlier. I suggest that we should go with the in-memory replicated cache option to cache only\nthe data necessary for the Ticket Assignment Service. Any other trade-offs you can think of?\u201d\nBoth Taylen and Sydney sat there for while trying to think of some negatives for the replicated cache\napproach. \u201cWhat if the User Management Service goes down?\u201d asked Sydney. \u201cAs long as the cache is populated, then the Ticket Assignment Service would be fine,\u201d said Addison. \u201cWait, you mean to tell me that the data would be in-memory, even if the User Management Service\nis unavailable?\u201d asked Taylen. \u201cAs long as the User Management Service starts before the Ticket Assignment Service, then yes,\u201d said\nAddison. \u201cAh!\u201d said Taylen. \u201cThen there\u2019s our first trade-off. Ticket assignment cannot function unless the User\nManagement Service is started. That\u2019s not good.\u201d\n\u201cBut,\u201d said Addison, \u201cif we made remote calls to the User Management Service and it goes down, the\nTicket Assignment Service becomes nonoperational. At least with the replicated cache option, once\nUser Management is up and running, we are no longer dependent on it. So, replicated caching is\nactually more fault tolerant in this case.\u201d\n\u201cTrue,\u201d said Taylen.", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 314", "position": 314, "chunk_type": "semantic", "token_estimate": 412}
{"text": "\u201cAnything else you can think of as a negative?\u201d asked Addison, knowing another obvious trade-off: but wanting the development team to come up with it on their own. \u201cUm,\u201d said Sydney, \u201cyeah. I have one. What caching product are we going to use?\u201d\n\u201cAh,\u201d said Addison, \u201cthat is in fact another trade-off. Have either of you done replicated caching\nbefore? Or anyone on the development team for that matter?\u201d\nBoth Taylen and Sydney shook their heads. \u201cThen we have some risk here,\u201d said Addison. \u201cActually,\u201d said Taylen, \u201cI\u2019ve been hearing a lot about this caching technique for a while and have\nbeen dying to try it out. I would volunteer to research some of the products and do some proof-of-\nconcepts on this approach.\u201d\n\u201cGreat,\u201d said Addison. \u201cIn the meantime, I will research what the licensing cost would be for those\nproducts as well, and if there\u2019s any technical limitation with respect to our deployment environment. You know, things like availability zone crossovers, firewalls, that sort of stuff.\u201d\nThe team began their research and proof-of-concept work, and found that this is indeed not only a\nfeasible solution cost and effort wise, but would solve the issue of data access to the expert profile\ntable. Addison discussed this approach with Logan, who approved the solution. Addison created an\nADR outlining and justifying this decision. ADR: Use of In-Memory Replicated Caching for Expert Profile Data\nContext\nThe Ticket Assignment Service needs continuous access to the expert profile table, which\nis owned by the User Management Service in a separate bounded context. Access to the\nexpert profile information can be done through interservice communication, in-memory\nreplicated caching, or a common data domain. Decision\nWe will use replicated caching between the User Management Service and the Ticket\nAssignment Service, with the User Management Service being the sole owner for write\noperations. Because the Ticket Assignment Service already connects to the shared ticket data domain\nschema, it cannot connect to an additional schema. In addition, since the user manage-\nment functionality and the core ticketing functionality are in two separate domains, we do\nnot want to combine the data tables in a single schema. Therefore, using a common data\ndomain is not an option. Using an in-memory replicated cache resolves the performance and fault-tolerance issues\nassociated with the interservice communication option. Sysops Squad Saga: Data Access for Ticket Assignment \n| \n297", "domains": ["Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 315", "position": 315, "chunk_type": "semantic", "token_estimate": 396}
{"text": "CHAPTER 11: Managing Distributed Workflows\nTuesday, February 15, 14:34\nAusten bolted into Logan\u2019s office just after lunch. \u201cI\u2019ve been looking at the new architec-\nture designs, and I want to help out. Do you need me to write up some ADRs or help\nwith some spikes? I\u2019d be happy to write up the ADR that states that we\u2019re only going to\nuse choreography in the new architecture to keep things decoupled.\u201d\n\u201cWhoa, there, you maniac,\u201d said Logan. \u201cWhere did you hear that? What gives you that\nimpression?\u201d\n\u201cWell, I\u2019ve been reading a lot about microservices, and everyone\u2019s advice seems to be to keep things\nhighly decoupled. When I look at the patterns for communication, it seems that choreography is the\nmost decoupled, so we should always use it, right?\u201d\n\"Always is a tricky term in software architecture. I had a mentor who had a memorable perspective\non this, who always said, Never use absolutes when talking about architecture, except when talking\nabout absolutes. In other words, never say never. I can\u2019t think of many decisions in architecture where\nalways or never applies.\u201d\n\u201cOK,\u201d said Austen. \u201cSo how do architects decide between the different communication patterns?\u201d\nAs part of our ongoing analysis of the trade-offs associated with modern distributed\narchitectures, we reach the dynamic part of quantum coupling, realizing many of the\npatterns we described and named in Chapter 2. In fact, even our named patterns only\ntouch on the many permutations possible with modern architectures. Thus, an archi\u2010\ntect should understand the forces at work so that they can make a most objective\ntrade-off analysis. 299", "domains": ["Architectural Patterns and Styles", "Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 317", "position": 317, "chunk_type": "semantic", "token_estimate": 266}
{"text": "Figure 11-2. Orchestration versus choreography in distributed architectures: Orchestration Communication Style\nThe orchestration pattern uses an orchestrator (sometimes called a mediator) compo\u2010\nnent to manage workflow state, optional behavior, error handling, notification, and a\nhost of other workflow maintenance. It is named for the distinguishing feature of a\nmusical orchestra, which utilizes a conductor to synchronize the incomplete parts of\nthe overall score to create a unified piece of music. Orchestration is illustrated in the\nmost generic representation in Figure 11-3. In this example, services A-D are domain services, each responsible for its own boun\u2010\nded context, data, and behavior. The Orchestrator component generally doesn\u2019t\ninclude any domain behavior outside of the workflow it mediates. Notice that micro\u2010\nservices architectures have an orchestrator per workflow, not a global orchestrator\nsuch as an enterprise service bus (ESB). One of the primary goals of the microservices\narchitecture style is decoupling, and using a global component such as an ESB creates\nOrchestration Communication Style \n| \n301", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 319", "position": 319, "chunk_type": "semantic", "token_estimate": 163}
{"text": "an undesirable coupling point. Thus, microservices tend to have an orchestrator per: workflow. Figure 11-3. Orchestration among distributed microservices\nOrchestration is useful when an architect must model a complex workflow that\nincludes more than just the single \u201chappy path,\u201d but also alternate paths and error\nconditions. However, to understand the basic shape of the pattern, we start with the\nnonerror happy path. Consider a very simple example of Penultimate Electronics sell\u2010\ning a device to one of its customers online, shown in Figure 11-4. This system passes the Place Order request to the Order Placement Orchestrator,\nwhich makes a synchronous call to the Order Placement Service, which records the\norder and returns a status message. Next, the mediator calls the Payment Service,\nwhich updates payment information. Next, the orchestrator makes an asynchronous\ncall to the Fulfillment Service to handle the order. The call is asynchronous because\nno strict timing dependencies exist for order fulfillment, unlike payment verification. For example, if order fulfillment happens only a few times a day, there is no reason\nfor the overhead of a synchronous call. Similarly, the orchestrator then calls the Email\nService to notify the user of a successful electronics order. If the world consisted of only happy paths, software architecture would be easy. How\u2010\never, one of the primary hard parts of software architecture is error conditions and\npathways. 302 \n| \nChapter 11: Managing Distributed Workflows", "domains": ["Architectural Patterns and Styles", "Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 320", "position": 320, "chunk_type": "semantic", "token_estimate": 233}
{"text": "Figure 11-4. A \u201chappy path\u201d workflow using an orchestrator to purchase electronic: equipment (note the asynchronous calls denoted by dotted lines for less time-sensitive\ncalls)\nConsider two potential error scenarios for electronics purchasing. First, what hap\u2010\npens if the customer\u2019s payment method is rejected? This error scenario appears in\nFigure 11-5. Here, the Order Placement Orchestrator updates the order via the Order Placement\nService as before. However, when trying to apply payment, it is rejected by the pay\u2010\nment service, perhaps because of an expired credit card number. In that case, the Pay\u2010\nment Service notifies the orchestrator, which then places a (typically) asynchronous\ncall to send a message to the Email Service to notify the customer of the failed order. Additionally, the orchestrator updates the state of the Order Placement Service, which\nstill thinks this is an active order. Notice in this example we\u2019re allowing each service to maintain its own transactional\nstate, modeling our \u201cFairy Tale Saga(seo) Pattern\u201d on page 333. One of the hardest parts\nof modern architectures is managing transactions, which we cover in Chapter 12. Orchestration Communication Style \n| \n303", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 321", "position": 321, "chunk_type": "semantic", "token_estimate": 185}
{"text": "Figure 11-5. Payment rejected error condition: In the second error scenario, the workflow has progressed further along: what hap\u2010\npens when the Fulfillment Service reports a back order? This error scenario appears\nin Figure 11-6. Figure 11-6. When an item is back-ordered, the orchestrator must rectify the state\n304 \n| \nChapter 11: Managing Distributed Workflows", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 322", "position": 322, "chunk_type": "semantic", "token_estimate": 55}
{"text": "As you can see, the workflow preceeds as normal until the Fulfillment Service notifies: the orchestrator that the current item is out of stock, necessitating a back order. In\nthat case, the orchestrator must refund the payment (this is why many online services\ndon\u2019t charge until shipment, not at order time) and update the state of the Order\nPlacement Service. One interesting characteristic to note in Figure 11-6: even in the most elaborate error\nscenarios, the architect wasn\u2019t required to add additional communication paths that\nweren\u2019t already there to facilitate the normal workflow, which differs from the \u201cChor\u2010\neography Communication Style\u201d on page 306. General advantages of the orchestration communication style include the following:\nCentralized workflow\nAs complexity goes up, having a unified component for state and behavior\nbecomes beneficial. Error handling\nError handling is a major part of many domain workflows, assisted by having a\nstate owner for the workflow. Recoverability\nBecause an orchestrator monitors the state of the workflow, an architect may add\nlogic to retry if one or more domain services suffers from a short-term outage. State management\nHaving an orchestrator makes the state of the workflow queriable, providing a\nplace for other workflows and other transient states. General disadvantages of the orchestration communication style include the\nfollowing:\nResponsiveness\nAll communication must go through the mediator, creating a potential through\u2010\nput bottleneck that can harm responsiveness. Fault tolerance\nWhile orchestration enhances recoverability for domain services, it creates a\npotential single point of failure for the workflow, which can be addressed with\nredundancy but adds more complexity. Scalability\nThis communication style doesn\u2019t scale as well as choreography because it has\nmore coordination points (the orchestrator), which cuts down on potential paral\u2010\nlelism. As we discussed in Chapter 2, several dynamic coupling patterns utilize\nchoreography and thus achieve higher scale (notably \u201cTime Travel Saga(sec) Pat\u2010\ntern\u201d on page 336 and \u201cAnthology Saga(aec) Pattern\u201d on page 349). Orchestration Communication Style \n| \n305", "domains": ["Design Patterns", "Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 323", "position": 323, "chunk_type": "semantic", "token_estimate": 322}
{"text": "Service coupling: Having a central orchestrator creates higher coupling between it and domain\ncomponents, which is sometimes necessary. The orchestration communication\nstyle\u2019s trade-offs appear in Table 11-1. Trade-Offs\nTable 11-1. Trade-offs for orchestration\nAdvantage\nDisadvantage\nCentralized workflow\nResponsiveness\nError handling\nFault tolerance\nRecoverability\nScalability\nState management\nService coupling\nChoreography Communication Style\nWhereas the Orchestration Communication Style was named for the metaphorical\ncentral coordination offered by an orchestrator, the choreography pattern visually\nillustrates intent of the communication style that has no central coordination. Instead, each service participates with the others, similar to dance partners. It isn\u2019t an\nad hoc performance\u2014the moves were planned beforehand by the choreographer/\narchitect but executed without a central coordinator. Figure 11-4 described the orchestrated workflow for a customer purchasing electron\u2010\nics from Penultimate Electronics; the same workflow modeled in the choreography\ncommunication style appears in Figure 11-7. In this workflow, the initiating request goes to the first service in the chain of respon\u2010\nsibility\u2014in this case, the Order Placement Service. Once it has updated internal\nrecords about the order, it sends an asynchronous request that the Payment Service\nreceives. Once payment has been applied, the Payment Service generates a message\nreceived by the Fulfillment Service, which plans for delivery and sends a message to\nthe Email Service. At first glance, the choreography solution seems simpler\u2014fewer services (no orches\u2010\ntrator), and a simple chain of events/commands (messages). However, as with many\nissues in software architecture, the difficulties lie not with the default paths but rather\nwith boundary and error conditions. 306 \n| \nChapter 11: Managing Distributed Workflows", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 324", "position": 324, "chunk_type": "semantic", "token_estimate": 260}
{"text": "However, consider the increasing complexity imposed by the other error scenario for: a product back order, shown in Figure 11-9. Figure 11-9. Managing the workflow error condition of product backlog\nMany steps of this workflow have already completed before the event (out of stock)\nthat causes the error. Because each of these services implement its own transactional\u2010\nity (this is an example of the \u201cAnthology Saga(aec) Pattern\u201d on page 349), when an error\noccurs, each service must issue compensating messages to other services. Once the\nFulfillment Service realizes the error condition, it should generate events suited to its\nbounded context, perhaps a broadcast message subscribed to by the Email, Payment,\nand Order Placement services. The example shown in Figure 11-9 illustrates the dependency between complex\nworkflows and mediators. While the initial workflow in choreography illustrated in\nFigure 11-7 seemed simpler than Figure 11-4, the error case (and others) keeps\nadding more complexity to the choreographed solution. In Figure 11-10, each error\nscenario forces domain services to interact with each other, adding communication\nlinks that weren\u2019t necessary for the happy path. 308 \n| \nChapter 11: Managing Distributed Workflows", "domains": ["Design Patterns", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 326", "position": 326, "chunk_type": "semantic", "token_estimate": 187}
{"text": "Figure 11-10. Error conditions in choreography typically add communication links: Every workflow that architects need to model in software has a certain amount of\nsemantic coupling\u2014the inherent coupling that exists in the problem domain. For\nexample, the process of assigning a ticket to a Sysops Squad member has a certain\nworkflow: a client must request service, skills must be matched to particular special\u2010\nists, then cross-referenced to schedules and locations. The way an architect models\nthat interaction is the implementation coupling. The semantic coupling of a workflow is mandated by the domain requirements of the\nsolution and must be modeled somehow. However clever an architect is, they cannot\nreduce the amount of semantic coupling, but their implementation choices may\nincrease it. This doesn\u2019t mean that an architect might not push back on impractical or\nimpossible semantics defined by business users\u2014some domain requirements create\nextraordinarily difficult problems in architecture. Here is a common example. Consider the standard layered monolithic architecture\ncompared to the more modern style of a modular monolith, shown in Figure 11-11. The architecture on the left represents the traditional layered architecture, separated\nby technical capabilities such as persistence, business rules, and so on. On the right,\nthe same solution appears, but separated by domain concerns such as Catalog Check\nout and Update Inventory rather than technical capabilities. Choreography Communication Style \n| \n309", "domains": ["Architectural Patterns and Styles"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 327", "position": 327, "chunk_type": "semantic", "token_estimate": 224}
{"text": "Catalog Checkout is \u201csmeared\u201d across the layers of the technical architecture,: whereas it appears only in the matching domain component and database in the\ndomain partitioned example. Of course, aligning a domain with domain partitioned\narchitecture isn\u2019t a revelation\u2014one of the insights of domain-driven design was the\nprimacy of the domain workflows. No matter what, if an architect wants to model a\nworkflow, they must make those moving parts work together. If the architect has\norganized their architecture the same as the domains, the implementation of the\nworkflow should have similar complexity. However, if the architect has imposed addi\u2010\ntional layers (as in technical partitioning, shown in Figure 11-12), it increases the\noverall implementation complexity because now the architect must design for the\nsemantic complexity along with the additional implementation complexity. Sometimes the extra complexity is warranted. For example, many layered architec\u2010\ntures came from a desire by architects to gain cost savings by consolidating on archi\u2010\ntecture patterns, such as database connection pooling. In that case, an architect\nconsidered the trade-offs of the cost saving associated with technically partitioning\ndatabase connectivity versus the imposed complexity and cost won in many cases. The major lesson of the last decade of architecture design is to model the semantics of\nthe workflow as closely as possible with the implementation. An \narchitect \ncan \nnever \nreduce \nsemantic \ncoupling \nvia\nimplementation, but they can make it worse. Thus, we can establish a relationship between the semantic coupling and the need for\ncoordination\u2014the more steps required by the workflow, the more potential error and\nother optional paths appear. Workflow State Management\nMost workflows include transient state about the status of the workflow: what ele\u2010\nments have executed, which ones are left, ordering, error conditions, retries, and so\non. For orchestrated solutions, the obvious workflow state owner is the orchestrator\n(although some architectural solutions create stateless orchestrators for higher scale). However, for choreography, no obvious owner for workflow state exists. Many com\u2010\nmon options exist to manage state in choreography; here are three common ones. First, the Front Controller pattern places the responsibility for state on the first called\nservice in the chain of responsibility, which in this case is Order Placement Service. If\nthat service contains information about both orders and the state of the workflow,\nsome of the domain services must have a communication link to query and update\nthe order state, as illustrated in Figure 11-13. Choreography Communication Style \n| \n311", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Design Principles", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 329", "position": 329, "chunk_type": "semantic", "token_estimate": 407}
{"text": "Figure 11-13. In choreography, a Front Controller is a domain service that owns: workflow state in addition to domain behavior\nIn this scenario, some services must communicate back to the Order Placement Ser\u2010\nvice to update the state of the order, as it is the state owner. While this simplifies the\nworkflow, it increases communication overhead and makes the Order Placement Ser\u2010\nvice more complex than one that handled only domain behavior. While the Front\nController pattern has some advantageous characteristics, it also has trade-offs, as\nshown in Table 11-2. Trade-Offs\nTable 11-2. Trade-offs for the Front Controller pattern\nAdvantage\nDisadvantage\nCreates a pseudo-orchestrator within\nchoreography\nAdds additional workflow state to a domain service\nMakes querying the state of an order\ntrivial\nIncreases communication overhead\nDetrimental to performance and scale as it increases\nintegration communication chatter\n312 \n| \nChapter 11: Managing Distributed Workflows", "domains": ["Design Patterns", "Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 330", "position": 330, "chunk_type": "semantic", "token_estimate": 143}
{"text": "A second way for an architect to manage the transactional state is to keep no transi\u2010: ent workflow state at all, relying on querying the individual services to build a real-\ntime snapshot. This is known as stateless choreography. While this simplifies the state\nof the first service, it greatly increases network overhead in terms of chatter between\nservices to build a stateful snapshot. For example, consider a workflow like the simple\nchoreography happy path in Figure 11-7 with no extra state. If a customer wants to\nknow the state of their order, the architect must build a workflow that queries the\nstate of each domain service to determine the most up-to-date order status. While\nthis makes for a highly flexible solution, rebuilding state can be complex and costly in\nterms of operational architecture characteristics like scalability and performance. Stateless choreography trades high performance for workflow control, as illustrated in\nTable 11-3. Trade-Offs\nTable 11-3. Stateless choreography trade-offs\nAdvantage\nDisadvantage\nOffers high performance and scale\nWorkflow state must be built on the fly\nExtremely decoupled\nComplexity rises swiftly with complex workflows\nA third solution utilizes stamp coupling (described in more detail in \u201cStamp Coupling\nfor Workflow Management\u201d on page 378), storing extra workflow state in the mes\u2010\nsage contract sent between services. Each domain service updates its part of the over\u2010\nall state and passes that to the next in the chain of responsibility. Thus, any consumer\nof that contract can check on the status of the workflow without querying each\nservice. This is a partial solution, as it still does not provide a single place for users to query\nthe state of the ongoing workflow. However, it does provide a way to pass the state\nbetween services as part of the workflow, providing each service with additional\npotentially useful context. As in all features of software architecture, stamp coupling\nhas good and bad characteristics, shown in Table 11-4. Choreography Communication Style \n| \n313", "domains": ["Design Patterns", "Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 331", "position": 331, "chunk_type": "semantic", "token_estimate": 324}
{"text": "Trade-Offs: Table 11-4. Stamp coupling trade-offs\nAdvantage\nDisadvantage\nAllows domain services to pass workflow state\nwithout additional queries to a state owner\nContracts must be larger to accommodate\nworkflow state\nEliminates need for a front controller\nDoesn\u2019t provide just-in-time status queries\nIn Chapter 13, we discuss how contracts can reduce or increase workflow coupling in\nchoreographed solutions. Advantages of the choreography communication style include the following:\nResponsiveness\nThis communication style has fewer single choke points, thus offering more\nopportunities for parallelism. Scalability\nSimilar to responsiveness, lack of coordination points like orchestrators allows\nmore independent scaling. Fault tolerance\nThe lack of a single orchestrator allows an architect to enhance fault tolerance\nwith the use of multiple instances. Service decoupling\nNo orchestrator means less coupling. Disadvantages of the choreography communication style include the following:\nDistributed workflow\nNo workflow owner makes error management and other boundary conditions\nmore difficult. State management\nNo centralized state holder hinders ongoing state management. Error handling\nError handling becomes more difficult without an orchestrator because the\ndomain services must have more workflow knowledge. Recoverability\nSimilarly, recoverability becomes more difficult without an orchestrator to\nattempt retries and other remediation efforts. 314 \n| \nChapter 11: Managing Distributed Workflows", "domains": ["Design Patterns", "Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 332", "position": 332, "chunk_type": "semantic", "token_estimate": 198}
{"text": "Like \u201cOrchestration Communication Style\u201d on page 301, choreography has a number: of good and bad trade-offs, often opposites, summarized in Table 11-5. Trade-Offs\nTable 11-5. Trade-offs for the choreography communication style\nAdvantage\nDisadvantage\nResponsiveness\nDistributed workflow\nScalability\nState management\nFault tolerance\nError handling\nService decoupling\nRecoverability\nTrade-Offs Between Orchestration and Choreography\nAs with all things in software architecture, neither orchestration nor choreography\nrepresent the perfect solution for all possibilities. A number of key trade-offs, includ\u2010\ning some delineated here, will lead an architect toward one of these two solutions. State Owner and Coupling\nAs illustrated in Figure 11-13, state ownership typically resides somewhere, either in a\nformal mediator acting as an orchestrator, or a front controller in a choreographed\nsolution. In the choreographed solution, removing the mediator forces higher levels\nof communication between services. This might be a perfectly suitable trade-off. For\nexample, if an architect has a workflow that needs higher scale and typically has few\nerror conditions, it might be worth trading the higher scale of choreography with the\ncomplexity of error handling. However, as workflow complexity goes up, the need for an orchestrator rises propor\u2010\ntionally, as illustrated in Figure 11-14. In addition, the more semantic complexity contained in a workflow, the more utili\u2010\ntarian an orchestrator is. Remember, implementation coupling can\u2019t make semantic\ncoupling better, only worse. Trade-Offs Between Orchestration and Choreography \n| \n315", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 333", "position": 333, "chunk_type": "semantic", "token_estimate": 228}
{"text": "Ultimately, the sweet spot for choreography lies with workflows that need responsive\u2010: ness and scalability, and either don\u2019t have complex error scenarios or they are infre\u2010\nquent. This communication style allows for high throughput; it is used by the\ndynamic coupling patterns \u201cPhone Tag Saga(sac) Pattern\u201d on page 330, \u201cTime Travel\nSaga(sec) Pattern\u201d on page 336, and \u201cAnthology Saga(aec) Pattern\u201d on page 349. However,\nit can also lead to extremely difficult implementations when other forces are mixed\nin, leading to the \u201cHorror Story(aac) Pattern\u201d on page 343. Figure 11-14. As the complexity of the workflow rises, orchestration becomes more useful\nOn the other hand, orchestration is best suited for complex workflows that include\nboundary and error conditions. While this style doesn\u2019t provide as much scale as\nchoreography, it greatly reduces complexity in most cases. This communication style\nappears in \u201cEpic Saga(sao) Pattern\u201d on page 325, \u201cFairy Tale Saga(seo) Pattern\u201d on page\n333, \u201cFantasy Fiction Saga(aao) Pattern\u201d on page 340, and \u201cParallel Saga(aeo) Pattern\u201d on\npage 346. Coordination is one of the primary forces that create complexity for architects when\ndetermining how to best communicate between microservices. Next, we investigate\nhow this force intersects with another primary force, consistency. 316 \n| \nChapter 11: Managing Distributed Workflows", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 334", "position": 334, "chunk_type": "semantic", "token_estimate": 205}
{"text": "Logan pondered the diagrams for a moment, then pronounced, \u201cWell, there doesn\u2019t seem to be an: obvious winner here. You know what that means.\u201d\nAusten piped up, \u201cTrade-offs!\u201d\n\u201cOf course,\u201d laughed Logan. \" Let\u2019s think about the likely scenarios and see how each solution reacts\nto them. What are the primary issues you are concerned with?\u201d\n\u201cThe first is lost or misrouted tickets. The business has been complaining about it, and it has become\na priority,\u201d said Addison. \u201cOK, which handles that problem better\u2014orchestration or choreography?\u201d\n\u201cEasier control of the workflow sounds like the orchestrator version is better\u2014we can handle all the\nworkflow issues there,\u201d volunteered Austen. \u201cOK, let\u2019s build a table of issues and preferred solutions in Table 11-6.\u201d\nTrade-Offs\nTable 11-6. Trade-off between orchestration and choreography for ticket\nworkflow\nOrchestration\nChoreography\nWorkflow control\n\u201cWhat\u2019s the next issue we should model?\u201d Addison asked. \u201cWe need to know the status of a trouble ticket at any given moment\u2014the business has requested\nthis feature, and it makes it easier to track several metrics. That implies we need an orchestrator so\nthat we can query the state of the workflow.\u201d\n\u201cBut you don\u2019t have to have an orchestrator for that\u2014we can query any given service to see if it has\nhandled a particular part of the workflow, or use stamp coupling,\u201d said Addison. \u201cThat\u2019s right\u2014this isn\u2019t a zero-sum game,\u201d said Logan. \u201cIt\u2019s possible that both or neither work just as\nwell. We\u2019ll give both solutions credit in our updated table in Table 11-7.\u201d\nSysops Squad Saga: Managing Workflows \n| \n319", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 337", "position": 337, "chunk_type": "semantic", "token_estimate": 258}
{"text": "Trade-Offs: Table 11-7. Updated trade-offs between orchestration and choreography for\nticket workflow\nOrchestration\nChoreography\nWorkflow control\nState query\nState query\n\u201cOK, what else?\u201d\n\u201cJust one more that I can think of,\u201d Addison said. \u201cTickets can get canceled by the customer, and tick-\nets can get reassigned because of expert availability, lost connections to the expert\u2019s mobile device,\nor expert delays at a customer site. Therefore, proper error handling is important. That means\norchestration?\u201d\n\u201cYes, generally. Complex workflows must go somewhere, either in an orchestrator or scattered\nthrough services. It\u2019s nice to have a single place to consolidate error handling. And choreography\ndefinitely does not score well here, so we\u2019ll update our table in Table 11-8.\u201d\nTrade-Offs\nTable 11-8. Final trade-offs between orchestration and choreography for ticket\nworkflow\nOrchestration\nChoreography\nWorkflow control\nState query\nState query\nError handling\n\u201cThat looks pretty good. Any more?\u201d\n\u201cNothing that\u2019s not obvious,\u201d said Addison. \u201cWe\u2019ll write this up in an ADR; in case we think of any\nother issues, we can add them there.\u201d\n320 \n| \nChapter 11: Managing Distributed Workflows", "domains": ["Design Patterns", "Design Principles", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 338", "position": 338, "chunk_type": "semantic", "token_estimate": 176}
{"text": "ADR: Use Orchestration for Primary Ticket Workflow: Context\nFor the primary ticket workflow, the architecture must support easy tracking of lost or mis-\ntracked messages, excellent error handling, and the ability to track ticket status. Either an\norchestration solution illustrated in Figure 11-16 or a choreography solution illustrated in\nFigure 11-15 will work. Decision\nWe will use orchestration for the primary ticketing workflow. We modeled orchestration and choreography and arrived at the trade-offs in Table 11-8. Consequences\nTicketing workflow might have scalability issues around a single orchestrator, which\nshould be reconsidered if current scalability requirements change. Sysops Squad Saga: Managing Workflows \n| \n321", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 339", "position": 339, "chunk_type": "semantic", "token_estimate": 103}
{"text": "CHAPTER 12: Transactional Sagas\nThursday, March 31, 16:55\nAusten showed up at Logan\u2019s office late on a windy Thursday afternoon. \u201cAddison just\nsent me over here to ask you about some horror story?\u201d\nLogan stopped and looked up. \u201cIs that a description of whatever crazy extreme sport\nyou\u2019re doing this weekend? What is it this time?\u201d\n\u201cIt\u2019s late spring, so a bunch of us are going ice skating on the thawing lake. We\u2019re wearing\nbody suits, so it\u2019s really a combination of skating and swimming. But that\u2019s not what\nAddison meant at all. When I showed Addison my design for the Ticketing workflow, I was immedi-\nately instructed to come to you and tell you I\u2019ve created a horror story.\u201d\nLogan laughed. \u201cOh, I see what\u2019s going on\u2014you stumbled into the Horror Story saga communica-\ntion pattern. You designed a workflow with asynchronous communication, atomic transactionality,\nand choreography, right?\u201d\n\u201cHow did you know?\u201d\n\u201cThat\u2019s the Horror Story saga pattern, or really, anti-pattern. There are eight generic saga patterns we\nstart from, so it\u2019s good to know what they are, because each has a different balance of trade-offs.\u201d\nThe concept of a saga in architecture predates microservices, originally concerned\nwith limiting the scope of database locks in early distributed architectures\u2014the paper\nlargely assumed to have coined the concept is from the Proceedings of the 1987 ACM\nconference. In his book Microservices Patterns (Manning Publications) and also out\u2010\nlined in the \u201cSaga Pattern\u201d section of his website, Chris Richardson describes the saga\npattern for microservices as a sequence of local transactions where each update pub\u2010\nlishes an event, thus triggering the next update in the sequence. If any of those\n323", "domains": ["Architectural Patterns and Styles"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 341", "position": 341, "chunk_type": "semantic", "token_estimate": 279}
{"text": "Here, an orchestrator service orchestrates a workflow that includes updates for three: services, expected to occur transactionally\u2014either all three calls succeed or none do. If one of the calls fails, they all fail and return to the previous state. An architect can\nsolve this coordination problem in a variety of ways, all complex in distributed archi\u2010\ntectures. However, such transactions limit the choice of databases and have legendary\nfailure modes. Many nascent or naive architects trust that, because a pattern exists for a problem, it\nrepresents a clean solution. However, the pattern is recognition of only commonality,\nnot solvability. Distributed transactions provide an excellent example of this phe\u2010\nnomenon\u2014architects accustomed to modeling transactions in nondistributed sys\u2010\ntems sometimes believe that moving that capability to the distributed world is an\nincremental change. However, transactions in distributed architectures present a\nnumber of challenges, which become proportionally worse depending on the com\u2010\nplexity of the semantic coupling of the problem. Consider a common implementation of the Epic Saga(sao) pattern, utilizing compen\u2010\nsating transactions. A compensating update is one that reverses a data write action\nperformed by another service (such as reversing an update, reinserting a previously\ndeleted row, or deleting a previously inserted row) during the course of the dis\u2010\ntributed transaction scope. While compensating updates attempt to reverse changes\nin order to bring distributed data sources back to their original state prior to the start\nof the distributed transaction, they are riddled with complex issues, challenges, and\ntrade-offs. A compensating transaction pattern assigns a service to monitor the transactional\ncompleteness of a request, as shown in Figure 12-4. Figure 12-4. A successful orchestrated transactional Epic Saga using a compensating\ntransaction\nTransactional Saga Patterns \n| \n327", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 345", "position": 345, "chunk_type": "semantic", "token_estimate": 283}
{"text": "However, as with many things in architecture, the error conditions cause the difficul\u2010: ties. In a compensating transaction framework, the mediator monitors the success of\ncalls, and issues compensating calls to other services if one or more of the requests\nfail, as shown in Figure 12-5. Figure 12-5. When an error occurs, a mediator must send compensating requests to\nother services\nA mediator both accepts requests and mediates the workflow, and synchronous calls\nto the first two services succeed. However, when trying to make the call to the last\nservice, it fails (from a possibly a wide variety of both domain and operational rea\u2010\nsons). Because the goal of the Epic Saga(sao) is atomic consistency, the mediator must\nutilize compensating transactions and request that the other two services undo the\noperation from before, returning the overall state to what it was before the transac\u2010\ntion started. This pattern is widely used: it models familiar behavior, and it has a well-established\npattern name. Many architects default to the Epic Saga(sao) pattern because it feels\nfamiliar to monolithic architectures, combined with a request (sometimes demand)\nfrom stakeholders that state changes must synchronize, regardless of technical con\u2010\nstraints. However, many of the other dynamic quantum coupling patterns may offer a\nbetter set of trade-offs. The clear advantage of the Epic Saga(sao) is the transactional coordination that mimics\nmonolithic systems, coupled with the clear workflow owner represented via an\norchestrator. However, the disadvantages are varied. First, orchestration plus transac\u2010\ntionality may have an impact on operational architecture characteristics such as per\u2010\nformance, scale, elasticity, and so on\u2014the orchestrator must make sure that all\nparticipants in the transaction have succeeded or failed, creating timing bottlenecks. Second, the various patterns used to implement distributed transactionality (such as\n328 \n| \nChapter 12: Transactional Sagas", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 346", "position": 346, "chunk_type": "semantic", "token_estimate": 297}
{"text": "compensating transactions) succumb to a wide variety of failure modes and bound\u2010: ary conditions, along with adding inherent complexity via undo operations. Dis\u2010\ntributed transactions present a host of difficulties and thus are best avoided if\npossible. The Epic Saga(sao) pattern features the following characteristics:\nCoupling level\nThis pattern exhibits extremely high levels of coupling across all possible dimen\u2010\nsions: synchronous communication, atomic consistency, and orchestrated coor\u2010\ndination\u2014it is in fact the most highly coupled pattern in the list. This isn\u2019t\nsurprising, as it models the behavior of highly coupled monolithic system com\u2010\nmunication, but creates a number of issues in distributed architectures. Complexity level\nError conditions and other intensive coordination added to the requirement of\natomicity add complexity to this architecture. The synchronous calls this archi\u2010\ntecture uses mitigate some of the complexity, as architects don\u2019t have to worry\nabout race conditions and deadlocks during calls. Responsiveness/availability\nOrchestration creates a bottleneck, especially when it must also coordinate trans\u2010\nactional atomicity, which reduces responsiveness. This pattern uses synchronous\ncalls, further impacting performance and responsiveness. If any of the services\nare not available or an unrecoverable error occurs, this pattern will fail. Scale/elasticity\nSimilar to responsiveness, the bottleneck and coordination required to implement\nthis pattern make scale and other operational concerns difficult. While the Epic Saga(sao) is popular because of familiarity, it creates a number of chal\u2010\nlenges, both from a design and operational characteristics standpoint, as shown in\nTable 12-2. Table 12-2. Ratings for the Epic Saga(sao)\nEpic Saga(sao) pattern\nRatings\nCommunication\nSynchronous\nConsistency\nAtomic\nCoordination\nOrchestrated\nCoupling\nVery high\nComplexity\nLow\nResponsiveness/availability\nLow\nScale/elasticity\nVery low\nTransactional Saga Patterns \n| \n329", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 347", "position": 347, "chunk_type": "semantic", "token_estimate": 272}
{"text": "Fortunately, architects need not default to patterns that, while seemingly familiar, cre\u2010: ate accidental complexity\u2014a variety of other patterns exist with differing sets of\ntrade-offs. Refer to the \u201cSysops Squad Saga: Atomic Transactions and Compensating\nUpdates\u201d on page 358 for a concrete example of the Epic Saga(sao) and some of the\ncomplex challenges it presents (and how to address those challenges). Phone Tag Saga(sac) Pattern\nThe Phone Tag Saga(sac) pattern changes one of the dimensions of the Epic Saga(sao),\nchanging coordination from orchestrated to choreographed; this change is illustrated in\nFigure 12-6. Figure 12-6. The Phone Tag pattern utilizes loosely coupled communication\nThe pattern name is Phone Tag because it resembles a well-known children\u2019s game\nknown as Telephone in North America: children form a circle, and one person whis\u2010\npers a secret to the next person, who passes it along to the next, until the final version\nis spoken by the last person. In Figure 12-6, choreography is favored over orchestra\u2010\ntion, creating the corresponding change in the structural communication shown in\nFigure 12-7. 330 \n| \nChapter 12: Transactional Sagas", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 348", "position": 348, "chunk_type": "semantic", "token_estimate": 180}
{"text": "Figure 12-7. Because of a lack of orchestration, each participant must coordinate status: The Phone Tag Saga(sac) pattern features atomicity but also choreography, meaning\nthat the architect designates no formal orchestrator. Yet atomicity requires some\ndegree of coordination. In Figure 12-7, the initially called service becomes the coordi\u2010\nnation point (sometimes called the front controller). Once it has finished its work, it\npasses a request on to the next service in the workflow, which continues until the\nworkflow succeeds. However, if an error condition occurs, each service must have\nbuilt-in logic to send compensating requests back along the chain. Because the architectural goal is transactional atomicity, logic to coordinate that\natomicity must reside somewhere. Thus, domain services must contain more logic\nabout the workflow context they participate within, including error handling and\nrouting. For complex workflows, the front controller in this pattern will become as\ncomplex as most mediators, reducing the appeal and applicability of this pattern. Thus, this pattern is commonly used for simple workflows that need higher scale, but\nwith a potential performance impact. How does choreography versus orchestration improve operational architecture char\u2010\nacteristics like scale? Using choreography even with synchronous communication\ncuts down on bottlenecks\u2014in nonerror conditions, the last service in the workflow\ncan return the result, allowing for higher throughput and fewer choke points. Perfor\u2010\nmance for happy path workflows can be faster than in an Epic Saga(sao) because of\nlack of coordination. However, error conditions will be much slower without a medi\u2010\nator\u2014each service must unwind the call chain, which also increases coupling between\nservices. Transactional Saga Patterns \n| \n331", "domains": ["Design Patterns", "Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 349", "position": 349, "chunk_type": "semantic", "token_estimate": 265}
{"text": "Generally, the Phone Tag Saga(sac) offers slightly better scale than the Epic Saga(sao): because of the lack of a mediator, which can sometimes become a limiting bottleneck. However, this pattern also features lower performance for error conditions and other\nworkflow complexities\u2014without a mediator, the workflow must be resolved via com\u2010\nmunication between services, which impacts performance. A nice feature of nonorchestrated architectures is the lack of a coupling singularity, a\nsingle place the workflow couples to. Even though this pattern utilizes synchronous\nrequests, fewer wait conditions for happy path workflows exist, allowing for higher\nscale. In general, reducing coupling increases scale. With the improved scalability brought about because of a lack of orchestration comes\nthe increased complexity of the domain services to manage the workflow concerns in\naddition to their nominal responsibility. For complex workflows, increased complex\u2010\nity and interservice communication may drive architects back toward orchestration\nand its trade-offs. The Phone Tag Saga(sac) has a fairly rare combination of features\u2014generally, if an\narchitect chooses choreography, they also choose asynchronicity. However, in some\ncases where an architect might choose this combination instead: synchronous calls\nensure that each domain service completes its part of the workflow before invoking\nthe next, eliminating race conditions. If error conditions are easy to resolve, or\ndomain services can utilize idempotence and retries, then architects can build higher\nparallel scale using this pattern compared to an Epic Saga(sao). The Phone Tag Saga(sac) pattern has the following characteristics:\nCoupling level\nThis pattern relaxes one of the coupling dimensions of the Epic Saga(sao) pattern,\nutilizing a choreographed rather than orchestrated workflow. Thus, this pattern is\nslightly less coupled, but with the same transactional requirement, meaning that\nthe complexity of the workflow must be distributed between the domain services. Complexity level\nThis pattern is significantly more complex than the Epic Saga(sao); complexity in\nthis pattern rises linearly proportionally to the semantic complexity of the work\u2010\nflow: the more complex the workflow, the more logic must appear in each service\nto compensate for lack of orchestrator. Alternatively, an architect might add\nworkflow information to the messages themselves as a form of stamp coupling\n(see \u201cStamp Coupling for Workflow Management\u201d on page 378) to maintain state\nbut adding to the overhead context required by each service. 332 \n| \nChapter 12: Transactional Sagas", "domains": ["Design Patterns", "Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 350", "position": 350, "chunk_type": "semantic", "token_estimate": 381}
{"text": "Responsiveness/availability: Less orchestration generally leads to better responsiveness, but error conditions\nin this pattern become more difficult to model without an orchestrator, requiring\nmore coordination via callbacks and other time-consuming activities. Scale/elasticity\nLack of orchestration translates to fewer bottlenecks, generally increasing scala\u2010\nbility, but only slightly. This pattern still utilizes tight coupling around two of the\nthree dimensions, so scalability isn\u2019t a highlight, especially if error conditions are\ncommon. The ratings for the Phone Tag Saga(sac) appear in Table 12-3. Table 12-3. Ratings for the Phone Tag Saga(sac)\nPhone Tag Saga(sac)\nRatings\nCommunication\nSynchronous\nConsistency\nAtomic\nCoordination\nChoreographed\nCoupling\nHigh\nComplexity\nHigh\nResponsiveness/availability\nLow\nScale/elasticity\nLow\nThe Phone Tag Saga(sac) pattern is better for simple workflows that don\u2019t have many\ncommon error conditions. While it offers a few better characteristics than the Epic\nSaga(sao), the complexity introduced by lack of an orchestrator offsets many of the\nadvantages. Fairy Tale Saga(seo) Pattern\nTypical fairy tales provide happy stories with easy-to-follow plots, thus the name\nFairy Tale Saga(seo), which utilizes synchronous communication, eventual consistency,\nand orchestration, as shown in Figure 12-8. Transactional Saga Patterns \n| \n333", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 351", "position": 351, "chunk_type": "semantic", "token_estimate": 184}
{"text": "Figure 12-8. The Fairy Tale Saga(seo) illustrates eventual consistency: This communication pattern relaxes the difficult atomic requirement, providing\nmany more options for architects to design systems. For example, if a service is down\ntemporarily, eventual consistency allows for caching a change until the service\nrestores. The communication structure for the Fairy Tale Saga(seo) is illustrated in Fig\u2010\nure 12-9. In this pattern, an orchestrator exists to coordinate request, response, and error han\u2010\ndling. However, the orchestrator isn\u2019t responsible for managing transactions, which\neach domain service retains responsibility for (for examples of common workflows,\nsee Chapter 11). Thus the orchestrator can manage compensating calls, but without\nthe requirement of occurring within an active transaction. 334 \n| \nChapter 12: Transactional Sagas", "domains": ["Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 352", "position": 352, "chunk_type": "semantic", "token_estimate": 119}
{"text": "Figure 12-9. Isomorphic illustration of a Fairy Tale interaction: This is a much more attractive pattern and appears commonly in many microservices\narchitectures. Having a mediator makes managing workflows easier, synchronous\ncommunication is the easier of the two choices, and eventual consistency removes the\nmost difficult coordination challenge, especially for error handling. The biggest appealing advantage of the Fairy Tale Saga(seo) is the lack of holistic trans\u2010\nactions. Each domain service manages its own transactional behavior, relying on\neventual consistency for the overall workflow. Compared to many other patterns, this pattern generally exhibits a good balance of\ntrade-offs:\nCoupling level\nThe Fairy Tale Saga(seo) features high coupling, with two of the three coupling\ndrivers maximized in this pattern (synchronous communication and orchestra\u2010\nted coordination). However, the worse driver of coupling complexity\u2014transac\u2010\ntionality\u2014disappears in this pattern in favor of eventual consistency. The\norchestrator must still manage complex workflows, but without the stricture of\ndoing so within a transaction. Complexity level\nComplexity for the Fairy Tale Saga(seo) is quite low; it includes the most conve\u2010\nnient options (orchestrated, synchronicity) with the loosest restriction (eventual\nconsistency). Thus the name Fairy Tale Saga(seo)\u2014a simple story with a happy\nending. Transactional Saga Patterns \n| \n335", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 353", "position": 353, "chunk_type": "semantic", "token_estimate": 200}
{"text": "Responsiveness/availability: Responsiveness is typically better in communication styles of this type because,\neven though the calls are synchronous, the mediator needs to contain less time-\nsensitive state about ongoing transactions, allowing for better load balancing. However, true distinctions in performance come with asynchronicity, illustrated\nin future patterns. Scale/elasticity\nLack of coupling generally leads to higher scale; removing transactional coupling\nallows each service to scale more independently. The ratings for the Fairy Tale Saga(seo) appear in Table 12-4. Table 12-4. Ratings for the Fairy Tale Saga(seo)\nFairy Tale Saga(seo)\nRatings\nCommunication\nSynchronous\nConsistency\nEventual\nCoordination\nOrchestrated\nCoupling\nHigh\nComplexity\nVery low\nResponsiveness/availability\nMedium\nScale/elasticity\nHigh\nIf an architect can take advantage of eventual consistency, this pattern is quite attrac\u2010\ntive, combining the easy moving parts with the fewest scary restrictions, making it a\npopular choice among architects. Time Travel Saga(sec) Pattern\nThe Time Travel Saga(sec) pattern features synchronous communication, and eventual\nconsistency, but choreographed workflow. In other words, this pattern avoids a cen\u2010\ntral mediator, placing the workflow responsibilities entirely on the participating\ndomain services, as illustrated in Figure 12-10. 336 \n| \nChapter 12: Transactional Sagas", "domains": ["Design Patterns", "Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 354", "position": 354, "chunk_type": "semantic", "token_estimate": 185}
{"text": "Figure 12-10. The Time Travel Saga(sec) pattern uses two of three decoupling techniques: The structural topology illustrates the lack of orchestration, shown in Figure 12-11. Figure 12-11. Complex workflows become difficult to manage without orchestration\nTransactional Saga Patterns \n| \n337", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 355", "position": 355, "chunk_type": "semantic", "token_estimate": 40}
{"text": "In this workflow, each service accepts a request, performs an action, and then for\u2010: wards the request on to another service. This architecture can implement the Chain\nof Responsibility design pattern or the Pipes and Filters architecture style\u2014any work\u2010\nflow with a one-way series of steps. Each service in this pattern \u201cowns\u201d its own trans\u2010\nactionality, so architects must design workflow error conditions into the domain\ndesign. In general, a proportional complexity relationship exists between workflow\ncomplexity and choreographed solutions because of a lack of built-in coordination\nvia a mediator\u2014the more complex the workflow, the more difficult choreography\nbecomes. It is called Time Travel Saga(sec) because everything is decoupled from a\ntime standpoint: each service owns its own transactional context, making workflow\nconsistency temporally gradual\u2014the state will become consistent over time based on\nthe design of the interaction. The lack of transactions in the Time Travel Saga(sec) pattern makes workflows easier\nto model; however, the lack of an orchestrator means that each domain service must\ninclude most workflow state and information. As in all choreographed solutions, a\ndirect correlation exists between workflow complexity and the utility of an orchestra\u2010\ntor; thus, this pattern is best suited for simple workflows. For solutions that benefit from high throughput, this pattern works extremely well for\n\u201cfire and forget\u201d style workflows, such as electronic data ingestion, bulk transactions,\nand so on. However, because no orchestrator exists, domain services must deal with\nerror conditions and coordination. Lack of coupling increases scalability with this pattern; only adding asynchronicity\nwould make it more scalable (as in the Anthology Saga(aec) pattern). However,\nbecause this pattern lacks holistic transactional coordination, architects must take\nextra effort to synchronize data. Here is the qualitative evaluation of the Time Travel Saga(sec) pattern:\nCoupling level\nThe coupling level falls in the medium range with the Time Travel Saga(sec), with\nthe decreased coupling brought on by the absence of an orchestrator balanced by\nthe still remaining coupling of synchronous communication. As with all eventual\nconsistency patterns, the absence of transactional coupling eases many data\nconcerns. Complexity level\nThe loss of transactionality provides a decrease in complexity for this pattern. This pattern is quasi-special-purpose, superbly suited to fast throughput, one-\nway communication architectures, and the coupling level matches that style of\narchitecture well. 338 \n| \nChapter 12: Transactional Sagas", "domains": ["Design Patterns", "Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 356", "position": 356, "chunk_type": "semantic", "token_estimate": 384}
{"text": "Responsiveness/availability: Responsiveness scores a medium with this architectural pattern: it is quite high for\nbuilt-to-purpose systems, as described previously, and quite low for complex\nerror handling. Because no orchestrator exists in this pattern, each domain ser\u2010\nvice must handle the scenario to restore eventual consistency in the case of an\nerror condition, which will cause a lot of overhead with synchronous calls,\nimpacting responsiveness and performance. Scale/elasticity\nThis architecture pattern offers extremely good scale and elasticity; it could only\nbe made better with asynchronicity (see the Anthology Saga(aec) pattern). The ratings for the Time Travel Saga(sec) pattern appear in Table 12-5. Table 12-5. Ratings for the Time Travel Saga(sec)\nTime Travel Saga(sec)\nRatings\nCommunication\nSynchronous\nConsistency\nEventual\nCoordination\nChoreographed\nCoupling\nMedium\nComplexity\nLow\nResponsiveness/availability\nMedium\nScale/elasticity\nHigh\nThe Time Travel Saga(sec) pattern provides an on-ramp to the more complex but ulti\u2010\nmately scalable Anthology Saga(aec) pattern. Architects and developers find dealing\nwith synchronous communication easier to reason about, implement, and debug; if\nthis pattern provides adequate scalability, teams don\u2019t have to embrace the more com\u2010\nplex but more scalable alternatives. Transactional Saga Patterns \n| \n339", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 357", "position": 357, "chunk_type": "semantic", "token_estimate": 185}
{"text": "Fantasy Fiction Saga(aao) Pattern: The Fantasy Fiction Saga(aao) uses atomic consistency, asynchronous communication,\nand orchestrated coordination, as shown in Figure 12-12. Figure 12-12. Asynchronous communication makes transactionality difficult in this pat\u2010\ntern\nThe structure representation shown in Figure 12-13 starts to show some of the diffi\u2010\nculties with this pattern. Just because a combination of architectural forces exists doesn\u2019t mean it forms an\nattractive pattern, yet this relatively implausible combination has uses. This pattern\nresembles the Epic Saga(sao) in all aspects except for communication\u2014this pattern uses\nasynchronous rather than synchronous communication. Traditionally, one way that\narchitects increase the responsiveness of distributed systems is by using asynchronic\u2010\nity, allowing operations to occur in parallel rather than serially. This may seem like a\ngood way to increase the perceived performance over an Epic Saga(sao). 340 \n| \nChapter 12: Transactional Sagas", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 358", "position": 358, "chunk_type": "semantic", "token_estimate": 138}
{"text": "Figure 12-13. The Fantasy Fiction Saga(aao) pattern is far-fetched because transaction: coordination for asynchronous communication presents difficulties\nHowever, asynchronicity isn\u2019t a simple change\u2014it adds many layers of complexity to\narchitecture, especially around coordination, requiring much more complexity in the\nmediator. For example, suppose a transactional workflow Alpha begins. Because\neverything is asynchronous, while Alpha is pending, transactional workflow Beta\nbegins. Now, the mediator must keep track of the state of all ongoing transactions in\npending state. It gets worse. Suppose that workflow Gamma begins, but the first call to the domain\nservice depends on the still pending outcome of Alpha\u2014how can an architect model\nthis behavior? While possible, the complexity grows and grows. Adding asynchronicity to orchestrated workflows adds asynchronous transactional\nstate to the equation, removing serial assumptions about ordering and adding the\npossibilities of deadlocks, race conditions, and a host of other parallel system\nchallenges. Transactional Saga Patterns \n| \n341", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 359", "position": 359, "chunk_type": "semantic", "token_estimate": 151}
{"text": "This pattern offers the following challenges:: Coupling level\nThe coupling level is extremely high in this pattern, using an orchestrator and\natomicity but with asynchronous communication, which makes coordination\nmore difficult because architects and developers must deal with race conditions\nand other out-of-order problems imposed by asynchronous communication. Complexity level\nBecause the coupling is so difficult, the complexity rises in this pattern as well. There\u2019s not only design complexity, requiring architects to develop overly com\u2010\nplex workflows, but also debugging and operational complexity of dealing with\nasynchronous workflows at scale. Responsiveness/availability\nBecause this pattern attempts transactional coordination across calls, responsive\u2010\nness will be impacted overall and be extremely bad if one or more of the services\nisn\u2019t available. Scale/elasticity\nHigh scale is virtually impossible in transaction systems, even with asynchronic\u2010\nity. Scale is much better in the similar pattern Parallel Saga(aeo), which switches\natomic to eventual consistency. The ratings for the Fantasy Fiction Saga(aao) pattern appear in Table 12-6. Table 12-6. Ratings for the Fantasy Fiction Saga(aao)\nFantasy Fiction\nRatings\nCommunication\nAsynchronous\nConsistency\nAtomic\nCoordination\nOrchestrated\nCoupling\nHigh\nComplexity\nHigh\nResponsiveness/availability\nLow\nScale/elasticity\nLow\nThis pattern is unfortunately more popular than it should be, mostly from the mis-\nguided attempt to improve the performance of Epic Saga(sao) while maintaining trans\u2010\nactionality; a better option is usually Parallel Saga(aeo). 342 \n| \nChapter 12: Transactional Sagas", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 360", "position": 360, "chunk_type": "semantic", "token_estimate": 224}
{"text": "Horror Story(aac) Pattern: One of the patterns must be the worst possible combination; it is the aptly named\nHorror Story(aac) pattern, characterized by asynchronous communication, atomic con\u2010\nsistency, and choreographed coordination, illustrated in Figure 12-14. Figure 12-14. The most difficult combination: achieving transactionality while asynchro\u2010\nnous and choreographed\nWhy is this combination so horrible? It combines the most stringent coupling around\nconsistency (atomic) with the two loosest coupling styles, asynchronous and choreog\u2010\nraphy. The structural communication for this pattern appears in Figure 12-15. Transactional Saga Patterns \n| \n343", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 361", "position": 361, "chunk_type": "semantic", "token_estimate": 88}
{"text": "Figure 12-15. This pattern requires a lot of interservice communication because of: required transactionality and the lack of a mediator\nIn this pattern, no mediator exists to manage transactional consistency across multi\u2010\nple services\u2014while using asynchronous communication. Thus, each domain service\nmust track undo information about multiple pending transactions, potentially out of\norder because of asynchronicity, and coordinate with each other during error condi\u2010\ntions. For just one of many possible horrible examples, imagine that transaction\nAlpha starts and, while pending, transaction Beta starts. One of the calls for the Alpha\ntransaction fails\u2014now, the choreographed services have to reverse the order of firing,\nundoing each (potentially out-of-order) element of the transaction along the way. The multiplicity and complexity of error conditions makes this a daunting option. Why might an architect choose this option? Asynchronicity is appealing as a perfor\u2010\nmance boost, yet the architect may still try to maintain transactional integrity, which\nhas many myriad failure modes. Instead, an architect would be better off choosing the\nAnthology Saga(aec) pattern, which removes holistic transactionality. The qualitative evaluations for the Horror Story(aac) pattern are as follows:\nCoupling level\nSurprisingly, the coupling level for this pattern isn\u2019t the worst (that \u201chonor\u201d goes\nto the Epic Saga(sao) pattern). While this pattern does attempt the worst kind of\nsingle coupling (transactionality), it relieves the other two, lacking both a media\u2010\ntor and the coupling\u2014increasing synchronous communication. 344 \n| \nChapter 12: Transactional Sagas", "domains": ["Design Patterns", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 362", "position": 362, "chunk_type": "semantic", "token_estimate": 237}
{"text": "Complexity level: Just as the name implies, the complexity of this pattern is truly horrific, the worst\nof any because it requires the most stringent requirement (transactionality) with\nthe most difficult combination of other factors to achieve that (asynchronicity\nand choreography). Scale/elasticity\nThis pattern does scale better than ones with a mediator, and asynchronicity also\nadds the ability to perform more work in parallel. Responsiveness/availability\nResponsiveness is low for this pattern, similar to the other patterns that require\nholistic transactions: coordination for the workflow requires a large amount of\ninterservice \u201cchatter,\u201d hurting performance and responsiveness. The trade-offs for the Horror Story(aac) pattern appear in Table 12-7. Table 12-7. Ratings for the Horror Story(aac)\nHorror Story(aac)\nRatings\nCommunication\nAsynchronous\nConsistency\nAtomic\nCoordination\nChoreographed\nCoupling\nMedium\nComplexity\nVery high\nResponsiveness/availability\nLow\nScale/elasticity\nMedium\nThe aptly named Horror Story(aac) pattern is often the result of a well-meaning archi\u2010\ntect starting with an Epic Saga(sao) pattern, noticing slow performance because of\ncomplex workflows, and realizing that techniques to improve performance include\nasynchronous communication and choreography. However, this thinking provides an\nexcellent example of not considering all the entangled dimensions of a problem\nspace. In isolation, asynchronous communication improves performance. However,\nas architects, we cannot consider it in isolation when it is entangled with other archi\u2010\ntecture dimensions, such as consistency and coordination. Transactional Saga Patterns \n| \n345", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 363", "position": 363, "chunk_type": "semantic", "token_estimate": 223}
{"text": "Parallel Saga(aeo) Pattern: The Parallel Saga(aeo) pattern is named after the \u201ctraditional\u201d Epic Saga(sao) pattern\nwith two key differences that ease restrictions and therefore make it an easier pattern\nto implement: asynchronous communication and eventual consistency. The dimen\u2010\nsional diagram of the Parallel Saga(aeo) pattern appears in Figure 12-16. Figure 12-16. Parallel Saga(aeo) offers performance improvements over traditional sagas\nThe most difficult goals in the Epic Saga(sao) pattern revolve around transactions and\nsynchronous communication, both of which cause bottlenecks and performance deg\u2010\nradation. As shown in Figure 12-16, the pattern loosens both restraints. The isomorphic representation of Parallel Saga(aeo) appears in Figure 12-17. This pattern uses a mediator, making it suitable for complex workflows. However, it\nuses asynchronous communication, allowing for better responsiveness and parallel\nexecution. Consistency in the pattern lies with the domain services, which may\nrequire some synchronization of shared data, either in the background or driven via\nthe mediator. As in other architectural problems that require coordination, a media\u2010\ntor becomes quite useful. 346 \n| \nChapter 12: Transactional Sagas", "domains": ["Design Patterns", "Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 364", "position": 364, "chunk_type": "semantic", "token_estimate": 172}
{"text": "Figure 12-17. Each service owns its own transactionality; the mediator coordinates: request and response\nFor example, if an error occurs during the execution of a workflow, the mediator can\nsend asynchronous messages to each involved domain service to compensate for the\nfailed change, which may entail retries, data synchronization, or a host of other\nremediations. Of course, the loosening of constraints implies that some benefits will be traded off,\nwhich is the nature of software architecture. Lack of transactionality imposes more\nburden on the mediator to resolve error and other workflow issues. Asynchronous\ncommunication, while offering better responsiveness, makes resolving timing and\nsynchronization issues difficult\u2014race conditions, deadlocks, queue reliability, and a\nhost of other distributed architecture headaches reside in this space. The Parallel Saga(aeo) pattern exhibits the following qualitative scores:\nCoupling level\nThis pattern has a low coupling level, isolating the coupling-intensifying force of\ntransactions to the scope of the individual domain services. It also utilizes asyn\u2010\nchronous communication, further decoupling services from wait states, allowing\nfor more parallel processing but adding a time element to an architect\u2019s coupling\nanalysis. Complexity level\nThe complexity of the Parallel Saga(aeo) is also low, reflecting the lessening of cou\u2010\npling stated previously. This pattern is fairly easy for architects to understand,\nand orchestration allows for simpler workflow and error-handling designs. Transactional Saga Patterns \n| \n347", "domains": ["Design Patterns", "Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 365", "position": 365, "chunk_type": "semantic", "token_estimate": 222}
{"text": "Scale/elasticity: Using asynchronous communication and smaller transaction boundaries allows\nthis architecture to scale quite nicely, and with good levels of isolation between\nservices. For example, in a microservices architecture, some public-facing serv\u2010\nices might need higher levels of scale and elasticity, where back office services\ndon\u2019t need scale but higher levels of security. Isolating transactions at the domain\nlevel frees the architecture to scale around domain concepts. Responsiveness/availability\nBecause of lack of coordinated transactions and asynchronous communication,\nthe responsiveness of this architecture is high. In fact, because each of these serv\u2010\nices maintains its own transactional context, this architecture is well suited to\nhighly variable service performance footprints between services, allowing archi\u2010\ntects to scale some services more than others because of demand. The ratings associated with the Parallel Saga(aeo) pattern appear in Table 12-8. Table 12-8. Ratings for the Parallel Saga(aeo)\nParallel Saga(aeo)\nRatings\nCommunication\nAsynchronous\nConsistency\nEventual\nCoordination\nOrchestrated\nCoupling\nLow\nComplexity\nLow\nResponsiveness/availability\nHigh\nScale/elasticity\nHigh\nOverall, the Parallel Saga(aeo) pattern offers an attractive set of trade-offs for many\nscenarios, especially with complex workflows that need high scale. 348 \n| \nChapter 12: Transactional Sagas", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 366", "position": 366, "chunk_type": "semantic", "token_estimate": 187}
{"text": "Anthology Saga(aec) Pattern: The Anthology Saga(aec) pattern provides the exact opposite set of characteristics to\nthe traditional Epic Saga(sao) pattern: it utilizes asynchronous communication, even\u2010\ntual consistency, and choreographed coordination, providing the least coupled exem\u2010\nplar among all these patterns. The dimensional view of the Anthology Saga(aec)\npattern appears in Figure 12-18. Figure 12-18. The Anthology Saga(aec) pattern offers the opposite extremes of the Epic\nSaga, and is therefore the least coupled pattern\nThe anthology pattern uses message queues to send asynchronous messages to other\ndomain services without orchestration, as illustrated in Figure 12-19. As you can see, each service maintains its own transactional integrity, and no orches\u2010\ntrator exists, forcing each domain service to include more context about the work\u2010\nflows they participate in, including error handling and other coordination strategies. Transactional Saga Patterns \n| \n349", "domains": ["Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 367", "position": 367, "chunk_type": "semantic", "token_estimate": 137}
{"text": "Figure 12-19. Lack of orchestration, eventual consistency, and asynchronicity make this: pattern highly decoupled but a challenge for coordination\nThe lack of orchestration makes services more complex but allows for much higher\nthroughput, scalability, elasticity, and other beneficial operational architecture char\u2010\nacteristics. No bottlenecks or coupling choke points exist in this architecture, allow\u2010\ning for high responsiveness and scalability. However, this pattern doesn\u2019t work particularly well for complex workflows, espe\u2010\ncially around resolving data consistency errors. While it may not seem possible\nwithout an orchestrator, stamp coupling (\u201cStamp Coupling for Workflow Manage\u2010\nment\u201d on page 378) may be used to carry workflow state, as described in the similar\nPhone Tag Saga(sac) pattern. This pattern works best for simple, mostly linear workflows, where architects desire\nhigh processing throughput. This pattern provides the most potential for both high\nperformance and scale, making it an attractive choice when those are key drivers for\nthe system. However, the degree of decoupling makes coordination difficult, prohibi\u2010\ntively so for complex or critical workflows. The \nshort-story-inspired \nAnthology \nSaga(aec) \npattern \nhas \nthe \nfollowing\ncharacteristics:\nCoupling level\nCoupling for this pattern is the lowest for any other combination of forces, creat\u2010\ning a highly decoupled architecture well suited for high scale and elasticity. Complexity level\nWhile the coupling is extremely low, complexity is correspondingly high, espe\u2010\ncially for complex workflows where an orchestrator (lacking here) is convenient. 350 \n| \nChapter 12: Transactional Sagas", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 368", "position": 368, "chunk_type": "semantic", "token_estimate": 235}
{"text": "Scale/elasticity: This pattern scores the highest in the scale and elasticity category, correlating\nwith the overall lack of coupling found in this pattern. Responsiveness\nResponsiveness is high in this architecture because of a lack of speed governors\n(transactional consistency, synchronous communication) and use of responsive\u2010\nness accelerators (choreographed coordination). The ratings table for the Anthology Saga(aec) pattern appear in Table 12-9. Table 12-9. Ratings for the Anthology Saga(aec)\nAnthology Saga(aec)\nRatings\nCommunication\nAsynchronous\nConsistency\nEventual\nCoordination\nChoreographed\nCoupling\nVery low\nComplexity\nHigh\nResponsiveness/availability\nHigh\nScale/elasticity\nVery high\nThe Anthology Saga(aec) pattern is well suited to extremely high throughput commu\u2010\nnication with simple or infrequent error conditions. For example, a Pipes and Filters\narchitecture would fit this pattern exactly. Architects can implement the patterns described in this section in a variety of ways. For example, architects can manage transactional sagas through atomic transactions\nby using compensating updates or by managing transactional state with eventual con\u2010\nsistency. This section showed the advantages and disadvantages of each approach,\nwhich will help an architect decide which transactional saga pattern to use. State Management and Eventual Consistency\nState management and eventual consistency leverage finite state machines (see \u201cSaga\nState Machines\u201d on page 352) to always know the current state of the transactional\nsaga, and to also eventually correct the error condition through retries or some sort\nof automated or manual corrective action. To illustrate this approach, consider the\nFairy Tale Saga(seo) implementation of the ticket completion example illustrated in\nFigure 12-20. State Management and Eventual Consistency \n| \n351", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 369", "position": 369, "chunk_type": "semantic", "token_estimate": 253}
{"text": "Figure 12-20. The Fairy Tale Saga leads to better responsiveness, but leaves data sources: out of sync with one another until they can be corrected\nNotice that the Survey Service is not available during the scope of the distributed\ntransaction. However, with this type of saga, rather than issue a compensating update,\nthe state of the saga is changed to NO_SURVEY and a successful response is sent to the\nSysops Expert (step 7 in the diagram). The Ticket Orchestrator Service then works\nasynchronously (behind the scenes) to resolve the error programmatically by retries\nand error analysis. If it cannot resolve the error, the Ticket Orchestrator Service sends\nthe error to an administrator or supervisor for manual repair and processing. By managing the state of the saga rather than issuing compensating updates, the end\nuser (in this case, the Sysops Squad expert) doesn\u2019t need to be concerned that the sur\u2010\nvey was not sent to the customer\u2014that responsibility is for the Ticket Orchestrator\nService to worry about. Responsiveness is good from the end user\u2019s perpective, and\nthe user can work on other tasks while the errors are handled by the system. Saga State Machines\nA state machine is a pattern that describes all of the possible paths that can exist\nwithin a distributed architecture. A state machine always starts with a beginning state\nthat launches the transactional saga, then contains transition states and correspond\u2010\ning action that should occur when the transition state happens. 352 \n| \nChapter 12: Transactional Sagas", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 370", "position": 370, "chunk_type": "semantic", "token_estimate": 249}
{"text": "To illustrate how a saga state machine works, consider the following workflow of a: new problem ticket created by a customer in the Sysops Squad system:\n1. The customer enters a new problem ticket into the system. 2. The ticket is assigned to the next available Sysops Squad expert. 3. The ticket is then routed to the expert\u2019s mobile device. 4. The expert receives the ticket and works on the issue. 5. The expert finishes the repair and marks the ticket as complete. 6. A survey is sent to the customer. The various states that can exist within this transactional saga, as well as the corre\u2010\nsponding transition actions, are illustrated in Figure 12-21. Notice that the transac\u2010\ntional saga begins with the START node indicating the saga entry point, and terminates\nwith the CLOSED node indicating the saga exit point. Figure 12-21. State diagram for creating a new problem ticket\nState Management and Eventual Consistency \n| \n353", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 371", "position": 371, "chunk_type": "semantic", "token_estimate": 158}
{"text": "The following items describe in more detail this transactional saga and the corre\u2010: sponding states and transition actions that happen within each state:", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 372", "position": 372, "chunk_type": "semantic", "token_estimate": 23}
{"text": "The transactional saga starts with a customer entering a new problem ticket into: the system. The customer\u2019s support plan is verified, and the ticket data is valida\u2010\nted. Once the ticket is inserted into the ticket table in the database, the transac\u2010\ntional saga state moves to CREATED and the customer is notified that the ticket\nhas been successfully created. This is the only possible outcome for this state\ntransition\u2014any errors within this state prevent the saga from starting.", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 372", "position": 372, "chunk_type": "semantic", "token_estimate": 79}
{"text": "Once the ticket is successfully created, it is assigned to a Sysops Squad expert. If: no expert is available to service the ticket, it is held in a wait state until an expert\nis available. Once an expert is assigned, the saga state moves to the ASSIGNED\nstate. This is the only outcome for this state transition, meaning the ticket is held\nin CREATED state until it can be assigned.", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 372", "position": 372, "chunk_type": "semantic", "token_estimate": 70}
{"text": "Once a ticket is assigned to an expert, the only possible outcome is to route the: ticket to the expert. It is assumed that during the assignment algorithm, the\nexpert has been located and is available. If the ticket cannot be routed because the\nexpert cannot be located or is unavailable, the saga stays in this state until it can\nbe routed. Once routed, the expert must acknowledge that the ticket has been\nreceived. Once this happens, the transactional saga state moves to ACCEPTED. This is the only possible outcome for this state transition.", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 372", "position": 372, "chunk_type": "semantic", "token_estimate": 94}
{"text": "There are two possible states once a ticket has been accepted by a Sysops Squad: expert: COMPLETED or REASSIGN. Once the expert finishes the repair and\nmarks the ticket as \u201ccomplete,\u201d the state of the saga moves to COMPLETED. However, if for some reason the ticket was wrongly assigned or the expert is not\nable to finish the repair, the expert notifies the system and the state moves to\nREASSIGN.", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 372", "position": 372, "chunk_type": "semantic", "token_estimate": 70}
{"text": "Once in this saga state, the system will reassign the ticket to a different expert.: Like the CREATED state, if an expert is not available, the transactional saga will\nremain in the REASSIGN state until an expert is assigned. Once a different expert\nis found and the ticket is once again assigned, the state moves into the\nASSIGNED state, waiting to be accepted by the other expert. This is the only pos\u2010\nsible outcome for this state transition, and the saga remains in this state until an\nexpert is assigned to the ticket. 354 \n| \nChapter 12: Transactional Sagas", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 372", "position": 372, "chunk_type": "semantic", "token_estimate": 99}
{"text": "COMPLETED: The two possible states once an expert completes a ticket are CLOSED or\nNO_SURVEY. When the ticket is in this state, a survey is sent to the customer to\nrate the expert and the service, and the saga state is moved to CLOSED, thus end\u2010\ning the transaction saga. However, if the Survey Service is unavailable or an error\noccurs while sending the survey, the state moves to NO_SURVEY, indicating that\nthe issue was fixed but no survey was sent to the customer. NO_SURVEY\nIn this error condition state, the system continues to try sending the survey to the\ncustomer. Once successfully sent, the state moves to CLOSED, marking the end\nof the transactional saga. This is the only possible outcome of this state\ntransaction. In many cases, it\u2019s useful to put the list of all possible state transitions and the corre\u2010\nsponding transition action in some sort of table. Developers can then use this table to\nimplement the state transition triggers and possible error conditions in an orchestra\u2010\ntion service (or respective services if using choreography). An example of this prac\u2010\ntice is shown in Table 12-10, which lists all the possible states and actions that are\ntriggered when the state transition occurs. Table 12-10. Saga state machine for a new problem ticket in the Sysops Squad system\nInitiating state\nTransition state\nTransaction action", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 373", "position": 373, "chunk_type": "semantic", "token_estimate": 226}
{"text": "Ticket saga done: The choice between using compensating updates or state management for distributed\ntransaction workflows depends on the situation as well as trade-off analysis between\nresponsiveness and consistency. Regardless of the technique used to manage errors\nwithin a distributed transaction, the state of the distributed transaction should be\nknown and also managed. Table 12-11 summarizes the trade-offs associated with using state management rather\nthan atomic distributed transactions with compensating updates. State Management and Eventual Consistency \n| \n355", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 373", "position": 373, "chunk_type": "semantic", "token_estimate": 78}
{"text": "Trade-Offs: Table 12-11. Trade-offs associated with state management rather than atomic\ndistributed transactions with compensating updates\nAdvantages\nDisadvantages\nGood responsiveness\nData may be out of sync when errors occur\nLess impact to end user for errors\nEventual consistency may take some time\nTechniques for Managing Sagas\nDistributed transactions are not something that can be simply \u201cdropped into\u201d a sys\u2010\ntem. They cannot be downloaded or purchased using some sort of framework or\nproduct like ACID transaction managers\u2014they must be designed, coded, and main\u2010\ntained by developers and architects. One of the techniques we like to use to help manage distributed transactions is to lev\u2010\nerage annotations (Java) or custom attributes (C#), or other similar artifacts in other\nlanguages. While these language artifacts themselves don\u2019t contain any actual func\u2010\ntionality, they do provide a programmatic way of capturing and documenting the\ntransactional sagas in the system, as well as provide a means for associating services\nwith transactional sagas. The source listings in Example 12-1 (Java) and Example 12-2 (C#) show an example\nof implementing these annotations and custom attributes. Notice that in both imple\u2010\nmentations, the transactional sagas (NEW_TICKET, CANCEL_TICKET, and so on) are con\u2010\ntained within the Transaction enum, providing a single place within the source code\nfor listing and documenting the various sagas that exist within an application context. Example 12-1. Source code defining a transactional saga annotation (Java)\n@Retention(RetentionPolicy.RUNTIME)\n@Target(ElementType.TYPE)\npublic @interface Saga {\n   public Transaction[] value();\n   public enum Transaction {\n      NEW_TICKET,\n      CANCEL_TICKET,\n      NEW_CUSTOMER,\n      UNSUBSCRIBE,\n      NEW_SUPPORT_CONTRACT\n   }\n}\n356 \n| \nChapter 12: Transactional Sagas", "domains": ["Design Patterns", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 374", "position": 374, "chunk_type": "semantic", "token_estimate": 256}
{"text": "Using these annotations and custom attributes, architects and developers can write: simple command-line interface (CLI) tools to walk through a codebase or source\ncode repository to provide saga information in real time. For example, using a simple\ncustom code-walk tool, a developer, architect, or even a business analyst can query\nwhat services are involved for the NEW_TICKET saga:\n$ ./sagatool.sh NEW_TICKET -services\n-> Ticket Service\n-> Assignment Service\n-> Routing Service\n-> Survey Service\n$\nA custom code-walking tool can look at each class file in the application context con\u2010\ntaining the @ServiceEntrypoint custom annotation (or attribute) and check the\n@Saga custom annotation for the presence of the particular saga (in this case, Transac\ntion.NEW_TICKET). This sort of custom tool is not complicated to write, and can help\nprovide valuable information when managing transactional sagas. Sysops Squad Saga: Atomic Transactions and\nCompensating Updates\nTuesday, April 5, 09:44\nAddison and Austen met first thing with Logan to hash out the issues around transac-\ntionality in the new microservices architecture in the longish conference room. Logan began, \u201cI know that not everyone is on the same page about how what you\u2019ve\nread applies to what we\u2019re doing here. So, I\u2019ve prepared some workflows and diagrams\nto help everyone get on the same page. Today, we\u2019re discussing marking a ticket com-\nplete in the system. For this workflow, the Sysops Squad expert completes a job and\nmarks the ticket as \u201ccomplete\u201d using the mobile application on the expert\u2019s mobile\ndevice. I want to talk about the Epic Saga pattern and the issues around compensating updates. I\u2019ve\ncreated a diagram to illustrate this workflow in Figure 12-22 Can everyone see it?\u201d\n358 \n| \nChapter 12: Transactional Sagas", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 376", "position": 376, "chunk_type": "semantic", "token_estimate": 283}
{"text": "Figure 12-22. The epic saga requires the ticket status to be updated and survey to be sent: in one synchronous atomic operation\nLogan continued, \u201cI\u2019ve also created a list that describes each step. The circled numbers on the dia-\ngram match up with the workflow.\u201d\n1. The Sysops Squad expert marks the ticket as complete using an app on their mobile device,\nwhich is synchronously received by the Ticket Orchestrator Service. 2. The Ticket Orchestrator Service sends a synchronous request to the Ticket Service to change\nthe state of the ticket from \u201cin-progress\u201d to \u201ccomplete.\u201d\n3. The Ticket Service updates the ticket number to \u201ccomplete\u201d in the database table and commits\nthe update. 4. As part of the ticket completion process, the Ticket Service asynchronously sends ticketing\ninformation (such as ticket repair time, ticket wait time, duration, and so on) to a queue to be\npicked up by the Analytics Service. Once sent, the Ticket Service sends an acknowledgment to\nthe Ticket Orchestrator Service that the update is complete. 5. At about the same time, the Analytics Service asynchronously receives the updated ticket ana-\nlytics and starts to process the ticket information. 6. The Ticket Orchestrator Service then sends a synchronous request to the Survey Service to pre-\npare and send the customer survey to the customer. Sysops Squad Saga: Atomic Transactions and Compensating Updates \n| \n359", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 377", "position": 377, "chunk_type": "semantic", "token_estimate": 227}
{"text": "7. The Survey Service inserts data into a table with the survey information (customer, ticket info,: and timestamp) and commits the insert. 8. The Survey Service then sends the survey to the customer via email and returns an acknowl-\nedgment back to the Ticket Orchestrator Service that the survey processing is complete. 9. Finally, the Ticket Orchestrator Service sends a response back to the Sysops Squad expert\u2019s\nmobile device stating that the ticket completion processing is done. Once this happens, the\nexpert can select the next problem ticket assigned to them. \u201cWow, this is really helpful. How long did it take you to create this?\u201d said Addison. \u201cNot a little time, but it\u2019s come in handy. You aren\u2019t the only group that\u2019s confused about how to get\nall these moving pieces to work together. This is the hard part of software architecture. Everyone\nunderstand the basics of the workflow?\u201d\nTo a sea of nods, Logan continued, \u201cOne of the first issues that occurs with compensating updates is\nthat since there\u2019s no transactional isolation within a distributed transaction (see \u201cDistributed Transac-\ntions\u201d on page 263), other services may have taken action on the data updated within the scope of\nthe distributed transaction before the distributed transaction is complete. To illustrate this issue,\nconsider the same Epic Saga example appearing in Figure 12-23: the Sysops Squad expert marks a\nticket as complete, but this time the Survey Service is not available. In this case, a compensating\nupdate (step 7 in the diagram) is sent to the Ticket Service to reverse the update, changing the ticket\nstate from completed back to in-progress (step 8 in the diagram).\u201d\n\u201cNotice also in Figure 12-23 that since this is an atomic distributed transaction, an error is then sent\nback to the Sysops Squad expert indicating that the action was not successful and to try again. Now,\na question for you: why should the Sysops Squad expert have to worry that the survey is not sent?\u201d\nAusten pondered a moment. \u201cBut wasn\u2019t that part of the workflow in the monolith? All that stuff\nhappened within a transaction, if I remember correctly.\u201d\n\u201cYeah, but I always thought that was weird, just never said anything,\u201d said Addison. \u201cI don\u2019t see why\nthe expert should worry about the survey. The expert just wants to get on to the next ticket\nassigned to them.\u201d\n\u201cRight,\u201d Logan said. \u201cThis is the issue with atomic distributed transactions\u2014the end user is unneces-\nsarily semantically coupled to the business process.", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 378", "position": 378, "chunk_type": "semantic", "token_estimate": 415}
{"text": "Figure 12-23. Epic Saga(sao) requires compensation, but side effects can occur: \u201cWe call this a side effect within distributed architectures. By reversing the transaction in the Ticket\nService, actions performed by other services using data from the prior update may have already\ntaken place and might not be able to be reversed. This scenario points to the importance of isolation\nwithin a transaction, something that distributed transactions do not support. To address this issue,\nthe Ticket Service could send another request through the data pump to the Analytics Service, tell-\ning that service to ignore the prior ticket information, but just imagine the amount of complex code\nand timing logic that would be required in the Analytics Service to address this compensating\nchange. Furthermore, there may have been additional downstream actions taken on the analytical\ndata already processed by the Analytics Service, further complicating the chain of events to reverse\nand correct. With distributed architectures and distributed transactions, it really is sometimes turtles\nall the way down.\u201d\nLogan paused for a moment, then continued, \u201cAnother issue\u2014\u201d\nAusten interrupted, \u201cAnother issue?\u201d\nLogan smiled. \u201cAnother issue regarding compensating updates is compensation failures. Keeping\nwith the same Epic Saga example for completing a ticket, notice in Figure 12-24 that in step 7 a\ncompensating update is issued to the Ticket Service to change the state from completed back to in-\nprogress. However, in this case, the Ticket Service generates an error when trying to change the state\nof the ticket (step 8).\u201d\nSysops Squad Saga: Atomic Transactions and Compensating Updates \n| \n361", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 379", "position": 379, "chunk_type": "semantic", "token_estimate": 259}
{"text": "\u201cYes, while responsiveness can sometimes be resolved by asynchronously issuing compensating: updates through eventual consistency (such as with the Parallel Saga and the Anthology Saga pat-\ntern), nevertheless most atomic distributed transactions have worse responsiveness when compen-\nsating updates are involved.\u201d\n\u201cOK, that makes sense\u2014atomic coordination will always have overhead,\u201d Austen said. \u201cThat\u2019s a lot of information. Let\u2019s build a table to summarize some of the trade-offs associated with\natomic distributed transactions and compensating updates.\u201d (See Table 12-12.) Trade-Offs\nTable 12-12. Trade-offs associated with atomic distributed transactions and\ncompensating updates\nAdvantages\nDisadvantages\nAll data restored to prior state\nNo transaction isolation\nAllows retries and restart\nSide effects may occur on compensation\nCompensation may fail\nPoor responsiveness for the end user\nLogan said, \u201cWhile this compensating transaction pattern exists, it also offers a number of chal-\nlenges. Who wants to name one?\u201d\n\u201cI know: a service cannot perform a rollback,\u201d said Austen. \u201cWhat if one of the services cannot success-\nfully undo the previous operation? The orchestrator must have coordination code to indicate that\nthe transaction wasn\u2019t successful.\u201d\n\u201cRight\u2014what about another?\u201d\n\"To lock or not lock participating services?\" said Addison. \u201cWhen the mediator places a call to a service\nand it updates a value, the mediator will make calls to subsequent services that are part of the work-\nflow. However, what happens if another request appears for the first service contingent on the out-\ncome of the first request\u2019s resolution, either from the same mediator or a different context? This\ndistributed architecture problem becomes worse when the calls are asynchronous rather than syn-\nchronous (illustrated in \u201cPhone Tag Saga(sac) Pattern\u201d on page 330). Alternatively, the mediator could\ninsist that other services don\u2019t accept calls during the course of a workflow, which guarantees a valid\ntransaction but destroys performance and scalability.\u201d\nLogan said, \u201cCorrect. Let\u2019s get philosophical for a moment. Conceptually, transactions force partici-\npants to stop their individual worlds and synchronize on a particular value. This is so easy to model\nwith monolithic architectures and relational databases that architects overuse transactions in those\nSysops Squad Saga: Atomic Transactions and Compensating Updates \n| \n363", "domains": ["Design Patterns", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 381", "position": 381, "chunk_type": "semantic", "token_estimate": 352}
{"text": "systems. Much of the real world isn\u2019t transactional, as observed in the famous essay by Gregor: Hohpe, \u201cStarbucks Does Not Use Two-Phase Commit\u201d. Transactional coordination is one of the hard-\nest parts of architecture, and the broader the scope, the worse it becomes.\u201d\n\u201cIs there an alternative to using an Epic Saga?\u201d Addison asked. \u201cYes!\u201d Logan said. \u201cA more realistic approach to the scenario described in Figure 12-24 might be to\nuse either a Fairy Tale Saga or a Parallel Saga pattern. These sagas rely on asynchronous eventual\nconsistency and state management rather than atomic distributed transactions with compensating\nupdates when errors occur. With these types of sagas, the user is less impacted by errors that might\noccur within the distributed transaction, because the error is addressed behind the scenes, without\nend-user involvement. Responsiveness is also better with the state management and eventual con-\nsistency approach, because the user does not have to wait for corrective action to be taken within\nthe distributed transaction. If we have issues with atomicity, we can investigate those patterns as\nalternatives.\u201d\n\u201cThanks\u2014that\u2019s a lot of material, but now I see why the architects made some of the decisions in the\nnew architecture,\u201d Addison said. 364 \n| \nChapter 12: Transactional Sagas", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 382", "position": 382, "chunk_type": "semantic", "token_estimate": 206}
{"text": "The concept of profile appears in both examples but with different values. In this sce\u2010: nario, the Customer Wishlist doesn\u2019t have internal access to the customer\u2019s name,\nonly a unique identifier. Thus, it needs access to a Customer Profile that maps the\nidentifier to the customer name. The Customer Profile, on the other hand, includes a\nlarge amount of information about the customer in addition to the name. As far as\nWishlist is concerned, the only interesting thing in Profile is the name. A common anti-pattern that some architects fall victim to is to assume that Wishlist\nmight eventually need all the other parts, so the architects include them in the con\u2010\ntract from the outset. This is an example of stamp coupling and an anti-pattern in\nmost cases, because it introduces breaking changes where they aren\u2019t needed, making\nthe architecture fragile yet providing little benefit. For example, if the Wishlist cares\nabout only the customer name from Profile, but the contract specifies every field in\nProfile (just in case), then a change in Profile that Wishlist doesn\u2019t care about causes a\ncontract breakage and coordination to fix. Keeping contracts at a \u201cneed to know\u201d\nlevel strikes a balance between semantic coupling and necessary information without\ncreating needless fragility in integration architecture. At the far end of the spectrum of contract coupling lie extremely loose contracts,\noften expressed as name-value pairs in formats like YAML or JSON, as illustrated in\nExample 13-4. Example 13-4. Name-value pairs in JSON\n{\n  \"name\": \"Mark\",\n  \"status\": \"active\",\n  \"joined\": \"2003\"\n}\nNothing but the raw facts in this example! No additional metadata, type information,\nor anything else, just name-value pairs. Using such loose contracts allows for extremely decoupled systems, often one of the\ngoals in architectures, such as microservices. However, the looseness of the contract\ncomes with trade-offs such as lack of contract certainty, verification, and increased\napplication logic. We illustrate in \u201cContracts in Microservices\u201d on page 372 how\narchitects resolve this problem by using contract fitness functions. Strict Versus Loose Contracts \n| \n369", "domains": ["Architectural Patterns and Styles"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 387", "position": 387, "chunk_type": "semantic", "token_estimate": 339}
{"text": "Trade-Offs Between Strict and Loose Contracts: When should an architect use strict contracts and when should they use looser ones? Like all the hard parts of architecture, no generic answer exists for this question, so it\nis important for architects to understand when each is most suitable. Strict contracts\nStricter contracts have a number of advantages, including these:\nGuaranteed contact fidelity\nBuilding schema verification within contracts ensures exact adherence to the val\u2010\nues, types, and other governed metadata. Some problem spaces benefit from tight\ncoupling for contract changes. Versioned\nStrict contracts generally require a versioning strategy to support two endpoints\nthat accept different values or to manage domain evolution over time. This\nallows gradual changes to integration points while supporting a selective number\nof past versions to make integration collaboration easier. Easier to verify at build time\nMany schema tools provide mechanisms to verify contracts at build time, adding\na level of type checking for integration points. Better documentation\nDistinct parameters and types provide excellent documentation with no\nambiguity. Strict contracts also have a few disadvantages:\nTight coupling\nBy our general definition of coupling, strict contracts create tight coupling\npoints. If two services share a strict contract and the contract changes, both serv\u2010\nices must change. Versioned\nThis appears in both advantages and disadvantages. While keeping distinct ver\u2010\nsions allows for precision, it can become an integration nightmare if the team\ndoesn\u2019t have a clear deprecation strategy or tries to support too many versions. 370 \n| \nChapter 13: Contracts", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 388", "position": 388, "chunk_type": "semantic", "token_estimate": 249}
{"text": "The trade-offs for strict contracts are summarized in Table 13-1.: Trade-Offs\nTable 13-1. Trade-offs for strict contracts\nAdvantage\nDisadvantage\nGuaranteed contract fidelity\nTight coupling\nVersioned\nVersioned\nEasier to verify at build time\nBetter documentation\nLoose contracts\nLoose contracts, such as name-value pairs, offer the least coupled integration points,\nbut they too have trade-offs, as summarized in Table 13-2. These are some advantages of loose contracts:\nHighly decoupled\nMany architects have a stated goal for microservices architectures that includes\nhigh levels of decoupling, and loose contracts provide the most flexibility. Easier to evolve\nBecause little or no schema information exists, these contracts can evolve more\nfreely. Of course, semantic coupling changes still require coordination across all\ninterested parties\u2014implementation cannot reduce semantic coupling\u2014but loose\ncontracts allow easier implementation evolution. Loose contracts also have a few disadvantages:\nContract management\nLoose contracts by definition don\u2019t have strict contract features, which may cause\nproblems such as misspelled names, missing name-value pairs, and other defi\u2010\nciencies that schemas would fix. Requires fitness functions\nTo solve the contract issues just described, many teams use consumer-driven\ncontracts as an architecture fitness function to make sure that loose contracts still\ncontain sufficient information for the contract to function. Strict Versus Loose Contracts \n| \n371", "domains": ["Architectural Patterns and Styles", "Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 389", "position": 389, "chunk_type": "semantic", "token_estimate": 205}
{"text": "Trade-Offs: Table 13-2. Trade-offs for loose contracts\nAdvantage\nDisadvantage\nHighly decoupled\nContract management\nEasier to evolve\nRequires fitness functions\nFor an example of the common trade-offs encountered by architects, consider the\nexample of contracts in microservice architectures. Contracts in Microservices\nArchitects must constantly make decisions about how services interact with one\nanother, what information to pass (the semantics), how to pass it (the implementa\u2010\ntion), and how tightly to couple the services. Coupling levels\nConsider two microservices with independent transactionality that must share\ndomain information such as Customer Address, shown in Figure 13-3. Figure 13-3. Two services that must share domain information about the customer\nThe architect could implement both services in the same technology stack and use a\nstrictly typed contract, either a platform-specific remote procedure protocol (such as\nRMI) or an implementation-independent one like gRPC, and pass the customer\ninformation from one to another with high confidence of contract fidelity. However,\nthis tight coupling violates one of the aspirational goals of microservices architec\u2010\ntures, where architects try to create decoupled services. 372 \n| \nChapter 13: Contracts", "domains": ["Architectural Patterns and Styles"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 390", "position": 390, "chunk_type": "semantic", "token_estimate": 178}
{"text": "Consider the alternative approach, where each service has its own internal represen\u2010: tation of Customer, and the integration uses name-value pairs to pass information\nfrom one service to another, as illustrated in Figure 13-4. Here, each service has its own bounded-context definition of Customer. When pass\u2010\ning information, the architect utilizes name-value pairs in JSON to pass the relevant\ninformation in a loose contract. Figure 13-4. Microservices with their own internal semantic representation can pass\nvalues in simple messages\nThis loose coupling satisfies many of the overarching goals of microservices. First, it\ncreates highly decoupled services modeled after bounded contexts, allowing each\nteam to evolve internal representations as aggressively as needed. Second, it creates\nimplementation decoupling. If both services start in the same technology stack, but\nthe team in the second decides to move to another platform, it likely won\u2019t affect the\nfirst service at all. All platforms in common use can produce and consume name-\nvalue pairs, making them the lingua franca of integration architecture. The biggest downside of loose contracts is contract fidelity\u2014as an architect, how can\nI know that developers pass the correct number and type of parameters for integra\u2010\ntion calls? Some protocols, such as JSON, include schema tools to allow architects to\noverlay loose contracts with more metadata. Architects can also use a style of archi\u2010\ntect fitness function called a consumer-driven contract. Consumer-driven contracts\nA common problem in microservices architectures is the seemingly contradictory\ngoals of loose coupling yet contract fidelity. One innovative approach that utilizes\nadvances in software development is a consumer-driven contract, common in micro\u2010\nservices architectures. In many architecture integration scenarios, a service decides what information to\nemit to other integration partners (a push model\u2014the service provider pushes a con\u2010\nStrict Versus Loose Contracts \n| \n373", "domains": ["Architectural Patterns and Styles", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 391", "position": 391, "chunk_type": "semantic", "token_estimate": 295}
{"text": "tract to consumers). The concept of a consumer-driven contract inverses that rela\u2010: tionship into a pull model; here, the consumer puts together a contract for the items\nthey need from the provider, and passes the contract to the provider, who includes it\nin their build and keeps the contract test green at all times. The contract encapsulates\nthe information the consumer needs from the provider. This may work for a network\nof interlocking requests that the Provider must honor, as illustrated in Figure 13-5. Figure 13-5. Consumer-driven contracts allow the provider and consumers to stay in\nsync via automated architectural governance\nIn this example, the team on the left provides bits of (likely) overlapping information\nto each of the consumer teams on the right. Each consumer creates a contract specify\u2010\ning required information and passes it to the provider, who includes their tests as part\nof a continuous integration or deployment pipeline. This allows each team to specify\nthe contract as strictly or loosely as needed while guaranteeing contract fidelity as\npart of the build process. Many consumer-driven contract testing tools provide facili\u2010\nties to automate build-time checks of contracts, providing another layer of benefit\nsimilar to stricter contracts. Consumer-driven contracts are quite common in microservices architecture because\nthey allow architects to solve the dual problems of loose coupling and governed inte\u2010\ngration. Trade-offs of consumer-driven contracts are shown in Table 13-3. Advantages of consumer-driven contracts are as follows:\nAllow loose contract coupling between services\nUsing name-value pairs is the loosest possible coupling between two services,\nallowing implementation changes with the least chance of breakage. 374 \n| \nChapter 13: Contracts", "domains": ["Architectural Patterns and Styles"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 392", "position": 392, "chunk_type": "semantic", "token_estimate": 270}
{"text": "Stamp Coupling: A common pattern and sometimes anti-pattern in distributed architectures is stamp\ncoupling, which describes passing a large data structure between services, but each\nservice interacts with only a small part of the data structure. Consider the example of\nfour services shown in Figure 13-6. Figure 13-6. Stamp coupling between four services\nEach service accesses (either reads, writes, or both) only a small portion of the data\nstructure passed between each service. This pattern is common when an industry-\nstandard document format exists, typically in XML. For example, the travel industry\nhas a global standard XML document format that specifies details about travel itin\u2010\neraries. Several systems that work with travel-related services pass the entire docu\u2010\nment around, updating only their relevant sections. Stamp coupling, however, is often an accidental anti-pattern, where an architect has\nover-specified the details in a contract that aren\u2019t needed or accidentally consumes far\ntoo much bandwidth for mundane calls. Over-Coupling via Stamp Coupling\nGoing back to our Wishlist and Profile Services, consider tying the two together with\na strict contract combined with stamp coupling, as illustrated in Figure 13-7. In this example, even though the Wishlist Service needs only the name (accessed via a\nunique ID), the architect has coupled Profile\u2019s entire data structure as the contract,\nperhaps in a misguided effort for future proofing. However, the negative side effect of\ntoo much coupling in contracts is brittleness. If Profile changes a field that Wishlist\ndoesn\u2019t care about, such as state, it still breaks the contract. 376 \n| \nChapter 13: Contracts", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 394", "position": 394, "chunk_type": "semantic", "token_estimate": 257}
{"text": "Stamp Coupling for Workflow Management: In Chapter 12, we covered a number of dynamic quantum communication patterns,\nincluding several that featured the coordination style of choreography. Architects tend\ntoward mediation for complex workflows for the many reasons we\u2019ve delineated. However, what if other factors, such as scalability, drive an architect toward a solution\nthat is both choreographed and complex? Architects can use stamp coupling to manage the workflow state between services,\npassing both domain knowledge and workflow state as part of the contract, as illus\u2010\ntrated in Figure 13-8. Figure 13-8. Using stamp coupling for workflow management\nIn this example, an architect designs the contract to include workflow information:\nstatus of the workflow, transactional state, and so on. As each domain service accepts\nthe contract, it updates its portion of the contract and state for the workflow, then\npasses it along. At the end of the workflow, the receiver can query the contract to\ndetermine success or failure, along with status and information such as error mes\u2010\nsages. If the system needs to implement transactional consistency throughout, then\ndomain services should rebroadcast the contract to previously visited services to\nrestore atomic consistency. Using stamp coupling to manage workflow does create higher coupling between serv\u2010\nices than nominal, but the semantic coupling must go somewhere\u2014remember, an\narchitect cannot reduce semantic coupling via implementation. However, in many\ncases, switching to choreography can improve throughput and scalability, making the\nchoice of stamp coupling over mediation an attractive one. Table 13-4 shows the\ntrade-offs for stamp coupling. 378 \n| \nChapter 13: Contracts", "domains": ["Design Patterns", "Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 396", "position": 396, "chunk_type": "semantic", "token_estimate": 258}
{"text": "CHAPTER 14: Managing Analytical Data\nTuesday, May 31, 13:23\nLogan and Dana (the data architect) were standing outside the big conference room,\nchatting after the weekly status meeting. \u201cHow are we going to handle analytical data in this new architecture?\u201d asked Dana. \u201cWe\u2019re splitting the databases into small parts, but we\u2019re going to have to glue all that\ndata back together for reporting and analytics. One of the improvements we\u2019re trying to\nimplement is better predictive planning, which means we are using more data science\nand statistics to make more strategic decisions. We now have a team that thinks about\nanalytical data, and we need a part of the system to handle this need. Are we going to have a data\nwarehouse?\u201d\nLogan said, \u201cWe looked into creating a data warehouse, and while it solved the consolidation prob-\nlem, it had a bunch of issues for us.\u201d\nMuch of this book has been concerned with how to analyze trade-offs within existing\narchitectural styles such as microservices. However, the techniques we highlight can\nalso be used to understand brand-new capabilities as they appear in the software\ndevelopment ecosystem; data mesh is an excellent example. Analytical and operational data have widely different purposes in modern architec\u2010\ntures (see \u201cThe Importance of Data in Architecture\u201d on page 4); much of this book\nhas dealt with the difficult trade-offs associated with operational data. When client/\nserver systems became popular and powerful enough for large enterprises, architects\nand database administrators looked for a solution that would allow specialized\nqueries. 381", "domains": ["Architectural Patterns and Styles", "Design Principles"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 399", "position": 399, "chunk_type": "semantic", "token_estimate": 255}
{"text": "Transformed to single schema: Often, operational schemas don\u2019t match the ones needed for reporting. For\nexample, an operational system needs to structure schemas and behavior around\ntransactions, whereas an analytical system is rarely OLTP data (see Chapter 1)\nbut typically deals with large amounts of data, for reporting, aggregations, and so\non. Thus, most data warehouses utilized a Star Schema to implement dimensional\nmodelling, transforming data from operational systems in differing formats into\nthe warehouse schema. To facilitate speed and simplicity, warehouse designers\ndenormalize the data to facilitate performance and simpler queries. Loaded into warehouse\nBecause the operational data resides in individual systems, the warehouse must\nbuild mechanisms to regularly extract the data, transform it, and place it in the\nwarehouse. Designers either used built-in relational database mechanisms like\nreplication or specialized tools to build translators from the original schema to\nthe warehouse schema. Of course, any changes to operational systems schemas\nmust be replicated in the transformed schema, making change coordination\ndifficult. Analysis done on the warehouse\nBecause the data \u201clives\u201d in the warehouse, all analysis is done there. This is desir\u2010\nable from an operational standpoint: the data warehouse machinery typically fea\u2010\ntured massively capable storage and compute, offloading the heavy requirements\ninto its own ecosystem. Used by data analysts\nThe data warehouse utilized data analysts, whose job included building reports\nand other business intelligence assets. However, building useful reports requires\ndomain understanding, meaning that domain expertise must reside in both the\noperational data system and the analytical systems, where query designers must\nuse the same data in a transformed schema to build meaningful reports and busi\u2010\nness intelligence. BI reports and dashboards\nThe output of the data warehouse included business intelligence reports, dash\u2010\nboards that provide analytical data, reports, and any other information to allow\nthe company to make better decisions. SQL-ish interface\nTo make it easier for DBAs to use, most data warehouse query tools provided\nfamiliar affordances, such as a SQL-like language for forming queries. One of the\nreasons for the data transformation step mentioned previously was to provide\nusers with a simpler way to query complex aggregations and other intelligence. Previous Approaches \n| \n383", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 401", "position": 401, "chunk_type": "semantic", "token_estimate": 360}
{"text": "1 Martin Fowler posted an influential message about the Data Lake pattern on his blog in 2015 at https://martin: fowler.com/bliki/DataLake.html. Tuesday, May 31, 13:33\n\u201cWe looked at creating a data warehouse, but realized that it fit better with older, mono-\nlithic kinds of architectures than modern distributed ones,\u201d said Logan. \u201cPlus, we have a\nton more machine learning cases now that we need to support.\u201d\n\u201cWhat about the data lake idea I\u2019ve been hearing about?\u201d asked Dana. \u201cI read a blog post\non Martin Fowler\u2019s site.1 It seems like it addresses a bunch of the issues with the data\nwarehouse, and it is more suitable for ML use cases.\u201d\n\u201cOh, yes, I read that post when it came out,\u201d Logan said. \u201cHis site is a treasure trove of good informa-\ntion, and that post came out right after the topic of microservices became hot. In fact, I first read\nabout microservices on that same site in 2014, and one of the big questions at the time was, How do\nwe manage reporting in architectures like that? The data lake was one of the early answers, mostly as a\ncounter to the data warehouse, which definitely won\u2019t work in something like microservices.\u201d\n\u201cWhy not?\u201d Dana asked. The Data Lake\nAs in many reactionary responses to the complexity, expense, and failures of the data\nwarehouse, the design pendulum swung to the opposite pole, exemplified by the Data\nLake pattern, intentionally the inverse of the Data Warehouse pattern. While it keeps\nthe centralized model and pipelines, it inverts the \u201ctransform and load\u201d model of the\ndata warehouse to a \u201cload and transform\u201d one. Rather than do the immense work of\ntransformation, the philosophy of the Data Lake pattern holds that, rather than do\nuseless transformations that may never be used, do no transformations, allowing\nbusiness users access to analytical data in its natural format, which typically required\ntransformation and massaging for their purpose. Thus, the burden of work was made\nreactive rather than proactive\u2014rather than do work that might not be needed, do\ntransformation work only on demand. The basic observation that many architects made was that the prebuilt schemas in\ndata warehouses were frequently not suited to the type of report or inquiry required\nby users, requiring extra work to understand the warehouse schema enough to craft a\nsolution. Additionally, many machine learning models work better with data \u201ccloser\u201d\nto the semi-raw format rather than a transformed version.", "domains": ["Architectural Patterns and Styles"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 404", "position": 404, "chunk_type": "semantic", "token_estimate": 406}
{"text": "discovery process, domain experts have the knowledge necessary to avoid acci\u2010: dental exposures, forcing them to reanalyze data in the lake. Still technically, not domain, partitioned\nThe current trend in software architecture shifts focus from partitioning a system\nbased on technical capabilities into ones based on domains, whereas both the\nData Warehouse and Data Lake patterns focus on technical partitioning. Gener\u2010\nally, architects design each of those solutions with distinct ingestion, transforma\u2010\ntion, loading, and serving partitions, each focused on a technical capability. Modern architecture patterns favor domain partitioning, encapsulating technical\nimplementation details. For example, the microservices architecture attempts to\nseparate services by domain rather than technical capabilities, encapsulating\ndomain knowledge, including data, inside the service boundary. However, both\nthe Data Warehouse and Data Lake patterns try to separate data as a separate\nentity, losing or obscuring important domain perspectives (such as PII data) in\nthe process. The last point is critical\u2014increasingly, architects design around domain rather than\ntechnical partitioning in architecture, and both previous approaches exemplify sepa\u2010\nrating data from its context. What architects and data scientists need is a technique\nthat preserves the appropriate kind of macro-level partitioning, yet supports a clean\nseparation of analytical from operational data. Table 14-2 lists the trade-offs for the\nData Lake pattern. Trade-Offs\nTable 14-2. Trade-offs for the Data Lake pattern\nAdvantage\nDisadvantage\nLess structured than data warehouse\nSometimes difficult to understand relationships\nLess up-front transformation\nRequires ad hoc transformations\nBetter suited to distributed architectures\nThe disadvantages around brittleness and pathological coupling of pipelines remain. Although they do less transformation in the Data Lake pattern, it is still common, as\nwell as data cleansing. The Data Lake pattern pushes data integrity testing, data quality, and other quality\nissues to downstream lake pipelines, which can create some of the same operational\nbottlenecks that manifest in the Data Warehouse pattern. 388 \n| \nChapter 14: Managing Analytical Data", "domains": ["Architectural Patterns and Styles", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 406", "position": 406, "chunk_type": "semantic", "token_estimate": 313}
{"text": "Because of both technical partitioning and the batch-like nature, solutions may suffer: from data staleness. Without careful coordination, architects either ignore the\nchanges in upstream systems, resulting in stale data, or allow the coupled pipelines to\nbreak. Tuesday, May 31, 14:43\n\u201cOK, so we can\u2019t use the data lake either!\u201d exclaimed Dana. \u201cWhat now?\u201d\n\u201cFortunately, some recent research has found a way to solve the problem of analytical\ndata with distributed architectures like microservices,\u201d replied Logan. \u201cIt adheres to the\ndomain boundaries we\u2019re trying to achieve, but also allows us to project analytical data\nin a way that the data scientists can use. And, it eliminates the PII problems our lawyers\nare worried about.\u201d\n\u201cGreat!\u201d Dana replied. \u201cHow does it work?\u201d\nThe Data Mesh\nObserving other trends in distributed architectures, Zhamak Dehghani and several\nother innovators derived the core idea of the Data Mesh pattern from domain-\noriented decoupling of microservices, service mesh, and sidecars (see \u201cSidecars and\nService Mesh\u201d on page 234), and applied it to analytical data, with modifications. As\nwe mentioned in Chapter 8, the Sidecar Pattern provides a nonentangling way to\norganize orthogonal coupling (see \u201cOrthogonal Coupling\u201d on page 238); the separa\u2010\ntion between operational and analytical data is another excellent example of just such\na coupling, but with more complexity than simple operational coupling. Definition of Data Mesh\nData mesh is a sociotechnical approach to sharing, accessing, and managing analyti\u2010\ncal data in a decentralized fashion. It satisfies a wide range of analytical use cases,\nsuch as reporting, ML model training, and generating insights. Contrary to the previ\u2010\nous architecture, it does so by aligning the architecture and ownership of the data\nwith the business domains and enabling a peer-to-peer consumption of data. Data mesh is founded on four principles:\nDomain ownership of data\nData is owned and shared by the domains that are most intimately familiar with\nthe data: the domains that either are originating the data, or are the first-class\nconsumers of the data. This architecture allows for distributed sharing and\naccessing the data from multiple domains and in a peer-to-peer fashion without\nThe Data Mesh \n| \n389", "domains": ["Architectural Patterns and Styles"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 407", "position": 407, "chunk_type": "semantic", "token_estimate": 355}
{"text": "any intermediary and centralized lake or warehouse, and without a dedicated: data team. Data as a product\nTo prevent siloing of data and encourage domains to share their data, data mesh\nintroduces the concept of data served as a product. It puts in place the organiza\u2010\ntional roles and success metrics necessary to ensure that domains provide their\ndata in a way that delights the experience of data consumers across the organiza\u2010\ntion. This principle leads to the introduction of a new architectural quantum\ncalled data product quantum, to maintain and serve discoverable, understanda\u2010\nble, timely, secure, and high-quality data to the consumers. This chapter introdu\u2010\nces the architectural aspect of the data product quantum. Self-serve data platform\nTo empower the domain teams to build and maintain their data products, data\nmesh introduces a new set of self-serve platform capabilities. The capabilities\nfocus on improving the experience of data product developers and consumers. It\nincludes features such as declarative creation of data products, discoverability of\ndata products across the mesh through search and browsing, and managing the\nemergence of other intelligent graphs, such as lineage of data and knowledge\ngraphs. Computational federated governance\nThis principle assures that despite decentralized ownership of the data,\norganization-wide governance requirements\u2014such as compliance, security, pri\u2010\nvacy, and quality of data, as well as interoperability of data products\u2014are met\nconsistently across all domains. Data mesh introduces a federated decision-\nmaking model composed of domain data product owners. The policies they for\u2010\nmulate are automated and embedded as code in each and every data product. The architectural implication of this approach to governance is a platform-\nsupplied embedded sidecar in each data product quantum to store and execute\nthe policies at the point of access: data read or write. Data mesh is a wide-ranging topic, fully covered in the book Data Mesh by Zhamak\nDehghani (O\u2019Reilly). In this chapter, we focus on the core architectural element, the\ndata product quantum. Data Product Quantum\nThe core tenet of the data mesh overlays modern distributed architectures such as\nmicroservices. Just as in the service mesh, teams build a data product quantum (DPQ)\nadjacent but coupled to their service, as illustrated in Figure 14-1. 390 \n| \nChapter 14: Managing Analytical Data", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 408", "position": 408, "chunk_type": "semantic", "token_estimate": 371}
{"text": "Figure 14-1. Structure of a data product quantum: In this example, the service Alpha contains both behavior and transactional (opera\u2010\ntional) data. The domain includes a data product quantum, which also contains code\nand data, and which acts as an interface to the overall analytical and reporting por\u2010\ntion of the system. The DPQ acts as an operationally independent but highly coupled\nset of behaviors and data. Several types of DPQs commonly exist in modern architectures:\nSource-aligned (native) DPQ\nProvides analytical data on behalf of the collaborating architecture quantum, typ\u2010\nically a microservice, acting as a cooperative quantum. Aggregate DQP\nAggregates data from multiple inputs, either synchronously or asynchronously. For example, for some aggregations, an asynchronous request may be sufficient;\nfor others, the aggregator DPQ may need to perform synchronous queries for a\nsource-aligned DPQ. The Data Mesh \n| \n391", "domains": ["Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 409", "position": 409, "chunk_type": "semantic", "token_estimate": 140}
{"text": "Fit-for-purpose DPQ: A custom-made DPQ to serve a particular requirement, which may encompass\nanalytical reporting, business intelligence, machine learning, or other supporting\ncapability. Each domain that also contributes to analysis and business intelligence includes a\nDPQ, as illustrated in Figure 14-2. Figure 14-2. The data product quantum acts as a separate but highly coupled adjunct to\na service\nHere, the DPQ represents a component owned by the domain team responsible for\nimplementing the service. It overlaps information stored in the database, and may\nhave interactions with some of the domain behavior asynchronously. The data\n392 \n| \nChapter 14: Managing Analytical Data", "domains": ["Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 410", "position": 410, "chunk_type": "semantic", "token_estimate": 101}
{"text": "product quantum also likely has behavior as well as data for the purposes of analytics: and business intelligence. Each data product quantum acts as a cooperative quantum for the service itself:\nCooperative quantum\nAn operationally separate quantum that communicates with its cooperator via\nasynchronous communication and eventual consistency, yet features tight con\u2010\ntract coupling with its cooperator and generally looser contract coupling to the\nanalytics quantum, the service responsible for reports, analysis, business intelli\u2010\ngence, and so on. While the two cooperating quanta are operationally independ\u2010\nent, they represent two sides of data: operational data in the quantum and\nanalytical data in the data product quantum. Some portion of the system will carry the responsibility for analytics and business\nintelligence, which will form its own domain and quantum. To operate, this analytical\nquantum has static quantum coupling to the individual data product quanta it needs\nfor information. This service may make either synchronous or asynchronous calls to\nthe DPQ, depending on the type of request. For example, some DPQs will feature a\nSQL interface to the analytical DPQ, allowing synchronous queries. Other require\u2010\nments may aggregate information across multiple DPQs. Data Mesh, Coupling, and Architecture Quantum\nBecause analytical reporting is probably a required feature of a solution, the DPQ and\nits communication implementation belong to the static coupling of an architecture\nquantum. For example, in a microservices architecture, the service plane must be\navailable, just as a message broker must be available if the design calls for messaging. However, like the Sidecar pattern in a service mesh, the DPQ should be orthogonal to\nimplementation changes within the service, and maintain a separate contract with the\ndata plane. From a dynamic quantum coupling standpoint, the data sidecar should always imple\u2010\nment one of the communication patterns that features both eventual consistency and\nasynchronicity: either the \u201cParallel Saga(aeo) Pattern\u201d on page 346 or \u201c\nAnthology\nSaga(aec) Pattern\u201d on page 349. In other words, a data sidecar should never include a\ntransactional requirement to keep operational and analytical data in sync, which\nwould defeat the purpose of using a DPQ for orthogonal decoupling. Similarly, com\u2010\nmunication to the data plane should genearlly be asynchronous, so as to have mini\u2010\nmal impact on the operational architecture characteristics of the domain service. When to Use Data Mesh\nLike all things in architecture, this pattern has trade-offs associated with it, as shown\nin Table 14-3. The Data Mesh \n| \n393", "domains": ["Architectural Patterns and Styles", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 411", "position": 411, "chunk_type": "semantic", "token_estimate": 404}
{"text": "Trade-Offs: Table 14-3. Trade-offs for the Data Mesh pattern\nAdvantage\nDisadvantage\nHighly suitable for microservices architectures\nRequires contract coordination\nwith data product quantum\nFollows modern architecture principles and engineering\npractices\nRequires asynchronous\ncommunication and eventual\nconsistency\nAllows excellent decoupling between analytical and\noperational data\nCarefully formed contracts allow loosely coupled evolution\nof analytical capabilities\nIt is most suitable in modern distributed architectures such as microservices with\nwell-contained transactionality and good isolation between services. It allows domain\nteams to determine the amount, cadence, quality, and transparency of the data con\u2010\nsumed by other quanta. It is more difficult in architectures where analytical and operational data must stay in\nsync at all times, which presents a daunting challenge in distributed architectures. Finding ways to support eventual consistency, perhaps with very strict contracts,\nallows many patterns that don\u2019t impose other difficulties. Data mesh is an outstanding example of the constant incremental evolution that\noccurs in the software development ecosystem; new capabilities create new perspec\u2010\ntives, which in turn help address some persistent headaches from the past, such as the\nartificial separation of domain from data, both operational and analytical. Sysops Squad Saga: Data Mesh\nFriday, June 10, 09:55\nLogan, Dana, and Addison met in the big conference room, which often had leftover\nsnacks (or, this early in the day, breakfast) from previous meetings. \u201cI just returned from a meeting with our data scientists, and they are trying to figure out\na way we can solve a long-term problem for us\u2014we need to become data-driven in\nexpert supply planning, for skill sets demand for different geographical locations at dif-\nferent points in time. That capability will help recruitment, training, and other supply-\nrelated functions,\u201d said Logan. 394 \n| \nChapter 14: Managing Analytical Data", "domains": ["Architectural Patterns and Styles"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 412", "position": 412, "chunk_type": "semantic", "token_estimate": 289}
{"text": "\u201cI haven\u2019t been involved in much of the data mesh implementation\u2014how far along are we?\u201d asked: Addison. \u201cEach new service we\u2019ve implemented includes a DPQ. The domain team is responsible for running\nand maintaining the DQP cooperative quantum for their service. We\u2019ve only just started. We\u2019re grad-\nually building out the capabilities as we identify the needs. I have a picture of the Ticket Manage-\nment Domain in Figure 14-3.\u201d\nFigure 14-3. Ticket Management Domain, including two services with their own DPQs,\nwith a Tickets DPQ\nLogan said, \u201cTickets DPQ is its own architecture quantum, and acts as an aggregation point for a\ncouple of different ticket views that other systems care about.\u201d\n\u201cHow much does each team have to build versus already supplied?\u201d Addison asked. \u201cI can answer that,\u201d said Dana. \u201cThe data mesh platform team is supplying the data users and data\nproduct developers with a set of self-serve capabilities. That allows any team that wants to build a\nnew analytical use case to search and find the data products of choice within existing architecture\nquanta, directly connect to them, and start using them. The platform also supports domains that\nwant to create new data products. The platform continuously monitors the mesh for any data prod-\nuct downtimes, or incompatibility with the governance policies and informs the domain teams to\ntake actions.\u201d\nLogan said, \u201cThe domain data product owners in collaboration with security, legal, risk, and compli-\nance SMEs, as well as the platform product owners, have formed a global federated governance\ngroup, which decides on aspects of the DPQs that must be standardized, such as their data-sharing\ncontracts, modes of asynchronous transport of data, access control, and so on. The platform team,\nover a span of time, enriches the DPQ\u2019s sidecar with new policy execution capabilities and upgrades\nthe sidecars uniformly across the mesh.\u201d\nSysops Squad Saga: Data Mesh \n| \n395", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 413", "position": 413, "chunk_type": "semantic", "token_estimate": 314}
{"text": "\u201cWow, we\u2019re further along that I thought,\u201d said Dana. \u201cWhat data do we need in order to supply the: information for the expert supply problem?\u201d\nLogan replied, \u201cIn collaboration with the data scientists, we have determined what information we\nneed to aggregate. It looks like we have the correct information: the Tickets DPQ serves the long-\nterm view of all tickets raised and resolved, the User Maintenance DPQ provides daily snapshots for\nall expert profiles, and the Survey DPQ provides a log of all survey results from customers.\u201d\n\u201cAwesome,\u201d said Addison. \u201cPerhaps we should create a new DPQ named something like Experts Sup-\nply DPQ, which takes asynchronous inputs from those three DPQs? Its first product can be called\nsupply recommendations, which uses an ML model trained using data aggregated from DPQs in sur-\nveys, tickets, and maintenance domains. The Experts Supply DPQ will provide daily recommenda-\ntions data, as new data becomes available about tickets, surveys and expert profiles. The overall\ndesign looks like Figure 14-4.\u201d\nFigure 14-4. Implementing the Experts Supply DPQ\n\u201cOK, that looks perfectly reasonable,\u201d said Dana. \u201cThe services are already done; we just have to make\nsure the specific endpoints exist in each of the source DPQs, and implement the new Experts Supply\nDPQ.\u201d\n\u201cThat\u2019s right,\u201d said Logan. \u201cOne thing we need to worry about, though\u2014trend analysis depends on\nreliable data. What happens if one of the feeder source systems returns incomplete information for a\nchunk of time? Won\u2019t that throw off the trend analysis?\u201d\n396 \n| \nChapter 14: Managing Analytical Data", "domains": ["Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 414", "position": 414, "chunk_type": "semantic", "token_estimate": 257}
{"text": "\u201cThat\u2019s correct\u2014no data for a time period is better than incomplete data, which makes it seem like: there was less traffic than there was,\u201d Dana said. \u201cWe can just exempt an empty day, as long as it\ndoesn\u2019t happen much.\u201d\n\u201cOK, Addison, you know what than means, right?\u201d Logan said. \u201cYes, I certainly do\u2014an ADR that specifies complete information or none, and a fitness function to\nmake sure we get complete data.\u201d\nADR: Ensure that Expert Supply DPQ Sources Supply an Entire Day\u2019s Data or None\nContext\nThe Expert Supply DPQ performs trend analysis over specified time periods. Incomplete\ndata for a particular day will skew trend results and should be avoided. Decision\nWe will ensure that each data source for the Expert Supply DPQ receives complete snap-\nshots for daily trends or no data for that day, allowing data scientists to exempt that day. The contracts between source feeds and the Expert Supply DPQ should be loosely cou-\npled to prevent brittleness. Consequences\nIf too many days become exempt because of availability or other problems, accuracy of\ntrends will be negatively impacted. Fitness functions:\nComplete daily snapshot. Check timestamps on messages as they arrive. Given typical mes-\nsage volume, any gap of more than one minute indicates a gap in processing, marking\nthat day as exempt. Consumer-driven contract fitness function for Ticket DPQ and Expert Supply DPQ. To ensure\nthat internal evolution of the Ticket Domain doesn\u2019t break the Experts Supply DPQ. Sysops Squad Saga: Data Mesh \n| \n397", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 415", "position": 415, "chunk_type": "semantic", "token_estimate": 250}
{"text": "Finding Entangled Dimensions: An architect\u2019s first step in this process is to discover what dimensions are entangled,\nor braided, together. This is unique within a particular architecture but discoverable\nby experienced developers, architects, operations folks, and other roles familiar with\nthe existing overall ecosystem and its capabilities and constraints. Coupling\nThe first part of the analysis answers this question for an architect: how are parts\nwithin an architecture coupled to one another? The software development world has\na wide variety of definitions of coupling, but we use the simplest, most intuitive ver\u2010\nsion for this exercise: if someone changes X, will it possibly force Y to change? In Chapter 2, we described the concept of the static coupling between architecture\nquanta, which provides a comprehensive structural diagram of technical coupling. No generic tool exists to build this because each architecture is unique. However,\nwithin an organization, a development team can build a static coupling diagram,\neither manually or via automation. For example, to create a static coupling diagram for a microservice within an archi\u2010\ntecture, an architect needs to gather the following details:\n\u2022 Operating systems/container dependencies\n\u2022 Dependencies delivered via transitive dependency management (frameworks,\nlibraries, etc.) \u2022 Persistence dependencies on databases, search engines, cloud environments, etc. \u2022 Architecture integration points required for the service to bootstrap itself\n\u2022 Messaging infrastructure (such as a message broker) required to enable commu\u2010\nnication to other quanta\nThe static coupling diagram does not consider other quanta whose only coupling\npoint is workflow communication with this quantum. For example, if an AssignTicket\nService cooperates with the ManageTicket within a workflow but has no other cou\u2010\npling points, they are statically independent (but dynamically coupled during the\nactual workflow). Teams that already have most of their environments built via automation can build\ninto that generative mechanism an extra capability to document the coupling points\nas the system builds. For this book, our goal was to measure the trade-offs in distributed architecture cou\u2010\npling and communication. To determine what became our three dimensions of\ndynamical quantum coupling, we looked at hundreds of examples of distributed\nFinding Entangled Dimensions \n| \n401", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 419", "position": 419, "chunk_type": "semantic", "token_estimate": 354}
{"text": "architectures (both microservices and others) to determine the common coupling: points. In other words, all the examples we looked at were sensitive to changes to the\ndimensions of communication, consistency, and coordination. This process highlights the importance of iterative design in architecture. No archi\u2010\ntect is so brilliant that their first draft is always perfect. Building sample topologies for\nworkflows (much as we do in this book) allows an architect or team to build a matrix\nview of trade-offs, allowing quicker and more thorough analysis than ad hoc\napproaches. Analyze Coupling Points\nOnce an architect or team has identified the coupling points they want to analyze, the\nnext step is to model the possible combinations in a lightweight way. Some of the\ncombinations may not be feasible, allowing the architect to skip modeling those com\u2010\nbinations. The goal of the analysis is to determine what forces the architect needs to\nstudy\u2014in other words, which forces require trade-off analysis? For example, for our\narchitecture quantum dynamic coupling analysis, we chose coupling, complexity,\nresponsiveness/availability, and scale/elasticity as our primary trade-off concerns, in\naddition to analyzing the three forces of communication, consistency, and coordina\u2010\ntion, as shown in the ratings table for the \u201cParallel Saga(aeo) Pattern\u201d on page 346,\nappearing again in Table 15-1. Table 15-1. Ratings for the Parallel Saga pattern\nParallel Saga\nRatings\nCommunication\nAsynchronous\nConsistency\nEventual\nCoordination\nCentralized\nCoupling\nLow\nComplexity\nLow\nResponsiveness/availability\nHigh\nScale/elasticity\nHigh\nWhen building these ratings lists, we considered each design solution (our named\npatterns) in isolation, combining them only at the end to see the differences, shown\nin Table 15-2. 402 \n| \nChapter 15: Build Your Own Trade-Off Analysis", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 420", "position": 420, "chunk_type": "semantic", "token_estimate": 274}
{"text": "Trade-Offs: Table 15-2. Consolidated comparison of dynamic coupling patterns\nPattern\nCoupling level Complexity\nResponsiveness/\navailability\nScale/elasticity\nEpic Saga\nVery high\nLow\nLow\nVery Low\nPhone Tag Saga\nHigh\nHigh\nLow\nLow\nFairy Tale Saga\nHigh\nVery low\nMedium\nHigh\nTime Travel Saga\nMedium\nLow\nMedium\nHigh\nFantasy Fiction Saga\nHigh\nHigh\nLow\nLow\nHorror Story\nMedium\nVery high\nLow\nMedium\nParallel Saga\nLow\nLow\nHigh\nHigh\nAnthology Saga\nVery low\nHigh\nHigh\nVery high\nOnce we had analyzed each pattern independently, we created a matrix to compare\nthe characteristics, leading to interesting observations. First, notice the direct inverse\ncorrelation between coupling level and scale/elasticity: the more coupling present in\nthe pattern, the worse its scalability. This intuitively makes sense; the more services\ninvolved in a workflow, the more difficult for an architect to design for scale. Second, we made a similar observation around responsiveness/availability and cou\u2010\npling level, which is not quite as direct as the preceding correlation but also signifi\u2010\ncant: higher coupling leads to less responsiveness and availability because the more\nservices involved in a workflow, the more likely the entire workflow will fail based on\na service failure. This analysis technique exemplifies iterative architecture. No architect, regardless of\ntheir cleverness, can instantly understand the nuances of a truly unique situation\u2014\nand these nuances constantly present themselves in complex architectures. Building a\nmatrix of possibilities informs the modeling exercises an architect might want to do\nin order to study the implications of permutating one or more dimensions to see the\nresulting effect. Assess Trade-Offs\nOnce you have built a platform that allows iterative \u201cwhat if\u201d scenarios, focus on the\nfundamental trade-offs for a given situation. For example, we focused on synchro\u2010\nnous versus asynchronous communication, a choice that creates a host of possibilities\nand restrictions\u2014everything in software architecture is a trade-off. Thus, choosing a\nfundamental dimension like synchronicity first limits future choices. With that\nFinding Entangled Dimensions \n| \n403", "domains": ["Design Principles", "Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 421", "position": 421, "chunk_type": "semantic", "token_estimate": 321}
{"text": "dimension now fixed, perform the same kind of iterative analysis on subsequent deci\u2010: sions encouraged or forced by the first. An architect team can iterate on this process\nuntil they have solved the difficult decisions\u2014in other words, decisions with entan\u2010\ngled dimensions. What\u2019s left is design. Trade-Off Techniques\nOver time, the authors have created a number of trade-off analyses and have built up\nsome advice on how to approach them. Qualitative Versus Quantative Analysis\nYou may have noticed that virtually none of our trade-off tables are quantitative\u2014\nbased on numbers\u2014but are rather qualitative\u2014measuring the quality of something\nrather than the quantity, which is necessary because two architectures will always dif\u2010\nfer enough to prevent true quantitative comparisons. However, using statistical analy\u2010\nsis over a large data set allows reasonable qualitative analysis. For example, when comparing the scalability of patterns, we looked at multiple differ\u2010\nent implementations of communication, consistency, and coordination combinations,\nassessing scalability in each case, and allowing us to build the comparative scale\nshown in Table 15-2. Similarly, architects within a particular organization can carry out the same exercise,\nbuilding a dimensional matrix of coupled concerns, and look at representative exam\u2010\nples (either within the existing organization or localized spikes to test theories). We recommend you hone the skill of performing qualitative analysis, as few opportu\u2010\nnities for true quantitative analysis exist in architecture. MECE Lists\nIt is important for architects to be sure they are comparing the same things rather\nthan wildly different ones. For example, it\u2019s not a valid comparison to compare a sim\u2010\nple message queue to an enterprise service bus, which contains a message queue but\ndozens of other components as well. A useful concept borrowed from the technology strategy world to help architects get\nthe correct match of things to compare is a MECE list, an acronym for mutually exclu\u2010\nsive, collectively exhaustive:\nMutually exclusive\nNone of the capabilities can overlap between the compared items. As in the pre\u2010\nceding example, it is invalid to compare a message queue to an entire ESB\nbecause they aren\u2019t really the same category of thing. If you want to compare just\n404 \n| \nChapter 15: Build Your Own Trade-Off Analysis", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 422", "position": 422, "chunk_type": "semantic", "token_estimate": 364}
{"text": "the messaging capabilities absent the other parts, that reduces the comparison to: two mutually comparable things. Collectively exhaustive\nThis suggests that you\u2019ve covered all the possibilities in the decision space, and\nthat you haven\u2019t left out any obvious capabilities. For example, if a team of archi\u2010\ntects is evaluating high-performance message queues and consider only an ESB\nand simple message queue but not Kafka, they haven\u2019t considered all the possibil\u2010\nities in the space. The goal of a MECE list is to cover a category space completely, with no holes or\noverlaps, as shown pictorially in Figure 15-1. Figure 15-1. A MECE list is mutually exclusive and collectively exhaustive\nThe software development ecosystem constantly evolves, uncovering new capabilities\nalong the way. When making a decision with long-term implications, an architect\nshould make sure a new capability hasn\u2019t just arrived that changes the criteria. Ensur\u2010\ning that comparison criteria is collectively exhaustive encourages that exploration. The \u201cOut-of-Context\u201d Trap\nWhen assessing trade-offs, architects must make sure to keep the decision in context;\notherwise, external factors will unduly affect their analysis. Often, a solution has\nmany beneficial aspects, but lacks critical capabilities that prevent success. Architects\nneed to make sure they balance the correct set of trade-offs, not all available ones. For example, perhaps an architect is trying to decide whether to use a shared service\nor shared library for common functionality within a distributed architecture, as illus\u2010\ntrated in Figure 15-2. Trade-Off Techniques \n| \n405", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 423", "position": 423, "chunk_type": "semantic", "token_estimate": 243}
{"text": "Model Relevant Domain Cases: Architects shouldn\u2019t make decisions in a vacuum, without relevant drivers that add\nvalue to the specific solution. Adding those domain drivers back to the decision pro\u2010\ncess can help the architect filter the available options and focus on the really impor\u2010\ntant trade-offs. For example, consider this decision by an architect as to whether to create a single\npayment service or a separate service for each payment type, as illustrated in Figure\n15-5. Figure 15-5. Choosing between a single payment service or one per payment type\nAs we discussed in Chapter 7, architects can choose from a number of integrators and\ndisintegrators to assist this decision. However, those forces are generic\u2014an architect\nmay add more nuance to the decision by modeling some likely scenarios. For example, consider the first scenario, illustrated in Figure 15-6, to update a credit\ncard processing service. In this scenario, having separate services provides better maintainability, testability,\nand deployability, all based on quantum-level isolation of the services. However, the\ndownside of separate services is often duplicated code to prevent static quantum cou\u2010\npling between the services, which damages the benefit of having separate services. 408 \n| \nChapter 15: Build Your Own Trade-Off Analysis", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 426", "position": 426, "chunk_type": "semantic", "token_estimate": 201}
{"text": "Figure 15-6. Scenario 1: update credit card processing service: In the second scenario, the architect models what happens when the system adds a\nnew payment type, as shown in Figure 15-7. Figure 15-7. Scenario 2: adding a payment type\nThe architect adds a reward points payment type to see what impact it has on the\narchitecture characteristics of interest, highlighting extensibility as a benefit of sepa\u2010\nrate services. So far, separate services look appealing. However, as in many cases, more complex workflows highlight the difficult parts of\nthe architecture, as shown in the third scenario in Figure 15-8. In this scenario, the architect starts gaining insight into the real trade-offs involved in\nthis decision. Utilizing separate services requires coordination for this workflow, best\nhandled by an orchestrater. However, as we discussed in Chapter 11, moving to an\norchestrator likely impacts performance negatively and makes data consistency more\nof a challenge. The architect could avoid the orchestrator, but the workflow logic\nmust reside somewhere\u2014remember, semantic coupling can only be increased via\nimplementation, never decreased. Trade-Off Techniques \n| \n409", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 427", "position": 427, "chunk_type": "semantic", "token_estimate": 177}
{"text": "Figure 15-8. Scenario 3: using multiple types for payment: Having modeled these three scenarios, the architect realizes that the real trade-off\nanalysis comes down to which is more important: performance and data consistency\n(a single payment service) or extensibility and agility (separate services). Thinking about architecture problems in the generic and abstract gets an architect\nonly so far. As architecture generally evades generic solutions, it is important for\narchitects to build their skills in modeling relevant domain scenarios to home in on\nbetter trade-off analysis and decisions. Prefer Bottom Line over Overwhelming Evidence\nIt\u2019s easy for architects to build up an enormous amount of information in pursuit of\nlearning all the facets of a particular trade-off analysis. Additionally, anyone who\nlearns something new generally wants to tell others about it, especially if they think\nthe other party will be interested. However, many of the technical details that archi\u2010\ntects uncover are arcane to nontechnical stakeholders, and the amount of detail may\noverwhelm their ability to add meaningful insight into the decision. Rather than show all the information they have gathered, an architect should reduce\nthe trade-off analysis to a few key points, which are sometimes aggregates of individ\u2010\nual trade-offs. Consider the common problem an architect might face in a microservices architec\u2010\nture about the choice of synchronous or asynchronous communication, illustrated in\nFigure 15-9. 410 \n| \nChapter 15: Build Your Own Trade-Off Analysis", "domains": ["Architectural Patterns and Styles", "Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 428", "position": 428, "chunk_type": "semantic", "token_estimate": 234}
{"text": "Avoiding Snake Oil and Evangelism: One unfortunate side effect of enthusiasm for technology is evangelism, which should\nbe a luxury reserved for tech leads and developers but tends to get architects in\ntrouble. Trouble comes because, when someone evangelizes a tool, technique, approach, or\nanything else people build enthusiasm for, they start enhancing the good parts and\ndiminishing the bad parts. Unfortunately, in software architecture, the trade-offs\nalways eventually return to complicate things. An architect should also be wary of any tool or technique that promises any shocking\nnew capabilities, which come and go on a regular basis. Always force evangelists for\nthe tool or technique to provide an honest assessment of the good and bad\u2014nothing\nin software architecture is all good\u2014which allows a more balanced decision. For example, consider an architect who has had success in the past with a particular\napproach and becomes an evangelist for it, as illustrated in Figure 15-10. Figure 15-10. An architect evangelist who thinks they have found a silver bullet\nThis architect has likely worked on problems in the past where extensibility was a key\ndriving architecture characteristic and believes that capability will always drive the\ndecision process. However, solutions in architecture rarely scale outside narrow con\u2010\nfines of a particular problem space. On the other hand, anecdotal evidence is often\ncompelling. How do you get to the real trade-off hiding behind the knee-jerk\nevangelism? While experience is useful, scenario analysis is one of an architect\u2019s most powerful\ntools to allow iterative design without building whole systems. By modeling likely\nscenarios, an architect can discover if a particular solution will, in fact, work well. 412 \n| \nChapter 15: Build Your Own Trade-Off Analysis", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 430", "position": 430, "chunk_type": "semantic", "token_estimate": 280}
{"text": "In the example shown in Figure 15-10, an existing system uses a single topic to: broadcast changes. The architect\u2019s goal is to add bid history to the workflow\u2014should\nthe team keep the existing publish-and-subscribe approach or move to point-to-point\nmessaging for each consumer? To discover the trade-offs for this specific problem, the architect should model likely\ndomain scenarios using the two topologies. Adding bid history to the existing\npublish-and-subscribe design appears in Figure 15-11. Figure 15-11. Scenario 1: Adding bid history to the existing topic\nWhile this solution works, it has issues. First, what if the teams need different con\u2010\ntracts for each consumer? Building a single large contract that encompasses every\u2010\nthing implements the \u201cStamp Coupling for Workflow Management\u201d on page 378\nanti-pattern; forcing each team to unify on a single contract creates an accidental\ncoupling point in the architecture\u2014if one team changes its required information, all\nthe teams must coordinate on that change. Second, what about data security? Using a\nsingle publish-and-subscribe topic, each consumer has access to all the data, which\ncan create both security problems and PII (Personally Identifiable Information, dis\u2010\ncussed in Chapter 14) issues as well. Third, the architect should consider the opera\u2010\ntional architecture characteristic differences between the different consumers. For\nexample, if the operations team wanted to monitor queue depth and use auto-scaling\nfor bid capture and bid tracking but not for the other two services, using a single topic\nprevents that capability\u2014the consumers are now operationally coupled together. To mitigate these shortcomings, the architect should model the alternative solution to\nsee if it addresses the preceding problems (and doesn\u2019t introduce new intractable\nones). The individual queue version appears in Figure 15-12. Trade-Off Techniques \n| \n413", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 431", "position": 431, "chunk_type": "semantic", "token_estimate": 284}
{"text": "Figure 15-12. Using individual queues to capture bid information: Each part of this workflow (bid capture, bid tracking, bid analytics, and bid history)\nutilizes its own message queues and addresses many of the preceding problems. First,\neach consumer can have their own contract, decoupling the consumers from each\nother. Second, security access and control of data resides within the contract between\nthe producer and each consumer, allowing differences in both information and rate\nof change. Third, each queue can now be monitored and scaled independently. Of course, by this point in the book, you should realize that the point-to-point based\nsystem isn\u2019t perfect either but offers a different set of trade-offs. Once the architect has modeled both approaches, it seems that the differences boil\ndown to the choices shown in Table 15-4. Trade-Offs\nTable 15-4. Trade-offs between point-to-point versus publish-and-subscribe\nmessaging\nPoint-to-point\nPublish-and-subscribe\nAllows heterogeneous contracts\nExtensibility (easy to add new consumers)\nMore granular security access and data control\nIndividual operational profiles per consumer\n414 \n| \nChapter 15: Build Your Own Trade-Off Analysis", "domains": ["Software Quality Attributes"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 432", "position": 432, "chunk_type": "semantic", "token_estimate": 173}
{"text": "In the end, the architect should consult with interested parties (operations, enterprise: architects, business analysts, and so on) to determine which of these sets of trade-offs\nis more important. Sometimes an architect doesn\u2019t choose to evangelize something but is rather coerced\ninto playing an opposite foil, particularly for something that has no clear advantage. Technologies develop fans, sometimes fervent ones, who tend to downplay disadvan\u2010\ntages and enhance upsides. For example, recently a tech lead on a project tried to wrangle one of the authors into\nan argument about monorepo versus trunk-based development. Both have good and\nbad aspects, a classic software architecture decision. The tech lead was a fervent sup\u2010\nporter of the monorepo approach, and tried to force the author to take the opposing\nposition\u2014it\u2019s not an argument if two sides don\u2019t exist. Instead, the architect pointed out that it was a trade-off, gently explaining that many\nof the advantages touted by the tech lead required a level of discipline that had never\nmanifested within the team in the past, but will surely improve. Rather than be forced into taking the opposing position, instead the architect forced a\nreal-world trade-off analysis, not based on generic solutions. The architect agreed to\ntry the Monorepo approach but also gather metrics to make sure that the negative\naspects of the solution didn\u2019t manifest. For example, one of the damaging anti-\npatterns they wanted to avoid was accidental coupling between two projects because\nof repository proximity, so the architect and team built a series of fitness functions to\nensure that, while technically possible to create a coupling point, the fitness function\nprevented it. Don\u2019t allow others to force you into evangelizing something\u2014bring\nit back to trade-offs. We advise architects to avoid evangelizing and to try to become the objective arbiter\nof trade-offs. An architect adds real value to an organization not by chasing silver\nbullet after silver bullet but rather by honing their skills at analyzing the trade-offs as\nthey appear. Trade-Off Techniques \n| \n415", "domains": ["Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 433", "position": 433, "chunk_type": "semantic", "token_estimate": 333}
{"text": "Sysops Squad Saga: Epilogue: Monday, June 20, 16:55\n\u201cOK, I think I finally get it. We can\u2019t really rely on generic advice for our architecture\u2014it\u2019s\ntoo different from all the others. We have to do the hard work of trade-off analysis\nconstantly.\u201d\n\u201cThat\u2019s correct. But it\u2019s not a disadvantage\u2014it\u2019s an advantage. Once we all learn how to\nisolate dimensions and perform trade-off analysis, we\u2019re learning concrete things about\nour architecture. Who cares about other, generic ones? If we can boil the number of\ntrade-offs for a problem down to a small enough number to actually model and test\nthem, we gain invaluable knowledge about our ecosystem. You know, structural engineers have\nbuilt a ton of math and other predictive tools, but building their stuff is difficult and expensive. Soft-\nware is a lot\u2026well, softer. I\u2019ve always said that testing is the engineering rigor of software development. While we don\u2019t have the kind of math other engineers have, we can incrementally build and test our\nsolutions, allowing much more flexibility and leveraging the advantage of a more flexible medium. Testing with objective outcomes allows our trade-off analyses to go from qualitative to quantitative\n\u2014from speculation to engineering. The more concrete facts we can learn about our unique ecosys-\ntem, the more precise our analysis can become.\u201d\n\u201cYeah, that makes sense. Want to go to the after-work gathering to celebrate the big turnaround?\u201d\n\u201cSure.\u201d\n416 \n| \nChapter 15: Build Your Own Trade-Off Analysis", "domains": ["Design Patterns"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 434", "position": 434, "chunk_type": "semantic", "token_estimate": 241}
{"text": "APPENDIX A: Concept and Term References\nIn this book, we\u2019ve made several references to terms or concepts that are explained in\ndetail in our previous book, Fundamentals of Software Architecture. The following is a\nforward reference for those terms and concepts:\nCyclomatic complexity: Chapter 6, page 81\nComponent coupling: Chapter 7, page 92\nComponent cohesion: Chapter 7, page 93\nTechnical versus domain partitioning: Chapter 8, page 103\nLayered architecture: Chapter 10, page 135\nService-based architecture: Chapter 13, page 163\nMicroservices architecture: Chapter 12, page 151\n417", "domains": ["Architectural Patterns and Styles", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 435", "position": 435, "chunk_type": "semantic", "token_estimate": 86}
{"text": "APPENDIX B: Architecture Decision Record References\nEach Sysops Squad decision in this book was accompanied by a corresponding Archi\u2010\ntecture Decision Record. We consolidated all the ADRs here for easy reference:\n\u201cADR: A short noun phrase containing the architecture decision\u201d on page 5\n\u201cADR: Migrate Sysops Squad Application to a Distributed Architecture\u201d on page 61\n\u201cADR: Migration Using the Component-Based Decomposition Approach\u201d on page 79\n\u201cADR: Use of Document Database for Customer Survey\u201d on page 184\n\u201cADR: Consolidated Service for Ticket Assignment and Routing\u201d on page 211\n\u201cADR: Consolidated Service for Customer-Related Functionality\u201d on page 215\n\u201cADR: Using a Sidecar for Operational Coupling\u201d on page 241\n\u201cADR: Use of a Shared Library for Common Ticketing Database Logic\u201d on page 247\n\u201cADR: Single Table Ownership for Bounded Contexts\u201d on page 280\n\u201c\nADR: Survey Service Owns the Survey Table\u201d on page 281\n\u201c\nADR: Use of In-Memory Replicated Caching for Expert Profile Data\u201d on page 297\n\u201cADR: Use Orchestration for Primary Ticket Workflow\u201d on page 321\n\u201c\nADR: Loose Contract for Sysops Squad Expert Mobile Application\u201d on page 380\n419", "domains": ["Design Principles", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 437", "position": 437, "chunk_type": "semantic", "token_estimate": 179}
{"text": "Table 8-4, \u201cTrade-offs for the Sidecar pattern / service mesh technique\u201d on page 239: Table 9-1, \u201cJoint ownership table split technique trade-offs\u201d on page 256\nTable 9-2, \u201cJoint ownership data-domain technique trade-offs\u201d on page 258\nTable 9-3, \u201cJoint ownership delegate technique trade-offs\u201d on page 261\nTable 9-4, \u201cJoint ownership service consolidation technique trade-offs\u201d on page 262\nTable 9-5, \u201cBackground synchronization pattern trade-offs\u201d on page 272\nTable 9-6, \u201cOrchestrated request-based pattern trade-offs\u201d on page 277\nTable 9-7, \u201cEvent-based pattern trade-offs\u201d on page 279\nTable 10-1, \u201cTrade-offs for the Interservice Communication data access pattern\u201d on\npage 286\nTable 10-2, \u201cTrade-offs for the Column Schema Replication data access pattern\u201d on\npage 288\nTable 10-3, \u201cTrade-offs associated with the replicated caching data access pattern\u201d on\npage 293\nTable 10-4, \u201cTrade-offs associated with the data domain data access pattern\u201d on page\n295\nTable 11-1, \u201cTrade-offs for orchestration\u201d on page 306\nTable 11-2, \u201cTrade-offs for the Front Controller pattern\u201d on page 312\nTable 11-3, \u201cStateless choreography trade-offs\u201d on page 313\nTable 11-4, \u201cStamp coupling trade-offs\u201d on page 314\nTable 11-5, \u201cTrade-offs for the choreography communication style\u201d on page 315\nTable 11-6, \u201cTrade-off between orchestration and choreography for ticket workflow\u201d\non page 319\nTable 11-7, \u201cUpdated trade-offs between orchestration and choreography for ticket\nworkflow\u201d on page 320\nTable 11-8, \u201cFinal trade-offs between orchestration and choreography for ticket\nworkflow\u201d on page 320\nTable 12-11, \u201cTrade-offs associated with state management rather than atomic dis\u2010\ntributed transactions with compensating updates\u201d on page 356\nTable 12-12, \u201cTrade-offs associated with atomic distributed transactions and compen\u2010\nsating updates\u201d on page 363\nTable 13-1, \u201cTrade-offs for strict contracts\u201d on page 371\n422 \n| \nAppendix C: Trade-Off References", "domains": ["Design Patterns", "Design Principles"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 440", "position": 440, "chunk_type": "semantic", "token_estimate": 273}
{"text": "Index: A\nabstractness of codebase, 67\ndistance from main sequence, 69\nACID (atomicity, consistency, isolation, dura\u2010\nbility) transactions\nabout, 263-267\ndistributed transactions versus, 265\nexample, 264\nBASE transactions versus, 267\ndatabase types\ncloud native databases, 176\ndocument databases, 169\ngraph databases, 173\nNewSQL databases, 173\nrelational databases, 163, 164\ntime series databases, 178\nafferent coupling, 66, 68\naggregate orientation, 165\nGraphQL read-only aggregated data, 368\nkey-value databases, 166\nNoSQL databases understanding, 171\nagility\nmodularity providing, 49, 49\nspeed-to-market via, 49\ntestability for, 54\nversioning shared libraries, 225\nAmazon DynamoDB key-value database, 165\nAmbler, Scott, 154\nanalytical data\nabout, 4, 381\ndata meshes, 389-394\nanalytical reporting coupled, 393\nwhen to use, 393\ndefinition, 5\ndomain over technical partitioning, 388\nprevious approaches\ndata lakes, 386-389\ndata warehouses, 382-385\nSysops Squad saga, 381, 386, 389\ndata mesh, 394-397\nannotations\ndistributed transaction management, 356\nmetadata for, 222\nAnthology Saga pattern, 349-351\nApache Cassandra column family database, 170\nApache Ignite replicated caching, 290\nAPIs\nendpoint versioning for shared services, 230\nplatforms for code reuse, 244\nArchitectural Decision Records (ADRs), 5\narchitectural decomposition\nabout, 64\neating the elephant, 63\ncodebase analysis\nabout, 65\nabstractness and instability, 67\nafferent and efferent coupling, 66\ndistance from main sequence, 69\ncomponent-based, 71-72\nabout, 64, 82\nabout patterns, 71, 82\narchitecture stories, 84\ncreate component domains, 120-122\ncreate domain services, 126-129\ndetermine component dependencies,\n111-118\nflatten components, 101-107\n425", "domains": ["Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 443", "position": 443, "chunk_type": "semantic", "token_estimate": 229}
{"text": "gather common domain components,: 94-96\nidentify and size components, 84-93\nservices built from components, 71\ndata\nabout, 132\nabout decomposing monolithic, 151\nassign tables to data domains, 156-158\ncreate data domains, 156\ndrivers of decomposition, 132-149\nRefactoring Databases (Ambler and\nSadalage), 154\nschemas to separate servers, 159\nseparate connections to data domains,\n158\nswitch over to independent servers, 161\nSysops Squad saga, 150\ntactical forking, 72-78\nabout, 64\ntrade-offs, 77\nvolatility-based decomposition, 191\narchitectural fitness functions (see fitness func\u2010\ntions for governance)\narchitecture\narchitecture governance, 6\nBig Ball of Mud anti-pattern, 65\nEquifax data breach, 12\nfitness functions, 7\n(see also fitness functions)\narchitecture quantum, 28\n(see also architecture quantum)\nbrittleness, 243\ndata warehouses, 384\nstrict contracts creating, 367\ncomposition versus inheritance, 228\ndata importance, 4\n(see also data)\ndefinitions of terms, 14\narchitecture definition, 3\ndesign versus, 13-15\nsecurity controlled through design, 215\nevolution of, 3, 6, 47\n(see also modularity)\niterative nature of analysis, 403\nwhy more important than how, 13\narchitecture governance, 6\nBig Ball of Mud anti-pattern, 65\nEquifax data breach, 12\nfitness functions, 7\n(see also fitness functions)\narchitecture quantum\nabout, 28, 144\ncoupling, static versus dynamic, 28\ndynamic quantum coupling, 38-41\nhigh static coupling, 30-37\nuser interface, 36\ndata disintegration driver, 144\nseparate databases, 159\nSysops Squad saga, 151\ndata product quantum, 390-393\nhigh functional cohesion, 30\ndatabase, 145\nindependently deployable, 29\nSysops Squad saga, 42\narchitecture stories, 84\nArchUnit tool, 129\nAslett, Matthew, 173\nasynchronous communication\nabout complexity of, 341, 345\ndefinition, 14\ndelegate technique of data ownership, 259\ndistributed data access, 287\nfault tolerance, 59\nperformance issues, 231\nmessaging mitigating, 231\npublish-and-subscribe messaging, 277\ndead letter queue, 279\ndurable subscribers, 278\nsynchronous versus, 38, 43\natomic fitness functions, 7\natomic workflow\nconsistency, 40\ndefinition, 15\neventual consistency versus, 15, 40\natomicity in ACID transactions, 263\ndistributed transactions versus, 266\nattributes for distributed transaction manage\u2010\nment, 356\navailability\nbasic availability of BASE transactions, 267\ncoupling relationship, 403\ndatabase types\nabout availability, 162\ncloud native databases, 176\ncolumn family databases, 170\ndocument databases, 168\ngraph databases, 173\nkey-value databases, 166\n426 \n| \nIndex", "domains": ["Software Quality Attributes", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 444", "position": 444, "chunk_type": "semantic", "token_estimate": 349}
{"text": "NewSQL databases, 174: relational databases, 164\ntime series databases, 178\nmodularity and, 58\nAWS Redshift cloud native database, 175\nAzure CosmosDB cloud native database, 175\nB\nbackpressure information link, 212\nbackup and restore databases, 160\nbandwidth and stamp coupling, 377\nBASE (basic availability, soft state, eventual\nconsistency) transactions, 267\nBeck, Kent, 6\nBerners-Lee, Tim, 4\nbest practices nonexistent, 1\nBig Ball of Mud anti-pattern, 65\ntactical forking, 72-78\nbounded context in microservices, 4, 26, 135\narchitecture quanta, 35\narchitecture quantum versus, 42\nbreaking database changes controlled,\n135-138\ndatabase abstraction, 136-138\ndata domains, 155\ncombining data domains, 157\ndata requirement in some architectures, 132\ndata sovereignty per service, 159\ngranularity and, 206\nhigh functional cohesion, 30\nownership of data (see ownership of data)\nbreaking changes to data structure, 134\nbounded context controlling, 135-138\ndatabase abstraction, 136-138\nbrittleness in architecture, 243\ndata warehouses, 384\nstrict contracts creating, 367\nbroker style event-driven architecture quan\u2010\ntum, 33, 42\nBrooks, Fred, 2\nbusiness case for modularity, 59-62\nC\ncaching for distributed data access, 288-293\nChain of Responsibility design pattern, 338\nThe Checklist Manifesto (Gawande), 13\nchoreographed coordination, 306-311\nabout coordination, 40, 300\nabout orchestrated versus, 300, 331\ntrade-offs, 315\ndefinition, 15\nservice granularity and, 200-203\nstamp coupling for management, 378\nworkflow state management, 311-315\nFront Controller pattern, 311\nstamp coupling, 313\nstateless choreography, 313\nclient/server applications, 382\ncloud native databases, 175\nCockburn, Alistair, 235\nCockroachDB NewSQL database, 174\ncode replication as reuse pattern, 221-223\ncode reuse patterns\nabout, 220\ncode replication, 221-223\ngranularity and shared code, 203-205, 224\northogonal reuse pattern, 238\nshared libraries\nabout, 223\ngranularity and, 224\nversioning strategies, 225-227\nshared services, 228-234\nchange risk, 229\nfault tolerance, 232\nperformance, 231\nscalability, 232\nversioning via API endpoints, 230\nsidecars and service mesh, 234-239\nSysops Squad saga, 239\nstrict contracts and, 367\nSysops Squad saga, 219\nshared domain functionality, 244\nsidecars and service mesh, 239\nvalue added, 242\nreuse via platforms, 244\ncode volatility as granularity driver, 191\ncohesion\nhigh functional cohesion, 30\ndatabase architecture quantum, 145\nmaintainability and, 51\nmetrics information link, 191\nservice granularity, 189, 194\ncolumn family databases, 169\ncommunication\nabout synchronous versus asynchronous,\n38, 43\nanalytical data mesh, 393\ncoordination required, 40\nIndex \n| \n427", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 445", "position": 445, "chunk_type": "semantic", "token_estimate": 365}
{"text": "database connection pool, 138: Sysops Squad saga, 150\ndistributed data access, 287\ndynamic coupling as, 23, 29\ncommunication dependencies, 29\nsaga pattern matrix, 41, 324, 402\neast-west communication, 200-203\ninterservice communication, 200-203\nAPI endpoint versioning for shared\nservices, 230\nbounded context data, 206\ndata ownership, 252, 259\ndistributed data access, 285\nperformance issues, 231\nlatency with distributed data, 285\npublish-and-subscribe messaging, 277\ndead letter queue, 279\ndurable subscribers, 278\nservice scalability and elasticity, 57\ncompensating updates in distributed transac\u2010\ntions, 275, 327\nstate management instead, 352, 355\nSysops Squad saga, 358-364\ncompetitive advantage as modularity driver, 49\ncompile-based versus runtime changes, 229\ncomponent-based decomposition, 71-72\nabout, 64, 82\nabout components, 71\npatterns\nabout, 71, 82\narchitecture stories, 84\ncreate component domains, 120-122\ncreate domain services, 126-129\ndetermine component dependencies,\n111-118\nflatten components, 101-107\ngather common domain components,\n94-96\nidentify and size components, 84-93\ncomponents\nCreate Component Domains pattern,\n120-122\nfitness functions, 122\ndefinition, 14, 51, 71, 103\ndependencies determination, 112\nIdentify and Size Components pattern,\n84-93\nfitness functions, 87-89\nmaintainability and, 51\nnames, 85\nnamespaces, 85\nsize determination, 84\nsize and granularity, 189\ncomposite architecture characteristics, 7\ncomposition versus inheritance in design, 228\nconnection management\nconnection quotas, 139\ndata disintegration driver, 138-141\nscalability includes connections, 141\ndatabase per service, 143\nSysops Squad saga, 150\nconsistency\nabout, 40, 162\nACID transactions, 264\ndistributed transactions versus, 266\ndatabase types\ncloud native ACID, 176\ncolumn family tunable, 171\ndocument ACID, 169\ngraph ACID, 173\nkey-value tunable, 167\nNewSQL ACID, 175\nrelational ACID, 164\ntime series ACID or tunable, 178\ndynamic coupling relationship matrix, 40,\n324, 402\neventual consistency\nabout patterns, 267\natomic workflow versus, 15, 40\nbackground synchronization pattern,\n269-272\nBASE transactions, 267\nevent-based pattern, 277-279\nfinite state machines for, 351\norchestrated request-based pattern,\n272-277\nmajority quorum, 167, 169\ntunable consistency, 167, 171\nConstantine, Larry, 66, 217\nconsumer-driven contracts, 373\ncontext for trade-off analysis, 405-407\ncontracts\nabout, 365\nbounded context data ownership, 136-138\ncontract fidelity and microservices, 373\nconsumer-driven contracts, 373\ndefinition, 15\ndistributed data access\n428 \n| \nIndex", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 446", "position": 446, "chunk_type": "semantic", "token_estimate": 336}
{"text": "Data Domain not requiring, 294: Interservice Communication requiring,\n286, 294\nReplicated Cache requiring, 294\nhigh static coupling, 30\nloose contracts, 368\ndecoupling via, 369, 371\nfitness functions required, 371\ntrade-offs, 371\nmicroservice contracts, 372\nstamp coupling\nabout, 376\nbandwidth, 377\nover-coupling via, 376\nworkflow management, 378\nstrict contracts, 367\nJSON example, 367\ntrade-offs, 370\nSysops Squad saga, 365, 379\ncooperative quantum, 393\ncoordination of workflow\nabout, 40, 300\nabout trade-offs, 315\nchoreographed coordination, 306-311\nabout orchestration versus, 300, 331\ndefinition, 15\nstamp coupling for management, 378\nworkflow state management, 311\ndynamic coupling relationship matrix, 40,\n324, 402\norchestrated coordination, 301-306\nabout choreographed versus, 300, 331\ndefinition, 14\nworkflow state management, 311\nstamp coupling for management, 378\nSysops Squad saga, 299, 317-321\ncoupling\nabout static versus dynamic, 29\ndynamic coupling, 23, 29\nstatic coupling, 23, 28\nSysops Squad saga, 42\narchitectural decomposition\nafferent coupling, 66, 68\ncommon domain functionality consoli\u2010\ndated, 99\ndetermining component dependencies,\n112\nefferent coupling, 66, 68\narchitecture quantum, 28, 34\ndynamic quantum coupling, 38-41\nhigh static coupling, 30-37\nindependently deployable, 29\nSysops Squad saga, 42\nuser interface, 36\navailability relationship, 403\ndata relationships, 147\ndefinition, 14, 27\ninterservice communication pattern, 286\nloose contracts for decoupling, 369, 371\nconsumer-driven contracts, 373\nmicroservices, 372\nmicroservices and contract fidelity, 373\nstrict contract tight coupling, 370\nmaintainability and, 51\noperational coupling, 234-239\nSysops Squad saga, 239\norchestration, 306\nchoreography versus, 332\northogonal coupling, 238\nscaling relationship, 332, 403\nsemantic coupling, 309\nimplementation coupling, 309\nstamp coupling, 378\nstamp coupling\nbandwidth, 377\nchoreography state management, 313\nover-coupling via, 376\nworkflow management, 378\ntrade-off analysis, 26-28, 401-403\ncoupling points, 402\nstatic coupling diagram, 401\nCreate Component Domains pattern, 120-122\nfitness functions, 122\nSysops Squad saga, 123-126, 129\nCreate Domain Services pattern, 126-129\nfitness functions, 129\nSysops Squad saga, 129\ncredit card data (see PCI (Payment Card Indus\u2010\ntry) data)\ncustom attributes for distributed transaction\nmanagement, 356\ncyclomatic complexity and maintainability, 51\nD\ndata\nabout importance of, 4\nabout operational versus analytical, 4\nanalytical data\nIndex \n| \n429", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 447", "position": 447, "chunk_type": "semantic", "token_estimate": 334}
{"text": "about, 4, 381: data lakes, 386-389\ndata meshes, 389-394\ndata meshes coupled to reporting, 393\ndata warehouses, 382-385\ndefinition, 5\ndomain over technical partitioning, 388\nSysops Squad saga, 381, 386, 389,\n394-397\nbreaking changes, 134\nbounded contexts controlling, 135-138\ncoupling\nmicroservices, 35\nmultiple quanta event-driven architec\u2010\nture, 34\nrelationships among data, 147\ndata domains (see data domains)\ndatabase-per-service, 143\nbreaking apart database to achieve, 147\ndecomposition of operational data\nabout, 132\nabout decomposing monolithic, 151\nassign tables to data domains, 156-158\ncreate data domains, 156\ndata access in other domains, 158\ndatabase-per-service requiring, 147\ndependencies, 153-155\ndrivers of, 132-149\nRefactoring Databases (Ambler and\nSadalage), 154\nschemas to separate servers, 159\nseparate connections to data domains,\n158\nswitch over to independent servers, 161\nSysops Squad saga, 131, 150\ndisintegration drivers\nabout, 133\narchitecture quantum, 144\nconnection management, 138-141\ndatabase change control, 134-138\ndatabase type optimization, 146\nfault tolerance, 143\nscalability, 141\ndistributed data access (see distributed data\naccess)\ngranularity and data relationships, 205-207\nintegration drivers\nabout, 146\ndatabase transactions, 148\nrelationships, 147\nlatency with distributed data, 285\nownership\nabout, 249\nassigning, 250\ncommon ownership, 252\ndata mesh domain ownership, 389\ndata sovereignty per service, 159\ndistributed data access, 287, 289\njoint ownership, 253-260\nservice consolidation technique, 261\nsingle ownership, 251\nsummary, 262\nSysops Squad saga, 249, 279-282\nsecurity\ncredit card data, 195\ngranularity driver, 195\nservices sharing, 132\nchanges to database, 134\nconnection management, 138-141\nsingle point of failure, 143\nsingle point of failure vulnerability, 143\nSysops Squad saga, 151\nSysops Squad data model, 19\ndata domains, 152\ndata domain technique of joint ownership,\n256-258\ndata domains\nabout, 152\nabout decomposing monolithic data, 151\nassign tables to data domains, 156-158\nbounded context rules, 155\ncreate data domains, 156\nschemas to separate servers, 159\nseparate connections to data domains,\n158\nswitch over to independent servers, 161\ncombining\nservice consolidation, 261\ntables tightly coupled, 157\ndata access in other domains, 158\ndata schemas versus, 157\nsynonyms for tables, 157\ndistributed data access, 293-295\njoint ownership of data, 256-258\nsoccer ball visualization, 153\nSysops Squad data model, 152\ndata lakes, 386-389\nData Mesh (Dehghani), 390\n430 \n| \nIndex", "domains": ["Architectural Patterns and Styles", "Design Principles", "Software Quality Attributes", "Domain-Driven Design"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 448", "position": 448, "chunk_type": "semantic", "token_estimate": 354}
{"text": "data meshes: about, 389\nData Mesh (Dehghani), 390\ndata product quantum, 390-393\nSysops Squad saga, 394-397\nwhen to use, 393\ndata product quantum (DPQ), 390-393\naggregate DPQ, 391\ncooperative quantum, 393\nfit-for-purpose DPQ, 392\nsource-aligned (native) DPQ, 391\ndata warehouses, 382-385\nStar Schema, 383, 384\ntechnical partitioning, 384\ndatabase abstraction via bounded context,\n136-138\nDatabase as a Service (DBaaS), 164\nNewSQL databases, 174\ndatabase refactoring book, 154\ndatabase structure Sysops Squad saga, 180-184\ndatabase transactions and granularity, 198\ndatabase types, 178\nabout characteristics rated, 161\naggregate orientation, 165\ncloud native databases, 175\ncolumn family databases, 169\ndatabase type optimization, 146\ndocument databases, 167\ngraph databases, 171-173\nkey-value databases, 165-167\nNewSQL databases, 173\nNoSQL databases, 165\nrelational databases, 163-164\nschema-less databases, 169\nSysops Squad saga, 179\ntime series databases, 177-178\ndatabase-per-service, 143\nbreaking apart database to achieve, 147\ndatabases separated physically, 159\nDatomic cloud native database, 175\nDDD (see Domain-Driven Design)\nDe La Torre, Fausto, 73\ndead letter queue (DLQ), 279\ndecision making, 26-28\n(see also trade-off analysis)\ndecomposition (see architectural decomposi\u2010\ntion)\nDecorator Design Pattern, 239\nDehghani, Zhamak, 389, 390\ndelegate technique of joint data ownership,\n258-261\ndependencies\nDetermine Component Dependencies pat\u2010\ntern, 111-118\ndynamic coupling as communication\ndependencies, 29\nmonolithic data domains, 153-155\nRefactoring Databases (Ambler and\nSadalage), 154\nsynonyms for tables, 157\nshared libraries, 224, 227\nstatic coupling as operational dependencies,\n23, 28\ntools\nJDepend, 9, 66\nNetArchTest, 11\nvisualization importance, 116\nvisualization tools resource, 113\ndeployment\narchitectural modularity described, 54\ndeployability in modularity, 55\nfault tolerance, 58\ncloud native Production Topology, 176\nindependent deployability, 29\ndeprecation strategies for shared libraries, 226\ndesign\narchitecture versus, 13-15\nsecurity controlled through design, 215\nChain of Responsibility, 338\ncomposition versus inheritance, 228\nDecorator Design Pattern, 239\nPipes and Filters, 338, 351\nDetermine Component Dependencies pattern,\n111-118\nfitness functions, 117\nSysops Squad saga, 118-120\ndimensional models of Star Schema, 384\ndistance from main sequence, 69\nzones of uselessness and pain, 69\ndistributed architectures\nbounded context in microservices, 26\n(see also bounded context)\ncode reuse patterns, 220\n(see also code reuse patterns)\ndata importance, 4\n(see also data)\ndata sovereignty per service, 159\ndatabase connection pool, 138\nIndex \n| \n431", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 449", "position": 449, "chunk_type": "semantic", "token_estimate": 357}
{"text": "connection management, 139: Sysops Squad saga, 150\ndynamic coupling matrix, 41, 324, 402\neventual consistency, 267\n(see also eventual consistency)\nevolution of architecture, 3\nsagas as transactional behavior, 15\n(see also sagas)\nsharing data, 257\nside effects, 361\nstate machines, 352-355\nstatic coupling, 32\nmultiple quanta, 34\nsingle quantum, 33\ntransactionality challenges, 40\ndistributed data access\nColumn Schema Replication pattern, 287\nData Domain pattern, 293-295\nInterservice Communication pattern, 285\nReplicated Caching pattern, 288-293\nSysops Squad saga, 283, 295-298\ndistributed transactions\nabout, 265, 327\nACID transactions, 263-267\nexample, 264\nBASE transactions, 267\ncompensating updates, 275, 327\nSysops Squad saga, 358-364\neventual consistency, 267\n(see also eventual consistency)\nmanaging sagas, 356\n(see also transactional saga patterns)\nSysops Squad sagas, 279-282, 358-364\ndistributed workflows\nabout, 299\nabout coordination, 40, 300\nabout trade-offs, 315\nchoreography, 306-311\nabout orchestration versus, 300\ndefinition, 15\nworkflow state management, 311-315\norchestration, 301-306\nabout choreographed versus, 300\ndefinition, 14\nworkflow state management, 311\nsemantic coupling, 309\nimplementation coupling, 309\nservice granularity and workflow, 200-203\nstamp coupling for management, 378\nSysops Squad saga, 299, 317-321\ntransactional saga patterns, 323\n(see also transactional saga patterns)\ndocument databases, 167\ndocumentation for managing distributed trans\u2010\nactions, 222, 356\ndomain partitioned architecture, 50\ndata domains, 152\ndata lakes losing relationships, 387\ndata mesh domain ownership, 389\ngranularity and shared domain functional\u2010\nity, 204\nmaintainability and, 51\ntechnical partitioning\ndata warehouses as, 384\ndomain preferred, 388\ndomain workflows, 311\ndomain services\nabout, 126\ncomponent domains into, 121\nCreate Domain Services pattern, 126-129\nfitness function for namespace, 129\ndomain cohesion and granularity, 194\nFront Controller in choreography, 311\nservice-based architecture definition, 72,\n126\ndomain versus infrastructure shared function\u2010\nality, 94\nDomain-Driven Design (DDD)\nbounded context, 4\narchitecture quantum, 42\nhigh functional cohesion and, 30\ninformation link, 121\nnamespaces from applications prior to, 121\ndurability in ACID transactions, 264\ndistributed transactions versus, 266\ndurable subscribers in pub/sub messaging, 278\ndynamic coupling, 23, 29\nabout dimensions of, 300, 401\nanalytical data communication pattern, 393\ncommunication, 38, 43\nconsistency, 40\ncoordination, 40\nrelationships among factors, 40, 324\n(see also sagas)\nstatic versus, 29\nSysops Squad saga, 42\n432 \n| \nIndex", "domains": ["Design Patterns", "Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 450", "position": 450, "chunk_type": "semantic", "token_estimate": 351}
{"text": "E: east-west communication, 200-203\nefferent coupling, 66, 68\nelasticity\ncoupling, 43\ndefinition, 56\nmean time to startup and, 56\nscalability versus, 43, 56\nEnterprise Integration Patterns (Hohpe and\nWoolf), 156\nenterprise service bus (ESB) global orchestra\u2010\ntor, 301\nEpic Saga pattern, 325-330\nEquifax data breach, 12\nEvans, Erik, 165\nevent-based eventual consistency, 277-279\npublish-and-subscribe messaging, 277\ndead letter queue, 279\ndurable subscribers, 278\nevent-driven architecture (EDA)\nmultiple architecture quanta, 34\nsingle architecture quantum\nbroker style as, 33, 42\nmediator style as, 33\neventual consistency\nabout consistency, 40\nabout patterns, 267\natomic workflow versus, 15, 40\nbackground synchronization pattern,\n269-272\nBASE transactions, 267\nevent-based pattern, 277-279\npublish-and-subscribe messaging, 277\nfinite state machines for, 351\norchestrated request-based pattern, 272-277\nextensibility as granularity driver, 196\neXtreme Programming (XP), 6\nF\nFairy Tale Saga pattern, 333-336\nFantasy Fiction Saga pattern, 340-342\nfault tolerance\nchoreography, 314\ndata disintegration driver, 143\ndefinition, 58, 143, 193\ngranularity disintegration driver, 193\ngranularity integration driver, 200-203\nmodularity driver, 49, 58\norchestration, 305\nshared libraries, 232\nshared services, 232\nsingle point of failure, 143\nSysops Squad saga, 151\nfitness functions for governance\nabout, 7-13\natomic, 7\ncomposite architecture characteristics, 7\nholistic, 8\nmanual versus automated, 11\noutcomes, 11\noveruse avoided, 13\ndynamic coupling monitored, 29\nexamples\ncomponent cycles avoided, 8\nlayer relationship verification, 10\nloose coupling requiring, 371\npatterns for component-based decomposi\u2010\ntion\ncreate component domains, 122\ncreate domain services, 129\ndetermine component dependencies,\n117\nflatten components, 107\ngather common domain components, 95\nidentify and size components, 87-89\nzero-day exploits, 12\nFlatten Components pattern, 101-107\nfitness functions, 107\nmetrics for shared components, 106\nSysops Squad saga, 107-110\nFoote, Brian, 65\nFord, Neal, xi, 417\nFowler, Martin, 165, 228\nFront Controller in choreography, 311\nFundamentals of Software Architecture\n(Richards and Ford), xi, 417-423\nG\nGather Common Domain Components pat\u2010\ntern, 94-96\nfitness functions, 95\nSysops Squad saga, 97-99\nGawande, Atul, 13\nGee, Trisha, 222\ngranularity\nbalancing trade-offs, 208\ndefinition, 57\ndisintegrators of granularity\nIndex \n| \n433", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 451", "position": 451, "chunk_type": "semantic", "token_estimate": 325}
{"text": "about, 187, 188: code volatility, 191\nextensibility, 196\nfault tolerance, 193\nscalability and throughput, 192\nsecurity, 195\nservice scope and function, 189\nelasticity as function of, 57\nintegrators of granularity\nabout, 187, 197\ndata relationships, 205-207\ndatabase transactions, 198\nshared code, 203-205\nworkflow and choreography, 200-203\nmetrics, 187\nmicroservice single responsibility, 186, 190\nmodularity versus, 186\nshared data and, 257\nshared libraries, 224\nSysops Squad saga, 185, 209-216\ngraph databases, 171-173\nGraphQL loose contracts, 368\nGremlin database language, 173\ngRPC protocol\ninformation link, 230\nperformance mitigation by, 231\nstrict contracts by default, 367\nH\nhard parts of architecture, 2\nhash table data structure, 165\nHello2morrow, 50\nHenney, Kevlin, 222\nHexagonal Architecture, 235\nhigh functional cohesion, 30\ndatabase architecture quantum, 145\nHohpe, Gregor, 156, 364\nHorror Story pattern, 343-345\nI\nIdentify and Size Components pattern, 84-93\nfitness functions, 87-89\nSysops Squad saga, 81\nimplementation coupling, 309\nindependent deployability, 29\nInfluxDB time series database, 177\ninfrastructure versus domain shared function\u2010\nality, 94\ninheritance versus composition in design, 228\ninstability of codebase, 68\ndistance from main sequence, 69\ninterservice communication, 200-203\nAPI endpoint versioning for shared services,\n230\nbounded context data, 206\ndata ownership\ncommon ownership, 252\ndelegate technique of joint ownership,\n259\ndistributed data access, 285\nisolation in ACID transactions, 264\ndistributed transactions versus, 266\nimportance of, 361\nisomorphic diagrams, 325\nJ\nJava Message Service (Richards), 231\nJDepend tool, 9\nEclipse plug-in, 66\njoint ownership of data\nabout, 253\ndata domain technique, 256-258\ndelegate technique, 258-261\nservice consolidation technique, 261\ntable split technique, 254-256\nK\nkey-value databases, 165-167\nKx time series database, 177\nL\nlatency in communication, 285\nLATEST caution for versioning, 227\nlayered architecture\narchitecture governance, 10\nmodular architecture\nmaintainability, 51\nscalability and elasticity, 57\ntestability, 54\nloose contracts\ndecoupling via, 369\nfitness functions required, 371\nREST and GraphQL, 368\ntrade-offs, 371\nM\nmaintainability of modular architecture, 50-54\n434 \n| \nIndex", "domains": ["Architectural Patterns and Styles", "Design Principles", "Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 452", "position": 452, "chunk_type": "semantic", "token_estimate": 310}
{"text": "metric for tracking, 50: majority quorum, 167, 169\nMartin, Robert C., 67, 190\nmean time to startup and elasticity, 56\nMECE (Mutually Exclusive, Collectively\nExhaustive) lists, 404\nmediator in orchestration, 301\nmediator style event-driven architecture quan\u2010\ntum, 33\nMemcacheDB key-value database, 165\nmessaging asynchronicity, 231\nJava Message Service (Richards), 231\nmetadata annotations, 222\nmicrokernel architecture, 50\nmicroservices\narchitecture quantum, 28, 29\ndecoupled services, 35\nhigh functional cohesion, 30\nuser interface coupling, 36\nbounded context, 4, 26\narchitecture quanta, 35\narchitecture quantum versus, 42\nbreaking database changes controlled,\n135-138\ndata domains, 155\ndata domains combined, 157\ndata requirement, 132\ndatabase abstraction, 136-138\ngranularity and, 206\nhigh functional cohesion, 30\nownership of data (see ownership of\ndata)\ncontracts\nabout, 372\nconsumer-driven contracts, 373\ncontract fidelity, 373\ncoupling levels, 372\ncoordination, 40\n(see also coordination)\ndefinition, 191\ndomain partitioning of, 388\nmodularity, 52\ndeployability, 56\nscalability and elasticity, 57\noperational coupling, 234-239\norchestrator per workflow, 301\nsaga pattern, 323\nservice-based architecture as stepping-\nstone, 72, 121, 127\nshared database changed, 134\nsingle responsibility principle and granular\u2010\nity, 186, 190\nMicroservices Patterns (Richardson), 323\nMicrosoft SQL Server relational database, 164\nmodularity\nabout, 48, 50\nabout change, 47\narchitectural governance, 8\ndrivers of\nabout, 49\nagility, 49, 49\ndeployability, 55\nfault tolerance, 58\nmaintainability, 50-54\nscalability, 48, 56-57\ntestability, 54\ngranularity versus, 186\nmonolithic architectures\nabout modularity, 50\nabout need for modularity, 47\ndeployability, 55\nfault tolerance, 58\nmaintainability, 51\nmodular monoliths, 50\nscalability and elasticity, 57\nSysops Squad saga, 45, 59-62\ntestability, 54\nwater glass analogy, 47\nmonolithic architectures\narchitectural decomposition (see architec\u2010\ntural decomposition)\nBig Ball of Mud anti-pattern, 65\ntactical forking, 72-78\ndata decomposition\nabout, 151\nassign tables to data domains, 156-158\ncreate data domains, 156\nRefactoring Databases (Ambler and\nSadalage), 154\nschemas to separate servers, 159\nseparate connections to data domains,\n158\nswitch over to independent servers, 161\ndata importance, 4\ndatabase connection pool, 138\ndatabase type optimization, 146\nEpic Saga mimicking, 325-330\nmodular architecture\nIndex \n| \n435", "domains": ["Architectural Patterns and Styles", "Design Patterns", "Design Principles", "Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 453", "position": 453, "chunk_type": "semantic", "token_estimate": 327}
{"text": "about need for, 47: deployability, 55\nfault tolerance, 58\nmaintainability, 51\nmodular monoliths, 50\nscalability and elasticity, 57\nSysops Squad saga, 45, 59-62\ntestability, 54\nwater glass analogy, 47\nnamespaces prior to Domain-Driven\nDesign, 121\npatterns for migration to service-based\nabout, 71, 82\narchitecture stories, 84\ncreate component domains, 120-122\ncreate domain services, 126-129\ndetermine component dependencies,\n111-118\nflatten components, 101-107\ngather common domain components,\n94-96\nidentify and size components, 84-93\nsingle architecture quantum, 29\nfunctional cohesion, 30\nstatic coupling, 31\nMonorepo, 415\nMySQL relational database, 164\nN\nnamespaces\ncomponent domains, 121\napplications prior to Domain-Driven\nDesign, 121\nfitness functions for governance, 122\ncomponents, 85\nextend then prior a subdomain, 102\nfitness function, 129\nroot namespace definition, 103\nNeo4j graph database, 173\nNetArchTest tool, 11\nnetwork latency, 285\nNewSQL databases, 173\n97 Things Every Java Programmer Should\nKnow (Henney and Gee), 222\nNoSQL databases\naggregate orientation understood, 171\ncolumn family databases, 169\ndocument databases, 167\nkey-value databases, 165\nNoSQL Distilled (Sadalage and Fowler), 165\nschema-less databases, 169\nNygard, Michael, 5\nO\nonline resources (see resources online)\nOnline Transactional Processing (OLTP), 4\noperational coupling, 234-239\nSysops Squad saga, 239\noperational data definition, 4\nOracle Coherence replicated caching, 290\nOracle relational database, 164\norchestrated coordination, 301-306\nabout choreographed versus, 300, 331\ntrade-offs, 315\nabout coordination, 40, 300\ndefinition, 14\norchestrator, 301\nworkflow state management, 311\norchestrated request-based eventual consis\u2010\ntency, 272-277\nOrchestrated Saga pattern, 325-330\norphaned classes definition, 103\northogonal code reuse pattern, 238\northogonal coupling, 238\nout-of-context trap, 405-407\nownership of data\nabout, 249\nassigning, 250\ncommon ownership, 252\ndata mesh domain ownership, 389\ndata sovereignty per service, 159\ndistributed data access, 287, 289\njoint ownership\nabout, 253\ndata domain technique, 256-258\ndelegate technique, 258-261\ntable split technique, 254-256\nservice consolidation technique, 261\nsingle ownership, 251\nsummary, 262\nSysops Squad saga, 249, 279-282\nP\nPage-Jones, Meilir, 23\nParallel Saga pattern, 346-348\nPCI (Payment Card Industry) data\nsecurity and granularity, 195\nSysops Squad saga, 212-216\n436 \n| \nIndex", "domains": ["Design Patterns", "Design Principles", "Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 454", "position": 454, "chunk_type": "semantic", "token_estimate": 326}
{"text": "Personally Identifiable Information (PII), 387: Phone Tag Saga pattern, 330-333\nPII (Personally Identifiable Information), 387\nPipes and Filters design pattern, 338, 351\nplatforms for code reuse, 244\nPorts and Adaptors Pattern, 235\nPostgreSQL relational database, 164\npublish-and-subscribe messaging, 277\ndead letter queue, 279\ndurable subscribers, 278\npull model consumer-driven contracts, 373\npush model definition, 373\nQ\nquanta, 28\n(see also architecture quantum)\nquorum mechanism, 167, 169\nR\nRedis key-value database, 165\nRefactoring Databases (Ambler and Sadalage),\n154\nrelational databases (RDBMS), 163-164\nlinks to most popular, 164\ntransactions and service granularity, 198\nreplicate databases, 160\nreplicated in-memory caching, 288-293\nresources online\nArchUnit tool, 129\nbook website, xii\ncohesion metrics information, 191\ndatabase links\ncloud native databases, 175\nNoSQL databases, 165\nrelational databases, 164\ndependency tools\nJDepend, 9, 66\nNetArchTest, 11\nvisualization, 113\nRefactoring Databases (Ambler and Sada\u2010\nlage), 154\nSaga Pattern (Richardson), 323\nstamp coupling information, 376\nstatic code analysis tools, 85\nREST (representational state transfer) loose\ncontracts, 368\nreuse patterns (see code reuse patterns)\nRiak KV key-value database, 165\nRichards, Mark, xi, 231, 417\nRichardson, Chris, 323\nroot namespace definition, 103\nruntime versus compile-based changes, 229\nS\nSadalage, Pramod, 154, 165\nsagas\nabout, 323\ndefinition, 15\ndynamic coupling matrix, 41, 324, 402\nmicroservice saga pattern, 323\nstate machines, 352-355\nSysops Squad saga background, 15\n(see also Sysops Squad sagas)\ntechniques for managing, 356\ntransactional saga patterns\nabout, 41, 324\nabout state management, 351\nAnthology Saga, 349-351\nEpic Saga, 325-330\nFairy Tale Saga, 333-336\nFantasy Fiction Saga, 340-342\nHorror Story, 343-345\nParallel Saga, 346-348\nPhone Tag Saga, 330-333\nSysops Squad saga, 323, 358-364\nTime Travel Saga, 336-339\nscalability\nchoreography, 314, 331\nconnection management, 141\ndatabase-per-service, 143\nSysops Squad saga, 150\ncoupling relationship, 332, 403\ndata disintegration driver, 141\ndatabase types\ncloud native scalable, 176\ncolumn family scalable, 170\ndocument scalable, 168\ngraph read versus write, 172\nkey-value scalable, 166\nNewSQL scalable, 174\nrelational complexity, 164\ntime series scalable, 178\ndefinition, 56\nelasticity versus, 43, 56\ngranularity disintegration driver, 192\nmodularity providing, 47, 57\nmodularity driver, 49, 56-57\norchestration, 305\nIndex \n| \n437", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 455", "position": 455, "chunk_type": "semantic", "token_estimate": 341}
{"text": "choreography versus, 331: service consolidation for data ownership,\n262\nshared services, 232\nschemas for database tables, 156\ncross-schema access resolved, 158\ndata domain versus data schema, 157\ndatabases separated physically, 159\nschema-less databases, 169\nsynonyms for tables, 157\nScylla column family database, 171\nsecurity\ngranularity disintegration driver, 195\nPCI data Sysops Squad saga, 212-216\nsecurity latency, 285\nsemantic coupling, 309\nimplementation coupling, 309\nstamp coupling to manage workflow, 378\nservice consolidation technique of joint owner\u2010\nship, 261\nservice granularity (see granularity)\nservice mesh, 237-239\nSysops Squad saga, 239\nservice-based architecture\nabout, 32, 72, 126\ndata access in other domains, 158\ndata shared by services, 132\nchanges to database, 134\nconnection management, 138-141\nsingle point of failure, 143\ndomain services from component domains,\n121\nCreate Domain Services pattern, 126-129\nevolution of architecture, 3\nmodular architecture scalability and elastic\u2010\nity, 57\nmonolithic migration to\nabout, 71, 82\narchitecture stories, 84\ncreate component domains, 120-122\ncreate domain services, 126-129\ndetermine component dependencies,\n111-118\nflatten components pattern, 101-107\ngather common domain components,\n94-96\nidentify and size components, 84-93\nstatic coupling, 32\nstepping-stone to microservices, 72, 121,\n127\nservices\nbounded contexts (see bounded context)\nchoreographed coordination definition, 15\ncomponent-based decomposition, 71\nCreate Component Domains pattern,\n120\ncoupling definition, 14\n(see also coupling)\ndata access in other domains, 158\ndata shared by services, 132\nchanges to database, 134\nconnection management, 138-141\nsingle point of failure, 143\ndatabase connection pool, 138\nSysops Squad saga, 150\ndatabase-per-service, 143\ndefinition, 14\ndistributed data access\nColumn Schema Replication pattern,\n287\nData Domain pattern, 293-295\nInterservice Communication Pattern,\n285\nReplicated Caching pattern, 288-293\nSysops Squad saga, 283, 295-298\nextensibility, 196\ngranularity (see granularity)\nidentifying in sagas, 357\nmodularity\ndeployability, 56\nscalability and elasticity, 57\ntestability, 54\norchestrated coordination definition, 14\nownership of data\nassigning, 250\ncommon ownership, 252\ndata sovereignty per service, 159\ndistributed data access, 287, 289\njoint ownership, 253-260\nservice consolidation technique, 261\nsingle ownership, 251\nsummary, 262\nSysops Squad saga, 249, 279-282\nservice mesh, 237-239\nshared services for code reuse, 228-234\nSidecar pattern, 234-239\nsharding in databases, 167\n438 \n| \nIndex", "domains": ["Architectural Patterns and Styles", "Design Principles", "Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 456", "position": 456, "chunk_type": "semantic", "token_estimate": 343}
{"text": "document databases, 168: NewSQL databases, 174\nshared code and granularity, 203-205\n(see also code reuse patterns)\nshared libraries, 224\nshared domain functionality\ngranularity and, 204\nshared infrastructure functionality versus,\n94\nSysops Squad saga, 244\nshared libraries\nabout, 223\nfault tolerance, 232\ngranularity and, 224\nversioning strategies, 225-227\nshared services for code reuse, 228-234\nchange risk, 229\nfault tolerance, 232\nperformance, 231\nscalability, 232\nSysops Squad saga, 244\nversioning via API endpoints, 230\nside effects in distributed architectures, 361\nSidecar pattern, 234-239\nSysops Squad saga, 239\nsilver-bullet solutions, 2\nsingle point of failure (SPOF), 143\nSysops Squad saga, 151\nsingle responsibility principle, 190\ngranularity and, 186, 190\nSMS information link, 189\nSnowflake cloud native database, 175\nsoft state of BASE transactions, 267\nsoftware architecture (see architecture)\nSOLID principles (Martin), 190\nSonarQube code hygiene tool, 10\nspeed-to-market as modularity driver, 49\nstamp coupling\nabout, 376\nbandwidth, 377\nchoreography state management, 313, 332\nover-coupling via, 376\nworkflow management, 378\nstandard deviation calculation, 88\nStar Schema of data warehouses, 383, 384\nStarbucks Does Not Use Two-Phase Commit\n(Hohpe), 364\nstate machines, 352-355\nstate management\ncompensating updates versus, 355\nfinite state machines for, 351\nabout state machines, 352-355\norchestration, 305\nstamp coupling for workflow management,\n378\nworkflow state management, 311-315\nstateless choreography, 313\nstatements counted, 84, 187\nstatic code analysis tools resource, 85\nstatic coupling, 23, 28\narchitecture quantum, 34\nhigh static coupling, 30-37\nindependently deployable, 29\nSysops Squad saga, 42\ndynamic versus, 29\nSysops Squad saga, 42\noperational dependencies as, 23, 28\ntrade-off analysis, 401\ncoupling points, 402\nstatic coupling diagram, 401\nStine, Matt, 56\nstrict contracts, 367\nJSON example, 367\ntrade-offs, 370\nStructured Design (Yourdon and Constantine),\n66\nsurvey database structure, 180-184\nsynchronous communication\nasynchronous versus, 38, 43\ndefinition, 14\ndelegate technique of data ownership, 259\nfault tolerance, 59\ngranularity and, 200-203\nscalability and elasticity versus, 57\nsynonyms for tables, 157\nSysops Squad sagas\nabout, 15\nnonticketing workflow, 16\nproblem, 17\nticketing workflow, 17\nanalytical data, 381, 386, 389\ndata mesh, 394-397\narchitectural components, 18\narchitecture quantum, 42\ncontracts, 365, 379\ndata model, 19\nACID transaction example, 264\ndata domains, 152\nIndex \n| \n439", "domains": ["Design Patterns", "Design Principles", "Software Quality Attributes", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 457", "position": 457, "chunk_type": "semantic", "token_estimate": 351}
{"text": "distributed data access, 283, 295-298: Column Schema Replication pattern,\n287\nData Domain pattern, 293-295\nInterservice Communication Pattern,\n285\nReplicated Caching pattern, 288-293\ndistributed transactions, 279-282\ndistributed workflows, 299, 317-321\nepilogue, 416\neventual consistency\nabout, 267\nbackground synchronization, 269-272\nevent-based pattern, 277-279\norchestrated request-based pattern,\n272-277\nmonolithic application broken apart, 45\n6. create domain services, 127\nbusiness case for, 59-62\ncode reuse patterns, 219, 239, 244\ncreate component domains, 123-126\ncreate domain services, 129\ndata pulled apart, 131, 150\ndatabase structure, 180-184\ndatabase types, 179\ndecomposition method, 63, 78\ndecomposition pattern, 81\ndetermine component dependencies,\n118-120\nflatten components, 107-110\ngather common domain components,\n97-99\ngranularity, 185, 209-216\nidentify and size components, 90-93\nownership of data, 249, 279-282\nstate machines, 352-355\ntrade-off analysis, 399, 416\ntransactional sagas, 323, 358-364\nT\ntable split technique of joint ownership,\n254-256\ntactical forking, 72-78\nabout, 64\ntrade-offs, 77\ntechnical partitioning\ndata warehouses as, 384\ndomain partitioning preferred, 388\ndomain workflows, 311\nmaintainability and, 51\ntestability as modularity driver, 54\nThoughtworks\nArchitectural Decision Records, 5\ncomposition versus inheritance, 228\ntime series databases, 177-178\nTime Travel Saga pattern, 336-339\nTimeScale time series database, 177\ntools\nArchUnit, 129\ndependencies\nJDepend, 9, 66\nNetArchTest, 11\nvisualization, 66, 113, 116\nSonarQube code hygiene tool, 10\nstatic code analysis, 85\nvisualization\nabout importance of, 116\nafferent and efferent coupling, 66\ndata domains via soccer ball, 153\ndependencies, 66, 113, 116\ndistance from main sequence, 69\ndomain diagramming exercise, 123\ngraph databases, 171-173\nisomorphic diagrams, 325\nstatic coupling diagram, 401\ntrade-off analysis, 26-28\nassessing trade-offs, 403\nleast worst combination, 2\nbuilding your own, 400-403\nconsistency and availability, 178\ncoordination, 315\ncoupling, 26-28, 401-403\ncoupling points, 402\ndynamic quantum coupling factors, 41\nstatic coupling diagram, 401\ndocumenting versus governing, 5\ngranularity, 208\niterative nature of, 403\nloose contracts, 371\nstrict contracts, 370\nSysops Squad saga, 399, 416\ntactical forking, 77\ntechniques of\nbottom line decision, 410\nMECE lists, 404\nmodeling relevant domain cases,\n408-410\nout-of-context trap, 405-407\n440 \n| \nIndex", "domains": ["Design Patterns", "Software Quality Attributes", "Domain-Driven Design", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 458", "position": 458, "chunk_type": "semantic", "token_estimate": 326}
{"text": "qualitative versus quantitative analysis,: 404\nsnake oil and evangelism, 412\ntransactional saga patterns\nabout, 324\npattern matrix, 41, 324, 402\nabout sagas, 323\nmanagement techniques, 356\nabout state management, 351\nabout state machines, 352-355\ncompensating updates versus, 355\nAnthology Saga, 349-351\nEpic Saga, 325-330\nFairy Tale Saga, 333-336\nFantasy Fiction Saga, 340-342\nHorror Story, 343-345\nParallel Saga, 346-348\nPhone Tag Saga, 330-333\nSysops Squad saga, 323, 358-364\nTime Travel Saga, 336-339\ntransactionality\ndistributed architecture challenge, 40\nsecurity versus, 212-216\ntransactions\ndata integration driver, 148\ndatabase transactions and granularity, 198\ndistributed transactions\nabout, 265, 327\nACID transactions, 263-267\nBASE transactions, 267\ncompensating updates, 275, 327\neventual consistency, 267\n(see also eventual consistency)\nSysops Squad saga, 279-282, 358-364\nTrunk-based Development, 415\ntunable consistency, 167, 171\nU\nuser interface quantum, 36\nV\nversion control systems\ncode change frequency metric, 191\nLATEST caution, 227\nshared code libraries, 225-227\ngranularity and, 204, 205\nshared service API endpoint versioning, 230\nstrict contracts requiring, 370\nvisualization tools\nafferent and efferent coupling, 66\ndata domains via soccer ball, 153\ndependencies, 113\nimportance of visualization, 116\nJDepend Eclipse plug-in, 66\ndistance from main sequence, 69\ndomain diagramming exercise, 123\ngraph databases, 171-173\nisomorphic diagrams, 325\nstatic coupling diagram, 401\nvolatility-based decomposition, 191\nW\nWhat Every Programmer Should Know About\nObject-Oriented Design (Page-Jones), 23\nwhy more important than how, 13\nwide column databases, 169\nWoolf, Bobby, 156\nworkflow and service granularity, 200-203\n(see also distributed workflows)\nY\nYourdon, Edward, 66\nZ\nzero-day exploits, 12\nZitzewitz, Alexander von, 50\nzones of uselessness and pain, 69\nIndex \n| \n441", "domains": ["Design Patterns", "Software Quality Attributes", "Code Organization"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 459", "position": 459, "chunk_type": "semantic", "token_estimate": 257}
{"text": "About the Authors: Neal Ford is a director, software architect, and meme wrangler at Thoughtworks, a\nsoftware company and a community of passionate, purpose-led individuals who\nthink disruptively to deliver technology that addresses the toughest challenges, all\nwhile seeking to revolutionize the IT industry and create positive social change. He\u2019s\nan internationally recognized expert on software development and delivery, especially\nin the intersection of Agile engineering techniques and software architecture. Neal\nhas authored seven books (and counting), a number of magazine articles, and dozens\nof video presentations and spoken at hundreds of developers conferences worldwide. His topics include software architecture, continuous delivery, functional program\u2010\nming, cutting-edge software innovations, and a business-focused book and video on\nimproving technical presentations. Check out his website, Nealford.com. Mark Richards is an experienced, hands-on software architect involved in the archi\u2010\ntecture, design, and implementation of microservices architectures, service-oriented\narchitectures, and distributed systems in a variety of technologies. He has been in the\nsoftware industry since 1983 and has significant experience and expertise in applica\u2010\ntion, integration, and enterprise architecture. Mark is the author of numerous techni\u2010\ncal books and videos, including the Fundamentals of Software Architecture, the\n\u201cSoftware Architecture Fundamentals\u201d video series, and several books and videos on\nmicroservices as well as enterprise messaging. Mark is also a conference speaker and\ntrainer and has spoken at hundreds of conferences and user groups around the world\non a variety of enterprise-related technical topics. Pramod Sadalage is director of data and DevOps at Thoughtworks. His expertise\nincludes application development, Agile database development, evolutionary data\u2010\nbase design, algorithm design, and database administration. Zhamak Dehghani is director of emerging technologies at Thoughtworks. Previ\u2010\nously, she worked at Silverbrook Research as a principal software engineer, and Fox\nTechnology as a senior software engineer.", "domains": ["Architectural Patterns and Styles"], "source": "Software.Architecture.The.Hard.Parts.pdf", "section": "Page 460", "position": 460, "chunk_type": "semantic", "token_estimate": 293}
