{"text": "Buy Volume II\nIntroduction\nAmy Brown and Greg Wilson\n1.\nScalable Web\nArchitecture and\nDistributed Systems\nKate Matsudaira\n2.\nFirefox Release\nEngineering\nChris AtLee, Lukas Blakk, John\nO'Duinn, and Armen Zambrano\nGasparnian\n3.\nFreeRTOS\nChristopher Svec\n4.\nGDB\nStan Shebs\n5.\nThe Glasgow Haskell\nCompiler\nSimon Marlow and Simon\nPeyton-Jones\n6.\nGit\nSusan Potter\n7.\nGPSD\nEric Raymond\n8.\nThe Dynamic Language\nRuntime and the Iron\nLanguages\nJeff Hardy\n9.\nITK\nLuis Ibanez and Brad King\n10. GNU Mailman\nBarry Warsaw\n11. matplotlib\nJohn Hunter and Michael\nDroettboom\n12. MediaWiki\nSumana Harihareswara and\nGuillaume Paumier\n13. Moodle\nTim Hunt\n14. nginx\nAndrey Alexeev\n15. Open MPI\nJeff Squyres\n16. OSCAR\nJennifer Ruttan\n17. Processing.js\nMike Kamermans\n18. Puppet\nLuke Kanies\n", "page": 1, "type": "text", "section": "Page 1"}
{"text": "19. PyPy\nBenjamin Peterson\n20. SQLAlchemy\nMichael Bayer\n21. Twisted\nJessica McKellar\n22. Yesod\nMichael Snoyman\n23. Yocto\nElizabeth Flanagan\n24. ZeroMQ\nMartin S\u00fastrik\nBibliography\n", "page": 2, "type": "text", "section": "Page 2"}
{"text": "<>\nIn the introduction to Volume 1 of this series, we wrote:\nBuilding architecture and software architecture have a lot in common, but there is one crucial difference. While architects study thousands of buildings in\ntheir training and during their careers, most software developers only ever get to know a handful of large programs well\u2026 As a result, they repeat one\nanother's mistakes rather than building on one another's successes\u2026 This book is our attempt to change that.\nIn the year since that book appeared, over two dozen people have worked hard to create the sequel you have in your hands. They have done so because they\nbelieve, as we do, that software design can and should be taught by example\u2014that the best way to learn how think like an expert is to study how experts think.\nFrom web servers and compilers through health record management systems to the infrastructure that Mozilla uses to get Firefox out the door, there are\nlessons all around us. We hope that by collecting some of them together in this book, we can help you become a better developer.\n\u2014 Amy Brown and Greg Wilson\nContributors\nAndrew Alexeev (nginx): Andrew is a co-founder of Nginx, Inc.\u2014the company behind nginx. Prior to joining Nginx, Inc. at the beginning of 2011, Andrew\nworked in the Internet industry and in a variety of ICT divisions for enterprises. Andrew holds a diploma in Electronics from St. Petersburg Electrotechnical\nUniversity and an executive MBA from Antwerp Management School.\nChris AtLee (Firefox Release Engineering): Chris is loving his job managing Release Engineers at Mozilla. He has a BMath in Computer Science from the\nUniversity of Waterloo. His online ramblings can be found at http://atlee.ca.\nMichael Bayer (SQLAlchemy): Michael Bayer has been working with open source software and databases since the mid-1990s. Today he's active in the Python\ncommunity, working to spread good software practices to an ever wider audience. Follow Mike on Twitter at @zzzeek .\nLukas Blakk (Firefox Release Engineering): Lukas graduated from Toronto's Seneca College with a bachelor of Software Development in 2009, but started\nworking with Mozilla's Release Engineering team while still a student thanks to Dave Humphrey's (http://vocamus.net/dave/) Topics in Open Source classes.\nLukas Blakk's adventures with open source can be followed on her blog at http://lukasblakk.com.\nAmy Brown (editorial): Amy worked in the software industry for ten years before quitting to create a freelance editing and book production business. She has an\nunderused degree in Math from the University of Waterloo. She can be found online at http://www.amyrbrown.ca/.\nMichael Droettboom (matplotlib): Michael Droettboom works for STScI developing science and calibration software for the Hubble and James Webb Space\nTelescopes. He has worked on the matplotlib project since 2007.\nElizabeth Flanagan (Yocto): Elizabeth Flanagan works for the Open Source Technologies Center at Intel Corp as the Yocto Project's Build and Release\nengineer. She is the maintainer of the Yocto Autobuilder and contributes to the Yocto Project and OE-Core. She lives in Portland, Oregon and can be found\nonline at http://www.hacklikeagirl.com.\nJeff Hardy (The Dynamic Language Runtime and the Iron Languages): Jeff started programming in high school, which led to a bachelor's degree in Software\nEngineering from the University of Alberta and his current position writing Python code for Amazon.com in Seattle. He has also led IronPython's development\nsince 2010. You can find more information about him at http://jdhardy.ca.\nSumana Harihareswara (MediaWiki): Sumana is the community manager for MediaWiki as the volunteer development coordinator for the Wikimedia\nFoundation. She previously worked with the GNOME, Empathy, Telepathy, Miro, and AltLaw projects. Sumana is an advisory board member for the Ada\nInitiative, which supports women in open technology and culture. She lives in New York City. Her personal site is at http://www.harihareswara.net/.\nTim Hunt (Moodle): Tim Hunt started out as a mathematician, getting as far as a PhD in non-linear dynamics from the University of Cambridge before deciding\nto do something a bit less esoteric with his life. He now works as a Leading Software Developer at the Open University in Milton Keynes, UK, working on their\nlearning and teaching systems which are based on Moodle. Since 2006 he has been the maintainer of the Moodle quiz module and the question bank code, a\nrole he still enjoys. From 2008 to 2009, Tim spent a year in Australia working at the Moodle HQ offices. He blogs at http://tjhunt.blogspot.com and can be found\n@tim_hunt  on Twitter.\nJohn Hunter (matplotlib): John Hunter is a Quantitative Analyst at TradeLink Securities. He received his doctorate in neurobiology at the University of Chicago\nfor experimental and numerical modeling work on synchronization, and continued his work on synchronization processes as a postdoc in Neurology working on\nepilepsy. He left academia for quantitative finance in 2005. An avid Python programmer and lecturer in scientific computing in Python, he is original author and\nlead developer of the scientific visualization package matplotlib.\nLuis Ib\u00e1\u00f1ez (ITK): Luis has worked for 12 years on the development of the Insight Toolkit (ITK), an open source library for medical imaging analysis. Luis is a\nstrong supporter of open access and the revival of reproducibility verification in scientific publishing. Luis has been teaching a course on Open Source Software\nIntroduction\nAmy Brown and Greg Wilson\n", "page": 3, "type": "text", "section": "Page 3"}
{"text": "Practices at Rensselaer Polytechnic Institute since 2007.\nMike Kamermans (Processing.js): Mike started his career in computer science by failing technical Computer Science and promptly moved on to getting a\nmaster's degree in Artificial Intelligence, instead. He's been programming in order not to have to program since 1998, with a focus on getting people the tools\nthey need to get the jobs they need done, done. He has focussed on many other things as well, including writing a book on Japanese grammar, and writing a\ndetailed explanation of the math behind B\u00e9zier curves. His under-used home page is at http://pomax.nihongoresources.com.\nLuke Kanies (Puppet): Luke founded Puppet and Puppet Labs in 2005 out of fear and desperation, with the goal of producing better operations tools and\nchanging how we manage systems. He has been publishing and speaking on his work in Unix administration since 1997, focusing on development since 2001.\nHe has developed and published multiple simple sysadmin tools and contributed to established products like Cfengine, and has presented on Puppet and other\ntools around the world, including at OSCON, LISA, Linux.Conf.au, and FOSS.in. His work with Puppet has been an important part of DevOps and delivering on\nthe promise of cloud computing.\nBrad King (ITK): Brad King joined Kitware as a founding member of the Software Process group. He earned a PhD in Computer Science from Rensselaer\nPolytechnic Institute. He is one of the original developers of the Insight Toolkit (ITK), an open source library for medical imaging analysis. At Kitware Dr. King's\nwork focuses on methods and tools for open source software development. He is a core developer of CMake and has made contributions to many open source\nprojects including VTK and ParaView.\nSimon Marlow (The Glasgow Haskell Compiler): Simon Marlow is a developer at Microsoft Research's Cambridge lab, and for the last 14 years has been doing\nresearch and development using Haskell. He is one of the lead developers of the Glasgow Haskell Compiler, and amongst other things is responsible for its\nruntime system. Recently, Simon's main focus has been on providing great support for concurrent and parallel programming with Haskell. Simon can be\nreached via @simonmar  on Twitter, or +Simon Marlow on Google+.\nKate Matsudaira (Scalable Web Architecture and Distributed Systems): Kate Matsudaira has worked as the VP Engineering/CTO at several technology startups,\nincluding currently at Decide, and formerly at SEOmoz and Delve Networks (acquired by Limelight). Prior to joining the startup world she spent time as a\nsoftware engineer and technical lead/manager at Amazon and Microsoft. Kate has hands-on knowledge and experience with building large scale distributed web\nsystems, big data, cloud computing and technical leadership. Kate has a BS in Computer Science from Harvey Mudd College, and has completed graduate\nwork at the University of Washington in both Business and Computer Science (MS). You can read more on her blog and website http://katemats.com.\nJessica McKellar (Twisted): Jessica is a software engineer from Boston, MA. She is a Twisted maintainer, Python Software Foundation member, and an\norganizer for the Boston Python user group. She can be found online at http://jesstess.com.\nJohn O'Duinn (Firefox Release Engineering): John has led Mozilla's Release Engineering group since May 2007. In that time, he's led work to streamline\nMozilla's release mechanics, improve developer productivity\u2014and do it all while also making the lives of Release Engineers better. John got involved in\nRelease Engineering 19 years ago when he shipped software that reintroduced a bug that had been fixed in a previous release. John's blog is at\nhttp://oduinn.com/.\nGuillaume Paumier (MediaWiki): Guillaume is Technical Communications Manager at the Wikimedia Foundation, the nonprofit behind Wikipedia and MediaWiki.\nA Wikipedia photographer and editor since 2005, Guillaume is the author of a Wikipedia handbook in French. He also holds an engineering degree in Physics\nand a PhD in microsystems for life sciences. His home online is at http://guillaumepaumier.com.\nBenjamin Peterson (PyPy): Benjamin contributes to CPython and PyPy as well as several Python libraries. In general, he is interested in compilers and\ninterpreters, particularly for dynamic languages. Outside of programming, he enjoys music (clarinet, piano, and composition), pure math, German literature, and\ngreat food. His website is http://benjamin-peterson.org.\nSimon Peyton Jones (The Glasgow Haskell Compiler): Simon Peyton Jones is a researcher at Microsoft Research Cambridge, before which he was a professor\nof computer science at Glasgow University. Inspired by the elegance of purely-functional programming when he was a student, Simon has focused nearly thirty\nyears of research on pursuing that idea to see where it leads. Haskell is his first baby, and still forms the platform for much of his research.\nhttp://research.microsoft.com/~simonpj\nSusan Potter (Git): Susan is a polyglot software developer with a penchant for skepticism. She has been designing, developing and deploying distributed\ntrading services and applications since 1996, recently switching to building multi-tenant systems for software firms. Susan is a passionate power user of Git,\nLinux, and Vim. You can find her tweeting random thoughts on Erlang, Haskell, Scala, and (of course) Git @SusanPotter .\nEric Raymond (GPSD): Eric S. Raymond is a wandering anthropologist and trouble-making philosopher. He's written some code, too. If you're not laughing by\nnow, why are you reading this book?\nJennifer Ruttan (OSCAR): Jennifer Ruttan lives in Toronto. Since graduating from the University of Toronto with a degree in Computer Science, she has\nworked as a software engineer for Indivica, a company devoted to improving patient health care through the use of new technology. Follow her on Twitter\n@jenruttan .\nStan Shebs (GDB): Stan has had open source as his day job since 1989, when a colleague at Apple needed a compiler to generate code for an experimental\nVM and GCC 1.31 was conveniently at hand. After following up with the oft-disbelieved Mac System 7 port of GCC (it was the experiment's control case), Stan\nwent to Cygnus Support, where he maintained GDB for the FSF and helped on many embedded tools projects. Returning to Apple in 2000, he worked on GCC\nand GDB for Mac OS X. A short time at Mozilla preceded a jump to CodeSourcery, now part of Mentor Graphics, where he continues to develop new features\nfor GDB. Stan's professorial tone is explained by his PhD in Computer Science from the University of Utah.\nMichael Snoyman (Yesod): Michael Snoyman received his BS in Mathematics from UCLA. After working as an actuary in the US, he moved to Israel and began\na career in web development. In order to produce high-performance, robust sites quickly, he created the Yesod Web Framework and its associated libraries.\nJeffrey M. Squyres (Open MPI): Jeff works in the rack server division at Cisco; he is Cisco's representative to the MPI Forum standards body and is a chapter\nauthor of the MPI-2 standard. Jeff is Cisco's core software developer in the open source Open MPI project. He has worked in the High Performance Computing\n(HPC) field since his early graduate-student days in the mid-1990s. After some active duty tours in the military, Jeff received his doctorate in Computer Science\nand Engineering from the University of Notre Dame in 2004. He blogs about High Performance Computing Networking.\nMartin S\u00fastrik (ZeroMQ): Martin S\u00fastrik is an expert in the field of messaging middleware, and participated in the creation and reference implementation of the\nAMQP standard. He has been involved in various messaging projects in the financial industry. He is a founder of the \u00d8MQ project, and currently is working on\nintegration of messaging technology with operating systems and the Internet stack. He can be reached at sustrik@250bpm.com , http://www.250bpm.com and\non Twitter as @sustrik .\n", "page": 4, "type": "text", "section": "Page 4"}
{"text": "Johan Harjono\nJustin Sheehy\nNikita Pchelin\nLaurie McDougall Sookraj\nTom Plaskon\nGreg Lapouchnian\nWill Schroeder\nBill Hoffman\nAudrey Tang\nJames Crook\nTodd Ritchie\nJosh McCarthy\nAndrew Petersen\nPascal Rapicault\nEric Aderhold\nJonathan Deber\nTrevor Bekolay\nTaavi Burns\nTina Yee\nColin Morris\nChristian Muise\nDavid Scannell\nVictor Ng\nBlake Winton\nKim Moir\nSimon Stewart\nJonathan Dursi\nRichard Barry\nRic Holt\nMaria Khomenko\nErick Dransch\nIan Bull\nEllen Hsiang\nChristopher Svec (FreeRTOS): Chris is an embedded software engineer who currently develops firmware for low power wireless chips. In a previous life he\ndesigned x86 processors, which comes in handy more often than you'd think when working on non-x86 processors. Chris has bachelor's and master's degrees\nin Electrical and Computer Engineering, both from Purdue University. He lives in Boston with his wife and golden retriever. You can find him on the web at\nhttp://saidsvec.com.\nBarry Warsaw (GNU Mailman): Barry Warsaw is the project leader for GNU Mailman. He has been a core Python developer since 1995, and release manager\nfor several Python versions. He currently works for Canonical as a software engineer on the Ubuntu Platform Foundations team. He can be reached at\nbarry@python.org  or @pumpichank  on Twitter. His home page is http://barry.warsaw.us.\nGreg Wilson (editorial): Greg has worked over the past 25 years in high-performance scientific computing, data visualization, and computer security, and is the\nauthor or editor of several computing books (including the 2008 Jolt Award winner Beautiful Code) and two books for children. Greg received a PhD in\nComputer Science from the University of Edinburgh in 1993.\nArmen Zambrano Gasparnian (Firefox Release Engineering): Armen has been working for Mozilla since 2008 as a Release Engineer. He has worked on\nreleases, developers' infrastructure optimization and localization. Armen works with youth at the Church on the Rock, Toronto, and has worked with international\nChristian non-profits for years. Armen has a bachelor in Software Development from Seneca College and has taken a few years of Computer Science at the\nUniversity of Malaga. He blogs at http://armenzg.blogspot.com.\nAcknowledgments\nWe would like to thank Google for their support of Amy Brown's work on this project, and Cat Allman for arranging it. We would also like to thank all of our\ntechnical reviewers:\nespecially Tavish Armstrong and Trevor Bekolay, without whose above-and-beyond assistance this book would have taken a lot longer to produce. Thanks also\nto everyone who offered to review but was unable to for various reasons, and to everyone else who helped and supported the production of this book.\nThank you also to James Howe (http://jameshowephotography.com/), who kindly let us use his picture of New York's Equitable Building for the cover.\nContributing\nDozens of volunteers worked hard to create this book, but there is still a lot to do. You can help by reporting errors, helping to translate the content into other\nlanguages, or describing the architecture of other open source projects. Please contact us at aosa@aosabook.org  if you would like to get involved.\n", "page": 5, "type": "text", "section": "Page 5"}
{"text": "<>\nOpen source software has become a fundamental building block for some of the biggest websites. And\nas those websites have grown, best practices and guiding principles around their architectures have\nemerged. This chapter seeks to cover some of the key issues to consider when designing large websites,\nas well as some of the building blocks used to achieve these goals.\nThis chapter is largely focused on web systems, although some of the material is applicable to other\ndistributed systems as well.\n1.1. Principles of Web Distributed Systems Design\nWhat exactly does it mean to build and operate a scalable web site or application? At a primitive level it's\njust connecting users with remote resources via the Internet\u2014the part that makes it scalable is that the\nresources, or access to those resources, are distributed across multiple servers.\nLike most things in life, taking the time to plan ahead when building a web service can help in the long\nrun; understanding some of the considerations and tradeoffs behind big websites can result in smarter\ndecisions at the creation of smaller web sites. Below are some of the key principles that influence the\ndesign of large-scale web systems:\nAvailability: The uptime of a website is absolutely critical to the reputation and functional of many\ncompanies. For some of the larger online retail sites, being unavailable for even minutes can result in\nthousands or millions of dollars in lost revenue, so designing their systems to be constantly available\nand resilient to failure is both a fundamental business and a technology requirement. High availability\nin distributed systems requires the careful consideration of redundancy for key components, rapid\nrecovery in the event of partial system failures, and graceful degradation when problems occur.\nPerformance: Website performance has become an important consideration for most sites. The\nspeed of a website affects usage and user satisfaction, as well as search engine rankings, a factor\nthat directly correlates to revenue and retention. As a result, creating a system that is optimized for\nfast responses and low latency is key.\nReliability: A system needs to be reliable, such that a request for data will consistently return the\nsame data. In the event the data changes or is updated, then that same request should return the\nnew data. Users need to know that if something is written to the system, or stored, it will persist and\ncan be relied on to be in place for future retrieval.\nScalability: When it comes to any large distributed system, size is just one aspect of scale that\nneeds to be considered. Just as important is the effort required to increase capacity to handle\ngreater amounts of load, commonly referred to as the scalability of the system. Scalability can refer\nto many different parameters of the system: how much additional traffic can it handle, how easy is it\nto add more storage capacity, or even how many more transactions can be processed.\nManageability: Designing a system that is easy to operate is another important consideration. The\nmanageability of the system equates to the scalability of operations: maintenance and updates.\nThings to consider for manageability are the ease of diagnosing and understanding problems when\nthey occur, ease of making updates or modifications, and how simple the system is to operate. (I.e.,\ndoes it routinely operate without failure or exceptions?)\nCost: Cost is an important factor. This obviously can include hardware and software costs, but it is\nalso important to consider other facets needed to deploy and maintain the system. The amount of\ndeveloper time the system takes to build, the amount of operational effort required to run the system,\nand even the amount of training required should all be considered. Cost is the total cost of\nownership.\nScalable Web\nArchitecture\nand Distributed Systems\nKate Matsudaira\n", "page": 6, "type": "text", "section": "Page 6"}
{"text": "Each of these principles provides the basis for decisions in designing a distributed web architecture.\nHowever, they also can be at odds with one another, such that achieving one objective comes at the cost\nof another. A basic example: choosing to address capacity by simply adding more servers (scalability)\ncan come at the price of manageability (you have to operate an additional server) and cost (the price of\nthe servers).\nWhen designing any sort of web application it is important to consider these key principles, even if it is to\nacknowledge that a design may sacrifice one or more of them.\n1.2. The Basics\nWhen it comes to system architecture there are a few things to consider: what are the right pieces, how\nthese pieces fit together, and what are the right tradeoffs. Investing in scaling before it is needed is\ngenerally not a smart business proposition; however, some forethought into the design can save\nsubstantial time and resources in the future.\nThis section is focused on some of the core factors that are central to almost all large web applications:\nservices, redundancy, partitions, and handling failure. Each of these factors involves choices and\ncompromises, particularly in the context of the principles described in the previous section. In order to\nexplain these in detail it is best to start with an example.\nExample: Image Hosting Application\nAt some point you have probably posted an image online. For big sites that host and deliver lots of\nimages, there are challenges in building an architecture that is cost-effective, highly available, and has\nlow latency (fast retrieval).\nImagine a system where users are able to upload their images to a central server, and the images can\nbe requested via a web link or API, just like Flickr or Picasa. For the sake of simplicity, let's assume that\nthis application has two key parts: the ability to upload (write) an image to the server, and the ability to\nquery for an image. While we certainly want the upload to be efficient, we care most about having very\nfast delivery when someone requests an image (for example, images could be requested for a web page\nor other application). This is very similar functionality to what a web server or Content Delivery Network\n(CDN) edge server (a server CDN uses to store content in many locations so content is\ngeographically/physically closer to users, resulting in faster performance) might provide.\nOther important aspects of the system are:\nThere is no limit to the number of images that will be stored, so storage scalability, in terms of image\ncount needs to be considered.\nThere needs to be low latency for image downloads/requests.\nIf a user uploads an image, the image should always be there (data reliability for images).\nThe system should be easy to maintain (manageability).\nSince image hosting doesn't have high profit margins, the system needs to be cost-effective\nFigure 1.1 is a simplified diagram of the functionality.\nFigure 1.1: Simplified architecture diagram for image hosting application\nIn this image hosting example, the system must be perceivably fast, its data stored reliably and all of\nthese attributes highly scalable. Building a small version of this application would be trivial and easily\nhosted on a single server; however, that would not be interesting for this chapter. Let's assume that we\nwant to build something that could grow as big as Flickr.\nServices\nWhen considering scalable system design, it helps to decouple functionality and think about each part of\nthe system as its own service with a clearly defined interface. In practice, systems designed in this way\nare said to have a Service-Oriented Architecture (SOA). For these types of systems, each service has its\n", "page": 7, "type": "text", "section": "Page 7"}
{"text": "own distinct functional context, and interaction with anything outside of that context takes place through\nan abstract interface, typically the public-facing API of another service.\nDeconstructing a system into a set of complementary services decouples the operation of those pieces\nfrom one another. This abstraction helps establish clear relationships between the service, its underlying\nenvironment, and the consumers of that service. Creating these clear delineations can help isolate\nproblems, but also allows each piece to scale independently of one another. This sort of service-oriented\ndesign for systems is very similar to object-oriented design for programming.\nIn our example, all requests to upload and retrieve images are processed by the same server; however,\nas the system needs to scale it makes sense to break out these two functions into their own services.\nFast-forward and assume that the service is in heavy use; such a scenario makes it easy to see how\nlonger writes will impact the time it takes to read the images (since they two functions will be competing\nfor shared resources). Depending on the architecture this effect can be substantial. Even if the upload\nand download speeds are the same (which is not true of most IP networks, since most are designed for\nat least a 3:1 download-speed:upload-speed ratio), read files will typically be read from cache, and\nwrites will have to go to disk eventually (and perhaps be written several times in eventually consistent\nsituations). Even if everything is in memory or read from disks (like SSDs), database writes will almost\nalways be slower than reads. (Pole Position, an open source tool for DB benchmarking,\nhttp://polepos.org/ and results http://polepos.sourceforge.net/results/PolePositionClientServer.pdf.).\nAnother potential problem with this design is that a web server like Apache or lighttpd typically has an\nupper limit on the number of simultaneous connections it can maintain (defaults are around 500, but can\ngo much higher) and in high traffic, writes can quickly consume all of those. Since reads can be\nasynchronous, or take advantage of other performance optimizations like gzip compression or chunked\ntransfer encoding, the web server can switch serve reads faster and switch between clients quickly\nserving many more requests per second than the max number of connections (with Apache and max\nconnections set to 500, it is not uncommon to serve several thousand read requests per second). Writes,\non the other hand, tend to maintain an open connection for the duration for the upload, so uploading a\n1MB file could take more than 1 second on most home networks, so that web server could only handle\n500 such simultaneous writes.\nFigure 1.2: Splitting out reads and writes\nPlanning for this sort of bottleneck makes a good case to split out reads and writes of images into their\nown services, shown in Figure 1.2. This allows us to scale each of them independently (since it is likely\nwe will always do more reading than writing), but also helps clarify what is going on at each point.\nFinally, this separates future concerns, which would make it easier to troubleshoot and scale a problem\nlike slow reads.\nThe advantage of this approach is that we are able to solve problems independently of one another\u2014we\ndon't have to worry about writing and retrieving new images in the same context. Both of these services\nstill leverage the global corpus of images, but they are free to optimize their own performance with\nservice-appropriate methods (for example, queuing up requests, or caching popular images\u2014more on\nthis below). And from a maintenance and cost perspective each service can scale independently as\nneeded, which is great because if they were combined and intermingled, one could inadvertently impact\nthe performance of the other as in the scenario discussed above.\nOf course, the above example can work well when you have two different endpoints (in fact this is very\nsimilar to several cloud storage providers' implementations and Content Delivery Networks). There are\nlots of ways to address these types of bottlenecks though, and each has different tradeoffs.\nFor example, Flickr solves this read/write issue by distributing users across different shards such that\neach shard can only handle a set number of users, and as users increase more shards are added to the\ncluster (see the presentation on Flickr's scaling, http://mysqldba.blogspot.com/2008/04/mysql-uc-2007-\npresentation-file.html). In the first example it is easier to scale hardware based on actual usage (the\n", "page": 8, "type": "text", "section": "Page 8"}
{"text": "number of reads and writes across the whole system), whereas Flickr scales with their user base (but\nforces the assumption of equal usage across users so there can be extra capacity). In the former an\noutage or issue with one of the services brings down functionality across the whole system (no-one can\nwrite files, for example), whereas an outage with one of Flickr's shards will only affect those users. In the\nfirst example it is easier to perform operations across the whole dataset\u2014for example, updating the write\nservice to include new metadata or searching across all image metadata\u2014whereas with the Flickr\narchitecture each shard would need to be updated or searched (or a search service would need to be\ncreated to collate that metadata\u2014which is in fact what they do).\nWhen it comes to these systems there is no right answer, but it helps to go back to the principles at the\nstart of this chapter, determine the system needs (heavy reads or writes or both, level of concurrency,\nqueries across the data set, ranges, sorts, etc.), benchmark different alternatives, understand how the\nsystem will fail, and have a solid plan for when failure happens.\nRedundancy\nIn order to handle failure gracefully a web architecture must have redundancy of its services and data.\nFor example, if there is only one copy of a file stored on a single server, then losing that server means\nlosing that file. Losing data is seldom a good thing, and a common way of handling it is to create\nmultiple, or redundant, copies.\nThis same principle also applies to services. If there is a core piece of functionality for an application,\nensuring that multiple copies or versions are running simultaneously can secure against the failure of a\nsingle node.\nCreating redundancy in a system can remove single points of failure and provide a backup or spare\nfunctionality if needed in a crisis. For example, if there are two instances of the same service running in\nproduction, and one fails or degrades, the system can failover to the healthy copy. Failover can happen\nautomatically or require manual intervention.\nAnother key part of service redundancy is creating a shared-nothing architecture. With this architecture,\neach node is able to operate independently of one another and there is no central \"brain\" managing state\nor coordinating activities for the other nodes. This helps a lot with scalability since new nodes can be\nadded without special conditions or knowledge. However, and most importantly, there is no single point of\nfailure in these systems, so they are much more resilient to failure.\nFor example, in our image server application, all images would have redundant copies on another piece\nof hardware somewhere (ideally in a different geographic location in the event of a catastrophe like an\nearthquake or fire in the data center), and the services to access the images would be redundant, all\npotentially servicing requests. (See Figure 1.3.) (Load balancers are a great way to make this possible,\nbut there is more on that below).\nFigure 1.3: Image hosting application with redundancy\nPartitions\nThere may be very large data sets that are unable to fit on a single server. It may also be the case that\nan operation requires too many computing resources, diminishing performance and making it necessary\nto add capacity. In either case you have two choices: scale vertically or horizontally.\nScaling vertically means adding more resources to an individual server. So for a very large data set, this\nmight mean adding more (or bigger) hard drives so a single server can contain the entire data set. In the\ncase of the compute operation, this could mean moving the computation to a bigger server with a faster\nCPU or more memory. In each case, vertical scaling is accomplished by making the individual resource\ncapable of handling more on its own.\nTo scale horizontally, on the other hand, is to add more nodes. In the case of the large data set, this\nmight be a second server to store parts of the data set, and for the computing resource it would mean\nsplitting the operation or load across some additional nodes. To take full advantage of horizontal scaling,\nit should be included as an intrinsic design principle of the system architecture, otherwise it can be quite\n", "page": 9, "type": "text", "section": "Page 9"}
{"text": "cumbersome to modify and separate out the context to make this possible.\nWhen it comes to horizontal scaling, one of the more common techniques is to break up your services\ninto partitions, or shards. The partitions can be distributed such that each logical set of functionality is\nseparate; this could be done by geographic boundaries, or by another criteria like non-paying versus\npaying users. The advantage of these schemes is that they provide a service or data store with added\ncapacity.\nIn our image server example, it is possible that the single file server used to store images could be\nreplaced by multiple file servers, each containing its own unique set of images. (See Figure 1.4.) Such an\narchitecture would allow the system to fill each file server with images, adding additional servers as the\ndisks become full. The design would require a naming scheme that tied an image's filename to the server\ncontaining it. An image's name could be formed from a consistent hashing scheme mapped across the\nservers. Or alternatively, each image could be assigned an incremental ID, so that when a client makes\na request for an image, the image retrieval service only needs to maintain the range of IDs that are\nmapped to each of the servers (like an index).\nFigure 1.4: Image hosting application with redundancy and partitioning\nOf course there are challenges distributing data or functionality across multiple servers. One of the key\nissues is data locality; in distributed systems the closer the data to the operation or point of computation,\nthe better the performance of the system. Therefore it is potentially problematic to have data spread\nacross multiple servers, as any time it is needed it may not be local, forcing the servers to perform a\ncostly fetch of the required information across the network.\nAnother potential issue comes in the form of inconsistency. When there are different services reading and\nwriting from a shared resource, potentially another service or data store, there is the chance for race\nconditions\u2014where some data is supposed to be updated, but the read happens prior to the update\u2014and\nin those cases the data is inconsistent. For example, in the image hosting scenario, a race condition\ncould occur if one client sent a request to update the dog image with a new title, changing it from \"Dog\"\nto \"Gizmo\", but at the same time another client was reading the image. In that circumstance it is unclear\nwhich title, \"Dog\" or \"Gizmo\", would be the one received by the second client.\nThere are certainly some obstacles associated with partitioning data, but partitioning allows each problem\nto be split\u2014by data, load, usage patterns, etc.\u2014into manageable chunks. This can help with scalability\nand manageability, but is not without risk. There are lots of ways to mitigate risk and handle failures;\nhowever, in the interest of brevity they are not covered in this chapter. If you are interested in reading\nmore, you can check out my blog post on fault tolerance and monitoring.\n1.3. The Building Blocks of Fast and Scalable Data\nAccess\nHaving covered some of the core considerations in designing distributed systems, let's now talk about the\nhard part: scaling access to the data.\nMost simple web applications, for example, LAMP stack applications, look something like Figure 1.5.\nFigure 1.5: Simple web applications\nAs they grow, there are two main challenges: scaling access to the app server and to the database. In a\nhighly scalable application design, the app (or web) server is typically minimized and often embodies a\nshared-nothing architecture. This makes the app server layer of the system horizontally scalable. As a\n", "page": 10, "type": "text", "section": "Page 10"}
{"text": "result of this design, the heavy lifting is pushed down the stack to the database server and supporting\nservices; it's at this layer where the real scaling and performance challenges come into play.\nThe rest of this chapter is devoted to some of the more common strategies and methods for making\nthese types of services fast and scalable by providing fast access to data.\nFigure 1.6: Oversimplified web application\nMost systems can be oversimplified to Figure 1.6. This is a great place to start. If you have a lot of data,\nyou want fast and easy access, like keeping a stash of candy in the top drawer of your desk. Though\noverly simplified, the previous statement hints at two hard problems: scalability of storage and fast\naccess of data.\nFor the sake of this section, let's assume you have many terabytes (TB) of data and you want to allow\nusers to access small portions of that data at random. (See Figure 1.7.) This is similar to locating an\nimage file somewhere on the file server in the image application example.\nFigure 1.7: Accessing specific data\nThis is particularly challenging because it can be very costly to load TBs of data into memory; this directly\ntranslates to disk IO. Reading from disk is many times slower than from memory\u2014memory access is as\nfast as Chuck Norris, whereas disk access is slower than the line at the DMV. This speed difference\nreally adds up for large data sets; in real numbers memory access is as little as 6 times faster for\nsequential reads, or 100,000 times faster for random reads, than reading from disk (see \"The\nPathologies of Big Data\", http://queue.acm.org/detail.cfm?id=1563874). Moreover, even with unique IDs,\nsolving the problem of knowing where to find that little bit of data can be an arduous task. It's like trying\nto get that last Jolly Rancher from your candy stash without looking.\nThankfully there are many options that you can employ to make this easier; four of the more important\nones are caches, proxies, indexes and load balancers. The rest of this section discusses how each of\nthese concepts can be used to make data access a lot faster.\nCaches\nCaches take advantage of the locality of reference principle: recently requested data is likely to be\nrequested again. They are used in almost every layer of computing: hardware, operating systems, web\nbrowsers, web applications and more. A cache is like short-term memory: it has a limited amount of\nspace, but is typically faster than the original data source and contains the most recently accessed items.\nCaches can exist at all levels in architecture, but are often found at the level nearest to the front end,\nwhere they are implemented to return data quickly without taxing downstream levels.\nHow can a cache be used to make your data access faster in our API example? In this case, there are a\ncouple of places you can insert a cache. One option is to insert a cache on your request layer node, as\nin Figure 1.8.\n", "page": 11, "type": "text", "section": "Page 11"}
{"text": "Figure 1.8: Inserting a cache on your request layer node\nPlacing a cache directly on a request layer node enables the local storage of response data. Each time a\nrequest is made to the service, the node will quickly return local, cached data if it exists. If it is not in the\ncache, the request node will query the data from disk. The cache on one request layer node could also\nbe located both in memory (which is very fast) and on the node's local disk (faster than going to network\nstorage).\nFigure 1.9: Multiple caches\nWhat happens when you expand this to many nodes? As you can see in Figure 1.9, if the request layer\nis expanded to multiple nodes, it's still quite possible to have each node host its own cache. However, if\nyour load balancer randomly distributes requests across the nodes, the same request will go to different\nnodes, thus increasing cache misses. Two choices for overcoming this hurdle are global caches and\ndistributed caches.\nGlobal Cache\nA global cache is just as it sounds: all the nodes use the same single cache space. This involves adding\na server, or file store of some sort, faster than your original store and accessible by all the request layer\nnodes. Each of the request nodes queries the cache in the same way it would a local one. This kind of\ncaching scheme can get a bit complicated because it is very easy to overwhelm a single cache as the\nnumber of clients and requests increase, but is very effective in some architectures (particularly ones with\nspecialized hardware that make this global cache very fast, or that have a fixed dataset that needs to be\ncached).\nThere are two common forms of global caches depicted in the diagrams. In Figure 1.10, when a cached\n", "page": 12, "type": "text", "section": "Page 12"}
{"text": "response is not found in the cache, the cache itself becomes responsible for retrieving the missing piece\nof data from the underlying store. In Figure 1.11 it is the responsibility of request nodes to retrieve any\ndata that is not found in the cache.\nFigure 1.10: Global cache where cache is responsible for retrieval\nFigure 1.11: Global cache where request nodes are responsible for retrieval\nThe majority of applications leveraging global caches tend to use the first type, where the cache itself\nmanages eviction and fetching data to prevent a flood of requests for the same data from the clients.\nHowever, there are some cases where the second implementation makes more sense. For example, if\nthe cache is being used for very large files, a low cache hit percentage would cause the cache buffer to\nbecome overwhelmed with cache misses; in this situation it helps to have a large percentage of the total\ndata set (or hot data set) in the cache. Another example is an architecture where the files stored in the\ncache are static and shouldn't be evicted. (This could be because of application requirements around that\ndata latency\u2014certain pieces of data might need to be very fast for large data sets\u2014where the application\nlogic understands the eviction strategy or hot spots better than the cache.)\nDistributed Cache\nIn a distributed cache (Figure 1.12), each of its nodes own part of the cached data, so if a refrigerator\nacts as a cache to the grocery store, a distributed cache is like putting your food in several locations\u2014\nyour fridge, cupboards, and lunch box\u2014convenient locations for retrieving snacks from, without a trip to\nthe store. Typically the cache is divided up using a consistent hashing function, such that if a request\nnode is looking for a certain piece of data it can quickly know where to look within the distributed cache\nto determine if that data is available. In this case, each node has a small piece of the cache, and will\nthen send a request to another node for the data before going to the origin. Therefore, one of the\n", "page": 13, "type": "text", "section": "Page 13"}
{"text": "advantages of a distributed cache is the increased cache space that can be had just by adding nodes to\nthe request pool.\nA disadvantage of distributed caching is remedying a missing node. Some distributed caches get around\nthis by storing multiple copies of the data on different nodes; however, you can imagine how this logic\ncan get complicated quickly, especially when you add or remove nodes from the request layer. Although\neven if a node disappears and part of the cache is lost, the requests will just pull from the origin\u2014so it\nisn't necessarily catastrophic!\nFigure 1.12: Distributed cache\nThe great thing about caches is that they usually make things much faster (implemented correctly, of\ncourse!) The methodology you choose just allows you to make it faster for even more requests. However,\nall this caching comes at the cost of having to maintain additional storage space, typically in the form of\nexpensive memory; nothing is free. Caches are wonderful for making things generally faster, and\nmoreover provide system functionality under high load conditions when otherwise there would be\ncomplete service degradation.\nOne example of a popular open source cache is Memcached (http://memcached.org/) (which can work\nboth as a local cache and distributed cache); however, there are many other options (including many\nlanguage- or framework-specific options).\nMemcached is used in many large web sites, and even though it can be very powerful, it is simply an in-\nmemory key value store, optimized for arbitrary data storage and fast lookups (O(1)).\nFacebook uses several different types of caching to obtain their site performance (see \"Facebook caching\nand performance\"). They use $GLOBALS  and APC caching at the language level (provided in PHP at the\ncost of a function call) which helps make intermediate function calls and results much faster. (Most\nlanguages have these types of libraries to improve web page performance and they should almost always\nbe used.) Facebook then use a global cache that is distributed across many servers (see \"Scaling\nmemcached at Facebook\"), such that one function call accessing the cache could make many requests in\nparallel for data stored on different Memcached servers. This allows them to get much higher\nperformance and throughput for their user profile data, and have one central place to update data (which\nis important, since cache invalidation and maintaining consistency can be challenging when you are\nrunning thousands of servers).\nNow let's talk about what to do when the data isn't in the cache\u2026\nProxies\nAt a basic level, a proxy server is an intermediate piece of hardware/software that receives requests from\nclients and relays them to the backend origin servers. Typically, proxies are used to filter requests, log\nrequests, or sometimes transform requests (by adding/removing headers, encrypting/decrypting, or\n", "page": 14, "type": "text", "section": "Page 14"}
{"text": "compression).\nFigure 1.13: Proxy server\nProxies are also immensely helpful when coordinating requests from multiple servers, providing\nopportunities to optimize request traffic from a system-wide perspective. One way to use a proxy to\nspeed up data access is to collapse the same (or similar) requests together into one request, and then\nreturn the single result to the requesting clients. This is known as collapsed forwarding.\nImagine there is a request for the same data (let's call it littleB) across several nodes, and that piece of\ndata is not in the cache. If that request is routed thought the proxy, then all of those requests can be\ncollapsed into one, which means we only have to read littleB off disk once. (See Figure 1.14.) There is\nsome cost associated with this design, since each request can have slightly higher latency, and some\nrequests may be slightly delayed to be grouped with similar ones. But it will improve performance in high\nload situations, particularly when that same data is requested over and over. This is similar to a cache,\nbut instead of storing the data/document like a cache, it is optimizing the requests or calls for those\ndocuments and acting as a proxy for those clients.\nIn a LAN proxy, for example, the clients do not need their own IPs to connect to the Internet, and the\nLAN will collapse calls from the clients for the same content. It is easy to get confused here though,\nsince many proxies are also caches (as it is a very logical place to put a cache), but not all caches act\nas proxies.\nFigure 1.14: Using a proxy server to collapse requests\nAnother great way to use the proxy is to not just collapse requests for the same data, but also to collapse\nrequests for data that is spatially close together in the origin store (consecutively on disk). Employing\nsuch a strategy maximizes data locality for the requests, which can result in decreased request latency.\nFor example, let's say a bunch of nodes request parts of B: partB1, partB2, etc. We can set up our proxy\nto recognize the spatial locality of the individual requests, collapsing them into a single request and\nreturning only bigB, greatly minimizing the reads from the data origin. (See Figure 1.15.) This can make a\nreally big difference in request time when you are randomly accessing across TBs of data! Proxies are\nespecially helpful under high load situations, or when you have limited caching, since they can essentially\nbatch several requests into one.\nFigure 1.15: Using a proxy to collapse requests for data that is spatially close together\nIt is worth noting that you can use proxies and caches together, but generally it is best to put the cache\nin front of the proxy, for the same reason that it is best to let the faster runners start first in a crowded\nmarathon race. This is because the cache is serving data from memory, it is very fast, and it doesn't\nmind multiple requests for the same result. But if the cache was located on the other side of the proxy\nserver, then there would be additional latency with every request before the cache, and this could hinder\nperformance.\nIf you are looking at adding a proxy to your systems, there are many options to consider; Squid and\nVarnish have both been road tested and are widely used in many production web sites. These proxy\nsolutions offer many optimizations to make the most of client-server communication. Installing one of\nthese as a reverse proxy (explained in the load balancer section below) at the web server layer can\n", "page": 15, "type": "text", "section": "Page 15"}
{"text": "improve web server performance considerably, reducing the amount of work required to handle incoming\nclient requests.\nIndexes\nUsing an index to access your data quickly is a well-known strategy for optimizing data access\nperformance; probably the most well known when it comes to databases. An index makes the trade-offs\nof increased storage overhead and slower writes (since you must both write the data and update the\nindex) for the benefit of faster reads.\nJust as to a traditional relational data store, you can also apply this concept to larger data sets. The trick\nwith indexes is you must carefully consider how users will access your data. In the case of data sets that\nare many TBs in size, but with very small payloads (e.g., 1 KB), indexes are a necessity for optimizing\ndata access. Finding a small payload in such a large data set can be a real challenge since you can't\npossibly iterate over that much data in any reasonable time. Furthermore, it is very likely that such a\nlarge data set is spread over several (or many!) physical devices\u2014this means you need some way to find\nthe correct physical location of the desired data. Indexes are the best way to do this.\nFigure 1.16: Indexes\nAn index can be used like a table of contents that directs you to the location where your data lives. For\nexample, let's say you are looking for a piece of data, part 2 of B\u2014how will you know where to find it? If\nyou have an index that is sorted by data type\u2014say data A, B, C\u2014it would tell you the location of data B\nat the origin. Then you just have to seek to that location and read the part of B you want. (See Figure\n1.16.)\nThese indexes are often stored in memory, or somewhere very local to the incoming client request.\nBerkeley DBs (BDBs) and tree-like data structures are commonly used to store data in ordered lists, ideal\nfor access with an index.\nOften there are many layers of indexes that serve as a map, moving you from one location to the next,\nand so forth, until you get the specific piece of data you want. (See Figure 1.17.)\nFigure 1.17: Many layers of indexes\nIndexes can also be used to create several different views of the same data. For large data sets, this is a\ngreat way to define different filters and sorts without resorting to creating many additional copies of the\ndata.\nFor example, imagine that the image hosting system from earlier is actually hosting images of book\npages, and the service allows client queries across the text in those images, searching all the book\n", "page": 16, "type": "text", "section": "Page 16"}
{"text": "content about a topic, in the same way search engines allow you to search HTML content. In this case,\nall those book images take many, many servers to store the files, and finding one page to render to the\nuser can be a bit involved. First, inverse indexes to query for arbitrary words and word tuples need to be\neasily accessible; then there is the challenge of navigating to the exact page and location within that\nbook, and retrieving the right image for the results. So in this case the inverted index would map to a\nlocation (such as book B), and then B may contain an index with all the words, locations and number of\noccurrences in each part.\nAn inverted index, which could represent Index1 in the diagram above, might look something like the\nfollowing\u2014each word or tuple of words provide an index of what books contain them.\nWord(s)\nBook(s)\nbeing awesome\nBook B, Book C, Book D\nalways\nBook C, Book F\nbelieve\nBook B\nThe intermediate index would look similar but would contain just the words, location, and information for\nbook B. This nested index architecture allows each of these indexes to take up less space than if all of\nthat info had to be stored into one big inverted index.\nAnd this is key in large-scale systems because even compressed, these indexes can get quite big and\nexpensive to store. In this system if we assume we have a lot of the books in the world\u2014100,000,000\n(see Inside Google Books blog post)\u2014and that each book is only 10 pages long (to make the math\neasier), with 250 words per page, that means there are 250 billion words. If we assume an average of 5\ncharacters per word, and each character takes 8 bits (or 1 byte, even though some characters are 2\nbytes), so 5 bytes per word, then an index containing only each word once is over a terabyte of storage.\nSo you can see creating indexes that have a lot of other information like tuples of words, locations for the\ndata, and counts of occurrences, can add up very quickly.\nCreating these intermediate indexes and representing the data in smaller sections makes big data\nproblems tractable. Data can be spread across many servers and still accessed quickly. Indexes are a\ncornerstone of information retrieval, and the basis for today's modern search engines. Of course, this\nsection only scratched the surface, and there is a lot of research being done on how to make indexes\nsmaller, faster, contain more information (like relevancy), and update seamlessly. (There are some\nmanageability challenges with race conditions, and with the sheer number of updates required to add\nnew data or change existing data, particularly in the event where relevancy or scoring is involved).\nBeing able to find your data quickly and easily is important; indexes are an effective and simple tool to\nachieve this.\nLoad Balancers\nFinally, another critical piece of any distributed system is a load balancer. Load balancers are a principal\npart of any architecture, as their role is to distribute load across a set of nodes responsible for servicing\nrequests. This allows multiple nodes to transparently service the same function in a system. (See Figure\n1.18.) Their main purpose is to handle a lot of simultaneous connections and route those connections to\none of the request nodes, allowing the system to scale to service more requests by just adding nodes.\nFigure 1.18: Load balancer\nThere are many different algorithms that can be used to service requests, including picking a random\nnode, round robin, or even selecting the node based on certain criteria, such as memory or CPU\nutilization. Load balancers can be implemented as software or hardware appliances. One open source\nsoftware load balancer that has received wide adoption is HAProxy).\nIn a distributed system, load balancers are often found at the very front of the system, such that all\nincoming requests are routed accordingly. In a complex distributed system, it is not uncommon for a\nrequest to be routed to multiple load balancers as shown in Figure 1.19.\n", "page": 17, "type": "text", "section": "Page 17"}
{"text": "Figure 1.19: Multiple load balancers\nLike proxies, some load balancers can also route a request differently depending on the type of request it\nis. (Technically these are also known as reverse proxies.)\nOne of the challenges with load balancers is managing user-session-specific data. In an e-commerce\nsite, when you only have one client it is very easy to allow users to put things in their shopping cart and\npersist those contents between visits (which is important, because it is much more likely you will sell the\nproduct if it is still in the user's cart when they return). However, if a user is routed to one node for a\nsession, and then a different node on their next visit, there can be inconsistencies since the new node\nmay be missing that user's cart contents. (Wouldn't you be upset if you put a 6 pack of Mountain Dew in\nyour cart and then came back and it was empty?) One way around this can be to make sessions sticky\nso that the user is always routed to the same node, but then it is very hard to take advantage of some\nreliability features like automatic failover. In this case, the user's shopping cart would always have the\ncontents, but if their sticky node became unavailable there would need to be a special case and the\nassumption of the contents being there would no longer be valid (although hopefully this assumption\nwouldn't be built into the application). Of course, this problem can be solved using other strategies and\ntools in this chapter, like services, and many not covered (like browser caches, cookies, and URL\nrewriting).\nIf a system only has a couple of a nodes, systems like round robin DNS may make more sense since\nload balancers can be expensive and add an unneeded layer of complexity. Of course in larger systems\nthere are all sorts of different scheduling and load-balancing algorithms, including simple ones like\nrandom choice or round robin, and more sophisticated mechanisms that take things like utilization and\ncapacity into consideration. All of these algorithms allow traffic and requests to be distributed, and can\nprovide helpful reliability tools like automatic failover, or automatic removal of a bad node (such as when\nit becomes unresponsive). However, these advanced features can make problem diagnosis cumbersome.\nFor example, when it comes to high load situations, load balancers will remove nodes that may be slow\nor timing out (because of too many requests), but that only exacerbates the situation for the other nodes.\nIn these cases extensive monitoring is important, because overall system traffic and throughput may look\nlike it is decreasing (since the nodes are serving less requests) but the individual nodes are becoming\nmaxed out.\nLoad balancers are an easy way to allow you to expand system capacity, and like the other techniques in\nthis article, play an essential role in distributed system architecture. Load balancers also provide the\ncritical function of being able to test the health of a node, such that if a node is unresponsive or over-\nloaded, it can be removed from the pool handling requests, taking advantage of the redundancy of\ndifferent nodes in your system.\nQueues\nSo far we have covered a lot of ways to read data quickly, but another important part of scaling the data\nlayer is effective management of writes. When systems are simple, with minimal processing loads and\nsmall databases, writes can be predictably fast; however, in more complex systems writes can take an\nalmost non-deterministically long time. For example, data may have to be written several places on\ndifferent servers or indexes, or the system could just be under high load. In the cases where writes, or\nany task for that matter, may take a long time, achieving performance and availability requires building\nasynchrony into the system; a common way to do that is with queues.\n", "page": 18, "type": "text", "section": "Page 18"}
{"text": "Figure 1.20: Synchronous request\nImagine a system where each client is requesting a task to be remotely serviced. Each of these clients\nsends their request to the server, where the server completes the tasks as quickly as possible and\nreturns the results to their respective clients. In small systems where one server (or logical service) can\nservice incoming clients just as fast as they come, this sort of situation should work just fine. However,\nwhen the server receives more requests than it can handle, then each client is forced to wait for the\nother clients' requests to complete before a response can be generated. This is an example of a\nsynchronous request, depicted in Figure 1.20.\nThis kind of synchronous behavior can severely degrade client performance; the client is forced to wait,\neffectively performing zero work, until its request can be answered. Adding additional servers to address\nsystem load does not solve the problem either; even with effective load balancing in place it is extremely\ndifficult to ensure the even and fair distribution of work required to maximize client performance. Further,\nif the server handling requests is unavailable, or fails, then the clients upstream will also fail. Solving this\nproblem effectively requires abstraction between the client's request and the actual work performed to\nservice it.\nFigure 1.21: Using queues to manage requests\nEnter queues. A queue is as simple as it sounds: a task comes in, is added to the queue and then\nworkers pick up the next task as they have the capacity to process it. (See Figure 1.21.) These tasks\ncould represent simple writes to a database, or something as complex as generating a thumbnail preview\nimage for a document. When a client submits task requests to a queue they are no longer forced to wait\nfor the results; instead they need only acknowledgement that the request was properly received. This\nacknowledgement can later serve as a reference for the results of the work when the client requires it.\n", "page": 19, "type": "text", "section": "Page 19"}
{"text": "This work is made available under the Creative Commons Attribution 3.0 Unported license. Please see\nthe full description of the license for details.\nQueues enable clients to work in an asynchronous manner, providing a strategic abstraction of a client's\nrequest and its response. On the other hand, in a synchronous system, there is no differentiation\nbetween request and reply, and they therefore cannot be managed separately. In an asynchronous\nsystem the client requests a task, the service responds with a message acknowledging the task was\nreceived, and then the client can periodically check the status of the task, only requesting the result once\nit has completed. While the client is waiting for an asynchronous request to be completed it is free to\nperform other work, even making asynchronous requests of other services. The latter is an example of\nhow queues and messages are leveraged in distributed systems.\nQueues also provide some protection from service outages and failures. For instance, it is quite easy to\ncreate a highly robust queue that can retry service requests that have failed due to transient server\nfailures. It is more preferable to use a queue to enforce quality-of-service guarantees than to expose\nclients directly to intermittent service outages, requiring complicated and often-inconsistent client-side\nerror handling.\nQueues are fundamental in managing distributed communication between different parts of any large-\nscale distributed system, and there are lots of ways to implement them. There are quite a few open\nsource queues like RabbitMQ, ActiveMQ, BeanstalkD, but some also use services like Zookeeper, or\neven data stores like Redis.\n1.4. Conclusion\nDesigning efficient systems with fast access to lots of data is exciting, and there are lots of great tools\nthat enable all kinds of new applications. This chapter covered just a few examples, barely scratching the\nsurface, but there are many more\u2014and there will only continue to be more innovation in the space.\nBack to top\nBack to The Architecture of Open Source Applications.\n", "page": 20, "type": "text", "section": "Page 20"}
{"text": "<>\nRecently, the Mozilla Release Engineering team has made numerous advances in release automation for\nFirefox. We have reduced the requirements for human involvement during signing and sending notices to\nstakeholders, and have automated many other small manual steps, because each manual step in the\nprocess is an opportunity for human error. While what we have now isn't perfect, we're always striving to\nstreamline and automate our release process. Our final goal is to be able to push a button and walk\naway; minimal human intervention will eliminate many of the headaches and do-overs we experienced\nwith our older part-manual, part-automated release processes. In this chapter, we will explore and\nexplain the scripts and infrastructure decisions that comprise the complete Firefox rapid release system,\nas of Firefox 10.\nYou'll follow the system from the perspective of a release-worthy Mercurial changeset as it is turned into\na release candidate\u2014and then a public release\u2014available to over 450 million daily users worldwide. We'll\nstart with builds and code signing, then customized partner and localization repacks, the QA process,\nand how we generate updates for every supported version, platform and localization. Each of these steps\nmust be completed before the release can be pushed out to Mozilla Community's network of mirrors\nwhich provide the downloads to our users.\nWe'll look at some of the decisions that have been made to improve this process; for example, our\nsanity-checking script that helps eliminate much of what used to be vulnerable to human error; our\nautomated signing script; our integration of mobile releases into the desktop release process; patcher,\nwhere updates are created; and AUS (Application Update Service), where updates are served to our\nusers across multiple versions of the software.\nThis chapter describes the mechanics of how we generate release builds for Firefox. Most of this chapter\ndetails the significant steps that occur in a release process once the builds start, but there is also plenty\nof complex cross-group communication to deal with before Release Engineering even starts to generate\nrelease builds, so let's start there.\n2.1. Look N Ways Before You Start a Release\nFirefox\nRelease\nEngineering\nChris AtLee, Lukas Blakk, John O'Duinn, and Armen Zambrano Gasparian\n", "page": 21, "type": "text", "section": "Page 21"}
{"text": "Figure 2.1: Getting from code to \"Go to build\"\nWhen we started on the project to improve Mozilla's release process, we began with the premise that the\nmore popular Firefox became, the more users we would have, and the more attractive a target Firefox\nwould become to blackhat hackers looking for security vulnerabilities to exploit. Also, the more popular\nFirefox became, the more users we would have to protect from a newly discovered security vulnerability,\nso the more important it would be to be able to deliver a security fix as quickly as possible. We even\nhave a term for this: a \"chemspill\" release (short for \"chemical spill\"). Instead of being surprised by the\noccasional need for a chemspill release in between our regularly scheduled releases, we decided to plan\nas if every release could be a chemspill release, and designed our release automation accordingly.\nThis mindset has three important consequences:\n1. We do a postmortem after every release, and look to see where things could be made smoother,\neasier, and faster next time. If at all possible, we find and fix at least one thing, no matter how small,\nimmediately\u2014before the next release. This constant polishing of our release automation means we're\nalways looking for new ways to rely on less human involvement while also improving robustness and\nturnaround time. A lot of effort is spent making our tools and processes bulletproof so that \"rare\"\nevents like network hiccups, disk space issues or typos made by real live humans are caught and\nhandled as early as possible. Even though we're already fast enough for regular, non-chemspill\nreleases, we want to reduce the risk of any human error in a future release. This is especially true in\na chemspill release.\n2. When we do have a chemspill release, the more robust the release automation, the less stressed\nthe humans in Release Engineering are. We're used to the idea of going as fast as possible with\ncalm precision, and we've built tools to do this as safely and robustly as we know how. Less stress\nmeans more calm and precise work within a well-rehearsed process, which in turn helps chemspill\nreleases go smoothly.\n3. We created a Mozilla-wide \"go to build\" process. When doing a regular (non-chemspill) release, it's\npossible to have everyone look through the same bug triage queries, see clearly when the last fix\nwas landed and tested successfully, and reach consensus on when to start builds. However, in a\nchemspill release\u2014where minutes matter\u2014keeping track of all the details of the issue as well as\nfollowing up bug confirmations and fixes gets very tricky very quickly. To reduce complexity and the\nrisk of mistakes, Mozilla now has a full-time person to track the readiness of the code for a \"go to\nbuild\" decision. Changing processes during a chemspill is risky, so in order to make sure everyone is\nfamiliar with the process when minutes matter, we use this same process for chemspill and regular\n", "page": 22, "type": "text", "section": "Page 22"}
{"text": "releases.\nFigure 2.2: Complete release timeline, using a chemspill as example\n2.2. \"Go to Build\"\nWho Can Send the \"Go to Build\"?\nBefore the start of the release, one person is designated to assume responsibility for coordinating the\nentire release. This person needs to attend triage meetings, understand the background context on all\nthe work being landed, referee bug severity disputes fairly, approve landing of late-breaking changes, and\nmake tough back-out decisions. Additionally, on the actual release day this person is on point for all\ncommunications with the different groups (developers, QA, Release Engineering, website developers, PR,\nmarketing, etc.).\nDifferent companies use different titles for this role. Some titles we've heard include Release Manager,\nRelease Engineer, Program Manager, Project Manager, Product Manager, Product Czar, Release Driver.\nIn this chapter, we will use the term \"Release Coordinator\" as we feel it most clearly defines the role in\nour process as described above. The important point here is that the role, and the final authority of the\nrole, is clearly understood by everyone before the release starts, regardless of their background or\nprevious work experiences elsewhere. In the heat of a release day, it is important that everyone knows to\nabide by, and trust, the coordination decisions that this person makes.\nThe Release Coordinator is the only person outside of Release Engineering who is authorized to send\n\"stop builds\" emails if a show-stopper problem is discovered. Any reports of suspected show-stopper\nproblems are redirected to the Release Coordinator, who will evaluate, make the final go/no-go decision\nand communicate that decision to everyone in a timely manner. In the heat of the moment of a release\nday, we all have to abide by, and trust, the coordination decisions that this person makes.\nHow to Send the \"Go to Build\"?\nEarly experiments with sending \"go to build\" in IRC channels or verbally over the phone led to\nmisunderstandings, occasionally causing problems for the release in progress. Therefore, we now require\nthat the \"go to build\" signal for every release is done by email, to a mailing list that includes everyone\nacross all groups involved in release processes. The subject of the email includes \"go to build\" and the\nexplicit product name and version number; for example:\ngo to build Firefox 6.0.1\nSimilarly, if a problem is found in the release, the Release Coordinator will send a new \"all stop\" email to\nthe same mailing list, with a new subject line. We found that it was not sufficient to just hit reply on the\nmost recent email about the release; email threading in some email clients caused some people to not\nnotice the \"all stop\" email if it was way down a long and unrelated thread.\nWhat Is In the \"Go to Build\" Email?\n1. The exact code to be used in the build; ideally, the URL to the specific change in the source code\nrepository that the release builds are to be created from.\n1. Instructions like \"use the latest code\" are never acceptable; in one release, after the \"go to build\"\nemail was sent and before builds started, a well-intentioned developer landed a change, without\napproval, in the wrong branch. The release included that unwanted change in the builds.\nThankfully the mistake was caught before we shipped, but we did have to delay the release\nwhile we did a full stop and rebuilt everything.\n2. In a time-based version control system like CVS, be fully explicit about the exact time to use;\ngive the time down to seconds, and specify timezone. In one release, when Firefox was still\nbased on CVS, the Release Coordinator specified the cutoff time to be used for the builds but\ndid not give the timezone. By the time Release Engineering noticed the missing timezone info,\nthe Release Coordinator was asleep. Release Engineering correctly guessed that the intent was\nlocal time (in California), but in a late-night mixup over PDT instead of PST we ended up\n", "page": 23, "type": "text", "section": "Page 23"}
{"text": "missing the last critical bug fix. This was caught by QA before we shipped, but we had to stop\nbuilds and start the build over using the correct cutoff time.\n2. A clear sense of the urgency for this particular release. While it sounds obvious, it is important when\nhandling some important edge cases, so here is a quick summary:\n1. Some releases are \"routine\", and can be worked on in normal working hours. They are a pre-\nscheduled release, they are on schedule, and there is no emergency. Of course, all release\nbuilds need to be created in a timely manner, but there is no need for release engineers to pull\nall-nighters and burn out for a routine release. Instead, we schedule them properly in advance\nso everything stays on schedule with people working normal hours. This keeps people fresh and\nbetter able to handle unscheduled urgent work if the need arises.\n2. Some releases are \"chemspills\", and are urgent, where minutes matter. These are typically to fix\na published security exploit, or to fix a newly introduced top-crash problem impacting a large\npercentage of our user base. Chemspills need to be created as quickly as possible and are\ntypically not pre-scheduled releases.\n3. Some releases change from routine to chemspill or from chemspill to routine. For example, if a\nsecurity fix in a routine release was accidentally leaked, it is now a chemspill release. If a\nbusiness requirement like a \"special sneak preview\" release for an upcoming conference\nannouncement was delayed for business reasons, the release now changes from chemspill to\nroutine.\n4. Some releases have different people holding different opinions on whether the release is normal\nor urgent, depending on their perspective on the fixes being shipped in the release.\nIt is the role of the Release Coordinator to balance all the facts and opinions, reach a decision, and then\ncommunicate that decision about urgency consistently across all groups. If new information arrives, the\nRelease Coordinator reassesses, and then communicates the new urgency to all the same groups.\nHaving some groups believe a release is a chemspill, while other groups believe the same release is\nroutine can be destructive to cross-group cohesion.\nFinally, these emails also became very useful to measure where time was spent during a release. While\nthey are only accurate to wall-clock time resolution, this accuracy is really helpful when figuring out\nwhere next to focus our efforts on making things faster. As the old adage goes, before you can improve\nsomething, you have to be able to measure it.\nThroughout the beta cycle for Firefox we also do weekly releases from our mozilla-beta  repository.\nEach one of these beta releases goes through our usual full release automation, and is treated almost\nidentically to our regular final releases. To minimize surprises during a release, our intent is to have no\nnew untested changes to release automation or infrastructure by the time we start the final release builds.\n2.3. Tagging, Building, and Source Tarballs\nFigure 2.3: Automated tagging\nIn preparation for starting automation, we recently started to use a script, release_sanity.py , that\nwas originally written by a Release Engineering summer intern. This Python script assists a release\nengineer with double-checking that all configurations for a release match what is checked into our tools\nand configuration repositories. It also checks what is in the specified release code revisions for\nmozilla-release  and all the (human) languages for this release, which will be what the builds and\nlanguage repacks are generated from.\nThe script accepts the buildbot config files for any release configurations that will be used (such as\ndesktop or mobile), the branch to look at (e.g., mozilla-release ), the build and version number, and\nthe names of the products that are to be built (such as \"fennec\" or \"firefox\"). It will fail if the release\nrepositories do not match what's in the configurations, if locale repository changesets don't match our\nshipping locales and localization changeset files, or if the release version and build number don't match\n", "page": 24, "type": "text", "section": "Page 24"}
{"text": "what has been given to our build tools with the tag generated using the product, version, and build\nnumber. If all the tests in the script pass, it will reconfigure the buildbot master where the script is being\nrun and where release builders will be triggered, and then generate the \"send change\" that starts the\nautomated release process.\nAfter a release engineer kicks off builders, the first automated step in the Firefox release process is\ntagging all the related source code repositories to record which revision of the source, language\nrepositories, and related tools are being used for this version and build number of a release candidate.\nThese tags allow us to keep a history of Firefox and Fennec (mobile Firefox) releases' version and build\nnumbers in our release repositories. For Firefox releases, one example tag set is\nFIREFOX_10_0_RELEASE FIREFOX_10_0_BUILD1 FENNEC_10_0_RELEASE FENNEC_10_0_BUILD1 .\nA single Firefox release uses code from about 85 version control repositories that host things such as the\nproduct code, localization strings, release automation code, and helper utilities. Tagging all these\nrepositories is critical to ensure that future steps of the release automation are all executed using the\nsame set of revisions. It also has a number of other benefits: Linux distributions and other contributors\ncan reproduce builds with exactly the same code and tools that go into the official builds, and it also\nrecords the revisions of source and tools used on a per-release basis for future comparison of what\nchanged between releases. Once all the repositories are branched and tagged, a series of dependent\nbuilders automatically start up: one builder for each release platform plus a source bundle that includes\nall source used in the release. The source bundle and built installers are all uploaded to the release\ndirectory as they become available. This allows anyone to see exactly what code is in a release, and\ngives a snapshot that would allow us to re-create the builds if we ever needed to (for example, if our\nVCS failed somehow). For the Firefox build's source, sometimes we need to import code from an earlier\nrepository. For example, with a beta release this means pulling in the signed-off revision from Mozilla-\nAurora (our more-stable-than-Nightly repository) for Firefox 10.0b1. For a release it means pulling in the\napproved changes from Mozilla-Beta (typically the same code used for 10.0b6) to the Mozilla-Release\nrepository. This release branch is then created as a named branch whose parent changeset is the\nsigned-off revision from the `go to build' provided by the Release Coordinator. The release branch can be\nused to make release-specific modifications to the source code, such as bumping version numbers or\nfinalizing the set of locales that will be built. If a critical security vulnerability is discovered in the future\nthat requires an immediate fix\u2014a chemspill\u2014a minimal set of changes to address the vulnerability will be\nlanded on this relbranch and a new version of Firefox generated and released from it. When we have to\ndo another round of builds for a particular release, buildN, we use these relbranches to grab the same\ncode that was signed off on for `go to build', which is where any changes to that release code will have\nbeen landed. The automation starts again and bumps the tagging to the new changeset on that\nrelbranch. Our tagging process does a lot of operations with local and remote Mercurial repositories. To\nstreamline some of the most common operations we've written a few tools to assist us: retry.py  and\nhgtool.py . retry.py  is a simple wrapper that can take a given command and run it, retrying several\ntimes if it fails. It can also watch for exceptional output conditions and retry or report failure in those\ncases. We've found it useful to wrap retry.py  around most of the commands which can fail due to\nexternal dependencies. For tagging, the Mercurial operations could fail due to temporary network\noutages, web server issues, or the backend Mercurial server being temporarily overloaded. Being able to\nautomatically retry these operations and continue saves a lot of our time, since we don't have to manually\nrecover, clean up any fallout and then get the release automation running again. hgtool.py  is a utility\nthat encapsulates several common Mercurial operations, like cloning, pulling, and updating with a single\ninvocation. It also adds support for Mercurial's share extension, which we use extensively to avoid having\nseveral full clones of repositories in different directories on the same machine. Adding support for shared\nlocal repositories significantly sped up our tagging process, since most full clones of the product and\nlocale repositories could be avoided. An important motivation for developing tools like these is making our\nautomation as testable as possible. Because tools like hgtool.py  are small, single-purpose utilities built\non top of reusable libraries, they're much easier to test in isolation.\nToday our tagging is done in two parallel jobs: one for desktop Firefox which takes around 20 minutes to\ncomplete as it includes tagging 80+ locale repositories, and another for mobile Firefox which takes\naround 10 minutes to complete since we have fewer locales currently available for our mobile releases. In\nthe future we would like to streamline our release automation process so that we tag all the various\nrepositories in parallel. The initial builds can be started as soon as the product code and tools\nrequirement repository is tagged, without having to wait for all the locale repositories to be tagged. By\nthe time these builds are finished, the rest of the repositories will have been tagged so that localization\nrepackages and future steps can be completed. We estimate this can reduce the total time to have builds\nready by 15 minutes.\n2.4. Localization Repacks and Partner Repacks\nOnce the desktop builds are generated and uploaded to ftp.mozilla.org , our automation triggers the\nlocalization repackaging jobs. A \"localization repack\" takes the original build (which contains the en-US\nlocale), unpacks it, replaces the en-US strings with the strings for another locale that we are shipping in\nthis release, then repackages all the files back up again (this is why we call them repacks). We repeat\nthis for each locale shipping in the release. Originally, we did all repacks serially. However, as we added\nmore locales, this took a long time to complete, and we had to restart from the beginning if anything\nfailed out mid-way through.\n", "page": 25, "type": "text", "section": "Page 25"}
{"text": "Figure 2.4: Repacking Firefox for each localization\nNow, we instead split the entire set of repacks into six jobs, each processed concurrently on six different\nmachines. This approach completes the work in approximately a sixth of the time. This also allows us to\nredo a subset of repacks if an individual repack fails, without having to redo all repacks. (We could split\nthe repacks into even more, smaller, concurrent jobs, but we found it took away too many machines from\nthe pool, which affected other unrelated jobs triggered by developers on our continuous integration\nsystem.)\nThe process for mobile (on Android) is slightly different, as we produce only two installers: an English\nversion and a multi-locale version with over a dozen languages built into the installer instead of a\nseparate build per locale. The size of this multi-locale version is an issue, especially with slow download\nspeeds onto small mobile devices. One proposal for the future is to have other languages be requested\non demand as add-ons from addons.mozilla.org .\nIn Figure 2.4, you can see that we currently rely on three different sources for our locale information:\nshipped_locales , l10_changesets  and l10n-changesets_mobile-release.json . (There is a\nplan to move all three into a unified JSON file.) These files contain information about the different\nlocalizations we have, and certain platform exceptions. Specifically, for a given localization we need to\nknow which revision of the repository to use for a given release and we need to know if the localization\ncan build on all of our supported platforms (e.g., Japanese for Mac comes from a different repository all\ntogether). Two of these files are used for the Desktop releases and one for the Mobile release (this\nJSON file contains both the list of platforms and the changesets).\nWho decides which languages we ship? First of all, localizers themselves nominate their specific\nchangeset for a given release. The nominated changeset gets reviewed by Mozilla's localization team\nand shows up in a web dashboard that lists the changesets needed for each language. The Release\nCoordinator reviews this before sending the \"go to build\" email. On the day of a release, we retrieve this\nlist of changesets and we repackage them accordingly.\nBesides localization repackages we also generate partner repackages. These are customized builds for\nvarious partners we have who want to customize the experience for their customers. The main type of\nchanges are custom bookmarks, custom homepage and custom search engines but many other things\ncan be changed. These customized builds are generated for the latest Firefox release and not for betas.\n2.5. Signing\nIn order for users to be sure that the copy of Firefox they have downloaded is indeed the unmodified\nbuild from Mozilla, we apply a few different types of digital signatures to the builds.\nThe first type of signing is for our Windows builds. We use a Microsoft Authenticode (signcode) signing\nkey to sign all our .exe  and .dll  files. Windows can use these signatures to verify that the\napplication comes from a trusted source. We also sign the Firefox installer executable with the\nAuthenticode key.\nNext we use GPG to generate a set of MD5 and SHA1 checksums for all the builds on all platforms, and\ngenerate detached GPG signatures for the checksum files as well as all the builds and installers. These\nsignatures are used by mirrors and other community members to validate their downloads.\n", "page": 26, "type": "text", "section": "Page 26"}
{"text": "For security purposes, we sign on a dedicated signing machine that is blocked off via firewall and VPN\nfrom outside connections. Our keyphrases, passwords, and keystores are passed among release\nengineers only in secure channels, often in person, to minimize the risk of exposure as much as\npossible.\nFigure 2.5: Signing Firefox installers\nUntil recently this signing process involved a release engineer working on a dedicated server (the\n\"signing master\") for almost an hour manually downloading builds, signing them, and uploading them\nback to ftp.mozilla.org  before the automation could continue. Once signing on the master was\ncompleted and all files were uploaded, a log file of all the signing activities was uploaded to the release\ncandidates directory on ftp.mozilla.org . The appearance of this log file on ftp.mozilla.org\nsignified the end of human signing work and from that point, dependent builders watching for that log file\ncould resume automation. Recently we've added an additional wrapper of automation around the signing\nsteps. Now the release engineer opens a Cygwin shell on the signing master and sets up a few\nenvironment variables pertaining to the release, like VERSION , BUILD , TAG , and RELEASE_CONFIG ,\nthat help the script find the right directories on ftp.mozilla.org  and know when all the deliverables\nfor a release have been downloaded so that the signing can start. After checking out the most recent\nproduction version of our signing tools, the release engineer simply runs make autosign . The release\nengineer then enters two passphrases, one for gpg and one for signcode. Once these passphrases are\nautomatically verified by the make scripts, the automation starts a download loop that watches for\nuploaded builds and repacks from the release automation and downloads them as they become available.\nOnce all items have been downloaded, the automation begins signing immediately, without human\nintervention.\nNot needing a human to sign is important for two reasons. Firstly, it reduces the risk of human error.\nSecondly, it allows signing to proceed during non-work hours, without needing a release engineer awake\nat a computer at the time.\nAll deliverables have an MD5SUM and SHA1SUM generated for them, and those hash values are written\nto files of the same name. These files will be uploaded back to the release-candidates directory as well\nas synced into the final location of the release on ftp.mozilla.org  once it is live, so that anyone who\n", "page": 27, "type": "text", "section": "Page 27"}
{"text": "downloads a Firefox installer from one of our mirrors can ensure they got the correct object. When all\nsigned bits are available and verified they are uploaded back to ftp.mozilla.org  along with the\nsigning log file, which the automation is waiting for to proceed.\nOur next planned round of improvements to the signing process will create a tool that allows us to sign\nbits at the time of build/repack. This work requires creating a signing server application that can receive\nrequests to sign files from the release build machines. It will also require a signing client tool which would\ncontact the signing server, authenticate itself as a machine trusted to request signing, upload the files to\nbe signed, wait for the build to be signed, download the signed bits, and then include them as part of the\npackaged build. Once these enhancements are in production, we can discontinue our current all-at-once\nsigning process, as well as our all-at-once generate-updates process (more on this below). We expect\nthis work to trim a few hours off our current end-to-end times for a release.\n2.6. Updates\nUpdates are created so users can update to the latest version of Firefox quickly and easily using our\nbuilt-in updater, without having to download and run a standalone installer. From the user's perspective,\nthe downloading of the update package happens quietly in the background. Only after the update files\nare downloaded, and ready to be applied, will Firefox prompt the user with the option to apply the update\nand restart.\nThe catch is, we generate a lot of updates. For a series of releases on a product line, we generate\nupdates from all supported previous releases in the series to the new latest release for that product line.\nFor Firefox LATEST , that means generating updates for every platform, every locale, and every installer\nfrom Firefox LATEST-1 , LATEST-2 , LATEST-3 , \u2026 in both complete and partial forms. We do all this\nfor several different product lines at a time.\nOur update generation automation modifies the update configuration files of each release's build off a\nbranch to maintain our canonical list of what version numbers, platforms, and localizations need to have\nupdates created to offer users this newest release. We offer updates as \"snippets\". As you can see in\nthe example below, this snippet is simply an XML pointer file hosted on our AUS (Application Update\nService) that informs the user's Firefox browser where the complete and/or partial .mar  (Mozilla\nArchive) files are hosted.\nMajor Updates vs. Minor Updates\n<updates>\n  <update type=\"minor\" version=\"7.0.1\" extensionVersion=\"7.0.1\"\n          buildID=\"20110928134238\"\n          detailsURL=\"https://www.mozilla.com/en-US/firefox/7.0.1/releasenotes/\">\n    <patch type=\"complete\"\n           URL=\"http://download.mozilla.org/?product=firefox-7.0.1-complete&os=osx&\nlang=en-US&force=1\"\n           hashFunction=\"SHA512\"\n           hashValue=\"7ecdbc110468b9b4627299794d793874436353dc36c80151550b08830f9d8\nc5afd7940c51df9270d54e11fd99806f41368c0f88721fa17e01ea959144f473f9d\"\n           size=\"28680122\"/>\n    <patch type=\"partial\"\n           URL=\"http://download.mozilla.org/?product=firefox-7.0.1-partial-6.0.2&os\n=osx&lang=en-US&force=1\"\n           hashFunction=\"SHA512\"\n           hashValue=\"e9bb49bee862c7a8000de6508d006edf29778b5dbede4deaf3cfa05c22521\nfc775da126f5057621960d327615b5186b27d75a378b00981394716e93fc5cca11a\"\n           size=\"10469801\"/>\n    </update>\n</updates>\nFigure 2.6: Sample update snippet\nAs you can see in Figure 2.6, update snippets have a type  attribute which can be either major  or\nminor . Minor updates keep people updated to the latest version available in their release train; for\nexample, it would update all 3.6.* release users to the latest 3.6 release, all rapid-release beta users to\nthe latest beta, all Nightly users to the latest Nightly build, etc. Most of the time, updates are minor and\ndon't require any user interaction other than a confirmation to apply the update and restart the browser.\nMajor updates are used when we need to advertise to our users that the latest and greatest release is\navailable, prompting them that \"A new version of Firefox is available, would you like to update?\" and\ndisplaying a billboard showcasing the leading features in the new release. Our new rapid-release system\nmeans we no longer need to do as many major updates; we'll be able to stop generating major updates\nonce the 3.6.* release train is no longer supported.\nComplete Updates vs. Partial Updates\nAt build time we generate \"complete update\" .mar  files which contain all the files for the new release,\ncompressed with bz2  and then archived into a .mar  file. Both complete and partial updates are\n", "page": 28, "type": "text", "section": "Page 28"}
{"text": "downloaded automatically through the update channel to which a user's Firefox is registered. We have\ndifferent update channels (that is, release users look for updates on release channel, beta users look on\nbeta channel, etc.) so that we can serve updates to, for example, release users at a different time than\nwe serve updates to beta users.\nPartial update .mar files are created by comparing the complete .mar for the old release with the\ncomplete .mar for the new release to create a \"partial-update\" .mar  file containing the binary diff of any\nchanged files, and a manifest file. As you can see in the sample snippet in Figure 2.6, this results in a\nmuch smaller file size for partial updates. This is very important for users with slower or dial-up Internet\nconnections.\nIn older versions of our update automation the generation of partial updates for all locales and platforms\ncould take six to seven hours for one release, as the complete .mar files were downloaded, diffed, and\npackaged into a partial-update .mar file. Eventually it was discovered that even across platforms, many\ncomponent changes were identical, therefore many diffs could be re-used. With a script that cached the\nhash for each part of the diff, our partial update creation time was brought down to approximately 40\nminutes.\nAfter the snippets are uploaded and are hosted on AUS, an update verification step is run to a) test\ndownloading the snippets and b) run the updater with the downloaded .mar file to confirm that the\nupdates apply correctly.\nGeneration of partial-update .mar files, as well as all the update snippets, is currently done after signing\nis complete. We do this because generation of the partial updates must be done between signed files of\nthe two releases, and therefore generation of the snippets must wait until the signed builds are available.\nOnce we're able to integrate signing into the build process, we can generate partial updates immediately\nafter completing a build or repack. Together with improvements to our AUS software, this means that\nonce we're finished builds and repacks we would be able to push immediately to mirrors. This effectively\nparallelizes the creation of all the updates, trimming several hours from our total time.\n2.7. Pushing to Internal Mirrors and QA\nVerifying that the release process is producing the expected deliverables is key. This is accomplished by\nQA's verification and sign offs process.\nOnce the signed builds are available, QA starts manual and automated testing. QA relies on a mix of\ncommunity members, contractors and employees in different timezones to speed up this verification\nprocess. Meanwhile, our release automation generates updates for all languages and all platforms, for all\nsupported releases. These update snippets are typically ready before QA has finished verifying the\nsigned builds. QA then verifies that users can safely update from various previous releases to the newest\nrelease using these updates.\nMechanically, our automation pushes the binaries to our \"internal mirrors\" (Mozilla-hosted servers) in\norder for QA to verify updates. Only after QA has finished verification of the builds and the updates will\nwe push them to our community mirrors. These community mirrors are essential to handle the global load\nof users, by allowing them to request their updates from local mirror nodes instead of from\nftp.mozilla.org  directly. It's worth noting that we do not make builds and updates available on the\ncommunity mirrors until after QA signoff, because of complications that arise if QA finds a last-minute\nshowstopper and the candidate build needs to be withdrawn.\nThe validation process after builds and updates are generated is:\nQA, along with community and contractors in other timezones, does manual testing.\nQA triggers the automation systems to do functional testing.\nQA independently verifies that fixed problems and new features for that release are indeed fixed and\nof good enough quality to ship to users.\nMeanwhile, release automation generates the updates.\nQA signs off the builds.\nQA signs off the updates.\nNote that users don't get updates until QA has signed off and the Release Coordinator has sent the email\nasking to push the builds and updates live.\n2.8. Pushing to Public Mirrors and AUS\nOnce the Release Coordinator gets signoff from QA and various other groups at Mozilla, they give\nRelease Engineering the go-ahead to push the files to our community mirror network. We rely on our\ncommunity mirrors to be able to handle a few hundred million users downloading updates over the next\nfew days. All the installers, as well as the complete and partial updates for all platforms and locales, are\nalready on our internal mirror network at this point. Publishing the files to our external mirrors involves\nmaking a change to an rsync exclude file for the public mirrors module. Once this change is made, the\nmirrors will start to synchronize the new release files. Each mirror has a score or weighting associated\nwith it; we monitor which mirrors have synchronized the files and sum their individual scores to compute\na total \"uptake\" score. Once a certain uptake threshold is reached, we notify the Release Coordinator\nthat the mirrors have enough uptake to handle the release.\nThis is the point at which the release becomes \"official\". After the Release Coordinator sends the final\n", "page": 29, "type": "text", "section": "Page 29"}
{"text": "\"go live\" email, Release Engineering will update the symlinks on the web server so that visitors to our\nweb and ftp sites can find the latest new version of Firefox. We also publish all the update snippets for\nusers on past versions of Firefox to AUS.\nFirefox installed on users' machines regularly checks our AUS servers to see if there's an updated version\nof Firefox available for them. Once we publish these update snippets, users are able to automatically\nupdate Firefox to the latest version.\n2.9. Lessons Learned\nAs software engineers, our temptation is to jump to solve what we see as the immediate and obvious\ntechnical problem. However, Release Engineering spans across different fields\u2014both technical and non-\ntechnical\u2014so being aware of technical and non-technical issues is important.\nThe Importance of Buy-in from Other Stakeholders\nIt was important to make sure that all stakeholders understood that our slow, fragile release engineering\nexposed the organization, and our users, to risks. This involved all levels of the organization\nacknowledging the lost business opportunities, and market risks, caused by slow fragile automation.\nFurther, Mozilla's ability to protect our users with super-fast turnaround on releases became more\nimportant as we grew to have more users, which in turn made us more attractive as a target.\nInterestingly, some people had only ever experienced fragile release automation in their careers, so came\nto Mozilla with low, \"oh, it's always this bad\" expectations. Explaining the business gains expected with a\nrobust, scalable release automation process helped everyone understand the importance of the \"invisible\"\nRelease Engineering improvement work we were about to undertake.\nInvolving Other Groups\nTo make the release process more efficient and more reliable required work, by Release Engineering and\nother groups across Mozilla. However, it was interesting to see how often \"it takes a long time to ship a\nrelease\" was mistranslated as \"it takes Release Engineering a long time to ship a release\". This\nmisconception ignored the release work done by groups outside of Release Engineering, and was\ndemotivating to the Release Engineers. Fixing this misconception required educating people across\nMozilla on where time was actually spent by different groups during a release. We did this with low-tech\n\"wall-clock\" timestamps on emails of clear handoffs across groups, and a series of \"wall-clock\" blog\nposts detailing where time was spent.\nThese helped raise awareness of which different groups were actually involved in a release.\nThese helped people appreciate whenever RelEng got processes to run faster, which in turn helped\nmotivate Release Engineers to make further improvements.\nThese helped other groups think about how they too could help improve the overall release process\n\u2014a big mindset shift for the entire organization.\nFinally, these also eliminated all the unclear handoff communications across groups, which had\nhistorically cost us many respins, false-starts, and other costly disruptions to the release process.\nEstablishing Clear Handoffs\nMany of our \"release engineering\" problems were actually people problems: miscommunication between\nteams; lack of clear leadership; and the resulting stress, fatigue and anxiety during chemspill releases.\nBy having clear handoffs to eliminate these human miscommunications, our releases immediately started\nto go more smoothly, and cross-group human interactions quickly improved.\nManaging Turnover\nWhen we started this project, we were losing team members too often. In itself, this is bad. However, the\nlack of accurate up-to-date documentation meant that most of the technical understanding of the release\nprocess was documented by folklore and oral histories, which we lost whenever a person left. We\nneeded to turn this situation around, urgently.\nWe felt the best way to improve morale and show that things were getting better was to make sure\npeople could see that we had a plan to make things better, and that people had some control over their\nown destiny. We did this by making sure that we set aside time to fix at least one thing\u2014anything!\u2014after\neach release. We implemented this by negotiating for a day or two of \"do not disturb\" time immediately\nafter we shipped a release. Solving immediate small problems, while they were still fresh in people's\nminds, helped clear distractions, so people could focus on larger term problems in subsequent releases.\nMore importantly, this gave people the feeling that we had regained some control over our own fate, and\nthat things were truly getting better.\nManaging Change\nBecause of market pressures, Mozilla's business and product needs from the release process changed\nwhile we were working on improving it. This is not unusual and should be expected.\nWe knew we had to continue shipping releases using the current release process, while we were building\nthe new process. We decided against attempting to build a separate \"greenfield project\" while also\nsupporting the existing systems; we felt the current systems were so fragile that we literally would not\nhave the time to do anything new.\n", "page": 30, "type": "text", "section": "Page 30"}
{"text": "This work is made available under the Creative Commons Attribution 3.0 Unported license. Please see\nthe full description of the license for details.\nWe also assumed from the outset that we didn't fully understand what was broken. Each incremental\nimprovement allowed us to step back and check for new surprises, before starting work on the next\nimprovement. Phrases like \"draining the swamp\", \"peeling the onion\", and \"how did this ever work?\" were\nheard frequently whenever we discovered new surprises throughout this project.\nGiven all this, we decided to make lots of small, continuous improvements to the existing process. Each\niterative improvement made the next release a little bit better. More importantly, each improvement freed\nup just a little bit more time during the next release, which allowed a release engineer a little more time\nto make the next improvement. These improvements snowballed until we found ourselves past the tipping\npoint, and able to make time to work on significant major improvements. At that point, the gains from\nrelease optimizations really kicked in.\n2.10. For More Information\nWe're really proud of the work done so far, and the abilities that it has brought to Mozilla in a newly\nheated-up global browser market.\nFour years ago, doing two chemspill releases in a month would be a talking point within Mozilla. By\ncontrast, last week a published exploit in a third-party library caused Mozilla to ship eight chemspills\nreleases in two low-fuss days.\nAs with everything, our release automation still has plenty of room for improvement, and our needs and\ndemands continue to change. For a look at our ongoing work, please see:\nChris AtLee's blog\nLukas Blakk's blog\nJohn O'Duinn's blog\nArmen Zambrano Gasparnian's blog\nDocumentation on the design and flow of our Mercurial-based release process\nRelease Engineering's build repositories. In particular, the buildbotcustom, buildbot-configs, and tools\nrepositories are used heavily for releases.\nThe Firefox 7.0 Beta 4 Build Notes. In addition to code, we document every aspect of a release. This\nlink is to our 7.0b4 release notes, but you can find all our release notes if you edit the URL\nappropriately.\nBack to top\nBack to The Architecture of Open Source Applications.\n", "page": 31, "type": "text", "section": "Page 31"}
{"text": "<>\nFreeRTOS (pronounced \"free-arr-toss\") is an open source real-time operating system (RTOS) for\nembedded systems. FreeRTOS supports many different architectures and compiler toolchains, and is\ndesigned to be \"small, simple, and easy to use\".\nFreeRTOS is under active development, and has been since Richard Barry started work on it in 2002. As\nfor me, I'm not a developer of or contributor to FreeRTOS, I'm merely a user and a fan. As a result, this\nchapter will favor the \"what\" and \"how\" of FreeRTOS's architecture, with less of the \"why\" than other\nchapters in this book.\nLike all operating systems, FreeRTOS's main job is to run tasks. Most of FreeRTOS's code involves\nprioritizing, scheduling, and running user-defined tasks. Unlike all operating systems, FreeRTOS is a\nreal-time operating system which runs on embedded systems.\nBy the end of this chapter I hope that you'll understand the basic architecture of FreeRTOS. Most of\nFreeRTOS is dedicated to running tasks, so you'll get a good look at exactly how FreeRTOS does that.\nIf this is your first look under the hood of an operating system, I also hope that you'll learn the basics\nabout how any OS works. FreeRTOS is relatively simple, especially when compared to Windows, Linux,\nor OS X, but all operating systems share the same basic concepts and goals, so looking at any OS can\nbe instructive and interesting.\n3.1. What is \"Embedded\" and \"Real-Time\"?\n\"Embedded\" and \"real-time\" can mean different things to different people, so let's define them as\nFreeRTOS uses them.\nAn embedded system is a computer system that is designed to do only a few things, like the system in a\nTV remote control, in-car GPS, digital watch, or pacemaker. Embedded systems are typically smaller and\nslower than general purpose computer systems, and are also usually less expensive. A typical low-end\nembedded system may have an 8-bit CPU running at 25MHz, a few KB of RAM, and maybe 32KB of\nflash memory. A higher-end embedded system may have a 32-bit CPU running at 750MHz, a GB of\nRAM, and multiple GB of flash memory.\nReal-time systems are designed to do something within a certain amount of time; they guarantee that\nstuff happens when it's supposed to.\nA pacemaker is an excellent example of a real-time embedded system. A pacemaker must contract the\nheart muscle at the right time to keep you alive; it can't be too busy to respond in time. Pacemakers and\nother real-time embedded systems are carefully designed to run their tasks on time, every time.\n3.2. Architecture Overview\nFreeRTOS is a relatively small application. The minimum core of FreeRTOS is only three source ( .c )\nfiles and a handful of header files, totalling just under 9000 lines of code, including comments and blank\nlines. A typical binary code image is less than 10KB.\nFreeRTOS's code breaks down into three main areas: tasks, communication, and hardware interfacing.\nTasks: Almost half of FreeRTOS's core code deals with the central concern in many operating\nsystems: tasks. A task is a user-defined C function with a given priority. tasks.c  and task.h  do\nall the heavy lifting for creating, scheduling, and maintaining tasks.\nCommunication: Tasks are good, but tasks that can communicate with each other are even better!\nWhich brings us to the second FreeRTOS job: communication. About 40% of FreeRTOS's core code\ndeals with communication. queue.c  and queue.h  handle FreeRTOS communication. Tasks and\ninterrupts use queues to send data to each other and to signal the use of critical resources using\nsemaphores and mutexes.\nThe Hardware Whisperer: The approximately 9000 lines of code that make up the base of\nFreeRTOS are hardware-independent; the same code runs whether FreeRTOS is running on the\nhumble 8051 or the newest, shiniest ARM core. About 6% of FreeRTOS's core code acts a shim\nFreeRTOS\nChristopher Svec\n", "page": 32, "type": "text", "section": "Page 32"}
{"text": "between the hardware-independent FreeRTOS core and the hardware-dependent code. We'll\ndiscuss the hardware-dependent code in the next section.\nHardware Considerations\nThe hardware-independent FreeRTOS layer sits on top of a hardware-dependent layer. This hardware-\ndependent layer knows how to talk to whatever chip architecture you choose. Figure 3.1 shows\nFreeRTOS's layers.\nFigure 3.1: FreeRTOS software layers\nFreeRTOS ships with all the hardware-independent as well as hardware-dependent code you'll need to\nget a system up and running. It supports many compilers (CodeWarrior, GCC, IAR, etc.) as well as many\nprocessor architectures (ARM7, ARM Cortex-M3, various PICs, Silicon Labs 8051, x86, etc.). See the\nFreeRTOS website for a list of supported architectures and compilers.\nFreeRTOS is highly configurable by design. FreeRTOS can be built as a single CPU, bare-bones RTOS,\nsupporting only a few tasks, or it can be built as a highly functional multicore beast with TCP/IP, a file\nsystem, and USB.\nConfiguration options are selected in FreeRTOSConfig.h  by setting various #defines . Clock speed,\nheap size, mutexes, and API subsets are all configurable in this file, along with many other options. Here\nare a few examples that set the maximum number of task priority levels, the CPU frequency, the system\ntick frequency, the minimal stack size and the total heap size:\n#define configMAX_PRIORITIES      ( ( unsigned portBASE_TYPE ) 5 )\n#define configCPU_CLOCK_HZ        ( 12000000UL )\n#define configTICK_RATE_HZ        ( ( portTickType ) 1000 )\n#define configMINIMAL_STACK_SIZE  ( ( unsigned short ) 100 )\n#define configTOTAL_HEAP_SIZE     ( ( size_t ) ( 4 * 1024 ) )\nHardware-dependent code lives in separate files for each compiler toolchain and CPU architecture. For\nexample, if you're working with the IAR compiler on an ARM Cortex-M3 chip, the hardware-dependent\ncode lives in the FreeRTOS/Source/portable/IAR/ARM_CM3/  directory. portmacro.h  declares all of\nthe hardware-specific functions, while port.c  and portasm.s  contain all of the actual hardware-\ndependent code. The hardware-independent header file portable.h  #include 's the correct\nportmacro.h  file at compile time. FreeRTOS calls the hardware-specific functions using #define 'd\nfunctions declared in portmacro.h .\nLet's look at an example of how FreeRTOS calls a hardware-dependent function. The hardware-\nindependent file tasks.c  frequently needs to enter a critical section of code to prevent preemption.\nEntering a critical section happens differently on different architectures, and the hardware-independent\ntasks.c  does not want to have to understand the hardware-dependent details. So tasks.c  calls the\nglobal macro portENTER_CRITICAL() , glad to be ignorant of how it actually works. Assuming we're\nusing the IAR compiler on an ARM Cortex-M3 chip, FreeRTOS is built with the file\nFreeRTOS/Source/portable/IAR/ARM_CM3/portmacro.h  which defines portENTER_CRITICAL()\nlike this:\n#define portENTER_CRITICAL()   vPortEnterCritical()\nvPortEnterCritical()  is actually defined in FreeRTOS/Source/portable/IAR/ARM_CM3/port.c .\nThe port.c  file is hardware-dependent, and contains code that understands the IAR compiler and the\nCortex-M3 chip. vPortEnterCritical()  enters the critical section using this hardware-specific\nknowledge and returns to the hardware-independent tasks.c .\nThe portmacro.h  file also defines an architecture's basic data types. Data types for basic integer\nvariables, pointers, and the system timer tick data type are defined like this for the IAR compiler on ARM\nCortex-M3 chips:\n#define portBASE_TYPE  long              // Basic integer variable type\n#define portSTACK_TYPE unsigned long     // Pointers to memory locations\ntypedef unsigned portLONG portTickType;  // The system timer tick type\nThis method of using data types and functions through thin layers of #defines  may seem a bit\n", "page": 33, "type": "text", "section": "Page 33"}
{"text": "complicated, but it allows FreeRTOS to be recompiled for a completely different system architecture by\nchanging only the hardware-dependent files. And if you want to run FreeRTOS on an architecture it\ndoesn't currently support, you only have to implement the hardware-dependent functionality which is\nmuch smaller than the hardware-independent part of FreeRTOS.\nAs we've seen, FreeRTOS implements hardware-dependent functionality with C preprocessor #define\nmacros. FreeRTOS also uses #define  for plenty of hardware-independent code. For non-embedded\napplications this frequent use of #define  is a cardinal sin, but in many smaller embedded systems the\noverhead for calling a function is not worth the advantages that \"real\" functions offer.\n3.3. Scheduling Tasks: A Quick Overview\nTask Priorities and the Ready List\nEach task has a user-assigned priority between 0 (the lowest priority) and the compile-time value of\nconfigMAX_PRIORITIES-1  (the highest priority). For instance, if configMAX_PRIORITIES  is set to 5,\nthen FreeRTOS will use 5 priority levels: 0 (lowest priority), 1, 2, 3, and 4 (highest priority).\nFreeRTOS uses a \"ready list\" to keep track of all tasks that are currently ready to run. It implements the\nready list as an array of task lists like this:\nstatic xList pxReadyTasksLists[ configMAX_PRIORITIES ];  /* Prioritised ready tasks.  */\npxReadyTasksLists[0]  is a list of all ready priority 0 tasks, pxReadyTasksLists[1]  is a list of all\nready priority 1 tasks, and so on, all the way up to pxReadyTasksLists[configMAX_PRIORITIES-1] .\nThe System Tick\nThe heartbeat of a FreeRTOS system is called the system tick. FreeRTOS configures the system to\ngenerate a periodic tick interrupt. The user can configure the tick interrupt frequency, which is typically in\nthe millisecond range. Every time the tick interrupt fires, the vTaskSwitchContext()  function is called.\nvTaskSwitchContext()  selects the highest-priority ready task and puts it in the pxCurrentTCB\nvariable like this:\n/* Find the highest-priority queue that contains ready tasks. */\nwhile( listLIST_IS_EMPTY( &( pxReadyTasksLists[ uxTopReadyPriority ] ) ) )\n{\n    configASSERT( uxTopReadyPriority );\n    --uxTopReadyPriority;\n}\n/* listGET_OWNER_OF_NEXT_ENTRY walks through the list, so the tasks of the same \npriority get an equal share of the processor time. */\nlistGET_OWNER_OF_NEXT_ENTRY( pxCurrentTCB, &( pxReadyTasksLists[ uxTopReadyPriority ] ) );\nBefore the while loop starts, uxTopReadyPriority  is guaranteed to be greater than or equal to the\npriority of the highest-priority ready task. The while() loop starts at priority level uxTopReadyPriority\nand walks down through the pxReadyTasksLists[]  array to find the highest-priority level with ready\ntasks. listGET_OWNER_OF_NEXT_ENTRY()  then grabs the next ready task from that priority level's ready\nlist.\nNow pxCurrentTCB  points to the highest-priority task, and when vTaskSwitchContext()  returns the\nhardware-dependent code starts running that task.\nThose nine lines of code are the absolute heart of FreeRTOS. The other 8900+ lines of FreeRTOS are\nthere to make sure those nine lines are all that's needed to keep the highest-priority task running.\nFigure 3.2 is a high-level picture of what a ready list looks like. This example has three priority levels,\nwith one priority 0 task, no priority 1 tasks, and three priority 2 tasks. This picture is accurate but not\ncomplete; it's missing a few details which we'll fill in later.\n", "page": 34, "type": "text", "section": "Page 34"}
{"text": "Figure 3.2: Basic view of FreeRTOS Ready List\nNow that we have the high-level overview out of the way, let's dive in to the details. We'll look at the\nthree main FreeRTOS data structures: tasks, lists, and queues.\n3.4. Tasks\nThe main job of all operating systems is to run and coordinate user tasks. Like many operating systems,\nthe basic unit of work in FreeRTOS is the task. FreeRTOS uses a Task Control Block (TCB) to represent\neach task.\nTask Control Block (TCB)\nThe TCB is defined in tasks.c  like this:\ntypedef struct tskTaskControlBlock\n{\n  volatile portSTACK_TYPE *pxTopOfStack;                  /* Points to the location of\n                                                             the last item placed on \n                                                             the tasks stack.  THIS \n                                                             MUST BE THE FIRST MEMBER \n                                                             OF THE STRUCT. */\n                                                         \n  xListItem    xGenericListItem;                          /* List item used to place \n                                                             the TCB in ready and \n                                                             blocked queues. */\n  xListItem    xEventListItem;                            /* List item used to place \n                                                             the TCB in event lists.*/\n  unsigned portBASE_TYPE uxPriority;                      /* The priority of the task\n                                                             where 0 is the lowest \n                                                             priority. */\n  portSTACK_TYPE *pxStack;                                /* Points to the start of \n                                                             the stack. */\n  signed char    pcTaskName[ configMAX_TASK_NAME_LEN ];   /* Descriptive name given \n                                                             to the task when created.\n                                                             Facilitates debugging \n                                                             only. */\n  #if ( portSTACK_GROWTH > 0 )\n    portSTACK_TYPE *pxEndOfStack;                         /* Used for stack overflow \n                                                             checking on architectures\n                                                             where the stack grows up\n                                                             from low memory. */\n  #endif\n  #if ( configUSE_MUTEXES == 1 )\n    unsigned portBASE_TYPE uxBasePriority;                /* The priority last \n                                                             assigned to the task - \n                                                             used by the priority \n                                                             inheritance mechanism. */\n", "page": 35, "type": "text", "section": "Page 35"}
{"text": "  #endif\n} tskTCB;\nThe TCB stores the address of the stack start address in pxStack  and the current top of stack in\npxTopOfStack . It also stores a pointer to the end of the stack in pxEndOfStack  to check for stack\noverflow if the stack grows \"up\" to higher addresses. If the stack grows \"down\" to lower addresses then\nstack overflow is checked by comparing the current top of stack against the start of stack memory in\npxStack .\nThe TCB stores the initial priority of the task in uxPriority  and uxBasePriority . A task is given a\npriority when it is created, and a task's priority can be changed. If FreeRTOS implements priority\ninheritance then it uses uxBasePriority  to remember the original priority while the task is temporarily\nelevated to the \"inherited\" priority. (See the discussion about mutexes below for more on priority\ninheritance.)\nEach task has two list items for use in FreeRTOS's various scheduling lists. When a task is inserted into\na list FreeRTOS doesn't insert a pointer directly to the TCB. Instead, it inserts a pointer to either the\nTCB's xGenericListItem  or xEventListItem . These xListItem  variables let the FreeRTOS lists\nbe smarter than if they merely held a pointer to the TCB. We'll see an example of this when we discuss\nlists later.\nA task can be in one of four states: running, ready to run, suspended, or blocked. You might expect each\ntask to have a variable that tells FreeRTOS what state it's in, but it doesn't. Instead, FreeRTOS tracks\ntask state implicitly by putting tasks in the appropriate list: ready list, suspended list, etc. The presence of\na task in a particular list indicates the task's state. As a task changes from one state to another,\nFreeRTOS simply moves it from one list to another.\nTask Setup\nWe've already touched on how a task is selected and scheduled with the pxReadyTasksLists  array;\nnow let's look at how a task is initially created. A task is created when the xTaskCreate()  function is\ncalled. FreeRTOS uses a newly allocated TCB object to store the name, priority, and other details for a\ntask, then allocates the amount of stack the user requests (assuming there's enough memory available)\nand remembers the start of the stack memory in TCB's pxStack  member.\nThe stack is initialized to look as if the new task is already running and was interrupted by a context\nswitch. This way the scheduler can treat newly created tasks exactly the same way as it treats tasks that\nhave been running for a while; the scheduler doesn't need any special case code for handling new tasks.\nThe way that a task's stack is made to look like it was interrupted by a context switch depends on the\narchitecture FreeRTOS is running on, but this ARM Cortex-M3 processor's implementation is a good\nexample:\nunsigned int *pxPortInitialiseStack( unsigned int *pxTopOfStack, \n                                     pdTASK_CODE pxCode,\n                                     void *pvParameters )\n{\n  /* Simulate the stack frame as it would be created by a context switch interrupt. */\n  pxTopOfStack--; /* Offset added to account for the way the MCU uses the stack on \n                     entry/exit of interrupts. */\n  *pxTopOfStack = portINITIAL_XPSR;  /* xPSR */\n  pxTopOfStack--;\n  *pxTopOfStack = ( portSTACK_TYPE ) pxCode;  /* PC */\n  pxTopOfStack--;\n  *pxTopOfStack = 0;  /* LR */\n  pxTopOfStack -= 5;  /* R12, R3, R2 and R1. */\n  *pxTopOfStack = ( portSTACK_TYPE ) pvParameters;  /* R0 */\n  pxTopOfStack -= 8;  /* R11, R10, R9, R8, R7, R6, R5 and R4. */\n  \n  return pxTopOfStack;\n}\nThe ARM Cortex-M3 processor pushes registers on the stack when a task is interrupted.\npxPortInitialiseStack()  modifies the stack to look like the registers were pushed even though the\ntask hasn't actually started running yet. Known values are stored to the stack for the ARM registers\nxPSR, PC, LR,  and R0 . The remaining registers R1  -- R12  get stack space allocated for them by\ndecrementing the top of stack pointer, but no specific data is stored in the stack for those registers. The\nARM architecture says that those registers are undefined at reset, so a (non-buggy) program will not rely\non a known value.\nAfter the stack is prepared, the task is almost ready to run. First though, FreeRTOS disables interrupts:\nWe're about to start mucking with the ready lists and other scheduler structures and we don't want\nanyone else changing them underneath us.\n", "page": 36, "type": "text", "section": "Page 36"}
{"text": "If this is the first task to ever be created, FreeRTOS initializes the scheduler's task lists. FreeRTOS's\nscheduler has an array of ready lists, pxReadyTasksLists[] , which has one ready list for each\npossible priority level. FreeRTOS also has a few other lists for tracking tasks that have been suspended,\nkilled, and delayed. These are all initialized now as well.\nAfter any first-time initialization is done, the new task is added to the ready list at its specified priority\nlevel. Interrupts are re-enabled and new task creation is complete.\n3.5. Lists\nAfter tasks, the most used FreeRTOS data structure is the list. FreeRTOS uses its list structure to keep\ntrack of tasks for scheduling, and also to implement queues.\nFigure 3.3: Full view of FreeRTOS Ready List\nThe FreeRTOS list is a standard circular doubly linked list with a couple of interesting additions. Here's a\nlist element:\nstruct xLIST_ITEM\n{\n  portTickType xItemValue;                   /* The value being listed.  In most cases\n                                                this is used to sort the list in \n                                                descending order. */\n  volatile struct xLIST_ITEM * pxNext;       /* Pointer to the next xListItem in the \n                                                list.  */\n  volatile struct xLIST_ITEM * pxPrevious;   /* Pointer to the previous xListItem in \n                                                the list. */\n  void * pvOwner;                            /* Pointer to the object (normally a TCB)\n                                                that contains the list item.  There is\n                                                therefore a two-way link between the \n                                                object containing the list item and \n                                                the list item itself. */\n  void * pvContainer;                        /* Pointer to the list in which this list\n                                                item is placed (if any). */\n};\n", "page": 37, "type": "text", "section": "Page 37"}
{"text": "Each list element holds a number, xItemValue , that is the usually the priority of the task being tracked\nor a timer value for event scheduling. Lists are kept in high-to-low priority order, meaning that the\nhighest-priority xItemValue  (the largest number) is at the front of the list and the lowest priority\nxItemValue  (the smallest number) is at the end of the list.\nThe pxNext  and pxPrevious  pointers are standard linked list pointers. pvOwner  is a pointer to the\nowner of the list element. This is usually a pointer to a task's TCB object. pvOwner  is used to make task\nswitching fast in vTaskSwitchContext() : once the highest-priority task's list element is found in\npxReadyTasksLists[] , that list element's pvOwner  pointer leads us directly to the TCB needed to\nschedule the task.\npvContainer  points to the list that this item is in. It is used to quickly determine if a list item is in a\nparticular list. Each list element can be put in a list, which is defined as:\ntypedef struct xLIST\n{\n  volatile unsigned portBASE_TYPE uxNumberOfItems;\n  volatile xListItem * pxIndex;           /* Used to walk through the list.  Points to\n                                             the last item returned by a call to \n                                             pvListGetOwnerOfNextEntry (). */\n  volatile xMiniListItem xListEnd;        /* List item that contains the maximum \n                                             possible item value, meaning it is always\n                                             at the end of the list and is therefore \n                                             used as a marker. */\n} xList;\nThe size of a list at any time is stored in uxNumberOfItems , for fast list-size operations. All new lists\nare initialized to contain a single element: the xListEnd  element. xListEnd.xItemValue  is a sentinel\nvalue which is equal to the largest value for the xItemValue  variable: 0xffff  when portTickType\nis a 16-bit value and 0xffffffff  when portTickType  is a 32-bit value. Other list elements may also\nhave the same value; the insertion algorithm ensures that xListEnd  is always the last item in the list.\nSince lists are sorted high-to-low, the xListEnd  element is used as a marker for the start of the list.\nAnd since the list is circular, this xListEnd  element is also a marker for the end of the list.\nMost \"traditional\" list accesses you've used probably do all of their work within a single for() loop or\nfunction call like this:\nfor (listPtr = listStart; listPtr != NULL; listPtr = listPtr->next) {\n  // Do something with listPtr here...\n}\nFreeRTOS frequently needs to access a list across multiple for() and while() loops as well as function\ncalls, and so it uses list functions that manipulate the pxIndex  pointer to walk the list. The list function\nlistGET_OWNER_OF_NEXT_ENTRY()  does pxIndex = pxIndex->pxNext;  and returns pxIndex . (Of\ncourse it does the proper end-of-list-wraparound detection too.) This way the list itself is responsible for\nkeeping track of \"where you are\" while walking it using pxIndex , allowing the rest of FreeRTOS to not\nworry about it.\n", "page": 38, "type": "text", "section": "Page 38"}
{"text": "Figure 3.4: Full view of FreeRTOS Ready List after a system timer tick\nThe pxReadyTasksLists[]  list manipulation done in vTaskSwitchContext()  is a good example of\nhow pxIndex  is used. Let's assume we have only one priority level, priority 0, and there are three tasks\nat that priority level. This is similar to the basic ready list picture we looked at earlier, but this time we'll\ninclude all of the data structures and fields.\nAs you can see in Figure 3.3, pxCurrentTCB  indicates that we're currently running Task B. The next\ntime vTaskSwitchContext()  runs, it calls listGET_OWNER_OF_NEXT_ENTRY()  to get the next task to\nrun. This function uses pxIndex->pxNext  to figure out the next task is Task C, and now pxIndex\npoints to Task C's list element and pxCurrentTCB  points to Task C's TCB, as shown in Figure 3.4.\nNote that each struct xListItem  object is actually the xGenericListItem  object from the\nassociated TCB.\n3.6. Queues\nFreeRTOS allows tasks to communicate and synchronize with each other using queues. Interrupt service\nroutines (ISRs) also use queues for communication and synchronization.\nThe basic queue data structure is:\ntypedef struct QueueDefinition\n{\n  signed char *pcHead;                      /* Points to the beginning of the queue \n                                               storage area. */\n  signed char *pcTail;                      /* Points to the byte at the end of the \n                                               queue storage area. One more byte is \n                                               allocated than necessary to store the \n                                             queue items; this is used as a marker. */\n  signed char *pcWriteTo;                   /* Points to the free next place in the \n                                               storage area. */\n  signed char *pcReadFrom;                  /* Points to the last place that a queued \n                                               item was read from. */\n                                           \n  xList xTasksWaitingToSend;                /* List of tasks that are blocked waiting \n                                               to post onto this queue.  Stored in \n", "page": 39, "type": "text", "section": "Page 39"}
{"text": "                                               priority order. */\n  xList xTasksWaitingToReceive;             /* List of tasks that are blocked waiting \n                                               to read from this queue. Stored in \n                                               priority order. */\n  volatile unsigned portBASE_TYPE uxMessagesWaiting;  /* The number of items currently\n                                                         in the queue. */\n  unsigned portBASE_TYPE uxLength;                    /* The length of the queue \n                                                         defined as the number of \n                                                         items it will hold, not the \n                                                         number of bytes. */\n  unsigned portBASE_TYPE uxItemSize;                  /* The size of each items that \n                                                         the queue will hold. */\n                                         \n} xQUEUE;\nThis is a fairly standard queue with head and tail pointers, as well as pointers to keep track of where\nwe've just read from and written to.\nWhen creating a queue, the user specifies the length of the queue and the size of each item to be\ntracked by the queue. pcHead  and pcTail  are used to keep track of the queue's internal storage.\nAdding an item into a queue does a deep copy of the item into the queue's internal storage.\nFreeRTOS makes a deep copy instead of storing a pointer to the item because the lifetime of the item\ninserted may be much shorter than the lifetime of the queue. For instance, consider a queue of simple\nintegers inserted and removed using local variables across several function calls. If the queue stored\npointers to the integers' local variables, the pointers would be invalid as soon as the integers' local\nvariables went out of scope and the local variables' memory was used for some new value.\nThe user chooses what to queue. The user can queue copies of items if the items are small, like in the\nsimple integer example in the previous paragraph, or the user can queue pointers to the items if the\nitems are large. Note that in both cases FreeRTOS does a deep copy: if the user chooses to queue\ncopies of items then the queue stores a deep copy of each item; if the user chooses to queue pointers\nthen the queue stores a deep copy of the pointer. Of course, if the user stores pointers in the queue\nthen the user is responsible for managing the memory associated with the pointers. The queue doesn't\ncare what data you're storing in it, it just needs to know the data's size.\nFreeRTOS supports blocking and non-blocking queue insertions and removals. Non-blocking operations\nreturn immediately with a \"Did the queue insertion work?\" or \"Did the queue removal work?\" status.\nBlocking operations are specified with a timeout. A task can block indefinitely or for a limited amount of\ntime.\nA blocked task\u2014call it Task A\u2014will remain blocked as long as its insert/remove operation cannot\ncomplete and its timeout (if any) has not expired. If an interrupt or another task modifies the queue so\nthat Task A's operation could complete, Task A will be unblocked. If Task A's queue operation is still\npossible by the time it actually runs then Task A will complete its queue operation and return \"success\".\nHowever, by the time Task A actually runs, it is possible that a higher-priority task or interrupt has\nperformed yet another operation on the queue that prevents Task A from performing its operation. In this\ncase Task A will check its timeout and either resume blocking if the timeout hasn't expired, or return with\na queue operation \"failed\" status.\nIt's important to note that the rest of the system keeps going while a task is blocking on a queue; other\ntasks and interrupts continue to run. This way the blocked task doesn't waste CPU cycles that could be\nused productively by other tasks and interrupts.\nFreeRTOS uses the xTasksWaitingToSend  list to keep track of tasks that are blocking on inserting into\na queue. Each time an element is removed from a queue the xTasksWaitingToSend  list is checked. If\na task is waiting in that list the task is unblocked.\nSimilarly, xTasksWaitingToReceive  keeps track of tasks that are blocking on removing from a queue.\nEach time a new element is inserted into a queue the xTasksWaitingToReceive  list is checked. If a\ntask is waiting in that list the task is unblocked.\nSemaphores and Mutexes\nFreeRTOS uses its queues for communication between and within tasks. FreeRTOS also uses its queues\nto implement semaphores and mutexes.\nWhat's The Difference?\nSemaphores and mutexes may sound like the same thing, but they're not. FreeRTOS implements them\nsimilarly, but they're intended to be used in different ways. How should they be used differently?\nEmbedded systems guru Michael Barr says it best in his article, \"Mutexes and Semaphores Demystified\":\nThe correct use of a semaphore is for signaling from one task to another. A mutex is meant to be\ntaken and released, always in that order, by each task that uses the shared resource it protects. By\ncontrast, tasks that use semaphores either signal [\"send\" in FreeRTOS terms] or wait [\"receive\" in\n", "page": 40, "type": "text", "section": "Page 40"}
{"text": "This work is made available under the Creative Commons Attribution 3.0 Unported license. Please see\nthe full description of the license for details.\nFreeRTOS terms] - not both.\nA mutex is used to protect a shared resource. A task acquires a mutex, uses the shared resource, then\nreleases the mutex. No task can acquire a mutex while the mutex is being held by another task. This\nguarantees that only one task is allowed to use a shared resource at a time.\nSemaphores are used by one task to signal another task. To quote Barr's article:\nFor example, Task 1 may contain code to post (i.e., signal or increment) a particular semaphore when\nthe \"power\" button is pressed and Task 2, which wakes the display, pends on that same semaphore.\nIn this scenario, one task is the producer of the event signal; the other the consumer.\nIf you're at all in doubt about semaphores and mutexes, please check out Michael's article.\nImplementation\nFreeRTOS implements an N-element semaphore as a queue that can hold N items. It doesn't store any\nactual data for the queue items; the semaphore just cares how many queue entries are currently\noccupied, which is tracked in the queue's uxMessagesWaiting  field. It's doing \"pure synchronization\",\nas the FreeRTOS header file semphr.h  calls it. Therefore the queue has a item size of zero bytes\n( uxItemSize == 0 ). Each semaphore access increments or decrements the uxMessagesWaiting\nfield; no item or data copying is needed.\nLike a semaphore, a mutex is also implemented as a queue, but several of the xQUEUE  struct fields are\noverloaded using #defines :\n/* Effectively make a union out of the xQUEUE structure. */\n#define uxQueueType           pcHead\n#define pxMutexHolder         pcTail\nSince a mutex doesn't store any data in the queue, it doesn't need any internal storage, and so the\npcHead  and pcTail  fields aren't needed. FreeRTOS sets the uxQueueType  field (really the\npcHead  field) to 0  to note that this queue is being used for a mutex. FreeRTOS uses the overloaded\npcTail  fields to implement priority inheritance for mutexes.\nIn case you're not familiar with priority inheritance, I'll quote Michael Barr again to define it, this time from\nhis article, \"Introduction to Priority Inversion\":\n[Priority inheritance] mandates that a lower-priority task inherit the priority of any higher-priority task\npending on a resource they share. This priority change should take place as soon as the high-priority\ntask begins to pend; it should end when the resource is released.\nFreeRTOS implements priority inheritance using the pxMutexHolder  field (which is really just the\noverloaded-by- #define  pcTail  field). FreeRTOS records the task that holds a mutex in the\npxMutexHolder  field. When a higher-priority task is found to be waiting on a mutex currently taken by\na lower-priority task, FreeRTOS \"upgrades\" the lower-priority task to the priority of the higher-priority\ntask until the mutex is available again.\n3.7. Conclusion\nWe've completed our look at the FreeRTOS architecture. Hopefully you now have a good feel for how\nFreeRTOS's tasks run and communicate. And if you've never looked at any OS's internals before, I hope\nyou now have a basic idea of how they work.\nObviously this chapter did not cover all of FreeRTOS's architecture. Notably, I didn't mention memory\nallocation, ISRs, debugging, or MPU support. This chapter also did not discuss how to set up or use\nFreeRTOS. Richard Barry has written an excellent book, Using the FreeRTOS Real Time Kernel: A\nPractical Guide, which discusses exactly that; I highly recommend it if you're going to use FreeRTOS.\n3.8. Acknowledgements\nI would like to thank Richard Barry for creating and maintaining FreeRTOS, and for choosing to make it\nopen source. Richard was very helpful in writing this chapter, providing some FreeRTOS history as well\nas a very valuable technical review.\nThanks also to Amy Brown and Greg Wilson for pulling this whole AOSA thing together.\nLast and most (the opposite of \"not least\"), thanks to my wife Sarah for sharing me with the research\nand writing for this chapter. Luckily she knew I was a geek when she married me!\nBack to top\nBack to The Architecture of Open Source Applications.\n", "page": 41, "type": "text", "section": "Page 41"}
{"text": "<>\nGDB, the GNU Debugger, was among the first programs to be written for the Free Software Foundation,\nand it has been a staple of free and open source software systems ever since. Originally designed as a\nplain Unix source-level debugger, it has since been expanded to a wide range of uses, including use with\nmany embedded systems, and has grown from a few thousand lines of C to over half a million.\nThis chapter will delve into the overall internal structure of GDB, showing how it has gradually developed\nas new user needs and new features have come in over time.\n4.1. The Goal\nGDB is designed to be a symbolic debugger for programs written in compiled imperative languages such\nas C, C++, Ada, and Fortran. Using its original command-line interface, a typical usage looks something\nlike this:\n% gdb myprog\n[...]\n(gdb) break buggy_function\nBreakpoint 1 at 0x12345678: file myprog.c, line 232.\n(gdb) run 45 92\nStarting program: myprog\nBreakpoint 1, buggy_function (arg1=45, arg2=92) at myprog.c:232\n232     result = positive_variable * arg1 + arg2;\n(gdb) print positive_variable\n$$1 = -34\n(gdb)\nGDB shows something that is not right, the developer says \"aha\" or \"hmmm\", and then has to decide\nboth what the mistake is and how to fix it.\nThe important point for design is that a tool like GDB is basically an interactive toolbox for poking around\nin a program, and as such it needs to be responsive to an unpredictable series of requests. In addition, it\nwill be used with programs that have been optimized by the compiler, and programs that exploit every\nhardware option for performance, so it needs to have detailed knowledge down to the lowest levels of a\nsystem.\nGDB also needs to be able to debug programs compiled by different compilers (not just the GNU C\ncompiler), to debug programs compiled years earlier by long-obsolete versions of compilers, and to\ndebug programs whose symbolic info is missing, out of date, or simply incorrect; so, another design\nconsideration is that GDB should continue to work and be useful even if data about the program is\nmissing, or corrupted, or simply incomprehensible.\nThe following sections assume a passing familiarity with using GDB from the command line. If you're new\nto GDB, give it a try and peruse the manual.[SPS+00]\n4.2. Origins of GDB\nGDB is an old program. It came into existence around 1985, written by Richard Stallman along with\nGCC, GNU Emacs, and other early components of GNU. (In those days, there were no public source\ncontrol repositories, and much of the detailed development history is now lost.)\nThe earliest readily available releases are from 1988, and comparison with present-day sources shows\nthat only a handful of lines bear much resemblance; nearly all of GDB has been rewritten at least once.\nAnother striking thing about early versions of GDB is that the original goals were rather modest, and\nmuch of the work since then has been extension of GDB into environments and usages that were not\npart of the original plan.\nGDB\nStan Shebs\n", "page": 43, "type": "text", "section": "Page 43"}
{"text": "4.3. Block Diagram\nFigure 4.1: Overall structure of GDB\nAt the largest scale, GDB can be said to have two sides to it:\n1. The \"symbol side\" is concerned with symbolic information about the program. Symbolic information\nincludes function and variable names and types, line numbers, machine register usage, and so on.\nThe symbol side extracts symbolic information from the program's executable file, parses\nexpressions, finds the memory address of a given line number, lists source code, and in general\nworks with the program as the programmer wrote it.\n2. The \"target side\" is concerned with the manipulation of the target system. It has facilities to start and\nstop the program, to read memory and registers, to modify them, to catch signals, and so on. The\nspecifics of how this is done can vary drastically between systems; most Unix-type systems provide\na special system call named ptrace  that gives one process the ability to read and write the state\nof a different process. Thus, GDB's target side is mostly about making ptrace  calls and\ninterpreting the results. For cross-debugging an embedded system, however, the target side\nconstructs message packets to send over a wire, and waits for response packets in return.\nThe two sides are somewhat independent of each other; you can look around your program's code,\ndisplay variable types, etc., without actually running the program. Conversely, it is possible to do pure\nmachine-language debugging even if no symbols are available.\nIn the middle, tying the two sides together, is the command interpreter and the main execution control\nloop.\n4.4. Examples of Operation\nTo take a simple case of how it all ties together, consider the print  command from above. The\ncommand interpreter finds the print  command function, which parses the expression into a simple\ntree structure and then evaluates it by walking the tree. At some point the evaluator will consult the\nsymbol table to find out that positive_variable  is an integer global variable that is stored at, say,\nmemory address 0x601028 . It then calls a target-side function to read the four bytes of memory at that\naddress, and hands the bytes to a formatting function that displays them as a decimal number.\n", "page": 44, "type": "text", "section": "Page 44"}
{"text": "To display source code and its compiled version, GDB does a combination of reads from the source file\nand the target system, then uses compiler-generated line number information to connect the two. In the\nexample here, line 232 has the address 0x4004be , line 233 is at 0x4004ce , and so on.\n[...]\n232  result = positive_variable * arg1 + arg2;\n0x4004be <+10>:  mov  0x200b64(%rip),%eax  # 0x601028 <positive_variable>\n0x4004c4 <+16>:  imul -0x14(%rbp),%eax\n0x4004c8 <+20>:  add  -0x18(%rbp),%eax\n0x4004cb <+23>:  mov  %eax,-0x4(%rbp)\n233  return result;\n0x4004ce <+26>:  mov  -0x4(%rbp),%eax\n[...]\nThe single-stepping command step  conceals a complicated dance going on behind the scenes. When\nthe user asks to step to the next line in the program, the target side is asked to execute only a single\ninstruction of the program and then stop it again (this is one of the things that ptrace  can do). Upon\nbeing informed that the program has stopped, GDB asks for the program counter (PC) register (another\ntarget side operation) and then compares it with the range of addresses that the symbol side says is\nassociated with the current line. If the PC is outside that range, then GDB leaves the program stopped,\nfigures out the new source line, and reports that to the user. If the PC is still in the range of the current\nline, then GDB steps by another instruction and checks again, repeating until the PC gets to a different\nline. This basic algorithm has the advantage that it always does the right thing, whether the line has\njumps, subroutine calls, etc., and does not require GDB to interpret all the details of the machine's\ninstruction set. A disadvantage is that there are many interactions with the target for each single-step\nwhich, for some embedded targets, results in noticeably slow stepping.\n4.5. Portability\nAs a program needing extensive access all the way down to the physical registers on a chip, GDB was\ndesigned from the beginning to be portable across a variety of systems. However, its portability strategy\nhas changed considerably over the years.\nOriginally, GDB started out similar to the other GNU programs of the time; coded in a minimal common\nsubset of C, and using a combination of preprocessor macros and Makefile fragments to adapt to a\nspecific architecture and operating system. Although the stated goal of the GNU project was a self-\ncontained \"GNU operating system\", bootstrapping would have to be done on a variety of existing\nsystems; the Linux kernel was still years in the future. The configure  shell script is the first key step\nof the process. It can do a variety of things, such as making a symbolic link from a system-specific file to\na generic header name, or constructing files from pieces, more importantly the Makefile used to build the\nprogram.\nPrograms like GCC and GDB have additional portability needs over something like cat  or diff , and\nover time, GDB's portability bits came to be separated into three classes, each with its own Makefile\nfragment and header file.\n\"Host\" definitions are for the machine that GDB itself runs on, and might include things like the sizes\nof the host's integer types. Originally done as human-written header files, it eventually occurred to\npeople that they could be calculated by having configure  run little test programs, using the same\ncompiler that was going to be used to build the tool. This is what autoconf [aut12] is all about, and\ntoday nearly all GNU tools and many (if not most) Unix programs use autoconf -generated\nconfigure scripts.\n\"Target\" definitions are specific to the machine running the program being debugged. If the target is\nthe same as the host, then we are doing \"native\" debugging, otherwise it is \"cross\" debugging, using\nsome kind of wire connecting the two systems. Target definitions fall in turn into two main classes:\n\"Architecture\" definitions: These define how to disassemble machine code, how to walk through\nthe call stack, and which trap instruction to insert at breakpoints. Originally done with macros,\nthey were migrated to regular C accessed by via \"gdbarch\" objects, described in more depth\nbelow.\n\"Native\" definitions: These define the specifics of arguments to ptrace  (which vary\nconsiderably between flavors of Unix), how to find shared libraries that have been loaded, and so\nforth, which only apply to the native debugging case. Native definitions are a last holdout of\n1980s-style macros, although most are now figured out using autoconf .\n4.6. Data Structures\nBefore drilling down into the parts of GDB, let's take a look at the major data structures that GDB works\nwith. As GDB is a C program, these are implemented as struct s rather than as C++-style objects, but\nin most cases they are treated as objects, and here we follow GDBers' frequent practice in calling them\nobjects.\nBreakpoints\n", "page": 45, "type": "text", "section": "Page 45"}
{"text": "A breakpoint is the main kind of object that is directly accessible to the user. The user creates a\nbreakpoint with the break  command, whose argument specifies a location, which can be a function\nname, a source line number, or a machine address. GDB assigns a small positive integer to the\nbreakpoint object, which the user subsequently uses to operate on the breakpoint. Within GDB, the\nbreakpoint is a C struct  with a number of fields. The location gets translated to a machine address,\nbut is also saved in its original form, since the address may change and need recomputation, for instance\nif the program is recompiled and reloaded into a session.\nSeveral kinds of breakpoint-like objects actually share the breakpoint struct , including watchpoints,\ncatchpoints, and tracepoints. This helps ensure that creation, manipulation, and deletion facilities are\nconsistently available.\nThe term \"location\" also refers to the memory addresses at which the breakpoint is to be installed. In the\ncases of inline functions and C++ templates, it may be that a single user-specified breakpoint may\ncorrespond to several addresses; for instance, each inlined copy of a function entails a separate location\nfor a breakpoint that is set on a source line in the function's body.\nSymbols and Symbol Tables\nSymbol tables are a key data structure to GDB, and can be quite large, sometimes growing to occupy\nmultiple gigabytes of RAM. To some extent, this is unavoidable; a large application in C++ can have\nmillions of symbols in its own right, and it pulls in system header files which can have millions more\nsymbols. Each local variable, each named type, each value of an enum\u2014all of these are separate\nsymbols.\nGDB uses a number of tricks to reduce symbol table space, such as partial symbol tables (more about\nthose later), bit fields in struct s, etc.\nIn addition to symbol tables that basically map character strings to address and type information, GDB\nbuilds line tables that support lookup in two directions; from source lines to addresses, and then from\naddresses back to source lines. (For instance, the single-stepping algorithm described earlier crucially\ndepends on the address-to-source mapping.)\nStack frames\nThe procedural languages for which GDB was designed share a common runtime architecture, in that\nfunction calls cause the program counter to be pushed on a stack, along with some combination of\nfunction arguments and local arguments. The assemblage is called a stack frame, or \"frame\" for short,\nand at any moment in a program's execution, the stack consists of a sequence of frames chained\ntogether. The details of a stack frame vary radically from one chip architecture to the next, and is also\ndependent on the operating system, compiler, and optimization options.\nA port of GDB to a new chip may need a considerable volume of code to analyze the stack, as programs\n(especially buggy ones, which are the ones debugger users are mostly interested in) can stop anywhere,\nwith frames possibly incomplete, or partly overwritten by the program. Worse, constructing a stack frame\nfor each function call slows down the application, and a good optimizing compiler will take every\nopportunity to simplify stack frames, or even eliminate them altogether, such as for tail calls.\nThe result of GDB's chip-specific stack analysis is recorded in a series of frame objects. Originally GDB\nkept track of frames by using the literal value of a fixed-frame pointer register. This approach breaks\ndown for inlined function calls and other kinds of compiler optimizations, and starting in 2002, GDBers\nintroduced explicit frame objects that recorded what had been figured out about each frame, and were\nlinked together, mirroring the program's stack frames.\nExpressions\nAs with stack frames, GDB assumes a degree of commonality among the expressions of the various\nlanguages it supports, and represents them all as a tree structure built out of node objects. The set of\nnode types is effectively a union of all the types of expressions possible in all the different languages;\nunlike in the compiler, there is no reason to prevent the user from trying to subtract a Fortran variable\nfrom a C variable\u2014perhaps the difference of the two is an obvious power of two, and that gives us the\n\"aha\" moment.\nValues\nThe result of evaluation may itself be more complex than an integer or memory address, and GDB also\nretains evaluation results in a numbered history list, which can then be referred to in later expressions.\nTo make all this work, GDB has a data structure for values. Value struct s have a number of fields\nrecording various properties; important ones include an indication of whether the value is an r-value or l-\nvalue (l-values can be assigned to, as in C), and whether the value is to be constructed lazily.\n4.7. The Symbol Side\nThe symbol side of GDB is mainly responsible for reading the executable file, extracting any symbolic\ninformation it finds, and building it into a symbol table.\nThe reading process starts with the BFD library. BFD is a sort of universal library for handling binary and\nobject files; running on any host, it can read and write the original Unix a.out  format, COFF (used on\nSystem V Unix and MS Windows), ELF (modern Unix, GNU/Linux, and most embedded systems), and\n", "page": 46, "type": "text", "section": "Page 46"}
{"text": "some other file formats. Internally, the library has a complicated structure of C macros that expand into\ncode incorporating the arcane details of object file formats for dozens of different systems. Introduced in\n1990, BFD is also used by the GNU assembler and linker, and its ability to produce object files for any\ntarget is key to cross-development using GNU tools. (Porting BFD is also a key first step in porting the\ntools to a new target.)\nGDB only uses BFD to read files, using it to pull blocks of data from the executable file into GDB's\nmemory. GDB then has two levels of reader functions of its own. The first level is for basic symbols, or\n\"minimal symbols\", which are just the names that the linker needs to do its work. These are strings with\naddresses and not much else; we assume that addresses in text sections are functions, addresses in\ndata sections are data, and so forth.\nThe second level is detailed symbolic information, which typically has its own format different from the\nbasic executable file format; for instance, information in the DWARF debug format is contained in\nspecially named sections of an ELF file. By contrast, the old stabs  debug format of Berkeley Unix used\nspecially flagged symbols stored in the general symbol table.\nThe code for reading symbolic information is somewhat tedious, as the different symbolic formats encode\nevery kind of type information that could be in a source program, but each goes about it in its own\nidiosyncratic way. A GDB reader just walks through the format, constructing GDB symbols that we think\ncorrespond to what the symbolic format intends.\nPartial Symbol Tables\nFor a program of significant size (such as Emacs or Firefox), construction of the symbol table can take\nquite a while, maybe even several minutes. Measurements consistently show that the time is not in file\nreading as one might expect, but in the in-memory construction of GDB symbols. There are literally\nmillions of small interconnected objects involved, and the time adds up.\nMost of the symbolic information will never be looked at in a session, since it is local to functions that the\nuser may never examine. So, when GDB first pulls in a program's symbols, it does a cursory scan\nthrough the symbolic information, looking for just the globally visible symbols and recording only them in\nthe symbol table. Complete symbolic info for a function or method is filled in only if the user stops inside\nit.\nPartial symbol tables allow GDB to start up in only a few seconds, even for large programs. (Shared\nlibrary symbols are also dynamically loaded, but the process is rather different. Typically GDB uses a\nsystem-specific technique to be notified when the library is loaded, then builds a symbol table with\nfunctions at the addresses that were decided on by the dynamic linker.)\nLanguage Support\nSource language support mainly consists of expression parsing and value printing. The details of\nexpression parsing are left up to each language, but in the general the parser is based on a Yacc\ngrammar fed by a hand-crafted lexical analyzer. In keeping with GDB's goal of providing more flexibility\nto the interactive user, the parser is not expected to be especially stringent; for instance, if it can guess\nat a reasonable type for an expression, it will simply assume that type, rather than require the user to\nadd a cast or type conversion.\nSince the parser need not handle statements or type declarations, it is much simpler than the full\nlanguage parser. Similarly, for printing, there are just a handful of types of values that need to be\ndisplayed, and oftentimes the language-specific print function can call out to generic code to finish the\njob.\n4.8. Target Side\nThe target side is all about manipulation of program execution and raw data. In a sense, the target side\nis a complete low-level debugger; if you are content to step by instructions and dump raw memory, you\ncan use GDB without needing any symbols at all. (You may end up operating in this mode anyway, if the\nprogram happens to stop in a library whose symbols have been stripped out.)\nTarget Vectors and the Target Stack\nOriginally the target side of GDB was composed of a handful of platform-specific files that handled the\ndetails of calling ptrace , launching executables, and so on. This is not sufficiently flexible for long-\nrunning debugging sessions, in which the user might switch from local to remote debugging, switch from\nfiles to core dumps to live programs, attach and detach, etc., so in 1990 John Gilmore redesigned the\ntarget side of GDB to send all target-specific operations through the target vector, which is basically a\nclass of objects, each of which defines the the specifics of a type of target system. Each target vector is\nimplemented as a structure of several dozen function pointers (often called \"methods\"), whose purposes\nrange from the reading and writing of memory and registers, to resuming program execution, to setting\nparameters for the handling of shared libraries. There are about 40 target vectors in GDB, ranging from\nthe well-used target vector for Linux to obscure vectors such as the one that operates a Xilinx\nMicroBlaze. Core dump support uses a target vector that gets data by reading a corefile, and there is\nanother target vector that reads data from the executable.\nIt is often useful to blend methods from several target vectors. Consider the printing of an initialized\nglobal variable on Unix; before the program starts running, printing the variable should work, but at that\n", "page": 47, "type": "text", "section": "Page 47"}
{"text": "point there is no process to read, and bytes need to come from the executable's .data  section. So,\nGDB uses the target vector for executables and reads from the binary file. But while the program is\nrunning, the bytes should instead come from the process's address space. So, GDB has a \"target stack\"\nwhere the target vector for live processes is pushed on top of the executable's target vector when the\nprocess starts running, and is popped when it exits.\nIn reality, the target stack turns out not to be quite as stack-like as one might think. Target vectors are\nnot really orthogonal to each other; if you have both an executable and a live process in the session,\nwhile it makes sense to have the live process's methods override the executable's methods, it almost\nnever makes sense to do the reverse. So GDB has ended up with a notion of a stratum in which\n\"process-like\" target vectors are all at one stratum, while \"file-like\" target vectors get assigned to a lower\nstratum, and target vectors can get inserted as well as pushed and popped.\n(Although GDB maintainers don't like the target stack much, no one has proposed\u2014or prototyped\u2014any\nbetter alternative.)\nGdbarch\nAs a program that works directly with the instructions of a CPU, GDB needs in-depth knowledge about\nthe details of the chip. It needs to know about all the registers, the sizes of the different kinds of data,\nthe size and shape of the address space, how the calling convention works, what instruction will cause a\ntrap exception, and so on. GDB's code for all this typically ranges from 1,000 to over 10,000 lines of C,\ndepending on the architecture's complexity.\nOriginally this was handled using target-specific preprocessor macros, but as the debugger became\nmore sophisticated, these got larger and larger, and over time long macro definitions were made into\nregular C functions called from the macros. While this helped, it did not help much with architectural\nvariants (ARM vs. Thumb, 32-bit vs. 64-bit versions of MIPS or x86, etc.), and worse, multiple-\narchitecture designs were on the horizon, for which macros would not work at all. In 1995, I proposed\nsolving this with an object-based design, and starting in 1998 Cygnus Solutions funded Andrew Cagney\nto start the changeover. (Cygnus Solutions was a company founded in 1989 to provide commercial\nsupport for free software, acquired in 2000 by Red Hat.) It took several years and contributions from\ndozens of hackers to finish the job, affecting perhaps 80,000 lines of code in all.\nThe introduced constructs are called gdbarch  objects, and at this point may contain as many as 130\nmethods and variables defining a target architecture, although a simple target might only need a dozen or\nso of these.\nTo get a sense of how the old and new ways compare, see the declaration that x86 long doubles are 96\nbits in size from gdb/config/i386/tm-i386.h , circa 2002:\n#define TARGET_LONG_DOUBLE_BIT 96\nand from gdb/i386-tdep.c , in 2012:\ni386_gdbarch_init( [...] )\n{\n  [...]\n  set_gdbarch_long_double_bit (gdbarch, 96);\n  [...]\n}\nExecution Control\nThe heart of GDB is its execution control loop. We touched on it earlier when describing single-stepping\nover a line; the algorithm entailed looping over multiple instructions until finding one associated with a\ndifferent source line. The loop is called wait_for_inferior , or \"wfi\" for short.\nConceptually it is inside the main command loop, and is only entered for commands that cause the\nprogram to resume execution. When the user types continue  or step  and then waits while nothing\nseems to be happening, GDB may in fact be quite busy. In addition to the single-stepping loop\nmentioned above, the program may be hitting trap instructions and reporting the exception to GDB. If the\nexception is due to the trap being a breakpoint inserted by GDB, it then tests the breakpoint's condition,\nand if false, it removes the trap, single-steps the original instruction, re-inserts the trap, and then lets the\nprogram resume. Similarly, if a signal is raised, GDB may choose to ignore it, or handle it one of several\nways specified in advance.\nAll of this activity is managed by wait_for_inferior . Originally this was a simple loop, waiting for the\ntarget to stop and then deciding what to do about it, but as ports to various systems needed special\nhandling, it grew to a thousand lines, with goto statements criss-crossing it for poorly understood\nreasons. For instance, with the proliferation of Unix variants, there was no one person who understood\nall their fine points, nor did we have access to all of them for regression testing, so there was a strong\nincentive to modify the code in a way that exactly preserved behavior for existing ports\u2014and a goto\nskipping over part of the loop was an all-too-easy tactic.\n", "page": 48, "type": "text", "section": "Page 48"}
{"text": "The single big loop was also a problem for any kind of asynchronous handling or debugging of threaded\nprograms, in which the user wants to start and stop a single thread while allowing the rest of the program\nto continue running.\nThe conversion to an event-oriented model took several years. I broke up wait_for_inferior  in 1999,\nintroducing an execution control state structure to replace the pile of local and global variables, and\nconverting the tangle of jumps into smaller independent functions. At the same time Elena Zannoni and\nothers introduced event queues that included both input from the user and notifications from the inferior.\nThe Remote Protocol\nAlthough GDB's target vector architecture allows for a broad variety of ways to control a program running\non a different computer, we have a single preferred protocol. It does not have a distinguishing name, and\nis typically called just the \"remote protocol\", \"GDB remote protocol\", \"remote serial protocol\" (abbreviating\nto \"RSP\"), \"remote.c protocol\" (after the source file that implements it), or sometimes the \"stub protocol\",\nreferring to the target's implementation of the protocol.\nThe basic protocol is simple, reflecting the desire to have it work on small embedded systems of the\n1980s, whose memories were measured in kilobytes. For instance, the protocol packet $g  requests all\nregisters, and expects a reply consisting of all the bytes of all the registers, all run together\u2014the\nassumption being that their number, size, and order will match what GDB knows about.\nThe protocol expects a single reply to each packet sent, and assumes the connection is reliable, adding\nonly a checksum to packets sent (so $g  is really sent as $g#67  over the wire).\nAlthough there are only a handful of required packet types (corresponding to the half-dozen target vector\nmethods that are most important), scores of additional optional packets have been added over the years,\nto support everything from hardware breakpoints, to tracepoints, to shared libraries.\nOn the target itself, the implementation of the remote protocol can take a wide variety of forms. The\nprotocol is fully documented in the GDB manual, which means that it is possible to write an\nimplementation that is not encumbered with a GNU license, and indeed many equipment manufacturers\nhave incorporated code that speaks the GDB remote protocol, both in the lab and in the field. Cisco's\nIOS, which runs much of their networking equipment, is one well-known example.\nA target's implementation of the protocol is often referred to as a \"debugging stub\", or just \"stub\",\nconnoting that it is not expected to do very much work on its own. The GDB sources include a few\nexample stubs, which are typically about 1,000 lines of low-level C. On a totally bare board with no OS,\nthe stub must install its own handlers for hardware exceptions, most importantly to catch trap instructions.\nIt will also need serial driver code if the hardware link is a serial line. The actual protocol handling is\nsimple, since all the required packets are single characters that can be decoded with a switch statement.\nAnother approach to remote protocol is to build a \"sprite\" that interfaces between GDB and dedicated\ndebugging hardware, including JTAG devices, \"wigglers\", etc. Oftentimes these devices have a library\nthat must run on the computer that is physically connected to a target board, and often the library API is\nnot architecturally compatible with GDB's internals. So, while configurations of GDB have called hardware\ncontrol libraries directly, it has proven simpler to run the sprite as an independent program that\nunderstands remote protocol and translates the packets into device library calls.\nGDBserver\nThe GDB sources do include one complete and working implementation of the target side of the remote\nprotocol: GDBserver. GDBserver is a native program that runs under the target's operating system, and\ncontrols other programs on the target OS using its native debugging support, in response to packets\nreceived via remote protocol. In other words, it acts as a sort of proxy for native debugging.\nGDBserver doesn't do anything that native GDB can't do; if your target system can run GDBserver, then\ntheoretically it can run GDB. However, GDBserver is 10 times smaller and doesn't need to manage\nsymbol tables, so it is very convenient for embedded GNU/Linux usages and the like.\n", "page": 49, "type": "text", "section": "Page 49"}
{"text": "Figure 4.2: GDBserver\nGDB and GDBserver share some code, but while it is an obvious idea to encapsulate OS-specific\nprocess control, there are practical difficulties with separating out tacit dependencies in native GDB, and\nthe transition has gone slowly.\n4.9. Interfaces to GDB\nGDB is fundamentally a command-line debugger. Over time people have tried various schemes to make\nit into a graphical windowed debugger but, despite all the time and effort, none of these are universally\naccepted.\nCommand-Line Interface\nThe command-line interface uses the standard GNU library readline  to handle the character-by-\ncharacter interaction with the user. Readline takes care of things like line editing and command\ncompletion; the user can do things like use cursor keys to go back in a line and fix a character.\nGDB then takes the command returned by readline  and looks it up using a cascaded structure of\ncommand tables, where each successive word of the command selects an additional table. For instance\nset print elements 80  involves three tables; the first is the table of all commands, the second is a\ntable of options that can be set , and the third is a table of value printing options, of which elements\nis the one that limits the number of objects printed from an aggregate like a string or array. Once the\ncascaded tables have called an actual command-handling function, it takes control, and argument\nparsing is completely up to the function. Some commands, such as run , handle their arguments\nsimilarly to traditional C argc / argv  standards, while others, such as print , assume that the\nremainder of the line is a single programming language expression, and give the entire line over to a\nlanguage-specific parser.\nMachine Interface\nOne way to provide a debugging GUI is to use GDB as a sort of \"backend\" to a graphical interface\nprogram, translating mouse clicks into commands and formatting print results into windows. This has\nbeen made to work several times, including KDbg and DDD (Data Display Debugger), but it's not the\nideal approach because sometimes results are formatted for human readability, omitting details and\nrelying on human ability to supply context.\nTo solve this problem, GDB has an alternate \"user\" interface, known as the Machine Interface or MI for\nshort. It is still fundamentally a command-line interface, but both commands and results have additional\nsyntax that makes everything explicit\u2014each argument is bounded by quotes, and complex output has\ndelimiters for subgroups and parameter names for component pieces. In addition, MI commands can be\nprefixed with sequence identifiers that are echoed back in results, ensuring reported results are matched\nup with the right commands.\nTo see how the two forms compare, here is a normal step command and GDB's response:\n(gdb) step\nbuggy_function (arg1=45, arg2=92) at ex.c:232\n", "page": 50, "type": "text", "section": "Page 50"}
{"text": "232  result = positive_variable * arg1 + arg2;\nWith the MI, the input and output are more verbose, but easier for other software to parse accurately:\n4321-exec-step\n4321^done,reason=\"end-stepping-range\",\n      frame={addr=\"0x00000000004004be\",\n             func=\"buggy_function\",\n             args=[{name=\"arg1\",value=\"45\"},\n                   {name=\"arg2\",value=\"92\"}],\n             file=\"ex.c\",\n             fullname=\"/home/sshebs/ex.c\",\n             line=\"232\"}\nThe Eclipse[ecl12] development environment is the most notable client of the MI.\nOther User Interfaces\nAdditional frontends include a tcl/tk-based version called GDBtk or Insight, and a curses-based interface\ncalled the TUI, originally contributed by Hewlett-Packard. GDBtk is a conventional multi-paned graphical\ninterface built using the tk library, while the TUI is a split-screen interface.\n4.10. Development Process\nMaintainers\nAs an original GNU program, GDB development started out following the \"cathedral\" model of\ndevelopment. Originally written by Stallman, GDB then went through a succession of \"maintainers\", each\nof whom was a combination of architect, patch reviewer, and release manager, with access to the source\nrepository limited to a handful of Cygnus employees.\nIn 1999, GDB migrated to a public source repository and expanded to a team of several dozen\nmaintainers, aided by scores of individuals with commit privileges. This has accelerated development\nconsiderably, with the 10-odd commits each week growing to 100 or more.\nTesting Testing\nAs GDB is highly system-specific, has a great many ports to systems ranging from the smallest to the\nlargest in computerdom, and has hundreds of commands, options, and usage styles, it is difficult for even\nan experienced GDB hacker to anticipate all the effects of a change.\nThis is where the test suite comes in. The test suite consists of a number of test programs combined with\nexpect  scripts, using a tcl-based testing framework called DejaGNU. The basic model is that each\nscript drives GDB as it debugs a test program, sending commands and then pattern-matching the output\nagainst regular expressions.\nThe test suite also has the ability to run cross-debugging to both live hardware and simulators, and to\nhave tests that are specific to a single architecture or configuration.\nAt the end of 2011, the test suite includes some 18,000 test cases, which include tests of basic\nfunctionality, language-specific tests, architecture-specific tests, and MI tests. Most of these are generic\nand are run for any configuration. GDB contributors are expected to run the test suite on patched sources\nand observe no regressions, and new tests are expected to accompany each new feature. However, as\nno one has access to all platforms that might be affected by a change, it is rare to get all the way to zero\nfailures; 10--20 failures is usually reasonable for a trunk snapshot configured for native debugging, and\nsome embedded targets will have more failures.\n4.11. Lessons Learned\nOpen Development Wins\nGDB started out as an exemplar of the \"cathedral\" development process, in which the maintainer keeps\nclose control of the sources, with the outside world only seeing progress via periodic snapshots. This\nwas rationalized by the relative infrequence of patch submissions, but the closed process was actually\ndiscouraging patches. Since the open process has been adopted, the number of patches is much larger\nthan ever before, and quality is just as good or better.\nMake a Plan, but Expect It to Change\nThe open source development process is intrinsically somewhat chaotic, as different individuals work on\nthe code for a while, then fall away, leaving others to continue on.\nHowever, it still makes sense to make a development plan and publish it. It helps guide developers as\nthey work on related tasks, it can be shown to potential funders, and it lets volunteers think about what\nthey can do to advance it.\nBut don't try to force dates or time frames; even if everyone is enthusiastic about a direction, it is unlikely\nthat people can guarantee full-time effort for long enough to finish by a chosen date.\n", "page": 51, "type": "text", "section": "Page 51"}
{"text": "This work is made available under the Creative Commons Attribution 3.0 Unported license. Please see\nthe full description of the license for details.\nFor that matter, don't cling to the plan itself if it has become outdated. For a long time, GDB had a plan\nto restructure as a library, libgdb , with a well-defined API, that could be linked into other programs (in\nparticular ones with GUIs); the build process was even changed to build a libgdb.a  as an\nintermediate step. Although the idea has come up periodically since then, the primacy of Eclipse and MI\nmeant that the library's main rationale has been sidestepped, and as of January 2012 we have\nabandoned the library concept and are expunging the now-pointless bits of code.\nThings Would Be Great If We Were Infinitely Intelligent\nAfter seeing some of the changes we made, you might be thinking: Why didn't we do things right in the\nfirst place? Well, we just weren't smart enough.\nCertainly we could have anticipated that GDB was going to be tremendously popular, and was going to\nbe ported to dozens and dozens of architectures, both native and cross. If we had known that, we could\nhave started with the gdbarch objects, instead of spending years upgrading old macros and global\nvariables; ditto for the target vector.\nCertainly we could have anticipated GDB was going to be used with GUIs. After all in 1986 both the Mac\nand the X Window System had already been out for two years! Instead of designing a traditional\ncommand interface, we could have set it up to handle events asynchronously.\nThe real lesson though is that not that GDBers were dumb, but that we couldn't possibly have been\nsmart enough to anticipate how GDB would need to evolve. In 1986 it was not at all clear that the\nwindows-and-mouse interface was going to become ubiquitous; if the first version of GDB was perfectly\nadapted for GUI use, we'd have looked like geniuses, but it would have been sheer luck. Instead, by\nmaking GDB useful in a more limited scope, we built a user base that enabled more extensive\ndevelopment and re-engineering later.\nLearn to Live with Incomplete Transitions\nTry to complete transitions, but they may take a while; expect to live with them being incomplete.\nAt the GCC Summit in 2003, Zack Weinberg lamented the \"incomplete transitions\" in GCC, where new\ninfrastructure had been introduced, but the old infrastructure could not be removed. GDB has these also,\nbut we can point to a number of transitions that have been completed, such as the target vector and\ngdbarch. Even so, they can take a number of years to complete, and in the meantime one has to keep\nthe debugger running.\nDon't Get Too Attached to the Code\nWhen you spend a long time with a single body of code, and it's an important program that also pays the\nbills, it's easy to get attached to it, and even to mold your thinking to fit the code, rather than the other\nway around.\nDon't.\nEverything in the code originated with a series of conscious decisions: some inspired, some less so. The\nclever space-saving trick of 1991 is a pointless complexity with the multi-gigabyte RAMs of 2011.\nGDB once supported the Gould supercomputer. When they turned off the last machine, around 2000,\nthere really wasn't any point in keeping those bits around. That episode was the genesis of an obsoletion\nprocess for GDB, and most releases now include the retirement of some piece or another.\nIn fact, there are a number of radical changes on the table or already underway, ranging from the\nadoption of Python for scripting, to support for debugging of highly parallel multicore systems, to recoding\ninto C++. The changes may take years to complete; all the more reason to get started on them now.\nBack to top\nBack to The Architecture of Open Source Applications.\n", "page": 52, "type": "text", "section": "Page 52"}
{"text": "<>\nThe Glasgow Haskell Compiler (GHC) started as part of an academic research project funded by the UK\ngovernment at the beginning of the 1990s, with several goals in mind:\nTo make freely available a robust and portable compiler for Haskell that generates high performance\ncode;\nTo provide a modular foundation that other researchers can extend and develop;\nTo learn how real programs behave, so that we can design and build better compilers.\nGHC is now over 20 years old, and has been under continuous active development since its inception.\nToday, GHC releases are downloaded by hundreds of thousands of people, the online repository of\nHaskell libraries has over 3,000 packages, GHC is used to teach Haskell in many undergraduate\ncourses, and there are a growing number of instances of Haskell being depended upon commercially.\nOver its lifetime GHC has generally had around two or three active developers, although the number of\npeople who have contributed some code to GHC is in the hundreds. While the ultimate goal for us, the\nmain developers of GHC, is to produce research rather than code, we consider developing GHC to be an\nessential prerequisite: the artifacts of research are fed back into GHC, so that GHC can then be used as\nthe basis for further research that builds on these previous ideas. Moreover, it is important that GHC is\nan industrial-strength product, since this gives greater credence to research results produced with it. So\nwhile GHC is stuffed full of cutting-edge research ideas, a great deal of effort is put into ensuring that it\ncan be relied on for production use. There has often been some tension between these two seemingly\ncontradictory goals, but by and large we have found a path that is satisfactory both from the research\nand the production-use angles.\nIn this chapter we want to give an overview of the architecture of GHC, and focus on a handful of the key\nideas that have been successful in GHC (and a few that haven't). Hopefully throughout the following\npages you will gain some insight into how we managed to keep a large software project active for over\n20 years without it collapsing under its own weight, with what is generally considered to be a very small\ndevelopment team.\n5.1. What is Haskell?\nHaskell is a functional programming language, defined by a document known as the \"Haskell Report\" of\nwhich the latest revision is Haskell 2010 [Mar10]. Haskell was created in 1990 by several members of\nthe academic research community interested in functional languages, to address the lack of a common\nlanguage that could be used as a focus for their research.\nTwo features of Haskell stand out amongst the programming languages crowd:\nIt is purely functional. That is, functions cannot have side effects or mutate data; for a given set of\ninputs (arguments) a function always gives the same result. The benefits of this model for reasoning\nabout code (and, we believe, writing code) are clear, but integrating input/output into the purely\nfunctional setting proved to be a significant challenge. Fortunately an elegant solution in the form of\nmonads was discovered, which not only allowed input/output to be neatly integrated with purely\nfunctional code, but introduced a powerful new abstraction that revolutionised coding in Haskell (and\nsubsequently had an impact on other languages too).\nIt is lazy. This refers to the evaluation strategy of the language: most languages use strict evaluation\nin which the arguments to a function are evaluated before the function is called, whereas in Haskell\nthe arguments to a function are passed\nunevaluated, and only evaluated on demand. This aspect of Haskell also has benefits for reasoning\nThe Glasgow\nHaskell\nCompiler\nSimon Marlow and Simon Peyton-Jones\n", "page": 53, "type": "text", "section": "Page 53"}
{"text": "about programs, but more than anything else serves as a barrier to prevent the leakage of impure\nnon-functional features into the language: such features fundamentally cannot work in conjunction\nwith lazy semantics.\nHaskell is also strongly-typed, while supporting type inference which means that type annotations are\nrarely necessary.\nThose interested in a complete history of Haskell should read [HHPW07].\n5.2. High-Level Structure\nAt the highest level, GHC can be divided into three distinct chunks:\nThe compiler itself. This is essentially a Haskell program whose job is to convert Haskell source\ncode into executable machine code.\nThe Boot Libraries. GHC comes with a set of libraries that we call the boot libraries, because they\nconstitute the libraries that the compiler itself depends on. Having these libraries in the source tree\nmeans that GHC can bootstrap itself. Some of these libraries are very tightly coupled to GHC,\nbecause they implement low-level functionality such as the Int  type in terms of primitives defined\nby the compiler and runtime system. Other libraries are more high-level and compiler-independent,\nsuch as the\nData.Map  library.\nThe Runtime System (RTS). This is a large library of C code that handles all the tasks associated\nwith running the compiled Haskell code, including garbage collection, thread scheduling, profiling,\nexception handling and so on. The RTS is linked into every compiled Haskell program. The RTS\nrepresents a significant chunk of the development effort put into GHC, and the design decisions\nmade there are responsible for some of Haskell's key strengths, such as its efficient support for\nconcurrency and parallelism. We'll describe the RTS in more detail in Section 5.5.\nIn fact, these three divisions correspond exactly to three subdirectories of a GHC source tree:\ncompiler , libraries , and rts  respectively.\nWe won't spend much time here discussing the boot libraries, as they are largely uninteresting from an\narchitecture standpoint. All the key design decisions are embodied in the compiler and runtime system,\nso we will devote the rest of this chapter to discussing these two components.\nCode Metrics\nThe last time we measured the number of lines in GHC was in 1992 (\"The Glasgow Haskell compiler: a\ntechnical overview\", JFIT technical conference digest, 1992), so it is interesting to look at how things have\nchanged since then. Figure 5.1 gives a breakdown of the number of lines of code in GHC divided up into\nthe major components, comparing the current tallies with those from 1992.\nModule\nLines (1992)\nLines (2011)\nIncrease\nCompiler\nMain\n997\n11,150\n11.2\nParser\n1,055\n4,098\n3.9\nRenamer\n2,828\n4,630\n1.6\nType checking\n3,352\n24,097\n7.2\nDesugaring\n1,381\n7,091\n5.1\nCore transformations\n1,631\n9,480\n5.8\nSTG transformations\n814\n840\n1\nData-Parallel Haskell\n\u2014\n3,718\n\u2014\nCode generation\n2913\n11,003\n3.8\nNative code generation\n\u2014\n14,138\n\u2014\nLLVM code generation\n\u2014\n2,266\n\u2014\nGHCi\n\u2014\n7,474\n\u2014\nHaskell abstract syntax\n2,546\n3,700\n1.5\nCore language\n1,075\n4,798\n4.5\nSTG language\n517\n693\n1.3\nC-- (was Abstract C)\n1,416\n7,591\n5.4\nIdentifier representations\n1,831\n3,120\n1.7\nType representations\n1,628\n3,808\n2.3\nPrelude definitions\n3,111\n2,692\n0.9\nUtilities\n1,989\n7,878\n3.96\n", "page": 54, "type": "text", "section": "Page 54"}
{"text": "Profiling\n191\n367\n1.92\nCompiler Total\n28,275\n139,955\n4.9\nRuntime System\nAll C and C-- code\n43,865\n48,450\n1.10\nFigure 5.1: Lines of code in GHC, past and present\nThere are some notable aspects of these figures:\nDespite nearly 20 years of non-stop development the compiler has only increased in size by a factor\nof 5, from around 28,000 to around 140,000 lines of Haskell code. We obsessively refactor while\nadding new code, keeping the code base as fresh as possible.\nThere are several new components, although these only account for about 28,000 new lines. Much of\nthe new components are concerned with code generation: native code generators for various\nprocessors, and an LLVM code generator. (Formerly the \"Low Level Virtual Machine\", the LLVM\nproject includes a generic code-generator with targets for many different processors. For more\ninformation see http://llvm.org/, and the chapter on LLVM in Volume 1 of The Architecture of Open\nSource Applications.) The infrastructure for the interactive interpreter GHCi also added over 7,000\nlines.\nThe biggest increase in a single component is the type checker, where over 20,000 lines were\nadded. This is unsurprising given that much of the recent research using GHC has been into new\ntype system extensions (for example GADTs [PVWW06] and Type Families [CKP05]).\nA lot of code has been added to the Main  component; this is partly because there was previously a\n3,000-line Perl script called the \"driver\" that was rewritten in Haskell and moved into GHC proper,\nand also because support for compiling multiple modules was added.\nThe runtime system has barely grown: it is only 10% larger, despite having accumulated a lot of new\nfunctionality and being ported to more platforms. We rewrote it completely around 1997.\nGHC has a complex build system, which today comprises about 6,000 lines of GNU make code. It is\non its fourth complete rewrite, the latest being about two years ago, and each successive iteration\nhas reduced the amount of code.\nThe Compiler\nWe can divide the compiler into three:\nThe compilation manager, which is responsible for the compilation of multiple Haskell source files.\nThe job of the compilation manager is to figure out in which order to compile the different files, and\nto decide which modules do not need to be recompiled because none of their dependencies have\nchanged since the last time they were compiled.\nThe Haskell compiler (we abbreviate this as Hsc  inside GHC), which handles the compilation of a\nsingle Haskell source file. As you might imagine, most of the action happens in here. The output of\nHsc  depends on what backend is selected: assembly, LLVM code, or bytecode.\nThe pipeline, which is responsible for composing together any necessary external programs with\nHsc  to compile a Haskell source file to object code. For example, a Haskell source file may need\npreprocessing with the C preprocessor before feeding to Hsc , and the output of Hsc  is usually an\nassembly file that must be fed into the assembler to create an object file.\nThe compiler is not simply an executable that performs these functions; it is itself a library with a large\nAPI that can be used to build other tools that work with Haskell source code, such as IDEs and analysis\ntools.\nCompiling Haskell Code\nAs with most compilers, compiling a Haskell source file proceeds in a sequence of phases, with the\noutput of each phase becoming the input of the subsequent phase. The overall structure of the different\nphases is illustrated in Figure 5.2.\n", "page": 55, "type": "text", "section": "Page 55"}
{"text": "Figure 5.2: The compiler phases\nParsing\nWe start in the traditional way with parsing, which takes as input a Haskell source file and produces as\noutput abstract syntax. In GHC the abstract syntax datatype HsSyn  is parameterised by the types of the\nidentifiers it contains, so an abstract syntax tree has type HsSyn t  for some type of identifiers t . This\nenables us to add more information to identifiers as the program passes through the various stages of\nthe compiler, while reusing the same type of abstract syntax trees.\nThe output of the parser is an abstract syntax tree in which the identifiers are simple strings, which we\ncall RdrName . Hence, the abstract syntax produced by the parser has type HsSyn RdrName .\nGHC uses the tools Alex  and Happy  to generate its lexical analysis and parsing code respectively,\nwhich are analogous to the tools lex  and yacc  for C.\nGHC's parser is purely functional. In fact, the API of the GHC library provides a pure function called\nparser  that takes a String  (and a few other things) and returns either the parsed abstract syntax or\nan error message.\nRenaming\nRenaming is the process of resolving all of the identifiers in the Haskell source code into fully qualified\nnames, at the same time identifying any out-of-scope identifiers and flagging errors appropriately.\nIn Haskell it is possible for a module to re-export an identifier that it imported from another module. For\nexample, suppose module A  defines a function called f , and module B  imports module A  and re-\nexports f . Now, if a module C  imports module B , it can refer to f  by the name B.f \u2014even though\nf  is originally defined in module A . This is a useful form of namespace manipulation; it means that a\nlibrary can use whatever module structure it likes internally, but expose a nice clean API via a few\ninterface modules that re-export identifiers from the internal modules.\n", "page": 56, "type": "text", "section": "Page 56"}
{"text": "The compiler however has to resolve all this, so that it knows what each name in the source code\ncorresponds to. We make a clean distinction between the entities, the \"things themselves\" (in our\nexample, A.f ), and the names by which the entities can be referred to (e.g., B.f ). At any given point\nin the source code, there are a set of entities in scope, and each may be known by one or more different\nnames. The job of the renamer is to replace each of the names in the compiler's internal representation\nof the code by a reference to a particular entity. Sometimes a name can refer to several different entities;\nby itself that is not an error, but if the name is actually used, then the renamer will flag an ambiguity\nerror and reject the program.\nRenaming takes Haskell abstract syntax ( HsSyn ,RdrName ) as input, and also produces abstract\nsyntax as output ( HsSyn Name ). Here a Name  is a reference to a particular entity.\nResolving names is the main job of the renamer, but it performs a plethora of other tasks too: collecting\nthe equations of a function together and flagging an error if they have differing numbers of arguments;\nrearranging infix expressions according to the fixity of the operators; spotting duplicate declarations;\ngenerating warnings for unused identifiers, and so on.\nType Checking\nType checking, as one might imagine, is the process of checking that the Haskell program is type-\ncorrect. If the program passes the type checker, then it is guaranteed to not crash at runtime. (The term\n\"crash\" here has a formal definition that includes hard crashes like \"segmentation fault\", but not things\nlike pattern-matching failure. The non-crash guarantee can be subverted by using certain unsafe\nlanguage features, such as the Foreign Function Interface.)\nThe input to the type checker is HsSyn Name  (Haskell source with qualified names), and the output is\nHsSyn Id . An Id  is a Name  with extra information: notably a type. In fact, the Haskell syntax\nproduced by the type checker is fully decorated with type information: every identifier has its type\nattached, and there is enough information to reconstruct the type of any subexpression (which might be\nuseful for an IDE, for example).\nIn practice, type checking and renaming may be interleaved, because the Template Haskell feature\ngenerates code at runtime that itself needs to be renamed and type checked.\nDesugaring, and the Core language\nHaskell is a rather large language, containing many different syntactic forms. It is intended to be easy for\nhumans to read and write\u2014there is a wide range of syntactic constructs which gives the programmer\nplenty of flexibility in choosing the most appropriate construct for the situation at hand. However, this\nflexibility means that there are often several ways to write the same code; for example, an if\nexpression is identical in meaning to a case  expression with True  and False  branches, and list-\ncomprehension notation can be translated into calls to map , filter , and concat . In fact, the\ndefinition of the Haskell language defines all these constructs by their translation into simpler constructs;\nthe constructs that can be translated away like this are called \"syntactic sugar\".\nIt is much simpler for the compiler if all the syntactic sugar is removed, because the subsequent\noptimisation passes that need to work with the Haskell program have a smaller language to deal with.\nThe process of desugaring therefore removes all the syntactic sugar, translating the full Haskell syntax\ninto a much smaller language that we call Core . We'll talk about Core  in detail later.\nOptimisation\nNow that the program is in Core , the process of optimisation begins. One of GHC's great strengths is in\noptimising away layers of abstraction, and all of this work happens at the Core  level. Core  is a tiny\nfunctional language, but it is a tremendously flexible medium for expressing optimisations, ranging from\nthe very high-level, such as strictness analysis, to the very low-level, such as strength reduction.\nEach of the optimisation passes takes Core  and produces Core . The main pass here is called the\nSimplifier, whose job it is to perform a large collection of correctness-preserving transformations, with the\ngoal of producing a more efficient program. Some of these transformations are simple and obvious, such\nas eliminating dead code or reducing a case expression when the value being scrutinised is known, and\nsome are more involved, such as function inlining and applying rewrite rules (discussed later).\nThe simplifier is normally run between the other optimisation passes, of which there are about six; which\npasses are actually run and in which order depends on the optimisation level selected by the user.\nCode Generation\nOnce the Core  program has been optimised, the process of code generation begins. After a couple of\nadministrative passes, the code takes one of two routes: either it is turned into byte code for execution by\nthe interactive interpreter, or it is passed to the code generator for eventual translation to machine code.\nThe code generator first converts the Core  into a language called STG , which is essentially just Core\nannotated with more information required by the code generator. Then, STG  is translated to Cmm , a\nlow-level imperative language with an explicit stack. From here, the code takes one of three routes:\nNative code generation: GHC contains simple native code generators for a few processor\narchitectures. This route is fast, and generates reasonable code in most cases.\nLLVM code generation: The Cmm  is converted to LLVM code and passed to the LLVM compiler.\nThis route can produce significantly better code in some cases, although it takes longer than the\n", "page": 57, "type": "text", "section": "Page 57"}
{"text": "native code generator.\nC code generation: GHC can produce ordinary C code. This route produces significantly slower\ncode than the other two routes, but can be useful for porting GHC to new platforms.\n5.3. Key Design Choices\nIn this section we focus on a handful of the design choices that have been particularly effective in GHC.\nThe Intermediate Language\nExpressions\nt,e,u ::= x\nVariables\n|\nK\nData constructors\n|\nk\nLiterals\n|\n\u03bb x:\u03c3.e | e u\nValue abstraction and application\n|\n\u039b a:\u03b7.e | e \u03c6\nType abstraction and application\n|\nlet x:\u03c4 = e in u Local bindings\n|\ncase e of p\u2192u Case expressions\n|\ne  \u03b3\nCasts\n|\n \u03b3 \nCoercions\np\n::= K c:\u03b7 x:\u03c4\nPatterns\nFigure 5.3: The syntax of Core\nA typical structure for a compiler for a statically-typed language is this: the program is type checked, and\ntransformed to some untyped intermediate language, before being optimised. GHC is different: it has a\nstatically-typed intermediate language. As it turns out, this design choice has had a pervasive effect on\nthe design and development of GHC.\nGHC's intermediate language is called Core  (when thinking of the implementation) or System FC (when\nthinking about the theory). Its syntax is given in Figure 5.3. The exact details are not important here; the\ninterested reader can consult [SCPD07] for more details. For our present purposes, however, the\nfollowing points are the key ones:\nHaskell is a very large source language. The data type representing its syntax tree has literally\nhundreds of constructors.\nIn contrast Core  is a tiny, principled, lambda calculus. It has extremely few syntactic forms, yet we\ncan translate all of Haskell into Core .\nHaskell is an implicitly-typed source language. A program may have few or no type annotations;\ninstead it is up to the type inference algorithm to figure out the type of every binder and sub-\nexpressions. This type inference algorithm is complex, and occasionally somewhat ad hoc, reflecting\nthe design compromises that every real programming language embodies.\nIn contrast Core  is an explicitly-typed language. Every binder has an explicit type, and terms\ninclude explicit type abstractions and applications. Core  enjoys a very simple, fast type checking\nalgorithm that checks that the program is type correct. The algorithm is entirely straightforward; there\nare no ad hoc compromises.\nAll of GHC's analysis and optimisation passes work on Core . This is great: because Core  is such a\ntiny language an optimisation has only a few cases to deal with. Although Core  is small, it is extremely\nexpressive\u2014System F was, after all, originally developed as a foundational calculus for typed\ncomputation. When new language features are added to the source language (and that happens all the\ntime) the changes are usually restricted to the front end; Core  stays unchanged, and hence so does\nmost of the compiler.\nBut why is Core  typed? After all, if the type inference engine accepts the source program, that program\nis presumably well typed, and each optimisation pass presumably maintains that type-correctness. Core\nmay enjoy a fast type checking algorithm, but why would you ever want to run it? Moreover, making\nCore  typed carries significant costs, because every transformation or optimisation pass must produce a\nwell-typed program, and generating all those type annotations is often non-trivial.\nNevertheless, it has been a huge win to have an explicitly-typed intermediate language, for several\nreasons:\nRunning the Core  type checker (we call it Lint ) is a very powerful consistency check on the\ncompiler itself. Imagine that you write an \"optimisation\" that accidentally generates code that treats\nan integer value as a function, and tries to call it. The chances are that the program will\nsegmentation fault, or fail at runtime in a bizarre way. Tracing a seg-fault back to the particular\noptimisation pass that broke the program is a long road.\nNow imagine instead that we run Lint  after every optimisation pass (and we do, if you use the flag\n-dcore-lint ): it will report a precisely located error immediately after the offending optimisation.\nWhat a blessing.\nOf course, type soundness is not the same as correctness: Lint  will not signal an error if you\n\"optimise\" (x*1)  to 1 instead of to x. But if the program passes Lint , it will guarantee to run\nwithout seg-faults; and moreover in practice we have found that it is surprisingly hard to accidentally\n", "page": 58, "type": "text", "section": "Page 58"}
{"text": "write optimisations that are type-correct but not semantically correct.\nThe type inference algorithm for Haskell is very large and very complex: a glance at Figure 5.1\nconfirms that the type checker is by far the largest single component of GHC. Large and complex\nmeans error-prone. But Lint  serves as an 100% independent check on the type inference engine;\nif the type inference engine accepts a program that is not, in fact, type-correct, Lint  will reject it.\nSo Lint  serves as a powerful auditor of the type inference engine.\nThe existence of Core  has also proved to be a tremendous sanity check on the design of the\nsource language. Our users constantly suggest new features that they would like in the language.\nSometimes these features are manifestly \"syntactic sugar\", convenient new syntax for something you\ncan do already. But sometimes they are deeper, and it can be hard to tell how far-reaching the\nfeature is.\nCore  gives us a precise way to evaluate such features. If the feature can readily be translated into\nCore , that reassures us that nothing fundamentally new is going on: the new feature is syntactic-\nsugar-like. On the other hand, if it would require an extension to Core , then we think much, much\nmore carefully.\nIn practice Core  has been incredibly stable: over a 20-year time period we have added exactly one new\nmajor feature to Core  (namely coercions and their associated casts). Over the same period, the source\nlanguage has evolved enormously. We attribute this stability not to our own brilliance, but rather to the\nfact that Core  is based directly on foundational mathematics: bravo Girard!\nType Checking the Source Language\nOne interesting design decision is whether type checking should be done before or after desugaring. The\ntrade-offs are these:\nType checking before desugaring means that the type checker must deal directly with Haskell's very\nlarge syntax, so the type checker has many cases to consider. If we desugared into (an untyped\nvariant of) Core  first, one might hope that the type checker would become much smaller.\nOn the other hand, type checking after desugaring would impose a significant new obligation: that\ndesugaring does not affect which programs are type-correct. After all, desugaring implies a\ndeliberate loss of information. It is probably the case that in 95% of the cases there is no problem,\nbut any problem here would force some compromise in the design of Core  to preserve some extra\ninformation.\nMost seriously of all, type checking a desugared program would make it much harder to report errors\nthat relate to the original program text, and not to its (sometimes elaborate) desugared version.\nMost compilers type check after desugaring, but for GHC we made the opposite choice: we type check\nthe full original Haskell syntax, and then desugar the result. It sounds as if adding a new syntactic\nconstruct might be complicated, but (following the French school) we have structured the type inference\nengine in a way that makes it easy. Type inference is split into two parts:\n1. Constraint generation: walk over the source syntax tree, generating a collection of type constraints.\nThis step deals with the full syntax of Haskell, but it is very straightforward code, and it is easy to\nadd new cases.\n2. Constraint solving: solve the gathered constraints. This is where the subtlety of the type inference\nengine lies, but it is independent of the source language syntax, and would be the same for a much\nsmaller or much larger language.\nOn the whole, the type-check-before-desugar design choice has turned out to be a big win. Yes, it adds\nlines of code to the type checker, but they are simple lines. It avoids giving two conflicting roles to the\nsame data type, and makes the type inference engine less complex, and easier to modify. Moreover,\nGHC's type error messages are pretty good.\nNo Symbol Table\nCompilers usually have one or more data structures known as symbol tables, which are mappings from\nsymbols (e.g., variables) to some information about the variable, such as its type, or where in the source\ncode it was defined.\nIn GHC we use symbol tables quite sparingly; mainly in the renamer and type checker. As far as\npossible, we use an alternative strategy: a variable is a data structure that contains all the information\nabout itself. Indeed, a large amount of information is reachable by traversing the data structure of a\nvariable: from a variable we can see its type, which contains type constructors, which contain their data\nconstructors, which themselves contain types, and so on. For example, here are some data types from\nGHC (heavily abbreviated and simplified):\n  data Id      = MkId Name Type\n  data Type    = TyConApp TyCon [Type]\n               | ....\n  data TyCon   = AlgTyCon Name [DataCon]\n               | ...\n  data DataCon = MkDataCon Name Type ...\nAn Id  contains its Type . A Type  might be an application of a type constructor to some arguments\n", "page": 59, "type": "text", "section": "Page 59"}
{"text": "(e.g., Maybe Int ), in which case it contains the TyCon . A TyCon  can be an algebraic data type, in\nwhich case it includes a list of its data constructors. Each DataCon  includes its Type , which of course\nmentions the TyCon . And so on. The whole structure is highly interconnected. Indeed it is cyclic; for\nexample, a TyCon  may contain a DataCon  which contains a Type , which contains the very TyCon\nwe started with.\nThis approach has some advantages and disadvantages:\nMany queries that would require a lookup in a symbol table are reduced to a simple field access,\nwhich is great for efficiency and code clarity.\nThere is no need to carry around extra symbol tables, the abstract syntax tree already contains all\nthe information.\nThe space overheads are better: all instances of the same variable share the same data structure,\nand there is no space needed for the table.\nThe only difficulties arise when we need to change any of the information associated with a variable.\nThis is where a symbol table has the advantage: we would just change the entry in the symbol table.\nIn GHC we have to traverse the abstract syntax tree and replace all the instances of the old variable\nwith the new one; indeed the simplifier does this regularly, as it needs to update certain optimisation-\nrelated information about each variable.\nIt is hard to know whether it would be better or worse overall to use symbol tables, because this aspect\nof the design is so fundamental that it is almost impossible to change. Still, avoiding symbol tables is a\nnatural choice in the purely functional setting, so it seems likely that this approach is a good choice for\nHaskell.\nInter-Module Optimisation\nFunctional languages encourage the programmer to write small definitions. For example, here is the\ndefinition of &&  from the standard library:\n(&&) :: Bool -> Bool -> Bool\nTrue && True = True\n_    && _    = False\nIf every use of such a function really required a function call, efficiency would be terrible. One solution is\nto make the compiler treat certain functions specially; another is to use a pre-processor to replace a\n\"call\" with the desired inline code. All of these solutions are unsatisfactory in one way or another,\nespecially as another solution is so obvious: simply inline the function. To \"inline a function\" means to\nreplace the call by a copy of the function body, suitably instantiating its parameters.\nIn GHC we have systematically adopted this approach [PM02]. Virtually nothing is built into the compiler.\nInstead, we define as much as possible in libraries, and use aggressive inlining to eliminate the\noverheads. This means that programmers can define their own libraries that will be inlined and optimised\nas well as the ones that come with GHC.\nA consequence is that GHC must be able to do cross-module, and indeed cross-package, inlining. The\nidea is simple:\nWhen compiling a Haskell module Lib.hs , GHC produces object code in Lib.o  and an \"interface\nfile\" in Lib.hi . This interface file contains information about all the functions that Lib  exports,\nincluding both their types and, for sufficiently small functions, their definitions.\nWhen compiling a module Client.hs  that imports Lib , GHC reads the interface Lib.hi . So if\nClient  calls a function Lib.f  defined in Lib , GHC can use the information in Lib.hi  to\ninline Lib.f .\nBy default GHC will expose the definition of a function in the interface file only if the function is \"small\"\n(there are flags to control this size threshold). But we also support an INLINE pragma, to instruct GHC to\ninline the definition aggressively at call sites, regardless of size, thus:\nfoo :: Int -> Int \n{-# INLINE foo #-}\nfoo x = <some big expression>\nCross-module inlining is absolutely essential for defining super-efficient libraries, but it does come with a\ncost. If the author upgrades his library, it is not enough to re-link Client.o  with the new Lib.o ,\nbecause Client.o  contains inlined fragments of the old Lib.hs , and they may well not be\ncompatible with the new one. Another way to say this is that the ABI (Application Binary Interface) of\nLib.o  has changed in a way that requires recompilation of its clients.\nIn fact, the only way for compilation to generate code with a fixed, predictable ABI is to disable cross-\nmodule optimisation, and this is typically too high a price to pay for ABI compatibility. Users working with\nGHC will usually have the source code to their entire stack available, so recompiling is not normally an\nissue (and, as we will describe later, the package system is designed around this mode of working).\nHowever, there are situations where recompiling is not practical: distributing bug fixes to libraries in a\nbinary OS distribution, for example. In the future we hope it may be possible to find a compromise\n", "page": 60, "type": "text", "section": "Page 60"}
{"text": "solution that allows retaining ABI compatibility while still allowing some cross-module optimisation to take\nplace.\n5.4. Extensibility\nIt is often the case that a project lives or dies according to how extensible it is. A monolithic piece of\nsoftware that is not extensible has to do everything and do it right, whereas an extensible piece of\nsoftware can be a useful base even if it doesn't provide all the required functionality out of the box.\nOpen source projects are of course extensible by definition, in that anyone can take the code and add\ntheir own features. But modifying the original source code of a project maintained by someone else is not\nonly a high-overhead approach, it is also not conducive to sharing your extension with others. Therefore\nsuccessful projects tend to offer forms of extensibility that do not involve modifying the core code, and\nGHC is no exception in this respect.\nUser-Defined Rewrite Rules\nThe core of GHC is a long sequence of optimisation passes, each of which performs some semantics-\npreserving transformation, Core  into Core . But the author of a library defines functions that often have\nsome non-trivial, domain-specific transformations of their own, ones that cannot possibly be predicted by\nGHC. So GHC allows library authors to define rewrite rules that are used to rewrite the program during\noptimisation [PTH01]. In this way, programmers can, in effect, extend GHC with domain-specific\noptimisations.\nOne example is the foldr/build  rule, which is expressed like this:\n{-# RULES \"fold/build\"    \n    forall k z (g::forall b. (a->b->b) -> b -> b) . \n       foldr k z (build g) = g k z\n #-}\nThe entire rule is a pragma, introduced by {-# RULES . The rule says that whenever GHC sees the\nexpression (foldr k z (build g))  it should rewrite it to (g k z) . This transformation is\nsemantics-preserving, but it takes a research paper to argue that it is [GLP93], so there is no chance of\nGHC performing it automatically. Together with a handful of other rules, and some INLINE pragmas, GHC\nis able to fuse together list-transforming functions. For example, the two loops in (map f (map g xs))\nare fused into one.\nAlthough rewrite rules are simple and easy to use, they have proved to be a very powerful extension\nmechanism. When we first introduced the feature into GHC ten years ago we expected it to be an\noccasionally useful facility. But in practice it has turned out to be useful in very many libraries, whose\nefficiency often depends crucially on rewrite rules. For example, GHC's own base  library contains\nupward of 100 rules, while the popular vector  library uses several dozen.\nCompiler Plugins\nOne way in which a compiler can offer extensibility is to allow programmers to write a pass that is\ninserted directly into the compiler's pipeline. Such passes are often called \"plugins\". GHC supports\nplugins in the following way:\nThe programmer writes a Core  to Core  pass, as an ordinary Haskell function in a module P.hs ,\nsay, and compiles it to object code.\nWhen compiling some module, the programmer uses the command-line flag -plugin P .\n(Alternatively, he can give the flag in a pragma at the start of the module.)\nGHC searches for P.o , dynamically links it into the running GHC binary, and calls it at the\nappropriate point in the pipeline.\nBut what is \"the appropriate point in the pipeline\"? GHC does not know, and so it allows the plugin to\nmake that decision. As a result of this and other matters, the API that the plugin must offer is a bit more\ncomplicated than a single Core  to Core  function\u2014but not much.\nPlugins sometimes require, or produce, auxiliary plugin-specific data. For example, a plugin might perform\nsome analysis on the functions in the module being compiled ( M.hs , say), and might want to put that\ninformation in the interface file M.hi , so that the plugin has access to that information when compiling\nmodules that import M . GHC offers an annotation mechanism to support this.\nPlugins and annotations are relatively new to GHC. They have a higher barrier to entry than rewrite rules,\nbecause the plugin is manipulating GHC's internal data structures, but of course they can do much more.\nIt remains to be seen how widely they will be used.\nGHC as a Library: The GHC API\nOne of GHC's original goals was to be a modular foundation that others could build on. We wanted the\ncode of GHC to be as transparent and well-documented as possible, so that it could be used as the\nbasis for research projects by others; we imagined that people would want to make their own\nmodifications to GHC to add new experimental features or optimisations. Indeed, there have been some\nexamples of this: for example, there exists a version of GHC with a Lisp front-end, and a version of GHC\n", "page": 61, "type": "text", "section": "Page 61"}
{"text": "that generates Java code, both developed entirely separately by individuals with little or no contact with\nthe GHC team.\nHowever, producing modified versions of GHC represents only a small subset of the ways in which the\ncode of GHC can be re-used. As the popularity of the Haskell language has grown, there has been an\nincreasing need for tools and infrastructure that understand Haskell source code, and GHC of course\ncontains a lot of the functionality necessary for building these tools: a Haskell parser, abstract syntax,\ntype checker and so on.\nWith this in mind, we made a simple change to GHC: rather than building GHC as a monolithic program,\nwe build GHC as a library, that is then linked with a small Main module to make the GHC executable\nitself, but also shipped in library form so that users can call it from their own programs. At the same time\nwe built an API to expose GHC's functionality to clients. The API provides enough functionality to\nimplement the GHC batch compiler and the GHCi interactive environment, but it also provides access to\nindividual passes such as the parser and type checker, and allows the data structures produced by these\npasses to be inspected. This change has given rise to a wide range of tools built using the GHC API,\nincluding:\nA documentation tool, Haddock, which reads Haskell source code and produces HTML\ndocumentation.\nNew versions of the GHCi front end with additional features; e.g., ghci-haskeline which was\nsubsequently merged back into GHC.\nIDEs that offer advanced navigation of Haskell source code; e.g., Leksah.\nhint, a simpler API for on-the-fly evaluation of Haskell source code.\nThe Package System\nThe package system has been a key factor in the growth in use of the Haskell language in recent years.\nIts main purpose is to enable Haskell programmers to share code with each other, and as such it is an\nimportant aspect of extensibility: the package system extends the shared codebase beyond GHC itself.\nThe package system embodies various pieces of infrastructure that together make sharing code easy.\nWith the package system as the enabler, the community has built a large body of shared code; rather\nthan relying on libraries from a single source, Haskell programmers draw on libraries developed by the\nwhole community. This model has worked well for other languages; CPAN for Perl, for example, although\nHaskell being a predominantly compiled rather than interpreted language presents a somewhat different\nset of challenges.\nBasically, the package system lets a user manage libraries of Haskell code written by other people, and\nuse them in their own programs and libraries. Installing a Haskell library is as simple as uttering a single\ncommand, for example:\n$ cabal install zlib\ndownloads the code for the zlib  package from http://hackage.haskell.org, compiles it using GHC,\ninstalls the compiled code somewhere on your system (e.g., in your home directory on a Unix system),\nand registers the installation with GHC. Furthermore, if zlib  depends on any other packages that are\nnot yet installed, those will also be downloaded, compiled and installed automatically before zlib  itself\nis compiled. It is a tremendously smooth way to work with libraries of Haskell code shared by others.\nThe package system is made of four components, only the first of which is strictly part of the GHC\nproject:\nTools for managing the package database, which is simply a repository for information about the\npackages installed on your system. GHC reads the package database when it starts up, so that it\nknows which packages are available and where to find them.\nA library called Cabal  (Common Architecture for Building Applications and Libraries), which\nimplements functionality for building, installing and registering individual packages.\nA website at http://hackage.haskell.org which hosts packages written and uploaded by users. The\nwebsite automatically builds documentation for the packages which can be browsed online. At the\ntime of writing, Hackage is hosting over 3,000 packages covering functionality including database\nlibraries, web frameworks, GUI toolkits, data structures, and networking.\nThe cabal  tool which ties together the Hackage website and the Cabal  library: it downloads\npackages from Hackage, resolves dependencies, and builds and installs packages in the right order.\nNew packages can also be uploaded to Hackage using cabal  from the command line.\nThese components have been developed over several years by members of the Haskell community and\nthe GHC team, and together they make a system that fits perfectly with the open source development\nmodel. There are no barriers to sharing code or using code that others have shared (provided you\nrespect the relevant licenses, of course). You can be using a package that someone else has written\nliterally within seconds of finding it on Hackage.\nHackage has been so successful that the remaining problems it has are now those of scale: users find it\ndifficult to choose amongst the four different database frameworks, for example. Ongoing developments\nare aimed at solving these problems in ways that leverage the community. For example, allowing users to\ncomment and vote on packages will make it easier to find the best and most popular packages, and\n", "page": 62, "type": "text", "section": "Page 62"}
{"text": "collecting data on build success or failures from users and reporting the results will help users avoid\npackages that are unmaintained or have problems.\n5.5. The Runtime System\nThe Runtime System is a library of mostly C code that is linked into every Haskell program. It provides\nthe support infrastructure needed for running the compiled Haskell code, including the following main\ncomponents:\nMemory management, including a parallel, generational, garbage collector;\nThread management and scheduling;\nThe primitive operations provided by GHC;\nA bytecode interpreter and dynamic linker for GHCi.\nThe rest of this section is divided into two: first we focus on a couple of the aspects of the design of the\nRTS that we consider to have been successful and instrumental in making it work so well, and secondly\nwe talk about the coding practices and infrastructure we have built in the RTS for coping with what is a\nrather hostile programming environment.\nKey Design Decisions\nIn this section we describe two of the design decisions in the RTS that we consider to have been\nparticularly successful.\nThe Block Layer\nThe garbage collector is built on top of a block layer that manages memory in units of blocks, where a\nblock is a multiple of 4 KB in size. The block layer has a very simple API:\ntypedef struct bdescr_ {\n    void *               start;\n    struct bdescr_ *     link;\n    struct generation_ * gen;   // generation\n    // .. various other fields\n} bdescr;\nbdescr * allocGroup (int n);\nvoid     freeGroup  (bdescr *p);\nbdescr * Bdescr     (void *p);  // a macro\nThis is the only API used by the garbage collector for allocating and deallocating memory. Blocks of\nmemory are allocated with allocGroup  and freed with freeGroup . Every block has a small structure\nassociated with it called a block descriptor ( bdescr ). The operation Bdescr(p)  returns the block\ndescriptor associated with an arbitrary address p ; this is purely an address calculation based on the\nvalue of p  and compiles to a handful of arithmetic and bit-manipulation instructions.\nBlocks may be linked together into chains using the link  field of the bdescr , and this is the real\npower of the technique. The garbage collector needs to manage several distinct areas of memory such as\ngenerations, and each of these areas may need to grow or shrink over time. By representing memory\nareas as linked lists of blocks, the GC is freed from the difficulties of fitting multiple resizable memory\nareas into a flat address space.\nThe implementation of the block layer uses techniques that are well-known from C's malloc()/free()\nAPI; it maintains lists of free blocks of various sizes, and coalesces free areas. The operations\nfreeGroup()  and allocGroup()  are carefully designed to be O(1).\nOne major advantage of this design is that it needs very little support from the OS, and hence is great for\nportability. The block layer needs to allocate memory in units of 1 MB, aligned to a 1 MB boundary. While\nnone of the common OSs provide this functionality directly, it is implementable without much difficulty in\nterms of the facilities they do provide. The payoff is that GHC has no dependence on the particular\ndetails of the address-space layout used by the OS, and it coexists peacefully with other users of the\naddress space, such as shared libraries and operating system threads.\nThere is a small up-front complexity cost for the block layer, in terms of managing chains of blocks rather\nthan contiguous memory. However, we have found that this cost is more than repaid in flexibility and\nportability; for example, the block layer enabled a particularly simple algorithm for parallel GC to be\nimplemented [MHJP08].\nLightweight Threads and Parallelism\nWe consider concurrency to be a vitally important programming abstraction, particularly for building\napplications like web servers that need to interact with large numbers of external agents simultaneously.\nIf concurrency is an important abstraction, then it should not be so expensive that programmers are\nforced to avoid it, or build elaborate infrastructure to amortise its cost (e.g., thread pools). We believe that\nconcurrency should just work, and be cheap enough that you don't worry about forking threads for small\ntasks.\nAll operating systems provide threads that work perfectly well, the problem is that they are far too\nexpensive. Typical OSs struggle to handle thousands of threads, whereas we want to manage threads by\n", "page": 63, "type": "text", "section": "Page 63"}
{"text": "the million.\nGreen threads, otherwise known as lightweight threads or user-space threads, are a well-known\ntechnique for avoiding the overhead of operating system threads. The idea is that threads are managed\nby the program itself, or a library (in our case, the RTS), rather than by the operating system. Managing\nthreads in user space should be cheaper, because fewer traps into the operating system are required.\nIn the GHC RTS we take full advantage of this idea. A context switch only occurs when the thread is at a\nsafe point, where very little additional state needs to be saved. Because we use accurate GC, the stack\nof the thread can be moved and expanded or shrunk on demand. Contrast these with OS threads, where\nevery context switch must save the entire processor state, and where stacks are immovable so a large\nchunk of address space has to be reserved up front for each thread.\nGreen threads can be vastly more efficient than OS threads, so why would anyone want to use OS\nthreads? It comes down to three main problems:\nBlocking and foreign calls. A thread should be able to make a call to an OS API or a foreign library\nthat blocks, without blocking all the other threads in the system.\nParallelism. Threads should automatically run in parallel if there are multiple processor cores on the\nsystem.\nSome external libraries (notably OpenGL and some GUI libraries) have APIs that must be called from\nthe same OS thread each time, because they use thread-local state.\nIt turns out that all of these are difficult to arrange with green threads. Nevertheless, we persevered with\ngreen threads in GHC and found solutions to all three:\nWhen a Haskell thread makes a foreign call, another OS thread takes over the execution of the\nremaining Haskell threads [MPT04]. A small pool of OS threads are maintained for this purpose, and\nnew ones are created on demand.\nGHC's scheduler multiplexes many lightweight Haskell threads onto a few heavyweight OS threads; it\nimplements a transparent M:N threading model. Typically N is chosen to be the same as the number\nof processor cores in the machine, allowing real parallelism to take place but without the overhead of\nhaving a full OS thread for each lightweight Haskell thread.\nIn order to run Haskell code, an OS thread must hold a Capability: a data structure that holds the\nresources required to execute Haskell code, such as the nursery (memory where new objects are\ncreated). Only one OS thread may hold a given Capability at a time. (We have also called it a\n\"Haskell Execution Context\", but the code currently uses the Capability terminology.)\nWe provide an API for creating a bound thread: a Haskell thread that is tied to one specific OS\nthread, such that any foreign calls made by this Haskell thread are guaranteed to be made by that\nOS thread.\nSo in the vast majority of cases, Haskell's threads behave exactly like OS threads: they can make\nblocking OS calls without affecting other threads, and they run in parallel on a multicore machine. But\nthey are orders of magnitude more efficient, in terms of both time and space.\nHaving said that, the implementation does have one problem that users occasionally run into, especially\nwhen running benchmarks. We mentioned above that lightweight threads derive some of their efficiency\nby only context-switching at \"safe points\", points in the code that the compiler designates as safe, where\nthe internal state of the virtual machine (stack, heap, registers, etc.) is in a tidy state and garbage\ncollection could take place. In GHC, a safe point is whenever memory is allocated, which in almost all\nHaskell programs happens regularly enough that the program never executes more than a few tens of\ninstructions without hitting a safe point. However, it is possible in highly optimised code to find loops that\nrun for many iterations without allocating memory. This tends to happen often in benchmarks (e.g.,\nfunctions like factorial and Fibonacci). It occurs less often in real code, although it does happen. The lack\nof safe points prevents the scheduler from running, which can have detrimental effects. It is possible to\nsolve this problem, but not without impacting the performance of these loops, and often people care\nabout saving every cycle in their inner loops. This may just be a compromise we have to live with.\n5.6. Developing GHC\nGHC is a single project with a twenty-year life span, and is still in a ferment of innovation and\ndevelopment. For the most part our infrastructure and tooling has been conventional. For example, we\nuse a bug tracker (Trac), a wiki (also Trac), and Git for revision control. (This revision-control mechanism\nevolved from purely manual, then CVS, then Darcs, before finally moving to Git in 2010.) There are a few\npoints that may be less universal, and we offer them here.\nComments and Notes\nOne of the most serious difficulties in a large, long-lived project is keeping technical documentation up to\ndate. We have no silver bullet, but we offer one low-tech mechanism that has served us particularly well:\nNotes .\nWhen writing code, there is often a moment when a careful programmer will mentally say something like\n\"This data type has an important invariant\". She is faced with two choices, both unsatisfactory. She can\nadd the invariant as a comment, but that can make the data type declaration too long, so that it is hard\nto see what the constructors are. Alternatively, she can document the invariant elsewhere, and risk it\n", "page": 64, "type": "text", "section": "Page 64"}
{"text": "going out of date. Over twenty years, everything goes out of date!\nThus motivated, we developed the following very simple convention:\nComments of any significant size are not interleaved with code, but instead set off by themselves,\nwith a heading in standard form, thus:\n  Note [Equality-constrained types]\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  The type   forall ab. (a ~ [b]) => blah\n  is encoded like this:\n     ForAllTy (a:*) $ ForAllTy (b:*) $\n     FunTy (TyConApp (~) [a, [b]]) $\n     blah\nA the point where the comment is relevant, we add a short comment referring to the Note:\n data Type\n   = FunTy Type Type -- See Note [Equality-constrained types]\n   | ...\nThe comment highlights that something interesting is going on, and gives a precise reference to the\ncomment that explains. It sounds trivial, but the precision is vastly better than our previous habit of\nsaying \"see the comment above\", because it often was not clear which of the many comments above\nwas intended, and after a few years the comment was not even above (it was below, or gone\naltogether).\nNot only is it possible to go from the code that refers to the Note  to the Note  itself, but the reverse is\nalso possible, and that is often useful. Moreover, the same Note  may be referred to from multiple points\nin the code.\nThis simple, ASCII-only technique, with no automated support, has transformed our lives: GHC has\naround 800 Notes , and the number grows daily.\nHow to Keep On Refactoring\nThe code of GHC is churning just as quickly as it was ten years ago, if not more so. There is no doubt\nthat the complexity of the system has increased manyfold over that same time period; we saw measures\nof the amount of code in GHC earlier. Yet, the system remains manageable. We attribute this to three\nmain factors:\nThere's no substitute for good software engineering. Modularity always pays off: making the APIs\nbetween components as small as possible makes the individual components more flexible because\nthey have fewer interdependencies. For example, GHC's Core{}  datatype being small reduces the\ncoupling between Core-to-Core passes, to the extent that they are almost completely independent\nand can be run in arbitrary order.\nDeveloping in a strongly-typed language makes refactoring a breeze. Whenever we need to change\na data type, or change the number of arguments or type of a function, the compiler immediately tells\nus what other places in the code need to be fixed. Simply having an absolute guarantee that a large\nclass of errors have been statically ruled out saves a huge amount of time, especially when\nrefactoring. It is scary to imagine how many hand-written test cases we would need to provide the\nsame level of coverage that the type system provides.\nWhen programming in a purely functional language, it is hard to introduce accidental dependencies\nvia state. If you decide that you suddenly need access to a piece of state deep in an algorithm, in an\nimperative language you might be tempted to just make the state globally visible rather than explicitly\npass it down to the place that needs it. This way eventually leads to a tangle of invisible\ndependencies, and brittle code: code that breaks easily when modified. Pure functional programming\nforces you to make all the dependencies explicit, which exerts some negative pressure on adding\nnew dependencies, and fewer dependencies means greater modularity. Certainly when it is\nnecessary to add a new dependency then purity makes you write more code to express the\ndependency, but in our view it is a worthwhile price to pay for the long-term health of the code base.\nAs an added benefit, purely functional code is thread-safe by construction and tends to be easier to\nparallelise.\nCrime Doesn't Pay\nLooking back over the changes we've had to make to GHC as it has grown, a common lesson emerges:\nbeing less than purely functional, whether for the purposes of efficiency or convenience, tends to have\nnegative consequences down the road. We have a couple of great examples of this:\nGHC uses a few data structures that rely on mutation internally. One is the FastString  type,\nwhich uses a single global hash table; another is a global NameCache  that ensures all external\nnames are assigned a unique number. When we tried to parallelise GHC (that is, make GHC compile\nmultiple modules in parallel on a multicore processor), these data structures based on mutation were\nthe only sticking points. Had we not resorted to mutation in these places, GHC would have been\n", "page": 65, "type": "text", "section": "Page 65"}
{"text": "almost trivial to parallelise.\nIn fact, although we did build a prototype parallel version of GHC, GHC does not currently contain\nsupport for parallel compilation, but that is largely because we have not yet invested the effort\nrequired to make these mutable data structures thread-safe.\nGHC's behaviour is governed to a large extent by command-line flags. These command-line flags\nare by definition constant over a given run of GHC, so in early versions of GHC we made the values\nof these flags available as top-level constants. For example, there was a top-level value\nopt_GlasgowExts  of type Bool , that governed whether certain language extensions should be\nenabled or not. Top-level constants are highly convenient, because their values don't have to be\nexplicitly passed as arguments to all the code that needs access to them.\nOf course these options are not really constants, because they change from run to run, and the\ndefinition of opt_GlasgowExts  involves calling unsafePerformIO  because it hides a side effect.\nNevertheless, this trick is normally considered \"safe enough\" because the value is constant within\nany given run; it doesn't invalidate compiler optimisations, for example.\nHowever, GHC was later extended from a single-module compiler to a multi-module compiler. At this\npoint the trick of using top-level constants for flags broke, because the flags may have different\nvalues when compiling different modules. So we had to refactor large amounts of code to pass\naround the flags explicitly.\nPerhaps you might argue that treating the flags as state in the first place, as would be natural in an\nimperative language, would have sidestepped the problem. To some extent this is true, although\npurely functional code has a number of other benefits, not least of which is that representing the\nflags by an immutable data structure means that the resulting code is already thread-safe and will\nrun in parallel without modification.\nDeveloping the RTS\nGHC's runtime system presents a stark contrast to the compiler in many ways. There is the obvious\ndifference that the runtime system is written in C rather than Haskell, but there are also considerations\nunique to the RTS that give rise to a different design philosophy:\n1. Every Haskell program spends a lot of time executing code in the RTS: 20--30% is typical, but\ncharacteristics of Haskell programs vary a lot and so figures greater or less than this range are also\ncommon. Every cycle saved by optimising the RTS is multiplied many times over, so it is worth\nspending a lot of time and effort to save those cycles.\n2. The runtime system is statically linked into every Haskell program (that is, unless dynamic linking is\nbeing used), so there is an incentive to keep it small.\n3. Bugs in the runtime system are often inscrutable to the user (e.g., \"segmentation fault\") and are hard\nto work around. For example, bugs in the garbage collector tend not to be tied to the use of a\nparticular language feature, but arise when some complex combination of factors emerges at\nruntime. Furthermore, bugs of this kind tend to be non-deterministic (only occurring in some runs),\nand highly sensitive (tiny changes to the program make the bug disappear). Bugs in the\nmultithreaded version of the runtime system present even greater challenges. It is therefore worth\ngoing to extra lengths to prevent these bugs, and also to build infrastructure to make identifying them\neasier.\nThe symptoms of an RTS bug are often indistinguishable from two other kinds of failure: hardware\nfailure, which is more common than you might think, and misuse of unsafe Haskell features like the\nFFI (Foreign Function Interface). The first job in diagnosing a runtime crash is to rule out these two\nother causes.\n4. The RTS is low-level code that runs on several different architectures and operating systems, and is\nregularly ported to new ones. Portability is important.\nEvery cycle and every byte is important, but correctness is even more so. Moreover, the tasks performed\nby the runtime system are inherently complex, so correctness is hard to begin with. Reconciling these\nhas lead us to some interesting defensive techniques, which we describe in the following sections.\nCoping With Complexity\nThe RTS is a complex and hostile programming environment. In contrast to the compiler, the RTS has\nalmost no type safety. In fact, it has even less type safety than most other C programs, because it is\nmanaging data structures whose types live at the Haskell level and not at the C level. For example, the\nRTS has no idea that the object pointed to by the tail of a cons cell is either []  or another cons: this\ninformation is simply not present at the C level. Moreover, the process of compiling Haskell code erases\ntypes, so even if we told the RTS that the tail of a cons cell is a list, it would still have no information\nabout the pointer in the head of the cons cell. So the RTS code has to do a lot of casting of C pointer\ntypes, and it gets very little help in terms of type safety from the C compiler.\nSo our first weapon in this battle is to avoid putting code in the RTS. Wherever possible, we put the\nminimum amount of functionality into the RTS and write the rest in a Haskell library. This has rarely\nturned out badly; Haskell code is far more robust and concise than C, and performance is usually\nperfectly acceptable. Deciding where to draw the line is not an exact science, although in many cases it\nis reasonably clear. For example, while it might be theoretically possible to implement the garbage\ncollector in Haskell, in practice it is extremely difficult because Haskell does not allow the programmer\nprecise control of memory allocation, and so dropping down to C for this kind of low-level task makes\n", "page": 66, "type": "text", "section": "Page 66"}
{"text": "This work is made available under the Creative Commons Attribution 3.0 Unported license. Please see\nthe full description of the license for details.\npractical sense.\nThere is plenty of functionality that can't be (easily) implemented in Haskell, and writing code in the RTS\nis not pleasant. In the next section we focus on one aspect of managing complexity and correctness in\nthe RTS: maintaining invariants.\nInvariants, and Checking Them\nThe RTS is full of invariants. Many of them are trivial and easy to check: for example, if the pointer to the\nhead of a queue is NULL , then the pointer to the tail should also be NULL . The code of the RTS is\nlittered with assertions to check these kinds of things. Assertions are our go-to tool for finding bugs\nbefore they manifest; in fact, when a new invariant is added, we often add the assertion before writing\nthe code that implements the invariant.\nSome of the invariants in the runtime are far more difficult to satisfy, and to check. One invariant of this\nkind that pervades more of the RTS than any other is the following: the heap has no dangling pointers.\nDangling pointers are easy to introduce, and there are many places both in the compiler and the RTS\nitself that can violate this invariant. The code generator could generate code that creates invalid heap\nobjects; the garbage collector might forget to update the pointers of some object when it scans the heap.\nTracking down these kinds of bugs can be extremely time-consuming (it is, however, one of the author's\nfavourite activities!), because by the time the program eventually crashes, execution might have\nprogressed a long way from where the dangling pointer was originally introduced. There are good\ndebugging tools available, but they tend not to be good at executing the program in reverse. (Recent\nversions of GDB and the Microsoft Visual Studio debugger do have some support for reverse execution,\nhowever.)\nThe general principle is: if a program is going to crash, it should crash as soon, as noisily, and as often\nas possible. (This quote comes from the GHC coding style guidelines, and was originally written by\nAlastair Reid, who worked on an early version of the RTS.)\nThe problem is, the no-dangling-pointer invariant is not something that can be checked with a constant-\ntime assertion. The assertion that checks it must do a full traversal of the heap! Clearly we cannot run\nthis assertion after every heap allocation, or every time the GC scans an object (indeed, this would not\neven be enough, as dangling pointers don't appear until the end of GC, when memory is freed).\nSo, the debug RTS has an optional mode that we call sanity checking. Sanity checking enables all kinds\nof expensive assertions, and can make the program run many times more slowly. In particular, sanity\nchecking runs a full scan of the heap to check for dangling pointers (amongst other things), before and\nafter every GC. The first job when investigating a runtime crash is to run the program with sanity\nchecking turned on; sometimes this will catch the invariant violation well before the program actually\ncrashes.\n5.7. Conclusion\nGHC has consumed a significant portion of the authors' lives over the last 20 years, and we are rather\nproud of how far it has come. It is not the only Haskell implementation, but it is the only one in regular\nuse by hundreds of thousands of people to get real work done. We are constantly surprised when Haskell\nturns up being used in unusual places; one recent example is Haskell being used to control the systems\nin a garbage truck.\nFor many, Haskell and GHC are synonymous: it was never intended to be so, and indeed in many ways\nit is counterproductive to have just one implementation of a standard, but the fact is that maintaining a\ngood implementation of a programming language is a lot of work. We hope that our efforts in GHC, to\nsupport the standard and to clearly delimit each separate language extension, will make it feasible for\nmore implementations to emerge and to integrate with the the package system and other infrastructure.\nCompetition is good for everyone!\nWe are deeply indebted to Microsoft in particular for giving us the opportunity to develop GHC as part of\nour research and to distribute it as open source.\nBack to top\nBack to The Architecture of Open Source Applications.\n", "page": 67, "type": "text", "section": "Page 67"}
{"text": "<>\n6.1. Git in a Nutshell\nGit enables the maintenance of a digital body of work (often, but not limited to, code) by many\ncollaborators using a peer-to-peer network of repositories. It supports distributed workflows, allowing a\nbody of work to either eventually converge or temporarily diverge.\nThis chapter will show how various aspects of Git work under the covers to enable this, and how it differs\nfrom other version control systems (VCSs).\n6.2. Git's Origin\nTo understand Git's design philosophy better it is helpful to understand the circumstances in which the\nGit project was started in the Linux Kernel Community.\nThe Linux kernel was unusual, compared to most commercial software projects at that time, because of\nthe large number of committers and the high variance of contributor involvement and knowledge of the\nexisting codebase. The kernel had been maintained via tarballs and patches for years, and the core\ndevelopment community struggled to find a VCS that satisfied most of their needs.\nGit is an open source project that was born out of those needs and frustrations in 2005. At that time the\nLinux kernel codebase was managed across two VCSs, BitKeeper and CVS, by different core\ndevelopers. BitKeeper offered a different view of VCS history lineage than that offered by the popular\nopen source VCSs at this time.\nDays after BitMover, the maker of BitKeeper, announced it would revoke the licenses of some core Linux\nkernel developers, Linus Torvalds began development, in haste, of what was to become Git. He began\nby writing a collection of scripts to help him manage email patches to apply one after the other. The aim\nof this initial collection of scripts was to be able to abort merges quickly so the maintainer could modify\nthe codebase mid-patch-stream to manually merge, then continue merging subsequent patches.\nFrom the outset, Torvalds had one philosophical goal for Git\u2014to be the anti-CVS\u2014plus three usability\ndesign goals:\nSupport distributed workflows similar to those enabled by BitKeeper\nOffer safeguards against content corruption\nOffer high performance\nThese design goals have been accomplished and maintained, to a degree, as I will attempt to show by\ndissecting Git's use of directed acyclic graphs (DAGs) for content storage, reference pointers for heads,\nobject model representation, and remote protocol; and finally how Git tracks the merging of trees.\nDespite BitKeeper influencing the original design of Git, it is implemented in fundamentally different ways\nand allows even more distributed plus local-only workflows, which were not possible with BitKeeper.\nMonotone, an open source distributed VCS started in 2003, was likely another inspiration during Git's\nearly development.\nDistributed version control systems offer great workflow flexibility, often at the expense of simplicity.\nSpecific benefits of a distributed model include:\nProviding the ability for collaborators to work offline and commit incrementally.\nAllowing a collaborator to determine when his/her work is ready to share.\nOffering the collaborator access to the repository history when offline.\nAllowing the managed work to be published to multiple repositories, potentially with different\nbranches or granularity of changes visible.\nAround the time the Git project started, three other open source distributed VCS projects were initiated.\n(One of them, Mercurial, is discussed in Volume 1 of The Architecture of Open Source Applications.) All\nof these dVCS tools offer slightly different ways to enable highly flexible workflows, which centralized\nVCSs before them were not capable of handling directly. Note: Subversion has an extension named SVK\nGit\nSusan Potter\n", "page": 68, "type": "text", "section": "Page 68"}
{"text": "maintained by different developers to support server-to-server synchronization.\nToday popular and actively maintained open source dVCS projects include Bazaar, Darcs, Fossil, Git,\nMercurial, and Veracity.\n6.3. Version Control System Design\nNow is a good time to take a step back and look at the alternative VCS solutions to Git. Understanding\ntheir differences will allow us to explore the architectural choices faced while developing Git.\nA version control system usually has three core functional requirements, namely:\nStoring content\nTracking changes to the content (history including merge metadata)\nDistributing the content and history with collaborators\nNote: The third requirement above is not a functional requirement for all VCSs.\nContent Storage\nThe most common design choices for storing content in the VCS world are with a delta-based\nchangeset, or with directed acyclic graph (DAG) content representation.\nDelta-based changesets encapsulate the differences between two versions of the flattened content, plus\nsome metadata. Representing content as a directed acyclic graph involves objects forming a hierarchy\nwhich mirrors the content's filesystem tree as a snapshot of the commit (reusing the unchanged objects\ninside the tree where possible). Git stores content as a directed acyclic graph using different types of\nobjects. The \"Object Database\" section later in this chapter describes the different types of objects that\ncan form DAGs inside the Git repository.\nCommit and Merge Histories\nOn the history and change-tracking front most VCS software uses one of the following approaches:\nLinear history\nDirected acyclic graph for history\nAgain Git uses a DAG, this time to store its history. Each commit contains metadata about its ancestors;\na commit in Git can have zero or many (theoretically unlimited) parent commits. For example, the first\ncommit in a Git repository would have zero parents, while the result of a three-way merge would have\nthree parents.\nAnother primary difference between Git and Subversion and its linear history ancestors is its ability to\ndirectly support branching that will record most merge history cases.\nFigure 6.1: Example of a DAG representation in Git\nGit enables full branching capability using directed acyclic graphs to store content. The history of a file is\nlinked all the way up its directory structure (via nodes representing directories) to the root directory, which\nis then linked to a commit node. This commit node, in turn, can have one or more parents. This affords\nGit two properties that allow us to reason about history and content in more definite ways than the family\nof VCSs derived from RCS do, namely:\nWhen a content (i.e., file or directory) node in the graph has the same reference identity (the SHA in\nGit) as that in a different commit, the two nodes are guaranteed to contain the same content,\nallowing Git to short-circuit content diffing efficiently.\nWhen merging two branches we are merging the content of two nodes in a DAG. The DAG allows\nGit to \"efficiently\" (as compared to the RCS family of VCS) determine common ancestors.\nDistribution\n", "page": 69, "type": "text", "section": "Page 69"}
{"text": "VCS solutions have handled content distribution of a working copy to collaborators on a project in one of\nthree ways:\nLocal-only: for VCS solutions that do not have the third functional requirement above.\nCentral server: where all changes to the repository must transact via one specific repository for it to\nbe recorded in history at all.\nDistributed model: where there will often be publicly accessible repositories for collaborators to\n\"push\" to, but commits can be made locally and pushed to these public nodes later, allowing offline\nwork.\nTo demonstrate the benefits and limitations of each major design choice, we will consider a Subversion\nrepository and a Git repository (on a server), with equivalent content (i.e., the HEAD of the default\nbranch in the Git repository has the same content as the Subversion repository's latest revision on trunk).\nA developer, named Alex, has a local checkout of the Subversion repository and a local clone of the Git\nrepository.\nLet us say Alex makes a change to a 1 MB file in the local Subversion checkout, then commits the\nchange. Locally, the checkout of the file mimics the latest change and local metadata is updated. During\nAlex's commit in the centralized Subversion repository, a diff is generated between the previous snapshot\nof the files and the new changes, and this diff is stored in the repository.\nContrast this with the way Git works. When Alex makes the same modification to the equivalent file in\nthe local Git clone, the change will be recorded locally first, then Alex can \"push\" the local pending\ncommits to a public repository so the work can be shared with other collaborators on the project. The\ncontent changes are stored identically for each Git repository that the commit exists in. Upon the local\ncommit (the simplest case), the local Git repository will create a new object representing a file for the\nchanged file (with all its content inside). For each directory above the changed file (plus the repository\nroot directory), a new tree object is created with a new identifier. A DAG is created starting from the\nnewly created root tree object pointing to blobs (reusing existing blob references where the files content\nhas not changed in this commit) and referencing the newly created blob in place of that file's previous\nblob object in the previous tree hierarchy. (A blob represents a file stored in the repository.)\nAt this point the commit is still local to the current Git clone on Alex's local device. When Alex \"pushes\"\nthe commit to a publicly accessible Git repository this commit gets sent to that repository. After the public\nrepository verifies that the commit can apply to the branch, the same objects are stored in the public\nrepository as were originally created in the local Git repository.\nThere are a lot more moving parts in the Git scenario, both under the covers and for the user, requiring\nthem to explicitly express intent to share changes with the remote repository separately from tracking the\nchange as a commit locally. However, both levels of added complexity offer the team greater flexibility in\nterms of their workflow and publishing capabilities, as described in the \"Git's Origin\" section above.\nIn the Subversion scenario, the collaborator did not have to remember to push to the public remote\nrepository when ready for others to view the changes made. When a small modification to a larger file is\nsent to the central Subversion repository the delta stored is much more efficient than storing the\ncomplete file contents for each version. However, as we will see later, there is a workaround for this that\nGit takes advantage of in certain scenarios.\n6.4. The Toolkit\nToday the Git ecosystem includes many command-line and UI tools on a number of operating systems\n(including Windows, which was originally barely supported). Most of these tools are mostly built on top of\nthe Git core toolkit.\nDue to the way Git was originally written by Linus, and its inception within the Linux community, it was\nwritten with a toolkit design philosophy very much in the Unix tradition of command line tools.\nThe Git toolkit is divided into two parts: the plumbing and the porcelain. The plumbing consists of low-\nlevel commands that enable basic content tracking and the manipulation of directed acyclic graphs\n(DAG). The porcelain is the smaller subset of git  commands that most Git end users are likely to need\nto use for maintaining repositories and communicating between repositories for collaboration.\nWhile the toolkit design has provided enough commands to offer fine-grained access to functionality for\nmany scripters, application developers complained about the lack of a linkable library for Git. Since the\nGit binary calls die() , it is not reentrant and GUIs, web interfaces or longer running services would\nhave to fork/exec a call to the Git binary, which can be slow.\nWork is being done to improve the situation for application developers; see the \"Current And Future\nWork\" section for more information.\n6.5. The Repository, Index and Working Areas\nLet's get our hands dirty and dive into using Git locally, if only to understand a few fundamental concepts.\nFirst to create a new initialized Git repository on our local filesystem (using a Unix inspired operating\nsystem) we can do:\n  $ mkdir testgit\n", "page": 70, "type": "text", "section": "Page 70"}
{"text": "  $ cd testgit\n  $ git init\nNow we have an empty, but initialized, Git repository sitting in our testgit directory. We can branch,\ncommit, tag and even communicate with other local and remote Git repositories. Even communication\nwith other types of VCS repositories is possible with just a handful of git  commands.\nThe git init  command creates a .git subdirectory inside of testgit. Let's have a peek inside it:\ntree .git/\n.git/\n|-- HEAD\n|-- config\n|-- description\n|-- hooks\n|   |-- applypatch-msg.sample\n|   |-- commit-msg.sample\n|   |-- post-commit.sample\n|   |-- post-receive.sample\n|   |-- post-update.sample\n|   |-- pre-applypatch.sample\n|   |-- pre-commit.sample\n|   |-- pre-rebase.sample\n|   |-- prepare-commit-msg.sample\n|   |-- update.sample\n|-- info\n|   |-- exclude\n|-- objects\n|   |-- info\n|   |-- pack\n|-- refs\n    |-- heads\n    |-- tags\nThe .git  directory above is, by default, a subdirectory of the root working directory, testgit . It\ncontains a few different types of files and directories:\nConfiguration: the .git/config , .git/description  and .git/info/exclude  files essentially\nhelp configure the local repository.\nHooks: the .git/hooks  directory contains scripts that can be run on certain lifecycle events of the\nrepository.\nStaging Area: the .git/index  file (which is not yet present in our tree listing above) will provide a\nstaging area for our working directory.\nObject Database: the .git/objects  directory is the default Git object database, which contains all\ncontent or pointers to local content. All objects are immutable once created.\nReferences: the .git/refs  directory is the default location for storing reference pointers for both\nlocal and remote branches, tags and heads. A reference is a pointer to an object, usually of type\ntag  or commit . References are managed outside of the Object Database to allow the references\nto change where they point to as the repository evolves. Special cases of references may point to\nother references, e.g. HEAD .\nThe .git  directory is the actual repository. The directory that contains the working set of files is the\nworking directory, which is typically the parent of the .git  directory (or repository). If you were creating\na Git remote repository that would not have a working directory, you could initialize it using the git\ninit --bare  command. This would create just the pared-down repository files at the root, instead of\ncreating the repository as a subdirectory under the working tree.\nAnother file of great importance is the Git index: .git/index . It provides the staging area between the\nlocal working directory and the local repository. The index is used to stage specific changes within one\nfile (or more), to be committed all together. Even if you make changes related to various types of\nfeatures, the commits can be made with like changes together, to more logically describe them in the\ncommit message. To selectively stage specific changes in a file or set of files you can using git add -\np .\nThe Git index, by default, is stored as a single file inside the repository directory. The paths to these\nthree areas can be customized using environment variables.\nIt is helpful to understand the interactions that take place between these three areas (the repository,\nindex and working areas) during the execution of a few core Git commands:\ngit checkout [branch]\nThis will move the HEAD reference of the local repository to branch reference path (e.g.\nrefs/heads/master ), populate the index with this head data and refresh the working directory to\n", "page": 71, "type": "text", "section": "Page 71"}
{"text": "represent the tree at that head.\ngit add [files]\nThis will cross reference the checksums of the files specified with the corresponding entries in the\nGit index to see if the index for staged files needs updating with the working directory's version.\nNothing changes in the Git directory (or repository).\nLet us explore what this means more concretely by inspecting the contents of files under the .git\ndirectory (or repository).\n  $ GIT_DIR=$PWD/.git\n  $ cat $GIT_DIR/HEAD\n  ref: refs/heads/master\n  $ MY_CURRENT_BRANCH=$(cat .git/HEAD | sed 's/ref: //g')\n  $ cat $GIT_DIR/$MY_CURRENT_BRANCH\n  cat: .git/refs/heads/master: No such file or directory\nWe get an error because, before making any commits to a Git repository at all, no branches exist except\nthe default branch in Git which is master , whether it exists yet or not.\nNow if we make a new commit, the master branch is created by default for this commit. Let us do this\n(continuing in the same shell, retaining history and context):\n  $ git commit -m \"Initial empty commit\" --allow-empty\n  $ git branch\n  * master\n  $ cat $GIT_DIR/$MY_CURRENT_BRANCH\n  3bce5b130b17b7ce2f98d17b2998e32b1bc29d68\n  $ git cat-file -p $(cat $GIT_DIR/$MY_CURRENT_BRANCH)\nWhat we are starting to see here is the content representation inside Git's object database.\n6.6. The Object Database\nFigure 6.2: Git objects\nGit has four basic primitive objects that every type of content in the local repository is built around. Each\nobject type has the following attributes: type, size and content. The primitive object types are:\nTree: an element in a tree can be another tree or a blob, when representing a content directory.\nBlob: a blob represents a file stored in the repository.\nCommit: a commit points to a tree representing the top-level directory for that commit as well as\nparent commits and standard attributes.\nTag: a tag has a name and points to a commit at the point in the repository history that the tag\nrepresents.\nAll object primitives are referenced by a SHA, a 40-digit object identity, which has the following\n", "page": 72, "type": "text", "section": "Page 72"}
{"text": "properties:\nIf two objects are identical they will have the same SHA.\nif two objects are different they will have different SHAs.\nIf an object was only copied partially or another form of data corruption occurred, recalculating the\nSHA of the current object will identify such corruption.\nThe first two properties of the SHA, relating to identity of the objects, is most useful in enabling Git's\ndistributed model (the second goal of Git). The latter property enables some safeguards against\ncorruption (the third goal of Git).\nDespite the desirable results of using DAG-based storage for content storage and merge histories, for\nmany repositories delta storage will be more space-efficient than using loose DAG objects.\n6.7. Storage and Compression Techniques\nGit tackles the storage space problem by packing objects in a compressed format, using an index file\nwhich points to offsets to locate specific objects in the corresponding packed file.\nFigure 6.3: Diagram of a pack file with corresponding index file\nWe can count the number of loose (or unpacked) objects in the local Git repository using git count-\nobjects . Now we can have Git pack loose objects in the object database, remove loose objects\nalready packed, and find redundant pack files with Git plumbing commands if desired.\nThe pack file format in Git has evolved, with the initial format storing CRC checksums for the pack file\nand index file in the index file itself. However, this meant there was the possibility of undetectable\ncorruption in the compressed data since the repacking phase did not involve any further checks. Version\n2 of the pack file format overcomes this problem by including the CRC checksums of each compressed\nobject in the pack index file. Version 2 also allows packfiles larger than 4 GB, which the initial format did\nnot support. As a way to quickly detect pack file corruption the end of the pack file contains a 20-byte\nSHA1 sum of the ordered list of all the SHAs in that file. The emphasis of the newer pack file format is\non helping fulfill Git's second usability design goal of safeguarding against data corruption.\nFor remote communication Git calculates the commits and content that need to be sent over the wire to\nsynchronize repositories (or just a branch), and generates the pack file format on the fly to send back\nusing the desired protocol of the client.\n6.8. Merge Histories\nAs mentioned previously, Git differs fundamentally in merge history approach than the RCS family of\nVCSs. Subversion, for example, represents file or tree history in a linear progression; whatever has a\nhigher revision number will supercede anything before it. Branching is not supported directly, only\nthrough an unenforced directory structure within the repository.\n", "page": 73, "type": "text", "section": "Page 73"}
{"text": "Figure 6.4: Diagram showing merge history lineage\nLet us first use an example to show how this can be problematic when maintaining multiple branches of a\nwork. Then we will look at a scenario to show its limitations.\nWhen working on a \"branch\" in Subversion at the typical root branches/branch-name , we are working\non directory subtree adjacent to the trunk  (typically where the live or master equivalent code resides\nwithin). Let us say this branch is to represent parallel development of the trunk  tree.\nFor example, we might be rewriting a codebase to use a different database. Part of the way through our\nrewrite we wish to merge in upstream changes from another branch subtree (not trunk). We merge in\nthese changes, manually if necessary, and proceed with our rewrite. Later that day we finish our\ndatabase vendor migration code changes on our branches/branch-name  branch and merge our\nchanges into trunk . The problem with the way linear-history VCSs like Subversion handle this is that\nthere is no way to know that the changesets from the other branch are now contained within the trunk.\nDAG-based merge history VCSs, like Git, handle this case reasonably well. Assuming the other branch\ndoes not contain commits that have not been merged into our database vendor migration branch (say,\ndb-migration  in our Git repository), we can determine\u2014from the commit object parent relationships\u2014\nthat a commit on the db-migration  branch contained the tip (or HEAD) of the other upstream branch.\nNote that a commit object can have zero or more (bounded by only the abilities of the merger) parents.\nTherefore the merge commit on the db-migration  branch knows it merged in the current HEAD of the\ncurrent branch and the HEAD of the other upstream branch through the SHA hashes of the parents. The\nsame is true of the merge commit in the master  (the trunk  equivalent in Git).\nA question that is hard to answer definitively using DAG-based (and linear-based) merge histories is\nwhich commits are contained within each branch. For example, in the above scenario we assumed we\nmerged into each branch all the changes from both branches. This may not be the case.\nFor simpler cases Git has the ability to cherry pick commits from other branches in to the current branch,\nassuming the commit can cleanly be applied to the branch.\n6.9. What's Next?\nAs mentioned previously, Git core as we know it today is based on a toolkit design philosophy from the\nUnix world, which is very handy for scripting but less useful for embedding inside or linking with longer\nrunning applications or services. While there is Git support in many popular Integrated Development\nEnvironments today, adding this support and maintaining it has been more challenging than integrating\nsupport for VCSs that provide an easy-to-link-and-share library for multiple platforms.\nTo combat this, Shawn Pearce (of Google's Open Source Programs Office) spearheaded an effort to\n", "page": 74, "type": "text", "section": "Page 74"}
{"text": "This work is made available under the Creative Commons Attribution 3.0 Unported license. Please see\nthe full description of the license for details.\ncreate a linkable Git library with more permissive licensing that did not inhibit use of the library. This was\ncalled libgit2. It did not find much traction until a student named Vincent Marti chose it for his Google\nSummer of Code project last year. Since then Vincent and Github engineers have continued contributing\nto the libgit2 project, and created bindings for numerous other popular languages such as Ruby, Python,\nPHP, .NET languages, Lua, and Objective-C.\nShawn Pearce also started a BSD-licensed pure Java library called JGit that supports many common\noperations on Git repositories. It is now maintained by the Eclipse Foundation for use in the Eclipse IDE\nGit integration.\nOther interesting and experimental open source endeavours outside of the Git core project are a number\nof implementations using alternative datastores as backends for the Git object database such as:\njgit_cassandra, which offers Git object persistence using Apache Cassandra, a hybrid datastore\nusing Dynamo-style distribution with BigTable column family data model semantics.\njgit_hbase, which enables read and write operations to Git objects stored in HBase, a distributed\nkey-value datastore.\nlibgit2-backends, which emerged from the libgit2 effort to create Git object database backends for\nmultiple popular datastores such as Memcached, Redis, SQLite, and MySQL.\nAll of these open source projects are maintained independently of the Git core project.\nAs you can see, today there are a large number of ways to use the Git format. The face of Git is no\nlonger just the toolkit command line interface of the Git Core project; rather it is the repository format and\nprotocol to share between repositories.\nAs of this writing, most of these projects, according to their developers, have not reached a stable\nrelease, so work in the area still needs to be done but the future of Git appears bright.\n6.10. Lessons Learned\nIn software, every design decision is ultimately a trade-off. As a power user of Git for version control and\nas someone who has developed software around the Git object database model, I have a deep fondness\nfor Git in its present form. Therefore, these lessons learned are more of a reflection of common recurring\ncomplaints about Git that are due to design decisions and focus of the Git core developers.\nOne of the most common complaints by developers and managers who evaluate Git has been the lack of\nIDE integration on par with other VCS tools. The toolkit design of Git has made this more challenging\nthan integrating other modern VCS tools into IDEs and related tools.\nEarlier in Git's history some of the commands were implemented as shell scripts. These shell script\ncommand implementations made Git less portable, especially to Windows. I am sure the Git core\ndevelopers did not lose sleep over this fact, but it has negatively impacted adoption of Git in larger\norganizations due to portability issues that were prevalent in the early days of Git's development. Today\na project named Git for Windows has been started by volunteers to ensure new versions of Git are ported\nto Windows in a timely manner.\nAn indirect consequence of designing Git around a toolkit design with a lot of plumbing commands is that\nnew users get lost quickly; from confusion about all the available subcommands to not being able to\nunderstand error messages because a low level plumbing task failed, there are many places for new\nusers to go astray. This has made adopting Git harder for some developer teams.\nEven with these complaints about Git, I am excited about the possibilities of future development on the\nGit Core project, plus all the related open source projects that have been launched from it.\nBack to top\nBack to The Architecture of Open Source Applications.\n", "page": 75, "type": "text", "section": "Page 75"}
{"text": "<>\nGPSD is a suite of tools for managing collections of GPS devices and other sensors related to navigation\nand precision timekeeping, including marine AIS (Automatic Identification System) radios and digital\ncompasses. The main program, a service daemon named gpsd , manages a collection of sensors and\nmakes reports from all of them available as a JSON object stream on a well-known TCP/IP port. Other\nprograms in the suite include demonstration clients usable as code models and various diagnostic tools.\nGPSD is widely deployed on laptops, smartphones, and autonomous vehicles including self-driving\nautomobiles and robot submarines. It features in embedded systems used for navigation, precision\nagriculture, location-sensitive scientific telemetry, and network time service. It's even used in the\nIdentification-Friend-or-Foe system of armored fighting vehicles including the M1 \"Abrams\"main battle\ntank.\nGPSD is a mid-sized project\u2014about 43 KLOC, mainly in C and Python\u2014with a history under its current\nlead going back to 2005 and a prehistory going back to 1997. The core team has been stable at about\nthree developers, with semi-regular contributions from about two dozen more and the usual one-off\npatches from hundreds of others.\nGPSD has historically had an exceptionally low defect rate, as measured both by auditing tools such as\nsplint , valgrind , and Coverity and by the incidence of bug reports on its tracker and elsewhere.\nThis did not come about by accident; the project has been very aggressive about incorporating\ntechnology for automated testing, and that effort has paid off handsomely.\nGPSD is sufficiently good at what it does that it has coopted or effectively wiped out all of its approximate\npredecessors and at least one direct attempt to compete with it. In 2010, GPSD won the first Good Code\nGrant from the Alliance for Code Excellence. By the time you finish this chapter you should understand\nwhy.\n7.1. Why GPSD Exists\nGPSD exists because the application protocols shipped with GPSs and other navigation-related sensors\nare badly designed, poorly documented, and highly variable by sensor type and model. See [Ray] for a\ndetailed discussion; in particular, you'll learn there about the vagaries of NMEA 0183 (the sort-of\nstandard for GPS reporting packets) and the messy pile of poorly documented vendor protocols that\ncompete with it.\nIf applications had to handle all this complexity themselves the result would be huge amounts of brittle\nand duplicative code, leading to high rates of user-visible defects and constant problems as hardware\ngradually mutated out from under the applications.\nGPSD isolates location-aware applications from hardware interface details by knowing about all the\nprotocols itself (at time of writing we support about 20 different ones), managing serial and USB devices\nso the applications don't have to, and reporting sensor payload information in a simple device-\nindependent JSON format. GPSD further simplifies life by providing client libraries so client applications\nneed not even know about that reporting format. Instead, getting sensor information becomes a simple\nprocedure call.\nGPSD also supports precision timekeeping; it can act as a time source for ntpd  (the Network Time\nProtocol Daemon) if any of its attached sensors have PPS (pulse-per-second) capability. The GPSD\ndevelopers cooperate closely with the ntpd  project in improving the network time service.\nWe are presently (mid-2011) working on completing support for the AIS network of marine navigational\nreceivers. In the future, we expect to support new kinds of location-aware sensors\u2014such as receivers for\nsecond-generation aircraft transponders\u2014as protocol documentation and test devices become available.\nTo sum up, the single most important theme in GPSD's design is hiding all the device-dependent\nugliness behind a simple client interface talking to a zero-configuration service.\n7.2. The External View\nGPSD\nEric Raymond\n", "page": 76, "type": "text", "section": "Page 76"}
{"text": "The main program in the GPSD suite is the gpsd  service daemon. It can collect the take from a set of\nattached sensor devices over RS232, USB, Bluetooth, TCP/IP, and UDP links. Reports are normally\nshipped to TCP/IP port 2947, but can also go out via a shared-memory or D-BUS interface.\nThe GPSD distribution ships with client libraries for C, C++, and Python. It includes sample clients in C,\nC++, Python, and PHP. A Perl client binding is available via CPAN. These client libraries are not merely\na convenience for application developers; they save GPSD's developers headaches too, by isolating\napplications from the details of GPSD's JSON reporting protocol. Thus, the API exposed to clients can\nremain the same even as the protocol grows new features for new sensor types.\nOther programs in the suite include a utility for low-level device monitoring ( gpsmon ), a profiler that\nproduces reports on error statistics and device timing ( gpsprof ), a utility for tweaking device settings\n( gpsctl ), and a program for batch-converting sensor logs into readable JSON ( gpsdecode ).\nTogether, they help technically savvy users look as deeply into the operation of the attached sensors as\nthey care to.\nOf course, these tools also help GPSD's own developers verify the correct operation of gpsd . The\nsingle most important test tool is gpsfake , a test harness for gpsd which can connect it to any number\nof sensor logs as though they were live devices. With gpsfake , we can re-run a sensor log shipped\nwith a bug report to reproduce specific problems. gpsfake  is also the engine of our extensive\nregression-test suite, which lowers the cost of modifying the software by making it easy to spot changes\nthat break things.\nOne of the most important lessons we think we have for future projects is that it is not enough for a\nsoftware suite to be correct, it should also be able to demonstrate its own correctness. We have found\nthat when this goal is pursued properly it is not a hair shirt but rather a pair of wings\u2014the time we've\ntake to write test harnesses and regression tests has paid for itself many times over in the freedom it\ngives us to modify code without fearing that we are wreaking subtle havoc on existing functionality.\n7.3. The Software Layers\nThere is a lot more going on inside GPSD than the \"plug a sensor in and it just works\" experience might\nlead people to assume. gpsd 's internals break naturally into four pieces: the drivers, the packet sniffer,\nthe core library and the multiplexer. We'll describe these from the bottom up.\nFigure 7.1: Software layers\nThe drivers are essentially user-space device drivers for each kind of sensor chipset we support. The\nkey entry points are methods to parse a data packet into time-position-velocity or status information,\nchange its mode or baud rate, probe for device subtype, etc. Auxiliary methods may support driver\ncontrol operations, such as changing the serial speed of the device. The entire interface to a driver is a\nC structure full of data and method pointers, deliberately modeled on a Unix device driver structure.\nThe packet sniffer is responsible for mining data packets out of serial input streams. It's basically a state\nmachine that watches for anything that looks like one of our 20 or so known packet types (most of which\nare checksummed, so we can have high confidence when we think we have identified one). Because\ndevices can hotplug or change modes, the type of packet that will come up the wire from a serial or USB\nport isn't necessarily fixed forever by the first one recognized.\nThe core library manages a session with a sensor device. The key entry points are:\nstarting a session by opening the device and reading data from it, hunting through baud rates and\n", "page": 77, "type": "text", "section": "Page 77"}
{"text": "parity/stopbit combinations until the packet sniffer achieves synchronization lock with a known packet\ntype;\npolling the device for a packet; and\nclosing the device and wrapping up the session.\nA key feature of the core library is that it is responsible for switching each GPS connection to using the\ncorrect device driver depending on the packet type that the sniffer returns. This is not configured in\nadvance and may change over time, notably if the device switches between different reporting protocols.\n(Most GPS chipsets support NMEA and one or more vendor binary protocols, and devices like AIS\nreceivers may report packets in two different protocols on the same wire.)\nFinally, the multiplexer is the part of the daemon that handles client sessions and device assignment. It is\nresponsible for passing reports up to clients, accepting client commands, and responding to hotplug\nnotifications. It is essentially all contained in one source file, gpsd.c , and never talks to the device\ndrivers directly.\nThe first three components (other than the multiplexer) are linked together in a library called libgpsd\nand can be used separately from the multiplexer. Our other tools that talk to sensors directly, such as\ngpsmon  and gpsctl , do it by calling into the core library and driver layer directly.\nThe most complex single component is the packet sniffer at about two thousand lines of code. This is\nirreducible; a state machine that can recognize as many different protocols as it does is bound to be\nlarge and gnarly. Fortunately, the packet sniffer is also easy to isolate and test; problems in it do not tend\nto be coupled to other parts of the code.\nThe multiplexer layer is about same size, but somewhat less gnarly. The device drivers make up the bulk\nof the daemon code at around 15 KLOC. All the rest of the code\u2014all the support tools and libraries and\ntest clients together\u2014adds up to about the size of the daemon (some code, notably the JSON parser, is\nshared between the daemon and the client libraries).\nThe success of this layering approach is demonstrated in a couple of different ways. One is that new\ndevice drivers are so easy to write that several have been contributed by people not on the core team:\nthe driver API is documented, and the individual drivers are coupled to the core library only via pointers\nin a master device types table.\nAnother benefit is that system integrators can drastically reduce GPSD's footprint for embedded\ndeployment simply by electing not to compile in unused drivers. The daemon is not large to begin with,\nand a suitably stripped-down build runs quite happily on low-power, low-speed, small-memory ARM\ndevices. (ARM is a 32-bit RISC instruction set architecture used in mobile and embedded electronics.\nSee http://en.wikipedia.org/wiki/ARM_architecture.)\nA third benefit of the layering is that the daemon multiplexer can be detached from atop the core library\nand replaced with simpler logic, such as the straight batch conversion of sensor logfiles to JSON reports\nthat the gpsdecode  utility does.\nThere is nothing novel about this part of the GPSD architecture. Its lesson is that conscious and rigorous\napplication of the design pattern of Unix device handling is beneficial not just in OS kernels but also in\nuserspace programs that are similarly required to deal with varied hardware and protocols.\n7.4. The Dataflow View\nNow we'll consider GPSD's architecture from a dataflow view. In normal operation, gpsd spins in a loop\nwaiting for input from one of these sources:\n1. A set of clients making requests over a TCP/IP port.\n2. A set of navigation sensors connected via serial or USB devices.\n3. The special control socket used by hotplug scripts and some configuration tools.\n4. A set of servers issuing periodic differential-GPS correction updates (DGPS and NTRIP). These are\nhandled as though they are navigation sensors.\nWhen a USB port goes active with a device that might be a navigation sensor, a hotplug script (shipped\nwith GPSD) sends a notification to the control socket. This is the cue for the multiplexer layer to put the\ndevice on its internal list of sensors. Conversely, a device-removal event can remove a device from that\nlist.\nWhen a client issues a watch request, the multiplexer layer opens the navigation sensors in its list and\nbegins accepting data from them (by adding their file descriptors to the set in the main select call).\nOtherwise all GPS devices are closed (but remain in the list) and the daemon is quiescent. Devices that\nstop sending data get timed out of the device list.\nFigure 7.2: Dataflow\nWhen data comes in from a navigation sensor, it's fed to the packet sniffer, a finite-state machine that\nworks like the lexical analyzer in a compiler. The packet sniffer's job is to accumulate data from each\nport (separately), recognizing when it has accumulated a packet of a known type.\n", "page": 78, "type": "text", "section": "Page 78"}
{"text": "A packet may contain a position fix from a GPS, a marine AIS datagram, a sensor reading from a\nmagnetic compass, a DGPS (Differential GPS) broadcast packet, or any of several other things. The\npacket sniffer doesn't care about the content of the packet; all it does is tell the core library when it has\naccumulated one and pass back the payload and the packet type.\nThe core library then hands the packet to the driver associated with its type. The driver's job is to mine\ndata out of the packet payload into a per-device session structure and set some status bits telling the\nmultiplexer layer what kind data it got.\nOne of those bits is an indication that the daemon has accumulated enough data to ship a report to its\nclients. When this bit is raised after a data read from a sensor device, it means we've seen the end of a\npacket, the end of a packet group (which may be one or more packets), and the data in the device's\nsession structure should be passed to one of the exporters.\nThe main exporter is the \"socket\" one; it generates a report object in JSON and ships it to all the clients\nwatching the device. There's a shared-memory exporter that copies the data to a shared-memory\nsegment instead. In either of these cases, it is expected that a client library will unmarshal the data into a\nstructure in the client program's memory space. A third exporter, which ships position updates via DBUS,\nis also available.\nThe GPSD code is as carefully partitioned horizontally as it vertically. The packet sniffer neither knows\nnor needs to know anything about packet payloads, and doesn't care whether its input source is a USB\nport, an RS232 device, a Bluetooth radio link, a pseudo-tty, a TCP socket connection, or a UDP packet\nstream. The drivers know how to analyze packet payloads, but know nothing about either the packet-\nsniffer internals nor the exporters. The exporters look only at the session data structure updated by the\ndrivers.\nThis separation of function has served GPSD very well. For example, when we got a request in early\n2010 to adapt the code to accept sensor data coming in as UDP packets for the on-board navigation\nsystem of a robot submarine, it was easy to implement that in a handful of lines of code without\ndisturbing later stages in the data pipeline.\nMore generally, careful layering and modularization has made it relatively easy to add new sensor types.\nWe incorporate new drivers every six months or so; some have been written by people who are not core\ndevelopers.\n7.5. Defending the Architecture\nAs an open source program like gpsd  evolves, one of the recurring themes is that each contributor will\ndo things to solve his or her particular problem case which gradually leak more information between\nlayers or stages that were originally designed with clean separation.\nOne that we're concerned about at the time of writing is that some information about input source type\n(USB, RS232, pty, Bluetooth, TCP, UDP) seems to need to be passed up to the multiplexer layer, to tell\nit, for example, whether probe strings should be sent to an unidentified device. Such probes are\nsometimes required to wake up RS232C sensors, but there are good reasons not to ship them to any\nmore devices than we have to. Many GPSs and other sensor devices are designed on low budgets and\nin a hurry; some can be confused to the point of catatonia by unexpected control strings.\nFor a similar reason, the daemon has a -b  option that prevents it from attempting baud-rate changes\nduring the packet-sniffer hunt loop. Some poorly made Bluetooth devices handle these so poorly that\nthey have to be power-cycled to function again; in one extreme case a user actually had to unsolder the\nbackup battery to unwedge his!\nBoth these cases are necessary exceptions to the project's design rules. Much more usually, though,\nsuch exceptions are a bad thing. For example, we've had some patches contributed to make PPS time\nservice work better that messed up the vertical layering, making it impossible for PPS to work properly\nwith more than the one driver they were intended to help. We rejected these in favor of working harder at\ndevice-type-independent improvement.\nOn one occasion some years ago, we had a request to support a GPS with the odd property that the\nchecksums in its NMEA packets may be invalid when the device doesn't have a location fix. To support\nthis device, we would have had to either (a) give up on validating the checksum on any incoming data\nthat looked like an NMEA packet, risking that the packet-sniffer would hand garbage to the NMEA driver,\nor (b) add a command-line option to force the sensor type.\nThe project lead (the author of this chapter) refused to do either. Giving up on NMEA packet validation\nwas an obvious bad idea. But a switch to force the sensor type would have been an invitation to get lazy\nabout proper autoconfiguration, which would cause problems all the way up to GPSD's client applications\nand their users. The next step down that road paved with good intentions would surely have been a\nbaud-rate switch. Instead, we declined to support this broken device.\nOne of the most important duties of a project's lead architect is to defend the architecture against\nexpedient \"fixes\" that would break it and cause functional problems or severe maintenance headaches\ndown the road. Arguments over this can get quite heated, especially when defending architecture\nconflicts against something that a developer or user considers a must-have feature. But these arguments\nare necessary, because the easiest choice is often the wrong one for the longer term.\n", "page": 79, "type": "text", "section": "Page 79"}
{"text": "7.6. Zero Configuration, Zero Hassles\nAn extremely important feature of gpsd  is that it is a zero-configuration service (with one minor\nexception for Bluetooth devices with broken firmware). It has no dotfile! The daemon deduces the sensor\ntypes it's talking to by sniffing the incoming data. For RS232 and USB devices gpsd  even autobauds\n(that is, automatically detects the serial line speed), so it is not necessary for the daemon to know in\nadvance the speed/parity/stopbits at which the sensor is shipping information.\nWhen the host operating system has a hotplug capability, hotplug scripts can ship device-activation and\ndeactivation messages to a control socket to notify the daemon of the change in its environment. The\nGPSD distribution supplies these scripts for Linux. The result is that end users can plug a USB GPS into\ntheir laptop and expect it to immediately begin supplying reports that location-aware applications can read\n\u2014no muss, no fuss, and no editing a dotfile or preferences registry.\nThe benefits of this ripple all the way up the application stack. Among other things, it means that\nlocation-aware applications don't have to have a configuration panel dedicated to tweaking the GPS and\nport settings until the whole mess works. This saves a lot of effort for application writers as well as users:\nthey get to treat location as a service that is nearly as simple as the system clock.\nOne consequence of the zero-configuration philosophy is that we do not look favorably on proposals to\nadd a config file or additional command-line options. The trouble with this is that configuration which can\nbe edited, must be edited. This implies adding setup hassle for end users, which is precisely what a\nwell-designed service daemon should avoid.\nThe GPSD developers are Unix hackers working from deep inside the Unix tradition, in which\nconfigurability and having lots of knobs is close to being a religion. Nevertheless, we think open source\nprojects could be trying a lot harder to throw away their dotfiles and autoconfigure to what the running\nenvironment is actually doing.\n7.7. Embedded Constraints Considered Helpful\nDesigning for embedded deployment has been a major goal of GPSD since 2005. This was originally\nbecause we got a lot of interest from system integrators working with single-board computers, but it has\nsince paid off in an unexpected way: deployment on GPS-enabled smartphones. (Our very favorite\nembedded-deployment reports are still the ones from the robot submarines, though.)\nDesigning for embedded deployment has influenced GPSD in important ways. We think a lot about ways\nto keep memory footprint and CPU usage low so the code will run well on low-speed, small-memory,\npower-constrained systems.\nOne important attack on this issue, as previously mentioned, is to ensure that gpsd  builds don't have to\ncarry any deadweight over the specific set of sensor protocols that a system integrator needs to support.\nIn June 2011 a minimum static build of gpsd  on an x86 system has a memory footprint of about 69K\n(that is with all required standard C libraries linked in) on 64-bit x86. For comparison, the static build with\nall drivers is about 418K.\nAnother is that we profile for CPU hotspots with a slightly different emphasis than most projects. Because\nlocation sensors tend to report only small amounts of data at intervals on the order of 1 second,\nperformance in the normal sense isn't a GPSD issue\u2014even grossly inefficient code would be unlikely to\nintroduce enough latency to be visible at the application level. Instead, our focus is on decreasing\nprocessor usage and power consumption. We've been quite successful at this: even on low-power ARM\nsystems without an FPU, gpsd 's fraction of CPU is down around the level of profiler noise.\nWhile designing the core code for low footprint and good power efficiency is at this point largely a solved\nproblem, there is one respect in which targeting embedded deployments still produces tension in the\nGPSD architecture: use of scripting languages. On the one hand, we want to minimize defects due to\nlow-level resource management by moving as much code as possible out of C. On the other hand,\nPython (our preferred scripting language) is simply too heavyweight and slow for most embedded\ndeployments.\nWe've split the difference in the obvious way: the gpsd  service daemon is C, while the test framework\nand several of the support utilities are written in Python. Over time, we hope to migrate more of the\nauxiliary code out of C and into Python, but embedded deployment makes those choices a continuing\nsource of controversy and discomfort.\nStill, on the whole we find the pressures from embedded deployment quite bracing. It feels good to write\ncode that is lean, tight, and sparing of processor resources. It has been said that art comes from\ncreativity under constraints; to the extent that's true, GPSD is better art for the pressure.\nThat feeling doesn't translate directly into advice for other projects, but something else definitely does:\ndon't guess, measure! There is nothing like regular profiling and footprint measurements to warn you\nwhen you're straying into committing bloat\u2014and to reassure you that you're not.\n7.8. JSON and the Architecturenauts\nOne of the most significant transitions in the history of the project was when we switched over from the\noriginal reporting protocol to using JSON as a metaprotocol and passing reports up to clients as JSON\n", "page": 80, "type": "text", "section": "Page 80"}
{"text": "objects. The original protocol had used one-letter keys for commands and responses, and we literally ran\nout of keyspace as the daemon's capabilities gradually increased.\nSwitching to JSON was a big, big win. JSON combines the traditional Unix virtues of a purely textual\nformat\u2014easy to examine with a Mark 1 Eyeball, easy to edit with standard tools, easy to generate\nprogrammatically\u2014with the ability to pass structured information in rich and flexible ways.\nBy mapping report types to JSON objects, we ensured that any report could contain mixes of string,\nnumeric, and Boolean data with structure (a capability the old protocol lacked). By identifying report types\nwith a \"class\"  attribute, we guaranteed that we would always be able to add new report types without\nstepping on old ones.\nThis decision was not without cost. A JSON parser is a bit more computationally expensive than the very\nsimple and limited parser it replaced, and certainly requires more lines of code (implying more places for\ndefects to occur). Also, conventional JSON parsers require dynamic storage allocation in order to cope\nwith the variable-length arrays and dictionaries that JSON describes, and dynamic storage allocation is a\nnotorious defect attractor.\nWe coped with these problems in several ways. The first step was to write a C parser for a (sufficiently)\nlarge subset of JSON that uses entirely static storage. This required accepting some minor restrictions;\nfor example, objects in our dialect cannot contain the JSON null  value, and arrays always have a\nfixed maximum length. Accepting these restrictions allowed us to fit the parser into 600 lines of C.\nWe then built a comprehensive set of unit tests for the parser in order to verify error-free operation.\nFinally, for very tight embedded deployments where the overhead of JSON might be too high, we wrote a\nshared-memory exporter that bypasses the need to ship and parse JSON entirely if the daemon and its\nclient have access to common memory.\nJSON isn't just for web applications anymore. We think anyone designing an application protocol should\nconsider an approach like GPSD's. Of course the idea of building your protocol on top of a standard\nmetaprotocol is not new; XML fans have been pushing it for many years, and that makes sense for\nprotocols with a document-like structure. JSON has the advantages of being lower-overhead than XML\nand better fitted to passing around array and record structures.\n7.9. Designing for Zero Defects\nBecause of its use in navigational systems, any software that lives between the user and a GPS or other\nlocation sensor is potentially life-critical, especially at sea or when airborne. Open source navigation\nsoftware has a tendency to try to evade this problem by shipping with disclaimers that say, \"Don't rely on\nthis if doing so might put lives at risk.\"\nWe think such disclaimers are futile and dangerous: futile because system integrators are quite likely to\ntreat them as pro-forma and ignore them, and dangerous because they encourage developers to fool\nthemselves that code defects won't have serious consequences, and that cutting corners in quality\nassurance is acceptable.\nThe GPSD project developers believe that the only acceptable policy is to design for zero defects.\nSoftware complexity being what it is, we have not quite achieved this\u2014but for a project GPSD's size and\nage and complexity we come very close.\nOur strategy for doing this is a combination of architecture and coding policies that aim to exclude the\npossibility of defects in shipped code.\nOne important policy is this: the gpsd  daemon never uses dynamic storage allocation\u2014no malloc  or\ncalloc , and no calls to any functions or libraries that require it. At a stroke this banishes the single\nmost notorious defect attractor in C coding. We have no memory leaks and no double-malloc or double-\nfree bugs, and we never will.\nWe get away with this because all of the sensors we handle emit packets with relatively small fixed\nmaximum lengths, and the daemon's job is to digest them and ship them to clients with minimal buffering.\nStill, banishing malloc  requires coding discipline and some design compromises, a few of which we\npreviously noted in discussing the JSON parser. We pay these costs willingly to reduce our defect rate.\nA useful side effect of this policy is that it increases the effectiveness of static code checkers such as\nsplint , cppcheck , and Coverity. This feeds into another major policy choice; we make extremely\nheavy use of both these code-auditing tools and a custom framework for regression testing. (We do not\nknow of any program suite larger than GPSD that is fully splint -annotated, and strongly suspect that\nnone such yet exist.)\nThe highly modular architecture of GPSD aids us here as well. The module boundaries serve as cut\npoints where we can rig test harnesses, and we have very systematically done so. Our normal regression\ntest checks everything from the floating-point behavior of the host hardware up through JSON parsing to\ncorrect reporting behavior on over seventy different sensor logs.\nAdmittedly, we have a slightly easier time being rigorous than many applications would because the\ndaemon has no user-facing interfaces; the environment around it is just a bunch of serial data streams\nand is relatively easy to simulate. Still, as with banishing malloc , actually exploiting that advantage\nrequires the right attitude, which very specifically means being willing to spend as much design and\n", "page": 81, "type": "text", "section": "Page 81"}
{"text": "coding time on test tools and harnesses as we do on the production code. This is a policy we think other\nopen-source projects can and should emulate.\nAs I write (July 2011), GPSD's project bug tracker is empty. It has been empty for weeks, and based on\npast rates of bug submissions we can expect it to stay that way for a good many more. We haven't\nshipped code with a crash bug in six years. When we do have bugs, they tend to be the sort of minor\nmissing feature or mismatch with specification that is readily fixed in a few minutes of work.\nThis is not to say that the project has been an uninterrupted idyll. Next, we'll review some of our\nmistakes\u2026\n7.10. Lessons Learned\nSoftware design is difficult; mistakes and blind alleys are all too normal a part of it, and GPSD has been\nno exception to that rule. The largest mistake in this project's history was the design of the original pre-\nJSON protocol for requesting and reporting GPS information. Recovering from it took years of effort, and\nthere are lessons in both the original mis-design and the recovery.\nThere were two serious problems with the original protocol:\n1. Poor extensibility. It used requests and response tags consisting of a single letter each, case-\ninsensitive. Thus, for example, the request to report longitude and latitude was \"P\"  and a response\nlooked like \"P -75.32 40.05\" . Furthermore, the parser interpreted a request like \"PA\"  as a\n\"P\"  request followed by an \"A\"  (altitude) request. As the daemon's capabilities gradually\nbroadened, we literally ran out of command space.\n2. A mismatch between the protocol's implicit model of sensor behavior and how they actually behave.\nThe old protocol was request/response: send a request for position (or altitude, or whatever) get\nback a report sometime later. In reality, it is usually not possible to request a report from a GPS or\nother navigation-related sensors; they stream out reports, and the best a request can do is query a\ncache. This mismatch encouraged sloppy data-handling from applications; too often, they would ask\nfor location data without also requesting a timestamp or any check information about the fix quality, a\npractice which could easily result in stale or invalid data getting presented to the user.\nIt became clear as early as 2006 that the old protocol design was inadequate, but it took nearly three\nyears of design sketches and false starts to design a new one. The transition took two years after that,\nand caused some pain for developers of client applications. It would have cost a lot more if the project\nhad not shipped client-side libraries that insulated users from most of the protocol details\u2014but we didn't\nget the API of those libraries quite right either at first.\nIf we had known then what we know now, the JSON-based protocol would have been introduced five\nyears sooner, and the API design of the client libraries would have required many fewer revisions. But\nthere are some kinds of lessons only experience and experiment can teach.\nThere are at least two design guidelines that future service daemons could bear in mind to avoid\nreplicating our mistakes:\n1. Design for extensibility. If your daemon's application protocol can run out of namespace the way our\nold one did, you're doing it wrong. Overestimating the short-term costs and underestimating the long-\nterm benefits of metaprotocols like XML and JSON is an error that's all too common.\n2. Client-side libraries are a better idea than exposing the application protocol details. A library may be\nable to adapt its internals to multiple versions of the application protocol, substantially reducing both\ninterface complexity and defect rates compared to the alternative, in which each application writer\nneeds to develop an ad hoc binding. This difference will translate directly into fewer bug reports on\nyour project's tracker.\nOne possible reply to our emphasis on extensibility, not just in GPSD's application protocol but in other\naspects of the project architecture like the packet-driver interface, is to dismiss it as an over-elaboration\nbrought about by mission creep. Unix programmers schooled in the tradition of \"do one thing well\" may\nask whether gpsd 's command set really needs to be larger in 2011 than it was in 2006, why gpsd\nnow handles non-GPS sensors like magnetic compasses and Marine AIS receivers, and why we\ncontemplate possibilities like ADS-B aircraft tracking.\nThese are fair questions. We can approach an answer by looking at the actual complexity cost of adding\na new device type. For very good reasons, including relatively low data volumes and the high electrical-\nnoise levels historically associated with serial wires to sensors, almost all reporting protocols for GPSs\nand other navigation-related sensors look broadly similar: small packets with a validation checksum of\nsome sort. Such protocols are fiddly to handle but not really difficult to distinguish from each other and\nparse, and the incremental cost of adding a new one tends to be less than a KLOC each. Even the most\ncomplex of our supported protocols with their own report generators attached, such as Marine AIS, only\ncost on the order of 3 KLOC each. In aggregate, the drivers plus the packet-sniffer and their associated\nJSON report generators are about 18 KLOC total.\nComparing this with 43 KLOC for the project as a whole, we see that most of the complexity cost of\nGPSD is actually in the framework code around the drivers\u2014and (importantly) in the test tools and\nframework for verifying the daemon's correctness. Duplicating these would be a much larger project than\nwriting any individual packet parser. So writing a GPSD-equivalent for a packet protocol that GPSD\ndoesn't handle would be a great deal more work than adding another driver and test set to GPSD itself.\n", "page": 82, "type": "text", "section": "Page 82"}
{"text": "This work is made available under the Creative Commons Attribution 3.0 Unported license. Please see\nthe full description of the license for details.\nConversely, the most economical outcome (and the one with the lowest expected cumulative rate of\ndefects) is for GPSD to grow packet drivers for many different sensor types.\nThe \"one thing\" that GPSD has evolved to do well is handle any collection of sensors that ship\ndistinguishable checksummed packets. What looks like mission creep is actually preventing many\ndifferent and duplicative handler daemons from having to be written. Instead, application developers get\none relatively simple API and the benefit of our hard-won expertise at design and testing across an\nincreasing range of sensor types.\nWhat distinguishes GPSD from a mere mission-creepy pile of features is not luck or black magic but\ncareful application of known best practices in software engineering. The payoff from these begins with a\nlow defect rate in the present, and continues with the ability to support new features with little effort or\nexpected impact on defect rates in the future.\nPerhaps the most important lesson we have for other open-source projects is this: reducing defect rates\nasymptotically close to zero is difficult, but it's not impossible\u2014not even for a project as widely and\nvariously deployed as GPSD is. Sound architecture, good coding practice, and a really determined focus\non testing can achieve it\u2014and the most important prerequisite is the discipline to pursue all three.\nBack to top\nBack to The Architecture of Open Source Applications.\n", "page": 83, "type": "text", "section": "Page 83"}
{"text": "<>\nThe Iron languages are an informal group of language implementations with \"Iron\" in their names, in\nhonour of the first one, IronPython. All of these languages have at least one thing in common\u2014they are\ndynamic languages that target the Common Language Runtime (CLR), which is more commonly known\nas the .NET Framework, and they are built on top of the Dynamic Language Runtime (DLR). (\"CLR\" is\nthe generic term; the .NET Framework is Microsoft's implementation, and there is also the open-source\nMono implementation.) The DLR is a set of libraries for the CLR that provide much better support for\ndynamic languages on the CLR. IronPython and IronRuby are both used in a few dozen closed and open\nsource projects, and are both under active development; the DLR, which started as an open-source\nproject, is included as part of the .NET Framework and Mono.\nArchitecturally, IronPython, IronRuby, and the DLR are both simple and devilishly complex. From a high\nlevel, the designs are similar to many other language implementations, with parsers and compilers and\ncode generators; however, look a little closer and the interesting details begin to emerge: call sites,\nbinders, adaptive compilation, and other techniques are used to make dynamic languages perform nearly\nas fast as static languages on a platform that was designed for static languages.\n8.1. History\nThe history of the Iron languages begins in 2003. Jim Hugunin had already written an implementation of\nPython, called Jython, for the Java Virtual Machine (JVM). At the time, the then-new .NET Framework\nCommon Language Runtime (CLR) was considered by some (exactly who, I'm not sure) to be poorly\nsuited for implementing dynamic languages such as Python. Having already implemented Python on the\nJVM, Jim was curious as to how Microsoft could have made .NET so much worse than Java. In a\nSeptember 2006 blog post, he wrote:\nI wanted to understand how Microsoft could have screwed up so badly that the CLR was a worse\nplatform for dynamic languages than the JVM. My plan was to take a couple of weeks to build a\nprototype implementation of Python on the CLR and then to use that work to write a short pithy article\ncalled, \"Why the CLR is a terrible platform for dynamic languages\". My plans quickly changed as I\nworked on the prototype, because I found that Python could run extremely well on the CLR\u2014in many\ncases noticeably faster than the C-based implementation. For the standard pystone benchmark,\nIronPython on the CLR was about 1.7x faster than the C-based implementation.\n(The \"Iron\" part of the name was a play on the name of Jim's company at the time, Want of a Nail\nSoftware.)\nShortly afterwards, Jim was hired by Microsoft to make .NET an even better platform for dynamic\nlanguages. Jim (and several others) developed the DLR by factoring the language-neutral parts out of\nthe original IronPython code. The DLR was designed to provide a common core for implementing\ndynamic languages for .NET, and was a major new feature of .NET 4.\nAt the same time as the DLR was announced (April 2007), Microsoft also announced that, in addition to\na new version of IronPython built on top of the DLR (IronPython 2.0), they would be developing IronRuby\non top of the DLR to demonstrate the DLR's adaptability to multiple languages. (In October of 2010,\nMicrosoft stopped developing IronPython and IronRuby and they became independent open-source\nprojects.) Integration with dynamic languages using the DLR would also be a major part of C# and Visual\nThe Dynamic\nLanguage\nRuntime and the Iron\nLanguages\nJeff Hardy\n", "page": 84, "type": "text", "section": "Page 84"}
{"text": "Basic, with a new keyword ( dynamic ) that allowed those languages to easily call into any language\nimplemented on the DLR, or any other dynamic data source. The CLR was already a good platform for\nimplementing static languages, and the DLR makes dynamic languages a first-class citizen.\nOther language implementations from outside of Microsoft also use the DLR, including IronScheme and\nIronJS. In addition, Microsoft's PowerShell v3 will use the DLR instead of its own dynamic object system.\n8.2. Dynamic Language Runtime Principles\nThe CLR is designed with statically-typed languages in mind; the knowledge of types is baked very\ndeeply into the runtime, and one of its key assumptions is that those types do not change\u2014that a\nvariable never changes its type, or that a type never has any fields or members added or removed while\nthe program is running. This is fine for languages like C# or Java, but dynamic languages, by definition,\ndo not follow those rules. The CLR also provides a common object system for static types, which means\nthat any .NET language can call objects written in any other .NET language with no extra effort.\nWithout the DLR, every dynamic language would have to provide its own object model; the various\ndynamic languages would not be able to call objects in another dynamic language, and C# would not be\nable to treat IronPython and IronRuby equally. Thus, the heart of the DLR is a standard way of\nimplementing dynamic objects while still allowing an object's behaviour to be customized for a particular\nlanguage by using binders. It also includes a mechanism known as call-site caching for ensuring that\ndynamic operations are as fast as possible, and a set of classes for building expression trees, which\nallow code to be stored as data and easily manipulated.\nThe CLR also provides several other features that are useful to dynamic languages, including a\nsophisticated garbage collector; a Just-in-Time (JIT) compiler that converts Common Intermediate\nLanguage (IL) bytecode, which is what .NET compilers output, into machine code at runtime; a runtime\nintrospection system, which allows dynamic languages to call objects written in any static language; and\nfinally, dynamic methods (also known as lightweight code generation) that allow code to be generated at\nruntime and then executed with only sightly more overhead than a static method call. (The JVM acquired\na similar mechanism with invokedynamic  in Java 7.)\nThe result of the DLR design is that languages like IronPython and IronRuby can call each other's objects\n(and those of any other DLR language), because they have a common dynamic object model. Support\nfor this object model was also added to C# 4 (with the dynamic  keyword) and Visual Basic 10 (in\naddition to VB's existing method of \"late binding\") so that they can perform dynamic calls on objects as\nwell. The DLR thus makes dynamic languages first-class citizens on .NET.\nInterestingly, the DLR is entirely implemented as a set of libraries and can be built and run on .NET 2.0\nas well. No changes to the CLR are required to implement it.\n8.3. Language Implementation Details\nEvery language implementation has two basic stages\u2014parsing (the front end) and code generation (the\nbackend). In the DLR, each language implements its own front end, which contains the language parser\nand syntax tree generator; the DLR provides a common backend that takes expression trees to produce\nIntermediate Language (IL) for the CLR to consume; the CLR will pass the IL to a Just-In-Time (JIT)\ncompiler, which produces machine code to run on the processor. Code that is defined at runtime (and\nrun using eval ) is handled similarly, except that everything happens at the eval  call site instead of\nwhen the file is loaded.\nThere are a few different way to implement the key pieces of a language front end, and while IronPython\nand IronRuby are very similar (they were developed side-by-side, after all) they differ in a few key areas.\nBoth IronPython and IronRuby have fairly standard parser designs\u2014both use a tokenizer (also known as\na lexer) to split the text into tokens, and then the parser turns those tokens into an abstract syntax tree\n(AST) that represents the program. However, the languages have completely different implementations of\nthese pieces.\n8.4. Parsing\nIronPython's tokenizer is in the IronPython.Compiler.Tokenizer  class and the parser is in the\nIronPython.Compiler.Parser  class. The tokenizer is a hand-written state machine that recognizes\nPython keywords, operators, and names and produces the corresponding tokens. Each token also carries\nwith it any additional information (such as the value of a constant or name), as well as where in the\nsource the token was found, to aid in debugging. The parser then takes this set of tokens and compares\nthem to the Python grammar to see if it matches legal Python constructs.\nIronPython's parser is an LL(1) recursive descent parser. The parser will look at the incoming token, call\na function if the token is allowed and return an error if it is not. A recursive descent parser is built from a\nset of mutually recursive functions; these functions ultimately implement a state machine, with each new\ntoken triggering a state transition. Like the tokenizer, IronPython's parser is written by hand.\nIronRuby, on the other hand, has a tokenizer and parser generated by the Gardens Point Parser\nGenerator (GPPG). The parser is is described in the Parser.y  file\n( Languages/Ruby/Ruby/Compiler/Parser/Parser.y ), which is a yacc -format file that describes the\ngrammar of IronRuby at a high level using rules that describe the grammar. GPPG then takes\n", "page": 85, "type": "text", "section": "Page 85"}
{"text": "Parser.y  and creates the actual parser functions and tables; the result is a table-based LALR(1)\nparser. The generated tables are long arrays of integers, where each integer represents a state; based\non the current state and the current token, the tables determine which state should be transitioned to\nnext. While IronPython's recursive descent parser is quite easy to read, IronRuby's generated parser is\nnot. The transition table is enormous (540 distinct states and over 45,000 transitions) and it is next to\nimpossible to modify it by hand.\nUltimately, this is an engineering tradeoff\u2014IronPython's parser is simple enough to modify by hand, but\ncomplex enough that it obscures the structure of the language. The IronRuby parser, on the other hand,\nmakes it much easier to understand the structure of the language in the Parser.y  file, but it is now\ndependent on a third-party tool that uses a custom (albeit well-known) domain-specific language and\nmay have its own bugs or quirks. In this case, the IronPython team didn't want to commit to a\ndependency on an external tool, while the IronRuby team didn't mind.\nWhat is clear, however, is how important state machines are to parsing, at every phase. For any parsing\ntask, no matter how simple, a state machine is always the right answer.\nThe output of the parser for either language is an abstract syntax tree (AST). This describes the structure\nof the program at a high level, with each node mapping directly to a language construct\u2014a statement or\nexpression. These trees can be manipulated at runtime, often to make optimizations to the program\nbefore compilation. However, a language's AST is tied to the language; the DLR needs to operate on\ntrees that do not contain any language-specific constructs, only general ones.\n8.5. Expression Trees\nAn expression tree is also a representation of a program that can be manipulated at runtime, but in a\nlower-level, language-independent form. In .NET, the node types are in the\nSystem.Linq.Expressions  namespace, and all of the node types are derived from the abstract\nExpression  class. (The namespace is a historical artifact; expression trees were originally added in\n.NET 3.5 to implement LINQ\u2014Language Integrated Query\u2014and the DLR expression trees extended\nthat.) These expression trees cover more than just expressions, however, as there are node types for\nif  statements, try  blocks, and loops as well; in some languages (Ruby, for one) these are\nexpressions and not statements.\nThere are nodes to cover almost every feature a programming language could want. However, they tend\nto be defined at a fairly low level\u2014instead of having ForExpression , WhileExpression , etc., there is\na single LoopExpression  which, when combined with a GotoExpression , can describe any type of\nloop. To describe a language at a higher level, languages can define their own node types by deriving\nfrom Expression  and overriding the Reduce()  method, which returns another expression tree. In\nIronPython, the parse tree is also a DLR expression tree, but it contains many custom nodes that the\nDLR would not normally understand (such as ForStatement ). These custom nodes can be reduced to\nexpression trees that the DLR does understand (such as a combination of LoopExpression s and\nGotoExpression s). A custom expression node can reduce to other custom expression nodes, so the\nreduction proceeds recursively until only the intrinsic DLR nodes remain. One key difference between\nIronPython and IronRuby is that while IronPython's AST is also an expression tree, IronRuby's is not.\nInstead, IronRuby's AST is transformed into an expression tree before moving onto the next stage. It's\narguable whether having the AST also be an expression tree is actually useful, so IronRuby did not\nimplement it that way.\nEach node type knows how to reduce itself, and it can usually only be reduced in one way. For\ntransformations that come from code outside the tree\u2014optimizations such as constant folding, for\nexample, or IronPython's implementation of Python generators\u2014a subclass of the ExpressionVisitor\nclass is used. ExpressionVisitor  has a Visit()  method that calls the Accept()  method on\nExpression , and subclasses of Expression  override Accept()  to call a specific Visit()  method\non ExpressionVisitor , such as VisitBinary() . This is a textbook implementation of the Visitor\npattern from Gamma et al.\u2014there's a fixed set of node types to visit, and an infinite number of operations\nthat could be performed upon them. When the expression visitor visits a node, it usually recursively visits\nits children as well, and its children, and so on down the tree. However, an ExpressionVisitor  can't\nactually modify the expression tree it is visiting, because expression trees are immutable. If the\nexpression visitor needs to modify a node (such as removing children), it must produce a new node that\nreplaces the old one instead, and all of its parents as well.\nOnce an expression tree has been created, reduced, and visited, it ultimately needs to be executed.\nWhile expression trees can be compiled directly to IL code, IronPython and IronRuby pass them to an\ninterpreter first, because compiling directly to IL is expensive for code that may only be executed a\nhandful of times.\n8.6. Interpreting and Compilation\nOne of the downsides to using a JIT compiler, like .NET does, is that it imposes a time penalty when\nstarting up because it takes time to convert the IL bytecode into machine code that the processor can\nrun. JIT compilation makes the code much faster while running than using an interpreter, but the initial\ncost can be prohibitive, depending on what is being done. For example, a long-lived server process such\nas a web application will benefit from the JIT because the startup time is mostly irrelevant but the per-\n", "page": 86, "type": "text", "section": "Page 86"}
{"text": "request time is critical, and it tends to run the same code repeatedly. On the other hand, a program that\nis run often but only for short periods of time, such as the Mercurial command-line client, would be better\noff with a short startup time because it likely only runs each chunk of code once, and the fact that the\nJIT'd code is faster doesn't overcome the fact that it takes longer to start running.\n.NET can't execute IL code directly; it always gets JIT compiled into machine code, and this takes time.\nIn particular, program startup times are one of the weak spots of the .NET Framework because much of\nthe code needs to be JIT compiled. While there are ways to avoid the JIT penalty in static .NET\nprograms (Native Image Generation, or NGEN), they don't work for dynamic programs. Rather than\nalways compile directly to IL, IronRuby and IronPython will use their own interpreter (found in\nMicrosoft.Scripting.Interpreter ) that isn't as fast as JIT-compiled code but takes much less time\nto get started. The interpreter is also useful in situations where dynamic code generation is not allowed,\nsuch as on mobile platforms; otherwise the DLR languages would not be able to run at all.\nBefore execution, the entire expression tree must be wrapped in a function so that it can be executed. In\nthe DLR, functions are represented as LambdaExpression  nodes. While in most languages a lambda\nis an anonymous function, the DLR has no concept of names; all functions are anonymous. The\nLambdaExpression  is unique in that it is the only node type that can be converted to a delegate, which\nis what .NET calls first-class functions, using its Compile()  method. A delegate is similar to a C\nfunction pointer\u2014it is simply a handle to a piece of code that can be called.\nInitially, the expression tree is wrapped in a LightLambdaExpression , which can also produce a\ndelegate that can be executed, but rather than generate IL code (which would then invoke the JIT), it\ninstead compiles the expression tree to a list of instructions that are then executed on the interpreter's\nsimple VM. The interpreter is a simple stack-based one; instructions pop values off of the stack, perform\nan operation, and then push the result back on the stack. Each instruction is an instance of a class\nderived from Microsoft.Scripting.Interpreter.Instruction  (such as AddInstruction  or\nBranchTrueInstruction ) that has properties describing how many items it takes off of the stack, how\nmany it will put on, and a Run()  method that executes the instruction by popping and pushing values\non the stack and returning the offset of the next instruction. The interpreter takes the list of instructions\nand executes them one by one, jumping forward or backwards depending on the return value of the\nRun()  method.\nOnce a a piece of code has been executed a certain number of times, it will be converted to a full\nLambdaExpression  by calling LightLambdaExpression.Reduce() , then compiled to a\nDynamicMethod  delegate (on a background thread for a bit of parallelism), and the old delegate call site\nwill be replaced with the newer, faster one. This greatly reduces the cost of executing functions that may\nonly be called a few times, such as the main function of a program, while making commonly called\nfunctions run as fast as possible. By default, the compilation threshold is set at 32 executions, but this\ncan be changed with a command-line option or by the host program, and can include disabling either\ncompilation or the interpreter entirely.\nWhether running through the interpreter or compiled to IL, the language's operations are not hard-coded\nby the expression tree compiler. Instead, the compiler generates call sites for each operation that may be\ndynamic (which is nearly all of them). These call sites give the objects a chance to implement dynamic\nbehaviour while still keeping performance high.\n8.7. Dynamic Call Sites\nIn a static .NET language, all of the decisions about what code should be called are made at compile\ntime. For example, consider the following line of C#:\nvar z = x + y;\nThe compiler knows what the types of `x' and `y' are and whether or not they can be added. The\ncompiler can emit the proper code for handling overloaded operators, type conversions, or whatever else\nmight be needed to make the code run properly, based solely on the static information it knows about the\ntypes involved. Now, consider the following line of Python code:\nz = x + y\nThe IronPython compiler has no idea what this might do when it encounters it, because it doesn't know\nwhat the types of x  and y  are, and even if it did know, the ability of x  and y  to be added could\nchange at runtime anyway. (In principle it could, but neither IronPython nor IronRuby do type inference.)\nInstead of emitting the IL code for adding numbers, the IronPython emits a call site that will be resolved\nat runtime.\nA call site is a placeholder for an operation to be determined at runtime; they are implemented as\ninstances of the System.Runtime.CompilerServices.CallSite  class. In a dynamic language like\nRuby or Python, just about every operation has a dynamic component; these dynamic operations are\nrepresented in the expression trees as DynamicExpression  nodes, which the expression tree compiler\nknows to convert to a call site. When a call site is created, it is does not yet know how to perform the\ndesired operation; however, it is created with an instance of the proper call site binder that is specific to\nthe language in use, and contains all of the necessary information about how to perform the operation.\n", "page": 87, "type": "text", "section": "Page 87"}
{"text": "Figure 8.1: CallSite class diagram\nEach language will have a different call site binder for each operation, and the binders often know many\ndifferent ways to perform an operation depending on the arguments given to the call site. However,\ngenerating these rules is expensive (in particular, compiling them to a delegate for execution, which\ninvolves invoking the .NET JIT), so the call site has a multi-level call site cache that stores the rules that\nhave already been created for later use.\nFigure 8.2: CallSite flowchart\nThe first level, L0, is the CallSite.Target  property on the call site instance itself. This stores the\nmost-recently-used rule for this call site; for a vast number of call sites, this is all that will ever be\nneeded as they are only ever called with one set of argument types. The call site also has another cache,\nL1, that stores a further 10 rules. If Target  is not valid for this call (for example, if the arguments types\nare different), the call site first checks its rules cache to see if it has already created the proper delegate\nfrom a previous call, and reuses that rule instead of creating a new one.\nStoring rules in the cache is driven by the time it takes to actually compile a new rule compared to the\ntime it takes to check the existing rules. Roughly speaking, it takes about 10 ns for .NET to execute a\ntype check on a variable (checking a binary function takes 20 ns, etc.), which is the most common type\nof rule predicate. Compiling a simple method to add doubles, on the other hand, takes about 80 \u00b5s, or\nthree orders of magnitude longer. The size of the caches is limited to prevent wasting memory storing\nevery rule that gets used at a call site; for a simple addition, each variation requires about 1 KB of\n", "page": 88, "type": "text", "section": "Page 88"}
{"text": "memory. However, profiling showed that very few call sites ever had more than 10 variations.\nFinally, there is the L2 cache, which is stored on the binder instance itself. The binder instance that is\nassociated with a call site may store some extra information with it that makes it specific to a call site,\nbut a large number of call sites aren't unique in any way and can share the same binder instance. For\nexample, in Python, the basic rules for addition are the same throughout the program; it depends on the\ntwo types on the either side of the + , and that's it. All of the addition operations in the program can\nshare the same binder, and if both the L0 and L1 caches miss, the L2 cache contains a much larger\nnumber of recent rules (128) collected from across the entire program. Even if a call site is on its first\nexecution, there's a good chance it might already find an appropriate rule in the L2 cache. To ensure that\nthis works most effectively, IronPython and IronRuby both have a set of canonical binder instances that\nare used for common operations like addition.\nIf the L2 cache misses, the binder is asked to create an implementation for the call site, taking into\naccount the types (and possibly even the values) of the arguments. In the above example, if x  and y\nare doubles (or another native type), then the implementation simply casts them to doubles and calls the\nIL add  instruction. The binder also produces a test that checks the arguments and ensures they are\nvalid for the implementation. Together, the implementation and the test make a rule. In most cases, both\nthe implementation and the test are created and stored as expression trees. (The call site infrastructure\ndoes not depend on expression trees, however; it can be used with delegates alone.)\nIf the expression trees were expressed in C#, the code would be similar to:\nif(x is double && y is double) {       // check for doubles\n      return (double)x + (double)y;    // execute if doubles\n }\n return site.Update(site, x, y);       // not doubles, so find/create another rule \n                                       // for these types\nThe binder then produces a delegate from the expression trees, which means the rule is compiled to IL\nand then to machine code. In the case of adding two numbers, this will likely become a quick type check\nand then a machine instruction to add the numbers. Even with all of the machinery involved, the ultimate\nend result is only marginally slower than static code. IronPython and IronRuby also include a set of\nprecompiled rules for common operations like addition of primitive types, which saves time because they\ndon't have to be created at runtime, but does cost some extra space on disk.\n8.8. Meta-Object Protocol\nBesides the language infrastructure, the other key part of the DLR is the ability for a language (the host\nlanguage) to make dynamic calls on objects defined in another language (the source language). To\nmake this possible, the DLR must be able to understand what operations are valid on an object, no\nmatter the language it was written in. Python and Ruby have fairly similar object models, but JavaScript\nhas a radically different prototype-based (as opposed class-based) type system. Instead of trying to unify\nthe various type systems, the DLR treats them all as if they were based on Smalltalk-style message\npassing.\nIn a message-passing object-oriented system, objects send messages to other objects (with parameters,\nusually), and the object can return another object as a result. Thus, while each language has its own idea\nof what an object is, they can almost all be made equivalent by viewing method calls as messages that\nare sent between objects. Of course, even static OO languages fit this model to some extent; what\nmakes dynamic languages different is that the method being called does not have to be known at\ncompile time, or even exist on the object at all (e.g., Ruby's method_missing ), and the target object\nusually has a chance to intercept the message and process it differently if necessary (e.g., Python's\n__getattr__ ).\nThe DLR defines the following messages:\n{Get|Set|Delete}Member : operations for manipulating an object's members\n{Get|Set|Delete}Index : operations for indexed objects (such as arrays or dictionaries)\nInvoke , InvokeMember : invoke an object or member of an object\nCreateInstance : create an instance of an object\nConvert : convert an object from one type to another\nUnaryOperation , BinaryOperation : perform operator-based operations, such as negate ( ! )\nor add ( + )\nTaken together, these operations should be sufficient for implementing just about any language's object\nmodel.\nBecause the CLR is inherently statically typed, dynamic language objects must still be represented by\nstatic classes. The usual technique is to have a static class such as PythonObject  and have the\nactual Python objects be instances of this class or its subclasses. For reasons of interoperability and\nperformance, the DLR's mechanism is a lot more complicated. Instead of dealing with language-specific\nobjects the DLR deals with meta-objects, which are subclasses of\nSystem.Dynamic.DynamicMetaObject  and have methods for handling all of the above messages.\nEach language has its own subclasses of DynamicMetaObject  that implement the language's object\n", "page": 89, "type": "text", "section": "Page 89"}
{"text": "model, such as IronPython's MetaPythonObject . The meta classes also have corresponding concrete\nclasses that implement the System.Dynamic.IDynamicMetaObjectProtocol  interface, which is how\nthe DLR identifies dynamic objects.\nFigure 8.3: IDMOP class diagram\nFrom a class that implements IDynamicMetaObjectProtocol , the DLR can get a\nDynamicMetaObject  by calling GetMetaObject() . This DynamicMetaObject  is provided by the\nlanguage and implements the binding functions as required by that object. Each DynamicMetaObject\nalso has the value and type, if available, of the underlying object. Finally, a DynamicMetaObject  stores\nan expression tree representing the call site so far and any restrictions on that expression, similar to the\ncall site binders.\nWhen the DLR is compiling a call to a method on a user-defined class, it first creates a call site (i.e., an\ninstance of the CallSite  class). The call site initiates the binding process as described above in\n\"Dynamic Call Sites\", which results in it eventually calling GetMetaObject()  on an instance of\nOldInstance , which returns a MetaOldInstance . (Python has old-style and new-style classes, but\nthat's not relevant here.) Next, a binder is called ( PythonGetMemberBinder.Bind() ) which in turn calls\nMetaOldInstance.BindGetMember() ; it returns a new DynamicMetaObject  that describes how to\nlook up the method name on the object. Then another binder, PythonInvokeBinder.Bind() , is called,\nwhich calls MetaOldInstance.BindInvoke() , wrapping the first DynamicMetaObject  with a new\none representing how to call the method that was looked up. This includes the original object, the\nexpression tree for looking up the method name, and DynamicMetaObject s representing the\narguments to the method.\nOnce the final DynamicMetaObject  in an expression has been built, its expression tree and restrictions\nare used to build a delegate which is then returned to the call site that initiated the binding. From there\nthe code can be stored in the call site caches, making operations on objects as fast as other dynamic\ncalls, and almost as fast as static calls.\nHost languages that want to perform dynamic operations on dynamic languages must derive their binders\nfrom DynamicMetaObjectBinder . The DynamicMetaObjectBinder  will first ask the target object to\nbind the operation (by calling GetMetaObject()  and going through the binding process described\nabove) before falling back on the host language's binding semantics. As a result, if an IronRuby object is\naccessed from an IronPython program, the binding is first attempted with Ruby (target language)\nsemantics; if that fails, the DynamicMetaObjectBinder  will fall back on the Python (host language)\nsemantics. If the object being bound is not dynamic (i.e., it does not implement\nIDynamicMetaObjectProvider ), such as classes from the .NET base class library, then it is accessed\nwith the host language's semantics using .NET reflection.\nLanguages do have some freedom in how they implement this; IronPython's PythonInvokeBinder\ndoes not derive from InvokeBinder  because it needs to do some extra processing specific to Python\nobjects. As long as it only deals with Python objects, there are no issues; if it encounters an object that\nimplements IDynamicMetaObjectProvider  but is not a Python object, it forwards to a\nCompatibilityInvokeBinder  class that does inherit from InvokeBinder  and can handle foreign\nobjects correctly.\n", "page": 90, "type": "text", "section": "Page 90"}
{"text": "If the fallback cannot bind the operation, it doesn't throw an exception; instead, it returns a\nDynamicMetaObject  representing the error. The host language's binder will then handle this in an\nappropriate manner for the host language; for example, accessing a missing member on an IronPython\nobject from a hypothetical JavaScript implementation could return undefined , while doing the same to\na JavaScript object from IronPython would raise an AttributeError .\nThe ability for languages to work with dynamic objects is rather useless without the ability to first load\nand execute code written in other languages, and for this the DLR provides a common mechanism for\nhosting other languages.\n8.9. Hosting\nIn addition to providing common language implementation details, the DLR also provides a shared\nhosting interface. The hosting interface is used by the host language (usually a static language like C#) to\nexecute code written in another language such as Python or Ruby. This is a common technique that\nallows end users to extend an application, and the DLR takes it step further by making it trivial to use any\nscripting language that has a DLR implementation. There are four key parts to the hosting interface: the\nruntime, engines, sources, and scopes.\nThe ScriptRuntime  is generally shared amongst all dynamic languages in an application. The runtime\nhandles all of the current assembly references that are presented to the loaded languages, provides\nmethods for quick execution of a file, and provides the methods for creating new engines. For simple\nscripting tasks, the runtime is the only interface that needs to be used, but the DLR also provides classes\nwith more control over how scripts are run.\nUsually, only one ScriptEngine  is used for each scripting language. The DLR's meta-object protocol\nmeans that a program can load scripts from multiple languages, and the objects created by each\nlanguage can all seamlessly interoperate. The engine wraps a language-specific LanguageContext\n(such as PythonContext  or RubyContext ) and is used for executing code from files or strings and\nperforming operations on dynamic objects from languages that don't natively support the DLR (such as\nC# prior to .NET 4). Engines are thread-safe, and can execute multiple scripts in parallel, as long as\neach thread has its own scope. It also provides methods for creating script sources, which allow for more\nfine-grained control of script execution.\nA ScriptSource  holds a chunk of code to be executed; it binds a SourceUnit  object, which holds\nthe actual code, to the ScriptEngine  that created the source. This class allows code to be compiled\n(which produces a CompiledCode  object that can be cached) or executed directly. If a chunk of code is\ngoing to be executed repeatedly, it's best to compile first, and then execute the compiled code; for scripts\nthat will only be executed once, it's best to just execute it directly.\nFinally, however the code gets to be executed, a ScriptScope  must be provided for the code to\nexecute in. The scope is used to hold all of script's variables, and can be pre-loaded with variables from\nthe host, if necessary. This allows a host to provide custom objects to the script when it starts running\u2014\nfor example, an image editor may provide a method to access the pixels of the image the script is\nworking on. Once a script has executed, any variables it created can be read from the scope. The other\nmain use of scopes is to provide isolation, so that multiple scripts can be loaded and executed at the\nsame time without interfering with each other.\nIt's important to note that all of these classes are provided by the DLR, not the language; only the\nLanguageContext  used by the engine comes from the language implementation. The language context\nprovides all of the functionality\u2014loading code, creating scopes, compilation, execution, and operations on\ndynamic objects\u2014that is needed by a host, and the DLR hosting classes provide a more usable interface\nto access that functionality. Because of this, the same hosting code can be used to host any DLR-based\nlanguage.\nFor dynamic language implementations written in C (such as the original Python and Ruby), special\nwrapper code must be written to access code not written in the dynamic language, and it must be\nrepeated for each supported scripting language. While software like SWIG exists to make this easier, it's\nstill not trivial to add a Python or Ruby scripting interface to a program and expose its object model for\nmanipulation by external scripts. For .NET programs, however, adding scripting is as simple as setting up\na runtime, loading the program's assemblies into the runtime, and using ScriptScope.SetVariable()\nto make the program's objects available to the scripts. Adding support for scripting to a .NET application\ncan be done in a matter of minutes, which is a huge bonus of the DLR.\n8.10. Assembly Layout\nBecause of how the DLR evolved from a separate library into part of the CLR, there are parts that are in\nthe CLR (call sites, expression trees, binders, code generation, and dynamic meta objects) and parts that\nare part of IronLanguages open-source project (hosting, the interpreter, and a few other bits not\ndiscussed here). The parts that are in the CLR are also included in the IronLanguages project in\nMicrosoft.Scripting.Core . The DLR parts are split into two assemblies, Microsoft.Scripting\nand Microsoft.Dynamic \u2014the former contains the hosting APIs and the latter contains code for COM\ninterop, the interpreter, and some other pieces common to dynamic languages.\nThe languages themselves are split in two as well: IronPython.dll  and IronRuby.dll  implement\n", "page": 91, "type": "text", "section": "Page 91"}
{"text": "This work is made available under the Creative Commons Attribution 3.0 Unported license. Please see\nthe full description of the license for details.\nthe languages themselves (parsers, binders, etc.) while IronPython.Modules.dll  and\nIronRuby.Libraries.dll  implement the portions of the standard library that are implemented in C in\nthe classic Python and Ruby implementations.\n8.11. Lessons Learned\nThe DLR is a useful example of a language-neutral platform for dynamic languages built on top of a\nstatic runtime. The techniques it uses to achieve high-performance dynamic code are tricky to implement\nproperly, so the DLR takes these techniques and makes them available to every dynamic language\nimplementation.\nIronPython and IronRuby are good examples of how to build a language on top of the DLR. The\nimplementations are very similar because they were developed at the same time by close teams, yet they\nstill have significant differences in implementation. Having multiple different languages co-developed\n(IronPython, IronRuby, a prototype JavaScript, and the mysterious VBx\u2014a fully dynamic version of VB),\nalong with C#'s and VB's dynamic features, made sure that the DLR design got plenty of testing during\ndevelopment.\nThe actual development of IronPython, IronRuby, and the DLR was handled very differently than most\nprojects within Microsoft at the time\u2014it was a very agile, iterative development model with continuous\nintegration running from day one. This enabled them to change very quickly when they had to, which was\ngood because the DLR became tied into C#'s dynamic features early in its development. While the DLR\ntests are very quick, only taking a dozen seconds or so, the language tests take far too long to run (the\nIronPython test suite takes about 45 minutes, even with parallel execution); improving this would have\nimproved the iteration speed. Ultimately, these iterations converged on the current DLR design, which\nseems overly complicated in parts but fits together quite nicely in total.\nHaving the DLR tied to C# was critically important because it made sure the DLR had a place and a\n\"purpose\", but once the C# dynamic features were done the political climate changed (coinciding with an\neconomic downturn) and the Iron languages lost their support within the company. The hosting APIs, for\nexample, never made it into the .NET Framework (and it's highly unlikely they ever will); this means that\nPowerShell 3, which is also based on the DLR, uses a completely different set of hosting APIs than\nIronPython and IronRuby, although all of their objects can still interact as described above. (Some of the\nDLR team members went on to work on the C# compiler-as-a-service library code-named \"Roslyn\",\nwhich bears a striking resemblance to the IronPython and IronRuby hosting APIs.) But, thanks to the\nwonder of open source licensing, they will continue to survive and even thrive.\nBack to top\nBack to The Architecture of Open Source Applications.\n", "page": 92, "type": "text", "section": "Page 92"}
{"text": "<>\n9.1. What Is ITK?\nITK, the Insight Toolkit, is a library for image analysis that was developed by the initiative, and mainly\nwith the funding, of the US National Library of Medicine. ITK can be thought of as a usable encyclopedia\nof image analysis algorithms, in particular for image filtering, image segmentation and image registration.\nThe library was developed by a consortium involving universities, commercial companies, and many\nindividual contributors from around the world. Development of ITK started in 1999, and recently after its\n10th anniversary the library underwent a refactoring process intended to remove crusty code and to\nreshape it for the next decade.\n9.2. Architectural Features\nSoftware toolkits have a very synergistic relationship with their communities. They shape one another in\na continuous iterative cycle. The software is continuously modified until it satisfies the needs of the\ncommunity, while the community behaviors themselves are adapted based on what the software\nempowers or restricts them to do. In order to better understand the nature of ITK's architecture, it is\ntherefore very useful to get a sense of what kind of problems the ITK community is usually addressing,\nand how they tend to go about solving them.\nThe Nature of the Beast\nIf you did not understand the nature of the beasts,\nit would be of little use to know the mechanics of their anatomy.\n\u2014\u00a0Dee Hock, One from Many: VISA and the Rise of Chaordic Organization\nIn a typical image analysis problem, a researcher or an engineer will take an input image, improve some\ncharacteristics of the image by, let's say, reducing noise or increasing contrast, and then proceed to\nidentify some features in the image, such as corners and strong edges. This type of processing is\nnaturally well-suited for a data pipeline architecture, as shown in Figure 9.1.\nFigure 9.1: Image processing pipeline\nTo illustrate this point, Figure 9.2 shows an image of a brain from a magnetic resonance image (MRI),\nand the result of processing it with a median filter to reduce its level of noise, as well as the outcome of\nan edge detection filter used to identify the borders of anatomical structures.\n \n \nITK\nLuis Ib\u00e1\u00f1ez and Brad King\n", "page": 93, "type": "text", "section": "Page 93"}
{"text": "Figure 9.2: MRI brain image, median filter, edge detection filter\nFor each one of these tasks, the image analysis community has developed a variety of algorithms, and\ncontinue developing new ones. Why do they continue doing this?, you may ask, and the answer is that\nimage processing is a combination of science, engineering, art, and \"cooking\" skills. Claiming that there is\nan algorithmic combination that is the \"right\" answer to an image processing task is as misleading as\nclaiming that there is such a thing as the \"right\" type of chocolate dessert for a dinner. Instead of\npursuing perfection, the community strives to produce a rich set of tools that ensures that there will be no\nshortage of options to try when facing a given image processing challenge. This state of affairs, of\ncourse, comes at a price. The cost is that the image analyst has the difficult task of choosing among\ndozens of different tools that can be used in different combinations to achieve similar results.\nThe image analysis community is closely integrated with the research community. It is common to find\nthat specific research groups become attached to the algorithmic families they have developed. This\ncustom of \"branding\", and up to some level \"marketing\", leads to a situation where the best that the\nsoftware toolkit can do for the community is to offer a very complete set of algorithmic implementations\nthat they can try, and then mix and match to create a recipe that satisfies their needs.\nThese are some of the reasons why ITK was designed and implemented as a large collection of\nsomewhat independent but coherent tools, the image filters, many of which can be used to solve similar\nproblems. In this context, a certain level of \"redundancy\"\u2014for example, offering three different\nimplementations of the Gaussian filter\u2014is not seen as a problem but as a valuable feature, because\ndifferent implementations can be used interchangeably to satisfy constraints and exploit efficiencies with\nrespect to image size, number of processors, and Gaussian kernel size that might be specific to a given\nimaging application.\nThe toolkit was also conceived as a resource that grows and renews itself continuously as new algorithms\nand better implementations become available, superseding existing ones, and as new tools are developed\nin response to the emerging needs of new medical imaging technologies.\nArmed with this quick insight into the daily routine of the image analysts in the ITK community, we can\nnow dive into the main features of the architecture:\nModularity\nData Pipeline\nFactories\nIO Factories\nStreaming\nReusability\nMaintainability\nModularity\nModularity is one of the main characteristics of ITK. This is a requirement that emerges from the way\npeople in the image analysis community work when solving their problems. Most image analysis\nproblems put one or more input images through a combination of processing filters that enhance or\nextract particular pieces of information from the images. Therefore there is no single large processing\nobject, but rather myriad small ones. This structural nature of the image processing problem logically\nimplies implementing the software as a large collection of image processing filters that can be combined\nin many different ways.\nIt is also the case that certain types of processing filters are clustered into families, inside which some of\ntheir implementation features can be factorized. This leads to natural grouping of the image filters into\nmodules and groups of modules.\nModularity, therefore occurs at three natural levels in ITK:\nFilter Level\nFilter Family Level\nFilter Family Group Level\nAt the image filter level, ITK has a collection of about 700 filters. Given that ITK is implemented in C++,\nthis is a natural level at which every one of those filters is implemented by a C++ Class following object-\noriented design patterns. At the filter family level, ITK groups filters together according to the nature of\nthe processing that they perform. For example, all filters that are related to Fourier transforms will be put\ntogether into a Module. At the C++ level, Modules map to directories in the source tree, and to libraries\nonce the software is compiled to its binary form. ITK has about 120 of these Modules. Each module\ncontains:\n1. The source code of the image filters that belong to that family.\n2. A set of configuration files that describe how to build the module and list dependencies between this\nmodule and other modules.\n3. The set of unit tests corresponding to each one of the filters.\n", "page": 94, "type": "text", "section": "Page 94"}
{"text": "Figure 9.3: Hierarchical structure of groups, modules and classes\nThe group level is mostly a conceptual division that has been drawn on top of the software to help locate\nfilters in the source tree. Groups are associated with high-level concepts such as Filtering, Segmentation,\nRegistration and IO. This hierarchical structure is illustrated in Figure 9.3. ITK currently has 124 modules,\nwhich are in turn aggregated into 13 major groups. The modules have a variety of different sizes. This\nsize distribution, in bytes, is presented in Figure 9.4.\n", "page": 95, "type": "text", "section": "Page 95"}
{"text": "Figure 9.4: Size distribution of 50 largest ITK modules in KB\nThe modularization in ITK also applies to a set of third-party libraries that are not directly part of the\ntoolkit, but that the toolkit depends upon, and that are distributed along with the rest of the code for the\nconvenience of users. Particular examples of these third-party libraries are the image file format libraries:\nHDF5, PNG, TIFF, JPEG and OpenJPEG among others. The third party libraries are highlighted here\nbecause they account for about 56 percent of the size of ITK. This reflects the usual nature of open\nsource applications that build upon existing platforms. The size distribution of the third-party libraries\ndoes not necessarily reflect the architectural organization of ITK, since we have adopted these useful\nlibraries just as they have been developed upstream. However, the third-party code is distributed along\nwith the toolkit, and partitioning it was one of the key driving directives for the modularization process.\nThe module size distribution is presented here because it is a measure of the proper modularization of\nthe code. One can see the modularization of the code as a continuous spectrum that ranges from the\nextremes of having all the code in a single module, the monolithic version, to partitioning the code in a\nvery large collection of equally sized modules. This size distribution was a tool used to monitor the\nprogression of the modularization process, particularly to ensure that no big blocks of code were left in\nthe same module unless true logical dependencies called for such grouping.\nThe modular architecture of ITK enables and facilitates:\nReduction and clarification of cross-dependencies\nAdoption of code contributed by the community\nEvaluation of quality metrics per module (for example, code coverage)\nBuilding selected subsets of the toolkit\nPackaging selected subsets of the toolkit for redistribution\nContinued growth by progressive addition of new modules\nThe modularization process made it possible to explicitly identify and declare the dependencies between\ndifferent portions of the toolkit as they were put into modules. In many cases, this exercise revealed\nartificial or incorrect dependencies that had been introduced in the toolkit over time, and that passed\nunnoticed when most of the code was put together in a few large groups.\nThe usefulness of evaluating quality metrics per module is twofold. First, it makes it easier to hold\ndevelopers accountable for the modules which they maintain. Second, it makes it possible to engage in\nclean-up initiatives in which a few developers focus for a short period of time on raising the quality of a\nspecific module. When concentrating on a small portion of the toolkit, it is easier to see the effect of the\neffort and to keep developers engaged and motivated.\nTo reiterate, we note that the structure of the toolkit reflects the organization of the community and in\nsome cases the processes that have been adopted for the continuous growth and quality control of the\nsoftware.\nData Pipeline\nThe staged nature of most image analysis tasks led naturally to the selection of a Data Pipeline\narchitecture as the backbone infrastructure for data processing. The Data Pipeline enables:\nFilter Concatenation: A set of image filters can be concatenated one after another, composing a\nprocessing chain in which a sequence of operations are applied to the input images.\nParameter Exploration: Once a processing chain is put together, it is easy to change the parameters\nof any filter in the chain, and to explore the effects that such change will have on the final output\nimage.\n", "page": 96, "type": "text", "section": "Page 96"}
{"text": "Memory Streaming: Large images can be managed by processing only sub-blocks of the image at a\ntime. In this way, it becomes possible to process large images that otherwise would not have fit into\nmain memory.\nFigures 9.1 and 9.2 have already presented a simplified representation of a data pipeline from the image\nprocessing point of view. Image filters typically have numeric parameters that are used to regulate the\nbehavior of the filter. Every time one of the numeric parameters is modified, the data pipeline marks its\noutput as \"dirty\" and knows that this particular filter, and all the downstream ones that use its output,\nshould be executed again. This feature of the pipeline facilitates the exploration of parameter space while\nusing a minimum amount of processing power for each instance of an experiment.\nThe process of updating the pipeline can be driven in such a way that only sub-pieces of the images are\nprocessed at a time. This is a mechanism necessary to support the functionality of streaming. In practice,\nthe process is controlled by the internal passing of a RequestedRegion  specification from one filter\ndownstream to its provider filter upstream. This communication is done through an internal API and it is\nnot exposed to the application developers.\nFor a more concrete example, if a Gaussian blur image filter is expecting to use as input a 100x100-pixel\nimage that is produced by a median image filter, the blur filter can ask the median filter to produce only a\nquarter of the image, that is, an image region of size 100x25 pixels. This request can be further\npropagated upstream, with the caveat that every intermediate filter may have to add an extra border to\nthe image region size in order to produce that requested output region size. There is more on data\nstreaming later.\nBoth a change in the parameters of a given filter, or a change in the specific requested region to be\nprocessed by that filter, will have the effect of marking the pipeline as \"dirty\" and indicating the need for a\nreexecution of that filter through the downstream filters in the pipeline.\nProcess and Data Objects\nTwo main types of objects were designed to hold the basic structure of the pipeline. They are the\nDataObject  and the ProcessObject . The DataObject  is the abstraction of classes that carry data;\nfor example, images and geometrical meshes. The ProcessObject  provides an abstraction for the\nimage filters and mesh filters that process such data. ProcessObject s take DataObject s as input\nand perform some type of algorithmic transformation on them, such as the ones illustrated in Figure 9.2.\nDataObject s are generated by ProcessObject s. This chain typically starts by reading a\nDataObject  from disk, for example by using a ImageFileReader  which is a type of\nProcessObject . The ProcessObject  that created a given DataObject  is the only one that should\nmodify such DataObject . This output DataObject  is typically connected as input to another\nProcessObject  downstream in the pipeline.\nFigure 9.5: Relationship between ProcessObject s and DataObject s\nThis sequence is illustrated in Figure 9.5. The same DataObject  may be passed as input to multiple\nProcessObject s, as it is shown in the figure, where the DataObject  is produced by the file reader\nat the beginning of the pipeline. In this particular case, the file reader is an instance of the\nImageFileReader  class, and the DataObject  that it produces as output is an instance of the\nImage  class. It is also common for some filters to require two DataObject s as input, as it is the case\nof the subtract filter indicated in the right side of the same figure.\nThe ProcessObject s and DataObject s are connected together as a side effect of constructing the\npipeline. From the application developer's point of view, the pipeline is linked together by invoking a\nsequence of calls involving the ProcessObject s such as:\nwriter->SetInput( canny->GetOutput() );\ncanny->SetInput( median->GetOutput() );\nmedian->SetInput( reader->GetOutput() );\nInternally, however, what is connected as a consequence of these calls is not one ProcessObject  to\nthe next ProcessObject , but the downstream ProcessObject  to the DataObject  that is produced\nby the upstream ProcessObject .\nThe internal chained structure of the pipeline is held together by three types of connections:\nThe ProcessObject  holds a list of pointers to its output DataObject s. Output DataObject s\nare owned and controlled by the ProcessObject  that produces them.\nThe ProcessObject  holds a list of pointers to its input DataObject s. Input DataObject s are\nowned by the upstream ProcessObject .\n", "page": 97, "type": "text", "section": "Page 97"}
{"text": "The DataObject  holds a pointer to its producer ProcessObject . That happens to be the\nProcessObject  that also owns and control this DataObject .\nThis collection of internal links is later exploited to propagate calls upstream and downstream in the\npipeline. During all these interactions, the ProcessObject  retains control and ownership of the\nDataObject  that it generates. The filters downstream gain access to the information about a given\nDataObject  through the pointer links that are established as a consequence of the calls to the\nSetInput()  and GetOutput()  methods, without ever taking control of that input data. For practical\npurposes, filters should treat their input data as read-only objects. This is enforced in the API by using\nthe C++ const  keyword in the arguments of SetInput()  methods. As a general rule, ITK embraces\na const-correct external API, even though internally this const-correctness is overridden by some of the\npipeline operations.\nThe Pipeline Class Hierarchy\nFigure 9.6: Hierarchy of ProcessObject s and DataObject s\nThe initial design and implementation of the Data Pipeline in ITK was derived from the Visualization\nToolkit (VTK), a mature project at the time when ITK development began. (See The Architecture of Open\nSource Applications, Volume 1.)\nFigure 9.6 shows the object-oriented hierarchy of the pipeline objects in ITK. In particular, note the\nrelationship between the basic Object , ProcessObject , DataObject , and some of the classes in\nthe filter family and the data family. In this abstraction, any object that is expected to be passed as input\nto a filter, or to be produced as output by a filter, must derive from the DataObject . All filters that\nproduce and consume data are expected to derive from the ProcessObject . The data negotiations\nrequired to move data through the pipeline are implemented partly in the ProcessObject  and partly in\nthe DataObject .\nThe LightObject  and Object  classes are above the dichotomy of the ProcessObject  and\nDataObject . The LightObject  and Object  classes provide common functionalities such as the\nAPI for communications of Events , and the support for multi-threading.\nThe Inner Workings of the Pipeline\nFigure 9.7 presents a UML sequence diagram describing the interactions between ProcessObject s\nand DataObject s in a minimal pipeline composed of an ImageFileReader , MedianImageFilter\nand ImageFileWriter .\nThe full interaction consist of four passes:\nUpdate Output Information (upstream call sequence)\nUpdate Requested Region (upstream call sequence)\nUpdate Output Data (upstream call sequence)\nGenerate Data (downstream call sequence)\n", "page": 98, "type": "text", "section": "Page 98"}
{"text": "Figure 9.7: UML sequence diagram\nThe whole process is triggered when an application invokes the Update()  method in the last filter of\nthe pipeline; in this concrete example this is the ImageFileWriter . The Update()  call initiates the\nfirst pass that goes in the upstream direction. That is, from the last filter in the pipeline, towards the first\nfilter in the pipeline.\nThe goal of this first pass is to ask the question, \"How much data can you generate for me?\" This\nquestion is codified in the method UpdateOutputInformation() . In this method, every filter computes\nthe amount of image data that can be produced as output with the given amount of data available to it as\ninput. Given that the amount of data input must be known first before the filter can answer the question\nabout the amount of data output, the question has to propagate to the filter upstream, until it reaches a\nsource filter that can answer the first question by itself. In this concrete example, that source filter is the\nImageFileReader . This filter can figure out the size of its output by gathering information from the\nimage file that it has been assigned to read. Once the first filter of the pipeline answers the question,\nthen the subsequent filters downstream can compute their respective amount of output one after another,\nuntil they make it to the last filter of the pipeline.\nThe second pass, which also travels in the upstream direction, informs filters as to the amount of output\nthat they are requested to produce during pipeline execution. The concept of Requested Region is\nessential in supporting the streaming capabilities of ITK. It makes it possible to tell the filters in the\npipeline not to generate the entire full image, but to focus instead in a subregion of the image, the\nRequested Region. This is very useful when the image at hand is larger than the RAM available in the\nsystem. The call propagates from the last filter to the first one, and at every intermediate filter the\nrequested region size is modified to take into account any extra borders that a filter may need in the\ninput so it can generate a given region size as output. In our concrete example, the median filter will\ntypically have to add a 2-pixel border to the size of its own input. That is, if the writer requests a region\nof size 500 x 500 pixels to the median filter, the median filter in its turn will request a region of 502 x 502\npixels to the reader, because the median filter by default needs a 3 x 3 pixel neighborhood region to\ncompute the value of one output pixel. The pass is encoded in the PropagateRequestedRegion()\nmethod.\nThe third pass is intended to trigger the computation on the data inside the Requested Region. This pass\nalso goes in the upstream direction and it is codified in the UpdateOutputData()  method. Since every\nfilter needs its input data before it can compute its output data, the call is passed to the respective\nupstream filter first, hence the upstream propagation. Upon return the current filter actually proceeds to\ncomputes its data.\nThe fourth and final pass proceeds downstream, and consists of the actual execution of computation by\nevery filter. The call is codified in the GenerateData()  method. The downstream direction is not a\nconsequence of one filter making calls on its downstream partner, but rather of the fact that the\nUpdateOutputData()  calls are executing in order from the first filter to the last filter. That is, the\n", "page": 99, "type": "text", "section": "Page 99"}
{"text": "sequence happens downstream due to timing of the calls, and not due to what filter is driving the calls.\nThis clarification is important because the ITK pipeline is by nature a Pull Pipeline, in which data is\nrequested from the end, and the logic is also controlled from the end.\nFactories\nOne of the fundamental design requirements of ITK is to provide support for multiple platforms. This\nrequirement emerges from the desire to maximize the impact of the toolkit by making it usable to a broad\ncommunity regardless of their platform of choice. ITK adopted the Factory design pattern to address the\nchallenge of supporting fundamental differences among the many hardware and software platforms,\nwithout sacrificing the fitness of a solution to each one of the individual platforms.\nThe Factory pattern in ITK uses class names as keys to a registry of class constructors. The registration\nof factories happens at run time, and can be done by simply placing dynamic libraries in specific\ndirectories that ITK applications search at start-up time. This last feature provides a natural mechanism\nfor implementing a plugin architecture in a clean and transparent way. The outcome is to facilitate the\ndevelopment of extensible image analysis applications, satisfying the need to provide an ever-growing set\nof image analysis capabilities.\nIO Factories\nThe factory mechanism is particularly important when performing IO.\nEmbracing Diversity with Facades\nThe image analysis community has developed a very large set of file formats to store image data. Many\nof these file formats are designed and implemented with specific uses in mind, and therefore are fine-\ntuned to specific types of images. As a consequence, on a regular basis, new image file formats are\nconceived and promoted across the community. Aware of this situation, the ITK development team\ndesigned an IO architecture suitable for ease of extensibility, in which it is easy to add support for more\nand more file formats on a regular basis.\nFigure 9.8: IO Factories dependencies\nThis IO extensible architecture is built upon the Factory mechanism described in the previous section.\nThe main difference is that in the case of IO, the IO Factories are registered in a specialized registry that\nis managed by the ImageIOFactory  base class, shown on the upper left corner of Figure 9.8. The\nactual functionality of reading and writing data from image file formats is implemented in a family of\nImageIO  classes, shown on the right side of Figure 9.8. These service classes are intended to be\ninstantiated on demand when the user requests to read or write an image. The service classes are not\nexposed to the application code. Instead, applications are expected to interact with the facade classes:\nImageFileReader\nImageFileWriter\nThese are the two classes with which the application will invoke code such as:\nreader->SetFileName(\"../image1.png\");\nreader->Update();\nor:\nwriter->SetFileName(\"../image2.jpg\");\nwriter->Update();\nIn both cases the call to Update()  triggers the execution of the upstream pipeline to which these\nProcessObject s are connected. Both the reader and the writer behave as one filter more in a pipeline.\n", "page": 100, "type": "text", "section": "Page 100"}
{"text": "In the particular case of the reader, the call to Update()  triggers the reading of the corresponding\nimage file into memory. In the case of the writer, the call to Update()  triggers the execution of the\nupstream pipeline that is providing the input to the writer, and finally results in an image being written out\nto disk into a particular file format.\nThese facade classes hide from the application developer the internal differences that are inherent to the\nparticularities of each file format. They even hide the existence of the file format itself. The facades are\ndesigned in such a way that most of the time application developers do not need to know what file\nformats are expected to be read by the application. The typical application will simply invoke code such\nas\nstd::string filename = this->GetFileNameFromGUI();\nwriter->SetFileName( filename );\nwriter->Update();\nThese calls will work fine regardless of whether the content of the filename  variable is any of the\nfollowing strings:\nimage1.png\nimage1.jpeg\nimage1.tiff\nimage1.dcm\nimage1.mha\nimage1.nii\nimage1.nii.gz\nwhere the file name extensions identify a different image file format in every case.\nKnow Thy Pixel Type\nDespite the assistance that the file reader and writer facades provide, it is still up to the application\ndeveloper to be aware of the pixel type that the application needs to process. In the context of medical\nimaging, it is reasonable to expect that the application developer will know whether the input image will\ncontain a MRI, a mammogram or a CT scan, and therefore be mindful of selecting the appropriate pixel\ntype and image dimensionality for each one of these different image modalities. This specificity of image\ntype might not be convenient for application settings where users wants to read any image type, which\nare most commonly found in the scenarios of rapid prototyping and teaching. In the context of deploying\na medical image application for production in a clinical setting, however, it is expected that the pixel type\nand dimension of the images will be clearly defined and specified based on the image modality to be\nprocessed. A concrete example, where an application manages 3D MRI scans, looks like:\ntypedef itk::Image< signed short, 3 >  MRImageType;\ntypedef itk::ImageFileWriter< MRImageType > MRIWriterType;\nMRIWriterType::Pointer writer = MRIWriterType::New();\nwriter->Update();\nThere is a limit, however, to how much the particularities of the image file formats can be hidden from the\napplication developer. For example, when reading images from DICOM files, or when reading RAW\nimages, the application developer may have to insert extra calls to further specify the characteristics of\nthe file format at hand. DICOM files will be the most commonly found in clinical environments, while\nRAW images are still a necessary evil for exchanging data in the research environment.\nTogether But Separate\nThe self-contained nature of every IO Factory and ImageIO service class is also reflected in the\nmodularization. Typically, an ImageIO class depends on a specialized library that is dedicated to\nmanaging a specific file format. That is the case for PNG, JPEG, TIFF and DICOM, for example. In those\ncases, the third-party library is managed as a self-contained module, and the specialized ImageIO code\nthat interfaces ITK to that third-party library is also put in a module by itself. In this way, specific\napplications may disable many file formats that are not relevant to their domain, and can focus on\noffering only those file formats that are useful for the anticipated scenarios of that application.\nJust as with standard factories, the IO factories can be loaded at run-time from dynamic libraries. This\nflexibility facilitates the use of specialized and in-house developed file formats without requiring all such\nfile formats to be incorporated directly into the ITK toolkit itself. The loadable IO factories has been one of\nthe most successful features in the architectural design of ITK. It has made it possible to easily manage a\nchallenging situation without placing a burden on the code or obscuring its implementation. More\nrecently, the same IO architecture has been adapted to manage the process of reading and writing files\ncontaining spatial transformations represented by the Transform  class family.\nStreaming\nITK was initially conceived as a set of tools for processing the images acquired by the Visible Human\nProject. At the time, it was clear that such a large dataset would not fit in the RAM of computers that\nwere typically available to the medical imaging research community. It is still the case that the dataset\nwill not fit in the typical desktop computers that we use today. Therefore, one of the requirements for\ndeveloping the Insight Toolkit was to enable the streaming of image data through the data pipeline. More\n", "page": 101, "type": "text", "section": "Page 101"}
{"text": "specifically, to be able to process large images by pushing sub-blocks of the image throughout the data\npipeline, and then assembling the resulting blocks on the output side of the pipeline.\nFigure 9.9: Illustration of image streaming process\nThis partitioning of the image domain is illustrated in Figure 9.9 for the concrete example of a median\nfilter. The median filter computes the value of one output pixel as the statistical median of the pixel\nvalues from the input image in a neighborhood around the pixel. The size of that neighborhood is a\nnumerical parameter of the filter. In this case we set it to 2 pixels, which means that we will take a\nneighborhood with a 2-pixel radius around our output pixel. This leads to a neighborhood of 5x5 pixels\nwith the position of the output pixel in the middle, and a rectangular border of 2 pixels around it. This is\nusually called a Manhattan radius. When the median filter is asked to computed a particular Requested\nRegion of the output image, it turns around and asks its upstream filter to provide a larger region that is\nformed by the Requested Region enlarged by a border of, in this case, 2 pixels. In the specific case of\nFigure 9.9, when asked for Region 2, of size 100x25 pixels, the median filter passes along that request\nto its upstream filter for a region of size 100x29 pixels. The 29-pixel size in the vertical direction is\ncomputed as 25 pixels plus two borders of 2-pixel radius each. Note that the horizontal dimension is not\nenlarged in this case because it is already at the maximum that the input image can provide; therefore,\nthe enlarged request of 104 pixels (100 pixels plus two borders of 2 pixels) gets cropped to the maximum\nsize of the image, which is 100 pixels in the horizontal dimension.\nITK filters that operate on neighborhoods will take care of the boundary conditions by using one of the\nthree typical approaches: considering a null value outside of the image, mirroring the pixels' values\nacross the border, or repeating the border value on the outside. In the case of the median filter, a zero-\nflux Neumann boundary condition is used, which simply means that the pixels outside of the region\nborder are assumed to be a repetition of the pixel values found in the last pixels inside the border.\nIt is a well-kept dirty little secret of the image processing literature that most of the implementation\ndifficulties with image filters are related to proper management of boundary conditions. This is a particular\nsymptom of the disconnection between the theoretical training found in many textbooks and the software\npractice of image processing. In ITK this was managed by implementing a collection of image iterator\nclasses and an associated family of boundary condition calculators. These two helper classes families\nhide from image filters the complexities of managing boundary conditions in N-dimensions.\nThe streaming process is driven from outside the filter, typically by the ImageFileWriter  or the\nStreamingImageFilter . These two classes implement the streaming functionality of taking the total\nsize of the image and partitioning it into a number of divisions requested by the application developer.\nThen, during their Update()  call, they go in an iteration loop asking for each one of the intermediate\npieces of the image. At that stage, they take advantage of the SetRequestedRegion()  API described\nin Figure 9.7. That constrains the computation of the upstream pipeline to a subregion of the image.\nThe application code driving the streaming process looks like\nmedian->SetInput( reader->GetOutput() );\nmedian->SetNeighborhoodRadius( 2 );\nwriter->SetInput( median->GetOutput() );\nwriter->SetFileName( filename );\nwriter->SetNumberOfStreamDivisions( 4 );\nwriter->Update();\nwhere the only new element is the SetNumberOfStreamDivisions()  call that defines the number of\npieces into which the image will be split for the purpose of streaming it through the pipeline. To match\nthe example of Figure 9.9 we have used four as the number of regions to split the image into. This\nmeans that the writer  is going to trigger the execution of the median  filter four times, each time with\na different Requested Region.\nThere are interesting similarities between the process of streaming and the process of parallelizing the\nexecution of a given filter. Both of them rely on the possibility of dividing the image processing work into\nimage chunks that are processed separately. In the streaming case, the image chunks are processed\nacross time, one after another, while in the parallelization case the image chunks are assigned to\ndifferent threads, which in turn are assigned to separate processor cores. At the end, it is the algorithmic\nnature of the filter that will determine whether it is possible to split the output image into chunks that can\nbe computed independently based on a corresponding set of image chunks from the input image. In ITK,\n", "page": 102, "type": "text", "section": "Page 102"}
{"text": "streaming and parallelization are actually orthogonal, in the sense that there is an API to take care of the\nstreaming process, and a separate API dedicated to support the implementation of parallel computation\nbase on multiple-threads and shared memory.\nStreaming, unfortunately, can not be applied to all types of algorithms. Specific cases that are not suitable\nfor streaming are:\nIterative algorithms that, to compute a pixel value at every iteration, require as input the pixel values\nof its neighbors. This is the case for most PDE-solving-based algorithms, such as anisotropic\ndiffusion, demons deformable registration, and dense level sets.\nAlgorithms that require the full set of input pixel values in order to compute the value of one of the\noutput pixels. Fourier transform and Infinite Impulse Response (IIR) filters, such as the Recursive\nGaussian filter, are examples of this class.\nRegion propagation or front propagation algorithms in which the modification of pixels also happens\nin an iterative way but for which the location of the regions or fronts can not be systematically\npartitioned in blocks in a predictable way. Region growing segmentation, sparse level sets, some\nimplementations of mathematical morphology operations and some forms of watersheds are typical\nexamples here.\nImage registration algorithms, given that they require access to the full input image data for\ncomputing metric values at every iteration of their optimization cycles.\nFortunately, on the other hand, the data pipeline structure of ITK enables support for streaming in a\nvariety of transformation filters by taking advantage of the fact that all filters create their own output, and\ntherefore they do not overwrite the memory of the input image. This comes at the price of memory\nconsumption, since the pipeline has to allocate both the input and output images in memory\nsimultaneously. Filters such as flipping, axes permutation, and geometric resampling fall in this category.\nIn these cases, the data pipeline manages the matching of input regions to output regions by requiring\nthat every filter provide a method called GenerateInputRequestedRegion()  that takes as an\nargument a rectangular output region. This method computes the rectangular input region that will be\nneeded by this filter to compute that specific rectangular output region. This continuous negotiation in the\ndata pipeline makes it possible to associate, for every output block, the corresponding section of input\nimage that is required for computation.\nTo be more precise here, we must say therefore that ITK supports streaming\u2014but only in algorithms that\nare \"streamable\" in nature. That said, in the spirit of being progressive regarding the remaining\nalgorithms, we should qualify this statement not by claiming that \"it is impossible to stream such\nalgorithms\", but rather that \"our typical approach to streaming is not suitable for these algorithms\" at this\npoint, and that hopefully new techniques will be devised by the community in the future to address these\ncases.\n9.3. Lessons Learned\nReusability\nThe principle of reusability can also be read as \"avoidance of redundancy\". In the case of ITK, this has\nbeen achieved with a three-pronged approach.\nFirst, the adoption of object-oriented programming, and in particular the proper creation of class\nhierarchies where common functionalities are factorized in base classes.\nSecond, the adoption of generic programming, implemented via the heavy use of C++ templates,\nfactorizing behaviors that are identified as patterns.\nThird, the generous use of C++ macros has also permitted reuse of standard snippets of code that\nare needed in myriad places across the toolkit.\nMany of these items may sound like platitudes and appear obvious today, but when ITK development\nstarted in 1999 some of them were not that obvious. In particular, at the time the support most C++\ncompilers offered for templates did not quite follow a consistent standard. Even today, decisions such as\nthe adoption of generic programming and the use of a widely templated implementation continue to be\ncontroversial in the community. This is manifested in the communities that prefer to use ITK via the\nwrapping layers to Python, Tcl or Java.\nGeneric Programming\nThe adoption of generic programming was one of the defining implementation features of ITK. It was a\ndifficult decision in 1999 when the compiler support for C++ templates was rather fragmented, and the\nStandard Template Library (STL) was still considered a bit exotic.\nGeneric programming was adopted in ITK by embracing the use of C++ templates for implementing\ngeneralization of concepts and in this way increasing code reuse. The typical example of C++ template\nparameterization in ITK is the Image  class, that can be instantiated in the following way:\ntypedef unsigned char PixelType;\nconst unsigned int Dimension = 3;\ntypedef itk::Image< PixelType, Dimension > ImageType;\nImageType::Pointer image = ImageType::New();\nIn this expression, the application developer chooses the type to be used to represent pixels in the\n", "page": 103, "type": "text", "section": "Page 103"}
{"text": "image, as well as the dimension of the image as a grid in space. In this particular example, we chose to\nuse an 8-bit  pixel represented in an unsigned char  type, for a 3D image. Thanks to the underlying\ngeneric implementation, it is possible to instantiate images of any pixel type and any dimension in ITK.\nTo make it possible to write these expressions, ITK developers had to implement the Image  class by\nbeing very careful with the assumptions made about the pixel type. Once the application developer has\ninstantiated the image type, the developer can create objects of that type, or proceed to instantiate image\nfilters whose types, in turn, depend on the image type. For example:\ntypedef itk::MedianImageFilter< ImageType, ImageType> FilterType;\nFilterType::Pointer median = FilterType::New();\nThe algorithmic specificity of different image filters restricts the actual pixel types that they can support.\nFor example, some image filters expect the image pixel type to be an integer scalar type while some\nother filters expect the pixel type to be a vector of floating point numbers. When instantiated with\ninappropriate pixel types, these filters will produce compilation errors or will result in erroneous\ncomputational results. To prevent incorrect instantiations and to facilitate the troubleshooting of\ncompilation errors, ITK adopted the use of concept checking that is based on forcing the exercise of\ncertain expected features of the types, with the goal of producing early failures combined with human-\nreadable error messages.\nC++ templates are also exploited in certain sections of the toolkit in the form of Template\nMetaprogramming, with the goal of increasing run-time speed performance of the code, in particular for\nunrolling loops that control the computation of low-dimensional vectors and matrices. Ironically, we have\nfound over time that certain compilers have become smarter at figuring out when to unroll loops, and no\nlonger need the help of Template MetaProgramming expressions in some cases.\nKnowing When to Stop\nThere is also the general risk of doing \"too much of a good thing\", meaning, there is a risk of overusing\ntemplates, or overusing macros. It is easy to go overboard and end up creating a new language on top of\nC++ that is essentially based on the use of templates and macros. This is a fine line, and it demands\ncontinuous attention from the development team to make sure that the language features are properly\nused without being abused.\nAs a concrete example, the widespread use of explicitly naming types via C++ typedefs  has proved to\nbe particularly important. This practice plays two roles: on the one hand it provides a human-readable\ninformative name describing the nature of the type and its purpose; on the other hand, it ensures that the\ntype is used consistently across the toolkit. As an example, during the refactoring of the toolkit for its 4.0\nversion, a massive effort was invested in collecting the cases where C++ integer types such as int ,\nunsigned int , long  and unsigned long  were used and to replace them with types named after\nthe proper concept that the associated variables were representing. This was the most costly part of the\ntask of ensuring that the toolkit was able to take advantage of 64-bit types for managing images larger\nthan four gigabytes in all platforms. This task was of the utmost importance for promoting the use of ITK\nin the fields of microscopy and remote sensing, where image of tens of gigabytes in size are common.\nMaintainability\nThe architecture satisfies the constraints that minimize maintenance cost.\nModularity (at the class level)\nMany small files\nCode reuse\nRepeated patterns\nThese characteristics reduce maintenance cost in the following ways:\nModularity (at the class level) makes it possible to enforce test-driven development techniques at the\nimage filter level, or in general the ITK class level. Stringent testing discipline applied to small and\nmodular pieces of code has the advantage of reducing the pools of code where bugs can hide, and\nwith the natural decoupling that results from modularization, it is a lot easier to locate defects and\neliminate them.\nMany small files facilitate the assignment of portions of the code to specific developers, and simplify\nthe tracking of defects when they are associated with specific commits in the revision control system.\nThe discipline of keeping small files also leads to the enforcement of the golden rule of functions and\nclasses: Do one thing, and do it right.\nCode reuse: When code is reused (instead of being copy-pasted and reimplemented) the code itself\nbenefits from the higher level of scrutiny that results from being exercised in many different\ncircumstances. It leads more eyes to look at the code, or at least at the effects of the code, and so\nthe code benefits from Linus's Law: \"Given enough eyeballs, all bugs are shallow.\"\nRepeated patterns simplify the work of maintainers, who in reality account for more than 75% of the\ncost of software development over the lifetime of a project. Using coding patterns that are\nconsistently repeated in different places in the code makes it a lot easier for a developer to open a\nfile and quickly understand what the code is doing, or what it is intended to do.\nAs the developers got involved in regular maintenance activities they were exposed to some \"common\nfailures\", in particular:\n", "page": 104, "type": "text", "section": "Page 104"}
{"text": "Assumptions that some filters make regarding specific pixel types for their input or output images,\nbut that are not enforced via types or concept checking, and that are not specified in the\ndocumentation.\nNot writing for readability. This is one of the most common challenges for any software whose new\nalgorithm implementations originate in the research community. It is common in that environment to\nwrite code that \"just works\", and to forget that the purpose of code is not just to be executed at run\ntime, but to be easily read by the next developer. Typical good rules of \"clean code\" writing\u2014for\nexample, write small functions that do one thing and one thing only (the Single Responsibility\nPrinciple and the Principle of Least Surprise), adhere to proper naming of variables and functions\u2014\ntend to be ignored when researchers are excited about getting their new shiny algorithm to work.\nIgnoring failure cases and error management. It is common to focus on the \"nice cases\" of data\nprocessing and to fail to provide code for managing all the cases that can go wrong. Adopters of the\ntoolkit quickly run into such cases once they start developing and deploying applications in the real\nworld.\nInsufficient testing. It requires a lot of discipline to follow the practice of test-driven development,\nespecially the notion of writing the tests first and only implementing functionalities as you test them. It\nis almost always the case that bugs in the code are hiding behind the cases that were skipped while\nimplementing the testing code.\nThanks to the communication practices of open source communities, many of these items end up being\nexposed through questions that are commonly asked in the mailing lists, or are directly reported as bugs\nby users. After dealing with many such issues, developers learn to write code that is \"good for\nmaintenance\". Some of these traits apply to both coding style and the actual organization of the code. It\nis our view that a developer only reaches mastery after spending some time\u2014at least a year\u2014doing\nmaintenance and getting exposed to \"all the things that can go wrong\".\nThe Invisible Hand\nSoftware should look like it was written by a single person. The best developers are the ones who write\ncode that, should they be hit by the proverbial bus, can be taken over by anybody else. We have grown\nto recognize that any trace of a \"personal touch\" is an indication of a defect introduced in the software.\nIn order to enforce and promote code style uniformity, the following tools have proved to be very\neffective:\nKWStyle for automatic source code style checking. This is a simplified C++ parser that checks\ncoding style and flags any violations.\nGerrit for regular code reviews. This tools serves two purposes: On one hand, it prevents immature\ncode from entering the code base by distilling its errors, deficiencies and imperfections during\niterative review cycles where other developers contribute to improve the code. On the other hand, it\nprovides a virtual training camp in which new developers get to learn from more experienced\ndevelopers (read \"experienced\" as have made all the mistakes and know where the bodies are\nburied\u2026) how to improve the code and avoid known problems that have been observed during\nmaintenance cycles.\nGit hooks that enforce the use of the KWStyle and Gerrit and that also perform some of their own\nchecks. For example, ITK uses Git hooks that prevent commits of code with tabs or with trailing\nblank spaces.\nThe team has also explored the use of Uncrustify as a tool for enforcing a consistent style.\nIt is worth emphasizing that uniformity of style is not a simple matter of aesthetic appeal, it is really a\nmatter of economics. Studies on the Total Cost of Ownership (TCO) of software projects have estimated\nthat in the life-cycle of a project, the cost of maintenance will be about 75% of the TCO, and given that\nmaintenance cost is applied on an annual basis, it typically surpasses the cost of initial development\ncosts by the first five years of the life-cycle of a software project. (See \"Software Development Cost\nEstimating Handbook\", Volume I, Naval Center for Cost Analysis, Air Force Cost Analysis Agency, 2008.)\nMaintenance is estimated to be about 80% of what software developers actually do, and when engaged\nin that activity the large majority of the developer's time is dedicated to reading someone else's code,\ntrying to figure out what it was supposed to do (see Clean Code, A Handbook of Agile Software\nCraftsmanship, Robert C. Martin, Prentice Hall, 2009). Uniform style does wonders for reducing the time\nit takes for developers to immerse themselves in a newly open source file and understand the code\nbefore they make any modifications to it. By the same token, it reduces the chances that developers will\nmisinterpret the code and make modifications that end up introducing new bugs when they were honestly\ntrying to fix old bugs (The Art of Readable Code, Dustin Boswell, Trevor Foucher, O'Reilly, 2012).\nThe key for making these tools effective is to make sure that they are:\nAvailable to all developers, hence our preference for Open Source tools.\nRun on a regular basis. In the case of ITK, these tools have been integrated in the Nightly and\nContinuous Dashboard builds managed by CDash.\nRun as close as possible to the point where the code is being written, so that deviations can be fixed\nimmediately, and so developers quickly learn what kind of practices break style rules.\nRefactoring\nITK started in 2000 and grew continuously until 2010. In 2011, thanks to an infusion of federal funding\n", "page": 105, "type": "text", "section": "Page 105"}
{"text": "investment, the development team had the truly unique opportunity to embark on a refactoring effort. The\nfunding was provided by the National Library of Medicine as part of the initiative of the American\nRecovery and Reinvestment Act (ARRA). This was not a minor undertaking. Imagine you have been\nworking on a piece of software for over a decade, and you are offered the opportunity to clean it up;\nwhat would you change?\nThis opportunity for widespread refactoring is very rare. For the previous ten years, we had relied on the\ndaily effort of performing small local refactorings, cleaning up specific corners of the toolkit as we ran into\nthem. This continuous process of clean up and improvement takes advantage of the massive\ncollaboration of open source communities, and it is safely enabled by the testing infrastructure driven by\nCDash, which regularly exercises about 84% of the code in the toolkit. Note that in contrast, the average\ncode coverage of software industry testing is estimated to be only 50%.\nAmong the many things that were changed in the refactoring effort, the ones that are most relevant to\nthe architecture are:\nModularization was introduced in the toolkit\nInteger types were standardized\nTypedefs were fixed to allow management of images larger than 4 GB on all platforms\nThe software process was revised:\nMigrated from CVS to Git\nIntroduced code review with Gerrit\nIntroduced testing on demand with CDash@home\nImproved method for downloading data for unit testing\nDeprecated support for obsolete compilers\nImproved support for many IO image file formats including:\nDICOM\nJPEG2000\nTIFF (BigTIFF)\nHDF5\nIntroduced a framework for supporting GPU computation\nIntroduced support for video processing\nAdded a bridge to OpenCV\nAdded a bridge to VXL\nMaintenance based on incremental modifications\u2014tasks such as adding features to an image filter,\nimproving performance of a given algorithm, addressing bug reports, and improving documentation of\nspecific image filters\u2014works fine for the local improvement of specific C++ classes. However, a massive\nrefactoring is needed for infrastructure modifications that affect a large number of classes across the\nboard, such as the ones listed above. For example, the set of changes needed to support images larger\nthan 4 GB was probably one of the largest patches ever applied to ITK. It required the modification of\nhundreds of classes and could not have been done incrementally without incurring in a great deal of\npain. The modularization is another example of a task that could not have been done incrementally. It\ntruly affected the entire organization of the toolkit, how its testing infrastructure works, how testing data is\nmanaged, how the toolkit is packaged and distributed, and how new contributions will be encapsulated to\nbe added to the toolkit in the future.\nReproducibility\nOne of the early lessons learned in ITK was that the many papers published in the field were not as\neasy to implement as we were led to believe. The computational field tends to over-celebrate algorithms\nand to dismiss the practical work of writing software as \"just an implementation detail\".\nThat dismissive attitude is quite damaging to the field, since it diminishes the importance of the first-hand\nexperience with the code and its proper use. The outcome is that most published papers are simply not\nreproducible, and when researchers and students attempt to use such techniques they end up spending\na lot of time in the process and deliver variations of the original work. It is actually quite difficult, in\npractice, to verify if an implementation matches what was described in a paper.\nITK disrupted, for the good, that environment and restored a culture of DIY to a field that had grown\naccustomed to theoretical reasoning, and that had learned to dismiss experimental work. The new culture\nbrought by ITK is a practical and pragmatic one in which the virtues of the software are judged by its\npractical results and not by the appearance of complexity that is celebrated in some scientific\npublications. It turns out that in practice the most effective processing methods are those that would\nappear to be too simple to be accepted for a scientific paper.\nThe culture of reproducibility is a continuation of the philosophy of test driven development, and\nsystematically results in better software; higher clarity, readability, robustness and focus.\nIn order to fill the gap of lack of reproducible publications, the ITK community created the Insight Journal.\nIt is an open-access, fully online publication in which contributions are required to include code, data,\nparameters, and tests in order to enable verification by reproducibility. Articles are published online less\nthan 24 hours after submission. Then they are made available for peer-review by any member of the\ncommunity. Readers get full access to all the materials accompanying the article, namely source code,\ndata, parameters, and testing scripts. The Journal has provided a productive space for sharing new code\ncontributions which from there make their way into the code base. The Journal recently received its 500th\n", "page": 106, "type": "text", "section": "Page 106"}
{"text": "This work is made available under the Creative Commons Attribution 3.0 Unported license. Please see\nthe full description of the license for details.\narticle, and continues to be used as the official gateway for new code to be added to ITK.\nBack to top\nBack to The Architecture of Open Source Applications.\n", "page": 107, "type": "text", "section": "Page 107"}
{"text": "<>\nGNU Mailman is free software for managing mailing lists. Almost everybody who writes or uses free and\nopen source software has encountered a mailing list. Mailing lists can be discussion-based or\nannouncement-based, with all kinds of variations in between. Sometimes mailing lists are gatewayed to\nnewsgroups on Usenet or similar services such as Gmane. Mailing lists typically have archives which\ncontain the historical record of all the messages that have been posted to the mailing list.\nGNU Mailman has been around since the early 1990s, when John Viega wrote the first version to\nconnect fans with the nascent Dave Matthews Band, the members of which he was friends with in\ncollege. This early version came to the attention of the Python community in the mid-'90s, when the\ncenter of the Python universe had moved from CWI, a scientific research institute in the Netherlands, to\nCNRI, the Corporation for National Research Initiatives in Reston, Virginia, USA. At CNRI we were\nrunning various Python-related mailing lists using Majordomo, a Perl-based mailing list manager. Of\ncourse, it just wouldn't do for the Python world to be maintaining so much Perl code. More importantly,\nbecause of its design, we found that modifying Majordomo for our purposes (such as to add minimal anti-\nspam measures) was too difficult.\nKen Manheimer was instrumental in much of the early GNU Mailman work, and many excellent\ndevelopers have contributed to Mailman since then. Today, Mark Sapiro is maintaining the stable 2.1\nbranch, while Barry Warsaw, the author of this chapter, concentrates on the new 3.0 version.\nMany of the original architectural decisions John made have lived on in the code right up until the\nMailman 3 branch, and can still be seen in the stable branch. In the sections that follow, I'll describe\nsome of the more problematic design decisions in Mailman 1 and 2, and how we've addressed them in\nMailman 3.\nIn the early Mailman 1 days, we had a lot of problems with messages getting lost, or bugs causing\nmessages to be re-delivered over and over again. This prompted us to articulate two overriding principles\nthat are critical to Mailman's ongoing success:\nNo message should ever be lost.\nNo message should ever be delivered more than once.\nIn Mailman 2 we re-designed the message handling system to ensure that these two principles would\nalways be of prime importance. This part of the system has been stable for at least a decade now, and is\none of the key reasons that Mailman is as ubiquitous as it is today. Despite modernizing this subsystem\nin Mailman 3, the design and implementation remains largely unchanged.\n10.1. The Anatomy of a Message\nOne of the core data structures in Mailman is the email message, represented by a message object.\nMany of the interfaces, functions, and methods in the system take three arguments: the mailing list\nobject, the message object, and a metadata dictionary that is used to record and communicate state\nwhile a message is processed through the system.\nGNU\nMailman\nBarry Warsaw\n", "page": 108, "type": "text", "section": "Page 108"}
{"text": "Figure 10.1: A MIME multipart/mixed  message containing text, images, and an audio file\nOn the face of it, an email message is a simple object. It consists of a number of colon-separated key-\nvalue pairs, called the headers, followed by an empty line which separates the headers from the\nmessage body. This textural representation should be easy to parse, generate, reason about, and\nmanipulate, but in fact it quickly gets quite complicated. There are countless RFCs that describe all the\nvariations that can occur, such as handling complex data types like images, audio, and more. Email can\ncontain ASCII English, or just about any language and character set in existence. The basic structure of\nan email message has been borrowed over and over again for other protocols, such as NNTP and HTTP,\nyet each is slightly different. Our work on Mailman has spawned several libraries just to deal with the\nvagaries of this format (often called \"RFC822\" for the founding 1982 IETF standard). The email libraries\noriginally developed for use by GNU Mailman have found their way into the Python standard library,\nwhere development continues to make them more standards-compliant and robust.\nEmail messages can act as containers for other types of data, as defined in the various MIME standards.\nA container message part can encode an image, some audio, or just about any type of binary or text\ndata, including other container parts. In mail reader applications, these are known as attachments. Figure\n10.1 shows the structure of a complex MIME message. The boxes with solid borders are the container\nparts, the boxes with dashed borders are Base64 encoded binary data, and the box with a dotted border\nis a plain text message.\nContainer parts can also be arbitrarily nested; these are called multiparts and can in fact get quite deep.\nBut any email message, regardless of its complexity, can be modeled as a tree with a single message\nobject at its root. Within Mailman, we often refer to this as the message object tree, and we pass this tree\naround by reference to the root message object. Figure 10.2 shows the object tree of the multipart\nmessage in Figure 10.1.\n", "page": 109, "type": "text", "section": "Page 109"}
{"text": "Figure 10.2: Message object tree of a complex MIME email message\nMailman will almost always modify the original message in some way. Sometimes the transformations\ncan be fairly benign, such as adding or removing headers. Sometimes we'll completely change the\nstructure of the message object tree, such as when the content filter removes certain content types like\nHTML, images, or other non-text parts. Mailman might even collapse \"multipart/alternatives\", where a\nmessage appears as both plain text and as some rich text type, or add additional parts containing\ninformation about the mailing list itself.\nMailman generally parses the on the wire bytes representation of a message just once, when it first\ncomes into the system. From then on, it deals only with the message object tree until it's ready to send it\nback out to the outgoing mail server. It's at that point that Mailman flattens the tree back into a bytes\nrepresentation. Along the way, Mailman pickles) the message object tree for quick storage to, and\nreconstruction from, the file system. Pickles are a Python technology for serializing any Python object,\nincluding all its subobjects, to a byte stream, and it's perfectly suited to optimizing the handling of email\nmessage object trees. Unpickling is deserializing this byte stream back into a live object. By storing these\nbyte streams in a file, Python programs gain low-cost persistence.\n10.2. The Mailing List\nThe mailing list is obviously another core object in the Mailman system, and most of the operations in\nMailman are mailing list-centric, such as:\nMembership is defined in terms of a user or address being subscribed to a specific mailing list.\nMailing lists have a large number of configuration options that are stored in the database, and which\ncontrol everything from posting privileges to how messages are modified before final delivery.\nMailing lists have owners and moderators which have greater permission to change aspects of the\nlist, or to approve and reject questionable postings.\nEvery mailing list has its own archive.\nUsers post new messages to a specific mailing list.\nand so on. Almost every operation in Mailman takes a mailing list as an argument\u2014it's that fundamental.\nMailing list objects have undergone a radical redesign in Mailman 3 to make them more efficient and to\nexpand their flexibility.\nOne of John's earliest design decisions was how to represent a mailing list object inside the system. For\nthis central data type, he chose a Python class with multiple base classes, each of which implements a\nsmall part of the mailing list's responsibility. These cooperating base classes, called mixin classes, were\na clever way to organize the code so that it was easy to add entirely new functionality. By grafting on a\nnew mixin base class, the core MailList  class could easily accommodate something new and cool.\nFor example, to add an auto-responder to Mailman 2, a mixin class was created that held the data\nspecific to that feature. The data would get automatically initialized when a new mailing list was created.\nThe mixin class also provided the methods necessary to support the auto-responder feature. This\nstructure was even more useful when it came to the design of the mailing MailList  object's\npersistence.\nAnother of John's early design decisions was to use Python pickles for storing MailList  state\npersistence.\nIn Mailman 2, the MailList  object's state is stored in a file called config.pck , which is just the\npickled representation of the MailList  object's dictionary. Every Python object has an attribute\ndictionary called __dict__ . So saving a mailing list object then is simply a matter of pickling its\n__dict__  to a file, and loading it just involves reading the pickle from the file and reconstituting its\n__dict__ .\n", "page": 110, "type": "text", "section": "Page 110"}
{"text": "Thus, when a new mixin class was added to implement some new functionality, all the attributes of the\nmixin were automatically pickled and unpickled appropriately. The only extra work we had to do was to\nmaintain a schema version number to automatically upgrade older mailing list objects when new\nattributes were added via the mixin, since the pickled representation of older MailList  objects would\nbe missing the new attributes.\nAs convenient as this was, both the mixin architecture and pickle persistence eventually crumbled under\ntheir own weight. Site administrators often requested ways to access the mailing list configuration\nvariables via external, non-Python systems. But the pickle protocol is entirely Python-specific, so\nsequestering all that useful data inside a pickle wouldn't work for them. Also, because the entire state of\na mailing list was contained in the config.pck , and Mailman has multiple processes that need to read,\nmodify, and write the mailing list state, we had to implement exclusive file-based and NFS-safe locks to\nensure data consistency. Every time some part of Mailman wants to change the state of a mailing list, it\nmust acquire the lock, write out the change, then release the lock. Even read operations can require a\nre-load of the list's config.pck  file, since some other process may have changed it before the read\noperation. This serialization of operations on a mailing list turned out to be horribly slow and inefficient.\nFor these reasons, Mailman 3 stores all of its data in a SQL database. By default SQLite3 is used,\nthough this is easily changed since Mailman 3 uses the Object Relational Mapper called Storm, which\nsupports a wide variety of databases. PostgreSQL support was added with just a few lines of code, and\na site administrator can enable it by changing one configuration variable.\nAnother, bigger problem is that in Mailman 2, each mailing list is a silo. Often operations span across\nmany mailing lists, or even all of them. For example, a user might want to temporarily suspend all their\nsubscriptions when they go on vacation. Or a site administrator might want to add some disclaimer to the\nwelcome message of all of the mailing lists on her system. Even the simple matter of figuring out which\nmailing lists a single address is subscribed to required unpickling the state of every mailing list on the\nsystem, since membership information was kept in the config.pck  file too.\nAnother problem was that each config.pck  file lived in a directory named after the mailing list, but\nMailman was originally designed without consideration for virtual domains. This lead to a very unfortunate\nproblem where two mailing lists could not have the same name in different domains. For example, if you\nowned both the example.com  and example.org  domains, and you wanted them to act independently\nand allow for a different support  mailing list in each, you cannot do this in Mailman 2, without\nmodifications to the code, a barely supported hook, or conventional workarounds that forced a different\nlist name under the covers, which is the approach used by large sites such as SourceForge.\nThis has been solved in Mailman 3 by changing the way mailing lists are identified, along with moving all\nthe data into a traditional database. The primary key for the mailing list table is the fully qualified list\nname or as you'd probably recognize it, the posting address. Thus support@example.com  and\nsupport@example.org  are now completely independent rows in the mailing list table, and can easily\nco-exist in a single Mailman system.\n10.3. Runners\nMessages flow through the system by way of a set of independent processes called runners. Originally\nconceived as a way of predictably processing all the queued message files found in a particular directory,\nthere are now a few runners which are simply independent, long-running processes that perform a\nspecific task and are managed by a master process; more on that later. When a runner does manage\nfiles in a directory, it is called a queue runner.\nMailman is religiously single-threaded, even though there is significant parallelism to exploit. For example,\nMailman can accept messages from the mail server at the same time it's sending messages out to\nrecipients, or processing bounces, or archiving a message. Parallelism in Mailman is achieved through\nthe use of multiple processes, in the form of these runners. For example, there is an incoming queue\nrunner with the sole job of accepting (or rejecting) messages from the upstream mail server. There is an\noutgoing queue runner with the sole job of communicating with the upstream mail server over SMTP in\norder to send messages out to the final recipients. There's an archiver queue runner, a bounce\nprocessing queue runner, a queue runner for forwarding messages to an NNTP server, a runner for\ncomposing digests, and several others. Runners which don't manage a queue include a Local Mail\nTransfer Protocol server and an administrative HTTP server.\nEach queue runner is responsible for a single directory, i.e., its queue. While the typical Mailman system\ncan perform perfectly well with a single process per queue, we use a clever algorithm for allowing\nparallelism within a single queue directory, without requiring any kind of cooperation or locking. The\nsecret is in the way we name the files within the queue directory.\nAs mentioned above, every message that flows through the system is also accompanied by a metadata\ndictionary that accumulates state and allows independent components of Mailman to communicate with\neach other. Python's pickle  library is able to serialize and deserialize multiple objects to a single file,\nso we can pickle both the message object tree and metadata dictionary into one file.\nThere is a core Mailman class called Switchboard  which provides an interface for enqueuing (i.e.,\nwriting) and dequeuing (i.e., reading) the message object tree and metadata dictionary to files in a\nspecific queue directory. Every queue directory has at least one switchboard instance, and every queue\n", "page": 111, "type": "text", "section": "Page 111"}
{"text": "runner instance has exactly one switchboard.\nPickle files all end in the .pck  suffix, though you may also see .bak , .tmp , and .psv  files in a\nqueue. These are used to ensure the two sacrosanct tenets of Mailman: no file should ever get lost, and\nno message should ever be delivered more than once. But things usually work properly and these files\ncan be pretty rare.\nAs indicated, for really busy sites Mailman supports running more than one runner process per queue\ndirectory, completely in parallel, with no communication between them or locking necessary to process\nthe files. It does this by naming the pickle files with a SHA1 hash, and then allowing a single queue\nrunner to manage just a slice of the hash space. So if a site wants to run two runners on the bounces\nqueue, one would process files from the top half of the hash space, and the other would process files\nfrom the bottom half of the hash space. The hashes are calculated using the contents of the pickled\nmessage object tree, the name of the mailing list that the message is destined for, and a time stamp.\nThe SHA1 hashes are effectively random, and thus on average a two-runner queue directory will have\nabout equal amounts of work per process. And because the hash space can be statically divided, these\nprocesses can operate on the same queue directory with no interference or communication necessary.\nThere's an interesting limitation to this algorithm. Since the splitting algorithm allots one or more bits of\nthe hash to each space, the number of runners per queue directory must be a power of 2. This means\nthere can be 1, 2, 4, or 8 runner processes per queue, but not, for example, 5. In practice this has never\nbeen a problem, since few sites will ever need more than 4 processes to handle their load.\nThere's another side effect of this algorithm that did cause problems during the early design of this\nsystem. Despite the unpredictability of email delivery in general, the best user experience is provided by\nprocessing the queue files in FIFO order, so that replies to a mailing list get sent out in roughly\nchronological order. Not making a best effort attempt at doing so can cause confusion for members. But\nusing SHA1 hashes as file names obliterates any timestamps, and for performance reasons stat()\ncalls on queue files, or unpickling the contents (e.g., to read a time stamp in the metadata) should be\navoided.\nMailman's solution was to extend the file naming algorithm to include a time stamp prefix, as the number\nof seconds since the epoch (e.g., <timestamp>+<sha1hash>.pck ). Each loop through the queue\nrunner starts by doing an os.listdir() , which returns all the files in the queue directory. Then for\neach file, it splits the file name and ignores any file names where the SHA1 hash doesn't match its slice\nof responsibility. The runner then sorts the remaining files based on the timestamp part of the file name.\nIt's true that with multiple queue runners each managing different slices of the hash space, this could\nlead to ordering problems between the parallel runners, but in practice, the timestamp ordering is enough\nto preserve end-user perception of best-effort sequential delivery.\nIn practice this has worked extremely well for at least a decade, with only the occasional minor bug fix or\nelaboration to handle obscure corner cases and failure modes. It's one of the most stable parts of\nMailman and was largely ported untouched from Mailman 2 to Mailman 3.\n10.4. The Master Runner\nWith all these runner processes, Mailman needed a simple way to start and stop them consistently; thus\nthe master watcher process was born. It must be able to handle both queue runners and runners which\ndo not manage a queue. For example, in Mailman 3, we accept messages from the incoming upstream\nmail server via LMTP, which is a protocol similar to SMTP, but which operates only for local delivery and\nthus can be much simpler as it doesn't need to deal with the vagaries of delivering mail over an\nunpredictable Internet. The LMTP runner simply listens on a port, waiting for its upstream mail server to\nconnect and send it a byte stream. It then parses this byte stream into a message object tree, creates an\ninitial metadata dictionary, and enqueues this into a processing queue directory.\nMailman also has a runner that listens on another port and processes REST requests over HTTP. This\nprocess doesn't handle queue files at all.\nA typical running Mailman system might have eight or ten processes, and they all need to be stopped\nand started appropriately and conveniently. They can also crash occasionally; for example, when a bug in\nMailman causes an unexpected exception to occur. When this happens, the message being delivered is\nshunted to a holding area, with the state of the system at the time of the exception preserved in the\nmessage metadata. This ensures that an uncaught exception does not cause multiple deliveries of the\nmessage. In theory, the Mailman site administrator could fix the problem, and then unshunt the offending\nmessages for redelivery, picking up where it left off. After shunting the problematic message, the master\nrestarts the crashed queue runner, which begins processing the remaining messages in its queue.\nWhen the master watcher starts, it looks in a configuration file to determine how many and which types of\nchild runners to start. For the LMTP and REST runners, there is usually a single process. For the queue\nrunners, as mentioned above, there can be a power-of-2 number of parallel processes. The master\nfork() s and exec() s all the runner processes based on the configuration file, passing in the\nappropriate command line arguments to each (e.g., to tell the subprocess which slice of the hash space\nto look at). Then the master basically sits in an infinite loop, blocking until one of its child processes exits.\nIt keeps track of the process ID for each child, along with a count of the number of times the child has\nbeen restarted. This count prevents a catastrophic bug from causing a cascade of unstoppable restarts.\nThere's a configuration variable which specifies how many restarts are allowed, after which an error is\n", "page": 112, "type": "text", "section": "Page 112"}
{"text": "logged and the runner is not restarted.\nWhen a child does exit, the master looks at both the exit code and the signal that killed the subprocess.\nEach runner process installs a number of signal handlers with the following semantics:\nSIGTERM : intentionally stop the subprocess. It is not restarted. SIGTERM  is what init  will kill the\nprocess with when changing run levels, and it's also the signal that Mailman itself uses to stop the\nsubprocess.\nSIGINT : also used to intentionally stop the subprocess, it's the signal that occurs when control-C is\nused in a shell. The runner is not restarted.\nSIGHUP : tells the process to close and reopen its log files, but to keep running. This is used when\nrotating log files.\nSIGUSR1 : initially stop the subprocess, but allow the master to restart the process. This is used in\nthe restart  command of init scripts.\nThe master also responds to all four of these signals, but it doesn't do much more than forward them to\nall its subprocesses. So if you sent SIGTERM  to the master, all the subprocesses would get SIGTERM 'd\nand exit. The master would know that the subprocess exited because of SIGTERM  and it would know\nthat this was an intentional stoppage, so it would not restart the runner.\nTo ensure that only one master is running at any time, it acquires a lock with a lifetime of about a day\nand a half. The master installs a SIGALRM  handler, which wakes the master up once per day so that it\ncan refresh the lock. Because the lock's lifetime is longer than the wake up interval, the lock should\nnever time out or be broken while Mailman is running, unless of course the system crashes or the master\nis killed with an uncatchable signal. In those cases, the command line interface to the master process\nprovides an option to override a stale lock.\nThis leads to the last bit of the master watcher story, the command line interface to it. The actual master\nscript takes very few command line options. Both it and the queue runner scripts are intentionally kept\nsimple. This wasn't the case in Mailman 2, where the master script was fairly complex and tried to do too\nmuch, which made it more difficult to understand and debug. In Mailman 3, the real command line\ninterface for the master process is in the bin/mailman  script, a kind of meta-script that contains a\nnumber of subcommands, in a style made popular by programs like Subversion. This reduces the\nnumber of programs that need to be installed on your shell's PATH . bin/mailman  has subcommands\nto start, stop, and restart the master, as well as all the subprocesses, and also to cause all the log files\nto be reopened. The start  subcommand fork() s and exec() s the master process, while the\nothers simply send the appropriate signal to the master, which then propagates it to its subprocesses as\ndescribed above. This improved separation of responsibility make it much easier to understand each\nindividual piece.\n10.5. Rules, Links, and Chains\nA mailing list posting goes through several phases from the time it's first received until the time it's sent\nout to the list's membership. In Mailman 2, each processing step was represented by a handler, and a\nstring of handlers were put together into a pipeline. So, when a message came into the system, Mailman\nwould first determine which pipeline would be used to process it, and then each handler in the pipeline\nwould be called in turn. Some handlers would do moderation functions (e.g., \"Is this person allowed to\npost to the mailing list?\"), others would do modification functions (e.g., \"Which headers should I remove\nor add?\"), and others would copy the message to other queues. A few examples of the latter are:\nA message accepted for posting would be copied to the archiver  queue at some point, so that its\nqueue runner would add the message to the archive.\nA copy of the message eventually had to end up in the outgoing  queue so that it could be\ndelivered to the upstream mail server, which has the ultimate responsibility of delivery to a list\nmember.\nA copy of the message had to get put into a digest for people who wanted only occasional, regular\ntraffic from the list, rather than an individual message whenever someone sent it.\nThe pipeline-of-handlers architecture proved to be quite powerful. It provided an easy way that people\ncould extend and modify Mailman to do custom operations. The interface for a handler was fairly\nstraightforward, and it was a simple matter to implement a new handler, ensuring it got added to the right\npipeline in the right location to accomplish the custom operation.\nOne problem with this was that mixing moderation and modification in the same pipeline became\nproblematic. The handlers had to be sequenced in the pipeline just so, or unpredictable or undesirable\nthings would happen. For example, if the handler that added the RFC 2369 List-*  headers came after\nthe handler to copy the message to the digest collator, then folks receiving digests would get incorrect\ncopies of the list posts. In different cases, it might be beneficial to moderate the message before or after\nmodifying it. In Mailman 3, the moderation and modification operations have been split into separate\nsubsystems for better control over the sequencing.\nAs described previously, the LMTP runner parses an incoming byte stream into a message object tree\nand creates an initial metadata dictionary for the message. It then enqueues these to one or another\nqueue directory. Some messages may be email commands (e.g., to join or leave a mailing list, to get\nautomated help, etc.) which are handled by a separate queue. Most messages are postings to the\n", "page": 113, "type": "text", "section": "Page 113"}
{"text": "mailing list, and these get put in the incoming queue. The incoming queue runner processes each\nmessage sequentially through a chain consisting of any number of links. There is a built-in chain that\nmost mailing lists use, but even this is configurable.\nFigure 10.3 illustrates the default set of chains in the Mailman 3 system. Each link in the chain is\nillustrated by a rounded rectangle. The built-in chain is where the initial rules of moderation are applied to\nthe incoming message, and in this chain, each link is associated with a rule. Rules are simply pieces of\ncode that get passed the three typical parameters: the mailing list, the message object tree, and the\nmetadata dictionary. Rules are not supposed to modify the message; they just make a binary decision\nand return a Boolean answering the question, \"Did the rule match or not?\". Rules can also record\ninformation in the metadata dictionary.\nIn the figure, solid arrows indicates message flow when the rule matches, while dotted arrows indicate\nmessage flow when the rule does not match. The outcome of each rule is recorded in the metadata\ndictionary so that later on, Mailman will know (and be able to report) exactly which rules matched and\nwhich ones missed. The dashed arrows indication transitions which are taken unconditionally, regardless\nof whether the rule matches or not.\nFigure 10.3: Simplified view of default chains with their links\nIt's important to note that the rules themselves do not dispatch based on outcome. In the built-in chain,\neach link is associated with an action which is performed when the rule matches. So for example, when\nthe \"loop\" rule matches (meaning, the mailing list has seen this message before), the message is\nimmediate handed off to the \"discard\" chain, which throws the message away after some bookkeeping. If\nthe \"loop\" rule does not match, the next link in the chain will process the message.\nIn Figure 10.3, the links associated with \"administrivia\", \"max-size\", and \"truth\" rules have no binary\ndecision. In case of the first two, this is because their action is deferred, so they simply record the match\noutcome and processing continues to the next link. The \"any\" rule then matches if any previous rule\nmatches. This way, Mailman can report on all the reasons why a message is not allowed to be posted,\ninstead of just the first reason. There are several more such rules not illustrated here for simplicity.\nThe \"truth\" rule is a bit different. It's always associated with the last link in the chain, and it always\nmatches. With the combination of the penultimate \"any\" rule sweeping aside all previously matching\nmessages, the last link then knows that any message making it this far is allowed to be posted to the\nmailing list, so it unconditionally moves the message to the \"accept\" chain.\nThere are a few other details of chain processing not described here, but the architecture is very flexible\n", "page": 114, "type": "text", "section": "Page 114"}
{"text": "and extensible so that just about any type of message processing can be implemented, and sites can\ncustomize and extend rules, links, and chains.\nWhat happens to the message when it hits the \"accept\" chain? The message, which is now deemed\nappropriate for the mailing list, is sent off to the pipeline queue for some modifications before it is\ndelivered to the end recipients. This process is described in more detail in the following section.\nThe \"hold\" chain puts the message into a special bucket for the human moderator to review. The\n\"moderation\" chain does a little additional processing to decide whether the message should be\naccepted, held for moderator approval, discarded, or rejected. In order to reduce clutter in the diagram,\nthe \"reject\" chain, which is used to bounce messages back to the original sender, is not illustrated.\n10.6. Handlers and Pipelines\nOnce a message has made its way through the chains and rules and is accepted for posting, the\nmessage must be further processed before it can be delivered to the final recipients. For example, some\nheaders may get added or deleted, and some messages may get some extra decorations that provide\nimportant disclaimers or information, such as how to leave the mailing list. These modifications are\nperformed by a pipeline which contains a sequence of handlers. In a manner similar to chains and rules,\npipelines and handlers are extensible, but there are a number of built-in pipelines for the common cases.\nHandlers have a similar interface as rules, accepting a mailing list, message object, and metadata\ndictionary. However, unlike rules, handlers can and do modify the message. Figure 10.4 illustrates the\ndefault pipeline and set of handlers (some handlers are omitted for simplicity).\nFigure 10.4: Pipeline queue handlers\nFor example, a posted message needs to have a Precedence:  header added, which tells other\nautomated software that this message came from a mailing list. This header is a de facto standard to\nprevent vacation programs from responding back to the mailing list. Adding this header (among other\n", "page": 115, "type": "text", "section": "Page 115"}
{"text": "header modifications) is done by the \"add headers\" handler. Unlike rules, handler order generally doesn't\nmatter, and messages always flow through all handlers in the pipeline.\nSome handlers send copies of the message to other queues. As shown in Figure 10.4, there is a handler\nthat makes a copy of the message for folks who want to receive digests. Copies are also sent to the\narchive queue for eventual delivery to the mailing list archives. Finally, the message is copied to the\noutgoing queue for final delivery to the mailing list's members.\n10.7. VERP\nVERP stands for Variable Envelope Return Path, and it is a well-known technique that mailing lists use\nto unambiguously determine bouncing recipient addresses. When an address on a mailing list is no\nlonger active, the recipient's mail server will send a notification back to the sender. In the case of a\nmailing list, you want this bounce to go back to the mailing list, not to the original author of the message;\nthe author can't do anything about the bounce, and worse, sending the bounce back to the author can\nleak information about who is subscribed to the mailing list. When the mailing list gets the bounce,\nhowever, it can do something useful, such as disable the bouncing address or remove it from the list's\nmembership.\nThere are two general problems with this. First, even though there is a standard format for these bounces\n(called delivery status notifications, many deployed mail servers do not conform to it. Instead, the body of\ntheir bounce messages can contain just about any amount of difficult-to-machine-parse gobbledygook,\nwhich makes automated parsing difficult. In fact, Mailman uses a library that contains dozens of bounce\nformat heuristics, all of which have been seen in the wild during the 15 years of Mailman's existence.\nSecond, imagine the situation where a member of a mailing list has several forwards. She might be\nsubscribed to the list with her anne@example.com  address, but this might forward to\nperson@example.org , which might further forward the message to me@example.net . When the final\ndestination server at example.net receives the message, it will usually just send a bounce saying that\nme@example.net  is no longer valid. But the Mailman server that sent the message only knows the\nmember as anne@example.com , so a bounce flagging me@example.net  will not contain a subscribed\naddress, and Mailman will ignore it.\nAlong comes VERP, which exploits a requirement of the fundamental SMTP protocol to provide\nunambiguous bounce detection, by returning such bounce messages to the envelope sender. This is not\nthe From:  field in the message body, but in fact the MAIL FROM  value set during the SMTP dialog.\nThis is preserved along the delivery route, and the ultimate receiving mail server is required, by the\nstandards, to send the bounces to this address. Mailman uses this fact to encode the original recipient\nemail address into the MAIL FROM  value.\nIf the Mailman server is mylist@example.org , then the VERP-encoded envelope sender for a mailing\nlist posting sent to anne@example.com  will be:\nmylist-bounce+anne=example.com@example.org\nHere, the +  is a local address separator, which is a format supported by most modern mail servers. So\nwhen the bounce comes back, it will actually be delivered to mylist-bounce@example.com  but with\nthe To:  header still set to VERP-encoded recipient address. Mailman can then parse this To:  header\nto decode the original recipient as anne@example.com .\nWhile VERP is an extremely powerful tool for culling bad addresses from the mailing list, it does have\none potentially important disadvantage. Using VERP requires that Mailman send out exactly one copy of\nthe message per recipient. Without VERP, Mailman can bundle up identical copies of an outgoing\nmessage for multiple recipients, thus reducing overall bandwidth and processing time. But VERP requires\na unique MAIL FROM  for each recipient, and the only way to do that is to send a unique copy of the\nmessage. Generally this is an acceptable trade-off, and in fact, once these individualized messages are\nbeing sent for VERP anyway, there are a lot of useful things Mailman can also do. For example, it can\nembed a URL in the footer of the message customized for each recipient which gives them a direct link\nto unsubscribe from the list. You could even imagine various types of mail-merge operations for\ncustomizing the body of the message for each individual recipient.\n10.8. REST\nOne of the key architectural changes in Mailman 3 addresses a common request over the years: to allow\nMailman to be more easily integrated with external systems. When I was hired by Canonical, the\ncorporate sponsor of the Ubuntu project, in 2007 my job was originally to add mailing lists to Launchpad,\na collaboration and hosting platform for software projects. I knew that Mailman 2 could do the job, but\nthere was a requirement to use Launchpad's web user interface instead of Mailman's default user\ninterface. Since Launchpad mailing lists were almost always going to be discussion lists, we wanted very\nlittle variability in the way they operated. List administrators would not need the plethora of options\navailable in the typical Mailman site, and what few options they would need would be exposed through\nthe Launchpad web user interface.\nAt the time, Launchpad was not free software (this changed in 2009), so we had to design the integration\nin such a way that Mailman 2's GPLv2 code could not infect Launchpad. This led to a number of\n", "page": 116, "type": "text", "section": "Page 116"}
{"text": "architectural decisions during that integration design that were quite tricky and somewhat inefficient.\nBecause Launchpad is now free software licensed under the AGPLv3, these hacks wouldn't be\nnecessary today, but having to do it this way did provide some very valuable lessons on how a web-\nuser-interface-less Mailman could be integrated with external systems. The vision that emerged was of a\ncore engine that implemented mailing list operations efficiently and reliably, and that could be managed\nby any kind of web front-end, including ones written in Zope, Django, or PHP, or with no web user\ninterface at all.\nThere were a number of technologies at the time that would allow this, and in fact Mailman's integration\nwith Launchpad is based on XMLRPC. But XMLRPC has a number of problems that make it a less-than-\nideal protocol.\nMailman 3 has adopted the Representational State Transfer (REST) model for external administrative\ncontrol. REST is based on HTTP, and Mailman's default object representation is JSON. These protocols\nare ubiquitous and well-supported in a large variety of programming languages and environments, making\nit fairly easy to integrate Mailman with third party systems. REST was the perfect fit for Mailman 3, and\nnow much of its functionality is exposed through a REST API.\nThis is a powerful paradigm that more applications should adopt: deliver a core engine that implements\nits basic functionality well, exposing a REST API to query and control it. The REST API provides yet\nanother way of integrating with Mailman, the others being utilizing the command line interface, and\nwriting Python code to access the internal API. This architecture is extremely flexible and can be used\nand integrated in ways that are beyond the initial vision of the system designers.\nNot only does this design allow for much greater choices for deployment, but it even allowed the official\ncomponents of the system to be designed and implemented independently. For example, the new official\nweb user interface for Mailman 3 is technically a separate project with its own code base, driven primarily\nby experienced web designers. These outstanding developers are empowered to make decisions, create\ndesigns, and execute implementations without the core engine development being a bottleneck. The web\nuser interface work feeds back into the core engine implementation by requesting additional functionality,\nexposed through the REST API, but they needn't wait for it, since they can mock up the server side on\ntheir end and continue experimenting and developing the web user interface while the core engine\ncatches up.\nWe plan to use the REST API for many more things, including allowing the scripting of common\noperations and integration with IMAP or NNTP servers for alternative access to the archives.\n10.9. Internationalization\nGNU Mailman was one of the first Python programs to embrace internationalization. Of course, because\nMailman does not usually modify the contents of email messages posted through it, those messages can\nbe in any language of the original author's choosing. However, when interacting directly with Mailman,\neither through the web interface or via email commands, users would prefer to use their own natural\nlanguage.\nMailman pioneered many of the internationalization technologies used in the Python world, but it is\nactually much more complex than most applications. In a typical desktop environment, the natural\nlanguage is chosen when the user logs in, and remains static throughout the desktop session. However,\nMailman is a server application, so it must be able to handle dozens of languages, separate from the\nlanguage of the system on which it runs. In fact, Mailman must somehow determine the language context\nthat a response is to be returned under, and translate its text to that language. Sometimes a response\nmay even involve multiple languages; for example, if a bounce message from a Japanese user is to be\nforwarded to list administrators who speak German, Italian, and Catalan.\nAgain, Mailman pioneered some key Python technologies to handle complex language contexts such as\nthese. It utilizes a library that manages a stack of languages which can be pushed onto and popped from\nas the context changes, even within the processing of a single message. It also implements an elaborate\nscheme for customizing its response templates based on site preferences, list owner preferences, and\nlanguage choice. For example, if a list owner wants to customize a response template for one of her lists,\nbut only for Japanese users, she would place the specific template in the appropriate place on the file\nsystem, and this would override more generic defaults.\n10.10. Lessons Learned\nWhile this article has provided an overview of Mailman 3's architecture and insight into how that\narchitecture has evolved over the 15 years of its existence (through three major rewrites), there are lots of\nother interesting architectural decisions in Mailman which I can't cover. These include the configuration\nsubsystem, the testing infrastructure, the database layer, the programmatic use of formal interfaces,\narchiving, mailing list styles, the email commands and command-line interface, and integration with the\noutgoing mail server. Contact us on the mailman-developers mailing list if you're interested in more\ndetails.\nHere are some lessons we've learned while rewriting a popular, established, and stable piece of the open\nsource ecosystem.\nUse test driven development (TDD). There really is no other way! Mailman 2 largely lacks an\n", "page": 117, "type": "text", "section": "Page 117"}
{"text": "This work is made available under the Creative Commons Attribution 3.0 Unported license. Please see\nthe full description of the license for details.\nautomated test suite, and while it's true that not all of the Mailman 3 code base is covered by its test\nsuite, most of it is, and all new code is required to be accompanied by tests, using either\nunittests  or doctests . Doing TDD is the only way to gain the confidence that the changes you\nmake today do not introduce regressions in existing code. Yes, TDD can sometimes take longer, but\nthink of it as an investment in the future quality of your code. In that way, not having a good test\nsuite means you're just wasting your time. Remember the mantra: untested code is broken code.\nGet your bytes/strings story straight from the beginning. In Python 3, a sharp distinction is made\nbetween Unicode text strings and byte arrays, which, while initially painful, is a huge benefit to writing\ncorrect code. Python 2 blurred this line by having both Unicode and 8-bit ASCII strings, with some\nautomated coercions between them. While appearing to be a useful convenience, problems with this\nfuzzy line are the number one cause of bugs in Mailman 2. This is not helped by the fact that email\nis notoriously difficult to classify into strings and bytes. Technically, the on-the-wire representation of\nan email is as a sequence of bytes, but these bytes are almost always ASCII, and there is a strong\ntemptation to manipulate message components as text. The email standards themselves describe\nhow human-readable, non-ASCII text can be safely encoded, so even things like finding a Re:\nprefix in a Subject:  header will be text operations, not byte operations. Mailman's principle is to\nconvert all incoming data from bytes to Unicode as early as possible, deal with the text as Unicode\ninternally, and only convert it back to bytes on the way out. It's critical to be crystal clear from the\nstart when you're dealing with bytes and when you're dealing with text, since it's very difficult to\nretrofit this fundamental model shift later.\nInternationalize your application from the start. Do you want your application to be used only by the\nminority of the world that speaks English? Think about how many fantastic users this ignores! It's not\nhard to set up internationalization, and there are lots of good tools for making this easy, many of\nwhich were pioneered in Mailman. Don't worry about the translations to start with; if your application\nis accessible to the world's wealth of languages, you will have volunteer translators knocking down\nyour door to help.\nGNU Mailman is a vibrant project with a healthy user base, and lots of opportunities for contributions.\nHere are some resources you can use if you think you'd like to help us out, which I hope you do!\nPrimary web site\nProject wiki\nDeveloper mailing list\nUsers mailing list\nFreenode IRC channel: #mailman\nA Final Note\nWhile this chapter was being written, we learned with sadness of the passing of Tokio Kikuchi\n(http://wiki.list.org/display/COM/TokioKikuchi), a Japanese professor who contributed heavily to Mailman,\nand was especially knowledgeable about internationalization and the idiosyncrasies of Japanese mail\nuser agents. He will be greatly missed.\nBack to top\nBack to The Architecture of Open Source Applications.\n", "page": 118, "type": "text", "section": "Page 118"}
{"text": "<>\nmatplotlib is a Python-based plotting library with full support for 2D and limited support for 3D graphics,\nwidely used in the Python scientific computing community. The library targets a broad range of use\ncases. It can embed graphics in the user interface toolkit of your choice, and currently supports\ninteractive graphics on all major desktop operating systems using the GTK+, Qt, Tk, FLTK, wxWidgets\nand Cocoa toolkits. It can be called interactively from the interactive Python shell to produce graphics\nwith simple, procedural commands, much like Mathematica, IDL or MATLAB. matplotlib can also be\nembedded in a headless webserver to provide hardcopy in both raster-based formats like Portable\nNetwork Graphics (PNG) and vector formats like PostScript, Portable Document Format (PDF) and\nScalable Vector Graphics (SVG) that look great on paper.\n11.1. The Dongle Problem\nmatplotlib's origin dates to an attempt by one of us (John Hunter) to free himself and his fellow epilepsy\nresearchers from a proprietary software package for doing electrocorticography (ECoG) analysis. The\nlaboratory in which he worked had only one license for the software, and the various graduate students,\nmedical students, postdocs, interns, and investigators took turns sharing the hardware key dongle.\nMATLAB is widely used in the biomedical community for data analysis and visualization, so Hunter set\nout, with some success, to replace the proprietary software with a MATLAB-based version that could be\nutilized and extended by multiple investigators. MATLAB, however, naturally views the world as an array\nof floating point numbers, and the complexities of real-world hospital records for epilepsy surgery patients\nwith multiple data modalities (CT, MRI, ECoG, EEG) warehoused on different servers pushed MATLAB to\nits limits as a data management system. Unsatisfied with the suitability of MATLAB for this task, Hunter\nbegan working on a new Python application built on top of the user interface toolkit GTK+, which was at\nthe time the leading desktop windowing system for Linux.\nmatplotlib was thus originally developed as an EEG/ECoG visualization tool for this GTK+ application,\nand this use case directed its original architecture. matplotlib was originally designed to serve a second\npurpose as well: as a replacement for interactive command-driven graphics generation, something that\nMATLAB does very well. The MATLAB design makes the simple task of loading a data file and plotting\nvery straightforward, where a full object-oriented API would be too syntactically heavy. So matplotlib also\nprovides a stateful scripting interface for quick and easy generation of graphics similar to MATLAB's.\nBecause matplotlib is a library, users have access to all of the rich built-in Python data structures such\nas lists, dictionaries, sets and more.\nmatplotlib\nJohn Hunter and Michael Droettboom\n", "page": 119, "type": "text", "section": "Page 119"}
{"text": "Figure 11.1: The original matplotlib application: an ECoG viewer\n11.2. Overview of matplotlib Architecture\nThe top-level matplotlib object that contains and manages all of the elements in a given graphic is called\nthe Figure . One of the core architectural tasks matplotlib must solve is implementing a framework for\nrepresenting and manipulating the Figure  that is segregated from the act of rendering the Figure  to\na user interface window or hardcopy. This enables us to build increasingly sophisticated features and\nlogic into the Figure s, while keeping the \"backends\", or output devices, relatively simple. matplotlib\nencapsulates not just the drawing interfaces to allow rendering to multiple devices, but also the basic\nevent handling and windowing of most popular user interface toolkits. Because of this, users can create\nfairly rich interactive graphics and toolkits incorporating mouse and keyboard input that can be plugged\nwithout modification into the six user interface toolkits we support.\nThe architecture to accomplish this is logically separated into three layers, which can be viewed as a\nstack. Each layer that sits above another layer knows how to talk to the layer below it, but the lower\nlayer is not aware of the layers above it. The three layers from bottom to top are: backend, artist, and\nscripting.\nBackend Layer\nAt the bottom of the stack is the backend layer, which provides concrete implementations of the abstract\ninterface classes:\nFigureCanvas  encapsulates the concept of a surface to draw onto (e.g. \"the paper\").\nRenderer  does the drawing (e.g. \"the paintbrush\").\nEvent  handles user inputs such as keyboard and mouse events.\nFor a user interface toolkit such as Qt, the FigureCanvas  has a concrete implementation which knows\nhow to insert itself into a native Qt window ( QtGui.QMainWindow ), transfer the matplotlib Renderer\ncommands onto the canvas ( QtGui.QPainter ), and translate native Qt events into the matplotlib\nEvent  framework, which signals the callback dispatcher to generate the events so upstream listeners\ncan handle them. The abstract base classes reside in matplotlib.backend_bases  and all of the\nderived classes live in dedicated modules like matplotlib.backends.backend_qt4agg . For a pure\nimage backend dedicated to producing hardcopy output like PDF, PNG, SVG, or PS, the\nFigureCanvas  implementation might simply set up a file-like object into which the default headers,\nfonts, and macro functions are defined, as well as the individual objects (lines, text, rectangles, etc.) that\nthe Renderer  creates.\nThe job of the Renderer  is to provide a low-level drawing interface for putting ink onto the canvas. As\nmentioned above, the original matplotlib application was an ECoG viewer in a GTK+ application, and\nmuch of the original design was inspired by the GDK/GTK+ API available at that time. The original\nRenderer  API was motivated by the GDK Drawable  interface, which implements such primitive\nmethods as draw_point , draw_line , draw_rectangle , draw_image , draw_polygon , and\ndraw_glyphs . Each additional backend we implemented\u2014the earliest were the PostScript backend and\nDrawable\n", "page": 120, "type": "text", "section": "Page 120"}
{"text": "the GD backend\u2014implemented the GDK \n API and translated these into native backend-\ndependent drawing commands. As we discuss below, this unnecessarily complicated the implementation\nof new backends with a large proliferation of methods, and this API has subsequently been dramatically\nsimplified, resulting in a simple process for porting matplotlib to a new user interface toolkit or file\nspecification.\nOne of the design decisions that has worked quite well for matplotlib is support for a core pixel-based\nrenderer using the C++ template library Anti-Grain Geometry or \"agg\" [She06]. This is a high-\nperformance library for rendering anti-aliased 2D graphics that produces attractive images. matplotlib\nprovides support for inserting pixel buffers rendered by the agg backend into each user interface toolkit\nwe support, so one can get pixel-exact graphics across UIs and operating systems. Because the PNG\noutput matplotlib produces also uses the agg renderer, the hardcopy is identical to the screen display, so\nwhat you see is what you get across UIs, operating systems and PNG output.\nThe matplotlib Event  framework maps underlying UI events like key-press-event  or mouse-\nmotion-event  to the matplotlib classes KeyEvent  or MouseEvent . Users can connect to these\nevents to callback functions and interact with their figure and data; for example, to pick  a data point or\ngroup of points, or manipulate some aspect of the figure or its constituents. The following code sample\nillustrates how to toggle all of the lines in an Axes  window when the user types `t'.\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef on_press(event):\n    if event.inaxes is None: return\n    for line in event.inaxes.lines:\n        if event.key=='t':\n            visible = line.get_visible()\n            line.set_visible(not visible)\n    event.inaxes.figure.canvas.draw()\nfig, ax = plt.subplots(1)\nfig.canvas.mpl_connect('key_press_event', on_press)\nax.plot(np.random.rand(2, 20))\nplt.show()\nThe abstraction of the underlying UI toolkit's event framework allows both matplotlib developers and end-\nusers to write UI event-handling code in a \"write once run everywhere\" fashion. For example, the\ninteractive panning and zooming of matplotlib figures that works across all user interface toolkits is\nimplemented in the matplotlib event framework.\nArtist Layer\nThe Artist  hierarchy is the middle layer of the matplotlib stack, and is the place where much of the\nheavy lifting happens. Continuing with the analogy that the FigureCanvas  from the backend is the\npaper, the Artist  is the object that knows how to take the Renderer  (the paintbrush) and put ink on\nthe canvas. Everything you see in a matplotlib Figure  is an Artist  instance; the title, the lines, the\ntick labels, the images, and so on all correspond to individual Artist  instances (see Figure 11.3). The\nbase class is matplotlib.artist.Artist , which contains attributes that every Artist  shares: the\ntransformation which translates the artist coordinate system to the canvas coordinate system (discussed\nin more detail below), the visibility, the clip box which defines the region the artist can paint into, the\nlabel, and the interface to handle user interaction such as \"picking\"; that is, detecting when a mouse click\nhappens over the artist.\n", "page": 121, "type": "text", "section": "Page 121"}
{"text": "Figure 11.2: A figure\nFigure 11.3: The hierarchy of artist instances used to draw Figure 11.2.\nThe coupling between the Artist  hierarchy and the backend happens in the draw  method. For\nexample, in the mockup class below where we create SomeArtist  which subclasses Artist , the\nessential method that SomeArtist  must implement is draw , which is passed a renderer from the\nbackend. The Artist  doesn't know what kind of backend the renderer is going to draw onto (PDF,\nSVG, GTK+ DrawingArea, etc.) but it does know the Renderer  API and will call the appropriate method\n( draw_text  or draw_path ). Since the Renderer  has a pointer to its canvas and knows how to paint\nonto it, the draw  method transforms the abstract representation of the Artist  to colors in a pixel\nbuffer, paths in an SVG file, or any other concrete representation.\nclass SomeArtist(Artist):\n    'An example Artist that implements the draw method'\n    def draw(self, renderer):\n        \"\"\"Call the appropriate renderer methods to paint self onto canvas\"\"\"\n        if not self.get_visible():  return\n        # create some objects and use renderer to draw self here\n        renderer.draw_path(graphics_context, path, transform)\nThere are two types of Artist s in the hierarchy. Primitive artists represent the kinds of objects you see\nin a plot: Line2D , Rectangle , Circle , and Text . Composite artists are collections of Artist s\nsuch as the Axis , Tick , Axes , and Figure . Each composite artist may contain other composite\n", "page": 122, "type": "text", "section": "Page 122"}
{"text": "artists as well as primitive artists. For example, the Figure  contains one or more composite Axes  and\nthe background of the Figure  is a primitive Rectangle .\nThe most important composite artist is the Axes , which is where most of the matplotlib API plotting\nmethods are defined. Not only does the Axes  contain most of the graphical elements that make up the\nbackground of the plot\u2014the ticks, the axis lines, the grid, the patch of color which is the plot background\n\u2014it contains numerous helper methods that create primitive artists and add them to the Axes  instance.\nFor example, Table 11.1 shows a small sampling of Axes  methods that create plot objects and store\nthem in the Axes  instance.\nmethod\ncreates\nstored in\nAxes.imshow\none or more matplotlib.image.AxesImage s\nAxes.images\nAxes.hist\nmany matplotlib.patch.Rectangle s\nAxes.patches\nAxes.plot\none or more matplotlib.lines.Line2D s\nAxes.lines\nTable 11.1: Sampling of Axes  methods and the Artist  instances they create\nBelow is a simple Python script illustrating the architecture above. It defines the backend, connects a\nFigure  to it, uses the array library numpy  to create 10,000 normally distributed random numbers, and\nplots a histogram of these.\n# Import the FigureCanvas from the backend of your choice\n#  and attach the Figure artist to it.\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\nfrom matplotlib.figure import Figure\nfig = Figure()\ncanvas = FigureCanvas(fig)\n# Import the numpy library to generate the random numbers.\nimport numpy as np\nx = np.random.randn(10000)\n# Now use a figure method to create an Axes artist; the Axes artist is\n#  added automatically to the figure container fig.axes.\n# Here \"111\" is from the MATLAB convention: create a grid with 1 row and 1\n#  column, and use the first cell in that grid for the location of the new\n#  Axes.\nax = fig.add_subplot(111)\n# Call the Axes method hist to generate the histogram; hist creates a\n#  sequence of Rectangle artists for each histogram bar and adds them\n#  to the Axes container.  Here \"100\" means create 100 bins.\nax.hist(x, 100)\n# Decorate the figure with a title and save it.\nax.set_title('Normal distribution with $\\mu=0, \\sigma=1$')\nfig.savefig('matplotlib_histogram.png')\nScripting Layer (pyplot)\nThe script using the API above works very well, especially for programmers, and is usually the\nappropriate programming paradigm when writing a web application server, a UI application, or perhaps a\nscript to be shared with other developers. For everyday purposes, particularly for interactive exploratory\nwork by bench scientists who are not professional programmers, it is a bit syntactically heavy. Most\nspecial-purpose languages for data analysis and visualization provide a lighter scripting interface to\nsimplify common tasks, and matplotlib does so as well in its matplotlib.pyplot  interface. The same\ncode above, using pyplot , reads\nimport matplotlib.pyplot as plt\nimport numpy as np\nx = np.random.randn(10000)\nplt.hist(x, 100)\nplt.title(r'Normal distribution with $\\mu=0, \\sigma=1$')\nplt.savefig('matplotlib_histogram.png')\nplt.show()\n", "page": 123, "type": "text", "section": "Page 123"}
{"text": "Figure 11.4: A histogram created using pyplot\npyplot  is a stateful interface that handles much of the boilerplate for creating figures and axes and\nconnecting them to the backend of your choice, and maintains module-level internal data structures\nrepresenting the current figure and axes to which to direct plotting commands.\nLet's dissect the important lines in the script to see how this internal state is managed.\nimport matplotlib.pyplot as plt : When the pyplot  module is loaded, it parses a local\nconfiguration file in which the user states, among many other things, their preference for a default\nbackend. This might be a user interface backend like QtAgg , in which case the script above will\nimport the GUI framework and launch a Qt window with the plot embedded, or it might be a pure\nimage backend like Agg , in which case the script will generate the hard-copy output and exit.\nplt.hist(x, 100) : This is the first plotting command in the script. pyplot  will check its internal\ndata structures to see if there is a current Figure  instance. If so, it will extract the current Axes\nand direct plotting to the Axes.hist  API call. In this case there is none, so it will create a Figure\nand Axes , set these as current, and direct the plotting to Axes.hist .\nplt.title(r'Normal distribution with $\\mu=0, \\sigma=1$') : As above, pyplot will look to\nsee if there is a current Figure  and Axes . Finding that there is, it will not create new instances\nbut will direct the call to the existing Axes  instance method Axes.set_title .\nplt.show() : This will force the Figure  to render, and if the user has indicated a default GUI\nbackend in their configuration file, will start the GUI mainloop and raise any figures created to the\nscreen.\nA somewhat stripped-down and simplified version of pyplot 's frequently used line plotting function\nmatplotlib.pyplot.plot  is shown below to illustrate how a pyplot function wraps functionality in\nmatplotlib's object-oriented core. All other pyplot  scripting interface functions follow the same design.\n@autogen_docstring(Axes.plot)\ndef plot(*args, **kwargs):\n    ax = gca()\n    ret = ax.plot(*args, **kwargs)\n    draw_if_interactive()\n    return ret\nThe Python decorator @autogen_docstring(Axes.plot)  extracts the documentation string from the\ncorresponding API method and attaches a properly formatted version to the pyplot.plot  method; we\nhave a dedicated module matplotlib.docstring  to handle this docstring magic. The *args  and\n**kwargs  in the documentation signature are special conventions in Python to mean all the arguments\nand keyword arguments that are passed to the method. This allows us to forward them on to the\ncorresponding API method. The call ax = gca()  invokes the stateful machinery to \"get current Axes\"\n(each Python interpreter can have only one \"current axes\"), and will create the Figure  and Axes  if\n", "page": 124, "type": "text", "section": "Page 124"}
{"text": "necessary. The call to ret = ax.plot(*args, **kwargs)  forwards the function call and its arguments\nto the appropriate Axes  method, and stores the return value to be returned later. Thus the pyplot\ninterface is a fairly thin wrapper around the core Artist  API which tries to avoid as much code\nduplication as possible by exposing the API function, call signature and docstring in the scripting interface\nwith a minimal amount of boilerplate code.\n11.3. Backend Refactoring\nOver time, the drawing API of the output backends grew a large number of methods, including:\ndraw_arc, draw_image, draw_line_collection, draw_line, draw_lines, draw_point,\ndraw_quad_mesh, draw_polygon_collection, draw_polygon, draw_rectangle,\ndraw_regpoly_collection\nUnfortunately, having more backend methods meant it took much longer to write a new backend, and as\nnew features were added to the core, updating the existing backends took considerable work. Since each\nof the backends was implemented by a single developer who was expert in a particular output file format,\nit sometimes took a long time for a new feature to arrive in all of the backends, causing confusion for the\nuser about which features were available where.\nFor matplotlib version 0.98, the backends were refactored to require only the minimum necessary\nfunctionality in the backends themselves, with everything else moved into the core. The number of\nrequired methods in the backend API was reduced considerably, to only:\ndraw_path : Draws compound polygons, made up of line and B\u00e9ezier segments. This interfaces\nreplaces many of the old methods: draw_arc , draw_line , draw_lines , and\ndraw_rectangle .\ndraw_image : Draws raster images.\ndraw_text : Draws text with the given font properties.\nget_text_width_height_descent : Given a string of text, return its metrics.\nIt's possible to implement all of the drawing necessary for a new backend using only these methods. (We\ncould also go one step further and draw text using draw_path , removing the need for the draw_text\nmethod, but we haven't gotten around to making that simplification. Of course, a backend would still be\nfree to implement its own draw_text  method to output \"real\" text.) This is useful for getting a new\nbackend up and running more easily. However, in some cases, a backend may want to override the\nbehavior of the core in order to create more efficient output. For example, when drawing markers (small\nsymbols used to indicate the vertices in a line plot), it is more space-efficient to write the marker's shape\nonly once to the file, and then repeat it as a \"stamp\" everywhere it is used. In that case, the backend can\nimplement a draw_markers  method. If it's implemented, the backend writes out the marker shape once\nand then writes out a much shorter command to reuse it in a number of locations. If it's not implemented,\nthe core simply draws the marker multiple times using multiple calls to draw_path .\nThe full list of optional backend API methods is:\ndraw_markers : Draws a set of markers.\ndraw_path_collection : Draws a collection of paths.\ndraw_quad_mesh : Draws a quadrilateral mesh.\n11.4. Transforms\nmatplotlib spends a lot of time transforming coordinates from one system to another. These coordinate\nsystems include:\ndata: the original raw data values\naxes: the space defined by a particular axes rectangle\nfigure: the space containing the entire figure\ndisplay: the physical coordinates used in the output (e.g. points in PostScript, pixels in PNG)\nEvery Artist  has a transformation node that knows how to transform from one coordinate system to\nanother. These transformation nodes are connected together in a directed graph, where each node is\ndependent on its parent. By following the edges to the root of the graph, coordinates in data space can\nbe transformed all the way to coordinates in the final output file. Most transformations are invertible, as\nwell. This makes it possible to click on an element of the plot and return its coordinate in data space. The\ntransform graph sets up dependencies between transformation nodes: when a parent node's\ntransformation changes, such as when an Axes 's limits are changed, any transformations related to that\nAxes  are invalidated since they will need to be redrawn. Transformations related to other Axes  in the\nfigure, of course, may be left alone, preventing unnecessary recomputations and contributing to better\ninteractive performance.\nTransform nodes may be either simple affine transformations and non-affine transformations. Affine\ntransformations are the family of transformations that preserve straight lines and ratios of distances,\nincluding rotation, translation, scale and skew. Two-dimensional affine transformations are represented\nusing a 3\u00d73 affine transformation matrix. The transformed point (x', y') is obtained by matrix-multiplying\nthe original point (x, y) by this matrix:\n", "page": 125, "type": "text", "section": "Page 125"}
{"text": "Two-dimensional coordinates can then easily be transformed by simply multiplying them by the\ntransformation matrix. Affine transformations also have the useful property that they can be composed\ntogether using matrix multiplication. This means that to perform a series of affine transformations, the\ntransformation matrices can first be multiplied together only once, and the resulting matrix can be used to\ntransform coordinates. matplotlib's transformation framework automatically composes (freezes) affine\ntransformation matrices together before transforming coordinates to reduce the amount of computation.\nHaving fast affine transformations is important, because it makes interactive panning and zooming in a\nGUI window more efficient.\nNon-affine transformations in matplotlib are defined using Python functions, so they are truly arbitrary.\nWithin the matplotlib core, non-affine transformations are used for logarithmic scaling, polar plots and\ngeographical projections (Figure 11.5). These non-affine transformations can be freely mixed with affine\nones in the transformation graph. matplotlib will automatically simplify the affine portion and only fall back\nto the arbitrary functions for the non-affine portion.\nFigure 11.5: The same data plotted with three different non-affine transformations: logarithmic, polar and\nLambert\nFrom these simple pieces, matplotlib can do some pretty advanced things. A blended transformation is a\nspecial transformation node that uses one transformation for the x axis and another for the y axis. This is\nof course only possible if the given transformations are \"separable\", meaning the x and y coordinates are\nindependent, but the transformations themselves may be either affine or non-affine. This is used, for\nexample, to plot logarithmic plots where either or both of the x and y axes may have a logarithmic scale.\nHaving a blended transformation node allow the available scales to be combined in arbitrary ways.\nAnother thing the transform graph allows is the sharing of axes. It is possible to \"link\" the limits of one\nplot to another and ensure that when one is panned or zoomed, the other is updated to match. In this\ncase, the same transform node is simply shared between two axes, which may even be on two different\nfigures. Figure 11.6 shows an example transformation graph with some of these advanced features at\nwork. axes1 has a logarithmic x axis; axes1 and axes2 share the same y axis.\nFigure 11.6: An example transformation graph\n11.5. The Polyline Pipeline\nWhen plotting line plots, there are a number of steps that are performed to get from the raw data to the\nline drawn on screen. In an earlier version of matplotlib, all of these steps were tangled together. They\nhave since been refactored so they are discrete steps in a \"path conversion\" pipeline. This allows each\nbackend to choose which parts of the pipeline to perform, since some are only useful in certain contexts.\nTransformation: The coordinates are transformed from data coordinates to figure coordinates. If this\nis a purely affine transformation, as described above, this is as simple as a matrix multiplication. If\nthis involves arbitrary transformations, transformation functions are called to transform the\ncoordinates into figure space.\nHandle missing data: The data array may have portions where the data is missing or invalid. The\nuser may indicate this either by setting those values to NaN , or using numpy  masked arrays.\nVector output formats, such as PDF, and rendering libraries, such as Agg, do not often have a\nconcept of missing data when plotting a polyline, so this step of the pipeline must skip over the\n", "page": 126, "type": "text", "section": "Page 126"}
{"text": "missing data segments using MOVETO  commands, which tell the renderer to pick up the pen and\nbegin drawing again at a new point.\nClipping: Points outside of the boundaries of the figure can increase the file size by including many\ninvisible points. More importantly, very large or very small coordinate values can cause overflow\nerrors in the rendering of the output file, which results in completely garbled output. This step of the\npipeline clips the polyline as it enters and exits the edges of the figure to prevent both of these\nproblems.\nSnapping: Perfectly vertical and horizontal lines can look fuzzy due to antialiasing when their centers\nare not aligned to the center of a pixel (see Figure 11.7). The snapping step of the pipeline first\ndetermines whether the entire polyline is made up of horizontal and vertical segments (such as an\naxis-aligned rectangle), and if so, rounds each resulting vertex to the nearest pixel center. This step\nis only used for raster backends, since vector backends should continue to have exact data points.\nSome renderers of vector file formats, such as Adobe Acrobat, perform pixel snapping when viewed\non screen.\nFigure 11.7: A close-up view of the effect of pixel snapping. On the left, without pixel snapping; on\nthe right, with pixel snapping.\nSimplification: When plotting really dense plots, many of the points on the line may not actually be\nvisible. This is particularly true of plots representing a noisy waveform. Including these points in the\nplot increases file size, and may even hit limits on the number of points allowed in the file format.\nTherefore, any points that lie exactly on the line between their two neighboring points are removed\n(see Figure 11.8). The determination depends on a threshold based on what would be visible at a\ngiven resolution specified by the user.\nFigure 11.8: The figure on the right is a close-up of the figure on the left. The circled vertex is\nautomatically removed by the path simplification algorithm, since it lies exactly on the line between\nits neighboring vertices, and therefore is redundant.\n11.6. Math Text\nSince the users of matplotlib are often scientists, it is useful to put richly formatted math expressions\ndirectly on the plot. Perhaps the most widely used syntax for math expressions is from Donald Knuth's\nTeX typesetting system. It's a way to turn input in a plain-text language like this:\n\\sqrt{\\frac{\\delta x}{\\delta y}}\ninto a properly formatted math expression.\nmatplotlib provides two ways to render math expressions. The first, usetex , uses a full copy of TeX on\nthe user's machine to render the math expression. TeX outputs the location of the characters and lines in\nthe expression in its native DVI (device independent) format. matplotlib then parses the DVI file and\nconverts it to a set of drawing commands that one of its output backends then renders directly onto the\nplot. This approach handles a great deal of obscure math syntax. However, it requires that the user have\na full and working installation of TeX. Therefore, matplotlib also includes its own internal math rendering\nengine, called mathtext .\nmathtext  is a direct port of the TeX math-rendering engine, glued onto a much simpler parser written\nusing the pyparsing  [McG07] parsing framework. This port was written based on the published copy of\nthe TeX source code [Knu86]. The simple parser builds up a tree of boxes and glue (in TeX\nnomenclature), that are then laid out by the layout engine. While the complete TeX math rendering\nengine is included, the large set of third-party TeX and LaTeX math libraries is not. Features in such\nlibraries are ported on an as-needed basis, with an emphasis on frequently used and non-discipline-\nspecific features first. This makes for a nice, lightweight way to render most math expressions.\n11.7. Regression Testing\n", "page": 127, "type": "text", "section": "Page 127"}
{"text": "Historically, matplotlib has not had a large number of low-level unit tests. Occasionally, if a serious bug\nwas reported, a script to reproduce it would be added to a directory of such files in the source tree. The\nlack of automated tests created all of the usual problems, most importantly regressions in features that\npreviously worked. (We probably don't need to sell you on the idea that automated testing is a good\nthing.) Of course, with so much code and so many configuration options and interchangeable pieces\n(e.g., the backends), it is arguable that low-level unit tests alone would ever be enough; instead we've\nfollowed the belief that it is most cost-effective to test all of the pieces working together in concert.\nTo this end, as a first effort, a script was written that generated a number of plots exercising various\nfeatures of matplotlib, particularly those that were hard to get right. This made it a little easier to detect\nwhen a new change caused inadvertent breakage, but the correctness of the images still needed to be\nverified by hand. Since this required a lot of manual effort, it wasn't done very often.\nAs a second pass, this general approach was automated. The current matplotlib testing script generates\na number of plots, but instead of requiring manual intervention, those plots are automatically compared to\nbaseline images. All of the tests are run inside of the nose testing framework, which makes it very easy\nto generate a report of which tests failed.\nComplicating matters is that the image comparison cannot be exact. Subtle changes in versions of the\nFreetype font-rendering library can make the output of text slightly different across different machines.\nThese differences are not enough to be considered \"wrong\", but are enough to throw off any exact bit-\nfor-bit comparison. Instead, the testing framework computes the histogram of both images, and\ncalculates the root-mean-square of their difference. If that difference is greater than a given threshold,\nthe images are considered too different and the comparison test fails. When tests fail, difference images\nare generated which show where on the plot a change has occurred (see Figure 11.9). The developer\ncan then decide whether the failure is due to an intentional change and update the baseline image to\nmatch the new image, or decide the image is in fact incorrect and track down and fix the bug that caused\nthe change.\nFigure 11.9: A regression test image comparison. From left to right: a) The expected image, b) the result\nof broken legend placement, c) the difference between the two images.\nSince different backends can contribute different bugs, the testing framework tests multiple backends for\neach plot: PNG, PDF and SVG. For the vector formats, we don't compare the vector information directly,\nsince there are multiple ways to represent something that has the same end result when rasterized. The\nvector backends should be free to change the specifics of their output to increase efficiency without\ncausing all of the tests to fail. Therefore, for vector backends, the testing framework first renders the file\nto a raster using an external tool (Ghostscript for PDF and Inkscape for SVG) and then uses those\nrasters for comparison.\nUsing this approach, we were able to bootstrap a reasonably effective testing framework from scratch\nmore easily than if we had gone on to write many low-level unit tests. Still, it is not perfect; the code\ncoverage of the tests is not very complete, and it takes a long time to run all of the tests. (Around 15\nminutes on a 2.33 GHz Intel Core 2 E6550.) Therefore, some regressions do still fall through the cracks,\nbut overall the quality of the releases has improved considerably since the testing framework was\nimplemented.\n11.8. Lessons Learned\nOne of the important lessons from the development of matplotlib is, as Le Corbusier said, \"Good\narchitects borrow\". The early authors of matplotlib were largely scientists, self-taught programmers trying\nto get their work done, not formally trained computer scientists. Thus we did not get the internal design\nright on the first try. The decision to implement a user-facing scripting layer largely compatible with the\nMATLAB API benefited the project in three significant ways: it provided a time-tested interface to create\nand customize graphics, it made for an easy transition to matplotlib from the large base of MATLAB\nusers, and\u2014most importantly for us in the context of matplotlib architecture\u2014it freed developers to\nrefactor the internal object-oriented API several times with minimal impact to most users because the\nscripting interface was unchanged. While we have had API users (as opposed to scripting users) from the\noutset, most of them are power users or developers able to adapt to API changes. The scripting users,\non the other hand, can write code once and pretty much assume it is stable for all subsequent releases.\nFor the internal drawing API, while we did borrow from GDK, we did not spend enough effort determining\nwhether this was the right drawing API, and had to expend considerable effort subsequently after many\nbackends were written around this API to extend the functionality around a simpler and more flexible\ndrawing API. We would have been well-served by adopting the PDF drawing specification [Ent11b],\nwhich itself was developed from decades of experience Adobe had with its PostScript specification; it\n", "page": 128, "type": "text", "section": "Page 128"}
{"text": "This work is made available under the Creative Commons Attribution 3.0 Unported license. Please see\nthe full description of the license for details.\nwould have given us mostly out-of-the-box compatibility with PDF itself, the Quartz Core Graphics\nframework, and the Enthought Enable Kiva drawing kit [Ent11a].\nOne of the curses of Python is that it is such an easy and expressive language that developers often find\nit easier to re-invent and re-implement functionality that exists in other packages than work to integrate\ncode from other packages. matplotlib could have benefited in early development from expending more\neffort on integration with existing modules and APIs such as Enthought's Kiva and Enable toolkits which\nsolve many similar problems, rather than reinventing functionality. Integration with existing functionality is,\nhowever, a double edge sword, as it can make builds and releases more complex and reduce flexibility in\ninternal development.\nBack to top\nBack to The Architecture of Open Source Applications.\n", "page": 129, "type": "text", "section": "Page 129"}
{"text": "<>\nFrom the start, MediaWiki was developed specifically to be Wikipedia's software. Developers have worked\nto facilitate reuse by third-party users, but Wikipedia's influence and bias have shaped MediaWiki's\narchitecture throughout its history.\nWikipedia is one of the top ten websites in the world, currently getting about 400 million unique visitors a\nmonth. It gets over 100,000 hits per second. Wikipedia isn't commercially supported by ads; it is entirely\nsupported by a non-profit organization, the Wikimedia Foundation, which relies on donations as its\nprimary funding model. This means that MediaWiki must not only run a top-ten website, but also do so\non a shoestring budget. To meet these demands, MediaWiki has a heavy bias towards performance,\ncaching and optimization. Expensive features that can't be enabled on Wikipedia are either reverted or\ndisabled through a configuration variable; there is an endless balance between performance and\nfeatures.\nThe influence of Wikipedia on MediaWiki's architecture isn't limited to performance. Unlike generic content\nmanagement systems (CMSes), MediaWiki was originally written for a very specific purpose: supporting a\ncommunity that creates and curates freely reusable knowledge on an open platform. This means, for\nexample, that MediaWiki doesn't include regular features found in corporate CMSes, like a publication\nworkflow or access control lists, but does offer a variety of tools to handle spam and vandalism.\nSo, from the start, the needs and actions of a constantly evolving community of Wikipedia participants\nhave affected MediaWiki's development, and vice versa. The architecture of MediaWiki has been driven\nmany times by initiatives started or requested by the community, such as the creation of Wikimedia\nCommons, or the Flagged Revisions feature. Developers made major architectural changes because the\nway that MediaWiki was used by Wikipedians made it necessary.\nMediaWiki has also gained a solid external user base by being open source software from the beginning.\nThird-party reusers know that, as long as such a high-profile website as Wikipedia uses MediaWiki, the\nsoftware will be maintained and improved. MediaWiki used to be really focused on Wikimedia sites, but\nefforts have been made to make it more generic and better accommodate the needs of these third-party\nusers. For example, MediaWiki now ships with an excellent web-based installer, making the installation\nprocess much less painful than when everything had to be done via the command line and the software\ncontained hardcoded paths for Wikipedia.\nStill, MediaWiki is and remains Wikipedia's software, and this shows throughout its history and\narchitecture.\nThis chapter is organized as follows:\nHistorical Overview gives a short overview of the history of MediaWiki, or rather its prehistory, and\nthe circumstances of its creation.\nMediaWiki Code Base and Practices explains the choice of PHP, the importance and implementation\nof secure code, and how general configuration is handled.\nDatabase and Text Storage dives into the distributed data storage system, and how its structure\nevolved to accommodate growth.\nRequests, Caching and Delivery follows the execution of a web request through the components of\nMediaWiki it activates. This section includes a description of the different caching layers, and the\nasset delivery system.\nLanguages details the pervasive internationalization and localization system, why it matters, and how\nit is implemented.\nUsers presents how users are represented in the software, and how user permissions work.\nContent details how content is structured, formatted and processed to generate the final HTML. A\nsubsection focuses on how MediaWiki handles media files.\nCustomizing and Extending MediaWiki explains how JavaScript, CSS, extensions, and skins can be\nused to customize a wiki, and how they modify its appearance and behavior. A subsection presents\nthe software's machine-readable web API.\nMediaWiki\nSumana Harihareswara and Guillaume\nPaumier\n", "page": 130, "type": "text", "section": "Page 130"}
{"text": "12.1. Historical Overview\nPhase I: UseModWiki\nWikipedia was launched in January 2001. At the time, it was mostly an experiment to try to boost the\nproduction of content for Nupedia, a free-content, but peer-reviewed, encyclopedia created by Jimmy\nWales. Because it was an experiment, Wikipedia was originally powered by UseModWiki, an existing\nGPL wiki engine written in Perl, using CamelCase and storing all pages in individual text files with no\nhistory of changes made.\nIt soon appeared that CamelCase wasn't really appropriate for naming encyclopedia articles. In late\nJanuary 2001, UseModWiki developer and Wikipedia participant Clifford Adams added a new feature to\nUseModWiki: free links; i.e., the ability to link to pages with a special syntax (double square brackets),\ninstead of automatic CamelCase linking. A few weeks later, Wikipedia upgraded to the new version of\nUseModWiki supporting free links, and enabled them.\nWhile this initial phase isn't about MediaWiki per se, it provides some context and shows that, even\nbefore MediaWiki was created, Wikipedia started to shape the features of the software that powered it.\nUseModWiki also influenced some of MediaWiki's features; for example, its markup language. The\nNostalgia Wikipedia contains a complete copy of the Wikipedia database from December 2001, when\nWikipedia still used UseModWiki.\nPhase II: The PHP Script\nIn 2001, Wikipedia was not yet a top ten website; it was an obscure project sitting in a dark corner of the\nInterwebs, unknown to most search engines, and hosted on a single server. Still, performance was\nalready an issue, notably because UseModWiki stored its content in a flat file database. At the time,\nWikipedians were worried about being inundated with traffic following articles in the New York Times,\nSlashdot and Wired.\nSo in summer 2001, Wikipedia participant Magnus Manske (then a university student) started to work on\na dedicated Wikipedia wiki engine in his free time. He aimed to improve Wikipedia's performance using a\ndatabase-driven app, and to develop Wikipedia-specific features that couldn't be provided by a \"generic\"\nwiki engine. Written in PHP and MySQL-backed, the new engine was simply called the \"PHP script\",\n\"PHP wiki\", \"Wikipedia software\" or \"phase II\".\nThe PHP script was made available in August 2001, shared on SourceForge in September, and tested\nuntil late 2001. As Wikipedia suffered from recurring performance issues because of increasing traffic, the\nEnglish language Wikipedia eventually switched from UseModWiki to the PHP script in January 2002.\nOther language versions also created in 2001 were slowly upgraded as well, although some of them\nwould remain powered by UseModWiki until 2004.\nAs PHP software using a MySQL database, the PHP script was the first iteration of what would later\nbecome MediaWiki. It introduced many critical features still in use today, like namespaces to organize\ncontent (including talk pages), skins, and special pages (including maintenance reports, a contributions\nlist and a user watchlist).\nPhase III: MediaWiki\nDespite the improvements from the PHP script and database backend, the combination of increasing\ntraffic, expensive features and limited hardware continued to cause performance issues on Wikipedia. In\n2002, Lee Daniel Crocker rewrote the code again, calling the new software \"Phase III\"\n(http://article.gmane.org/gmane.science.linguistics.wikipedia.technical/2794). Because the site was\nexperiencing frequent difficulties, Lee thought there \"wasn't much time to sit down and properly architect\nand develop a solution\", so he \"just reorganized the existing architecture for better performance and\nhacked all the code\". Profiling features were added to track down slow functions.\nThe Phase III software kept the same basic interface, and was designed to look and behave as much like\nthe Phase II software as possible. A few new features were also added, like a new file upload system,\nside-by-side diffs of content changes, and interwiki links.\nOther features were added over 2002, like new maintenance special pages, and the \"edit on double\nclick\" option. Performance issues quickly reappeared, though. For example, in November 2002,\nadministrators had to temporarily disable the \"view count\" and \"site\" statistics which were causing two\ndatabase writes on every page view. They would also occasionally switch the site to read-only mode to\nmaintain the service for readers, and disable expensive maintenance pages during high-access times\nbecause of table locking problems.\nIn early 2003, developers discussed whether they should properly re-engineer and re-architect the\nsoftware from scratch, before the fire-fighting became unmanageable, or continue to tweak and improve\nthe existing code base. They chose the latter solution, mostly because most developers were sufficiently\nhappy with the code base, and confident enough that further iterative improvements would be enough to\nkeep up with the growth of the site.\nIn June 2003, administrators added a second server, the first database server separate from the web\nserver. (The new machine was also the web server for non-English Wikipedia sites.) Load-balancing\nbetween the two servers would be set up later that year. Admins also enabled a new page-caching\nsystem that used the file system to cache rendered, ready-to-output pages for anonymous users.\n", "page": 131, "type": "text", "section": "Page 131"}
{"text": "June 2003 is also when Jimmy Wales created the non-profit Wikimedia Foundation to support Wikipedia\nand manage its infrastructure and day-to-day operations. The \"Wikipedia software\" was officially named\n\"MediaWiki\" in July, as wordplay on the Wikimedia Foundation's name. What was thought at the time to\nbe a clever pun would confuse generations of users and developers.\nNew features were added in July, like the automatically generated table of contents and the ability to edit\npage sections, both still in use today. The first release under the name \"MediaWiki\" happened in August\n2003, concluding the long genesis of an application whose overall structure would remain fairly stable\nfrom there on.\n12.2. MediaWiki Code Base and Practices\nPHP\nPHP was chosen as the framework for Wikipedia's \"Phase II\" software in 2001; MediaWiki has grown\norganically since then, and is still evolving. Most MediaWiki developers are volunteers contributing in their\nfree time, and there were very few of them in the early years. Some software design decisions or\nomissions may seem wrong in retrospect, but it's hard to criticize the founders for not implementing some\nabstraction which is now found to be critical, when the initial code base was so small, and the time taken\nto develop it so short.\nFor example, MediaWiki uses unprefixed class names, which can cause conflicts when PHP core and\nPECL (PHP Extension Community Library) developers add new classes: MediaWiki Namespace  class\nhad to be renamed to MWNamespace  to be compatible with PHP 5.3. Consistently using a prefix for all\nclasses (e.g., \" MW \") would have made it easier to embed MediaWiki inside another application or library.\nRelying on PHP was probably not the best choice for performance, since it has not benefitted from\nimprovements that some other dynamic languages have seen. Using Java would have been much better\nfor performance, and simplified execution scaling for back-end maintenance tasks. On the other hand,\nPHP is very popular, which facilitates recruiting new developers.\nEven if MediaWiki still contains \"ugly\" legacy code, major improvements have been made over the years,\nand new architectural elements have been introduced to MediaWiki throughout its history. They include\nthe Parser , SpecialPage , and Database  classes, the Image  class and the FileRepo  class\nhierarchy, ResourceLoader, and the Action  hierarchy. MediaWiki started without any of these things,\nbut all of them support features that have been around since the beginning. Many developers are\ninterested primarily in feature development and architecture is often left behind, only to catch up later as\nthe cost of working within an inadequate architecture becomes apparent.\nSecurity\nBecause MediaWiki is the platform for high-profile sites such as Wikipedia, core developers and code\nreviewers have enforced strict security rules. (See the detailed guide.) To make it easier to write secure\ncode, MediaWiki gives developers wrappers around HTML output and database queries to handle\nescaping. To sanitize user input, a develop uses the WebRequest  class, which analyzes data passed in\nthe URL or via a POSTed form. It removes \"magic quotes\" and slashes, strips illegal input characters\nand normalizes Unicode sequences. Cross-site request forgery (CSRF) is avoided by using tokens, and\ncross-site scripting (XSS) by validating inputs and escaping outputs, usually with PHP's\nhtmlspecialchars()  function. MediaWiki also provides (and uses) an XHTML sanitizer with the\nSanitizer  class, and database functions that prevent SQL injection.\nConfiguration\nMediaWiki offers hundreds of configuration settings, stored in global PHP variables. Their default value is\nset in DefaultSettings.php , and the system administrator can override them by editing\nLocalSettings.php .\nMediaWiki used to over-depend on global variables, including for configuration and context processing.\nGlobals cause serious security implications with PHP's register_globals  function (which MediaWiki\nhasn't needed since version 1.2). This system also limits potential abstractions for configuration, and\nmakes it more difficult to optimize the start-up process. Moreover, the configuration namespace is shared\nwith variables used for registration and object context, leading to potential conflicts. From a user\nperspective, global configuration variables have also made MediaWiki seem difficult to configure and\nmaintain. MediaWiki development has been a story of slowly moving context out of global variables and\ninto objects. Storing processing context in object member variables allows those objects to be reused in a\nmuch more flexible way.\n12.3. Database and Text Storage\nMediaWiki has been using a relational database backend since the Phase II software. The default (and\nbest-supported) database management system (DBMS) for MediaWiki is MySQL, which is the one that\nall Wikimedia sites use, but other DBMSes (such as PostgreSQL, Oracle, and SQLite) have community-\nsupported implementations. A sysadmin can choose a DBMS while installing MediaWiki, and MediaWiki\nprovides both a database abstraction and a query abstraction layer that simplify database access for\ndevelopers.\n", "page": 132, "type": "text", "section": "Page 132"}
{"text": "Figure 12.1: Database schema\nThe current layout contains dozens of tables. Many are about the wiki's content (e.g., page ,\nrevision , category , and recentchanges ). Other tables include data about users ( user ,\nuser_groups ), media files ( image , filearchive ), caching ( objectcache , l10n_cache ,\nquerycache ) and internal tools ( job  for the job queue), among others, as shown in Figure 12.2.\n(Complete documentation of the database layout in MediaWiki is available.) Indices and summary tables\nare used extensively in MediaWiki, since SQL queries that scan huge numbers of rows can be very\nexpensive, particularly on Wikimedia sites. Unindexed queries are usually discouraged.\nThe database went through dozens of schema changes over the years, the most notable being the\ndecoupling of text storage and revision tracking in MediaWiki 1.5.\nFigure 12.2: Main content tables in MediaWiki 1.4 and 1.5\nIn the 1.4 model, the content was stored in two important tables, cur  (containing the text and metadata\nof the current revision of the page) and old  (containing previous revisions); deleted pages were kept in\narchive . When an edit was made, the previously current revision was copied to the old  table, and\nthe new edit was saved to cur . When a page was renamed, the page title had to be updated in the\nmetadata of all the old  revisions, which could be a long operation. When a page was deleted, its\nentries in both the cur  and old  tables had to be copied to the archive  table before being deleted;\nthis meant moving the text of all revisions, which could be very large and thus take time.\nIn the 1.5 model, revision metadata and revision text were split: the cur  and old  tables were replaced\nwith page  (pages' metadata), revision  (metadata for all revisions, old or current) and text  (text of\nall revisions, old, current or deleted). Now, when an edit is made, revision metadata don't need to be\ncopied around tables: inserting a new entry and updating the page_latest  pointer is enough. Also, the\n", "page": 133, "type": "text", "section": "Page 133"}
{"text": "revision metadata don't include the page title anymore, only its ID: this removes the need for renaming all\nrevisions when a page is renamed\nThe revision  table stores metadata for each revision, but not their text; instead, they contain a text ID\npointing to the text  table, which contains the actual text. When a page is deleted, the text of all\nrevisions of the page stays there and doesn't need to be moved to another table. The text  table is\ncomposed of a mapping of IDs to text blobs; a flags  field indicates if the text blob is gzipped (for\nspace savings) or if the text blob is only a pointer to external text storage. Wikimedia sites use a MySQL-\nbacked external storage cluster with blobs of a few dozen revisions. The first revision of the blob is\nstored in full, and following revisions to the same page are stored as diffs relative to the previous\nrevision; the blobs are then gzipped. Because the revisions are grouped per page, they tend to be\nsimilar, so the diffs are relatively small and gzip works well. The compression ratio achieved on\nWikimedia sites nears 98%.\nOn the hardware side, MediaWiki has built-in support for load balancing, added as early as 2004 in\nMediaWiki 1.2 (when Wikipedia got its second server\u2014a big deal at the time). The load balancer\n(MediaWiki's PHP code that decides which server to connect to) is now a critical part of Wikimedia's\ninfrastructure, which explains its influence on some algorithm decisions in the code. The system\nadministrator can specify, in MediaWiki's configuration, that there is one master database server and any\nnumber of slave database servers; a weight can be assigned to each server. The load balancer will send\nall writes to the master, and will balance reads according to the weights. It also keeps track of the\nreplication lag of each slave. If a slave's replication lag exceeds 30 seconds, it will not receive any read\nqueries to allow it to catch up; if all slaves are lagged more than 30 seconds, MediaWiki will\nautomatically put itself in read-only mode.\nMediaWiki's \"chronology protector\" ensures that replication lag never causes a user to see a page that\nclaims an action they've just performed hasn't happened yet: for instance, if a user renames a page,\nanother user may still see the old name, but the one who renamed will always see the new name,\nbecause he's the one who renamed it. This is done by storing the master's position in the user's session\nif a request they made resulted in a write query. The next time the user makes a read request, the load\nbalancer reads this position from the session, and tries to select a slave that has caught up to that\nreplication position to serve the request. If none is available, it will wait until one is. It may appear to other\nusers as though the action hasn't happened yet, but the chronology remains consistent for each user.\n12.4. Requests, Caching and Delivery\nExecution Workflow of a Web Request\nindex.php  is the main entry point for MediaWiki, and handles most requests processed by the\napplication servers (i.e., requests that were not served by the caching infrastructure; see below). The\ncode executed from index.php  performs security checks, loads default configuration settings from\nincludes/DefaultSettings.php , guesses configuration with includes/Setup.php  and then\napplies site settings contained in LocalSettings.php . Next it instantiates a MediaWiki  object\n( $mediawiki ), and creates a Title  object ( $wgTitle ) depending on the title and action parameters\nfrom the request.\nindex.php  can take a variety of action parameters in the URL request; the default action is view ,\nwhich shows the regular view of an article's content. For example, the request\nhttps://en.wikipedia.org/w/index.php?title=Apple&action=view displays the content of the article \"Apple\" on\nthe English Wikipedia. (View requests are usually prettified with URL rewriting, in this example to\nhttps://en.wikipedia.org/wiki/Apple.) Other frequent actions include edit  (to open an article for editing),\nsubmit  (to preview or save an article), history  (to show an article's history) and watch  (to add an\narticle to the user's watchlist). Administrative actions include delete  (to delete an article) and\nprotect  (to prevent edits to an article).\nMediaWiki::performRequest()  is then called to handle most of the URL request. It checks for bad\ntitles, read restrictions, local interwiki redirects, and redirect loops, and determines whether the request is\nfor a normal or a special page.\nNormal page requests are handed over to MediaWiki::initializeArticle() , to create an Article\nobject for the page ( $wgArticle ), and then to MediaWiki::performAction() , which handles\n\"standard\" actions. Once the action has been completed, MediaWiki::finalCleanup()  finalizes the\nrequest by committing database transactions, outputting the HTML and launching deferred updates\nthrough the job queue. MediaWiki::restInPeace()  commits the deferred updates and closes the task\ngracefully.\nIf the page requested is a Special page (i.e., not a regular wiki content page, but a special software-\nrelated page such as Statistics ), SpecialPageFactory::executePath  is called instead of\ninitializeArticle() ; the corresponding PHP script is then called. Special pages can do all sorts of\nmagical things, and each has a specific purpose, usually independent of any one article or its content.\nSpecial pages include various kinds of reports (recent changes, logs, uncategorized pages) and wiki\nadministration tools (user blocks, user rights changes), among others. Their execution workflow depends\non their function.\nMany functions contain profiling code, which makes it possible to follow the execution workflow for\n", "page": 134, "type": "text", "section": "Page 134"}
{"text": "debugging if profiling is enabled. Profiling is done by calling the wfProfileIn  and wfProfileOut\nfunctions to respectively start and stop profiling a function; both functions take the function's name as a\nparameter. On Wikimedia sites, profiling is done for a percentage of all requests, to preserve\nperformance. MediaWiki sends UDP packets to a central server that collects them and produces profiling\ndata.\nCaching\nMediaWiki itself is improved for performance because it plays a central role on Wikimedia sites, but it is\nalso part of a larger operational ecosystem that has influenced its architecture. Wikimedia's caching\ninfrastructure (structured in layers) has imposed limitations in MediaWiki; developers worked around the\nissues, not by trying to shape Wikimedia's extensively optimized caching infrastructure around MediaWiki,\nbut rather by making MediaWiki more flexible, so it could work within that infrastructure without\ncompromising on performance and caching needs. For example, by default MediaWiki displays the user's\nIP in the top-right corner of the interface (for left-to-right languages) as a reminder that that's how they're\nknown to the software when they're not logged in. The $wgShowIPinHeader  configuration variable\nallows the system administrator to disable this feature, thus making the page content independent of the\nuser: all anonymous visitors can then be served the exact same version of each page.\nThe first level of caching (used on Wikimedia sites) consists of reverse caching proxies (Squids) that\nintercept and serve most requests before they make it to the MediaWiki application servers. Squids\ncontain static versions of entire rendered pages, served for simple reads to users who aren't logged in to\nthe site. MediaWiki natively supports Squid and Varnish, and integrates with this caching layer by, for\nexample, notifying them to purge a page from the cache when it has been changed. For logged-in users,\nand other requests that can't be served by Squids, Squid forwards the requests to the web server\n(Apache).\nThe second level of caching happens when MediaWiki renders and assembles the page from multiple\nobjects, many of which can be cached to minimize future calls. Such objects include the page's interface\n(sidebar, menus, UI text) and the content proper, parsed from wikitext. The in-memory object cache has\nbeen available in MediaWiki since the early 1.1 version (2003), and is particularly important to avoid re-\nparsing long and complex pages.\nLogin session data can also be stored in memcached, which lets sessions work transparently on multiple\nfront-end web servers in a load-balancing setup (Wikimedia heavily relies on load balancing, using LVS\nwith PyBal).\nSince version 1.16, MediaWiki uses a dedicated object cache for localized UI text; this was added after\nnoticing that a large part of the objects cached in memcached consisted of UI messages localized into\nthe user's language. The system is based on fast fetches of individual messages from constant\ndatabases (CDB), e.g., files with key-value pairs. CDBs minimize memory overhead and start-up time in\nthe typical case; they're also used for the interwiki cache.\nThe last caching layer consists of the PHP opcode cache, commonly enabled to speed up PHP\napplications. Compilation can be a lengthy process; to avoid compiling PHP scripts into opcode every\ntime they're invoked, a PHP accelerator can be used to store the compiled opcode and execute it directly\nwithout compilation. MediaWiki will \"just work\" with many accelerators such as APC, PHP accelerator and\neAccelerator.\nBecause of its Wikimedia bias, MediaWiki is optimized for this complete, multi-layer, distributed caching\ninfrastructure. Nonetheless, it also natively supports alternate setups for smaller sites. For example, it\noffers an optional simplistic file caching system that stores the output of fully rendered pages, like Squid\ndoes. Also, MediaWiki's abstract object caching layer lets it store the cached objects in several places,\nincluding the file system, the database, or the opcode cache.\nResourceLoader\nAs in many web applications, MediaWiki's interface has become more interactive and responsive over the\nyears, mostly through the use of JavaScript. Usability efforts initiated in 2008, as well as advanced media\nhandling (e.g., online editing of video files), called for dedicated front-end performance improvements.\nTo optimize the delivery of JavaScript and CSS assets, the ResourceLoader module was developed to\noptimize delivery of JS and CSS. Started in 2009, it was completed in 2011 and has been a core feature\nof MediaWiki since version 1.17. ResourceLoader works by loading JS and CSS assets on demand, thus\nreducing loading and parsing time when features are unused, for example by older browsers. It also\nminifies the code, groups resources to save requests, and can embed images as data URIs. (For more\non ResourceLoader, see the official documentation, and the talk Low Hanging Fruit vs. Micro-\noptimization: Creative Techniques for Loading Web Pages Faster given by Trevor Parscal and Roan\nKattouw at OSCON 2011.)\n12.5. Languages\nContext and Rationale\nA central part of effectively contributing and disseminating free knowledge to all is to provide it in as\nmany languages as possible. Wikipedia is available in more than 280 languages, and encyclopedia\narticles in English represent less than 20% of all articles. Because Wikipedia and its sister sites exist in\n", "page": 135, "type": "text", "section": "Page 135"}
{"text": "so many languages, it is important not only to provide the content in the readers' native language, but\nalso to provide a localized interface, and effective input and conversion tools, so that participants can\ncontribute content.\nFor this reason, localization and internationalization (l10n and i18n) are central components of MediaWiki.\nThe i18n system is pervasive, and impacts many parts of the software; it's also one of the most flexible\nand feature-rich. (There is an exhaustive guide to internationalization and localization in MediaWiki.)\nTranslator convenience is usually preferred to developer convenience, but this is believed to be an\nacceptable cost.\nMediaWiki is currently localized in more than 350 languages, including non-Latin and right-to-left (RTL)\nlanguages, with varying levels of completion. The interface and content can be in different languages, and\nhave mixed directionality.\nContent Language\nMediaWiki originally used per-language encoding, which led to a lot of issues; for example, foreign scripts\ncould not be used in page titles. UTF-8 was adopted instead. Support for character sets other than UTF-\n8 was dropped in 2005, along with the major database schema change in MediaWiki 1.5; content must\nnow be encoded in UTF-8.\nCharacters not available on the editor's keyboard can be customized and inserted via MediaWiki's\nEdittools, an interface message that appears below the edit window; its JavaScript version automatically\ninserts the character clicked into the edit window. The WikiEditor extension for MediaWiki, developed as\npart of a usability effort, merges special characters with the edit toolbar. Another extension, called\nNarayam, provides additional input methods and key mapping features for non-ASCII characters.\nInterface Language\nInterface messages have been stored in PHP arrays of key-values pairs since the Phase III software\nwas created. Each message is identified by a unique key, which is assigned different values across\nlanguages. Keys are determined by developers, who are encouraged to use prefixes for extensions; for\nexample, message keys for the UploadWizard extension will start with mwe-upwiz- , where mwe\nstands for MediaWiki extension.\nMediaWiki messages can embed parameters provided by the software, which will often influence the\ngrammar of the message. In order to support virtually any possible language, MediaWiki's localization\nsystem has been improved and complexified over time to accommodate languages' specific traits and\nexceptions, often considered oddities by English speakers.\nFor example, adjectives are invariable words in English, but languages like French require adjective\nagreement with nouns. If the user specified their gender in their preferences, the {{GENDER:}}  switch\ncan be used in interface messages to appropriately address them. Other switches include\n{{PLURAL:}} , for \"simple\" plurals and languages like Arabic with dual, trial or paucal numbers, and\n{{GRAMMAR:}} , providing grammatical transformation functions for languages like Finnish whose\ngrammatical cases cause alterations or inflections.\nLocalizing Messages\nLocalized interface messages for MediaWiki reside in MessagesXx.php  files, where Xx  is the ISO-639\ncode of the language (e.g. MessagesFr.php  for French); default messages are in English and stored in\nMessagesEn.php . MediaWiki extensions use a similar system, or host all localized messages in an\n<Extension-name>.i18n.php  file. Along with translations, Message files also include language-\ndependent information such as date formats.\nContributing translations used to be done by submitting PHP patches for the MessagesXx.php  files. In\nDecember 2003, MediaWiki 1.1 introduced \"database messages\", a subset of wiki pages in the\nMediaWiki namespace containing interface messages. The content of the wiki page\nMediaWiki:<Message-key>  is the message's text, and overrides its value in the PHP file. Localized\nversions of the message are at MediaWiki:<Message-key>/<language-code> ; for example,\nMediaWiki:Rollbacklink/de .\nThis feature has allowed power users to translate (and customize) interface messages locally on their\nwiki, but the process doesn't update i18n files shipping with MediaWiki. In 2006, Niklas Laxstr\u00f6m created\na special, heavily hacked MediaWiki website (now hosted at http://translatewiki.net) where translators can\neasily localize interface messages in all languages simply by editing a wiki page. The MessagesXx.php\nfiles are then updated in the MediaWiki code repository, where they can be automatically fetched by any\nwiki, and updated using the LocalisationUpdate extension. On Wikimedia sites, database messages are\nnow only used for customization, and not for localization any more. MediaWiki extensions and some\nrelated programs, such as bots, are also localized at translatewiki.net.\nTo help translators understand the context and meaning of an interface message, it is considered a good\npractice in MediaWiki to provide documentation for every message. This documentation is stored in a\nspecial Message file, with the qqq  language code which doesn't correspond to a real language. The\ndocumentation for each message is then displayed in the translation interface on translatewiki.net.\nAnother helpful tool is the qqx  language code; when used with the &uselang  parameter to display a\nwiki page (e.g., https://en.wikipedia.org/wiki/Special:RecentChanges?uselang=qqx), MediaWiki will display\n", "page": 136, "type": "text", "section": "Page 136"}
{"text": "the message keys instead of their values in the user interface; this is very useful to identify which\nmessage to translate or change.\nRegistered users can set their own interface language in their preferences, to override the site's default\ninterface language. MediaWiki also supports fallback languages: if a message isn't available in the\nchosen language, it will be displayed in the closest possible language, and not necessarily in English.\nFor example, the fallback language for Breton is French.\n12.6. Users\nUsers are represented in the code using instances of the User  class, which encapsulates all of the\nuser-specific settings (user id, name, rights, password, email address, etc.). Client classes use accessors\nto access these fields; they do all the work of determining whether the user is logged in, and whether the\nrequested option can be satisfied from cookies or whether a database query is needed. Most of the\nsettings needed for rendering normal pages are set in the cookie to minimize use of the database.\nMediaWiki provides a very granular permissions system, with a user permission for, basically, every\npossible action. For example, to perform the \"Rollback\" action (i.e., to \"quickly rollback the edits of the\nlast user who edited a particular page\"), a user needs the rollback  permission, included by default in\nMediaWiki's sysop  user group. But it can also be added to other user groups, or have a dedicated user\ngroup only providing this permission (this is the case on the English Wikipedia, with the Rollbackers\ngroup). Customization of user rights is done by editing the $wgGroupPermissions  array in\nLocalSettings.php ; for instance, $wgGroupPermissions['user']['movefile'] = true;  allows\nall registered users to rename files. A user can belong to several groups, and inherits the highest rights\nassociated with each of them.\nHowever, MediaWiki's user permissions system was really designed with Wikipedia in mind: a site whose\ncontent is accessible to all, and where only certain actions are restricted to some users. MediaWiki lacks\na unified, pervasive permissions concept; it doesn't provide traditional CMS features like restricting read\nor write access by topic or type of content. A few MediaWiki extensions provide such features to some\nextent.\n12.7. Content\nContent Structure\nThe concept of namespaces was used in the UseModWiki era of Wikipedia, where talk pages were at the\ntitle \"<article name>/Talk\". Namespaces were formally introduced in Magnus Manske's first \"PHP script\".\nThey were reimplemented a few times over the years, but have kept the same function: to separate\ndifferent kinds of content. They consist of a prefix separated from the page title by a colon (e.g. Talk:\nor File:  and Template: ); the main content namespace has no prefix. Wikipedia users quickly\nadopted them, and they provided the community with different spaces to evolve. Namespaces have\nproven to be an important feature of MediaWiki, as they create the necessary preconditions for a wiki's\ncommunity and set up meta-level discussions, community processes, portals, user profiles, etc.\nThe default configuration for MediaWiki's main content namespace is to be flat (no subpages), because\nit's how Wikipedia works, but it is trivial to enable subpages. They are enabled in other namespaces\n(e.g., User: , where people can, for instance, work on draft articles) and display breadcrumbs.\nNamespaces separate content by type; within the same namespace, pages can be organized by topic\nusing categories, a pseudo-hierarchical organization scheme introduced in MediaWiki 1.3.\nContent Processing: MediaWiki Markup Language and Parser\nThe user-generated content stored by MediaWiki isn't in HTML, but in a markup language specific to\nMediaWiki, sometimes called \"wikitext\". It allows users to make formatting changes (e.g. bold, italic using\nquotes), add links (using square brackets), include templates, insert context-dependent content (like a\ndate or signature), and make an incredible number of other magical things happen. (Detailed\ndocumentation is available.)\nTo display a page, this content needs to be parsed, assembled from all the external or dynamic pieces it\ncalls, and converted to proper HTML. The parser is one of the most essential parts of MediaWiki, which\nmakes it difficult to change or improve. Because hundreds of millions of wiki pages worldwide depend on\nthe parser to continue outputting HTML the way it always has, it has to remain extremely stable.\nThe markup language wasn't formally specced from the beginning; it started based on UseModWiki's\nmarkup, then morphed and evolved as needs demanded. In the absence of a formal specification, the\nMediaWiki markup language has become a complex and idiosyncratic language, basically only compatible\nwith MediaWiki's parser; it can't be represented as a formal grammar. The current parser's specification\nis jokingly referred to as \"whatever the parser spits out from wikitext, plus a few hundred test cases\".\nThere have been many attempts at alternative parsers, but none has succeeded so far. In 2004 an\nexperimental tokenizer was written by Jens Frank to parse wikitext, and enabled on Wikipedia; it had to\nbe disabled three days later because of the poor performance of PHP array memory allocations. Since\nthen, most of the parsing has been done with a huge pile of regular expressions, and a ton of helper\nfunctions. The wiki markup, and all the special cases the parser needs to support, have also become\nconsiderably more complex, making future attempts even more difficult.\n", "page": 137, "type": "text", "section": "Page 137"}
{"text": "A notable improvement was Tim Starling's preprocessor rewrite in MediaWiki 1.12, whose main\nmotivation was to improve the parsing performance on pages with complex templates. The preprocessor\nconverts wikitext to an XML DOM tree representing parts of the document (template invocations, parser\nfunctions, tag hooks, section headings, and a few other structures), but can skip \"dead branches\", such\nas unfollowed #switch  cases and unused defaults for template arguments, in template expansion. The\nparser then iterates through the DOM structure and converts its content to HTML.\nRecent work on a visual editor for MediaWiki has made it necessary to improve the parsing process (and\nmake it faster), so work has resumed on the parser and intermediate layers between MediaWiki markup\nand final HTML (see Future, below).\nMagic Words and Templates\nMediaWiki offers \"magic words\" that modify the general behavior of the page or include dynamic content\ninto it. They consist of: behavior switches like __NOTOC__  (to hide the automatic table of content) or\n__NOINDEX__  (to tell search engines not to index the page); variables like {{CURRENTTIME}}  or\n{{SITENAME}} ; and parser functions, i.e., magic words that can take parameters, like\n{{lc:<string>}}  (to output <string>  in lowercase). Constructs like {{GENDER:}} ,\n{{PLURAL:}}  and {{GRAMMAR:}} , used to localize the UI, are parser functions.\nThe most common way to include content from other pages in a MediaWiki page is to use templates.\nTemplates were really intended to be used to include the same content on different pages, e.g.,\nnavigation panels or maintenance banners on Wikipedia articles; having the ability to create partial page\nlayouts and reuse them in thousands of articles with central maintenance made a huge impact on sites\nlike Wikipedia.\nHowever, templates have also been used (and abused) by users for a completely different purpose.\nMediaWiki 1.3 made it possible for templates to take parameters that change their output; the ability to\nadd a default parameter (introduced in MediaWiki 1.6) enabled the construction of a functional\nprogramming language implemented on top of PHP, which was ultimately one of the most costly features\nin terms of performance.\nTim Starling then developed additional parser functions (the ParserFunctions extension), as a stopgap\nmeasure against insane constructs created by Wikipedia users with templates. This set of functions\nincluded logical structures like #if  and #switch , and other functions like #expr  (to evaluate\nmathematical expressions) and #time  (for time formatting).\nSoon enough, Wikipedia users started to create even more complex templates using the new functions,\nwhich considerably degraded the parsing performance on template-heavy pages. The new preprocessor\nintroduced in MediaWiki 1.12 (a major architectural change) was implemented to partly remedy this issue.\nRecently, MediaWiki developers have discussed the possibility of using an actual scripting language,\nperhaps Lua, to improve performance.\nMedia Files\nUsers upload files through the Special:Upload  page; administrators can configure the allowed file\ntypes through an extension whitelist. Once uploaded, files are stored in a folder on the file system, and\nthumbnails in a dedicated thumb  directory.\nBecause of Wikimedia's educational mission, MediaWiki supports file types that may be uncommon in\nother web applications or CMSes, like SVG vector images, and multipage PDFs and DjVus. They are\nrendered as PNG files, and can be thumbnailed and displayed inline, as are more common image files\nlike GIFs, JPGs and PNGs.\nWhen a file is uploaded, it is assigned a File:  page containing information entered by the uploader;\nthis is free text and usually includes copyright information (author, license) and items describing or\nclassifying the content of the file (description, location, date, categories, etc.). While private wikis may not\ncare much about this information, on media libraries like Wikimedia Commons it are critical to organise\nthe collection and ensure the legality of sharing these files. It has been argued that most of these\nmetadata should, in fact, be stored in a queryable structure like a database table. This would\nconsiderably facilitate search, but also attribution and reuse by third parties\u2014for example, through the\nAPI.\nMost Wikimedia sites also allow \"local\" uploads to each wiki, but the community tries to store freely\nlicensed media files in Wikimedia's free media library, Wikimedia Commons. Any Wikimedia site can\ndisplay a file hosted on Commons as if it were hosted locally. This custom avoids having to upload a file\nto every wiki to use it there.\nAs a consequence, MediaWiki natively supports foreign media repositories, i.e., the ability to access\nmedia files hosted on another wiki through its API and the ForeignAPIRepo  system. Since version\n1.16, any MediaWiki website can easily use files from Wikimedia Commons through the\nInstantCommons  feature. When using a foreign repository, thumbnails are stored locally to save\nbandwidth. However, it is not (yet) possible to upload to a foreign media repository from another wiki.\n12.8. Customizing and Extending MediaWiki\nLevels\n", "page": 138, "type": "text", "section": "Page 138"}
{"text": "MediaWiki's architecture provides different ways to customize and extend the software. This can be done\nat different levels of access:\nSystem administrators can install extensions and skins, and configure the wiki's separate helper\nprograms (e.g., for image thumbnailing and TeX rendering) and global settings (see Configuration\nabove).\nWiki sysops (sometimes called \"administrators\" too) can edit site-wide gadgets, JavaScript and CSS\nsettings.\nAny registered user can customize their own experience and interface using their preferences (for\nexisting settings, skins and gadgets) or make their own modifications (using their personal JS and\nCSS pages).\nExternal programs can also communicate with MediaWiki through its machine API, if it's enabled,\nbasically making any feature and data accessible to the user.\nJavaScript and CSS\nMediaWiki can read and apply site-wide or skin-wide JavaScript and CSS using custom wiki pages;\nthese pages are in the MediaWiki:  namespace, and thus can only be edited by sysops; for example,\nJavaScript modifications from MediaWiki:Common.js  apply to all skins, CSS from\nMediaWiki:Common.css  applies to all skins, but MediaWiki:Vector.css  only applies to users with\nthe Vector skin.\nUsers can do the same types of changes, which will only apply to their own interface, by editing\nsubpages of their user page (e.g. User:<Username>/common.js  for JavaScript on all skins,\nUser:<Username>/common.css  for CSS on all skins, or User:<Username>/vector.css  for CSS\nmodifications that only apply to the Vector skin).\nIf the Gadgets extension is installed, sysops can also edit gadgets, i.e., snippets of JavaScript code,\nproviding features that can be turned on and off by users in their preferences. Upcoming developments\non gadgets will make it possible to share gadgets across wikis, thus avoiding duplication.\nThis set of tools has had a huge impact and greatly increased the democratization of MediaWiki's\nsoftware development. Individual users are empowered to add features for themselves; power users can\nshare them with others, both informally and through globally configurable sysop-controlled systems. This\nframework is ideal for small, self-contained modifications, and presents a lower barrier to entry than\nheavier code modifications done through hooks and extensions.\nExtensions and Skins\nWhen JavaScript and CSS modifications are not enough, MediaWiki provides a system of hooks that let\nthird-party developers run custom PHP code before, after, or instead of MediaWiki code for particular\nevents. (MediaWiki hooks are referenced at https://www.mediawiki.org/wiki/Manual:Hooks.) MediaWiki\nextensions use hooks to plug into the code.\nBefore hooks existed in MediaWiki, adding custom PHP code meant modifying the core code, which was\nneither easy nor recommended. The first hooks were proposed and added in 2004 by Evan Prodromou;\nmany more have been added over the years when needed. Using hooks, it is even possible to extend\nMediaWiki's wiki markup with additional capabilities using tag extensions.\nThe extension system isn't perfect; extension registration is based on code execution at startup, rather\nthan cacheable data, which limits abstraction and optimization and hurts MediaWiki's performance. But\noverall, the extension architecture is now a fairly flexible infrastructure that has helped make specialized\ncode more modular, keeping the core software from expanding (too) much, and making it easier for third-\nparty users to build custom functionality on top of MediaWiki.\nConversely, it's very difficult to write a new skin for MediaWiki without reinventing the wheel. In\nMediaWiki, skins are PHP classes each extending the parent Skin  class; they contain functions that\ngather the information needed to generate the HTML. The long-lived \"MonoBook\" skin was difficult to\ncustomize because it contained a lot of browser-specific CSS to support old browsers; editing the\ntemplate or CSS required many subsequent changes to reflect the change for all browsers and platforms.\nAPI\nThe other main entry point for MediaWiki, besides index.php , is api.php , used to access its\nmachine-readable web query API (Application Programming Interface).\nWikipedia users originally created \"bots\" that worked by screen scraping the HTML content served by\nMediaWiki; this method was very unreliable and broke many times. To improve this situation, developers\nintroduced a read-only interface (located at query.php ), which then evolved into a full-fledged read and\nwrite machine API providing direct, high-level access to the data contained in the MediaWiki database.\n(Exhaustive documentation of the API is available.)\nClient programs can use the API to login, get data, and post changes. The API supports thin web-based\nJavaScript clients and end-user applications. Almost anything that can be done via the web interface can\nbasically be done through the API. Client libraries implementing the MediaWiki API are available in many\nlanguages, including Python and .NET.\n", "page": 139, "type": "text", "section": "Page 139"}
{"text": "This work is made available under the Creative Commons Attribution 3.0 Unported license. Please see\nthe full description of the license for details.\n12.9. Future\nWhat started as a summer project done by a single volunteer PHP developer has grown into MediaWiki,\na mature, stable wiki engine powering a top-ten website with a ridiculously small operational\ninfrastructure. This has been made possible by constant optimization for performance, iterative\narchitectural changes and a team of awesome developers.\nThe evolution of web technologies, and the growth of Wikipedia, call for ongoing improvements and new\nfeatures, some of which require major changes to MediaWiki's architecture. This is, for example, the case\nfor the ongoing visual editor project, which has prompted renewed work on the parser and on the wiki\nmarkup language, the DOM and final HTML conversion.\nMediaWiki is a tool used for very different purposes. Within Wikimedia projects, for instance, it's used to\ncreate and curate an encyclopedia (Wikipedia), to power a huge media library (Wikimedia Commons), to\ntranscribe scanned reference texts (Wikisource), and so on. In other contexts, MediaWiki is used as a\ncorporate CMS, or as a data repository, sometimes combined with a semantic framework. These\nspecialized uses that weren't planned for will probably continue to drive constant adjustments to the\nsoftware's internal structure. As such, MediaWiki's architecture is very much alive, just like the immense\ncommunity of users it supports.\n12.10. Further Reading\nMediaWiki documentation and support\nAutomatically generated MediaWiki documentation\nDomas Mituzas, Wikipedia: site internals, configuration, code examples and management issues,\nMySQL Users conference, 2007. Full text available at http://dom.as/talks/.\n12.11. Acknowledgments\nThis chapter was created collaboratively. Guillaume Paumier wrote most of the content by organizing the\ninput provided by MediaWiki users and core developers. Sumana Harihareswara coordinated the\ninterviews and input-gathering phases. Many thanks to Antoine Musso, Brion Vibber, Chad Horohoe, Tim\nStarling, Roan Kattouw, Sam Reed, Siebrand Mazeland, Erik M\u00f6ller, Magnus Manske, Rob Lanphier,\nAmir Aharoni, Federico Leva, Graham Pearce and others for providing input and/or reviewing the content.\nBack to top\nBack to The Architecture of Open Source Applications.\n", "page": 140, "type": "text", "section": "Page 140"}
{"text": "<>\nMoodle is a web application used in educational settings. While this chapter will try to give an overview\nof all aspects of how Moodle works, it focuses on those areas where Moodle's design is particularly\ninteresting:\nThe way the application is divided into plugins;\nThe permission system, which controls which users can perform which actions in different parts of\nthe system;\nThe way output is generated, so that different themes (skins) can be used to give different\nappearances, and so that the interface can be localised.\nThe database abstraction layer.\nMoodle provides a place online where students and teachers can come together to teach and learn. A\nMoodle site is divided into courses. A course has users enrolled in it with different roles, such as Student\nor Teacher. Each course comprises a number of resources and activities. A resource might be a PDF\nfile, a page of HTML within Moodle, or a link to something elsewhere on the web. An activity might be a\nforum, a quiz or a wiki. Within the course, these resources and activities will be structured in some way.\nFor example they may be grouped into logical topics, or into weeks on a calendar.\nFigure 13.1: Moodle course\nMoodle can be used as a standalone application. Should you wish to teach courses on software\narchitecture (for example) you could download Moodle to your web host, install it, start creating courses,\nand wait for students to come and self-register. Alternatively, if you are a large institution, Moodle would\nbe just one of the systems you run. You would probably also have the infrastructure shown in Figure\n13.2.\nMoodle\nTim Hunt\n", "page": 141, "type": "text", "section": "Page 141"}
{"text": "Figure 13.2: Typical university systems architecture\nAn authentication/identity provider (for example LDAP) to control user accounts across all your\nsystems.\nA student information system; that is, a database of all your students, which program of study they\nare on, and hence which courses they need to complete; and their transcript\u2014a high-level summary\nof the results of the courses they have completed. This would also deal with other administrative\nfunctions, like tracking whether they have paid their fees.\nA document repository (for example, Alfresco); to store files, and track workflow as users collaborate\nto create files.\nAn ePortfolio; this is a place where students can assemble assets, either to build a CV (resume), or\nto provide evidence that they have met the requirements of a practice-based course.\nA reporting or analytics tool; to generate high-level information about what is going on in your\ninstitution.\nMoodle focuses on providing an online space for teaching and learning, rather than any of the other\nsystems that an educational organisation might need. Moodle provides a basic implementation of the\nother functionalities, so that it can function either as a stand-alone system or integrated with other\nsystems. The role Moodle plays is normally called a virtual learning environment (VLE), or learning or\ncourse management system (LMS, CMS or even LCMS).\nMoodle is open source or free software (GPL). It is written in PHP. It will run on most common web\nservers, on common platforms. It requires a database, and will work with MySQL, PostgreSQL, Microsoft\nSQL Server or Oracle.\nThe Moodle project was started by Martin Dougiamas in 1999, while he was working at Curtin University,\nAustralia. Version 1.0 was released in 2002, at which time PHP4.2 and MySQL 3.23 were the\ntechnologies available. This limited the kind of architecture that was possible initially, but much has\nchanged since then. The current release is the Moodle 2.2.x series.\n13.1. An Overview of How Moodle Works\nA Moodle installation comprises three parts:\n1. The code, typically in a folder like /var/www/moodle  or ~/htdocs/moodle . This should not be\nwritable by the web server.\n2. The database, managed by one of the supported RDMSs. In fact, Moodle adds a prefix to all the\ntable names, so it can share a database with other applications if desired.\n3. The moodledata  folder. This is a folder where Moodle stores uploaded and generated files, and so\nneeds to be writable by the web server. For security reasons, the should be outside the web root.\nThese can all be on a single server. Alternatively, in a load-balanced set-up, there will be multiple copies\nof the code on each web server, but just one shared copy of the database and moodledata , probably\non other servers.\nThe configuration information about these three parts is stored in a file called config.php  in the root of\nthe moodle  folder when Moodle is installed.\nRequest Dispatching\nMoodle is a web applications, so users interact with it using their web browser. From Moodle's point of\nview that means responding to HTTP requests. An important aspect of Moodle's design is, therefore, the\nURL namespace, and how URLs get dispatched to different scripts.\nMoodle uses the standard PHP approach to this. To view the main page for a course, the URL would be\n.../course/view.php?id=123 , where 123  is the unique id of the course in the database. To view a\nforum discussion, the URL would be something like .../mod/forum/discuss.php?id=456789 . That\nis, these particular scripts, course/view.php  or mod/forum/discuss.php , would handle these\nrequests.\n", "page": 142, "type": "text", "section": "Page 142"}
{"text": "This is simple for the developer. To understand how Moodle handles a particular request, you look at the\nURL and start reading code there. It is ugly from the user's point of view. These URLs are, however,\npermanent. The URLs do not change if the course is renamed, or if a moderator moves a discussion to a\ndifferent forum. (This is a good property for URLs to have, as explained in Tim Berners-Lee's article Cool\nURIs don't change.)\nThe alternative approach one could take is to have a single entry point \u2026/index.php/[extra-\ninformation-to-make-the-request-unique] . The single script index.php  would then dispatch\nthe requests in some way. This approach adds a layer of indirection, which is something software\ndevelopers always like to do. The lack of this layer of indirection does not seem to hurt Moodle.\nPlugins\nLike many successful open source projects, Moodle is built out of many plugins, working together with the\ncore of the system. This is a good approach because at allows people to change and enhance Moodle in\ndefined ways. An important advantage of an open source system is that you can tailor it to your particular\nneeds. Making extensive customisations to the code can, however, lead to big problems when the time\ncomes to upgrade, even when using a good version control system. By allowing as many customisations\nand new features as possible to be implemented as self-contained plugins that interact with the Moodle\ncore through a defined API, it is easier for people to customise Moodle to their needs, and to share\ncustomisations, while still being able to upgrade the core Moodle system.\nThere are various ways a system can be built as a core surrounded by plugins. Moodle has a relatively\nfat core, and the plugins are strongly-typed. When I say a fat core, I mean that there is a lot of\nfunctionality in the core. This contrasts with the kind of architecture where just about everything, except\nfor a small plugin-loader stub, is a plugin.\nWhen I say plugins are strongly typed, I mean that depending on which type of functionality you want to\nimplement, you have to write a different type of plugin, and implement a different API. For example, a\nnew Activity module plugin would be very different from a new Authentication plugin or a new Question\ntype. At the last count there are about 35 different types of plugin. (There is a full list of Moodle plugin\ntypes.) This contrasts with the kind of architecture where all plugins use basically the same API and then,\nperhaps, subscribe to the subset of hooks or events they are interested in.\nGenerally, the trend in Moodle has been to try to shrink the core, by moving more functionality into\nplugins. This effort has only been somewhat successful, however, because an increasing feature-set\ntends to expand the core. The other trend has been to try to standardise the different types of plugin as\nmuch as possible, so that in areas of common functionality, like install and upgrade, all types of plugins\nwork the same way.\nA plugin in Moodle takes the form of a folder containing files. The plugin has a type and a name, which\ntogether make up the \"Frankenstyle\" component name of the plugin. (The word \"Frankenstyle\" arose out\nof an argument in the developers' Jabber channel, but everyone liked it and it stuck.) The plugin type and\nname determine the path to the plugin folder. The plugin type gives a prefix, and the foldername is the\nplugin name. Here are some examples:\nPlugin type\nPlugin name\nFrankenstyle\nFolder\nmod (Activity module)\nforum\nmod_forum\nmod/forum\nmod (Activity module)\nquiz\nmod_quiz\nmod/quiz\nblock (Side-block)\nnavigation\nblock_navigation\nblocks/navigation\nqtype (Question type)\nshortanswer\nqtype_shortanswer\nquestion/type /shortanswer\nquiz (Quiz report)\nstatistics\nquiz_statistics\nmod/quiz/report/statistics\nThe last example shows that each activity module is allowed to declare sub-plugin types. At the moment\nonly activity modules can do this, for two reasons. If all plugins could have sub-plugins that might cause\nperformance problems. Activity modules are the main educational activities in Moodle, and so are the\nmost important type of plugin, thus they get special privileges.\nAn Example Plugin\nI will explain a lot of details of the Moodle architecture by considering a specific example plugin. As is\ntraditional, I have chosen to implement a plugin that displays \"Hello world\".\nThis plugin does not really fit naturally into any of the standard Moodle plugin types. It is just a script,\nwith no connection to anything else, so I will choose to implement it as a \"local\" plugin. This is a catch-all\nplugin type for miscellaneous functionality that does not fit anywhere better. I will name my plugin\ngreet , to give a Frankensyle name of local_greet , and a folder path of local/greet . (The plugin\ncode can be downloaded.)\nEach plugin must contain a file called version.php  which defines some basic metadata about the\nplugin. This is used by the Moodle's plugin installer system to install and upgrade the plugin. For\nexample, local/greet/version.php  contains:\n", "page": 143, "type": "text", "section": "Page 143"}
{"text": "<?php\n$plugin->component    = 'local_greet';\n$plugin->version      = 2011102900;\n$plugin->requires     = 2011102700;\n$plugin->maturity     = MATURITY_STABLE;\nIt may seem redundant to include the component name, since this can be deduced from the path, but the\ninstaller uses this to verify that the plugin has been installed in the right place. The version field is the\nversion of this plugin. Maturity is ALPHA, BETA, RC (release candidate), or STABLE. Requires is the\nminimum version of Moodle that this plugin is compatible with. If necessary, one can also document other\nplugins that this one depends on.\nHere is the main script for this simple plugin (stored in local/greet/index.php ):\n<?php\nrequire_once(dirname(__FILE__) . '/../../config.php');        // 1\nrequire_login();                                              // 2\n$context = context_system::instance();                        // 3\nrequire_capability('local/greet:begreeted', $context);        // 4\n$name = optional_param('name', '', PARAM_TEXT);               // 5\nif (!$name) {\n    $name = fullname($USER);                                  // 6\n}\nadd_to_log(SITEID, 'local_greet', 'begreeted',\n        'local/greet/index.php?name=' . urlencode($name));    // 7\n$PAGE->set_context($context);                                 // 8\n$PAGE->set_url(new moodle_url('/local/greet/index.php'),\n        array('name' => $name));                              // 9\n$PAGE->set_title(get_string('welcome', 'local_greet'));       // 10\necho $OUTPUT->header();                                       // 11\necho $OUTPUT->box(get_string('greet', 'local_greet',\n        format_string($name)));                               // 12\necho $OUTPUT->footer();                                       // 13\nLine 1: Bootstrapping Moodle\nrequire_once(dirname(__FILE__) . '/../../config.php');        // 1\nThe single line of this script that does the most work is the first. I said above that config.php  contains\nthe details Moodle needs to connect to the database and find the moodledata folder. It ends, however,\nwith the line require_once('lib/setup.php') . This:\n1. loads all the standard Moodle libraries using require_once ;\n2. starts the session handling;\n3. connects to the database; and\n4. sets up a number of global variables, which we shall meet later.\nLine 2: Checking the User Is Logged In\nrequire_login();                                              // 2\nThis line causes Moodle to check that the current user is logged in, using whatever authentication plugin\nthe administrator has configured. If not, the user will be redirected to the log-in form, and this function\nwill never return.\nA script that was more integrated into Moodle would pass more arguments here, to say which course or\nactivity this page is part of, and then require_login  would also verify that the user is enrolled in, or\notherwise allowed to access this course, and is allowed to see this activity. If not, an appropriate error\nwould be displayed.\n13.2. Moodle's Roles and Permissions System\nThe next two lines of code show how to check that the user has permission to do something. As you can\nsee, from the developer's point of view, the API is very simple. Behind the scenes, however, there is a\nsophisticated access system which gives the administrator great flexibility to control who can do what.\nLine 3: Getting the Context\n$context = context_system::instance();                        // 3\n", "page": 144, "type": "text", "section": "Page 144"}
{"text": "In Moodle, users can have different permissions in different places. For example, a user might be a\nTeacher in one course, and a Student in another, and so have different permissions in each place. These\nplaces are called contexts. Contexts in Moodle form a hierarchy rather like a folder hierarchy in a file-\nsystem. At the top level is the System context (and, since this script is not very well integrated into\nMoodle, it uses that context).\nWithin the System context are a number of contexts for the different categories that have been created to\norganise courses. These can be nested, with one category containing other categories. Category contexts\ncan also contain Course contexts. Finally, each activity in a course will have its own Module context.\nFigure 13.3: Contexts\nLine 4: Checking the User Has Permission to Use This Script\nrequire_capability('local/greet:begreeted', $context);        // 4\nHaving got the context\u2014the relevant area of Moodle\u2014the permission can be checked. Each bit of\nfunctionality that a user may or may not have is called a capability. Checking a capability provides more\nfine-grained access control than the basic checks performed by require_login . Our simple example\nplugin has just one capability: local/greet:begreeted .\nThe check is done using the require_capability  function, which takes the capability name and the\ncontext. Like other require_\u2026  functions, it will not return if the user does not have the capability. It will\ndisplay an error instead. In other places the non-fatal has_capability  function, which returns a\nBoolean would be used, for example, to determine whether to display a link to this script from another\npage.\nHow does the administrator configure which user has which permission? Here is the calculation that\nhas_capability  performs (at least conceptually):\n1. Start from the current Context.\n2. Get a list of the Roles that the user has in this Context.\n3. Then work out what the Permission is for each Role in this Context.\n4. Aggregate those permissions to get a final answer.\nDefining Capabilities\nAs the example shows, a plugin can define new capabilities relating to the particular functionality it\nprovides. Inside each Moodle plugin there is a sub-folder of the code called db . This contains all the\ninformation required to install or upgrade the plugin. One of those bits of information is a file called\naccess.php  that defines the capabilities. Here is the access.php  file for our plugin, which lives in\nlocal/greet/db/access.php :\n<?php\n$capabilities = array('local/greet:begreeted' => array(\n    'captype' => 'read',\n    'contextlevel' => CONTEXT_SYSTEM,\n    'archetypes' => array('guest' => CAP_ALLOW, 'user' => CAP_ALLOW)\n));\nThis gives some metadata about each capability which are used when constructing the permissions\nmanagement user interface. It also give default permissions for common types of role.\n", "page": 145, "type": "text", "section": "Page 145"}
{"text": "Roles\nThe next part of the Moodle permissions system is roles. A role is really just a named set of permissions.\nWhen you are logged into Moodle, you will have the \"Authenticated user\" role in the System context, and\nsince the System context is the root of the hierarchy, that role will apply everywhere.\nWithin a particular course, you may be a Student, and that role assignment will apply in the Course\ncontext and all the Module contexts within it. In another course, however, you may have a different role.\nFor example, Mr Gradgrind may be Teacher in the \"Facts, Facts, Facts\" course, but a Student in the\nprofessional development course \"Facts Aren't Everything\". Finally, a user might be given the Moderator\nrole in one particular forum (Module context).\nPermissions\nA role defines a permission for each capability. For example the Teacher role will probably ALLOW\nmoodle/course:manage , but the Student role will not. However, both Student and Teacher will allow\nmod/forum:startdiscussion .\nThe roles are normally defined globally, but they can be re-defined in each context. For example, one\nparticular wiki can be made read-only to students by overriding the permission for the mod/wiki:edit\ncapability for the Student role in that wiki (Module) context, to PREVENT.\nThere are four Permissions:\nNOT SET/INHERIT (default)\nALLOW\nPREVENT\nPROHIBIT\nIn a given context, a role will have one of these four permissions for each capability. One difference\nbetween PROHIBIT and PREVENT is that a PROHIBIT cannot be overridden in sub-contexts.\nPermission Aggregation\nFinally the permissions for all the roles the user has in this context are aggregated.\nIf any role gives the permission PROHIBIT for this capability, return false.\nOtherwise, if any role gives ALLOW for this capability, return true.\nOtherwise return false.\nA use case for PROHIBIT is this: Suppose a user has been making abusive posts in a number of forums,\nand we want to stop them immediately. We can create a Naughty user role, which sets\nmod/forum:post  and other such capabilities to PROHIBIT. We can then assign this role to the abusive\nuser in the System context. That way, we can be sure that the user will not be able to post any more in\nany forum. (We would then talk to the student, and having reached a satisfactory outcome, remove that\nrole assignment so that they may use the system again.)\nSo, Moodle's permissions system gives administrators a huge amount of flexibility. They can define\nwhichever roles they like with different permissions for each capability; they can alter the role definitions\nin sub-contexts; and then they can assign different roles to users in different contexts.\n13.3. Back to Our Example Script\nThe next part of the script illustrates some miscellaneous points:\nLine 5: Get Data From the Request\n$name = optional_param('name', '', PARAM_TEXT);               // 5\nSomething that every web application has to do is get data from a request (GET or POST variables)\nwithout being susceptible to SQL injection or cross-site scripting attacks. Moodle provides two ways to do\nthis.\nThe simple method is the one shown here. It gets a single variable given the parameter name (here\nname ) a default value, and the expected type. The expected type is used to clean the input of all\nunexpected characters. There are numerous types like PARAM_INT , PARAM_ALPHANUM , PARAM_EMAIL ,\nand so on.\nThere is also a similar required_param  function, which like other require_\u2026  functions stops\nexecution and displays an error message if the expected parameter is not found.\nThe other mechanism Moodle has for getting data from the request is a fully fledged forms library. This is\na wrapper around the HTML QuickForm library from PEAR. (For non-PHP programmers, PEAR is PHP's\nequivalent of CPAN.) This seemed like a good choice when it was selected, but is now no longer\nmaintained. At some time in the future we will have to tackle moving to a new forms library, which many\nof us look forwards to, because QuickForm has several irritating design issues. For now, however, it is\nadequate. Forms can be defined as a collection of fields of various types (e.g. text box, select drop-down,\ndate-selector) with client- and server- side validation (including use of the same PARAM_\u2026  types).\nLine 6: Global Variables\n", "page": 146, "type": "text", "section": "Page 146"}
{"text": "if (!$name) {\n    $name = fullname($USER);                                  // 6\n}\nThis snippet shows the first of the global variables Moodle provides. $USER  makes accessible the\ninformation about the user accessing this script. Other globals include:\n$CFG : holds the commonly used configuration settings.\n$DB : the database connection.\n$SESSION : a wrapper around the PHP session.\n$COURSE : the course the current request relates to.\nand several others, some of which we will encounter below.\nYou may have read the words \"global variable\" with horror. Note, however, that PHP processes a single\nrequest at a time. Therefore these variables are not as global as all that. In fact, PHP global variables\ncan be seen as an implementation of the thread-scoped registry pattern (see Martin Fowler's Patterns of\nEnterprise Application Architecture) and this is the way in which Moodle uses them. It is very convenient\nin that it makes commonly used objects available throughout the code, without requiring them to be\npassed to every function and method. It is only infrequently abused.\nNothing is Simple\nThis line also serves to make a point about the problem domain: nothing is ever simple. To display a\nuser's name is more complicated than simply concatenating $USER->firstname , '~' , and $USER-\n>lastname . The school may have policies about showing either of those parts, and different cultures\nhave different conventions for which order to show names. Therefore, there are several configurations\nsettings and a function to assemble the full name according to the rules.\nDates are a similar problem. Different users may be in different time-zones. Moodle stores all dates as\nUnix time-stamps, which are integers, and so work in all databases. There is then a userdate  function\nto display the time-stamp to the user using the appropriate timezone and locale settings.\nLine 7: Logging\nadd_to_log(SITEID, 'local_greet', 'begreeted',\n        'local/greet/index.php?name=' . urlencode($name));    // 7\nAll significant actions in Moodle are logged. Logs are written to a table in the database. This is a trade-\noff. It makes sophisticated analysis quite easy, and indeed various reports based on the logs are\nincluded with Moodle. On a large and busy site, however, it is a performance problem. The log table gets\nhuge, which makes backing up the database more difficult, and makes queries on the log table slow.\nThere can also be write contention on the log table. These problems can be mitigated in various ways,\nfor example by batching writes, or archiving or deleting old records to remove them from the main\ndatabase.\n13.4. Generating Output\nOutput is mainly handled via two global objects.\nLine 8: The $PAGE  Global\n$PAGE->set_context($context);                                 // 8\n$PAGE  stores the information about the page to be output. This information is then readily available to\nthe code that generates the HTML. This script needs to explicitly specify the current context. (In other\nsituations, this might have been set automatically by require_login .) The URL for this page must also\nbe set explicitly. This may seem redundant, but the rationale for requiring it is that you might get to a\nparticular page using any number of different URLs, but the URL passed to set_url  should be the\ncanonical URL for the page\u2014a good permalink, if you like. The page title is also set. This will end up in\nthe head  element of the HTML.\nLine 9: Moodle URL\n$PAGE->set_url(new moodle_url('/local/greet/index.php'),\n        array('name' => $name));                              // 9\nI just wanted to flag this nice little helper class which makes manipulating URLs much easier. As an\naside, recall that the add_to_log  function call above did not use this helper class. Indeed, the log API\ncannot accept moodle_url  objects. This sort of inconsistency is a typical sign of a code-base as old as\nMoodle's.\nLine 10: Internationalisation\n$PAGE->set_title(get_string('welcome', 'local_greet'));       // 10\n", "page": 147, "type": "text", "section": "Page 147"}
{"text": "Moodle uses its own system to allow the interface to be translated into any language. There may now be\ngood PHP internationalisation libraries, but in 2002 when it was first implemented there was not one\navailable that was adequate. The system is based around the get_string  function. Strings are\nidentified by a key and the plugin Frankenstyle name. As can be seen on line 12, it is possible to\ninterpolate values into the string. (Multiple values are handled using PHP arrays or objects.)\nThe strings are looked up in language files that are just plain PHP arrays. Here is the language file\nlocal/greet/lang/en/local_greet.php  for our plugin:\n<?php\n$string['greet:begreeted'] = 'Be greeted by the hello world example';\n$string['welcome'] = 'Welcome';\n$string['greet'] = 'Hello, {$a}!';\n$string['pluginname'] = 'Hello world example';\nNote that, as well as the two string used in our script, there are also strings to give a name to the\ncapability, and the name of the plugin as it appears in the user interface.\nThe different languages are identified by the two-letter country code ( en  here). Languages packs may\nderive from other language packs. For example the fr_ca  (French Canadian) language pack declares\nfr  (French) as the parent language, and thus only has to define those strings that differ from the\nFrench. Since Moodle originated in Australia, en  means British English, and en_us  (American\nEnglish) is derived from it.\nAgain, the simple get_string  API for plugin developers hides a lot of complexity, including working out\nthe current language (which may depend on the current user's preferences, or the settings for the\nparticular course they are currently in), and then searching through all the language packs and parent\nlanguage packs to find the string.\nProducing the language pack files and co-ordinating the translation effort is managed at\nhttp://lang.moodle.org/, which is Moodle with a custom plugin ( local_amos ). It uses both Git and the\ndatabase as a backend to store the language files with full version history.\nLine 11: Starting Output\necho $OUTPUT->header();                                       // 11\nThis is another innocuous-looking line that does much more than it seems. The point is that before any\noutput can be done, the applicable theme (skin) must be worked out. This may depend on a combination\nof the page context and the user's preferences. $PAGE->context  was, however, only set on line 8, so\nthe $OUTPUT  global could not have been initialised at the start of the script. In order to solve this\nproblem, some PHP magic is used to create the proper $OUTPUT  object based on the information in\n$PAGE  the first time any output method is called.\nAnother thing to consider is that every page in Moodle may contain blocks. These are extra configurable\nbits of content that are normally displayed to the left or right of the main content. (They are a type of\nplugin.) Again, the exact collection of blocks to display depends, in a flexible way (that the administrator\ncan control) on the page context and some other aspects of the page identity. Therefore, another part of\npreparing for output is a call to $PAGE->blocks->load_blocks() .\nOnce all the necessary information has been worked out, the theme plugin (that controls the overall look\nof the page) is called to generate the overall page layout, including whatever standard header and footer\nis desired. This call is also responsible for adding the output from the blocks at the appropriate place in\nthe HTML. In the middle of the layout there will be a div  where the specific content for this page goes.\nThe HTML of this layout is generated, and then split in half after the start of the main content div . The\nfirst half is returned, and the rest is stored to be returned by $OUTPUT->footer() .\nLine 12: Outputting the Body of the Page\necho $OUTPUT->box(get_string('greet', 'local_greet',\n        format_string($name)));                               // 12\nThis line outputs the body of the page. Here it simply displays the greeting in a box. The greeting is,\nagain, a localised string, this time with a value substituted into a placeholder. The core renderer\n$OUTPUT  provides many convenience methods like box  to describe the required output in quite high-\nlevel terms. Different themes can control what HTML is actually output to make the box.\nThe content that originally came from the user ( $name ) is output though the format_string  function.\nThis is the other part of providing XSS protection. It also enables the user of text filters (another plugin\ntype). An example filter would be the LaTeX filter, which replaces input like $$x + 1$$  with an image\nof the equation. I will mention, but not explain, that there are actually three different functions ( s ,\nformat_string , and format_text ) depending on the particular type of content being output.\nLine 13: Finishing Output\n", "page": 148, "type": "text", "section": "Page 148"}
{"text": "echo $OUTPUT->footer();                                       // 13\nFinally, the footer of the page is output. This example does not show it, but Moodle tracks all the\nJavaScript that is required by the page, and outputs all the necessary script tags in the footer. This is\nstandard good practice. It allows users to see the page without waiting for all the JavaScript to load. A\ndeveloper would include JavaScript using API calls like $PAGE->requires-\n>js('/local/greet/cooleffect.js') .\nShould This Script Mix Logic and Output?\nObviously, putting the output code directly in index.php , even if at a high level of abstraction, limits\nthe flexibility that themes have to control the output. This is another sign of the age of the Moodle code-\nbase. The $OUTPUT  global was introduced in 2010 as a stepping stone on the way from the old code,\nwhere the output and controller code were in the same file, to a design where all the view code was\nproperly separated. This also explains the rather ugly way that the entire page layout is generated, then\nsplit in half, so that any output from the script itself can be placed between the header and the footer.\nOnce the view code has been separated out of the script, into what Moodle calls a renderer, the theme\ncan then choose to completely (or partially) override the view code for a given script.\nA small refactoring can move all the output code out of our index.php  and into a renderer. The end of\nindex.php  (lines 11 to 13) would change to:\n$output = $PAGE->get_renderer('local_greet');\necho $output->greeting_page($name);\nand there would be a new file local/greet/renderer.php :\n<?php\nclass local_greet_renderer extends plugin_renderer_base {\n    public function greeting_page($name) {\n        $output = '';\n        $output .= $this->header();\n        $output .= $this->box(get_string('greet', 'local_greet', $name));\n        $output .= $this->footer();\n        return $output;\n    }\n}\nIf the theme wished to completely change this output, it would define a subclass of this renderer that\noverrides the greeting_page  method. $PAGE->get_renderer()  determines the appropriate renderer\nclass to instantiate depending on the current theme. Thus, the output (view) code is fully separated from\nthe controller code in index.php , and the plugin has been refactored from typical legacy Moodle code\nto a clean MVC architecture.\n13.5. Database Abstraction\nThe \"Hello world\" script was sufficiently simple that it did not need to access the database, although\nseveral of the Moodle library calls used did do database queries. I will now briefly describe the Moodle\ndatabase layer.\nMoodle used to use the ADOdb library as the basis of its database abstraction layer, but there were\nissues for us, and the extra layer of library code had a noticeable impact on performance. Therefore, in\nMoodle 2.0 we switched to our own abstraction layer, which is a thin wrapper around the various PHP\ndatabase libraries.\nThe moodle_database  Class\nThe heart of the library is the moodle_database  class. This defines the interface provided by the $DB\nglobal variable, which gives access to the database connection. A typical usage might be:\n$course = $DB->get_record('course', array('id' => $courseid));\nThat translates into the SQL:\nSELECT * FROM mdl_course WHERE id = $courseid;\nand returns the data as a plain PHP object with public fields, so you could access $course->id ,\n$course->fullname , etc.\nSimple methods like this deal with basic queries, and simple updates and inserts. Sometimes it is\nnecessary to do more complex SQL, for example to run reports. In that case, there are methods to\nexecute arbitrary SQL:\n", "page": 149, "type": "text", "section": "Page 149"}
{"text": "$courseswithactivitycounts = $DB->get_records_sql(\n   'SELECT c.id, ' . $DB->sql_concat('shortname', \"' '\", 'fullname') . ' AS coursename,\n        COUNT(1) AS activitycount\n   FROM {course} c\n   JOIN {course_modules} cm ON cm.course = c.id\n   WHERE c.category = :categoryid\n   GROUP BY c.id, c.shortname, c.fullname ORDER BY c.shortname, c.fullname',\n   array('categoryid' => $category));\nSome things to note there:\nThe table names are wrapped in {}  so that the library can find them and prepend the table name\nprefix.\nThe library uses placeholders to insert values into the SQL. In some cases this uses the facilities of\nthe underlying database driver. In other cases the values have to be escaped and inserted into the\nSQL using string manipulation. The library supports both named placeholders (as above) and\nanonymous ones, using ?  as the placeholder.\nFor queries to work on all our supported databases a safe subset of standard SQL must be used.\nFor example, you can see that I have used the AS  keyword for column aliases, but not for table\naliases. Both of these usage rules are necessary.\nEven so, there are some situations where no subset of standard SQL will work on all our supported\ndatabases; for example, every database has a different way to concatenate strings. In these cases\nthere are compatibility functions to generate the correct SQL.\nDefining the Database Structure\nAnother area where database management systems differ a lot is in the SQL syntax required to define\ntables. To get around this problem, each Moodle plugin (and Moodle core) defines the required database\ntables in an XML file. The Moodle install system parses the install.xml  files and uses the\ninformation they contain to create the required tables and indexes. There is a developer tool called\nXMLDB built into Moodle to help create and edit these install files.\nIf the database structure needs to change between two releases of Moodle (or of a plugin) then the\ndeveloper is responsible for writing code (using an additional database object that provides DDL\nmethods) to update the database structure, while preserving all the users' data. Thus, Moodle will always\nself-update from one release to the next, simplifying maintenance for administrators.\nOne contentious point, stemming from the fact that Moodle started out using MySQL 3, is that the Moodle\ndatabase does not use foreign keys. This allows some buggy behaviour to remain undetected even\nthough modern databases would be capable of detecting the problem. The difficulty is that people have\nbeen running Moodle sites without foreign keys for years, so there is almost certainly inconsistent data\npresent. Adding the keys now would be impossible, without a very difficult clean-up job. Even so, since\nthe XMLDB system was added to Moodle 1.7 (in 2006!) the install.xml files have contained the\ndefinitions of the foreign keys that should exist, and we are still hoping, one day, to do all the work\nnecessary to allow us to create them during the install process.\n13.6. What Has Not Been Covered\nI hope I have given you a good overview of how Moodle works. Due to lack of space I have had to omit\nseveral interesting topics, including how authentication, enrolment and grade plugins allow Moodle to\ninteroperate with student information systems, and the interesting content-addressed way that Moodle\nstores uploaded files. Details of these, and other aspects of Moodle's design, can be found in the\ndeveloper documentation.\n13.7. Lessons Learned\nOne interesting aspect of working on Moodle is that it came out of a research project. Moodle enables\n(but does not enforce) a social constructivist pedagogy. That is, we learn best by actually creating\nsomething, and we learn from each other as a community. Martin Dougiamas's PhD question did not ask\nwhether this was an effective model for education, but rather whether it is an effective model for running\nan open source project. That is, can we view the Moodle project as an attempt to learn how to build and\nuse a VLE, and an attempt to learn that by actually building and using Moodle as a community where\nteachers, developers, administrators and students all teach and learn from each other? I find this a good\nmodel for thinking about an open source software development project. The main place where\ndevelopers and users learn from each other is in discussions in the Moodle project forums, and in the\nbug database.\nPerhaps the most important consequence of this learning approach is that you should not be afraid to\nstart by implementing the simplest possible solution first. For example, early versions of Moodle had just\na few hard-coded roles like Teacher, Student and Administrator. That was enough for many years, but\neventually the limitations had to be addressed. When the time came to design the Roles system for\nMoodle 1.7, there was a lot of experience in the community about how people were using Moodle, and\nmany little feature requests that showed what people needed to be able to adjust using a more flexible\naccess control system. This all helped design the Roles system to be as simple as possible, but as\ncomplex as necessary. (In fact, the first version of the roles system ended up slightly too complex, and it\n", "page": 150, "type": "text", "section": "Page 150"}
{"text": "This work is made available under the Creative Commons Attribution 3.0 Unported license. Please see\nthe full description of the license for details.\nwas subsequently simplified a little in Moodle 2.0.)\nIf you take the view that programming is a problem-solving exercise, then you might think that Moodle\ngot the design wrong the first time, and later had to waste time correcting it. I suggest that is an\nunhelpful viewpoint when trying to solve complex real-world problems. At the time Moodle started, no-\none knew enough to design the roles system we now have. If you take the learning viewpoint, then the\nvarious stages Moodle went through to reach the current design were necessary and inevitable.\nFor this perspective to work, it must be possible to change almost any aspect of a system's architecture\nonce you have learned more. I think Moodle shows that this is possible. For example, we found a way\nfor code to be gradually refactored from legacy scripts to a cleaner MVC architecture. This requires effort,\nbut it seems that when necessary, the resources to implement these changes can be found in open\nsource projects. From the user's point of view, the system gradually evolves with each major release.\nBack to top\nBack to The Architecture of Open Source Applications.\n", "page": 151, "type": "text", "section": "Page 151"}
{"text": "<>\nnginx (pronounced \"engine x\") is a free open source web server written by Igor Sysoev, a Russian\nsoftware engineer. Since its public launch in 2004, nginx has focused on high performance, high\nconcurrency and low memory usage. Additional features on top of the web server functionality, like load\nbalancing, caching, access and bandwidth control, and the ability to integrate efficiently with a variety of\napplications, have helped to make nginx a good choice for modern website architectures. Currently nginx\nis the second most popular open source web server on the Internet.\n14.1. Why Is High Concurrency Important?\nThese days the Internet is so widespread and ubiquitous it's hard to imagine it wasn't exactly there, as\nwe know it, a decade ago. It has greatly evolved, from simple HTML producing clickable text, based on\nNCSA and then on Apache web servers, to an always-on communication medium used by more than 2\nbillion users worldwide. With the proliferation of permanently connected PCs, mobile devices and recently\ntablets, the Internet landscape is rapidly changing and entire economies have become digitally wired.\nOnline services have become much more elaborate with a clear bias towards instantly available live\ninformation and entertainment. Security aspects of running online business have also significantly\nchanged. Accordingly, websites are now much more complex than before, and generally require a lot\nmore engineering efforts to be robust and scalable.\nOne of the biggest challenges for a website architect has always been concurrency. Since the beginning\nof web services, the level of concurrency has been continuously growing. It's not uncommon for a\npopular website to serve hundreds of thousands and even millions of simultaneous users. A decade ago,\nthe major cause of concurrency was slow clients\u2014users with ADSL or dial-up connections. Nowadays,\nconcurrency is caused by a combination of mobile clients and newer application architectures which are\ntypically based on maintaining a persistent connection that allows the client to be updated with news,\ntweets, friend feeds, and so on. Another important factor contributing to increased concurrency is the\nchanged behavior of modern browsers, which open four to six simultaneous connections to a website to\nimprove page load speed.\nTo illustrate the problem with slow clients, imagine a simple Apache-based web server which produces a\nrelatively short 100 KB response\u2014a web page with text or an image. It can be merely a fraction of a\nsecond to generate or retrieve this page, but it takes 10 seconds to transmit it to a client with a\nbandwidth of 80 kbps (10 KB/s). Essentially, the web server would relatively quickly pull 100 KB of\ncontent, and then it would be busy for 10 seconds slowly sending this content to the client before freeing\nits connection. Now imagine that you have 1,000 simultaneously connected clients who have requested\nsimilar content. If only 1 MB of additional memory is allocated per client, it would result in 1000 MB\n(about 1 GB) of extra memory devoted to serving just 1000 clients 100 KB of content. In reality, a typical\nweb server based on Apache commonly allocates more than 1 MB of additional memory per connection,\nand regrettably tens of kbps is still often the effective speed of mobile communications. Although the\nsituation with sending content to a slow client might be, to some extent, improved by increasing the size\nof operating system kernel socket buffers, it's not a general solution to the problem and can have\nundesirable side effects.\nWith persistent connections the problem of handling concurrency is even more pronounced, because to\navoid latency associated with establishing new HTTP connections, clients would stay connected, and for\neach connected client there's a certain amount of memory allocated by the web server.\nConsequently, to handle the increased workloads associated with growing audiences and hence higher\nlevels of concurrency\u2014and to be able to continuously do so\u2014a website should be based on a number of\nvery efficient building blocks. While the other parts of the equation such as hardware (CPU, memory,\ndisks), network capacity, application and data storage architectures are obviously important, it is in the\nweb server software that client connections are accepted and processed. Thus, the web server should be\nable to scale nonlinearly with the growing number of simultaneous connections and requests per second.\nIsn't Apache Suitable?\nApache, the web server software that still largely dominates the Internet today, has its roots in the\nnginx\nAndrew Alexeev\n", "page": 152, "type": "text", "section": "Page 152"}
{"text": "beginning of the 1990s. Originally, its architecture matched the then-existing operating systems and\nhardware, but also the state of the Internet, where a website was typically a standalone physical server\nrunning a single instance of Apache. By the beginning of the 2000s it was obvious that the standalone\nweb server model could not be easily replicated to satisfy the needs of growing web services. Although\nApache provided a solid foundation for future development, it was architected to spawn a copy of itself\nfor each new connection, which was not suitable for nonlinear scalability of a website. Eventually Apache\nbecame a general purpose web server focusing on having many different features, a variety of third-party\nextensions, and universal applicability to practically any kind of web application development. However,\nnothing comes without a price and the downside to having such a rich and universal combination of tools\nin a single piece of software is less scalability because of increased CPU and memory usage per\nconnection.\nThus, when server hardware, operating systems and network resources ceased to be major constraints\nfor website growth, web developers worldwide started to look around for a more efficient means of\nrunning web servers. Around ten years ago, Daniel Kegel, a prominent software engineer, proclaimed\nthat \"it's time for web servers to handle ten thousand clients simultaneously\" and predicted what we now\ncall Internet cloud services. Kegel's C10K manifest spurred a number of attempts to solve the problem of\nweb server optimization to handle a large number of clients at the same time, and nginx turned out to be\none of the most successful ones.\nAimed at solving the C10K problem of 10,000 simultaneous connections, nginx was written with a\ndifferent architecture in mind\u2014one which is much more suitable for nonlinear scalability in both the\nnumber of simultaneous connections and requests per second. nginx is event-based, so it does not\nfollow Apache's style of spawning new processes or threads for each web page request. The end result\nis that even as load increases, memory and CPU usage remain manageable. nginx can now deliver tens\nof thousands of concurrent connections on a server with typical hardware.\nWhen the first version of nginx was released, it was meant to be deployed alongside Apache such that\nstatic content like HTML, CSS, JavaScript and images were handled by nginx to offload concurrency and\nlatency processing from Apache-based application servers. Over the course of its development, nginx\nhas added integration with applications through the use of FastCGI, uswgi or SCGI protocols, and with\ndistributed memory object caching systems like memcached. Other useful functionality like reverse proxy\nwith load balancing and caching was added as well. These additional features have shaped nginx into an\nefficient combination of tools to build a scalable web infrastructure upon.\nIn February 2012, the Apache 2.4.x branch was released to the public. Although this latest release of\nApache has added new multi-processing core modules and new proxy modules aimed at enhancing\nscalability and performance, it's too soon to tell if its performance, concurrency and resource utilization\nare now on par with, or better than, pure event-driven web servers. It would be very nice to see Apache\napplication servers scale better with the new version, though, as it could potentially alleviate bottlenecks\non the backend side which still often remain unsolved in typical nginx-plus-Apache web configurations.\nAre There More Advantages to Using nginx?\nHandling high concurrency with high performance and efficiency has always been the key benefit of\ndeploying nginx. However, there are now even more interesting benefits.\nIn the last few years, web architects have embraced the idea of decoupling and separating their\napplication infrastructure from the web server. However, what would previously exist in the form of a\nLAMP (Linux, Apache, MySQL, PHP, Python or Perl)-based website, might now become not merely a\nLEMP-based one (`E' standing for `Engine x'), but more and more often an exercise in pushing the web\nserver to the edge of the infrastructure and integrating the same or a revamped set of applications and\ndatabase tools around it in a different way.\nnginx is very well suited for this, as it provides the key features necessary to conveniently offload\nconcurrency, latency processing, SSL (secure sockets layer), static content, compression and caching,\nconnections and requests throttling, and even HTTP media streaming from the application layer to a\nmuch more efficient edge web server layer. It also allows integrating directly with memcached/Redis or\nother \"NoSQL\" solutions, to boost performance when serving a large number of concurrent users.\nWith recent flavors of development kits and programming languages gaining wide use, more and more\ncompanies are changing their application development and deployment habits. nginx has become one of\nthe most important components of these changing paradigms, and it has already helped many companies\nstart and develop their web services quickly and within their budgets.\nThe first lines of nginx were written in 2002. In 2004 it was released to the public under the two-clause\nBSD license. The number of nginx users has been growing ever since, contributing ideas, and submitting\nbug reports, suggestions and observations that have been immensely helpful and beneficial for the entire\ncommunity.\nThe nginx codebase is original and was written entirely from scratch in the C programming language.\nnginx has been ported to many architectures and operating systems, including Linux, FreeBSD, Solaris,\nMac OS X, AIX and Microsoft Windows. nginx has its own libraries and with its standard modules does\nnot use much beyond the system's C library, except for zlib, PCRE and OpenSSL which can be\noptionally excluded from a build if not needed or because of potential license conflicts.\nA few words about the Windows version of nginx. While nginx works in a Windows environment, the\n", "page": 153, "type": "text", "section": "Page 153"}
{"text": "Windows version of nginx is more like a proof-of-concept rather than a fully functional port. There are\ncertain limitations of the nginx and Windows kernel architectures that do not interact well at this time. The\nknown issues of the nginx version for Windows include a much lower number of concurrent connections,\ndecreased performance, no caching and no bandwidth policing. Future versions of nginx for Windows will\nmatch the mainstream functionality more closely.\n14.2. Overview of nginx Architecture\nTraditional process- or thread-based models of handling concurrent connections involve handling each\nconnection with a separate process or thread, and blocking on network or input/output operations.\nDepending on the application, it can be very inefficient in terms of memory and CPU consumption.\nSpawning a separate process or thread requires preparation of a new runtime environment, including\nallocation of heap and stack memory, and the creation of a new execution context. Additional CPU time\nis also spent creating these items, which can eventually lead to poor performance due to thread thrashing\non excessive context switching. All of these complications manifest themselves in older web server\narchitectures like Apache's. This is a tradeoff between offering a rich set of generally applicable features\nand optimized usage of server resources.\nFrom the very beginning, nginx was meant to be a specialized tool to achieve more performance, density\nand economical use of server resources while enabling dynamic growth of a website, so it has followed a\ndifferent model. It was actually inspired by the ongoing development of advanced event-based\nmechanisms in a variety of operating systems. What resulted is a modular, event-driven, asynchronous,\nsingle-threaded, non-blocking architecture which became the foundation of nginx code.\nnginx uses multiplexing and event notifications heavily, and dedicates specific tasks to separate\nprocesses. Connections are processed in a highly efficient run-loop in a limited number of single-\nthreaded processes called worker s. Within each worker  nginx can handle many thousands of\nconcurrent connections and requests per second.\nCode Structure\nThe nginx worker  code includes the core and the functional modules. The core of nginx is responsible\nfor maintaining a tight run-loop and executing appropriate sections of modules' code on each stage of\nrequest processing. Modules constitute most of the presentation and application layer functionality.\nModules read from and write to the network and storage, transform content, do outbound filtering, apply\nserver-side include actions and pass the requests to the upstream servers when proxying is activated.\nnginx's modular architecture generally allows developers to extend the set of web server features without\nmodifying the nginx core. nginx modules come in slightly different incarnations, namely core modules,\nevent modules, phase handlers, protocols, variable handlers, filters, upstreams and load balancers. At\nthis time, nginx doesn't support dynamically loaded modules; i.e., modules are compiled along with the\ncore at build stage. However, support for loadable modules and ABI is planned for the future major\nreleases. More detailed information about the roles of different modules can be found in Section 14.4.\nWhile handling a variety of actions associated with accepting, processing and managing network\nconnections and content retrieval, nginx uses event notification mechanisms and a number of disk I/O\nperformance enhancements in Linux, Solaris and BSD-based operating systems, like kqueue , epoll ,\nand event ports . The goal is to provide as many hints to the operating system as possible, in regards\nto obtaining timely asynchronous feedback for inbound and outbound traffic, disk operations, reading from\nor writing to sockets, timeouts and so on. The usage of different methods for multiplexing and advanced\nI/O operations is heavily optimized for every Unix-based operating system nginx runs on.\nA high-level overview of nginx architecture is presented in Figure 14.1.\n", "page": 154, "type": "text", "section": "Page 154"}
{"text": "Figure 14.1: Diagram of nginx's architecture\nWorkers Model\nAs previously mentioned, nginx doesn't spawn a process or thread for every connection. Instead,\nworker  processes accept new requests from a shared \"listen\" socket and execute a highly efficient run-\nloop inside each worker  to process thousands of connections per worker . There's no specialized\narbitration or distribution of connections to the worker s in nginx; this work is done by the OS kernel\nmechanisms. Upon startup, an initial set of listening sockets is created. worker s then continuously\naccept, read from and write to the sockets while processing HTTP requests and responses.\nThe run-loop is the most complicated part of the nginx worker  code. It includes comprehensive inner\ncalls and relies heavily on the idea of asynchronous task handling. Asynchronous operations are\nimplemented through modularity, event notifications, extensive use of callback functions and fine-tuned\ntimers. Overall, the key principle is to be as non-blocking as possible. The only situation where nginx can\nstill block is when there's not enough disk storage performance for a worker  process.\nBecause nginx does not fork a process or thread per connection, memory usage is very conservative and\nextremely efficient in the vast majority of cases. nginx conserves CPU cycles as well because there's no\nongoing create-destroy pattern for processes or threads. What nginx does is check the state of the\nnetwork and storage, initialize new connections, add them to the run-loop, and process asynchronously\nuntil completion, at which point the connection is deallocated and removed from the run-loop. Combined\nwith the careful use of syscall s and an accurate implementation of supporting interfaces like pool and\nslab memory allocators, nginx typically achieves moderate-to-low CPU usage even under extreme\nworkloads.\nBecause nginx spawns several worker s to handle connections, it scales well across multiple cores.\nGenerally, a separate worker  per core allows full utilization of multicore architectures, and prevents\nthread thrashing and lock-ups. There's no resource starvation and the resource controlling mechanisms\nare isolated within single-threaded worker  processes. This model also allows more scalability across\nphysical storage devices, facilitates more disk utilization and avoids blocking on disk I/O. As a result,\nserver resources are utilized more efficiently with the workload shared across several workers.\nWith some disk use and CPU load patterns, the number of nginx worker s should be adjusted. The\nrules are somewhat basic here, and system administrators should try a couple of configurations for their\nworkloads. General recommendations might be the following: if the load pattern is CPU intensive\u2014for\ninstance, handling a lot of TCP/IP, doing SSL, or compression\u2014the number of nginx worker s should\nmatch the number of CPU cores; if the load is mostly disk I/O bound\u2014for instance, serving different sets\nof content from storage, or heavy proxying\u2014the number of worker s might be one and a half to two\ntimes the number of cores. Some engineers choose the number of worker s based on the number of\nindividual storage units instead, though efficiency of this approach depends on the type and configuration\nof disk storage.\nOne major problem that the developers of nginx will be solving in upcoming versions is how to avoid\nmost of the blocking on disk I/O. At the moment, if there's not enough storage performance to serve disk\noperations generated by a particular worker , that worker  may still block on reading/writing from disk.\nA number of mechanisms and configuration file directives exist to mitigate such disk I/O blocking\nscenarios. Most notably, combinations of options like sendfile and AIO typically produce a lot of\nheadroom for disk performance. An nginx installation should be planned based on the data set, the\namount of memory available for nginx, and the underlying storage architecture.\nAnother problem with the existing worker  model is related to limited support for embedded scripting.\nFor one, with the standard nginx distribution, only embedding Perl scripts is supported. There is a simple\nexplanation for that: the key problem is the possibility of an embedded script to block on any operation or\nexit unexpectedly. Both types of behavior would immediately lead to a situation where the worker is\nhung, affecting many thousands of connections at once. More work is planned to make embedded\nscripting with nginx simpler, more reliable and suitable for a broader range of applications.\nnginx Process Roles\nnginx runs several processes in memory; there is a single master process and several worker\nprocesses. There are also a couple of special purpose processes, specifically a cache loader and cache\nmanager. All processes are single-threaded in version 1.x of nginx. All processes primarily use shared-\nmemory mechanisms for inter-process communication. The master process is run as the root  user.\nThe cache loader, cache manager and worker s run as an unprivileged user.\nThe master process is responsible for the following tasks:\nreading and validating configuration\ncreating, binding and closing sockets\nstarting, terminating and maintaining the configured number of worker  processes\nreconfiguring without service interruption\ncontrolling non-stop binary upgrades (starting new binary and rolling back if necessary)\nre-opening log files\ncompiling embedded Perl scripts\nThe worker  processes accept, handle and process connections from clients, provide reverse proxying\n", "page": 155, "type": "text", "section": "Page 155"}
{"text": "and filtering functionality and do almost everything else that nginx is capable of. In regards to monitoring\nthe behavior of an nginx instance, a system administrator should keep an eye on worker s as they are\nthe processes reflecting the actual day-to-day operations of a web server.\nThe cache loader process is responsible for checking the on-disk cache items and populating nginx's in-\nmemory database with cache metadata. Essentially, the cache loader prepares nginx instances to work\nwith files already stored on disk in a specially allocated directory structure. It traverses the directories,\nchecks cache content metadata, updates the relevant entries in shared memory and then exits when\neverything is clean and ready for use.\nThe cache manager is mostly responsible for cache expiration and invalidation. It stays in memory during\nnormal nginx operation and it is restarted by the master process in the case of failure.\nBrief Overview of nginx Caching\nCaching in nginx is implemented in the form of hierarchical data storage on a filesystem. Cache keys are\nconfigurable, and different request-specific parameters can be used to control what gets into the cache.\nCache keys and cache metadata are stored in the shared memory segments, which the cache loader,\ncache manager and worker s can access. Currently there is not any in-memory caching of files, other\nthan optimizations implied by the operating system's virtual filesystem mechanisms. Each cached\nresponse is placed in a different file on the filesystem. The hierarchy (levels and naming details) are\ncontrolled through nginx configuration directives. When a response is written to the cache directory\nstructure, the path and the name of the file are derived from an MD5 hash of the proxy URL.\nThe process for placing content in the cache is as follows: When nginx reads the response from an\nupstream server, the content is first written to a temporary file outside of the cache directory structure.\nWhen nginx finishes processing the request it renames the temporary file and moves it to the cache\ndirectory. If the temporary files directory for proxying is on another file system, the file will be copied,\nthus it's recommended to keep both temporary and cache directories on the same file system. It is also\nquite safe to delete files from the cache directory structure when they need to be explicitly purged. There\nare third-party extensions for nginx which make it possible to control cached content remotely, and more\nwork is planned to integrate this functionality in the main distribution.\n14.3. nginx Configuration\nnginx's configuration system was inspired by Igor Sysoev's experiences with Apache. His main insight\nwas that a scalable configuration system is essential for a web server. The main scaling problem was\nencountered when maintaining large complicated configurations with lots of virtual servers, directories,\nlocations and datasets. In a relatively big web setup it can be a nightmare if not done properly both at the\napplication level and by the system engineer himself.\nAs a result, nginx configuration was designed to simplify day-to-day operations and to provide an easy\nmeans for further expansion of web server configuration.\nnginx configuration is kept in a number of plain text files which typically reside in\n/usr/local/etc/nginx  or /etc/nginx . The main configuration file is usually called nginx.conf .\nTo keep it uncluttered, parts of the configuration can be put in separate files which can be automatically\nincluded in the main one. However, it should be noted here that nginx does not currently support\nApache-style distributed configurations (i.e., .htaccess  files). All of the configuration relevant to nginx\nweb server behavior should reside in a centralized set of configuration files.\nThe configuration files are initially read and verified by the master process. A compiled read-only form of\nthe nginx configuration is available to the worker  processes as they are forked from the master\nprocess. Configuration structures are automatically shared by the usual virtual memory management\nmechanisms.\nnginx configuration has several different contexts for main , http , server , upstream , location\n(and also mail  for mail proxy) blocks of directives. Contexts never overlap. For instance, there is no\nsuch thing as putting a location  block in the main  block of directives. Also, to avoid unnecessary\nambiguity there isn't anything like a \"global web server\" configuration. nginx configuration is meant to be\nclean and logical, allowing users to maintain complicated configuration files that comprise thousands of\ndirectives. In a private conversation, Sysoev said, \"Locations, directories, and other blocks in the global\nserver configuration are the features I never liked in Apache, so this is the reason why they were never\nimplemented in nginx.\"\nConfiguration syntax, formatting and definitions follow a so-called C-style convention. This particular\napproach to making configuration files is already being used by a variety of open source and commercial\nsoftware applications. By design, C-style configuration is well-suited for nested descriptions, being logical\nand easy to create, read and maintain, and liked by many engineers. C-style configuration of nginx can\nalso be easily automated.\nWhile some of the nginx directives resemble certain parts of Apache configuration, setting up an nginx\ninstance is quite a different experience. For instance, rewrite rules are supported by nginx, though it\nwould require an administrator to manually adapt a legacy Apache rewrite configuration to match nginx\nstyle. The implementation of the rewrite engine differs too.\nIn general, nginx settings also provide support for several original mechanisms that can be very useful as\n", "page": 156, "type": "text", "section": "Page 156"}
{"text": "part of a lean web server configuration. It makes sense to briefly mention variables and the try_files\ndirective, which are somewhat unique to nginx. Variables in nginx were developed to provide an\nadditional even-more-powerful mechanism to control run-time configuration of a web server. Variables\nare optimized for quick evaluation and are internally pre-compiled to indices. Evaluation is done on\ndemand; i.e., the value of a variable is typically calculated only once and cached for the lifetime of a\nparticular request. Variables can be used with different configuration directives, providing additional\nflexibility for describing conditional request processing behavior.\nThe try_files  directive was initially meant to gradually replace conditional if  configuration\nstatements in a more proper way, and it was designed to quickly and efficiently try/match against different\nURI-to-content mappings. Overall, the try_files  directive works well and can be extremely efficient\nand useful. It is recommended that the reader thoroughly check the try_files  directive and adopt its\nuse whenever applicable.\n14.4. nginx Internals\nAs was mentioned before, the nginx codebase consists of a core and a number of modules. The core of\nnginx is responsible for providing the foundation of the web server, web and mail reverse proxy\nfunctionalities; it enables the use of underlying network protocols, builds the necessary run-time\nenvironment, and ensures seamless interaction between different modules. However, most of the\nprotocol- and application-specific features are done by nginx modules, not the core.\nInternally, nginx processes connections through a pipeline, or chain, of modules. In other words, for every\noperation there's a module which is doing the relevant work; e.g., compression, modifying content,\nexecuting server-side includes, communicating to the upstream application servers through FastCGI or\nuwsgi protocols, or talking to memcached.\nThere are a couple of nginx modules that sit somewhere between the core and the real \"functional\"\nmodules. These modules are http  and mail . These two modules provide an additional level of\nabstraction between the core and lower-level components. In these modules, the handling of the\nsequence of events associated with a respective application layer protocol like HTTP, SMTP or IMAP is\nimplemented. In combination with the nginx core, these upper-level modules are responsible for\nmaintaining the right order of calls to the respective functional modules. While the HTTP protocol is\ncurrently implemented as part of the http  module, there are plans to separate it into a functional\nmodule in the future, due to the need to support other protocols like SPDY (see \"SPDY: An experimental\nprotocol for a faster web\").\nThe functional modules can be divided into event modules, phase handlers, output filters, variable\nhandlers, protocols, upstreams and load balancers. Most of these modules complement the HTTP\nfunctionality of nginx, though event modules and protocols are also used for mail . Event modules\nprovide a particular OS-dependent event notification mechanism like kqueue  or epoll . The event\nmodule that nginx uses depends on the operating system capabilities and build configuration. Protocol\nmodules allow nginx to communicate through HTTPS, TLS/SSL, SMTP, POP3 and IMAP.\nA typical HTTP request processing cycle looks like the following.\n1. Client sends HTTP request.\n2. nginx core chooses the appropriate phase handler based on the configured location matching the\nrequest.\n3. If configured to do so, a load balancer picks an upstream server for proxying.\n4. Phase handler does its job and passes each output buffer to the first filter.\n5. First filter passes the output to the second filter.\n6. Second filter passes the output to third (and so on).\n7. Final response is sent to the client.\nnginx module invocation is extremely customizable. It is performed through a series of callbacks using\npointers to the executable functions. However, the downside of this is that it may place a big burden on\nprogrammers who would like to write their own modules, because they must define exactly how and\nwhen the module should run. Both the nginx API and developers' documentation are being improved and\nmade more available to alleviate this.\nSome examples of where a module can attach are:\nBefore the configuration file is read and processed\nFor each configuration directive for the location and the server where it appears\nWhen the main configuration is initialized\nWhen the server (i.e., host/port) is initialized\nWhen the server configuration is merged with the main configuration\nWhen the location configuration is initialized or merged with its parent server configuration\nWhen the master process starts or exits\nWhen a new worker process starts or exits\nWhen handling a request\nWhen filtering the response header and the body\nWhen picking, initiating and re-initiating a request to an upstream server\nWhen processing the response from an upstream server\n", "page": 157, "type": "text", "section": "Page 157"}
{"text": "When finishing an interaction with an upstream server\nInside a worker , the sequence of actions leading to the run-loop where the response is generated\nlooks like the following:\n1. Begin ngx_worker_process_cycle() .\n2. Process events with OS specific mechanisms (such as epoll  or kqueue ).\n3. Accept events and dispatch the relevant actions.\n4. Process/proxy request header and body.\n5. Generate response content (header, body) and stream it to the client.\n6. Finalize request.\n7. Re-initialize timers and events.\nThe run-loop itself (steps 5 and 6) ensures incremental generation of a response and streaming it to the\nclient.\nA more detailed view of processing an HTTP request might look like this:\n1. Initialize request processing.\n2. Process header.\n3. Process body.\n4. Call the associated handler.\n5. Run through the processing phases.\nWhich brings us to the phases. When nginx handles an HTTP request, it passes it through a number of\nprocessing phases. At each phase there are handlers to call. In general, phase handlers process a\nrequest and produce the relevant output. Phase handlers are attached to the locations defined in the\nconfiguration file.\nPhase handlers typically do four things: get the location configuration, generate an appropriate response,\nsend the header, and send the body. A handler has one argument: a specific structure describing the\nrequest. A request structure has a lot of useful information about the client request, such as the request\nmethod, URI, and header.\nWhen the HTTP request header is read, nginx does a lookup of the associated virtual server\nconfiguration. If the virtual server is found, the request goes through six phases:\n1. server rewrite phase\n2. location phase\n3. location rewrite phase (which can bring the request back to the previous phase)\n4. access control phase\n5. try_files phase\n6. log phase\nIn an attempt to generate the necessary content in response to the request, nginx passes the request to\na suitable content handler. Depending on the exact location configuration, nginx may try so-called\nunconditional handlers first, like perl , proxy_pass , flv , mp4 , etc. If the request does not match\nany of the above content handlers, it is picked by one of the following handlers, in this exact order:\nrandom index , index , autoindex , gzip_static , static .\nIndexing module details can be found in the nginx documentation, but these are the modules which\nhandle requests with a trailing slash. If a specialized module like mp4  or autoindex  isn't appropriate,\nthe content is considered to be just a file or directory on disk (that is, static) and is served by the\nstatic  content handler. For a directory it would automatically rewrite the URI so that the trailing slash\nis always there (and then issue an HTTP redirect).\nThe content handlers' content is then passed to the filters. Filters are also attached to locations, and there\ncan be several filters configured for a location. Filters do the task of manipulating the output produced by\na handler. The order of filter execution is determined at compile time. For the out-of-the-box filters it's\npredefined, and for a third-party filter it can be configured at the build stage. In the existing nginx\nimplementation, filters can only do outbound changes and there is currently no mechanism to write and\nattach filters to do input content transformation. Input filtering will appear in future versions of nginx.\nFilters follow a particular design pattern. A filter gets called, starts working, and calls the next filter until\nthe final filter in the chain is called. After that, nginx finalizes the response. Filters don't have to wait for\nthe previous filter to finish. The next filter in a chain can start its own work as soon as the input from the\nprevious one is available (functionally much like the Unix pipeline). In turn, the output response being\ngenerated can be passed to the client before the entire response from the upstream server is received.\nThere are header filters and body filters; nginx feeds the header and the body of the response to the\nassociated filters separately.\nA header filter consists of three basic steps:\n1. Decide whether to operate on this response.\n2. Operate on the response.\n3. Call the next filter.\nBody filters transform the generated content. Examples of body filters include:\n", "page": 158, "type": "text", "section": "Page 158"}
{"text": "server-side includes\nXSLT filtering\nimage filtering (for instance, resizing images on the fly)\ncharset modification\ngzip  compression\nchunked encoding\nAfter the filter chain, the response is passed to the writer. Along with the writer there are a couple of\nadditional special purpose filters, namely the copy  filter, and the postpone  filter. The copy  filter is\nresponsible for filling memory buffers with the relevant response content which might be stored in a proxy\ntemporary directory. The postpone  filter is used for subrequests.\nSubrequests are a very important mechanism for request/response processing. Subrequests are also one\nof the most powerful aspects of nginx. With subrequests nginx can return the results from a different URL\nthan the one the client originally requested. Some web frameworks call this an internal redirect. However,\nnginx goes further\u2014not only can filters perform multiple subrequests and combine the outputs into a\nsingle response, but subrequests can also be nested and hierarchical. A subrequest can perform its own\nsub-subrequest, and a sub-subrequest can initiate sub-sub-subrequests. Subrequests can map to files on\nthe hard disk, other handlers, or upstream servers. Subrequests are most useful for inserting additional\ncontent based on data from the original response. For example, the SSI (server-side include) module\nuses a filter to parse the contents of the returned document, and then replaces include  directives with\nthe contents of specified URLs. Or, it can be an example of making a filter that treats the entire contents\nof a document as a URL to be retrieved, and then appends the new document to the URL itself.\nUpstream and load balancers are also worth describing briefly. Upstreams are used to implement what\ncan be identified as a content handler which is a reverse proxy ( proxy_pass  handler). Upstream\nmodules mostly prepare the request to be sent to an upstream server (or \"backend\") and receive the\nresponse from the upstream server. There are no calls to output filters here. What an upstream module\ndoes exactly is set callbacks to be invoked when the upstream server is ready to be written to and read\nfrom. Callbacks implementing the following functionality exist:\nCrafting a request buffer (or a chain of them) to be sent to the upstream server\nRe-initializing/resetting the connection to the upstream server (which happens right before creating\nthe request again)\nProcessing the first bits of an upstream response and saving pointers to the payload received from\nthe upstream server\nAborting requests (which happens when the client terminates prematurely)\nFinalizing the request when nginx finishes reading from the upstream server\nTrimming the response body (e.g. removing a trailer)\nLoad balancer modules attach to the proxy_pass  handler to provide the ability to choose an upstream\nserver when more than one upstream server is eligible. A load balancer registers an enabling\nconfiguration file directive, provides additional upstream initialization functions (to resolve upstream\nnames in DNS, etc.), initializes the connection structures, decides where to route the requests, and\nupdates stats information. Currently nginx supports two standard disciplines for load balancing to\nupstream servers: round-robin and ip-hash.\nUpstream and load balancing handling mechanisms include algorithms to detect failed upstream servers\nand to re-route new requests to the remaining ones\u2014though a lot of additional work is planned to\nenhance this functionality. In general, more work on load balancers is planned, and in the next versions\nof nginx the mechanisms for distributing the load across different upstream servers as well as health\nchecks will be greatly improved.\nThere are also a couple of other interesting modules which provide an additional set of variables for use\nin the configuration file. While the variables in nginx are created and updated across different modules,\nthere are two modules that are entirely dedicated to variables: geo  and map . The geo  module is\nused to facilitate tracking of clients based on their IP addresses. This module can create arbitrary\nvariables that depend on the client's IP address. The other module, map , allows for the creation of\nvariables from other variables, essentially providing the ability to do flexible mappings of hostnames and\nother run-time variables. This kind of module may be called the variable handler.\nMemory allocation mechanisms implemented inside a single nginx worker  were, to some extent,\ninspired by Apache. A high-level description of nginx memory management would be the following: For\neach connection, the necessary memory buffers are dynamically allocated, linked, used for storing and\nmanipulating the header and body of the request and the response, and then freed upon connection\nrelease. It is very important to note that nginx tries to avoid copying data in memory as much as possible\nand most of the data is passed along by pointer values, not by calling memcpy .\nGoing a bit deeper, when the response is generated by a module, the retrieved content is put in a\nmemory buffer which is then added to a buffer chain link. Subsequent processing works with this buffer\nchain link as well. Buffer chains are quite complicated in nginx because there are several processing\nscenarios which differ depending on the module type. For instance, it can be quite tricky to manage the\nbuffers precisely while implementing a body filter module. Such a module can only operate on one buffer\n(chain link) at a time and it must decide whether to overwrite the input buffer, replace the buffer with a\nnewly allocated buffer, or insert a new buffer before or after the buffer in question. To complicate things,\n", "page": 159, "type": "text", "section": "Page 159"}
{"text": "This work is made available under the Creative Commons Attribution 3.0 Unported license. Please see\nthe full description of the license for details.\nsometimes a module will receive several buffers so that it has an incomplete buffer chain that it must\noperate on. However, at this time nginx provides only a low-level API for manipulating buffer chains, so\nbefore doing any actual implementation a third-party module developer should become really fluent with\nthis arcane part of nginx.\nA note on the above approach is that there are memory buffers allocated for the entire life of a\nconnection, thus for long-lived connections some extra memory is kept. At the same time, on an idle\nkeepalive connection, nginx spends just 550 bytes of memory. A possible optimization for future releases\nof nginx would be to reuse and share memory buffers for long-lived connections.\nThe task of managing memory allocation is done by the nginx pool allocator. Shared memory areas are\nused to accept mutex, cache metadata, the SSL session cache and the information associated with\nbandwidth policing and management (limits). There is a slab allocator implemented in nginx to manage\nshared memory allocation. To allow simultaneous safe use of shared memory, a number of locking\nmechanisms are available (mutexes and semaphores). In order to organize complex data structures,\nnginx also provides a red-black tree implementation. Red-black trees are used to keep cache metadata\nin shared memory, track non-regex location definitions and for a couple of other tasks.\nUnfortunately, all of the above was never described in a consistent and simple manner, making the job of\ndeveloping third-party extensions for nginx quite complicated. Although some good documents on nginx\ninternals exist\u2014for instance, those produced by Evan Miller\u2014such documents required a huge reverse\nengineering effort, and the implementation of nginx modules is still a black art for many.\nDespite certain difficulties associated with third-party module development, the nginx user community\nrecently saw a lot of useful third-party modules. There is, for instance, an embedded Lua interpreter\nmodule for nginx, additional modules for load balancing, full WebDAV support, advanced cache control\nand other interesting third-party work that the authors of this chapter encourage and will support in the\nfuture.\n14.5. Lessons Learned\nWhen Igor Sysoev started to write nginx, most of the software enabling the Internet already existed, and\nthe architecture of such software typically followed definitions of legacy server and network hardware,\noperating systems, and old Internet architecture in general. However, this didn't prevent Igor from thinking\nhe might be able to improve things in the web servers area. So, while the first lesson might seem\nobvious, it is this: there is always room for improvement.\nWith the idea of better web software in mind, Igor spent a lot of time developing the initial code structure\nand studying different ways of optimizing the code for a variety of operating systems. Ten years later he\nis developing a prototype of nginx version 2.0, taking into account the years of active development on\nversion 1. It is clear that the initial prototype of a new architecture, and the initial code structure, are\nvitally important for the future of a software product.\nAnother point worth mentioning is that development should be focused. The Windows version of nginx is\nprobably a good example of how it is worth avoiding the dilution of development efforts on something that\nis neither the developer's core competence or the target application. It is equally applicable to the rewrite\nengine that appeared during several attempts to enhance nginx with more features for backward\ncompatibility with the existing legacy setups.\nLast but not least, it is worth mentioning that despite the fact that the nginx developer community is not\nvery large, third-party modules and extensions for nginx have always been a very important part of its\npopularity. The work done by Evan Miller, Piotr Sikora, Valery Kholodkov, Zhang Yichun (agentzh) and\nother talented software engineers has been much appreciated by the nginx user community and its\noriginal developers.\nBack to top\nBack to The Architecture of Open Source Applications.\n", "page": 160, "type": "text", "section": "Page 160"}
{"text": "<>\n15.1. Background\nOpen MPI [GFB+04] is an open source software implementation of The Message Passing Interface (MPI)\nstandard. Before the architecture and innards of Open MPI will make any sense, a little background on\nthe MPI standard must be discussed.\nThe Message Passing Interface (MPI)\nThe MPI standard is created and maintained by the MPI Forum, an open group consisting of parallel\ncomputing experts from both industry and academia. MPI defines an API that is used for a specific type\nof portable, high-performance inter-process communication (IPC): message passing. Specifically, the MPI\ndocument describes the reliable transfer of discrete, typed messages between MPI processes. Although\nthe definition of an \"MPI process\" is subject to interpretation on a given platform, it usually corresponds\nto the operating system's concept of a process (e.g., a POSIX process). MPI is specifically intended to be\nimplemented as middleware, meaning that upper-level applications call MPI functions to perform\nmessage passing.\nMPI defines a high-level API, meaning that it abstracts away whatever underlying transport is actually\nused to pass messages between processes. The idea is that sending-process X can effectively say \"take\nthis array of 1,073 double precision values and send them to process Y\". The corresponding receiving-\nprocess Y effectively says \"receive an array of 1,073 double precision values from process X.\" A miracle\noccurs, and the array of 1,073 double precision values arrives in Y's waiting buffer.\nNotice what is absent in this exchange: there is no concept of a connection occurring, no stream of bytes\nto interpret, and no network addresses exchanged. MPI abstracts all of that away, not only to hide such\ncomplexity from the upper-level application, but also to make the application portable across different\nenvironments and underlying message passing transports. Specifically, a correct MPI application is\nsource-compatible across a wide variety of platforms and network types.\nMPI defines not only point-to-point communication (e.g., send and receive), it also defines other\ncommunication patterns, such as collective communication. Collective operations are where multiple\nprocesses are involved in a single communication action. Reliable broadcast, for example, is where one\nprocess has a message at the beginning of the operation, and at the end of the operation, all processes\nin a group have the message. MPI also defines other concepts and communications patterns that are not\ndescribed here. (As of this writing, the most recent version of the MPI standard is MPI-2.2 [For09]. Draft\nversions of the upcoming MPI-3 standard have been published; it may be finalized as early as late 2012.)\nUses of MPI\nThere are many implementations of the MPI standard that support a wide variety of platforms, operating\nsystems, and network types. Some implementations are open source, some are closed source. Open\nMPI, as its name implies, is one of the open source implementations. Typical MPI transport networks\ninclude (but are not limited to): various protocols over Ethernet (e.g., TCP, iWARP, UDP, raw Ethernet\nframes, etc.), shared memory, and InfiniBand.\nMPI implementations are typically used in so-called \"high-performance computing\" (HPC) environments.\nMPI essentially provides the IPC for simulation codes, computational algorithms, and other \"big number\ncrunching\" types of applications. The input data sets on which these codes operate typically represent\ntoo much computational work for just one server; MPI jobs are spread out across tens, hundreds, or even\nthousands of servers, all working in concert to solve one computational problem.\nThat is, the applications using MPI are both parallel in nature and highly compute-intensive. It is not\nunusual for all the processor cores in an MPI job to run at 100% utilization. To be clear, MPI jobs\ntypically run in dedicated environments where the MPI processes are the only application running on the\nmachine (in addition to bare-bones operating system functionality, of course).\nAs such, MPI implementations are typically focused on providing extremely high performance, measured\nby metrics such as:\nOpen MPI\nJeffrey M. Squyres\n", "page": 161, "type": "text", "section": "Page 161"}
{"text": "Extremely low latency for short message passing. As an example, a 1-byte message can be sent\nfrom a user-level Linux process on one server, through an InfiniBand switch, and received at the\ntarget user-level Linux process on a different server in a little over 1 microsecond (i.e., 0.000001\nsecond).\nExtremely high message network injection rate for short messages. Some vendors have MPI\nimplementations (paired with specified hardware) that can inject up to 28 million messages per\nsecond into the network.\nQuick ramp-up (as a function of message size) to the maximum bandwidth supported by the\nunderlying transport.\nLow resource utilization. All resources used by MPI (e.g., memory, cache, and bus bandwidth)\ncannot be used by the application. MPI implementations therefore try to maintain a balance of low\nresource utilization while still providing high performance.\nOpen MPI\nThe first version of the MPI standard, MPI-1.0, was published in 1994 [Mes93]. MPI-2.0, a set of\nadditions on top of MPI-1, was completed in 1996 [GGHL+96].\nIn the first decade after MPI-1 was published, a variety of MPI implementations sprung up. Many were\nprovided by vendors for their proprietary network interconnects. Many other implementations arose from\nthe research and academic communities. Such implementations were typically \"research-quality,\"\nmeaning that their purpose was to investigate various high-performance networking concepts and provide\nproofs-of-concept of their work. However, some were high enough quality that they gained popularity and\na number of users.\nOpen MPI represents the union of four research/academic, open source MPI implementations: LAM/MPI,\nLA/MPI (Los Alamos MPI), and FT-MPI (Fault-Tolerant MPI). The members of the PACX-MPI team\njoined the Open MPI group shortly after its inception.\nThe members of these four development teams decided to collaborate when we had the collective\nrealization that, aside from minor differences in optimizations and features, our software code bases were\nquite similar. Each of the four code bases had their own strengths and weaknesses, but on the whole,\nthey more-or-less did the same things. So why compete? Why not pool our resources, work together,\nand make an even better MPI implementation?\nAfter much discussion, the decision was made to abandon our four existing code bases and take only the\nbest ideas from the prior projects. This decision was mainly predicated upon the following premises:\nEven though many of the underlying algorithms and techniques were similar among the four code\nbases, they each had radically different implementation architectures, and would be incredible difficult\n(if not impossible) to merge.\nEach of the four also had their own (significant) strengths and (significant) weaknesses. Specifically,\nthere were features and architecture decisions from each of the four that were desirable to carry\nforward. Likewise, there were poorly optimized and badly designed code in each of the four that\nwere desirable to leave behind.\nThe members of the four developer groups had not worked directly together before. Starting with an\nentirely new code base (rather than advancing one of the existing code bases) put all developers on\nequal ground.\nThus, Open MPI was born. Its first Subversion commit was on November 22, 2003.\n15.2. Architecture\nFor a variety of reasons (mostly related to either performance or portability), C and C++ were the only\ntwo possibilities for the primary implementation language. C++ was eventually discarded because\ndifferent C++ compilers tend to lay out structs/classes in memory according to different optimization\nalgorithms, leading to different on-the-wire network representations. C was therefore chosen as the\nprimary implementation language, which influenced several architectural design decisions.\nWhen Open MPI was started, we knew that it would be a large, complex code base:\nIn 2003, the current version of the MPI standard, MPI-2.0, defined over 300 API functions.\nEach of the four prior projects were large in themselves. For example, LAM/MPI had over 1,900 files\nof source code, comprising over 300,000 lines of code (including comments and blanks).\nWe wanted Open MPI to support more features, environments, and networks than all four prior\nprojects put together.\nWe therefore spent a good deal of time designing an architecture that focused on three things:\n1. Grouping similar functionality together in distinct abstraction layers.\n2. Using run-time loadable plugins and run-time parameters to choose between multiple different\nimplementations of the same behavior.\n3. Not allowing abstraction to get in the way of performance.\nAbstraction Layer Architecture\nOpen MPI has three main abstraction layers, shown in Figure 15.1:\nOpen, Portable Access Layer (OPAL): OPAL is the bottom layer of Open MPI's abstractions. Its\n", "page": 162, "type": "text", "section": "Page 162"}
{"text": "abstractions are focused on individual processes (versus parallel jobs). It provides utility and glue\ncode such as generic linked lists, string manipulation, debugging controls, and other mundane\u2014yet\nnecessary\u2014functionality.\nOPAL also provides Open MPI's core portability between different operating systems, such as\ndiscovering IP interfaces, sharing memory between processes on the same server, processor and\nmemory affinity, high-precision timers, etc.\nOpen MPI Run-Time Environment (ORTE) (pronounced \"or-tay\"): An MPI implementation must\nprovide not only the required message passing API, but also an accompanying run-time system to\nlaunch, monitor, and kill parallel jobs. In Open MPI's case, a parallel job is comprised of one or more\nprocesses that may span multiple operating system instances, and are bound together to act as a\nsingle, cohesive unit.\nIn simple environments with little or no distributed computational support, ORTE uses rsh  or ssh\nto launch the individual processes in parallel jobs. More advanced, HPC-dedicated environments\ntypically have schedulers and resource managers for fairly sharing computational resources between\nmany users. Such environments usually provide specialized APIs to launch and regulate processes\non compute servers. ORTE supports a wide variety of such managed environments, such as (but not\nlimited to): Torque/PBS Pro, SLURM, Oracle Grid Engine, and LSF.\nOpen MPI (OMPI): The MPI layer is the highest abstraction layer, and is the only one exposed to\napplications. The MPI API is implemented in this layer, as are all the message passing semantics\ndefined by the MPI standard.\nSince portability is a primary requirement, the MPI layer supports a wide variety of network types and\nunderlying protocols. Some networks are similar in their underlying characteristics and abstractions;\nsome are not.\nFigure 15.1: Abstraction layer architectural view of Open MPI showing its three main layers: OPAL,\nORTE, and OMPI\nAlthough each abstraction is layered on top of the one below it, for performance reasons the ORTE and\nOMPI layers can bypass the underlying abstraction layers and interact directly with the operating system\nand/or hardware when needed (as depicted in Figure 15.1). For example, the OMPI layer uses OS-\nbypass methods to communicate with certain types of NIC hardware to obtain maximum networking\nperformance.\nEach layer is built into a standalone library. The ORTE library depends on the OPAL library; the OMPI\nlibrary depends on the ORTE library. Separating the layers into their own libraries has acted as a\nwonderful tool for preventing abstraction violations. Specifically, applications will fail to link if one layer\nincorrectly attempts to use a symbol in a higher layer. Over the years, this abstraction enforcement\nmechanism has saved many developers from inadvertently blurring the lines between the three layers.\nPlugin Architecture\nAlthough the initial members of the Open MPI collaboration shared a similar core goal (produce a\nportable, high-performance implementation of the MPI standard), our organizational backgrounds,\nopinions, and agendas were\u2014and still are\u2014wildly different. We therefore spent a considerable amount of\ntime designing an architecture that would allow us to be different, even while sharing a common code\nbase.\nRun-time loadable components were a natural choice (a.k.a., dynamic shared objects, or \"DSOs\", or\n\"plugins\"). Components enforce a common API but place few limitations on the implementation of that\nAPI. Specifically: the same interface behavior can be implemented multiple different ways. Users can then\nchoose, at run time, which plugin(s) to use. This even allows third parties to independently develop and\ndistribute their own Open MPI plugins outside of the core Open MPI package. Allowing arbitrary\nextensibility is quite a liberating policy, both within the immediate set of Open MPI developers and in the\ngreater Open MPI community.\nThis run-time flexibility is a key component of the Open MPI design philosophy and is deeply integrated\nthroughout the entire architecture. Case in point: the Open MPI v1.5 series includes 155 plugins. To list\njust a few examples, there are plugins for different memcpy()  implementations, plugins for how to\n", "page": 163, "type": "text", "section": "Page 163"}
{"text": "launch processes on remote servers, and plugins for how to communicate on different types of underlying\nnetworks.\nOne of the major benefits of using plugins is that multiple groups of developers have freedom to\nexperiment with alternate implementations without affecting the core of Open MPI. This was a critical\nfeature, particularly in the early days of the Open MPI project. Sometimes the developers didn't always\nknow what was the right way to implement something, or sometimes they just disagreed. In both cases,\neach party would implement their solution in a component, allowing the rest of the developer community\nto easily compare and contrast. Code comparisons can be done without components, of course, but the\ncomponent concept helps guarantee that all implementations expose exactly the same external API, and\ntherefore provide exactly the same required semantics.\nAs a direct result of the flexibility that it provides, the component concept is utilized heavily throughout all\nthree layers of Open MPI; in each layer there are many different types of components. Each type of\ncomponent is enclosed in a framework. A component belongs to exactly one framework, and a\nframework supports exactly one kind of component. Figure 15.2 is a template of Open MPI's architectural\nlayout; it shows a few of Open MPI's frameworks and some of the components that they contain. (The\nrest of Open MPI's frameworks and components are laid out in the same manner.) Open MPI's set of\nlayers, frameworks, and components is referred to as the Modular Component Architecture (MCA).\nFigure 15.2: Framework architectural view of Open MPI, showing just a few of Open MPI's frameworks\nand components (i.e., plugins). Each framework contains a base and one or more components. This\nstructure is replicated in each of the layers shown in Figure 15.1. The sample frameworks listed in this\nfigure are spread across all three layers: btl  and coll  are in the OMPI layer, plm  is in the ORTE\nlayer, and timer  is in the OPAL layer.\nFinally, another major advantage of using frameworks and components is their inherent composability.\nWith over 40 frameworks in Open MPI v1.5, giving users the ability to mix-n-match different plugins of\ndifferent types allows them to create a software stack that is effectively tailored to their individual system.\nPlugin Frameworks\nEach framework is fully self-contained in its own subdirectory in the Open MPI source code tree. The\nname of the subdirectory is the same name as the framework; for example, the memory  framework is in\nthe memory  directory. Framework directories contain at least the following three items:\n1. Component interface definition: A header file named <framework>.h  will be located in the top-level\nframework directory (e.g., the Memory framework contains memory/memory.h ). This well-known\nheader file defines the interfaces that each component in the framework must support. This header\nincludes function pointer typedefs for the interface functions, structs for marshaling these function\npointers, and any other necessary types, attribute fields, macros, declarations, etc.\n2. Base code: The base  subdirectory contains the glue code that provides the core functionality of the\nframework. For example, the memory  framework's base directory is memory/base . The base is\ntypically comprised of logistical grunt work such as finding and opening components at run-time,\ncommon utility functionality that may be utilized by multiple components, etc.\n3. Components: All other subdirectories in the framework directory are assumed to be components. Just\nlike the framework, the names of the components are the same names as their subdirectories (e.g.,\nthe memory/posix  subdirectory contains the POSIX component in the Memory framework).\nSimilar to how each framework defines the interfaces to which its components must adhere, frameworks\nalso define other operational aspects, such as how they bootstrap themselves, how they pick components\nto use, and how they are shut down. Two common examples of how frameworks differ in their setup are\nmany-of-many versus one-of-many frameworks, and static versus dynamic frameworks.\nMany-of-many frameworks.\nSome frameworks have functionality that can be implemented multiple different ways in the same\nprocess. For example, Open MPI's point-to-point network framework will load multiple driver plugins to\nallow a single process to send and receive messages on multiple network types.\nSuch frameworks will typically open all components that they can find and then query each component,\neffectively asking, \"Do you want to run?\" The components determine whether they want to run by\n", "page": 164, "type": "text", "section": "Page 164"}
{"text": "examining the system on which they are running. For example, a point-to-point network component will\nlook to see if the network type it supports is both available and active on the system. If it is not, the\ncomponent will reply \"No, I do not want to run\", causing the framework to close and unload that\ncomponent. If that network type is available, the component will reply \"Yes, I want to run\", causing the\nframework to keep the component open for further use.\nOne-of-many frameworks.\nOther frameworks provide functionality for which it does not make sense to have more than one\nimplementation available at run-time. For example, the creation of a consistent checkpoint of a parallel\njob\u2014meaning that the job is effectively \"frozen\" and can be arbitrarily resumed later\u2014must be performed\nusing the same back-end checkpointing system for each process in the job. The plugin that interfaces to\nthe desired back-end checkpointing system is the only checkpoint plugin that must be loaded in each\nprocess\u2014all others are unnecessary.\nDynamic frameworks.\nMost frameworks allow their components to be loaded at run-time via DSOs. This is the most flexible\nmethod of finding and loading components; it allows features such as explicitly not loading certain\ncomponents, loading third-party components that were not included in the main-line Open MPI\ndistribution, etc.\nStatic frameworks.\nSome one-of-many frameworks have additional constraints that force their one-and-only-one component\nto be selected at compile time (versus run time). Statically linking one-of-many components allows direct\ninvocation of its member functions (versus invocation via function pointer), which may be important in\nhighly performance-sensitive functionality. One example is the memcpy  framework, which provides\nplatform-optimized memcpy()  implementations.\nAdditionally, some frameworks provide functionality that may need to be utilized before Open MPI is fully\ninitialized. For example, the use of some network stacks require complicated memory registration models,\nwhich, in turn, require replacing the C library's default memory management routines. Since memory\nmanagement is intrinsic to an entire process, replacing the default scheme can only be done pre- main .\nTherefore, such components must be statically linked into Open MPI processes so that they can be\navailable for pre- main  hooks, long before MPI has even been initialized.\nPlugin Components\nOpen MPI plugins are divided into two parts: a component struct and a module struct. The component\nstruct and the functions to which it refers are typically collectively referred to as \"the component.\"\nSimilarly, \"the module\" collectively refers to the module struct and its functions. The division is somewhat\nanalogous to C++ classes and objects. There is only one component per process; it describes the overall\nplugin with some fields that are common to all components (regardless of framework). If the component\nelects to run, it is used to generate one or more modules, which typically perform the bulk of the\nfunctionality required by the framework.\nThroughout the next few sections, we'll build up the structures necessary for the TCP component in the\nBTL (byte transfer layer) framework. The BTL framework effects point-to-point message transfers; the\nTCP component, not surprisingly, uses TCP as its underlying transport for message passing.\nComponent struct.\nRegardless of framework, each component contains a well-known, statically allocated and initialized\ncomponent struct. The struct must be named according to the template\nmca_<framework>_<component>_component . For example, the TCP network driver component's struct\nin the BTL framework is named mca_btl_tcp_component .\nHaving templated component symbols both guarantees that there will be no name collisions between\ncomponents, and allows the MCA core to find any arbitrary component struct via dlsym(2)  (or the\nappropriate equivalent in each supported operating system).\nThe base component struct contains some logistical information, such as the component's formal name,\nversion, framework version adherence, etc. This data is used for debugging purposes, inventory listing,\nand run-time compliance and compatibility checking.\nstruct mca_base_component_2_0_0_t {\n    /* Component struct version number */\n    int mca_major_version, mca_minor_version, mca_release_version;\n    /* The string name of the framework that this component belongs to,\n       and the framework's API version that this component adheres to */\n    char mca_type_name[MCA_BASE_MAX_TYPE_NAME_LEN + 1];\n    int mca_type_major_version, mca_type_minor_version,  \n        mca_type_release_version;\n    /* This component's name and version number */\n    char mca_component_name[MCA_BASE_MAX_COMPONENT_NAME_LEN + 1];\n", "page": 165, "type": "text", "section": "Page 165"}
{"text": "    int mca_component_major_version, mca_component_minor_version,\n        mca_component_release_version;\n    /* Function pointers */  \n    mca_base_open_component_1_0_0_fn_t mca_open_component;\n    mca_base_close_component_1_0_0_fn_t mca_close_component;\n    mca_base_query_component_2_0_0_fn_t mca_query_component;\n    mca_base_register_component_params_2_0_0_fn_t \n        mca_register_component_params;\n};\nThe base component struct is the core of the TCP BTL component; it contains the following function\npointers:\nOpen. The open call is the initial query function invoked on a component. It allows a component to\ninitialize itself, look around the system where it is running, and determine whether it wants to run. If a\ncomponent can always be run, it can provide a NULL  open function pointer.\nThe TCP BTL component open function mainly initializes some data structures and ensures that\ninvalid parameters were not set by the user.\nClose. When a framework decides that a component is no longer needed, it calls the close function\nto allow the component to release any resources that it has allocated. The close function is invoked\non all remaining components when processes are shutting down. However, close can also be\ninvoked on components that are rejected at run time so that they can be closed and ignored for the\nduration of the process.\nThe TCP BTL component close function closes listening sockets and frees resources (e.g., receiving\nbuffers).\nQuery. This call is a generalized \"Do you want to run?\" function. Not all frameworks utilize this\nspecific call\u2014some need more specialized query functions.\nThe BTL framework does not use the generic query function (it defines its own; see below), so the\nTCP BTL does not fill it in.\nParameter registration. This function is typically the first function called on a component. It allows the\ncomponent to register any relevant run-time, user-settable parameters. Run-time parameters are\ndiscussed further below.\nThe TCP BTL component register function creates a variety of user-settable run-time parameters,\nsuch as one which allows the user to specify which IP interface(s) to use.\nThe component structure can also be extended on a per-framework and/or per-component basis.\nFrameworks typically create a new component struct with the component base struct as the first member.\nThis nesting allows frameworks to add their own attributes and function pointers. For example, a\nframework that needs a more specialized query function (as compared to the query function provided on\nthe basic component) can add a function pointer in its framework-specific component struct.\nThe MPI btl  framework, which provides point-to-point MPI messaging functionality, uses this\ntechnique.\nstruct mca_btl_base_component_2_0_0_t {\n    /* Base component struct */\n    mca_base_component_t btl_version;\n    /* Base component data block */\n    mca_base_component_data_t btl_data;\n    /* btl-framework specific query functions */\n    mca_btl_base_component_init_fn_t btl_init;\n    mca_btl_base_component_progress_fn_t btl_progress;\n};\nAs an example of the TCP BTL framework query functions, the TCP BTL component btl_init  function\ndoes several things:\nCreates a listening socket for each \"up\" IPv4 and IPv6 interface.\nCreates a module for each \"up\" IP interface.\nRegisters the tuple (IP address, port)  for each \"up\" IP interface with a central repository so\nthat other MPI processes know how to contact it.\nSimilarly, plugins can extend the framework-specific component struct with their own members. The\ntcp  component in the btl  framework does this; it caches many data members in its component\nstruct.\nstruct mca_btl_tcp_component_t {\n    /* btl framework-specific component struct */ \n    mca_btl_base_component_2_0_0_t super;\n", "page": 166, "type": "text", "section": "Page 166"}
{"text": "    /* Some of the TCP BTL component's specific data members */\n    /* Number of TCP interfaces on this server */\n    uint32_t tcp_addr_count;\n    \n    /* IPv4 listening socket descriptor */\n    int tcp_listen_sd;\n    /* ...and many more not shown here */\n};\nThis struct-nesting technique is effectively a simple emulation of C++ single inheritance: a pointer to an\ninstance of a struct mca_btl_tcp_component_t  can be cast to any of the three types such that it\ncan be used by an abstraction layer than does not understand the \"derived\" types.\nThat being said, casting is generally frowned upon in Open MPI because it can lead to incredibly subtle,\ndifficult-to-find bugs. An exception was made for this C++-emulation technique because it has well-\ndefined behaviors and helps enforce abstraction barriers.\nModule struct.\nModule structs are individually defined by each framework; there is little commonality between them.\nDepending on the framework, components generate one or more module struct instances to indicate that\nthey want to be used.\nFor example, in the BTL framework, one module usually corresponds to a single network device. If an\nMPI process is running on a Linux server with three \"up\" Ethernet devices, the TCP BTL component will\ngenerate three TCP BTL modules; one corresponding to each Linux Ethernet device. Each module will\nthen be wholly responsible for all sending and receiving to and from its Ethernet device.\nTying it all together.\nFigure 15.3 shows the nesting of the structures in the TCP BTL component, and how it generates one\nmodule for each of the three Ethernet devices.\nFigure 15.3: The left side shows the nesting of structures in the TCP BTL component. The right side\nshows how the component generates one module struct for each \"up\" Ethernet interface.\nComposing BTL modules this way allows the upper-layer MPI progression engine both to treat all\nnetwork devices equally, and to perform user-level channel bonding.\nFor example, consider sending a large message across the three-device configuration described above.\nAssume that each of the three Ethernet devices can be used to reach the intended receiver (reachability\nis determined by TCP networks and netmasks, and some well-defined heuristics). In this case, the\nsender will split the large message into multiple fragments. Each fragment will be assigned\u2014in a round-\nrobin fashion\u2014to one of the TCP BTL modules (each module will therefore be assigned roughly one third\nof the fragments). Each module then sends its fragments over its corresponding Ethernet device.\nThis may seem like a complex scheme, but it is surprisingly effective. By pipelining the sends of a large\nmessage across the multiple TCP BTL modules, typical HPC environments (e.g., where each Ethernet\ndevice is on a separate PCI bus) can sustain nearly maximum bandwidth speeds across multiple\nEthernet devices.\nRun-Time Parameters\nDevelopers commonly make decisions when writing code, such as:\nShould I use algorithm A or algorithm B?\nHow large of a buffer should I preallocate?\nHow long should the timeout be?\nAt what message size should I change network protocols?\n\u2026and so on.\nUsers tend to assume that the developers will answer such questions in a way that is generally suitable\nfor most types of systems. However, the HPC community is full of scientist and engineer power users\nwho want to aggressively tweak their hardware and software stacks to eke out every possible compute\ncycle. Although these users typically do not want to tinker with the actual code of their MPI\nimplementation, they do want to tinker by selecting different internal algorithms, choosing different\n", "page": 167, "type": "text", "section": "Page 167"}
{"text": "resource consumption patterns, or forcing specific network protocols in different circumstances.\nTherefore, the MCA parameter system was included when designing Open MPI; the system is a flexible\nmechanism that allows users to change internal Open MPI parameter values at run time. Specifically,\ndevelopers register string and integer MCA parameters throughout the Open MPI code base, along with\nan associated default value and descriptive string defining what the parameter is and how it is used. The\ngeneral rule of thumb is that rather than hard-coding constants, developers use run-time-settable MCA\nparameters, thereby allowing power users to tweak run-time behavior.\nThere are a number of MCA parameters in the base code of the three abstraction layers, but the bulk of\nOpen MPI's MCA parameters are located in individual components. For example, the TCL BTL plugin has\na parameter that specifies whether only TCPv4 interfaces, only TCPv6 interfaces, or both types of\ninterfaces should be used. Alternatively, another TCP BTL parameter can be set to specify exactly which\nEthernet devices to use.\nUsers can discover what parameters are available via a user-level command line tool ( ompi_info ).\nParameter values can be set in multiple ways: on the command line, via environment variables, via the\nWindows registry, or in system- or user-level INI-style files.\nThe MCA parameter system complements the idea of run-time plugin selection flexibility, and has proved\nto be quite valuable to users. Although Open MPI developers try hard to choose reasonable defaults for\na wide variety of situations, every HPC environment is different. There are inevitably environments where\nOpen MPI's default parameter values will be unsuitable\u2014and possibly even detrimental to performance.\nThe MCA parameter system allows users to be proactive and tweak Open MPI's behavior for their\nenvironment. Not only does this alleviate many upstream requests for changes and/or bug reports, it\nallows users to experiment with the parameter space to find the best configuration for their specific\nsystem.\n15.3. Lessons Learned\nWith such a varied group of core Open MPI members, it is inevitable that we would each learn\nsomething, and that as a group, we would learn many things. The following list describes just a few of\nthese lessons.\nPerformance\nMessage-passing performance and resource utilization are the king and queen of high-performance\ncomputing. Open MPI was specifically designed in such a way that it could operate at the very bleeding\nedge of high performance: incredibly low latencies for sending short messages, extremely high short\nmessage injection rates on supported networks, fast ramp-ups to maximum bandwidth for large\nmessages, etc. Abstraction is good (for many reasons), but it must be designed with care so that it does\nnot get in the way of performance. Or, put differently: carefully choose abstractions that lend themselves\nto shallow, performant call stacks (versus deep, feature-rich API call stacks).\nThat being said, we also had to accept that in some cases, abstraction\u2014not architecture\u2014must be\nthrown out the window. Case in point: Open MPI has hand-coded assembly for some of its most\nperformance-critical operations, such as shared memory locking and atomic operations.\nIt is worth noting that Figures 15.1 and 15.2 show two different architectural views of Open MPI. They do\nnot represent the run-time call stacks or calling invocation layering for the high performance code\nsections.\nLesson learned:\nIt is acceptable (albeit undesirable) and unfortunately sometimes necessary to have gross, complex code\nin the name of performance (e.g., the aforementioned assembly code). However, it is always preferable\nto spend time trying to figure out how to have good abstractions to discretize and hide complexity\nwhenever possible. A few weeks of design can save literally hundreds or thousands of developer-hours\nof maintenance on tangled, subtle, spaghetti code.\nStanding on the Shoulders of Giants\nWe actively tried to avoid re-inventing code in Open MPI that someone else has already written (when\nsuch code is compatible with Open MPI's BSD licensing). Specifically, we have no compunctions about\neither directly re-using or interfacing to someone else's code.\nThere is no place for the \"not invented here\" religion when trying to solve highly complex engineering\nproblems; it only makes good logistical sense to re-use external code whenever possible. Such re-use\nfrees developers to focus on the problems unique to Open MPI; there is no sense re-solving a problem\nthat someone else has solved already.\nA good example of this kind of code re-use is the GNU Libtool Libltdl package. Libltdl is a small library\nthat provides a portable API for opening DSOs and finding symbols in them. Libltdl is supported on a\nwide variety of operating systems and environments, including Microsoft Windows.\nOpen MPI could have provided this functionality itself\u2014but why? Libltdl is a fine piece of software, is\nactively maintained, is compatible with Open MPI's license, and provides exactly the functionality that\nwas needed. Given these points, there is no realistic gain for Open MPI developers to re-write this\nfunctionality.\n", "page": 168, "type": "text", "section": "Page 168"}
{"text": "This work is made available under the Creative Commons Attribution 3.0 Unported license. Please see\nthe full description of the license for details.\nLesson learned:\nWhen a suitable solution exists elsewhere, do not hesitate to integrate it and stop wasting time trying to\nre-invent it.\nOptimize for the Common Case\nAnother guiding architectural principle has been to optimize for the common case. For example, emphasis\nis placed on splitting many operations into two phases: setup and repeated action. The assumption is\nthat setup may be expensive (meaning: slow). So do it once and get it over with. Optimize for the much\nmore common case: repeated operation.\nFor example, malloc()  can be slow, especially if pages need to be allocated from the operating\nsystem. So instead of allocating just enough bytes for a single incoming network message, allocate\nenough space for a bunch of incoming messages, divide the result up into individual message buffers,\nand set up a freelist to maintain them. In this way, the first request for a message buffer may be slow,\nbut successive requests will be much faster because they will just be de-queues from a freelist.\nLesson learned:\nSplit common operations into (at least) two phases: setup and repeated action. Not only will the code\nperform better, it may be easier to maintain over time because the distinct actions are separated.\nMiscellaneous\nThere are too many more lessons learned to describe in detail here; the following are a few more\nlessons that can be summed up briefly:\nWe were fortunate to draw upon 15+ years of HPC research and make designs that have (mostly)\nsuccessfully carried us for more than eight years. When embarking on a new software project, look\nto the past. Be sure to understand what has already been done, why it was done, and what its\nstrengths and weaknesses were.\nThe concept of components\u2014allowing multiple different implementations of the same functionality\u2014\nhas saved us many times, both technically and politically. Plugins are good.\nSimilarly, we continually add and remove frameworks as necessary. When developers start arguing\nabout the \"right\" way to implement a new feature, add a framework that fronts components that\nimplement that feature. Or when newer ideas come along that obsolete older frameworks, don't\nhesitate to delete such kruft.\nConclusion\nIf we had to list the three most important things that we've learned from the Open MPI project, I think\nthey would be as follows:\nOne size does not fit all (users). The run-time plugin and companion MCA parameter system allow\nusers flexibility that is necessary in the world of portable software. Complex software systems cannot\n(always) magically adapt to a given system; providing user-level controls allows a human to figure\nout\u2014and override\u2014when the software behaves sub-optimally.\nDifferences are good. Developer disagreements are good. Embrace challenges to the status quo; do\nnot get complacent. A plucky grad student saying \"Hey, check this out\u2026\" can lead to the basis of a\nwhole new feature or a major evolution of the product.\nAlthough outside the scope of this book, people and community matter. A lot.\nBack to top\nBack to The Architecture of Open Source Applications.\n", "page": 169, "type": "text", "section": "Page 169"}
{"text": "<>\nSince their initial adoption, EMR (electronic medical record) systems have attempted to bridge the gap\nbetween the physical and digital worlds of patient care. Governments in countries around the world have\nattempted to come up with a solution that enables better care for patients at a lower cost while reducing\nthe paper trail that medicine typically generates. Many governments have been very successful in their\nattempts to create such a system\u2014some, like that of the Canadian province of Ontario, have not (some\nmay remember the so-called \"eHealth Scandal\" in Ontario that, according to the Auditor General, cost\ntaxpayers $1 billion CAD).\nAn EMR permits the digitization of a patient chart, and when used properly should make it easier for a\nphysician to deliver care. A good system should provide a physician a bird's eye view of a patient's\ncurrent and ongoing conditions, their prescription history, their recent lab results, history of their previous\nvisits, and so on. OSCAR (Open Source Clinical Application Resource), an approximately ten-year-old\nproject of McMaster University in Hamilton, Ontario, Canada, is the open source community's attempt to\nprovide such a system to physicians at low or no cost.\nOSCAR has many subsystems that provide functionality on a component-by-component basis. For\nexample, oscarEncounter provides an interface for interacting with a patient's chart directly; Rx3 is a\nprescription module that checks for allergies and drug interactions automatically and allows a physician to\ndirectly fax a prescription to a pharmacy from the UI; the Integrator is a component to enable data\nsharing between multiple compatible EMRs. All of these separate components come together to build the\ntypical OSCAR user experience.\nOSCAR won't be for every physician; for example, a specialist may not find all the features of the system\nuseful, and it is not easily customizable. However, it offers a complete set of features for a general\nphysician interacting with patients on a day-to-day basis.\nIn addition, OSCAR is CMS 3.0 certified (and has applied for CMS 4.0 certification)\u2014which allows\nphysicians to receive funding for installing it in their clinic (see EMR Advisor for details). Receiving CMS\ncertification involves passing a set of requirements from the Government of Ontario and paying a fee.\nThis chapter will discuss the architecture of OSCAR in fairly general terms, describing the hierarchy,\nmajor components, and most importantly the impact that past decisions have made on the project. As a\nconclusion and to wrap up, there will be a discussion on how OSCAR might have been designed today if\nthere was an opportunity to do so.\n16.1. System Hierarchy\nAs a Tomcat web application, OSCAR generally follows the typical model-view-controller design pattern.\nThis means that the model code (Data Access Objects, or DAOs) is separate from the controller code\n(servlets) and those are separated from the views (Java Server Pages, or JSPs). The most significant\ndifference between the two is that servlets are classes and JSPs are HTML pages marked up with Java\ncode. Data gets placed into memory when a servlet executes and the JSP reads that same data, usually\ndone via reads and writes to the attributes of the request object. Just about every JSP page in OSCAR\nhas this kind of design.\n16.2. Past Decision Making\nI mentioned that OSCAR is a fairly old project. This has implications for how effectively the MVC pattern\nhas been applied. In short, there are sections of the code that completely disregard the pattern as they\nwere written before tighter enforcement of the MVC pattern began. Some of the most common features\nare written this way; for example, performing many actions related to demographics (patient records) are\ndone via the demographiccontrol.jsp  file\u2014this includes creating patients and updating their data.\nOSCAR's age is a hurdle for tackling many of the problems that are facing the source tree today. Indeed,\nthere has been significant effort made to improve the situation, including enforcing design rules via a\ncode review process. This is an approach that the community at present has decided will allow better\ncollaboration in the future, and will prevent poor code from becoming part of the code base, which has\nOSCAR\nJennifer Ruttan\n", "page": 170, "type": "text", "section": "Page 170"}
{"text": "been a problem in the past.\nThis is by no means a restriction on how we could design parts of the system now; it does, however,\nmake it more complicated when deciding to fix bugs in a dated part of OSCAR. Do you, as somebody\ntasked to fix a bug in the Demographic Creation function, fix the bug with code in the same style as it\ncurrently exists? Or do you re-write the module completely so that it closely follows the MVC design\npattern?\nAs developers we must carefully weigh our options in situations like those. There is no guarantee that if\nyou re-architect a part of the system you will not create new bugs, and when patient data is on the line,\nwe must make the decision carefully.\n16.3. Version Control\nA CVS repository was used for much of OSCAR's life. Commits weren't often checked for consistency\nand it was possible to commit code that could break the build. It was tough for developers to keep up\nwith changes\u2014especially new developers joining the project late in its lifecycle. A new developer could\nsee something that they would want to change, make the change, and get it into the source branch\nseveral weeks before anybody would notice that something significant had been modified (this was\nespecially prevalent during long holidays, such as Christmas break, when not many people were\nwatching the source tree).\nThings have changed; OSCAR's source tree is now controlled by git. Any commits to the main branch\nhave to pass code-style checking and unit testing, successfully compile, and be code reviewed by the\ndevelopers. (Much of this is handled by the combination of Hudson, a continuous integration server and\nGerrit, a code review tool.) The project has become much more tightly controlled. Many or all of the\nissues caused by poor handling of the source tree have been solved.\n16.4. Data Models/DAOs\nWhen looking through the OSCAR source, you may notice that there are many different ways to access\nthe database: you can use a direct connection to the database via a class called DBHandler , use a\nlegacy Hibernate model, or use a generic JPA model. As new and easier database access models\nbecame available, they were integrated into OSCAR. The result is that there is now a slightly noisy\npicture of how OSCAR interacts with data in MySQL, and the differences between the three types of\ndata access methods are best described with examples.\nEForms (DBHandler)\nThe EForm system allows users to create their own forms to attach to patient records\u2014this feature is\nusually used to replace a paper-based form with a digital version. On each creation of a form of a\nparticular type, the form's template file is loaded; then the data in the form is stored in the database for\neach instance. Each instance is attached to a patient record.\nEForms allow you to pull in certain types of data from a patient chart or other area of the system via\nfree-form SQL queries (which are defined in a file called apconfig.xml ). This can be extremely useful,\nas a form can load and then immediately be populated with demographic or other relevant information\nwithout intervention from the user; for example, you wouldn't have to type in a patient's name, age, date\nof birth, hometown, phone number, or the last note that was recorded for that patient.\nA design decision was made, when originally developing the EForm module, to use raw database queries\nto populate a POJO (plain-old Java object) called EForm  in the controller that is then passed to the\nview layer to display data on the screen, sort of like a JavaBean. Using a POJO in this case is actually\ncloser in design to the Hibernate or JPA architecture, as I'll discuss in the next sections.\nAll of the functionality regarding saving EForm instances and templates is done via raw SQL queries run\nthrough the DBHandler  class. Ultimately, DBHandler  is a wrapper for a simple JDBC object and does\nnot scrutinize a query before sending it to the SQL server. It should be added here that DBHandler  is a\npotential security flaw as it allows unchecked SQL to be sent to the server. Any class that uses\nDBHandler  must implement its own checking to make sure that SQL injection doesn't occur.\nDepending on the type of application you're writing, direct access of a database is sometimes fine. In\ncertain cases, it can even speed development up. Using this method to access the database doesn't\nconform to the model-view-controller design pattern, though: if you're going to change your database\nstructure (the model), you have to change the SQL query elsewhere (in the controller). Sometimes,\nadding certain columns or changing their type in OSCAR's database tables requires this kind of invasive\nprocedure just to implement small features.\nIt may not surprise you to find out that the DBHandler  object is one of the oldest pieces of code still\nintact in the source. I personally don't know where it originated from but I consider it to be the most\n\"primitive\" of database access types that exist in the OSCAR source. No new code is permitted to use\nthis class, and if code is committed that uses it, the commit will be rejected automatically.\nDemographic Records (Hibernate)\nA demographic record contains general metadata about a patient; for example, their name, age, address,\nlanguage, and sex; consider it to be the result of an intake form that a patient fills out during their first\nvisit to a doctor. All of this data is retrieved and displayed as part of OSCAR's Master Record for a\n", "page": 171, "type": "text", "section": "Page 171"}
{"text": "specific demographic.\nUsing Hibernate to access the database is far safer than using DBHandler . For one, you have to\nexplicitly define which columns match to which fields in your model object (in this case, the Demographic\nclass). If you want to perform complex joins, they have to be done as prepared statements. Finally, you\nwill only ever receive an object of the type you ask for when performing a query, which is very\nconvenient.\nThe process of working with a Hibernate-style DAO and Model pair is quite simple. In the case of the\nDemographic object, there's a file called Demographic.hbm.xml  that describes the mapping between\nobject field and database column. The file describes which table to look at and what type of object to\nreturn. When OSCAR starts, this file will be read and a sanity check occurs to make sure that this kind of\nmapping can actually be made (server startup fails if it can't). Once running, you grab an instance of the\nDemographicDao  object and run queries against it.\nThe best part about using Hibernate over DBHandler  is that all of the queries to the server are\nprepared statements. This restricts you from running free-form SQL during runtime, but it also prevents\nany type of SQL injection attack. Hibernate will often build large queries to grab the data, and it doesn't\nalways perform in an extremely efficient way.\nIn the previous section I mentioned an example of the EForm module using DBHandler  to populate a\nPOJO. This is the next logical step to preventing that kind of code from being written. If the model has to\nchange, only the .hbm.xml  file and the model class have to change (a new field and getter/setter for\nthe new column), and doing so won't impact the rest of the application.\nWhile newer than DBHandler , the Hibernate method is also starting to show its age. It's not always\nconvenient to use and requires a big configuration file for each table you want to access. Setting up a\nnew object pair takes time and if you do it incorrectly OSCAR won't even start. For this reason, nobody\nshould be writing new code that uses pure Hibernate, either. Instead, generic JPA is being embraced in\nnew development.\nIntegrator Consent (JPA)\nThe newest form of database access is done via generic JPA. If the OSCAR project decided to switch\nfrom Hibernate to another database access API, conforming to the JPA standard for DAOs and Model\nobjects would make it very easy to migrate. Unfortunately, because this is so \"new\" to the OSCAR\nproject, there are almost no areas of the system that actually use this method to get data.\nIn any case, let me explain how it works. Instead of a .hbm.xml  file, you add annotations to your Model\nand DAO objects. These annotations describe the table to look in, column mappings for fields, and join\nqueries. Everything is contained inside the two files and nothing else is necessary for their operation.\nHibernate still runs behind the scenes, though, in actually retrieving the data from the database.\nAll of the Integrator's models are written using JPA\u2014and they are pretty good examples of both the new\nstyle of database access as well as demonstrating that as a new technology to be implemented into\nOSCAR, it hasn't been used in very many places yet. The Integrator is a relatively new addition to the\nsource. It makes quite a lot of sense to use this new data access model as opposed to Hibernate.\nTouching on a now-common theme in this section of the chapter, the annotated POJOs that JPA uses\nallow for a far more streamlined experience. For example, during the Integrator's build process, an SQL\nfile is created that sets up all of the tables for you\u2014an enormously useful thing to have. With that ability,\nit's impossible to create mismatching tables and model objects (as you can do with any other type of\ndatabase access method) and you never have to worry about naming of columns and tables. There are\nno direct SQL queries, so it's not possible to create SQL injection attacks. In short, it \"just works\".\nThe way that JPA works can be considered to be fairly similar to the way that ActiveRecord works in\nRuby on Rails. The model class defines the data type and the database stores it; what happens in\nbetween that\u2014getting data in and out\u2014is not up to the user.\nIssues with Hibernate and JPA\nBoth Hibernate and JPA offer some significant benefits in typical use cases. For simple retrieval and\nstorage, they really cut time out of development and debugging.\nHowever, that doesn't mean that their implementation into OSCAR has been without issue. Because the\nuser doesn't define the SQL between the database and the POJO referencing a specific row, Hibernate\ngets to choose the best way to do it. The \"best way\" can manifest itself in a couple of ways: Hibernate\ncan choose to just retrieve the simple data from the row, or it can perform a join and retrieve a lot of\ninformation at once. Sometimes these joins get out of hand.\nHere's another example: The casemgmt_note  table stores all patient notes. Each note object stores\nlots of metadata about the note\u2014but it also stores a list of all of the issues that the note deals with\n(issues can be things like, \"smoking cessation\" or \"diabetes\", which describe the contents of the note).\nThe list of issues is represented in the note object as a List<CaseManagementIssue> . In order to get\nthat list, the casemgmt_note  table is joined with the casemgmt_issue_notes  table (which acts as a\nmapping table) and finally the casemgmt_issue  table.\nWhen you want to write a custom query in Hibernate, which this situation requires, you don't write\n", "page": 172, "type": "text", "section": "Page 172"}
{"text": "standard SQL\u2014you write HQL (Hibernate Query Language) that is then translated to SQL (by inserting\ninternal column names for all the fields to be selected) before parameters are inserted and the query is\nsent to the database server. In this specific case, the query was written with basic joins with no join\ncolumns\u2014meaning that when the query was eventually translated to SQL, it was so large that it wasn't\nimmediately obvious what the query was gathering. Additionally, in almost all cases, this never created a\nlarge enough temporary table for it to matter. For most users, this query actually runs quickly enough that\nit's not noticeable. However, this query is unbelievably inefficient.\nLet's step back for a second. When you perform a join on two tables, the server has to create a\ntemporary table in memory. In the most generic type of joins, the number of rows is equal to the number\nof rows in the first table multiplied by the number of rows in the second table. So if your table has\n500,000 rows, and you join it with a table that has 10,000,000 rows, you've just created a 5\u00d710\n row\ntemporary table in memory, which the select statement is then run against and that temporary table is\ndiscarded.\nIn one extreme case that we ran into, the join across three tables caused a temporary table to be created\nthat was around 7\u00d710\n rows in length, of which about 1000 rows were eventually selected. This\noperation took about 5 minutes and locked the casemgmt_note  table while it was running.\nThe problem was solved, eventually, through the use of a prepared statement that restricted the scope of\nthe first table before joining with the other two. The newer, far more efficient query brought the number of\nrows to select down to a very manageable 300,000 and enormously improved performance of the notes\nretrieval operation (down to about 0.1 seconds to perform the same select statement).\nThe moral of the story is simply that while Hibernate does a fairly good job, unless the join is very\nexplicitly defined and controlled (either in the .hbm.xml  file or a join annotation in the object class for a\nJPA model), it can very quickly get out of control. Dealing with objects instead of SQL queries requires\nyou to leave the actual implementation of the query up to the database access library and only really\nallows you to control definition. Unless you're careful with how you define things, it can all fall apart under\nextreme conditions. Furthermore, if you're a database programmer with lots of SQL knowledge, it won't\nreally help much when designing a JPA-enabled class, and it removes some of the control that you\nwould have if you were writing an SQL statement manually. Ultimately, a good knowledge of both SQL\nand JPA annotations and how they affect queries is required.\n16.5. Permissions\nCAISI (Client Access to Integrated Services and Information) was originally a standalone product\u2014a fork\nof OSCAR\u2014to help manage homeless shelters in Toronto. A decision was eventually made to merge the\ncode from CAISI into the main source branch. The original CAISI project may no longer exist, but what it\ngave to OSCAR is very important: its permission model.\nThe permissions model in OSCAR is extremely powerful and can be used to create just about as many\nroles and permission sets as possible. Providers belong to programs (as staff) where they have a specific\nrole. Each program takes place at a facility. Each role has a description (for example, \"doctor\", \"nurse\",\n\"social worker\", and so on) and a set of attached global permissions. The permissions are written in a\nformat that makes them very easy to understand: \"read nurse notes\" may be a permission that a doctor\nrole may have, but the nurse role may not have the \"read doctor notes\" permission.\nThis format may be easy to understand, but under the hood it requires quite a bit of heavy lifting to\nactually check for these types of permissions. The name of the role that the current provider has is\nchecked against its list of permissions for a match with the action that they are trying to perform. For\nexample, a provider attempting to read a doctor's notes would cause \"read doctor notes\" to be checked\nfor each and every note written by a doctor.\nAnother problem is the reliance on English for permission definition. Anybody using OSCAR in a\nlanguage other than English would still need to write their permissions in a format such as \"read [role]\nnotes\", using the English words \"read\", \"write\", \"notes\", and so on.\nCAISI's permission model is a significant part of OSCAR, but it's not the only model in place. Before\nCAISI was implemented, another role-based (but not program-based) system was developed and is still\nin use in many parts of the system today.\nFor this second system, providers are assigned one or many roles (for example, \"doctor\", \"nurse\",\n\"admin\", and so on). They can be assigned as many roles as necessary\u2014the roles' permissions stack on\ntop of each other. These permissions are generally used for restricting access to parts of the system, as\nopposed to CAISI's permissions which restrict access to certain pieces of data on a patient's chart. For\nexample, a user has to have the \"_admin\" \"read\" permission on a role that they have assigned to them to\nbe able to access the Admin panel. Having the \"read\" permission will exempt them from being able to\nperform administrative tasks, however. They'll need the \"write\" permission as well for that.\nBoth of these systems accomplish roughly the same goal; it's due to CAISI's merge later in the project\nlifecycle that they both exist. They don't always exist happily together, so in reality it can be a lot easier\nto just focus on using one for day-to-day operations of OSCAR. You can generally date code in OSCAR\nby knowing which permissions model preceded which other permissions model: Provider Type then\nProvider Roles then CAISI Programs/Roles\nThe oldest type of permissions model, \"Provider Type\", is so dated that it's actually not used in most\n12\n12\n", "page": 173, "type": "text", "section": "Page 173"}
{"text": "parts of the system and is in fact defaulted to \"doctor\" during new provider creation\u2014having it as any\nother value (such as \"receptionist\") causes significant issues throughout the system. It's easier and more\nfine-grained to control permissions via Provider Roles instead.\n16.6. Integrator\nOSCAR's Integrator component is a separate web application that independent OSCAR instances use to\nexchange patient, program and provider information over a secure link. It can be optionally installed as a\ncomponent for an installation in an environment such as a LHN (Local Health Network) or a hospital. The\neasiest way to describe the Integrator is as a temporary storage facility.\nConsider the following use case and argument for use of the Integrator: in Hospital X, there is an ENT\n(ear, nose, and throat) clinic as well as an endocrinology clinic. If an ENT doctor refers their patient to an\nendocrinologist upstairs, they may be required to send along patient history and records. This is\ninconvenient and generates more paper than is necessary\u2014perhaps the patient is only seeing the\nendocrinologist once. By using the Integrator, the patient's data can be accessed on the endocrinologist's\nEMR, and access to the contents of the patient's chart can be revoked after the visit.\nA more extreme example: if an unconscious man shows up in an ER with nothing but his health card,\nbecause the home clinic and the hospital's system are connected via the Integrator, the man's record can\nbe pulled and it can be very quickly realized that he has been prescribed the blood thinner warfarin.\nUltimately, information retrieval like this is what an EMR like OSCAR paired with the Integrator can\nachieve.\nTechnical Details\nThe Integrator is available in source code form only, which requires the user to retrieve and build it\nmanually. Like OSCAR, it runs on a standard installation of Tomcat with MySQL.\nWhen the URL where the Integrator lives is accessed, it doesn't appear to display anything useful. This\ncomponent is almost purely a web service; OSCAR communicates via POST and GET requests to the\nIntegrator URL.\nAs an independently developed project (initially as part of the CAISI project), the Integrator is fairly strict\nin adhering to the MVC design pattern. The original developers have done an excellent job of setting it\nup with very clearly defined lines between the models, views, and controllers. The most recently\nimplemented type of database access layer that I mentioned earlier\u2014generic JPA\u2014is the only such layer\nin the project. (As an interesting side note: because the entire project is properly set up with JPA\nannotations on all the model classes, an SQL script is created at build time that can be used to initialize\nthe structure of the database; the Integrator, therefore, doesn't ship with a stand-alone SQL script.)\nCommunication is handled via web service calls described in WSDL XML files that are available on the\nserver. A client could query the Integrator to find out what kind of functions are available and adapt to it.\nThis really means that the Integrator is compatible with any kind of EMR that somebody decides to write\na client for; the data format is generic enough that it could easily be mapped to local types.\nFor OSCAR, though, a client library is built and included in the main source tree, for simplicity's sake.\nThat library only ever needs to be updated if new functions become available on the Integrator. A bug fix\non the Integrator doesn't require an update of that file.\nDesign\nData for the Integrator comes in from all of the connected EMRs at scheduled times and, once there,\nanother EMR can request that data. None of the data on the Integrator is stored permanently, though\u2014\nits database could be erased and it could be rebuilt from the client data.\nThe dataset sent is configured individually at each OSCAR instance which is connected to a particular\nIntegrator, and except in situations where the entire patient database has to be sent to the Integrator\nserver, only patient records that have been viewed since the previous push to the server are sent. The\nprocess isn't exactly like delta patching, but it's close.\n", "page": 174, "type": "text", "section": "Page 174"}
{"text": "Figure 16.1: Data exchange between OSCARs and Integrator\nLet me go into a little more detail about how the Integrator works with an example: a remote clinic seeing\nanother clinic's patient. When that clinic wants to access the patient's record, the clinics first have to have\nbeen connected to the same Integrator server. The receptionist can search the Integrator for the remote\npatient (by name and optionally date of birth or sex) and find their record stored on the server. They\ninitiate the copy of a limited set of the patient's demographic information and then double-check with the\npatient to make sure that they consent to the retrieval of their record by completing a consent form. Once\ncompleted, the Integrator server will deliver whatever information the Integrator knows about that patient\n\u2014notes, prescriptions, allergies, vaccinations, documents, and so on. This data is cached locally so that\nthe local OSCAR doesn't have to send a request to the Integrator every time it wants to see this data,\nbut the local cache expires every hour.\nFigure 16.2: The Demographic information and associated data is sent to the Integrator during a data\npush from the home clinic. The record on the Integrator may not be a representation of the complete\nrecord from the home clinic as the OSCAR can choose not to send all patient data.\nAfter the initial setup of a remote patient by copying their demographic data to the local OSCAR, that\npatient is set up as any other on the system. All of the remote data that is retrieved from the Integrator is\nmarked as such (and the clinic from which it came from is noted), but it's only temporarily cached on the\nlocal OSCAR. Any local data that is recorded is recorded just like any other patient data\u2014to the patient\nrecord, and sent to the Integrator\u2014but not permanently stored on any remote machine.\nFigure 16.3: A remote OSCAR requests data from the Integrator by asking for a specific patient record.\nThe Integrator server sends only the demographic information, which is stored permanently on the\n", "page": 175, "type": "text", "section": "Page 175"}
{"text": "remote OSCAR.\nThis has a very important implication, especially for patient consent and how that factors into the design\nof the Integrator. Let's say that a patient sees a remote physician and is fine with them having access to\ntheir record, but only temporarily. After their visit, they can revoke the consent for that clinic to be able to\nview that patient's record and the next time that clinic opens the patient's chart there won't be any data\nthere (with the exception of any data that was locally recorded). This ultimately gives control over how\nand when a record is viewed directly to the patient and is similar to walking into a clinic carrying a copy\nof your paper chart. They can see the chart while they're interacting with you, but you take it home with\nyou when you leave.\nFigure 16.4: A remote clinic can see the contents of a patient chart by asking for the data; if the\nappropriate consent is present, the data is sent. The data is never stored permanently on the remote\nOSCAR.\nAnother very important ability is for physicians to decide what kinds of data they want to share with the\nother connected clinics via their Integrator server. A clinic can choose to share all of a demographic\nrecord or only parts of it, such as notes but not documents, allergies but not prescriptions, and so on.\nUltimately it's up to the group of physicians who set up the Integrator server to decide what kinds of data\nthey're comfortable with sharing with each other.\nAs I mentioned before, the Integrator is only a temporary storage warehouse and no data is ever stored\npermanently there. This is another very important decision that was made during development; it allows\nclinics to back out of sharing any and all data via the Integrator very easily\u2014and in fact if necessary the\nentire Integrator database can be wiped. If the database is wiped, no user of a client will ever notice\nbecause the data will be accurately reconstructed from the original data on all of the various connected\nclients. An implication is that the OSCAR provider needs to trust the Integrator provider to have wiped the\ndatabase when they say so\u2014it is therefore best to deploy an Integrator to a group of physicians already\nin a legal organization such as a Family Health Organization or Family Health Team; the Integrator\nserver would be housed at one of these physician's clinics.\nData Format\nThe Integrator's client libraries are built via wsdl2java , which creates a set of classes representing the\nappropriate data types the web service communicates in. There are classes for each data type as well as\nclasses representing keys for each of these data types.\nIt's outside the scope of this chapter to describe how to build the Integrator's client library. What's\nimportant to know is that once the library is built, it must be included with the rest of the JARs in\nOSCAR. This JAR contains everything necessary to set up the Integrator connection and access all of\nthe data types that the Integrator server will return to OSCAR, such as CachedDemographic,\nCachedDemographicNote, and CachedProvider, among many others. In addition to the data types that\nare returned, there are \"WS\" classes that are used for the retrieval of such lists of data in the first place\n\u2014the most frequently used being DemographicWs.\nDealing with the Integrator data can sometimes be a little tricky. OSCAR doesn't have anything truly\nbuilt-in to handle this kind of data, so what usually happens is when retrieving a certain kind of patient\ndata (for example, notes for a patient's chart) the Integrator client is asked to retrieve data from the\nserver. That data is then manually transformed into a local class representing that data (in the case of\nnotes, it's a CaseManagementNote ). A Boolean flag is set inside the class to indicate that it's a piece of\nremote content and that is used to change how the data is displayed to the user on the screen. On the\nopposite end, CaisiIntegratorUpdateTask  handles taking local OSCAR data, converting it into the\nIntegrator's data format, and then sending that data to the Integrator server.\nThis design may not be as efficient or as clean as possible, but it does enable older parts of the system\nto become \"compatible\" with Integrator-delivered data without much modification. In addition, keeping the\nview as simple as possible by referring to only one type of class improves the readability of the JSP file\nand makes it easier to debug in the event of an error.\n16.7. Lessons Learned\nAs you can probably imagine, OSCAR has its share of issues when it comes to overall design. It does,\nhowever, provide a complete feature set that most users will find no issues with. That's ultimately the goal\nof the project: provide a good solution that works in most situations.\n", "page": 176, "type": "text", "section": "Page 176"}
{"text": "This work is made available under the Creative Commons Attribution 3.0 Unported license. Please see\nthe full description of the license for details.\nI can't speak for the entire OSCAR community, so this section will be highly subjective and from my point\nof view. I feel that there are some important takeaways from an architectural discussion about the project.\nFirst, it's clear that poor source control in the past has caused the architecture of the system to become\nhighly chaotic in parts, especially in areas where the controllers and the views blend together. The way\nthat the project was run in the past didn't prevent this from happening, but the process has changed\nsince and hopefully we won't have to deal with such a problem again.\nNext, because the project is so old, it's difficult to upgrade (or even change) libraries without causing\nsignificant disruption throughout the code base. That's exactly what has happened, though. I often find it\ndifficult to figure out what's necessary and what isn't when I'm looking in the library folder. In addition to\nthat, sometimes when libraries undergo major upgrades they break backwards compatibility (changing\npackage names is a common offense). There are often several libraries included with OSCAR that all\naccomplish the same task\u2014this goes back to poor source control, but also the fact that that there has\nbeen no list or documentation describing which library is required by which component.\nAdditionally, OSCAR is a little inflexible when it comes to adding new features to existing subsystems.\nFor example, if you want to add a new box to the E-Chart, you'll have to create a new JSP page and a\nnew servlet, modify the layout of the E-Chart (in a few places), and modify the configuration file of the\napplication so that your servlet can load.\nNext, due to the lack of documentation, sometimes it is nearly impossible to figure out how a part of the\nsystem works\u2014the original contributor may not even be part of the project anymore\u2014and often the only\ntool you have to figure it out is a debugger. As a project of this age, this is costing the community the\npotential for new contributors to get involved. However, it's something that, as a collaborative effort, the\ncommunity is working on.\nFinally, OSCAR is a repository for medical information and its security is compromised by the inclusion of\nthe DBHandler  class (discussed in a previous section). I personally feel that freeform database queries\nthat accept parameters should never be acceptable in an EMR because it's so easy to perform SQL\ninjection attacks. While it's good that no new code is permitted that uses this class, it should be a priority\nof the development team to remove all instances of its use.\nAll of that may sound like some harsh criticism of the project. In the past, all of these problems have\nbeen significant and, like I said, prevent the community from growing as the barrier to entry is so high.\nThis is something that is changing, so in the future, these issues won't be so much of a hindrance.\nIn looking back over the project's history (and especially over the past few versions) we can come up\nwith a better design for how the application would be built. The system still has to provide a base level of\nfunctionality (mandated by the Ontario government for certification as an EMR), so that all has to be\nbaked in by default. But if OSCAR were to be redesigned today, it should be designed in a truly modular\nfashion that would allow modules to be treated as plugins; if you didn't like the default E-Form module,\nyou could write your own (or even another module entirely). It should be able to speak to more systems\n(or more systems should be able to speak to it), including the medical hardware that you see in\nincreasing use throughout the industry, such as devices for measuring visual acuity. This also means that\nit would be easy to adapt OSCAR to the requirements of local and federal governments around the world\nfor storing medical data. Since every region has a different set of laws and requirements, this kind of\ndesign would be crucial for making sure that OSCAR develops a worldwide userbase.\nI also believe that security should be the most important feature of all. An EMR is only as secure as its\nleast secure component, so there should be focus on abstracting away as much data access as possible\nfrom the application so that it stores and retrieves data in a sandbox-style environment through a main\ndata access layer API that has been audited by a third-party and found to be adequate for storing\nmedical information. Other EMRs can hide behind obscurity and proprietary code as a security measure\n(which isn't really a security measure at all), but being open source, OSCAR should lead the charge with\nbetter data protection.\nI stand firmly as a believer in the OSCAR project. We have hundreds of users that we know about (and\nthe many hundreds that we don't), and we receive valuable feedback from the physicians who are\ninteracting with our project on a daily basis. Through the development of new processes and new\nfeatures, we hope to grow the installed base and to support users from other regions. It is our intention to\nmake sure that what we deliver is something that improves the lives of the physicians who use OSCAR\nas well as the lives of their patients, by creating better tools to help manage healthcare.\nBack to top\nBack to The Architecture of Open Source Applications.\n", "page": 177, "type": "text", "section": "Page 177"}
{"text": "<>\nOriginally developed by Ben Fry and Casey Reas, the Processing programming language started as an\nopen source programming language (based on Java) to help the electronic arts and visual design\ncommunities learn the basics of computer programming in a visual context. Offering a highly simplified\nmodel for 2D and 3D graphics compared to most programming languages, it quickly became well-suited\nfor a wide range of activities, from teaching programming through writing small visualisations to creating\nmulti-wall art installations, and became able to perform a wide variety of tasks, from simply reading in a\nsequence of strings to acting as the de facto IDE for programming and operating the popular \"Arduino\"\nopen source hardware prototyping boards. Continuing to gain popularity, Processing has firmly taken its\nplace as an easy to learn, widely used programming language for all things visual, and so much more.\nThe basic Processing program, called a \"sketch\", consists of two functions: setup  and draw . The first\nis the main program entry point, and can contain any amount of initialization instructions. After finishing\nsetup , Processing programs can do one of two things: 1) call draw , and schedule another call to\ndraw  at a fixed interval upon completion; or 2) call draw , and wait for input events from the user. By\ndefault, Processing does the former; calling noLoop  results in the latter. This allows for two modes to\npresent sketches, namely a fixed framerate graphical environment, and an interactive, event-based\nupdating graphical environment. In both cases, user events are monitored and can be handled either in\ntheir own event handlers, or for certain events that set persistent global values, directly in the draw\nfunction.\nProcessing.js is a sister project of Processing, designed to bring it to the web without the need for Java\nor plugins. It started as an attempt by John Resig to see if the Processing language could be ported to\nthe web, by using the\u2014at the time brand new\u2014HTML5 <canvas>  element as a graphical context, with\na proof of concept library released to the public in 2008. Written with the idea in mind that \"your code\nshould just work\", Processing.js has been refined over the years to make data visualisations, digital art,\ninteractive animations, educational graphs, video games, etc. work using web standards and without any\nplugins. You write code using the Processing language, either in the Processing IDE or your favourite\neditor of choice, include it on a web page using a <canvas>  element, and Processing.js does the rest,\nrendering everything in the <canvas>  element and letting users interact with the graphics in the same\nway they would with a normal standalone Processing program.\n17.1. How Does It Work?\nProcessing.js is a bit unusual as an open source project, in that the code base is a single file called\nprocessing.js  which contains the code for Processing, the single object that makes up the entire\nlibrary. In terms of how the code is structured, we constantly shuffle things around inside this object as\nwe try to clean it up a little bit with every release. Its design is relatively straightforward, and its function\ncan be described in a single sentence: it rewrites Processing source code into pure JavaScript source\ncode, and every Processing API function call is mapped to a corresponding function in the JavaScript\nProcessing object, which effects the same thing on a <canvas>  element as the Processing call would\neffect on a Java Applet canvas.\nFor speed, we have two separate code paths for 2D and 3D functions, and when a sketch is loaded,\neither one or the other is used for resolving function wrappers so that we don't add bloat to running\ninstances. However, in terms of data structures and code flow, knowing JavaScript means you can read\nprocessing.js , with the possible exception of the syntax parser.\nUnifying Java and JavaScript\nRewriting Processing source code into JavaScript source code means that you can simply tell the\nbrowser to execute the rewritten source, and if you rewrote it correctly, things just work. But, making sure\nthe rewrite is correct has taken, and still occasionally takes, quite a bit of effort. Processing syntax is\nbased on Java, which means that Processing.js has to essentially transform Java source code into\nJavaScript source code. Initially, this was achieved by treating the Java source code as a string, and\niteratively replacing substrings of Java with their JavaScript equivalents. (For those interested in an early\nincarnation of the parser, it can be found at here, running from line 37 to line 266.) For a small syntax\nProcessing.js\nMike Kamermans\n", "page": 178, "type": "text", "section": "Page 178"}
{"text": "set, this is fine, but as time went on and complexity added to complexity, this approach started to break\ndown. Consequently, the parser was completely rewritten to build an Abstract Syntax Tree (AST) instead,\nfirst breaking down the Java source code into functional blocks, and then mapping each of those blocks\nto their corresponding JavaScript syntax. The result is that, at the cost of readability, Processing.js now\neffectively contains an on-the-fly Java-to-JavaScript transcompiler. (Readers are welcome to peruse this\ncode, up to line 19217.)\nHere is the code for a Processing sketch:\n    void setup() {\n      size(200,200);\n      noCursor();\n      noStroke();\n      smooth(); }\n    void draw() {\n      fill(255,10);\n      rect(-1,-1,width+1,height+1);\n      float f = frameCount*PI/frameRate;\n      float d = 10+abs(60*sin(f));\n      fill(0,100,0,50);\n      ellipse(mouseX, mouseY, d,d); }\nAnd here is its Processing.js conversion:\n    function($p) {\n        function setup() {\n            $p.size(200, 200);\n            $p.noCursor();\n            $p.noStroke();\n            $p.smooth(); }\n        $p.setup = setup;\n        function draw() {\n            $p.fill(255, 10);\n            $p.rect(-1, -1, $p.width + 1, $p.height + 1);\n            var f = $p.frameCount * $p.PI / $p.__frameRate;\n            var d = 10 + $p.abs(60 * $p.sin(f));\n            $p.fill(0, 100, 0, 50);\n            $p.ellipse($p.mouseX, $p.mouseY, d, d); }\n        $p.draw = draw; }\nThis sounds like a great thing, but there are a few problems when converting Java syntax to JavaScript\nsyntax:\n1. Java programs are isolated entities. JavaScript programs share the world with a web page.\n2. Java is strongly typed. JavaScript is not.\n3. Java is a class/instance based object-oriented language. JavaScript is not.\n4. Java has distinct variables and methods. JavaScript does not.\n5. Java allows method overloading. JavaScript does not.\n6. Java allows importing compiled code. JavaScript has no idea what that even means.\nDealing with these problems has been a tradeoff between what users need, and what we can do given\nweb technologies. The following sections will discuss each of these issues in greater detail.\n17.2. Significant Differences\nJava programs have their own threads; JavaScript can lock up your browser.\nJava programs are isolated entities, running in their own thread in the greater pool of applications on your\nsystem. JavaScript programs, on the other hand, live inside a browser, and compete with each other in a\nway that desktop applications don't. When a Java program loads a file, the program waits until the\nresource is done loading, and operation resumes as intended. In a setting where the program is an\nisolated entity on its own, this is fine. The operating system stays responsive because it's responsible for\nthread scheduling, and even if the program takes an hour to load all its data, you can still use your\ncomputer. On a web page, this is not how things work. If you have a JavaScript \"program\" waiting for a\nresource to be done loading, it will lock its process until that resource is available. If you're using a\nbrowser that uses one process per tab, it will lock up your tab, and the rest of the browser is still usable.\nIf you're using a browser that doesn't, your entire browser will seem frozen. So, regardless of what the\nprocess represents, the page the script runs on won't be usable until the resource is done loading, and\nit's entirely possible that your JavaScript will lock up the entire browser.\nThis is unacceptable on the modern web, where resources are transferred asynchronously, and the page\nis expected to function normally while resources are loaded in the background. While this is great for\n", "page": 179, "type": "text", "section": "Page 179"}
{"text": "traditional web pages, for web applications this is a real brain twister: how do you make JavaScript idle,\nwaiting for a resource to load, when there is no explicit mechanism to make JavaScript idle? While there\nis no explicit threading in JavaScript, there is an event model, and there is an XMLHTTPRequest  object\nfor requesting arbitrary (not just XML or HTML) data from arbitrary URLS. This object comes with several\ndifferent status events, and we can use it to asynchronously get data while the browser stays responsive.\nWhich is great in programs in which you control the source code: you make it simply stop after\nscheduling the data request, and make it pick up execution when the data is available. However, this is\nnear impossible for code that was written based on the idea of synchronous resource loading. Injecting\n\"idling\" in programs that are supposed to run at a fixed framerate is not an option, so we have to come\nup with alternative approaches.\nFor some things, we decided to force synchronous waiting anyway. Loading a file with strings, for\ninstance, uses a synchronous XMLHTTPRequest , and will halt execution of the page until the data is\navailable. For other things, we had to get creative. Loading images, for instance, uses the browser's\nbuilt-in mechanism for loading images; we build a new Image  in JavaScript, set its src  attribute to the\nimage URL, and the browser does the rest, notifying us that the image is ready through the onload\nevent. This doesn't even rely on an XMLHTTPRequest , it simply exploits the browser's capabilities.\nTo make matters easier when you already know which images you are loading, we added preload\ndirectives so that the sketch does not start execution until preloading is complete. A user can indicate any\nnumber of images to preload via a comment block at the start of the sketch; Processing.js then tracks\noutstanding image loading. The onload  event for an image tells us that it is done transferring and is\nconsidered ready to be rendered (rather than simply having been downloaded but not decoded to a pixel\narray in memory yet), after which we can populate the corresponding Processing PImage  object with\nthe correct values ( width , height , pixel data, etc.) and clear the image from the list. Once the list is\nempty, the sketch gets executed, and images used during its lifetime will not require waiting.\nHere is an example of preload directives:\n    /* @pjs preload=\"./worldmap.jpg\"; */\n    PImage img;\n    void setup() {\n      size(640,480);\n      noLoop();\n      img = loadImage(\"worldmap.jpg\"); }\n    void draw() {\n      image(img,0,0); }\nFor other things, we've had to build more complicated \"wait for me\" systems. Fonts, unlike images, do\nnot have built-in browser loading (or at least not a system as functional as image loading). While it is\npossible to load a font using a CSS @font-face  rule and rely on the browser to make it all happen,\nthere are no JavaScript events that can be used to determine that a font finished loading. We are slowly\nseeing events getting added to browsers to generate JavaScript events for font download completion, but\nthese events come \"too early\", as the browser may need anywhere from a few to a few hundred more\nmilliseconds to actually parse the font for use on the page after download. Thus, acting on these events\nwill still lead to either no font being applied, or the wrong font being applied if there is a known fallback\nfont. Rather than relying on these events, we embed a tiny TrueType font that only contains the letter \"A\"\nwith impossibly small metrics, and instruct the browser to load this font via an @font-face  rule with a\ndata URI that contains the font's bytecode as a BASE64 string. This font is so small that we can rely on it\nbeing immediately available. For any other font load instruction we compare text metrics between the\ndesired font and this tiny font. A hidden <div>  is set up with text styled using the desired font, with our\ntiny font as fallback. As long as the text in that <div>  is impossibly small, we know the desired font is\nnot available yet, and we simply poll at set intervals until the text has sensible metrics.\nJava is strongly typed; JavaScript is not.\nIn Java, the number 2 and the number 2.0 are different values, and they will do different things during\nmathematical operations. For instance, the code i = 1/2  will result in i  being 0, because the\nnumbers are treated as integers, whereas i = 1/2.0 , i = 1.0/2 , and even i = 1./2.  will all\nresult in i  being 0.5, because the numbers are considered decimal fractions with a non-zero integer\npart, and a zero fractional part. Even if the intended data type is a floating point number, if the arithmetic\nuses only integers, the result will be an integer. This lets you write fairly creative math statements in\nJava, and consequently in Processing, but these will generate potentially wildly different results when\nported to Processing.js, as JavaScript only knows \"numbers\". As far as JavaScript is concerned, 2 and\n2.0 are the same number, and this can give rise to very interesting bugs when running a sketch using\nProcessing.js.\nThis might sound like a big issue, and at first we were convinced it would be, but you can't argue with\nreal world feedback: it turns out this is almost never an issue for people who put their sketches online\nusing Processing.js. Rather than solving this in some cool and creative way, the resolution of this\nproblem was actually remarkably straightforward; we didn't solve it, and as a design choice, we don't\n", "page": 180, "type": "text", "section": "Page 180"}
{"text": "intend to ever revisit that decision. Short of adding a symbol table with strong typing so that we can fake\ntypes in JavaScript and switch functionality based on type, this incompatibility cannot properly be solved\nwithout leaving much harder to find edge case bugs, and so rather than adding bulk to the code and\nslowdown to execution, we left this quirk in. It is a well-documented quirk, and \"good code\" won't try to\ntake advantage of Java's implicit number type casting. That said, sometimes you will forget, and the\nresult can be quite interesting.\nJava is a class/instance-based object-oriented language, with separate variable and method\nspaces; JavaScript is not.\nJavaScript uses prototype objects, and the inheritance model that comes with it. This means all objects\nare essentially key/value pairs where each key is a string, and values are either primitives, arrays,\nobjects, or functions. On the inheritance side, prototypes can extend other prototypes, but there is no real\nconcept of \"superclass\" and \"subclass\". In order to make \"proper\" Java-style object-oriented code work,\nwe had to implement classical inheritance for JavaScript in Processing.js, without making it super slow\n(we think we succeeded in that respect). We also had to come up with a way to prevent variable names\nand function names from stepping on each other. Because of the key/value nature of JavaScript objects,\ndefining a variable called line , followed by a function like line(x1,y1,x2,y2)  will leave you with an\nobject that uses whatever was declared last for a key. JavaScript first sets object.line = \"some\nvalue\"  for you, and then sets object.line = function(x1,y1,x2,y2){\u2026} , overriding what you\nthought your variable line  was.\nIt would have slowed down the library a lot to create separate administration for variables and\nmethods/functions, so again the documentation explains that it's a bad idea to use variables and\nfunctions with the same name. If everyone wrote \"proper\" code, this wouldn't be much of a problem, as\nyou want to name variables and functions based on what they're for, or what they do, but the real world\ndoes things differently. Sometimes your code won't work, and it's because we decided that having your\ncode break due to a naming conflict is preferable to your code always working, but always being slow. A\nsecond reason for not implementing variable and function separation was that this could break JavaScript\ncode used inside Processing sketches. Closures and the scope chain for JavaScript rely on the key/value\nnature of objects, so driving a wedge in that by writing our own administration would have also severely\nimpacted performance in terms of Just-In-Time compilation and compression based on functional\nclosures.\nJava allows method overloading; JavaScript does not.\nOne of Java's more powerful features is that you can define a function, let's say add(int,int) , and\nthen define another function with the same name, but a different number of arguments, e.g.\nadd(int,int,int) , or with different argument types, e.g. add(ComplexNumber,ComplexNumber) .\nCalling add  with two or three integer arguments will automatically call the appropriate function, and\ncalling add  with floats or Car objects will generate an error. JavaScript, on the other hand, does not\nsupport this. In JavaScript, a function is a property, and you can dereference it (in which case JavaScript\nwill give you a value based on type coercion, which in this case returns true  when the property points\nto a function definition, or false  when it doesn't), or you can call it as a function using the execution\noperators (which you will know as parentheses with zero or more arguments between them). If you define\na function as add(x,y)  and then call it as add(1,2,3,4,5,6) , JavaScript is okay with that. It will set\nx  to 1 and y  to 2 and simply ignore the rest of the arguments. In order to make overloading work, we\nrewrite functions with the same name but different argument count to a numbered function, so that\nfunction(a,b,c)  in the source becomes function$3(a,b,c)  in the rewritten code, and\nfunction(a,b,c,d)  becomes function$4(a,b,c,d) , ensuring the correct code paths.\nWe also mostly solved overloading of functions with the same number but differently typed arguments, as\nlong as the argument types can be seen as different by JavaScript. JavaScript can tell the functional type\nof properties using the typeof  operator, which will return either number , string , object  or\nfunction  depending on what a property represents. Declaring var x = 3  followed by x = '6'  will\ncause typeof x  to report number  after the initial declaration, and string  after reassignment. As\nlong as functions with the same argument count differ in argument type, we rename them and switch\nbased on the result of the typeof operation. This does not work when the functions take arguments of\ntype object , so for these functions we have an additional check involving the instanceof  operator\n(which returns the name of the function that was used to create the object) to make function overloading\nwork. In fact, the only place where we cannot successfully transcompile overloaded functions is where\nthe argument count is the same between functions, and the argument types are different numerical types.\nAs JavaScript only has one numerical type, declaring functions such as add(int x, int y) ,\nadd(float x, float y)  and add(double x, double y)  will clash. Everything else, however, will\nwork just fine.\nJava allows importing compiled code.\nSometimes, plain Processing is not enough, and additional functionality is introduced in the form of a\nProcessing library. These take the form of a .jarchive  with compiled Java code, and offer things like\nnetworking, audio, video, hardware interfacing and other exotic functions not covered by Processing itself.\nThis is a problem, because compiled Java code is Java byte code. This has given us many headaches:\nhow do we support library imports without writing a Java byte code decompiler? After about a year of\ndiscussions, we settled on what may seem the simplest solution. Rather than trying to also cover\nProcessing libraries, we decided to support the import keyword in sketches, and create a Processing.js\n", "page": 181, "type": "text", "section": "Page 181"}
{"text": "Library API, so that library developers can write a JavaScript version of their library (where feasible, given\nthe web's nature), so that if they write a package that is used via import processing.video , native\nProcessing will pick the .jarchive , and Processing.js will instead pick processing.video.js, thus\nensuring that things \"just work\". This functionality is slated for Processing.js 1.4, and library imports is the\nlast major feature that is still missing from Processing.js (we currently support the import  keyword only\nin the sense that it is removed from the source code before conversion), and will be the last major step\ntowards parity.\nWhy Pick JavaScript if It Can't Do Java?\nThis is not an unreasonable question, and it has multiple answers. The most obvious one is that\nJavaScript comes with the browser. You don't \"install\" JavaScript yourself, there's no plugin to download\nfirst; it's just there. If you want to port something to the web, you're stuck with JavaScript. Although, given\nthe flexibility of JavaScript, \"stuck with\" is really not doing justice to how powerful the language is. So,\none reason to pick JavaScript is \"because it's already there\". Pretty much every device that is of interest\ncomes with a JavaScript-capable browser these days. The same cannot be said for Java, which is being\noffered less and less as a preinstalled technology, if it is available at all.\nHowever, the proper answer is that it's not really true that JavaScript \"can't do\" the things that Java does;\nit can, it would just be slower. Even though out of the box JavaScript can't do some of the things Java\ndoes, it's still a Turing-complete programming language and it can be made to emulate any other\nprogramming language, at the cost of speed. We could, technically, write a full Java interpreter, with a\nString  heap, separate variable and method models, class/instance object-orientation with rigid class\nhierarchies, and everything else under the Sun (or, these days, Oracle), but that's not what we're in it for:\nProcessing.js is about offering a Processing-to-the-web conversion, in as little code as is necessary for\nthat. This means that even though we decided not to make it do certain Java things, our library has one\nhuge benefit: it can cope with embedded JavaScript really, really well.\nIn fact, during a meeting between the Processing.js and Processing people at Bocoup in Boston, in\n2010, Ben Fry asked John Resig why he used regular expression replacement and only partial\nconversion instead of doing a proper parser and compiler. John's response was that it was important to\nhim that people be able to mix Processing syntax (Java) and JavaScript without having to choose\nbetween them. That initial choice has been crucial in shaping the philosophy of Processing.js ever since.\nWe've worked hard to keep it true in our code, and we can see a clear payoff when we look at all the\n\"purely web\" users of Processing.js, who never used Processing, and will happily mix Processing and\nJavaScript syntax without a problem.\nThe following example shows how JavaScript and Processing work together.\n    // JavaScript (would throw an error in native Processing)\n    var cs = { x: 50,\n               y: 0,\n               label: \"my label\",\n               rotate: function(theta) {\n                         var nx = this.x*cos(theta) - this.y*sin(theta);\n                         var ny = this.x*sin(theta) + this.y*cos(theta);\n                         this.x = nx; this.y = ny; }};\n    // Processing\n    float angle = 0;\n    void setup() {\n      size(200,200);\n      strokeWeight(15); }\n    void draw() {\n      translate(width/2,height/2);\n      angle += PI/frameRate;\n      while(angle>2*PI) { angle-=2*PI; }\n      jQuery('#log').text(angle); // JavaScript (error in native Processing)\n      cs.rotate(angle);           // legal JavaScript as well as Processing\n      stroke(random(255));\n      point(cs.x, cs.y); }\nA lot of things in Java are promises: strong typing is a content promise to the compiler, visibility is a\npromise on who will call methods and reference variables, interfaces are promises that instances contain\nthe methods the interface describes, etc. Break those promises and the compiler complains. But, if you\ndon't\u2014and this is a one of the most important thoughts for Processing.js\u2014then you don't need the\nadditional code for those promises in order for a program to work. If you stick a number in a variable,\nand your code treats that variable as if it has a number in it, then at the end of the day var varname  is\njust as good as int varname . Do you need typing? In Java, you do; in JavaScript, you don't, so why\nforce it in? The same goes for other code promises. If the Processing compiler doesn't complain about\nyour code, then we can strip all the explicit syntax for your promises and it'll still work the same.\n", "page": 182, "type": "text", "section": "Page 182"}
{"text": "This has made Processing.js a ridiculously useful library for data visualisation, media presentation and\neven entertainment. Sketches in native Processing work, but sketches that mix Java and JavaScript also\nwork just fine, as do sketches that use pure JavaScript by treating Processing.js as a glorified canvas\ndrawing framework. In an effort to reach parity with native Processing, without forcing Java-only syntax,\nthe project has been taken in by an audience as wide as the web itself. We've seen activity all over the\nweb using Processing.js. Everyone from IBM to Google has built visualisations, presentations and even\ngames with Processing.js\u2014Processing.js is making a difference.\nAnother great thing about converting Java syntax to JavaScript while leaving JavaScript untouched is that\nwe've enabled something we hadn't even thought about ourselves: Processing.js will work with anything\nthat will work with JavaScript. One of the really interesting things that we're now seeing, for instance, is\nthat people are using CoffeeScript (a wonderfully simple, Ruby-like programming language that\ntranscompiles to JavaScript) in combination with Processing.js, with really cool results. Even though we\nset out to build \"Processing for the web\" based on parsing Processing syntax, people took what we did\nand used it with brand new syntaxes. They could never have done that if we had made Processing.js\nsimply be a Java interpreter. By sticking with code conversion rather than writing a code interpreter,\nProcessing.js has given Processing a reach on the web far beyond what it would have had if it had\nstayed Java-only, or even if it had kept a Java-only syntax, with execution on the web taken care of by\nJavaScript. The uptake of our code not just by end users, but also by people who try to integrate it with\ntheir own technologies, has been both amazing and inspiring. Clearly we're doing something right, and\nthe web seems happy with what we're doing.\nThe Result\nAs we are coming up to Processing.js 1.4.0, our work has resulted in a library that will run any sketch\nyou give it, provided it does not rely on compiled Java library imports. If you can write it in Processing,\nand it runs, you can put it on a webpage and it will just run. Due to the differences in hardware access\nand low level implementations of different parts of the rendering pipeline there will be timing differences,\nbut in general a sketch that runs at 60 frames per seconds in the Processing IDE will run at 60 frames\nper second on a modern computer, with a modern browser. We have reached a point where bug reports\nhave started to die down, and most work is no longer about adding feature support, but more about bug\nfixing and code optimization.\nThanks to the efforts of many developers working to resolve over 1800 bug reports, Processing sketches\nrun using Processing.js \"just work\". Even sketches that rely on library imports can be made to work,\nprovided that the library code is at hand. Under favourable circumstances, the library is written in a way\nthat lets you rewrite it to pure Processing code with a few search-replace operations. In this case the\ncode can be made to work online virtually immediately. When the library does things that cannot be\nimplemented in pure Processing, but can be implemented using plain JavaScript, more work is required\nto effectively emulate the library using JavaScript code, but porting is still possible. The only instances of\nProcessing code that cannot be ported are those that rely on functionality that is inherently unavailable to\nbrowsers, such as interfacing directly with hardware devices (such as webcams or Arduino boards) or\nperforming unattended disk writes, though even this is changing. Browsers are constantly adding\nfunctionality to allow for more elaborate applications, and limiting factors today may disappear a year\nfrom now, so that hopefully in the not too distant future, even sketches that are currently impossible to\nrun online will become portable.\n17.3. The Code Components\nProcessing.js is presented and developed as a large, single file, but architecturally it represents three\ndifferent components: 1) the launcher, responsible for converting Processing source to Processing.js\nflavoured JavaScript and executing it, 2) static functionality that can be used by all sketches, and 3)\nsketch functionality that has to be tied to individual instances.\nThe Launcher\nThe launcher component takes care of three things: code preprocessing, code conversion, and sketch\nexecution.\nPreprocessing\nIn the preprocessing step, Processing.js directives are split off from the code, and acted upon. These\ndirectives come in two flavours: settings and load instructions. There is a small number of directives,\nkeeping with the \"it should just work\" philosophy, and the only settings that sketch authors can change\nare related to page interaction. By default a sketch will keep running if the page is not in focus, but the\npauseOnBlur = true  directive sets up a sketch in such a way that it will halt execution when the page\nthe sketch is running on is not in focus, resuming execution when the page is in focus again. Also by\ndefault, keyboard input is only routed to a sketch when it is focussed. This is especially important when\npeople run multiple sketches on the same page, as keyboard input intended for one sketch should not be\nprocessed by another. However, this functionality can be disabled, routing keyboard events to every\nsketch that is running on a page, using the globalKeyEvents = true  directive.\nLoad instructions take the form of the aforementioned image preloading and font preloading. Because\nimages and fonts can be used by multiple sketches, they are loaded and tracked globally, so that different\nsketches don't attempt multiple loads for the same resource.\nCode Conversion\n", "page": 183, "type": "text", "section": "Page 183"}
{"text": "The code conversion component decomposes the source code into AST nodes, such as statements and\nexpressions, methods, variables, classes, etc. This AST then expanded to JavaScript source code that\nbuilds a sketch-equivalent program when executed. This converted source code makes heavy use of the\nProcessing.js instance framework for setting up class relations, where classes in the Processing source\ncode become JavaScript prototypes with special functions for determining superclasses and bindings for\nsuperclass functions and variables.\nSketch Execution\nThe final step in the launch process is sketch execution, which consists of determining whether or not all\npreloading has finished, and if it has, adding the sketch to the list of running instances and triggering its\nJavaScript onLoad  event so that any sketch listeners can take the appropriate action. After this the\nProcessing chain is run through: setup , then draw , and if the sketch is a looping sketch, setting up\nan interval call to draw  with an interval length that gets closest to the desired framerate for the sketch.\nStatic Library\nMuch of Processing.js falls under the \"static library\" heading, representing constants, universal functions,\nand universal data types. A lot of these actually do double duty, being defined as global properties, but\nalso getting aliased by instances for quicker code paths. Global constants such as key codes and color\nmappings are housed in the Processing object itself, set up once, and then referenced when instances\nare built via the Processing constructor. The same applies to self-contained helper functions, which lets\nus keep the code as close to \"write once, run anywhere\" as we can without sacrificing performance.\nProcessing.js has to support a large number of complex data types, not just in order to support the data\ntypes used in Processing, but also for its internal workings. These, too, are defined in the Processing\nconstructor:\nChar , an internal object used to overcome some of the behavioural quirks of Java's char\ndatatype.\nPShape , which represents shape objects.\nPShapeSVG , an extension for PShape  objects, which is built from and represents SVG XML.\nFor PShapeSVG , we implemented our own SVG-to- <canvas> -instructions code. Since Processing\ndoes not implement full SVG support, the code we saved by not relying on an external SVG library\nmeans that we can account for every line of code relating to SVG imports. It only parses what it has\nto, and doesn't waste space with code that follows the spec, but is unused because native\nProcessing does not support it.\nXMLElement , an XML document object.\nFor XMLElement , too, we implemented our own code, relying on the browser to first load the XML\nelement into a Node-based structure, then traveling the node structure to build a leaner object.\nAgain, this means we don't have any dead code sitting in Processing.js, taking up space and\npotentially causing bugs because a patch accidentally makes use of a function that shouldn't be\nthere.\nPMatrix2D  and PMatrix3D , which perform matrix operations in 2D and 3D mode.\nPImage , which represents an image resource.\nThis is effectively a wrapper of the Image object, with some additional functions and properties so\nthat its API matches the Processing API.\nPFont , which represents a font resource.\nThere is no Font object defined for JavaScript (at least for now), so rather than actually storing the\nfont as an object, our PFont  implementation loads a font via the browser, computes its metrics\nbased on how the browser renders text with it, and then caches the resultant PFont  object. For\nspeed, PFont s have a reference to the canvas that was used to determine the font properties, in\ncase textWidth  must be calculated, but because we track PFont  objects based on name/size\npair, if a sketch uses a lot of distinct text sizes, or fonts in general, this will consume too much\nmemory. As such, PFont s will clear their cached canvas and instead call a generic textWidth\ncomputation function when the cache grows too large. As a secondary memory preservation strategy,\nif the font cache continues to grow after clearing the cached canvas for each PFont , font caching\nis disabled entirely, and font changes in the sketch simply build new throwaway PFont  objects for\nevery change in font name, text size or text leading.\nDrawingShared , Drawing2D , and Drawing3D , which house all the graphics functions.\nThe DrawingShared  object is actually the biggest speed trap in Processing.js. It determines if a\nsketch is launching in 2D or 3D mode, and then rebinds all graphics functions to either the\nDrawing2D  or Drawing3D  object. This ensures short code path for graphics instructions, as 2D\nProcessing sketches cannot used 3D functions, and vice versa. By only binding one of the two sets\nof graphics functions, we gain speed from not having to switch on the graphics mode in every\nfunction to determine the code path, and we save space by not binding the graphics functions that\nare guaranteed not to be used.\nArrayList , a container that emulates Java's ArrayList .\nHashMap , a container that emulates Java's HashMap .\nArrayList , and HashMap  in particular, are special data structures because of how Java\n", "page": 184, "type": "text", "section": "Page 184"}
{"text": "implements them. These containers rely on the Java concepts of equality and hashing, and all\nobjects in Java have an equals  and a hashCode  method that allow them to be stored in lists and\nmaps.\nFor non-hashing containers, objects are resolved based on equality rather than identity. Thus,\nlist.remove(myobject)  iterates through the list looking for an element for which\nelement.equals(myobject) , rather than element == myobject , is true. Because all objects\nmust have an equals  method, we implemented a \"virtual equals\" function on the JavaScript side of\nthings. This function takes two objects as arguments, checks whether either of them implements their\nown equals  function, and if so, falls through to that function. If they don't, and the passed objects\nare primitives, primitive equality is checked. If they're not, then there is no equality.\nFor hashing containers, things are even more interesting, as hashing containers act as shortcut trees.\nThe container actually wraps a variable number of lists, each tied to a specific hash code. Objects\nare found based on first finding the container that matches their hash code, in which the object is\nthen searched for based on equality evaluation. As all objects in Java have a hashCode  method,\nwe also wrote a \"virtual hashcode\" function, which takes a single object as an argument. The\nfunction checks whether the object implements its own hashCode  function, and if so falls through to\nthat function. If it doesn't, the hash code is computed based on the same hashing algorithm that is\nused in Java.\nAdministration\nThe final piece of functionality in the static code library is the instance list of all sketches that are\ncurrently running on the page. This instance list stores sketches based on the canvas they have been\nloaded in, so that users can call Processing.getInstanceById('canvasid')  and get a reference to\ntheir sketch for page interaction purposes.\nInstance Code\nInstance code takes the form of p.functor = function(arg, \u2026)  definitions for the Processing API,\nand p.constant = \u2026  for sketch state variables (where p  is our reference to the sketch being set up).\nNeither of these are located in dedicated code blocks. Rather, the code is organized based on function,\nso that instance code relating to PShape operations is defined near the PShape object, and instance\ncode for graphics functions are defined near, or in, the Drawing2D and Drawing3D objects.\nIn order to keep things fast, a lot of code that could be written as static code with an instance wrapper is\nactually implemented as purely instance code. For instance, the lerpColor(c1, c2, ratio)  function,\nwhich determines the color corresponding to the linear interpolation of two colors, is defined as an\ninstance function. Rather than having p.lerpColor(c1, c2, ratio)  acting as a wrapper for some\nstatic function Processing.lerpColor(c1, c2, ratio) , the fact that nothing else in Processing.js\nrelies on lerpColor  means that code execution is faster if we write it as a pure instance function.\nWhile this does \"bloat\" the instance object, most functions for which we insist on an instance function\nrather than a wrapper to the static library are small. Thus, at the expense of memory we create really fast\ncode paths. While the full Processing object will take up a one-time memory slice worth around 5 MB\nwhen initially set up, the prerequisite code for individual sketches only takes up about 500 KB.\n17.4. Developing Processing.js\nProcessing.js is worked on intensively, which we can only do because our development approach sticks\nto a few basic rules. As these rules influence the architecture of Processing.js, it's worth having a brief\nlook at them before closing this chapter.\nMake It Work\nWriting code that works sounds like a tautological premise; you write code, and by the time you're done\nyour code either works, because that's what you set out to do, or it doesn't, and you're not done yet.\nHowever, \"make it work\" comes with a corollary: Make it work, and when you're done, prove it.\nIf there is one thing above all other things that has allowed Processing.js to grow at the pace it has, it is\nthe presence of tests. Any ticket that requires touching the code, be it either by writing new code or\nrewriting old code, cannot be marked as resolved until there is a unit or reference test that allows others\nto verify not only that the code works the way it should, but also that it breaks when it should. For most\ncode, this typically involves a unit test\u2014a short bit of code that calls a function and simply tests whether\nthe function returns the correct values, for both legal and illegal function calls. Not only does this allow us\nto test code contributions, it also lets us perform regression tests.\nBefore any code is accepted and merged into our stable development branch, the modified Processing.js\nlibrary is validated against an ever-growing battery of unit tests. Big fixes and performance tests in\nparticular are prone to passing their own unit tests, but breaking parts that worked fine before the rewrite.\nHaving tests for every function in the API, as well as internal functions, means that as Processing.js\ngrows, we don't accidentally break compatibility with previous versions. Barring destructive API changes,\nif none of the tests failed before a code contribution or modification, none of the tests are allowed to fail\nwith the new code in.\nThe following is an example of a unit test verifying inline object creation.\n", "page": 185, "type": "text", "section": "Page 185"}
{"text": "    interface I {\n      int getX();\n      void test(); }\n    I i = new I() {\n      int x = 5;\n      public int getX() {\n        return x; }\n      public void test() {\n        x++; }};\n    i.test();\n    _checkEqual(i.getX(), 6);\n    _checkEqual(i instanceof I, true);\n    _checkEqual(i instanceof Object, true);\nIn addition to regular code unit tests, we also have visual reference (or \"ref\") tests. As Processing.js is a\nport of a visual programming language, some tests cannot be performed using just unit tests. Testing to\nsee whether an ellipse gets drawn on the correct pixels, or whether a single-pixel-wide vertical line is\ndrawn crisp or smoothed cannot be determined without a visual reference. Because all mainstream\nbrowsers implement the <canvas>  element and Canvas2D API with subtle differences, these things can\nonly be tested by running code in a browser and verifying that the resulting sketch looks the same as\nwhat native Processing generates. To make life easier for developers, we use an automated test suite for\nthis, where new test cases are run through Processing, generating \"what it should look like\" data to be\nused for pixel comparison. This data is then stored as a comment inside the sketch that generated it,\nforming a test, and these tests are then run by Processing.js on a visual reference test page which\nexecutes each test and performs pixel comparisons between \"what it should look like\" and \"what it looks\nlike\". If the pixels are off, the test fails, and the developer is presented with three images: what it should\nlook like, how Processing.js rendered it, and the difference between the two, marking problem areas as\nred pixels, and correct areas as white. Much like unit tests, these tests must pass before any code\ncontribution can be accepted.\nMake It Fast\nIn an open source project, making things work is only the first step in the life of a function. Once things\nwork, you want to make sure things work fast. Based on the \"if you can't measure it, you can't improve it\"\nprinciple, most functions in Processing.js don't just come with unit or ref tests, but also with performance\n(or \"perf\") tests. Small bits of code that simply call a function, without testing the correctness of the\nfunction, are run several hundred times in a row, and their run time is recorded on a special performance\ntest web page. This lets us quantify how well (or not!) Processing.js performs in browsers that support\nHTML5's <canvas>  element. Every time an optimization patch passes unit and ref testing, it is run\nthrough our performance test page. JavaScript is a curious beast, and beautiful code can, in fact, run\nseveral orders of magnitude slower than code that contains the same lines several times over, with inline\ncode rather than function calls. This makes performance testing crucial. We have been able to speed up\ncertain parts of the library by three orders of magnitude simply by discovering hot loops during perf\ntesting, reducing the number of function calls by inlining code, and by making functions return the\nmoment they know what their return value should be, rather than having only a single return at the very\nend of the function.\nAnother way in which we try to make Processing.js fast is by looking at what runs it. As Processing.js is\nhighly dependent on the efficiency of JavaScript engines, it makes sense to also look at which features\nvarious engines offer to speed things up. Especially now that browsers are starting to support hardware\naccelerated graphics, instant speed boosts are possible when engines offer new and more efficient data\ntypes and functions to perform the low level operations that Processing.js depends on. For instance,\nJavaScript technically has no static typing, but graphics hardware programming environments do. By\nexposing the data structures used to talk to the hardware directly to JavaScript, it is possible to\nsignificantly speed up sections of code if we know that they will only use specific values.\nMake It Small\nThere are two ways to make code small. First, write compact code. If you're manipulating a variable\nmultiple times, compact it to a single manipulation (if possible). If you access an object variable multiple\ntimes, cache it. If you call a function multiple times, cache the result. Return once you have all the\ninformation you need, and generally apply all the tricks a code optimiser would apply yourself. JavaScript\nis a particularly nice language for this, since it comes with an incredible amount of flexibility. For example,\nrather than using:\nif ((result = functionresult)!==null) {\n  var = result;\n} else {\n  var = default;\n}\n", "page": 186, "type": "text", "section": "Page 186"}
{"text": "in JavaScript this becomes:\nvar = functionresult || default\nThere is also another form of small code, and that's in terms of runtime code. Because JavaScript lets\nyou change function bindings on the fly, running code becomes much smaller if you can say \"bind the\nfunction for line2D to the function call for line \" once you know that a program runs in 2D rather than\n3D mode, so that you don't have to perform:\nif(mode==2D) { line2D() } else { line3D() }\nfor every function call that might be either in 2D or 3D mode.\nFinally, there is the process of minification. There are a number of good systems that let you compress\nyour JavaScript code by renaming variables, stripping whitespace, and applying certain code\noptimisations that are hard to do by hand while still keeping the code readable. Examples of these are\nthe YUI minifier and Google's closure compiler. We use these technologies in Processing.js to offer end\nusers bandwidth convenience\u2014minification after stripping comments can shrink the library by as much as\n50%, and taking advantage of modern browser/server interaction for gzipped content, we can offer the\nentire Processing.js library in gzipped form in 65 KB.\nIf All Else Fails, Tell People\nNot everything that can currently be done in Processing can be done in the browser. Security models\nprevent certain things like saving files to the hard disk and performing USB or serial port I/O, and a lack\nof typing in JavaScript can have unexpected consequences (such as all math being floating point math).\nSometimes we're faced with the choice between adding an incredible amount of code to enable an edge\ncase, or mark the ticket as a \"wontfix\" issue. In such cases, a new ticket gets filed, typically titled \"Add\ndocumentation that explains why\u2026\".\nIn order to make sure these things aren't lost, we have documentation for people who start using\nProcessing.js with a Processing background, and for people who start using Processing.js with a\nJavaScript background, covering the differences between what is expected, and what actually happens.\nCertain things just deserve special mention, because no matter how much work we put into Processing.js,\nthere are certain things we cannot add without sacrificing usability. A good architecture doesn't just cover\nthe way things are, it also covers why; without that, you'll just end up having the same discussions about\nwhat the code looks like and whether it should be different every time the team changes.\n17.5. Lessons Learned\nThe most important lesson we learned while writing Processing.js is that when porting a language, what\nmatters is that the result is correct, not whether or not the code used in your port is similar to the\noriginal. Even though Java and JavaScript syntax are fairly similar, and modifying Java code to legal\nJavaScript code is fairly easy, it often pays to look at what JavaScript can natively do and exploit that to\nget the same functional result. Taking advantage of the lack of typing by recycling variables, using\ncertain built-in functions that are fast in JavaScript but slow in Java, or avoiding patterns that are fast in\nJava but slow in JavaScript means your code may look radically different, but has the exact same effect.\nYou often hear people say not to reinvent the wheel, but that only applies to working with a single\nprogramming language. When you're porting, reinvent as many wheels as you need to obtain the\nperformance you require.\nAnother important lesson is to return early, return often, and branch as little as possible. An if/then\nstatement followed by a return can be made (sometimes drastically) faster by using an if-return/return\nconstruction instead, using the return statement as a conditional shortcut. While it's conceptually pretty to\naggregate your entire function state before calling the ultimate return statement for that function, it also\nmeans your code path may traverse code that is entirely unrelated to what you will be returning. Don't\nwaste cycles; return when you have all the information you need.\nA third lesson concerns testing your code. In Processing.js we had the benefit of starting with very good\ndocumentation outlining how Processing was \"supposed\" to work, and a large set of test cases, most of\nwhich started out as \"known fail\". This allowed us to do two things: 1) write code against tests, and 2)\ncreate tests before writing code. The usual process, in which code is written and then test cases are\nwritten for that code, actually creates biased tests. Rather than testing whether or not your code does\nwhat it should do, according to the specification, you are only testing whether your code is bug-free. In\nProcessing.js, we instead start by creating test cases based on what the functional requirements for some\nfunction or set of functions is, based on the documentation for it. With these unbiased tests, we can then\nwrite code that is functionally complete, rather than simply bug-free but possibly deficient.\nThe last lesson is also the most general one: apply the rules of agile development to individual fixes as\nwell. No one benefits from you retreating into dev mode and not being heard from for three days straight\nwhile you write the perfect solution. Rather, get your solutions to the point where they work, and not\neven necessarily for all test cases, then ask for feedback. Working alone, with a test suite for catching\nerrors, is no guarantee of good or complete code. No amount of automated testing is going to point out\nthat you forgot to write tests for certain edge cases, or that there is a better algorithm than the one you\npicked, or that you could have reordered your statements to make the code better suited for JIT\n", "page": 187, "type": "text", "section": "Page 187"}
{"text": "This work is made available under the Creative Commons Attribution 3.0 Unported license. Please see\nthe full description of the license for details.\ncompilation. Treat fixes like releases: present fixes early, update often, and work feedback into your\nimprovements.\nBack to top\nBack to The Architecture of Open Source Applications.\n", "page": 188, "type": "text", "section": "Page 188"}
{"text": "<>\n18.1. Introduction\nPuppet is an open source IT management tool written in Ruby, used for datacenter automation and\nserver management at Google, Twitter, the New York Stock Exchange, and many others. It is primarily\nmaintained by Puppet Labs, which also founded the project. Puppet can manage as few as 2 machines\nand as many as 50,000, on teams with one system administrator or hundreds.\nPuppet is a tool for configuring and maintaining your computers; in its simple configuration language, you\nexplain to Puppet how you want your machines configured, and it changes them as needed to match\nyour specification. As you change that specification over time\u2014such as with package updates, new\nusers, or configuration updates\u2014Puppet will automatically update your machines to match. If they are\nalready configured as desired, then Puppet does nothing.\nIn general, Puppet does everything it can to use existing system features to do its work; e.g., on Red Hat\nit will use yum  for packages and init.d  for services, but on OS X it will use dmg  for packages and\nlaunchd  for services. One of the guiding goals in Puppet is to have the work it does make sense\nwhether you are looking at Puppet code or the system itself, so following system standards is critical.\nPuppet comes from multiple traditions of other tools. In the open source world, it is most influenced by\nCFEngine, which was the first open source general-purpose configuration tool, and ISconf, whose use of\nmake  for all work inspired the focus on explicit dependencies throughout the system. In the commercial\nworld, Puppet is a response to BladeLogic and Opsware (both since acquired by larger companies), each\nof which was successful in the market when Puppet was begun, but each of which was focused on\nselling to executives at large companies rather than building great tools directly for system administrators.\nPuppet is meant to solve similar problems to these tools, but it is focused on a very different user.\nFor a simple example of how to use Puppet, here is a snippet of code that will make sure the secure\nshell service (SSH) is installed and configured properly:\nclass ssh {\n    package { ssh: ensure => installed }\n    file { \"/etc/ssh/sshd_config\":\n        source => 'puppet:///modules/ssh/sshd_config',\n        ensure => present,\n        require => Package[ssh]\n    }\n    service { sshd:\n        ensure => running,\n        require => [File[\"/etc/ssh/sshd_config\"], Package[ssh]]\n    }\n}\nThis makes sure the package is installed, the file is in place, and the service is running. Note that we've\nspecified dependencies between the resources, so that we always perform any work in the right order.\nThis class could then be associated with any host to apply this configuration to it. Notice that the building\nblocks of a Puppet configuration are structured objects, in this case package , file , and service .\nWe call these objects resources in Puppet, and everything in a Puppet configuration comes down to\nthese resources and the dependencies between them.\nA normal Puppet site will have tens or even hundreds of these code snippets, which we call classes; we\nstore these classes on disk in files called manifests , and collect them in related groups called\nmodules. For instance, you might have an ssh  module with this ssh  class plus any other related\nclasses, along with modules for mysql , apache , and sudo .\nMost Puppet interactions are via the command line or long-running HTTP services, but there are\ngraphical interfaces for some things such as report processing. Puppet Labs also produces commercial\nPuppet\nLuke Kanies\n", "page": 189, "type": "text", "section": "Page 189"}
{"text": "products around Puppet, which tend more toward graphical web-based interfaces.\nPuppet's first prototype was written in the summer of 2004, and it was turned into a full-time focus in\nFebruary of 2005. It was initially designed and written by Luke Kanies, a sysadmin who had a lot of\nexperience writing small tools, but none writing tools greater than 10,000 lines of code. In essence, Luke\nlearned to be a programmer while writing Puppet, and that shows in its architecture in both positive and\nnegative ways.\nPuppet was first and foremost built to be a tool for sysadmins, to make their lives easier and allow them\nto work faster, more efficiently, and with fewer errors. The first key innovation meant to deliver on this\nwas the resources mentioned above, which are Puppet's primitives; they would both be portable across\nmost operating systems and also abstract away implementation detail, allowing the user to focus on\noutcomes rather than how to achieve them. This set of primitives was implemented in Puppet's Resource\nAbstraction Layer.\nPuppet resources must be unique on a given host. You can only have one package named \"ssh\", one\nservice named \"sshd\", and one file named \"/etc/ssh/sshd_config\". This prevents different parts of your\nconfigurations from conflicting with each other, and you find out about those conflicts very early in the\nconfiguration process. We refer to these resources by their type and title; e.g., Package[ssh]  and\nService[sshd] . You can have a package and a service with the same name because they are\ndifferent types, but not two packages or services with the same name.\nThe second key innovation in Puppet provides the ability to directly specify dependencies between\nresources. Previous tools focused on the individual work to be done, rather than how the various bits of\nwork were related; Puppet was the first tool to explicitly say that dependencies are a first-class part of\nyour configurations and must be modeled that way. It builds a graph of resources and their dependencies\nas one of the core data types, and essentially everything in Puppet hangs off of this graph (called a\nCatalog) and its vertices and edges.\nThe last major component in Puppet is its configuration language. This language is declarative, and is\nmeant to be more configuration data than full programming\u2014it most resembles Nagios's configuration\nformat, but is also heavily influenced by CFEngine and Ruby.\nBeyond the functional components, Puppet has had two guiding principles throughout its development: it\nshould be as simple as possible, always preferring usability even at the expense of capability; and it\nshould be built as a framework first and application second, so that others could build their own\napplications on Puppet's internals as desired. It was understood that Puppet's framework needed a killer\napplication to be adopted widely, but the framework was always the focus, not the application. Most\npeople think of Puppet as being that application, rather than the framework behind it.\nWhen Puppet's prototype was first built, Luke was essentially a decent Perl programmer with a lot of\nshell experience and some C experience, mostly working in CFEngine. The odd thing is he had\nexperience building parsers for simple languages, having built two as part of smaller tools and also\nhaving rewritten CFEngine's parser from scratch in an effort to make it more maintainable (this code was\nnever submitted to the project, because of small incompatibilities).\nA dynamic language was easily decided on for Puppet's implementation, based on much higher\ndeveloper productivity and time to market, but choosing the language proved difficult. Initial prototypes in\nPerl went nowhere, so other languages were sought for experimentation. Python was tried, but Luke\nfound the language quite at odds with how he thought about the world. Based on what amounted to a\nrumor of utility heard from a friend, Luke tried Ruby, and in four hours had built a usable prototype. When\nPuppet became a full-time effort in 2005 Ruby was a complete unknown, so the decision to stick with it\nwas a big risk, but again programmer productivity was deemed the primary driver in language choice.\nThe major distinguishing feature in Ruby, at least as opposed to Perl, was how easy it was to build non-\nhierarchical class relationships, but it also mapped very well to Luke's brain, which turned out to be\ncritical.\n18.2. Architectural Overview\nThis chapter is primarily about the architecture of Puppet's implementation (that is, the code that we've\nused to make Puppet do the things it's supposed to do) but it's worth briefly discussing its application\narchitecture (that is, how the parts communicate), so that the implementation makes some sense.\nPuppet has been built with two modes in mind: A client/server mode with a central server and agents\nrunning on separate hosts, or a serverless mode where a single process does all of the work. To ensure\nconsistency between these modes, Puppet has always had network transparency internally, so that the\ntwo modes used the same code paths whether they went over the network or not. Each executable can\nconfigure local or remote service access as appropriate, but otherwise they behave identically. Note also\nthat you can use the serverless mode in what amounts to a client/server configuration, by pulling all\nconfiguration files to each client and having it parse them directly. This section will focus on the\nclient/server mode, because it's more easily understood as separate components, but keep in mind that\nthis is all true of the serverless mode, too.\nOne of the defining choices in Puppet's application architecture is that clients should not get access to\nraw Puppet modules; instead, they get a configuration compiled just for them. This provides multiple\nbenefits: First, you follow the principle of least privilege, in that each host only knows exactly what it\n", "page": 190, "type": "text", "section": "Page 190"}
{"text": "needs to know (how it should be configured), but it does not know how any other servers are configured.\nSecond, you can completely separate the rights needed to compile a configuration (which might include\naccess to central data stores) from the need to apply that configuration. Third, you can run hosts in a\ndisconnected mode where they repeatedly apply a configuration with no contact to a central server,\nwhich means you remain in compliance even if the server is down or the client is disconnected (such as\nwould be the case in a mobile installation, or when the clients are in a DMZ).\nGiven this choice, the workflow becomes relatively straightforward:\n1. The Puppet agent process collects information about the host it is running on, which it passes to the\nserver.\n2. The parser uses that system information and Puppet modules on local disk to compile a\nconfiguration for that particular host and returns it to the agent.\n3. The agent applies that configuration locally, thus affecting the local state of the host, and files the\nresulting report with the server.\nFigure 18.1: Puppet dataflow\nFigure 18.2: Orchestration of data flow between Puppet processes and components\nThus, the agent has access to its own system information, its configuration, and each report it generates.\nThe server has copies of all of this data, plus access to all of the Puppet modules, and any back-end\ndatabases and services that might be needed to compile the configuration.\nBeyond the components that go into this workflow, which we'll address next, there are many data types\nthat Puppet uses for internal communication. These data types are critical, because they're how all\ncommunication is done and they're public types which any other tools can consume or produce.\nThe most important data types are:\nFacts: System data collected on each machine and used to compile configurations.\nManifest: Files containing Puppet code, generally organized into collections called \"modules\".\nCatalog: A graph of a given host's resources to be managed and the dependencies between them.\n", "page": 191, "type": "text", "section": "Page 191"}
{"text": "Report: The collection of all events generated during application of a given Catalog.\nBeyond Facts, Manifests, Catalogs, and Reports, Puppet supports data types for files, certificates (which\nit uses for authentication), and others.\n18.3. Component Analysis\nAgent\nThe first component encountered in a Puppet run is the agent  process. This was traditionally a\nseparate executable called puppetd , but in version 2.6 we reduced down to one executable so now it is\ninvoked with puppet agent , akin to how Git works. The agent has little functionality of its own; it is\nprimarily configuration and code that implements the client-side aspects of the above-described workflow.\nFacter\nThe next component after the agent is an external tool called Facter, which is a very simple tool used to\ndiscover information about the host it is running on. This is data like the operating system, IP address,\nand host name, but Facter is easily extensible so many organizations add their own plugins to discover\ncustom data. The agent sends the data discovered by Facter to the server, at which point it takes over\nthe workflow.\nExternal Node Classifier\nOn the server, the first component encountered is what we call the External Node Classifier, or ENC.\nThe ENC accepts the host name and returns a simple data structure containing the high-level\nconfiguration for that host. The ENC is generally a separate service or application: either another open\nsource project, such as Puppet Dashboard or Foreman, or integration with existing data stores, such as\nLDAP. The purpose of the ENC is to specify what functional classes a given host belongs to, and what\nparameters should be used to configure those classes. For example, a given host might be in the\ndebian  and webserver  classes, and have the parameter datacenter  set to atlanta .\nNote that as of Puppet 2.7, the ENC is not a required component; users can instead directly specify node\nconfigurations in Puppet code. Support for an ENC was added about 2 years after Puppet was launched\nbecause we realized that classifying hosts is fundamentally different than configuring them, and it made\nmore sense to split these problems into separate tools than to extend the language to support both\nfacilities. The ENC is always recommended, and at some point soon will become a required component\n(at which point Puppet will ship with a sufficiently useful one that that requirement will not be a burden).\nOnce the server receives classification information from the ENC and system information from Facter (via\nthe agent), it bundles all of the information into a Node object and passes it on to the Compiler.\nCompiler\nAs mentioned above, Puppet has a custom language built for specifying system configurations. Its\ncompiler is really three chunks: A Yacc-style parser generator and a custom lexer; a group of classes\nused to create our Abstract Syntax Tree (AST); and the Compiler class that handles the interactions of all\nof these classes and also functions as the API to this part of the system.\nThe most complicated thing about the compiler is the fact that most Puppet configuration code is lazily\nloaded on first reference (to reduce both load times and irrelevant logging about missing-but-unneeded\ndependencies), which means there aren't really explicit calls to load and parse the code.\nPuppet's parser uses a normal Yacc-style parser generator built using the open source Racc tool.\nUnfortunately, there were no open source lexer generators when Puppet was begun, so it uses a custom\nlexer.\nBecause we use an AST in Puppet, every statement in the Puppet grammar evaluates to an instance of\na Puppet AST class (e.g., Puppet::Parser::AST::Statement ), rather than taking action directly, and\nthese AST instances are collected into a tree as the grammar tree is reduced. This AST provides a\nperformance benefit when a single server is compiling configurations for many different nodes, because\nwe can parse once but compile many times. It also gives us the opportunity to perform some\nintrospection of the AST, which provides us information and capability we wouldn't have if parsing\noperated directly.\nVery few approachable AST examples were available when Puppet was begun, so there has been a lot\nof evolution in it, and we've arrived at what seems a relatively unique formulation. Rather than creating a\nsingle AST for the entire configuration, we create many small ASTs, keyed off their name. For instance,\nthis code:\nclass ssh {\n    package { ssh: ensure => present }\n}\ncreates a new AST containing a single Puppet::Parser::AST::Resource  instance, and stores that\nAST by the name \"ssh\" in the hash of all classes for this particular environment. (I've left out details\nabout other constructs akin to classes, but they are unnecessary for this discussion.)\nGiven the AST and a Node object (from the ENC), the compiler takes the classes specified in the node\n", "page": 192, "type": "text", "section": "Page 192"}
{"text": "object (if there are any), looks them up and evaluates them. In the course of this evaluation, the compiler\nis building up a tree of variable scopes; every class gets its own scope which is attached to the creating\nscope. This amounts to dynamic scoping in Puppet: if one class includes another class, then the included\nclass can look up variables directly in the including class. This has always been a nightmare, and we\nhave been on the path to getting rid of this capability.\nThe Scope tree is temporary and is discarded once compiling is done, but the artifact of compiling is also\nbuilt up gradually over the course of the compilation. We call this artifact a Catalog, but it is just a graph\nof resources and their relationships. Nothing of the variables, control structures, or function calls survive\ninto the catalog; it's plain data, and can be trivially converted to JSON, YAML, or just about anything\nelse.\nDuring compilation, we create containment relationships; a class \"contains\" all of the resources that come\nwith that class (e.g., the ssh package above is contained by the ssh class). A class might contain a\ndefinition, which itself contains either yet more definitions, or individual resources. A catalog tends to be a\nvery horizontal, disconnected graph: many classes, each no more than a couple of levels deep.\nOne of the awkward aspects of this graph is that it also contains \"dependency\" relationships, such as a\nservice requiring a package (maybe because the package installation actually creates the service), but\nthese dependency relationships are actually specified as parameter values on the resources, rather than\nas edges in the structure of the graph. Our graph class (called SimpleGraph, for historical reasons) does\nnot support having both containment and dependency edges in the same graph, so we have to convert\nbetween them for various purposes.\nTransaction\nOnce the catalog is entirely constructed (assuming there is no failure), it is passed on to the Transaction.\nIn a system with a separate client and server, the Transaction runs on the client, which pulls the Catalog\ndown via HTTP as in Figure 18.2.\nPuppet's transaction class provides the framework for actually affecting the system, whereas everything\nelse we've discussed just builds up and passes around objects. Unlike transactions in more common\nsystems such as databases, Puppet transactions do not have behaviors like atomicity.\nThe transaction performs a relatively straightforward task: walk the graph in the order specified by the\nvarious relationships, and make sure each resource is in sync. As mentioned above, it has to convert the\ngraph from containment edges (e.g., Class[ssh]  contains Package[ssh]  and Service[sshd] ) to\ndependency edges (e.g., Service[sshd]  depends on Package[ssh] ), and then it does a standard\ntopological sort of the graph, selecting each resource in turn.\nFor a given resource, we perform a simple three-step process: retrieve the current state of that resource,\ncompare it to the desired state, and make any changes necessary to fix discrepancies. For instance,\ngiven this code:\nfile { \"/etc/motd\":\n    ensure => file,\n    content => \"Welcome to the machine\",\n    mode => 644\n}\nthe transaction checks the content and mode of /etc/motd, and if they don't match the specified state, it\nwill fix either or both of them. If /etc/motd is somehow a directory, then it will back up all of the files in\nthat directory, remove it, and replace it with a file that has the appropriate content and mode.\nThis process of making changes is actually handled by a simple ResourceHarness class that defines the\nentire interface between Transaction and Resource. This reduces the number of connections between\nthe classes, and makes it easier to make changes to either independently.\nResource Abstraction Layer\nThe Transaction class is the heart of getting work done with Puppet, but all of the work is actually done\nby the Resource Abstraction Layer (RAL), which also happens to be the most interesting component in\nPuppet, architecturally speaking.\nThe RAL was the first component created in Puppet and, other than the language, it most clearly defines\nwhat the user can do. The job of the RAL is to define what it means to be a resource and how resources\ncan get work done on the system, and Puppet's language is specifically built to specify resources as\nmodeled by the RAL. Because of this, it's also the most important component in the system, and the\nhardest to change. There are plenty of things we would like to fix in the RAL, and we've made a lot of\ncritical improvements to it over the years (the most crucial being the addition of Providers), but there is\nstill a lot of work to do to the RAL in the long term.\nIn the Compiler subsystem, we model resources and resource types with separate classes (named,\nconveniently, Puppet::Resource  and Puppet::Resource::Type ). Our goal is to have these classes\nalso form the heart of the RAL, but for now these two behaviors (resource and type) are modeled within a\nsingle class, Puppet::Type . (The class is named poorly because it significantly predates our use of the\nterm Resource, and at the time we were directly serializing memory structures when communicating\n", "page": 193, "type": "text", "section": "Page 193"}
{"text": "between hosts, so it was actually quite complicated to change class names.)\nWhen Puppet::Type  was first created, it seemed reasonable to put resource and resource type\nbehaviors in the same class; after all, resources are just instances of resource types. Over time,\nhowever, it became clear that the relationship between a resource and its resource type aren't modeled\nwell in a traditional inheritance structure. Resource types define what parameters a resource can have,\nbut not whether it accepts parameters (they all do), for instance. Thus, our base class of\nPuppet::Type  has class-level behaviors that determine how resource types behave, and instance-level\nbehaviors that determine how resource instances behave. It additionally has the responsibility of\nmanaging registration and retrieval of resource types; if you want the \"user\" type, you call\nPuppet::Type.type(:user) .\nThis mix of behaviors makes Puppet::Type  quite difficult to maintain. The whole class is less than\n2,000 lines of code, but working at three levels\u2014resource, resource type, and resource type manager\u2014\nmakes it convoluted. This is obviously why it's a major target for being refactored, but it's more plumbing\nthan user-facing, so it's always been hard to justify effort here rather than directly in features.\nBeyond Puppet::Type , there are two major kinds of classes in the RAL, the most interesting of which\nare what we call Providers. When the RAL was first developed, each resource type mixed the definition\nof a parameter with code that knew how to manage it. For instance, we would define the \"content\"\nparameter, and then provide a method that could read the content of a file, and another method that\ncould change the content:\nPuppet::Type.newtype(:file) do\n    ...\n    newproperty(:content) do\n        def retrieve\n            File.read(@resource[:name])\n        end\n        def sync\n            File.open(@resource[:name], \"w\") { |f| f.print @resource[:content] }\n        end\n    end\nend\nThis example is simplified considerably (e.g., we use checksums internally, rather than the full content\nstrings), but you get the idea.\nThis became impossible to manage as we needed to support multiple varieties of a given resource type.\nPuppet now supports more than 30 kinds of package management, and it would have been impossible to\nsupport all of those within a single Package resource type. Instead, we provide a clean interface between\nthe definition of the resource type\u2014essentially, what the name of the resource type is and what\nproperties it supports\u2014from how you manage that type of resource. Providers define getter and setter\nmethods for all of a resource type's properties, named in obvious ways. For example, this is how a\nprovider of the above property would look:\nPuppet::Type.newtype(:file) do\n    newproperty(:content)\nend\nPuppet::Type.type(:file).provide(:posix) do\n    def content\n        File.read(@resource[:name])\n    end\n    def content=(str)\n        File.open(@resource[:name], \"w\") { |f| f.print(str) }\n    end\nend\nThis is a touch more code in the simplest cases, but is much easier to understand and maintain,\nespecially as either the number of properties or number of providers increases.\nI said at the beginning of this section that the Transaction doesn't actually affect the system directly, and\nit instead relies on the RAL for that. Now it's clear that it's the providers that do the actual work. In fact,\nin general the providers are the only part of Puppet that actually touch the system. The transaction asks\nfor a file's content, and the provider collects it; the transaction specifies that a file's content should be\nchanged, and the provider changes it. Note, however, that the provider never decides to affect the\nsystem\u2014the Transaction owns the decisions, and the provider does the work. This gives the Transaction\ncomplete control without requiring that it understand anything about files, users, or packages, and this\nseparation is what enables Puppet to have a full simulation mode where we can largely guarantee the\nsystem won't be affected.\nThe second major class type in the RAL is responsible for the parameters themselves. We actually\nsupport three kinds of parameters: metaparameters, which affect all resource types (e.g., whether you\nshould run in simulation mode); parameters, which are values that aren't reflected on disk (e.g., whether\n", "page": 194, "type": "text", "section": "Page 194"}
{"text": "you should follow links in files); and properties, which model aspects of the resource that you can change\non disk (e.g., a file's content, or whether a service is running). The difference between properties and\nparameters is especially confusing to people, but if you just think of properties as having getter and setter\nmethods in the providers, it's relatively straightforward.\nReporting\nAs the transaction walks the graph and uses the RAL to change the system's configuration, it\nprogressively builds a report. This report largely consists of the events generated by changes to the\nsystem. These events, in turn, are comprehensive reflections of what work was done: they retain a\ntimestamp the resource changed, the previous value, the new value, any message generated, and\nwhether the change succeeded or failed (or was in simulation mode).\nThe events are wrapped in a ResourceStatus object that maps to each resource. Thus, for a given\nTransaction, you know all of the resources that are run, and you know any changes that happen, along\nwith all of the metadata you might need about those changes.\nOnce the transaction is complete, some basic metrics are calculated and stored in the report, and then it\nis sent off to the server (if configured). With the report sent, the configuration process is complete, and\nthe agent goes back to sleep or the process just ends.\n18.4. Infrastructure\nNow that we have a thorough understanding of what Puppet does and how, it's worth spending a little\ntime on the pieces that don't show up as capabilities but are still critical to getting the job done.\nPlugins\nOne of the great things about Puppet is that it is very extensible. There are at least 12 different kinds of\nextensibility in Puppet, and most of these are meant to be usable by just about anyone. For example, you\ncan create custom plugins for these areas:\nresource types and custom providers\nreport handlers, such as for storing reports in a custom database\nIndirector plugins for interacting with existing data stores\nfacts for discovering extra information about your hosts\nHowever, Puppet's distributed nature means that agents need a way to retrieve and load new plugins.\nThus, at the start of every Puppet run, the first thing we do is download all plugins that the server has\navailable. These might include new resource types or providers, new facts, or even new report\nprocessors.\nThis makes it possible to heavily upgrade Puppet agents without ever changing the core Puppet\npackages. This is especially useful for highly customized Puppet installations.\nIndirector\nYou've probably detected by now that we have a tradition of bad class names in Puppet, and according\nto most people, this one takes the cake. The Indirector is a relatively standard Inversion of Control\nframework with significant extensibility. Inversion of Control systems allow you to separate development of\nfunctionality from how you control which functionality you use. In Puppet's case, this allows us to have\nmany plugins that provide very different functionality, such as reaching the compiler via HTTP or loading\nit in-process, and switch between them with a small configuration change rather than a code change. In\nother words, Puppet's Indirector is basically an implementation of a service locator, as described on the\nWikipedia page for \"Inversion of Control\". All of the hand-offs from one class to another go through the\nIndirector, via a standard REST-like interface (e.g., we support find, search, save, and destroy as\nmethods), and switching Puppet from serverless to client/server is largely a question of configuring the\nagent to use an HTTP endpoint for retrieving catalogs, rather than using a compiler endpoint.\nBecause it is an Inversion of Control framework where configuration is stringently separated from the\ncode paths, this class can also be difficult to understand, especially when you're debugging why a given\ncode path was used.\nNetworking\nPuppet's prototype was written in the summer of 2004, when the big networking question was whether to\nuse XMLRPC or SOAP. We chose XMLRPC, and it worked fine but had most of the problems everyone\nelse had: it didn't encourage standard interfaces between components, and it tended to get\novercomplicated very quickly as a result. We also had significant memory problems, because the\nencoding needed for XMLRPC resulted in every object appearing at least twice in memory, which quickly\ngets expensive for large files.\nFor our 0.25 release (begun in 2008), we began the process of switching all networking to a REST-like\nmodel, but we chose a much more complicated route than just changing out the networking. We\ndeveloped the Indirector as the standard framework for inter-component communication, and built REST\nendpoints as just one option. It took two releases to fully support REST, and we have not quite finished\nconverting to using JSON (instead of YAML) for all serialization. We undertook switching to JSON for two\nmajor reasons: first, YAML processing Ruby is painfully slow, and pure Ruby processing of JSON is a lot\nfaster; second, most of the web seems to be moving to JSON, and it tends to be implemented more\n", "page": 195, "type": "text", "section": "Page 195"}
{"text": "This work is made available under the Creative Commons Attribution 3.0 Unported license. Please see\nthe full description of the license for details.\nportably than YAML. Certainly in the case of Puppet, the first use of YAML was not portable across\nlanguages, and was often not portable across different versions of Puppet, because it was essentially\nserialization of internal Ruby objects.\nOur next major release of Puppet will finally remove all of the XMLRPC support.\n18.5. Lessons Learned\nIn terms of implementation, we're proudest of the various kinds of separation that exist in Puppet: the\nlanguage is completely separate from the RAL, the Transaction cannot directly touch the system, and the\nRAL can't decide to do work on its own. This gives the application developer a lot of control over\napplication workflow, along with a lot of access to information about what is happening and why.\nPuppet's extensibility and configurability are also major assets, because anyone can build on top of\nPuppet quite easily without having to hack the core. We've always built our own capabilities on the same\ninterfaces we recommend our users use.\nPuppet's simplicity and ease of use have always been its major draw. It's still too difficult to get running,\nbut it's miles easier than any of the other tools on the market. This simplicity comes with a lot of\nengineering costs, especially in the form of maintenance and extra design work, but it's worth it to allow\nusers to focus on their problems instead of the tool.\nPuppet's configurability is a real feature, but we took it a bit too far. There are too many ways you can\nwire Puppet together, and it's too easy to build a workflow on top of Puppet that will make you miserable.\nOne of our major near-term goals is to dramatically reduce the knobs you can turn in a Puppet\nconfiguration, so the user cannot so easily configure it poorly, and so we can more easily upgrade it over\ntime without worrying about obscure edge cases.\nWe also just generally changed too slowly. There are major refactors we've been wanting to do for years\nbut have never quite tackled. This has meant a more stable system for our users in the short term, but\nalso a more difficult-to-maintain system, and one that's much harder to contribute to.\nLastly, it took us too long to realize that our goals of simplicity were best expressed in the language of\ndesign. Once we began speaking about design rather than just simplicity, we acquired a much better\nframework for making decisions about adding or removing features, with a better means of\ncommunicating the reasoning behind those decisions.\n18.6. Conclusion\nPuppet is both a simple system and a complex one. It has many moving parts, but they're wired together\nquite loosely, and each of them has changed pretty dramatically since its founding in 2005. It is a\nframework that can be used for all manner of configuration problems, but as an application it is simple\nand approachable.\nOur future success rests on that framework becoming more solid and more simple, and that application\nstaying approachable while it gains capability.\nBack to top\nBack to The Architecture of Open Source Applications.\n", "page": 196, "type": "text", "section": "Page 196"}
{"text": "<>\nPyPy is a Python implementation and a dynamic language implementation framework.\nThis chapter assumes familiarity with some basic interpreter and compiler concepts like bytecode and\nconstant folding.\n19.1. A Little History\nPython is a high-level, dynamic programming language. It was invented by the Dutch programmer Guido\nvan Rossum in the late 1980s. Guido's original implementation is a traditional bytecode interpreter written\nin C, and consequently known as CPython. There are now many other Python implementations. Among\nthe most notable are Jython, which is written in Java and allows for interfacing with Java code,\nIronPython, which is written in C# and interfaces with Microsoft's .NET framework, and PyPy, the subject\nof this chapter. CPython is still the most widely used implementation and currently the only one to\nsupport Python 3, the next generation of the Python language. This chapter will explain the design\ndecisions in PyPy that make it different from other Python implementations and indeed from any other\ndynamic language implementation.\n19.2. Overview of PyPy\nPyPy, except for a negligible number of C stubs, is written completely in Python. The PyPy source tree\ncontains two major components: the Python interpreter and the RPython translation toolchain. The\nPython interpreter is the programmer-facing runtime that people using PyPy as a Python implementation\ninvoke. It is actually written in a subset of Python called Restricted Python (usually abbreviated RPython).\nThe purpose of writing the Python interpreter in RPython is so the interpreter can be fed to the second\nmajor part of PyPy, the RPython translation toolchain. The RPython translator takes RPython code and\nconverts it to a chosen lower-level language, most commonly C. This allows PyPy to be a self-hosting\nimplementation, meaning it is written in the language it implements. As we shall see throughout this\nchapter, the RPython translator also makes PyPy a general dynamic language implementation\nframework.\nPyPy's powerful abstractions make it the most flexible Python implementation. It has nearly 200\nconfiguration options, which vary from selecting different garbage collector implementations to altering\nparameters of various translation optimizations.\n19.3. The Python Interpreter\nSince RPython is a strict subset of Python, the PyPy Python interpreter can be run on top of another\nPython implementation untranslated. This is, of course, extremely slow but it makes it possible to quickly\ntest changes in the interpreter. It also enables normal Python debugging tools to be used to debug the\ninterpreter. Most of PyPy's interpreter tests can be run both on the untranslated interpreter and the\ntranslated interpreter. This allows quick testing during development as well as assurance that the\ntranslated interpreter behaves the same as the untranslated one.\nFor the most part, the details of the PyPy Python interpreter are quite similiar to that of CPython; PyPy\nand CPython use nearly identical bytecode and data structures during interpretation. The primary\ndifference between the two is PyPy has a clever abstraction called object spaces (or objspaces for short).\nAn objspace encapsulates all the knowledge needed to represent and manipulate Python data types. For\nexample, performing a binary operation on two Python objects or fetching an attribute of an object is\nhandled completely by the objspace. This frees the interpreter from having to know anything about the\nimplementation details of Python objects. The bytecode interpreter treats Python objects as black boxes\nand calls objspace methods whenever it needs to manipulate them. For example, here is a rough\nimplementation of the BINARY_ADD  opcode, which is called when two objects are combined with the +\noperator. Notice how the operands are not inspected by the interpreter; all handling is delegated\nimmediately to the objspace.\ndef BINARY_ADD(space, frame):\nPyPy\nBenjamin Peterson\n", "page": 197, "type": "text", "section": "Page 197"}
{"text": "    object1 = frame.pop() # pop left operand off stack\n    object2 = frame.pop() # pop right operand off stack\n    result = space.add(object1, object2) # perform operation\n    frame.push(result) # record result on stack\nThe objspace abstraction has numerous advantages. It allows new data type implementations to be\nswapped in and out without modifying the interpreter. Also, since the sole way to manipulate objects is\nthrough the objspace, the objspace can intercept, proxy, or record operations on objects. Using the\npowerful abstraction of objspaces, PyPy has experimented with thunking, where results can be lazily but\ncompletely transparently computed on demand, and tainting, where any operation on an object will raise\nan exception (useful for passing sensitive data through untrusted code). The most important application\nof objspaces, however, will be discussed in Section 19.4.\nThe objspace used in a vanilla PyPy interpreter is called the standard objspace (std objspace for short).\nIn addition to the abstraction provided by the objspace system, the standard objspace provides another\nlevel of indirection; a single data type may have multiple implementations. Operations on data types are\nthen dispatched using multimethods. This allows picking the most efficient representation for a given\npiece of data. For example, the Python long type (ostensibly a bigint data type) can be represented as a\nstandard machine-word-sized integer when it is small enough. The memory and computationally more\nexpensive arbitrary-precision long implementation need only be used when necessary. There's even an\nimplementation of Python integers available using tagged pointers. Container types can also be\nspecialized to certain data types. For example, PyPy has a dictionary (Python's hash table data type)\nimplementation specialized for string keys. The fact that the same data type can be represented by\ndifferent implementations is completely transparent to application-level code; a dictionary specialized to\nstrings is identical to a generic dictionary and will degenerate gracefully if non-string keys are put into it.\nPyPy distinguishes between interpreter-level (interp-level) and application-level (app-level) code. Interp-\nlevel code, which most of the interpreter is written in, must be in RPython and is translated. It directly\nworks with the objspace and wrapped Python objects. App-level code is always run by the PyPy\nbytecode interpreter. As simple as interp-level RPython code is, compared to C or Java, PyPy developers\nhave found it easiest to use pure app-level code for some parts of the interpreter. Consequently, PyPy\nhas support for embedding app-level code in the interpreter. For example, the functionality of the Python\nprint  statement, which writes objects to standard output, is implemented in app-level Python. Builtin\nmodules can also be written partially in interp-level code and partially in app-level code.\n19.4. The RPython Translator\nThe RPython translator is a toolchain of several lowering phases that rewrite RPython to a target\nlanguage, typically C. The higher-level phases of translation are shown in Figure 19.1. The translator is\nitself written in (unrestricted) Python and intimately linked to the PyPy Python interpreter for reasons that\nwill be illuminated shortly.\nFigure 19.1: Translation steps\nThe first thing the translator does is load the RPython program into its process. (This is done with the\nnormal Python module loading support.) RPython imposes a set of restrictions on normal, dynamic\nPython. For example, functions cannot be created at runtime, and a single variable cannot have the\npossibility of holding incompatible types, such as an integer and a object instance. When the program is\ninitially loaded by the translator, though, it is running on a normal Python interpreter and can use all of\nPython's dynamic features. PyPy's Python interpreter, a huge RPython program, makes heavy use of this\nfeature for metaprogramming. For example, it generates code for standard objspace multimethod\ndispatch. The only requirement is that the program is valid RPython by the time the translator starts the\nnext phase of translation.\nThe translator builds flow graphs of the RPython program through a process called abstract\n", "page": 198, "type": "text", "section": "Page 198"}
{"text": "interpretation. Abstract interpretation reuses the PyPy Python interpreter to interpret RPython programs\nwith a special objspace called the flow objspace. Recall that the Python interpreter treats objects in a\nprogram like black boxes, calling out to the objspace to perform any operation. The flow objspace,\ninstead of the standard set of Python objects, has only two objects: variables and constants. Variables\nrepresent values not known during translation, and constants, not surprisingly, represent immutable\nvalues that are known. The flow objspace has a basic facility for constant folding; if it is asked to do an\noperation where all the arguments are constants, it will statically evaluate it. What is immutable and must\nbe constant in RPython is broader than in standard Python. For example, modules, which are\nemphatically mutable in Python, are constants in the flow objspace because they don't exist in RPython\nand must be constant-folded out by the flow objspace. As the Python interpreter interprets the bytecode\nof RPython functions, the flow objspace records the operations it is asked to perform. It takes care to\nrecord all branches of conditional control flow constructs. The end result of abstract interpretation for a\nfunction is a flow-graph consisting of linked blocks, where each block has one or more operations.\nAn example of the flow-graph generating process is in order. Consider a simple factorial function:\ndef factorial(n):\n    if n == 1:\n        return 1\n    return n * factorial(n - 1)\nThe flow-graph for the function looks like this:\nFigure 19.2: Flow-graph of factorial\nThe factorial function has been divided into blocks containing the operations the flowspace recorded.\nEach block has input arguments and a list of operations on the variables and constants. The first block\nhas an exit switch at the end, which determines which block control-flow will pass to after the first block\nis run. The exit switch can be based on the value of some variable or whether an exception occurred in\nthe last operation of the block. Control-flow follows the lines between the blocks.\nThe flow-graph generated in the flow objspace is in static single assignment form, or SSA, an\nintermediate representation commonly used in compilers. The key feature of SSA is that every variable is\nonly assigned once. This property simplifies the implementation of many compiler transformations and\n", "page": 199, "type": "text", "section": "Page 199"}
{"text": "optimizations.\nAfter a function graph is generated, the annotation phase begins. The annotator assigns a type to the\nresults and arguments of each operation. For example, the factorial function above will be annotated to\naccept and return an integer.\nThe next phase is called RTyping. RTyping uses type information from the annotator to expand each\nhigh-level flow-graph operation into low-level ones. It is the first part of translation where the target\nbackend matters. The backend chooses a type system for the RTyper to specialize the program to. The\nRTyper currently has two type systems: A low-level typesystem for backends like C and one for higher-\nlevel typesystems with classes. High-level Python operations and types are transformed into the level of\nthe type system. For example, an add  operation with operands annotated as integers will generate a\nint_add  operation with the low-level type system. More complicated operations like hash table lookups\ngenerate function calls.\nAfter RTyping, some optimizations on the low-level flow-graph are performed. They are mostly of the\ntraditional compiler variety like constant folding, store sinking, and dead code removal.\nPython code typically has frequent dynamic memory allocations. RPython, being a Python derivative,\ninherits this allocation intensive pattern. In many cases, though, allocations are temporary and local to a\nfunction. Malloc removal is an optimization that addresses these cases. Malloc removal removes these\nallocations by \"flattening\" the previously dynamically allocated object into component scalars when\npossible.\nTo see how malloc removals works, consider the following function that computes the Euclidean distance\nbetween two points on the plane in a roundabout fashion:\ndef distance(x1, y1, x2, y2):\n    p1 = (x1, y1)\n    p2 = (x2, y2)\n    return math.hypot(p1[0] - p2[0], p1[1] - p2[1])\nWhen initially RTyped, the body of the function has the following operations:\nv60 = malloc((GcStruct tuple2))\nv61 = setfield(v60, ('item0'), x1_1)\nv62 = setfield(v60, ('item1'), y1_1)\nv63 = malloc((GcStruct tuple2))\nv64 = setfield(v63, ('item0'), x2_1)\nv65 = setfield(v63, ('item1'), y2_1)\nv66 = getfield(v60, ('item0'))\nv67 = getfield(v63, ('item0'))\nv68 = int_sub(v66, v67)\nv69 = getfield(v60, ('item1'))\nv70 = getfield(v63, ('item1'))\nv71 = int_sub(v69, v70)\nv72 = cast_int_to_float(v68)\nv73 = cast_int_to_float(v71)\nv74 = direct_call(math_hypot, v72, v73)\nThis code is suboptimal in several ways. Two tuples that never escape the function are allocated.\nAdditionally, there is unnecessary indirection accessing the tuple fields.\nRunning malloc removal produces the following concise code:\nv53 = int_sub(x1_0, x2_0)\nv56 = int_sub(y1_0, y2_0)\nv57 = cast_int_to_float(v53)\nv58 = cast_int_to_float(v56)\nv59 = direct_call(math_hypot, v57, v58)\nThe tuple allocations have been completely removed and the indirections flattened out. Later, we will see\nhow a technique similar to malloc removal is used on application-level Python in the PyPy JIT (Section\n19.5).\nPyPy also does function inlining. As in lower-level languages, inlining improves performance in RPython.\nSomewhat surprisingly, it also reduces the size of the final binary. This is because it allows more\nconstant folding and malloc removal to take place, which reduces overall code size.\nThe program, now in optimized, low-level flow-graphs, is passed to the backend to generate sources.\nBefore it can generate C code, the C backend must perform some additional transformations. One of\nthese is exception transformation, where exception handling is rewritten to use manual stack unwinding.\nAnother is the insertion of stack depth checks. These raise an exception at runtime if the recursion is too\ndeep. Places where stack depth checks are needed are found by computing cycles in the call graph of\nthe program.\n", "page": 200, "type": "text", "section": "Page 200"}
{"text": "Another one of the transformations performed by the C backend is adding garbage collection (GC).\nRPython, like Python, is a garbage-collected language, but C is not, so a garbage collector has to be\nadded. To do this, a garbage collection transformer converts the flow-graphs of the program into a\ngarbage-collected program. PyPy's GC transformers provide an excellent demonstration of how\ntranslation abstracts away mundane details. In CPython, which uses reference counting, the C code of\nthe interpreter must carefully keep track of references to Python objects it is manipulating. This not only\nhardcodes the garbage collection scheme in the entire codebase but is prone to subtle human errors.\nPyPy's GC transformer solves both problems; it allows different garbage collection schemes to be\nswapped in and out seamlessly. It is trivial to evaluate a garbage collector implementation (of which\nPyPy has many), simply by tweaking a configuration option at translation. Modulo transformer bugs, the\nGC transformer also never makes reference mistakes or forgets to inform the GC when an object is no\nlonger in use. The power of the GC abstraction allows GC implementations that would be practically\nimpossible to hardcode in an interpreter. For example, several of PyPy's GC implementations require a\nwrite barrier. A write barrier is a check which must be performed every time a GC-managed object is\nplaced in another GC-managed array or structure. The process of inserting write barriers would be\nlaborious and fraught with mistakes if done manually, but is trivial when done automatically by the GC\ntransformer.\nThe C backend can finally emit C source code. The generated C code, being generated from low-level\nflow-graphs, is an ugly mess of goto s and obscurely named variables. An advantage of writing C is\nthat the C compiler can do most of the complicated static transformation work required to make a final\nbinary-like loop optimizations and register allocation.\n19.5. The PyPy JIT\nPython, like most dynamic languages, has traditionally traded efficiency for flexibility. The architecture of\nPyPy, being especially rich in flexibility and abstraction, makes very fast interpretation difficult. The\npowerful objspace and multimethod abstractions in the std objspace do not come without a cost.\nConsequently, the vanilla PyPy interpreter performs up to 4 times slower than CPython. To remedy not\nonly this but Python's reputation as a sluggish language, PyPy has a just-in-time compiler (commonly\nwritten JIT). The JIT compiles frequently used codepaths into assembly during the runtime of the\nprogram.\nThe PyPy JIT takes advantage of PyPy's unique translation architecture described in Section 19.4. PyPy\nactually has no Python-specific JIT; it has a JIT generator. JIT generation is implemented as simply\nanother optional pass during translation. A interpreter desiring JIT generation need only make two special\nfunction calls called jit hints.\nPyPy's JIT is a tracing JIT. This means it detects \"hot\" (meaning frequently run) loops to optimize by\ncompiling to assembly. When the JIT has decided it is going to compile a loop, it records operations in\none iteration of the loop, a process called tracing. These operations are subsequently compiled to\nmachine code.\nAs mentioned above, the JIT generator requires only two hints in the interpreter to generate a JIT:\nmerge_point  and can_enter_jit . can_enter_jit  tells the JIT where in the interpreter a loop\nstarts. In the Python interpreter, this is the end of the JUMP_ABSOLUTE  bytecode. ( JUMP_ABSOLUTE\nmakes the interpreter jump to the head of the app-level loop.) merge_point  tells the JIT where it is\nsafe to return to the interpreter from the JIT. This is the beginning of the bytecode dispatch loop in the\nPython interpreter.\nThe JIT generator is invoked after the RTyping phase of translation. Recall that at this point, the\nprogram's flow-graphs consist of low-level operations nearly ready for target code generation. The JIT\ngenerator locates the hints mentioned above in the interpreter and replaces them with calls to invoke the\nJIT during runtime. The JIT generator then writes a serialized representation of the flow-graphs of every\nfunction that the interpreter wants jitted. These serialized flow-graphs are called jitcodes. The entire\ninterpreter is now described in terms of low-level RPython operations. The jitcodes are saved in the final\nbinary for use at runtime.\nAt runtime, the JIT maintains a counter for every loop that is executed in the program. When a loop's\ncounter exceeds a configurable threshold, the JIT is invoked and tracing begins. The key object in\ntracing is the meta-interpreter. The meta-interpreter executes the jitcodes created in translation. It is thus\ninterpreting the main interpreter, hence the name. As it traces the loop, it creates a list of the operations\nit is executing and records them in JIT intermediate representation (IR), another operation format. This\nlist is called the trace of the loop. When the meta-interpreter encounters a call to a jitted function (one for\nwhich jitcode exists), the meta-interpreter enters it and records its operations to original trace. Thus, the\ntracing has the effect of flattening out the call stack; the only calls in the trace are to interpreter functions\nthat are outside the knowledge of jit.\nThe meta-interpreter is forced to specialize the trace to properties of the loop iteration it is tracing. For\nexample, when the meta-interpreter encounters a conditional in the jitcode, it naturally must choose one\npath based on the state of the program. When it makes a choice based on runtime information, the\nmeta-interpreter records an IR operation called a guard. In the case of a conditional, this will be a\nguard_true  or guard_false  operation on the condition variable. Most arithmetic operations also\nhave guards, which ensure the operation did not overflow. Essentially, guards codify assumptions the\n", "page": 201, "type": "text", "section": "Page 201"}
{"text": "meta-interpreter is making as it traces. When assembly is generated, the guards will protect assembly\nfrom being run in a context it is not specialized for. Tracing ends when the meta-interpreter reaches the\nsame can_enter_jit  operation with which it started tracing. The loop IR can now be passed to the\noptimizer.\nThe JIT optimizer features a few classical compiler optimizations and many optimizations specialized for\ndynamic languages. Among the most important of the latter are virtuals and virtualizables.\nVirtuals are objects which are known not to escape the trace, meaning they are not passed as arguments\nto external, non-jitted function calls. Structures and constant length arrays can be virtuals. Virtuals do not\nhave to be allocated, and their data can be stored directly in registers and on the stack. (This is much\nlike the static malloc removal phase described in the section about translation backend optimizations.)\nThe virtuals optimization strips away the indirection and memory allocation inefficiencies in the Python\ninterpreter. For example, by becoming virtual, boxed Python integer objects are unboxed into simple\nword-sized integers and can be stored directly in machine registers.\nA virtualizable acts much like a virtual but may escape the trace (that is, be passed to non-jitted\nfunctions). In the Python interpreter the frame object, which holds variable values and the instruction\npointer, is marked virtualizable. This allows stack manipulations and other operations on the frame to be\noptimized out. Although virtuals and virtualizables are similar, they share nothing in implementation.\nVirtualizables are handled during tracing by the meta-interpreter. This is unlike virtuals, which are\nhandled during trace optimization. The reason for this is virtualizables require special treatment, since\nthey may escape the trace. Specifically, the meta-interpreter has to ensure that non-jitted functions that\nmay use the virtualizable don't actually try to fetch its fields. This is because in jitted code, the fields of\nvirtualizable are stored in the stack and registers, so the actual virtualizable may be out of date with\nrespect to its current values in the jitted code. During JIT generation, code which accesses a virtualizable\nis rewritten to check if jitted assembly is running. If it is, the JIT is asked to update the fields from data in\nassembly. Additionally when the external call returns to jitted code, execution bails back to the interpreter.\nAfter optimization, the trace is ready to be assembled. Since the JIT IR is already quite low-level,\nassembly generation is not too difficult. Most IR operations correspond to only a few x86 assembly\noperations. The register allocator is a simple linear algorithm. At the moment, the increased time that\nwould be spent in the backend with a more sophisticated register allocation algorithm in exchange for\ngenerating slightly better code has not been justified. The trickiest portions of assembly generation are\ngarbage collector integration and guard recovery. The GC has to be made aware of stack roots in the\ngenerated JIT code. This is accomplished by special support in the GC for dynamic root maps.\nWhen a guard fails, the compiled assembly is no longer valid and control must return to the bytecode\ninterpreter. This bailing out is one of the most difficult parts of JIT implementation, since the interpreter\nstate has to be reconstructed from the register and stack state at the point the guard failed. For each\nguard, the assembler writes a compact description of where all the values needed to reconstruct the\ninterpreter state are. At guard failure, execution jumps to a function which decodes this description and\npasses the recovery values to a higher level be reconstructed. The failing guard may be in the middle of\nthe execution of a complicated opcode, so the interpreter can not just start with the next opcode. To\nsolve this, PyPy uses a blackhole interpreter. The blackhole interpreter executes jitcodes starting from\nthe point of guard failure until the next merge point is reached. There, the real interpreter can resume.\nThe blackhole interpreter is so named because unlike the meta-interpreter, it doesn't record any of the\noperations it executes. The process of guard failure is depicted in Figure 19.3.\nFigure 19.3: Bailing back to the interpreter on guard failure\nAs described up to this point, the JIT would be essentially useless on any loop with a frequently\nchanging condition, because a guard failure would prevent assembly from running very many iterations.\nEvery guard has a failure counter. After the failure count has passed a certain threshold, the JIT starts\ntracing from the point of guard failure instead of bailing back to the interpreter. This new sub-trace is\ncalled a bridge. When the tracing reaches the end of the loop, the bridge is optimized and compiled and\nthe original loop is patched at the guard to jump to the new bridge instead of the failure code. This way,\nloops with dynamic conditions can be jitted.\nHow successful have the techniques used in the PyPy JIT proven? At the time of this writing, PyPy is a\ngeometric average of five times faster than CPython on a comprehensive suite of benchmarks. With the\nJIT, app-level Python has the possibility of being faster than interp-level code. PyPy developers have\nrecently had the excellent problem of having to write interp-level loops in app-level Python for\nperformance.\nMost importantly, the fact that the JIT is not specific to Python means it can be applied to any interpreter\n", "page": 202, "type": "text", "section": "Page 202"}
{"text": "written within the PyPy framework. This need not necessarily be a language interpreter. For example, the\nJIT is used for Python's regular expression engine. NumPy is a powerful array module for Python used in\nnumerical computing and scientific research. PyPy has an experimental reimplementation of NumPy. It\nharnesses the power of the PyPy JIT to speed up operations on arrays. While the NumPy\nimplementation is still in its early stages, initial performance results look promising.\n19.6. Design Drawbacks\nWhile it beats C any day, writing in RPython can be a frustrating experience. Its implicit typing is difficult\nto get used to at first. Not all Python language features are supported and others are arbitrarily restricted.\nRPython is not specified formally anywhere and what the translator accepts can vary from day to day as\nRPython is adapted to PyPy's needs. The author of this chapter often manages to create programs that\nchurn in the translator for half an hour, only to fail with an obscure error.\nThe fact that the RPython translator is a whole-program analyzer creates some practical problems. The\nsmallest change anywhere in translated code requires retranslating the entire interpreter. That currently\ntakes about 40 minutes on a fast, modern system. The delay is especially annoying for testing how\nchanges affect the JIT, since measuring performance requires a translated interpreter. The requirement\nthat the whole program be present at translation means modules containing RPython cannot be built and\nloaded separately from the core interpreter.\nThe levels of abstraction in PyPy are not always as clear cut as in theory. While technically the JIT\ngenerator should be able to produce an excellent JIT for a language given only the two hints mentioned\nabove, the reality is that it behaves better on some code than others. The Python interpreter has seen a\nlot of work towards making it more \"jit-friendly\", including many more JIT hints and even new data\nstructures optimized for the JIT.\nThe many layers of PyPy can make tracking down bugs a laborious process. A Python interpreter bug\ncould be directly in the interpreter source or buried somewhere in the semantics of RPython and the\ntranslation toolchain. Especially when a bug cannot be reproduced on the untranslated interpreter,\ndebugging is difficult. It typically involves running GDB on the nearly unreadable generated C sources.\nTranslating even a restricted subset of Python to a much lower-level language like C is not an easy task.\nThe lowering passes described in Section 19.4 are not really independent. Functions are being annotated\nand rtyped throughout translation, and the annotator has some knowledge of low-level types. The\nRPython translator is thus a tangled web of cross-dependencies. The translator could do with cleaning\nup in several places, but doing it is neither easy nor much fun.\n19.7. A Note on Process\nIn part to combat its own complexity (see Section 19.6), PyPy has adopted several so-called \"agile\"\ndevelopment methodologies. By far the most important of these is test-driven development. All new\nfeatures and bug fixes are required to have tests to verify their correctness. The PyPy Python interpreter\nis also run against CPython's regression test suite. PyPy's test driver, py.test, was spun off and is now\nused in many other projects. PyPy also has a continuous integration system that runs the test suite and\ntranslates the interpreter on a variety of platforms. Binaries for all platforms are produced daily and the\nbenchmark suite is run. All these tests ensure that the various components are behaving, no matter what\nchange is made in the complicated architecture.\nThere is a strong culture of experimentation in the PyPy project. Developers are encouraged to make\nbranches in the Mercurial repository. There, ideas in development can be refined without destabilizing the\nmain branch. Branches are not always successful, and some are abandoned. If anything though, PyPy\ndevelopers are tenacious. Most famously, the current PyPy JIT is the fifth attempt to add a JIT to PyPy!\nThe PyPy project also prides itself on its visualization tools. The flow-graph charts in Section 19.4 are\none example. PyPy also has tools to show invocation of the garbage collector over time and view the\nparse trees of regular expressions. Of special interest is jitviewer, a program that allows one to visually\npeel back the layers of a jitted function, from Python bytecode to JIT IR to assembly. (The jitviewer is\nshown in Figure 19.4.) Visualization tools help developers understand how PyPy's many layers interact\nwith each other.\n", "page": 203, "type": "text", "section": "Page 203"}
{"text": "Figure 19.4: The jitviewer showing Python bytecode and associated JIT IR operations\n19.8. Summary\nThe Python interpreter treats Python objects as black boxes and leaves all behavior to be defined by the\nobjspace. Individual objspaces can provide special extended behavior to Python objects. The objspace\napproach also enables the abstract interpretation technique used in translation.\nThe RPython translator allows details like garbage collection and exception handling to be abstracted\nfrom the language interpreter. It also opens up the possibly of running PyPy on many different runtime\nplatforms by using different backends.\nOne of the most important uses of the translation architecture is the JIT generator. The generality of the\nJIT generator allows JITs for new languages and sub-languages like regular expressions to be added.\nPyPy is the fastest Python implementation today because of its JIT generator.\nWhile most of PyPy's development effort has gone into the Python interpreter, PyPy can be used for the\nimplementation of any dynamic language. Over the years, partial interpreters for JavaScript, Prolog,\nScheme, and IO have been written with PyPy.\n19.9. Lessons Learned\nFinally, some of lessons to take away from the PyPy project:\nRepeated refactoring is often a necessary process. For example, it was originally envisioned that the C\nbackend for the translator would be able to work off the high-level flow graphs! It took several iterations\nfor the current multi-phase translation process to be born.\nThe most important lesson of PyPy is the power of abstraction. In PyPy, abstractions separate\nimplementation concerns. For example, RPython's automatic garbage collection allows a developer\nworking the interpreter to not worry about memory management. At the same time, abstractions have a\nmental cost. Working on the translation chain involves juggling the various phases of translation at once\nin one's head. What layer a bug resides in can also be clouded by abstractions; abstraction leakage,\nwhere swapping low-level components that should be interchangeable breaks higher-level code, is\nperennial problem. It is important that tests are used to verify that all parts of the system are working, so\na change in one system does not break a different one. More concretely, abstractions can slow a\nprogram down by creating too much indirection.\nThe flexibility of (R)Python as an implementation language makes experimenting with new Python\nlanguage features (or even new languages) easy. Because of its unique architecture, PyPy will play a\n", "page": 204, "type": "text", "section": "Page 204"}
{"text": "This work is made available under the Creative Commons Attribution 3.0 Unported license. Please see\nthe full description of the license for details.\nlarge role in the future of Python and dynamic language implementation.\nBack to top\nBack to The Architecture of Open Source Applications.\n", "page": 205, "type": "text", "section": "Page 205"}
{"text": "<>\nSQLAlchemy is a database toolkit and object-relational mapping (ORM) system for the Python\nprogramming language, first introduced in 2005. From the beginning, it has sought to provide an end-to-\nend system for working with relational databases in Python, using the Python Database API (DBAPI) for\ndatabase interactivity. Even in its earliest releases, SQLAlchemy's capabilities attracted a lot of attention.\nKey features include a great deal of fluency in dealing with complex SQL queries and object mappings,\nas well as an implementation of the \"unit of work\" pattern, which provides for a highly automated system\nof persisting data to a database.\nStarting from a small, roughly implemented concept, SQLAlchemy quickly progressed through a series of\ntransformations and reworkings, turning over new iterations of its internal architectures as well as its\npublic API as the userbase continued to grow. By the time version 0.5 was introduced in January of\n2009, SQLAlchemy had begun to assume a stable form that was already proving itself in a wide variety\nof production deployments. Throughout 0.6 (April, 2010) and 0.7 (May, 2011), architectural and API\nenhancements continued the process of producing the most efficient and stable library possible. As of\nthis writing, SQLAlchemy is used by a large number of organizations in a variety of fields, and is\nconsidered by many to be the de facto standard for working with relational databases in Python.\n20.1. The Challenge of Database Abstraction\nThe term \"database abstraction\" is often assumed to mean a system of database communication which\nconceals the majority of details of how data is stored and queried. The term is sometimes taken to the\nextreme, in that such a system should not only conceal the specifics of the relational database in use,\nbut also the details of the relational structures themselves and even whether or not the underlying\nstorage is relational.\nThe most common critiques of ORMs center on the assumption that this is the primary purpose of such a\ntool\u2014to \"hide\" the usage of a relational database, taking over the task of constructing an interaction with\nthe database and reducing it to an implementation detail. Central to this approach of concealment is that\nthe ability to design and query relational structures is taken away from the developer and instead handled\nby an opaque library.\nThose who work heavily with relational databases know that this approach is entirely impractical.\nRelational structures and SQL queries are vastly functional, and comprise the core of an application's\ndesign. How these structures should be designed, organized, and manipulated in queries varies not just\non what data is desired, but also on the structure of information. If this utility is concealed, there's little\npoint in using a relational database in the first place.\nThe issue of reconciling applications that seek concealment of an underlying relational database with the\nfact that relational databases require great specificity is often referred to as the \"object-relational\nimpedance mismatch\" problem. SQLAlchemy takes a somewhat novel approach to this problem.\nSQLAlchemy's Approach to Database Abstraction\nSQLAlchemy takes the position that the developer must be willing to consider the relational form of his or\nher data. A system which pre-determines and conceals schema and query design decisions marginalizes\nthe usefulness of using a relational database, leading to all of the classic problems of impedance\nmismatch.\nAt the same time, the implementation of these decisions can and should be executed through high-level\npatterns as much as possible. Relating an object model to a schema and persisting it via SQL queries is\na highly repetitive task. Allowing tools to automate these tasks allows the development of an application\nthat's more succinct, capable, and efficient, and can be created in a fraction of the time it would take to\ndevelop these operations manually.\nTo this end, SQLAlchemy refers to itself as a toolkit, to emphasize the role of the developer as the\ndesigner/builder of all relational structures and linkages between those structures and the application, not\nas a passive consumer of decisions made by a library. By exposing relational concepts, SQLAlchemy\nembraces the idea of \"leaky abstraction\", encouraging the developer to tailor a custom, yet fully\nSQLAlchemy\nMichael Bayer\n", "page": 206, "type": "text", "section": "Page 206"}
{"text": "automated, interaction layer between the application and the relational database. SQLAlchemy's\ninnovation is the extent to which it allows a high degree of automation with little to no sacrifice in control\nover the relational database.\n20.2. The Core/ORM Dichotomy\nCentral to SQLAlchemy's goal of providing a toolkit approach is that it exposes every layer of database\ninteraction as a rich API, dividing the task into two main categories known as Core and ORM. The Core\nincludes Python Database API (DBAPI) interaction, rendering of textual SQL statements understood by\nthe database, and schema management. These features are all presented as public APIs. The ORM, or\nobject-relational mapper, is then a specific library built on top of the Core. The ORM provided with\nSQLAlchemy is only one of any number of possible object abstraction layers that could be built upon the\nCore, and many developers and organizations build their applications on top of the Core directly.\nFigure 20.1: SQLAlchemy layer diagram\nThe Core/ORM separation has always been SQLAlchemy's most defining feature, and it has both pros\nand cons. The explicit Core present in SQLAlchemy leads the ORM to relate database-mapped class\nattributes to a structure known as a Table , rather than directly to their string column names as\nexpressed in the database; to produce a SELECT query using a structure called select , rather than\npiecing together object attributes directly into a string statement; and to receive result rows through a\nfacade called ResultProxy , which transparently maps the select  to each result row, rather than\ntransferring data directly from a database cursor to a user-defined object.\nCore elements may not be visible in a very simple ORM-centric application. However, as the Core is\ncarefully integrated into the ORM to allow fluid transition between ORM and Core constructs, a more\ncomplex ORM-centric application can \"move down\" a level or two in order to deal with the database in a\nmore specific and finely tuned manner, as the situation requires. As SQLAlchemy has matured, the Core\nAPI has become less explicit in regular use as the ORM continues to provide more sophisticated and\ncomprehensive patterns. However, the availability of the Core was also a contributor to SQLAlchemy's\nearly success, as it allowed early users to accomplish much more than would have been possible when\nthe ORM was still being developed.\nThe downside to the ORM/Core approach is that instructions must travel through more steps. Python's\ntraditional C implementation has a significant overhead penalty for individual function calls, which are the\nprimary cause of slowness in the runtime. Traditional methods of ameliorating this include shortening call\nchains through rearrangement and inlining, and replacing performance-critical areas with C code.\nSQLAlchemy has spent many years using both of these methods to improve performance. However, the\ngrowing acceptance of the PyPy interpreter for Python may promise to squash the remaining\nperformance problems without the need to replace the majority of SQLAlchemy's internals with C code,\nas PyPy vastly reduces the impact of long call chains through just-in-time inlining and compilation.\n20.3. Taming the DBAPI\nAt the base of SQLAlchemy is a system for interacting with the database via the DBAPI. The DBAPI\nitself is not an actual library, only a specification. Therefore, implementations of the DBAPI are available\nfor a particular target database, such as MySQL or PostgreSQL, or alternatively for particular non-DBAPI\ndatabase adapters, such as ODBC and JDBC.\nThe DBAPI presents two challenges. The first is to provide an easy-to-use yet full-featured facade\n", "page": 207, "type": "text", "section": "Page 207"}
{"text": "around the DBAPI's rudimentary usage patterns. The second is to handle the extremely variable nature of\nspecific DBAPI implementations as well as the underlying database engines.\nThe Dialect System\nThe interface described by the DBAPI is extremely simple. Its core components are the DBAPI module\nitself, the connection object, and the cursor object\u2014a \"cursor\" in database parlance represents the\ncontext of a particular statement and its associated results. A simple interaction with these objects to\nconnect and retrieve data from a database is as follows:\nconnection = dbapi.connect(user=\"user\", pw=\"pw\", host=\"host\")\ncursor = connection.cursor()\ncursor.execute(\"select * from user_table where name=?\", (\"jack\",))\nprint \"Columns in result:\", [desc[0] for desc in cursor.description]\nfor row in cursor.fetchall():\n    print \"Row:\", row\ncursor.close()\nconnection.close()\nSQLAlchemy creates a facade around the classical DBAPI conversation. The point of entry to this facade\nis the create_engine  call, from which connection and configuration information is assembled. An\ninstance of Engine  is produced as the result. This object then represents the gateway to the DBAPI,\nwhich itself is never exposed directly.\nFor simple statement executions, Engine  offers what's known as an implicit execution interface. The\nwork of acquiring and closing both a DBAPI connection and cursor are handled behind the scenes:\nengine = create_engine(\"postgresql://user:pw&#64;host/dbname\")\nresult = engine.execute(\"select * from table\")\nprint result.fetchall()\nWhen SQLAlchemy 0.2 was introduced the Connection  object was added, providing the ability to\nexplicitly maintain the scope of the DBAPI connection:\nconn = engine.connect()\nresult = conn.execute(\"select * from table\")\nprint result.fetchall()\nconn.close()\nThe result returned by the execute  method of Engine  or Connection  is called a ResultProxy ,\nwhich offers an interface similar to the DBAPI cursor but with richer behavior. The Engine ,\nConnection , and ResultProxy  correspond to the DBAPI module, an instance of a specific DBAPI\nconnection, and an instance of a specific DBAPI cursor, respectively.\nBehind the scenes, the Engine  references an object called a Dialect . The Dialect  is an abstract\nclass for which many implementations exist, each one targeted at a specific DBAPI/database\ncombination. A Connection  created on behalf of the Engine  will refer to this Dialect  for all\ndecisions, which may have varied behaviors depending on the target DBAPI and database in use.\nThe Connection , when created, will procure and maintain an actual DBAPI connection from a\nrepository known as a Pool  that's also associated with the Engine . The Pool  is responsible for\ncreating new DBAPI connections and, usually, maintaining them in an in-memory pool for frequent re-\nuse.\nDuring a statement execution, an additional object called an ExecutionContext  is created by the\nConnection . The object lasts from the point of execution throughout the lifespan of the ResultProxy .\nIt may also be available as a specific subclass for some DBAPI/database combinations.\nFigure 20.2 illustrates all of these objects and their relationships to each other as well as to the DBAPI\ncomponents.\n", "page": 208, "type": "text", "section": "Page 208"}
{"text": "Figure 20.2: Engine, Connection, ResultProxy API\nDealing with DBAPI Variability\nFor the task of managing variability in DBAPI behavior, first we'll consider the scope of the problem. The\nDBAPI specification, currently at version two, is written as a series of API definitions which allow for a\nwide degree of variability in behavior, and leave a good number of areas undefined. As a result, real-life\nDBAPIs exhibit a great degree of variability in several areas, including when Python unicode strings are\nacceptable and when they are not; how the \"last inserted id\"\u2014that is, an autogenerated primary key\u2014\nmay be acquired after an INSERT statement; and how bound parameter values may be specified and\ninterpreted. They also have a large number of idiosyncratic type-oriented behaviors, including the\nhandling of binary, precision numeric, date, Boolean, and unicode data.\nSQLAlchemy approaches this by allowing variability in both Dialect  and ExecutionContext  via\nmulti-level subclassing. Figure 20.3 illustrates the relationship between Dialect  and\nExecutionContext  when used with the psycopg2 dialect. The PGDialect  class provides behaviors\nthat are specific to the usage of the PostgreSQL database, such as the ARRAY datatype and schema\ncatalogs; the PGDialect_psycopg2  class then provides behaviors specific to the psycopg2 DBAPI,\nincluding unicode data handlers and server-side cursor behavior.\nFigure 20.3: Simple Dialect/ExecutionContext hierarchy\nA variant on the above pattern presents itself when dealing with a DBAPI that supports multiple\ndatabases. Examples of this include pyodbc, which deals with any number of database backends via\nODBC, and zxjdbc, a Jython-only driver which deals with JDBC. The above relationship is augmented by\n", "page": 209, "type": "text", "section": "Page 209"}
{"text": "the use of a mixin class from the sqlalchemy.connectors  package which provides DBAPI behavior\nthat is common to multiple backends. Figure 20.4 illustrates the common functionality of\nsqlalchemy.connectors.pyodbc  shared among pyodbc-specific dialects for MySQL and Microsoft\nSQL Server.\nFigure 20.4: Common DBAPI behavior shared among dialect hierarchies\nThe Dialect  and ExecutionContext  objects provide a means to define every interaction with the\ndatabase and DBAPI, including how connection arguments are formatted and how special quirks during\nstatement execution are handled. The Dialect  is also a factory for SQL compilation constructs that\nrender SQL correctly for the target database, and type objects which define how Python data should be\nmarshaled to and from the target DBAPI and database.\n20.4. Schema Definition\nWith database connectivity and interactivity established, the next task is to provide for the creation and\nmanipulation of backend-agnostic SQL statements. To achieve this, we need to define first how we will\nrefer to the tables and columns present in a database\u2014the so-called \"schema\". Tables and columns\nrepresent how data is organized, and most SQL statements consist of expressions and commands\nreferring to these structures.\nAn ORM or data access layer needs to provide programmatic access to the SQL language; at the base\nis a programmatic system of describing tables and columns. This is where SQLAlchemy offers the first\nstrong division of Core and ORM, by offering the Table  and Column  constructs that describe the\nstructure of the database independently of a user's model class definition. The rationale behind the\ndivision of schema definition from object relational mapping is that the relational schema can be designed\nunambiguously in terms of the relational database, including platform-specific details if necessary, without\nbeing muddled by object-relational concepts\u2014these remain a separate concern. Being independent of the\nORM component also means the schema description system is just as useful for any other kind of object-\nrelational system which may be built on the Core.\nThe Table  and Column  model falls under the scope of what's referred to as metadata, offering a\ncollection object called MetaData  to represent a collection of Table  objects. The structure is derived\nmostly from Martin Fowler's description of \"Metadata Mapping\" in Patterns of Enterprise Application\nArchitecture. Figure 20.5 illustrates some key elements of the sqlalchemy.schema  package.\n", "page": 210, "type": "text", "section": "Page 210"}
{"text": "Figure 20.5: Basic sqlalchemy.schema objects\nTable  represents the name and other attributes of an actual table present in a target schema. Its\ncollection of Column  objects represents naming and typing information about individual table columns. A\nfull array of objects describing constraints, indexes, and sequences is provided to fill in many more\ndetails, some of which impact the behavior of the engine and SQL construction system. In particular,\nForeignKeyConstraint  is central to determining how two tables should be joined.\nTable  and Column  in the schema package are unique versus the rest of the package in that they are\ndual-inheriting, both from the sqlalchemy.schema  package and the sqlalchemy.sql.expression\npackage, serving not just as schema-level constructs, but also as core syntactical units in the SQL\nexpression language. This relationship is illustrated in Figure 20.6.\nFigure 20.6: The dual lives of Table and Column\nIn Figure 20.6 we can see that Table  and Column  inherit from the SQL world as specific forms of\n\"things you can select from\", known as a FromClause , and \"things you can use in a SQL expression\",\nknown as a ColumnElement .\n20.5. SQL Expressions\nDuring SQLAlchemy's creation, the approach to SQL generation wasn't clear. A textual language might\nhave been a likely candidate; this is a common approach which is at the core of well-known object-\nrelational tools like Hibernate's HQL. For Python, however, a more intriguing choice was available: using\nPython objects and expressions to generatively construct expression tree structures, even re-purposing\nPython operators so that operators could be given SQL statement behavior.\nWhile it may not have been the first tool to do so, full credit goes to the SQLBuilder library included in\nIan Bicking's SQLObject as the inspiration for the system of Python objects and operators used by\nSQLAlchemy's expression language. In this approach, Python objects represent lexical portions of a SQL\nexpression. Methods on those objects, as well as overloaded operators, generate new lexical constructs\nderived from them. The most common object is the \"Column\" object\u2014SQLObject would represent these\non an ORM-mapped class using a namespace accessed via the .q  attribute; SQLAlchemy named the\nattribute .c . The .c  attribute remains today on Core selectable elements, such as those representing\ntables and select statements.\nExpression Trees\nA SQLAlchemy SQL expression construct is very much the kind of structure you'd create if you were\n", "page": 211, "type": "text", "section": "Page 211"}
{"text": "parsing a SQL statement\u2014it's a parse tree, except the developer creates the parse tree directly, rather\nthan deriving it from a string. The core type of node in this parse tree is called ClauseElement , and\nFigure 20.7 illustrates the relationship of ClauseElement  to some key classes.\nFigure 20.7: Basic expression hierarchy\nThrough the use of constructor functions, methods, and overloaded Python operator functions, a structure\nfor a statement like:\nSELECT id FROM user WHERE name = ?\nmight be constructed in Python like:\nfrom sqlalchemy.sql import table, column, select\nuser = table('user', column('id'), column('name'))\nstmt = select([user.c.id]).where(user.c.name=='ed')\nThe structure of the above select  construct is shown in Figure 20.8. Note the representation of the\nliteral value 'ed'  is contained within the _BindParam  construct, thus causing it to be rendered as a\nbound parameter marker in the SQL string using a question mark.\nFigure 20.8: Example expression tree\nFrom the tree diagram, one can see that a simple descending traversal through the nodes can quickly\ncreate a rendered SQL statement, as we'll see in greater detail in the section on statement compilation.\nPython Operator Approach\nIn SQLAlchemy, an expression like this:\ncolumn('a') == 2\nproduces neither True  nor False , but instead a SQL expression construct. The key to this is to\noverload operators using the Python special operator functions: e.g., methods like __eq__ , __ne__ ,\n__le__ , __lt__ , __add__ , __mul__ . Column-oriented expression nodes provide overloaded\nPython operator behavior through the usage of a mixin called ColumnOperators . Using operator\noverloading, an expression column('a') == 2  is equivalent to:\n", "page": 212, "type": "text", "section": "Page 212"}
{"text": "from sqlalchemy.sql.expression import _BinaryExpression\nfrom sqlalchemy.sql import column, bindparam\nfrom sqlalchemy.operators import eq\n_BinaryExpression(\n    left=column('a'),\n    right=bindparam('a', value=2, unique=True),\n    operator=eq\n)\nThe eq  construct is actually a function originating from the Python operator  built-in. Representing\noperators as an object (i.e., operator.eq ) rather than a string (i.e., = ) allows the string\nrepresentation to be defined at statement compilation time, when database dialect information is known.\nCompilation\nThe central class responsible for rendering SQL expression trees into textual SQL is the Compiled\nclass. This class has two primary subclasses, SQLCompiler  and DDLCompiler . SQLCompiler\nhandles SQL rendering operations for SELECT, INSERT, UPDATE, and DELETE statements, collectively\nclassified as DQL (data query language) and DML (data manipulation language), while DDLCompiler\nhandles various CREATE and DROP statements, classified as DDL (data definition language). There is\nan additional class hierarchy focused around string representations of types, starting at TypeCompiler .\nIndividual dialects then provide their own subclasses of all three compiler types to define SQL language\naspects specific to the target database. Figure 20.9 provides an overview of this class hierarchy with\nrespect to the PostgreSQL dialect.\nFigure 20.9: Compiler hierarchy, including PostgreSQL-specific implementation\nThe Compiled  subclasses define a series of visit methods, each one referred to by a particular\nsubclass of ClauseElement . A hierarchy of ClauseElement  nodes is walked and a statement is\nconstructed by recursively concatenating the string output of each visit function. As this proceeds, the\nCompiled  object maintains state regarding anonymous identifier names, bound parameter names, and\nnesting of subqueries, among other things, all of which aim for the production of a string SQL statement\nas well as a final collection of bound parameters with default values. Figure 20.10 illustrates the process\nof visit methods resulting in textual units.\nFigure 20.10: Call hierarchy of a statement compilation\nA completed Compiled  structure contains the full SQL string and collection of bound values. These are\ncoerced by an ExecutionContext  into the format expected by the DBAPI's execute  method, which\nincludes such considerations as the treatment of a unicode statement object, the type of collection used\nto store bound values, as well as specifics on how the bound values themselves should be coerced into\nrepresentations appropriate to the DBAPI and target database.\n20.6. Class Mapping with the ORM\nWe now shift our attention to the ORM. The first goal is to use the system of table metadata we've\n", "page": 213, "type": "text", "section": "Page 213"}
{"text": "defined to allow mapping of a user-defined class to a collection of columns in a database table. The\nsecond goal is to allow the definition of relationships between user-defined classes, based on\nrelationships between tables in a database.\nSQLAlchemy refers to this as \"mapping\", following the well known Data Mapper pattern described in\nFowler's Patterns of Enterprise Architecture. Overall, the SQLAlchemy ORM draws heavily from the\npractices detailed by Fowler. It's also heavily influenced by the famous Java relational mapper Hibernate\nand Ian Bicking's SQLObject product for Python.\nClassical vs. Declarative\nWe use the term classical mapping to refer to SQLAlchemy's system of applying an object-relational data\nmapping to an existing user class. This form considers the Table  object and the user-defined class to\nbe two individually defined entities which are joined together via a function called mapper . Once\nmapper  has been applied to a user-defined class, the class takes on new attributes that correspond to\ncolumns in the table:\nclass User(object):\n    pass\nmapper(User, user_table)\n# now User has an \".id\" attribute\nUser.id\nmapper  can also affix other kinds of attributes to the class, including attributes which correspond to\nreferences to other kinds of objects, as well as arbitrary SQL expressions. The process of affixing\narbitrary attributes to a class is known in the Python world as \"monkeypatching\"; however, since we are\ndoing it in a data-driven and non-arbitrary way, the spirit of the operation is better expressed with the\nterm class instrumentation.\nModern usage of SQLAlchemy centers around the Declarative extension, which is a configurational\nsystem that resembles the common active-record-like class declaration system used by many other\nobject-relational tools. In this system, the end user explicitly defines attributes inline with the class\ndefinition, each representing an attribute on the class that is to be mapped. The Table  object, in most\ncases, is not mentioned explicitly, nor is the mapper  function; only the class, the Column  objects, and\nother ORM-related attributes are named:\nclass User(Base):\n    __tablename__ = 'user'\n    id = Column(Integer, primary_key=True)\nIt may appear, above, that the class instrumentation is being achieved directly by our placement of id =\nColumn() , but this is not the case. The Declarative extension uses a Python metaclass, which is a\nhandy way to run a series of operations each time a new class is first declared, to generate a new\nTable  object from what's been declared, and to pass it to the mapper  function along with the class.\nThe mapper  function then does its job in exactly the same way, patching its own attributes onto the\nclass, in this case towards the id  attribute, and replacing what was there previously. By the time the\nmetaclass initialization is complete (that is, when the flow of execution leaves the block delineated by\nUser ), the Column  object marked by id  has been moved into a new Table , and User.id  has\nbeen replaced by a new attribute specific to the mapping.\nIt was always intended that SQLAlchemy would have a shorthand, declarative form of configuration.\nHowever, the creation of Declarative was delayed in favor of continued work solidifying the mechanics of\nclassical mapping. An interim extension called ActiveMapper, which later became the Elixir project,\nexisted early on. It redefines mapping constructs in a higher-level declaration system. Declarative's goal\nwas to reverse the direction of Elixir's heavily abstracted approach by establishing a system that\npreserved SQLAlchemy classical mapping concepts almost exactly, only reorganizing how they are used\nto be less verbose and more amenable to class-level extensions than a classical mapping would be.\nWhether classical or declarative mapping is used, a mapped class takes on new behaviors that allow it to\nexpress SQL constructs in terms of its attributes. SQLAlchemy originally followed SQLObject's behavior\nof using a special attribute as the source of SQL column expressions, referred to by SQLAlchemy as\n.c , as in this example:\nresult = session.query(User).filter(User.c.username == 'ed').all()\nIn version 0.4, however, SQLAlchemy moved the functionality into the mapped attributes themselves:\nresult = session.query(User).filter(User.username == 'ed').all()\nThis change in attribute access proved to be a great improvement, as it allowed the column-like objects\npresent on the class to gain additional class-specific capabilities not present on those originating directly\n", "page": 214, "type": "text", "section": "Page 214"}
{"text": "from the underlying Table  object. It also allowed usage integration between different kinds of class\nattributes, such as attributes which refer to table columns directly, attributes that refer to SQL expressions\nderived from those columns, and attributes that refer to a related class. Finally, it provided a symmetry\nbetween a mapped class, and an instance of that mapped class, in that the same attribute could take on\ndifferent behavior depending on the type of parent. Class-bound attributes return SQL expressions while\ninstance-bound attributes return actual data.\nAnatomy of a Mapping\nThe id  attribute that's been attached to our User  class is a type of object known in Python as a\ndescriptor, an object that has __get__ , __set__ , and __del__  methods, which the Python runtime\ndefers to for all class and instance operations involving this attribute. SQLAlchemy's implementation is\nknown as an InstrumentedAttribute , and we'll illustrate the world behind this facade with another\nexample. Starting with a Table  and a user defined class, we set up a mapping that has just one\nmapped column, as well as a relationship , which defines a reference to a related class:\nuser_table = Table(\"user\", metadata,\n    Column('id', Integer, primary_key=True),\n)\nclass User(object):\n    pass\nmapper(User, user_table, properties={\n    'related':relationship(Address)\n})\nWhen the mapping is complete, the structure of objects related to the class is detailed in Figure 20.11.\nFigure 20.11: Anatomy of a mapping\nThe figure illustrates a SQLAlchemy mapping defined as two separate layers of interaction between the\nuser-defined class and the table metadata to which it is mapped. Class instrumentation is pictured\ntowards the left, while SQL and database functionality is pictured towards the right. The general pattern\nat play is that object composition is used to isolate behavioral roles, and object inheritance is used to\ndistinguish amongst behavioral variances within a particular role.\nWithin the realm of class instrumentation, the ClassManager  is linked to the mapped class, while its\ncollection of InstrumentedAttribute  objects are linked to each attribute mapped on the class.\nInstrumentedAttribute  is also the public-facing Python descriptor mentioned previously, and\nproduces SQL expressions when used in a class-based expression (e.g., User.id==5 ). When dealing\nwith an instance of User , InstrumentedAttribute  delegates the behavior of the attribute to an\nAttributeImpl  object, which is one of several varieties tailored towards the type of data being\nrepresented.\nTowards the mapping side, the Mapper  represents the linkage of a user-defined class and a selectable\nunit, most typically Table . Mapper  maintains a collection of per-attribute objects known as\nMapperProperty , which deals with the SQL representation of a particular attribute. The most common\nvariants of MapperProperty  are ColumnProperty , representing a mapped column or SQL\nexpression, and RelationshipProperty , representing a linkage to another mapper.\nMapperProperty  delegates attribute loading behavior\u2014including how the attribute renders in a SQL\n", "page": 215, "type": "text", "section": "Page 215"}
{"text": "statement and how it is populated from a result row\u2014to a LoaderStrategy  object, of which there are\nseveral varieties. Different LoaderStrategies  determine if the loading behavior of an attribute is\ndeferred, eager, or immediate. A default version is chosen at mapper configuration time, with the option\nto use an alternate strategy at query time. RelationshipProperty  also references a\nDependencyProcessor , which handles how inter-mapper dependencies and attribute synchronization\nshould proceed at flush time. The choice of DependencyProcessor  is based on the relational geometry\nof the parent and target selectables linked to the relationship.\nThe Mapper / RelationshipProperty  structure forms a graph, where Mapper  objects are nodes and\nRelationshipProperty  objects are directed edges. Once the full set of mappers have been declared\nby an application, a deferred \"initialization\" step known as the configuration proceeds. It is used mainly\nby each RelationshipProperty  to solidify the details between its parent and target mappers,\nincluding choice of AttributeImpl  as well as DependencyProcessor . This graph is a key data\nstructure used throughout the operation of the ORM. It participates in operations such as the so-called\n\"cascade\" behavior that defines how operations should propagate along object paths, in query operations\nwhere related objects and collections are \"eagerly\" loaded at once, as well as on the object flushing side\nwhere a dependency graph of all objects is established before firing off a series of persistence steps.\n20.7. Query and Loading Behavior\nSQLAlchemy initiates all object loading behavior via an object called Query . The basic state Query\nstarts with includes the entities, which is the list of mapped classes and/or individual SQL expressions to\nbe queried. It also has a reference to the Session , which represents connectivity to one or more\ndatabases, as well as a cache of data that's been accumulated with respect to transactions on those\nconnections. Below is a rudimentary usage example:\nfrom sqlalchemy.orm import Session\nsession = Session(engine)\nquery = session.query(User)\nWe create a Query  that will yield instances of User , relative to a new Session  we've created.\nQuery  provides a generative builder pattern in the same way as the select  construct discussed\npreviously, where additional criteria and modifiers are associated with a statement construct one method\ncall at a time. When an iterative operation is called on the Query , it constructs a SQL expression\nconstruct representing a SELECT, emits it to the database, and then interprets the result set rows as\nORM-oriented results corresponding to the initial set of entities being requested.\nQuery  makes a hard distinction between the SQL rendering and the data loading portions of the\noperation. The former refers to the construction of a SELECT statement, the latter to the interpretation of\nSQL result rows into ORM-mapped constructs. Data loading can, in fact, proceed without a SQL\nrendering step, as the Query  may be asked to interpret results from a textual query hand-composed by\nthe user.\nBoth SQL rendering and data loading utilize a recursive descent through the graph formed by the series\nof lead Mapper  objects, considering each column- or SQL-expression-holding ColumnProperty  as a\nleaf node and each RelationshipProperty  which is to be included in the query via a so-called\n\"eager-load\" as an edge leading to another Mapper  node. The traversal and action to take at each node\nis ultimately the job of each LoaderStrategy  associated with every MapperProperty , adding\ncolumns and joins to the SELECT statement being built in the SQL rendering phase, and producing\nPython functions that process result rows in the data loading phase.\nThe Python functions produced in the data loading phase each receive a database row as they are\nfetched, and produce a possible change in the state of a mapped attribute in memory as a result. They\nare produced for a particular attribute conditionally, based on examination of the first incoming row in the\nresult set, as well as on loading options. If a load of the attribute is not to proceed, no callable function is\nproduced.\nFigure 20.12 illustrates the traversal of several LoaderStrategy  objects in a joined eager loading\nscenario, illustrating their connection to a rendered SQL statement which occurs during the\n_compile_context  method of Query . It also shows generation of row population functions which\nreceive result rows and populate individual object attributes, a process which occurs within the\ninstances  method of Query .\n", "page": 216, "type": "text", "section": "Page 216"}
{"text": "Figure 20.12: Traversal of loader strategies including a joined eager load\nSQLAlchemy's early approach to populating results used a traditional traversal of fixed object methods\nassociated with each strategy to receive each row and act accordingly. The loader callable system, first\nintroduced in version 0.5, represented a dramatic leap in performance, as many decisions regarding row\nhandling could be made just once up front instead of for each row, and a significant number of function\ncalls with no net effect could be eliminated.\n20.8. Session/Identity Map\nIn SQLAlchemy, the Session  object presents the public interface for the actual usage of the ORM\u2014\nthat is, loading and persisting data. It provides the starting point for queries and persistence operations\nfor a given database connection.\nThe Session , in addition to serving as the gateway for database connectivity, maintains an active\nreference to the set of all mapped entities which are present in memory relative to that Session . It's in\nthis way that the Session  implements a facade for the identity map and unit of work patterns, both\nidentified by Fowler. The identity map maintains a database-identity-unique mapping of all objects for a\nparticular Session , eliminating the problems introduced by duplicate identities. The unit of work builds\non the identity map to provide a system of automating the process of persisting all changes in state to\nthe database in the most effective manner possible. The actual persistence step is known as a \"flush\",\nand in modern SQLAlchemy this step is usually automatic.\nDevelopment History\nThe Session  started out as a mostly concealed system responsible for the single task of emitting a\nflush. The flush process involves emitting SQL statements to the database, corresponding to changes in\nthe state of objects tracked by the unit of work system and thereby synchronizing the current state of the\ndatabase with what's in memory. The flush has always been one of the most complex operations\nperformed by SQLAlchemy.\nThe invocation of flush started out in very early versions behind a method called commit , and it was a\nmethod present on an implicit, thread-local object called objectstore . When one used SQLAlchemy\n0.1, there was no need to call Session.add , nor was there any concept of an explicit Session  at all.\nThe only user-facing steps were to create mappers, create new objects, modify existing objects loaded\nthrough queries (where the queries themselves were invoked directly from each Mapper  object), and\nthen persist all changes via the objectstore.commit  command. The pool of objects for a set of\noperations was unconditionally module-global and unconditionally thread-local.\nThe objectstore.commit  model was an immediate hit with the first group of users, but the rigidity of\nthis model quickly ran into a wall. Users new to modern SQLAlchemy sometimes lament the need to\ndefine a factory, and possibly a registry, for Session  objects, as well as the need to keep their objects\norganized into just one Session  at a time, but this is far preferable to the early days when the entire\nsystem was completely implicit. The convenience of the 0.1 usage pattern is still largely present in\nmodern SQLAlchemy, which features a session registry normally configured to use thread local scoping.\nSession\n", "page": 217, "type": "text", "section": "Page 217"}
{"text": "The \n itself was only introduced in version 0.2 of SQLAlchemy, modeled loosely after the\nSession  object present in Hibernate. This version featured integrated transactional control, where the\nSession  could be placed into a transaction via the begin  method, and completed via the commit\nmethod. The objectstore.commit  method was renamed to objectstore.flush , and new\nSession  objects could be created at any time. The Session  itself was broken off from another object\ncalled UnitOfWork , which remains as a private object responsible for executing the actual flush\noperation.\nWhile the flush process started as a method explicitly invoked by the user, the 0.4 series of SQLAlchemy\nintroduced the concept of autoflush, which meant that a flush was emitted immediately before each\nquery. The advantage of autoflush is that the SQL statement emitted by a query always has access on\nthe relational side to the exact state that is present in memory, as all changes have been sent over.\nEarly versions of SQLAlchemy couldn't include this feature, because the most common pattern of usage\nwas that the flush statement would also commit the changes permanently. But when autoflush was\nintroduced, it was accompanied by another feature called the transactional Session , which provided a\nSession  that would start out automatically in a transaction that remained until the user called commit\nexplicitly. With the introduction of this feature, the flush  method no longer committed the data that it\nflushed, and could safely be called on an automated basis. The Session  could now provide a step-by-\nstep synchronization between in-memory state and SQL query state by flushing as needed, with nothing\npermanently persisted until the explicit commit  step. This behavior is, in fact, exactly the same in\nHibernate for Java. However, SQLAlchemy embraced this style of usage based on the same behavior in\nthe Storm ORM for Python, introduced when SQLAlchemy was in version 0.3.\nVersion 0.5 brought more transaction integration when post-transaction expiration was introduced; after\neach commit  or rollback , by default all states within the Session  are expired (erased), to be\npopulated again when subsequent SQL statements re-select the data, or when the attributes on the\nremaining set of expired objects are accessed in the context of the new transaction. Originally,\nSQLAlchemy was constructed around the assumption that SELECT statements should be emitted as little\nas possible, unconditionally. The expire-on-commit behavior was slow in coming for this reason;\nhowever, it entirely solved the issue of the Session  which contained stale data post-transaction with no\nsimple way to load newer data without rebuilding the full set of objects already loaded. Early on, it\nseemed that this problem couldn't be reasonably solved, as it wasn't apparent when the Session\nshould consider the current state to be stale, and thus produce an expensive new set of SELECT\nstatements on the next access. However, once the Session  moved to an always-in-a-transaction\nmodel, the point of transaction end became apparent as the natural point of data expiration, as the\nnature of a transaction with a high degree of isolation is that it cannot see new data until it's committed or\nrolled back anyway. Different databases and configurations, of course, have varied degrees of transaction\nisolation, including no transactions at all. These modes of usage are entirely acceptable with\nSQLAlchemy's expiration model; the developer only needs to be aware that a lower isolation level may\nexpose un-isolated changes within a Session if multiple Sessions share the same rows. This is not at all\ndifferent from what can occur when using two database connections directly.\nSession Overview\nFigure 20.13 illustrates a Session  and the primary structures it deals with.\nFigure 20.13: Session overview\nThe public-facing portions above are the Session  itself and the collection of user objects, each of\nwhich is an instance of a mapped class. Here we see that mapped objects keep a reference to a\nSQLAlchemy construct called InstanceState , which tracks ORM state for an individual instance\nincluding pending attribute changes and attribute expiration status. InstanceState  is the instance-level\nside of the attribute instrumentation discussed in the preceding section, Anatomy of a Mapping,\ncorresponding to the ClassManager  at the class level, and maintaining the state of the mapped object's\ndictionary (i.e., the Python __dict__  attribute) on behalf of the AttributeImpl  objects associated\nwith the class.\nState Tracking\nThe IdentityMap  is a mapping of database identities to InstanceState  objects, for those objects\n", "page": 218, "type": "text", "section": "Page 218"}
{"text": "which have a database identity, which are referred to as persistent. The default implementation of\nIdentityMap  works with InstanceState  to self-manage its size by removing user-mapped instances\nonce all strong references to them have been removed\u2014in this way it works in the same way as\nPython's WeakValueDictionary . The Session  protects the set of all objects marked as dirty or\ndeleted, as well as pending objects marked new, from garbage collection, by creating strong references\nto those objects with pending changes. All strong references are then discarded after the flush.\nInstanceState  also performs the critical task of maintaining \"what's changed\" for the attributes of a\nparticular object, using a move-on-change system that stores the \"previous\" value of a particular attribute\nin a dictionary called committed_state  before assigning the incoming value to the object's current\ndictionary. At flush time, the contents of committed_state  and the __dict__  associated with the\nobject are compared to produce the set of net changes on each object.\nIn the case of collections, a separate collections  package coordinates with the\nInstrumentedAttribute / InstanceState  system to maintain a collection of net changes to a\nparticular mapped collection of objects. Common Python classes such as set , list  and dict  are\nsubclassed before use and augmented with history-tracking mutator methods. The collection system was\nreworked in 0.4 to be open ended and usable for any collection-like object.\nTransactional Control\nSession , in its default state of usage, maintains an open transaction for all operations which is\ncompleted when commit  or rollback  is called. The SessionTransaction  maintains a set of zero\nor more Connection  objects, each representing an open transaction on a particular database.\nSessionTransaction  is a lazy-initializing object that begins with no database state present. As a\nparticular backend is required to participate in a statement execution, a Connection  corresponding to\nthat database is added to SessionTransaction 's list of connections. While a single connection at a\ntime is common, the multiple connection scenario is supported where the specific connection used for a\nparticular operation is determined based on configurations associated with the Table , Mapper , or\nSQL construct itself involved in the operation. Multiple connections can also coordinate the transaction\nusing two-phase behavior, for those DBAPIs which provide it.\n20.9. Unit of Work\nThe flush  method provided by Session  turns over its work to a separate module called\nunitofwork . As mentioned earlier, the flush process is probably the most complex function of\nSQLAlchemy.\nThe job of the unit of work is to move all of the pending state present in a particular Session  out to the\ndatabase, emptying out the new , dirty , and deleted  collections maintained by the Session .\nOnce completed, the in-memory state of the Session  and what's present in the current transaction\nmatch. The primary challenge is to determine the correct series of persistence steps, and then to perform\nthem in the correct order. This includes determining the list of INSERT, UPDATE, and DELETE\nstatements, including those resulting from the cascade of a related row being deleted or otherwise\nmoved; ensuring that UPDATE statements contain only those columns which were actually modified;\nestablishing \"synchronization\" operations that will copy the state of primary key columns over to\nreferencing foreign key columns, at the point at which newly generated primary key identifiers are\navailable; ensuring that INSERTs occur in the order in which objects were added to the Session  and\nas efficiently as possible; and ensuring that UPDATE and DELETE statements occur within a\ndeterministic ordering so as to reduce the chance of deadlocks.\nHistory\nThe unit of work implementation began as a tangled system of structures that was written in an ad hoc\nway; its development can be compared to finding the way out of a forest without a map. Early bugs and\nmissing behaviors were solved with bolted-on fixes, and while several refactorings improved matters\nthrough version 0.5, it was not until version 0.6 that the unit of work\u2014by that time stable, well-\nunderstood, and covered by hundreds of tests\u2014could be rewritten entirely from scratch. After many\nweeks of considering a new approach that would be driven by consistent data structures, the process of\nrewriting it to use this new model took only a few days, as the idea was by this time well understood. It\nwas also greatly helped by the fact that the new implementation's behavior could be carefully cross-\nchecked against the existing version. This process shows how the first iteration of something, however\nawful, is still valuable as long as it provides a working model. It further shows how total rewrites of a\nsubsystem is often not only appropriate, but an integral part of development for hard-to-develop systems.\nTopological Sort\nThe key paradigm behind the unit of work is that of assembling the full list of actions to be taken into a\ndata structure, with each node representing a single step; this is known in design patterns parlance as\nthe command pattern. The series of \"commands\" within this structure is then organized into a specific\nordering using a topological sort. A topological sort is a process that sorts items based on a partial\nordering, that is, only certain elements must precede others. Figure 20.14 illustrates the behavior of the\ntopological sort.\n", "page": 219, "type": "text", "section": "Page 219"}
{"text": "Figure 20.14: Topological sort\nThe unit of work constructs a partial ordering based on those persistence commands which must precede\nothers. The commands are then topologically sorted and invoked in order. The determination of which\ncommands precede which is derived primarily from the presence of a relationship  that bridges two\nMapper  objects\u2014generally, one Mapper  is considered to be dependent on the other, as the\nrelationship  implies that one Mapper  has a foreign key dependency on the other. Similar rules\nexist for many-to-many association tables, but here we focus on the case of one-to-many/many-to-one\nrelationships. Foreign key dependencies are resolved in order to prevent constraint violations from\noccurring, with no reliance on needing to mark constraints as \"deferred\". But just as importantly, the\nordering allows primary key identifiers, which on many platforms are only generated when an INSERT\nactually occurs, to be populated from a just-executed INSERT statement's result into the parameter list of\na dependent row that's about to be inserted. For deletes, the same ordering is used in reverse\u2014\ndependent rows are deleted before those on which they depend, as these rows cannot be present\nwithout the referent of their foreign key being present.\nThe unit of work features a system where the topological sort is performed at two different levels, based\non the structure of dependencies present. The first level organizes persistence steps into buckets based\non the dependencies between mappers, that is, full \"buckets\" of objects corresponding to a particular\nclass. The second level breaks up zero or more of these \"buckets\" into smaller batches, to handle the\ncase of reference cycles or self-referring tables. Figure 20.15 illustrates the \"buckets\" generated to insert\na set of User  objects, then a set of Address  objects, where an intermediary step copies newly\ngenerated User  primary key values into the user_id  foreign key column of each Address  object.\nFigure 20.15: Organizing objects by mapper\nIn the per-mapper sorting situation, any number of User  and Address  objects can be flushed with no\n", "page": 220, "type": "text", "section": "Page 220"}
{"text": "impact on the complexity of steps or how many \"dependencies\" must be considered.\nThe second level of sorting organizes persistence steps based on direct dependencies between individual\nobjects within the scope of a single mapper. The simplest example of when this occurs is a table which\ncontains a foreign key constraint to itself; a particular row in the table needs to be inserted before\nanother row in the same table which refers to it. Another is when a series of tables have a reference\ncycle: table A references table B, which references table C, that then references table A. Some A objects\nmust be inserted before others so as to allow the B and C objects to also be inserted. The table that\nrefers to itself is a special case of reference cycle.\nTo determine which operations can remain in their aggregated, per- Mapper  buckets, and which will be\nbroken into a larger set of per-object commands, a cycle detection algorithm is applied to the set of\ndependencies that exist between mappers, using a modified version of a cycle detection algorithm found\non Guido Van Rossum's blog. Those buckets involved in cycles are are then broken up into per-object\noperations and mixed into the collection of per-mapper buckets through the addition of new dependency\nrules from the per-object buckets back to the per-mapper buckets. Figure 20.16 illustrates the bucket of\nUser  objects being broken up into individual per-object commands, resulting from the addition of a new\nrelationship  from User  to itself called contact .\nFigure 20.16: Organizing reference cycles into individual steps\nThe rationale behind the bucket structure is that it allows batching of common statements as much as\npossible, both reducing the number of steps required in Python and making possible more efficient\ninteractions with the DBAPI, which can sometimes execute thousands of statements within a single\nPython method call. Only when a reference cycle exists between mappers does the more expensive per-\nobject-dependency pattern kick in, and even then it only occurs for those portions of the object graph\nwhich require it.\n20.10. Conclusion\nSQLAlchemy has aimed very high since its inception, with the goal of being the most feature-rich and\nversatile database product possible. It has done so while maintaining its focus on relational databases,\nrecognizing that supporting the usefulness of relational databases in a deep and comprehensive way is a\nmajor undertaking; and even now, the scope of the undertaking continues to reveal itself as larger than\npreviously perceived.\nThe component-based approach is intended to extract the most value possible from each area of\nfunctionality, providing many different units that applications can use alone or in combination. This system\nhas been challenging to create, maintain, and deliver.\nThe development course was intended to be slow, based on the theory that a methodical, broad-based\nconstruction of solid functionality is ultimately more valuable than fast delivery of features without\nfoundation. It has taken a long time for SQLAlchemy to construct a consistent and well-documented user\nstory, but throughout the process, the underlying architecture was always a step ahead, leading in some\ncases to the \"time machine\" effect where features can be added almost before users request them.\n", "page": 221, "type": "text", "section": "Page 221"}
{"text": "This work is made available under the Creative Commons Attribution 3.0 Unported license. Please see\nthe full description of the license for details.\nThe Python language has been a reliable host (if a little finicky, particularly in the area of performance).\nThe language's consistency and tremendously open run-time model has allowed SQLAlchemy to provide\na nicer experience than that offered by similar products written in other languages.\nIt is the hope of the SQLAlchemy project that Python gain ever-deeper acceptance into as wide a variety\nof fields and industries as possible, and that the use of relational databases remains vibrant and\nprogressive. The goal of SQLAlchemy is to demonstrate that relational databases, Python, and well-\nconsidered object models are all very much worthwhile development tools.\nBack to top\nBack to The Architecture of Open Source Applications.\n", "page": 222, "type": "text", "section": "Page 222"}
{"text": "<>\nTwisted is an event-driven networking engine in Python. It was born in the early 2000s, when the writers\nof networked games had few scalable and no cross-platform libraries, in any language, at their disposal.\nThe authors of Twisted tried to develop games in the existing networking landscape, struggled, saw a\nclear need for a scalable, event-driven, cross-platform networking framework and decided to make one\nhappen, learning from the mistakes and hardships of past game and networked application writers.\nTwisted supports many common transport and application layer protocols, including TCP, UDP, SSL/TLS,\nHTTP, IMAP, SSH, IRC, and FTP. Like the language in which it is written, it is \"batteries-included\";\nTwisted comes with client and server implementations for all of its protocols, as well as utilities that make\nit easy to configure and deploy production-grade Twisted applications from the command line.\n21.1. Why Twisted?\nIn 2000, glyph, the creator of Twisted, was working on a text-based multiplayer game called Twisted\nReality. It was a big mess of threads, 3 per connection, in Java. There was a thread for input that would\nblock on reads, a thread for output that would block on some kind of write, and a \"logic\" thread that\nwould sleep while waiting for timers to expire or events to queue. As players moved through the virtual\nlandscape and interacted, threads were deadlocking, caches were getting corrupted, and the locking logic\nwas never quite right\u2014the use of threads had made the software complicated, buggy, and hard to scale.\nSeeking alternatives, he discovered Python, and in particular Python's select  module for multiplexing\nI/O from stream objects like sockets and pipes (the Single UNIX Specification, Version 3 (SUSv3)\ndescribes the select  API). At the time, Java didn't expose the operating system's select  interface\nor any other asynchronous I/O API (The java.nio  package for non-blocking I/O was added in J2SE\n1.4, released in 2002). A quick prototype of the game in Python using select  immediately proved less\ncomplex and more reliable than the threaded version.\nAn instant convert to Python, select , and event-driven programming, glyph wrote a client and server\nfor the game in Python using the select  API. But then he wanted to do more. Fundamentally, he\nwanted to be able to turn network activity into method calls on objects in the game. What if you could\nreceive email in the game, like the Nethack mailer daemon? What if every player in the game had a\nhome page? Glyph found himself needing good Python IMAP and HTTP clients and servers that used\nselect .\nHe first turned to Medusa, a platform developed in the mid-'90s for writing networking servers in Python\nbased on the asyncore  module. asyncore  is an asynchronous socket handler that builds a\ndispatcher and callback interface on top of the operating system's select  API.\nThis was an inspiring find for glyph, but Medusa had two drawbacks:\n1. It was on its way towards being unmaintained by 2001 when glyph started working on Twisted\nReality.\n2. asyncore  is such a thin wrapper around sockets that application programmers are still required to\nmanipulate sockets directly. This means portability is still the responsibility of the programmer.\nAdditionally, at the time, asyncore 's Windows support was buggy, and glyph knew that he wanted\nto run a GUI client on Windows.\nGlyph was facing the prospect of implementing a networking platform himself and realized that Twisted\nReality had opened the door to a problem that was just as interesting as his game.\nOver time, Twisted Reality the game became Twisted the networking platform, which would do what\nexisting networking platforms in Python didn't:\nUse event-driven programming instead of multi-threaded programming.\nBe cross-platform: provide a uniform interface to the event notification systems exposed by major\noperating systems.\nBe \"batteries-included\": provide implementations of popular application-layer protocols out of the box,\nso that Twisted is immediately useful to developers.\nTwisted\nJessica McKellar\n", "page": 223, "type": "text", "section": "Page 223"}
{"text": "Conform to RFCs, and prove conformance with a robust test suite.\nMake it easy to use multiple networking protocols together.\nBe extensible.\n21.2. The Architecture of Twisted\nTwisted is an event-driven networking engine. Event-driven programming is so integral to Twisted's\ndesign philosophy that it is worth taking a moment to review what exactly event-driven programming\nmeans.\nEvent-driven programming is a programming paradigm in which program flow is determined by external\nevents. It is characterized by an event loop and the use of callbacks to trigger actions when events\nhappen. Two other common programming paradigms are (single-threaded) synchronous and multi-\nthreaded programming.\nLet's compare and contrast single-threaded, multi-threaded, and event-driven programming models with\nan example. Figure 21.1 shows the work done by a program over time under these three models. The\nprogram has three tasks to complete, each of which blocks while waiting for I/O to finish. Time spent\nblocking on I/O is greyed out.\nFigure 21.1: Threading models\nIn the single-threaded synchronous version of the program, tasks are performed serially. If one task\nblocks for a while on I/O, all of the other tasks have to wait until it finishes and they are executed in turn.\nThis definite order and serial processing are easy to reason about, but the program is unnecessarily slow\nif the tasks don't depend on each other, yet still have to wait for each other.\nIn the threaded version of the program, the three tasks that block while doing work are performed in\nseparate threads of control. These threads are managed by the operating system and may run\nconcurrently on multiple processors or interleaved on a single processor. This allows progress to be\nmade by some threads while others are blocking on resources. This is often more time-efficient than the\nanalogous synchronous program, but one has to write code to protect shared resources that could be\naccessed concurrently from multiple threads. Multi-threaded programs can be harder to reason about\nbecause one now has to worry about thread safety via process serialization (locking), reentrancy, thread-\nlocal storage, or other mechanisms, which when implemented improperly can lead to subtle and painful\nbugs.\nThe event-driven version of the program interleaves the execution of the three tasks, but in a single\nthread of control. When performing I/O or other expensive operations, a callback is registered with an\nevent loop, and then execution continues while the I/O completes. The callback describes how to handle\nan event once it has completed. The event loop polls for events and dispatches them as they arrive, to\nthe callbacks that are waiting for them. This allows the program to make progress when it can without\n", "page": 224, "type": "text", "section": "Page 224"}
{"text": "the use of additional threads. Event-driven programs can be easier to reason about than multi-threaded\nprograms because the programmer doesn't have to worry about thread safety.\nThe event-driven model is often a good choice when there are:\n1. many tasks, that are\u2026\n2. largely independent (so they don't have to communicate with or wait on each other), and\u2026\n3. some of these tasks block while waiting on events.\nIt is also a good choice when an application has to share mutable data between tasks, because no\nsynchronization has to be performed.\nNetworking applications often have exactly these properties, which is what makes them such a good fit\nfor the event-driven programming model.\nReusing Existing Applications\nMany popular clients and servers for various networking protocols already existed when Twisted was\ncreated. Why did glyph not just use Apache, IRCd, BIND, OpenSSH, or any of the other pre-existing\napplications whose clients and servers would have to get re-implemented from scratch for Twisted?\nThe problem is that all of these server implementations have networking code written from scratch,\ntypically in C, with application code coupled directly to the networking layer. This makes them very\ndifficult to use as libraries. They have to be treated as black boxes when used together, giving a\ndeveloper no chance to reuse code if he or she wanted to expose the same data over multiple protocols.\nAdditionally, the server and client implementations are often separate applications that don't share code.\nExtending these applications and maintaining cross-platform client-server compatibility is harder than it\nneeds to be.\nWith Twisted, the clients and servers are written in Python using a consistent interface. This makes it is\neasy to write new clients and servers, to share code between clients and servers, to share application\nlogic between protocols, and to test one's code.\nThe Reactor\nTwisted implements the reactor design pattern, which describes demultiplexing and dispatching events\nfrom multiple sources to their handlers in a single-threaded environment.\nThe core of Twisted is the reactor event loop. The reactor knows about network, file system, and timer\nevents. It waits on and then handles these events, abstracting away platform-specific behavior and\npresenting interfaces to make responding to events anywhere in the network stack easy.\nThe reactor essentially accomplishes:\nwhile True:\n    timeout = time_until_next_timed_event()\n    events = wait_for_events(timeout)\n    events += timed_events_until(now())\n    for event in events:\n        event.process()\nA reactor based on the poll  API (decribed in the Single UNIX Specification, Version 3 (SUSv3)) is the\ncurrent default on all platforms. Twisted additionally supports a number of platform-specific high-volume\nmultiplexing APIs. Platform-specific reactors include the KQueue reactor based on FreeBSD's kqueue\nmechanism, an epoll -based reactor for systems supporting the epoll  interface (currently Linux 2.6),\nand an IOCP reactor based on Windows Input/Output Completion Ports.\nExamples of polling implementation-dependent details that Twisted takes care of include:\nNetwork and filesystem limits.\nBuffering behavior.\nHow to detect a dropped connection.\nThe values returned in error cases.\nTwisted's reactor implementation also takes care of using the underlying non-blocking APIs correctly and\nhandling obscure edge cases correctly. Python doesn't expose the IOCP API at all, so Twisted maintains\nits own implementation.\nManaging Callback Chains\nCallbacks are a fundamental part of event-driven programming and are the way that the reactor indicates\nto an application that events have completed. As event-driven programs grow, handling both the success\nand error cases for the events in one's application becomes increasingly complex. Failing to register an\nappropriate callback can leave a program blocking on event processing that will never happen, and errors\nmight have to propagate up a chain of callbacks from the networking stack through the layers of an\napplication.\nLet's examine some of the pitfalls of event-driven programs by comparing synchronous and asynchronous\nversions of a toy URL fetching utility in Python-like pseudo-code:\n", "page": 225, "type": "text", "section": "Page 225"}
{"text": "Synchronous URL fetcher:\nimport getPage\ndef processPage(page):\n    print page\n    finishProcessing()\ndef logError(error):\n    print error\n    finishProcessing()\ndef finishProcessing(value):\n    print \"Shutting down...\"\n    exit(0)\nurl = \"http://google.com\"\ntry:\n    page = getPage(url)\n    processPage(page)\nexcept Error, e:\n    logError(error)\nfinally:\n    finishProcessing()\nAsynchronous URL fetcher:\nfrom twisted.internet import reactor\nimport getPage\ndef processPage(page):\n    print page\n    finishProcessing()\ndef logError(error):\n    print error\n    finishProcessing()\ndef finishProcessing(value):\n    print \"Shutting down...\"\n    reactor.stop()\nurl = \"http://google.com\"\n# getPage takes: url, \n#    success callback, error callback\ngetPage(url, processPage, logError)\nreactor.run()\nIn the asynchronous URL fetcher, reactor.run()  starts the reactor event loop. In both the\nsynchronous and asynchronous versions, a hypothetical getPage  function does the work of page\nretrieval. processPage  is invoked if the retrieval is successful, and logError  is invoked if an\nException  is raised while attempting to retrieve the page. In either case, finishProcessing  is\ncalled afterwards.\nThe callback to logError  in the asynchronous version mirrors the except  part of the try/except\nblock in the synchronous version. The callback to processPage  mirrors else , and the unconditional\ncallback to finishProcessing  mirrors finally .\nIn the synchronous version, by virtue of the structure of a try/except  block exactly one of logError\nand processPage  is called, and finishProcessing  is always called once; in the asynchronous\nversion it is the programmer's responsibility to invoke the correct chain of success and error callbacks. If,\nthrough programming error, the call to finishProcessing  were left out of processPage  or\nlogError  along their respective callback chains, the reactor would never get stopped and the program\nwould run forever.\nThis toy example hints at the complexity frustrating programmers during the first few years of Twisted's\ndevelopment. Twisted responded to this complexity by growing an object called a Deferred .\nDeferreds\nThe Deferred  object is an abstraction of the idea of a result that doesn't exist yet. It also helps manage\nthe callback chains for this result. When returned by a function, a Deferred  is a promise that the\nfunction will have a result at some point. That single returned Deferred  contains references to all of\nthe callbacks registered for an event, so only this one object needs to be passed between functions,\nwhich is much simpler to keep track of than managing callbacks individually.\nDeferred s have a pair of callback chains, one for success (callbacks) and one for errors (errbacks).\nDeferred s start out with two empty chains. One adds pairs of callbacks and errbacks to handle\nsuccesses and failures at each point in the event processing. When an asynchronous result arrives, the\nDeferred  is \"fired\" and the appropriate callbacks or errbacks are invoked in the order in which they\nwere added.\nHere is a version of the asynchronous URL fetcher pseudo-code which uses Deferred s:\nfrom twisted.internet import reactor\nimport getPage\ndef processPage(page):\n    print page\ndef logError(error):\n    print error\ndef finishProcessing(value):\n    print \"Shutting down...\"\n", "page": 226, "type": "text", "section": "Page 226"}
{"text": "    reactor.stop()\nurl = \"http://google.com\"\ndeferred = getPage(url) # getPage returns a Deferred\ndeferred.addCallbacks(success, failure)\ndeferred.addBoth(stop)\nreactor.run()\nIn this version, the same event handlers are invoked, but they are all registered with a single Deferred\nobject instead of spread out in the code and passed as arguments to getPage .\nThe Deferred  is created with two stages of callbacks. First, addCallbacks  adds the processPage\ncallback and logError  errback to the first stage of their respective chains. Then addBoth  adds\nfinishProcessing  to the second stage of both chains. Diagrammatically, the callback chains look like\nFigure 21.2.\nFigure 21.2: Callback chains\nDeferred s can only be fired once; attempting to re-fire them will raise an Exception . This gives\nDeferred s semantics closer to those of the try/except  blocks of their synchronous cousins, which\nmakes processing the asynchronous events easier to reason about and avoids subtle bugs caused by\ncallbacks being invoked more or less than once for a single event.\nUnderstanding Deferred s is an important part of understanding the flow of Twisted programs.\nHowever, when using the high-level abstractions Twisted provides for networking protocols, one often\ndoesn't have to use Deferred s directly at all.\nThe Deferred  abstraction is powerful and has been borrowed by many other event-driven platforms,\nincluding jQuery, Dojo, and Mochikit.\nTransports\nTransports represent the connection between two endpoints communicating over a network. Transports\nare responsible for describing connection details, like being stream- or datagram-oriented, flow control,\nand reliability. TCP, UDP, and Unix sockets are examples of transports. They are designed to be\n\"minimally functional units that are maximally reusable\" and are decoupled from protocol implementations,\nallowing for many protocols to utilize the same type of transport. Transports implement the ITransport\ninterface, which has the following methods:\nwrite\nWrite some data to the physical connection, in sequence, in a non-blocking\nfashion.\nwriteSequence\nWrite a list of strings to the physical connection.\nlosesConnection\nWrite all pending data and then close the connection.\ngetPeer\nGet the remote address of this connection.\n", "page": 227, "type": "text", "section": "Page 227"}
{"text": "getHost\nGet the address of this side of the connection.\nDecoupling transports from procotols also makes testing the two layers easier. A mock transport can\nsimply write data to a string for inspection.\nProtocols\nProcotols describe how to process network events asynchronously. HTTP, DNS, and IMAP are examples\nof application protocols. Protocols implement the IProtocol  interface, which has the following\nmethods:\nmakeConnection\nMake a connection to a transport and a server.\nconnectionMade\nCalled when a connection is made.\ndataReceived\nCalled whenever data is received.\nconnectionLost\nCalled when the connection is shut down.\nThe relationship between the reactor, protocols, and transports is best illustrated with an example. Here\nare complete implementations of an echo server and client, first the server:\nfrom twisted.internet import protocol, reactor\nclass Echo(protocol.Protocol):\n    def dataReceived(self, data):\n        # As soon as any data is received, write it back\n        self.transport.write(data)\nclass EchoFactory(protocol.Factory):\n    def buildProtocol(self, addr):\n        return Echo()\nreactor.listenTCP(8000, EchoFactory())\nreactor.run()\nAnd the client:\nfrom twisted.internet import reactor, protocol\nclass EchoClient(protocol.Protocol):\n   def connectionMade(self):\n       self.transport.write(\"hello, world!\")\n   def dataReceived(self, data):\n       print \"Server said:\", data\n       self.transport.loseConnection()\n   def connectionLost(self, reason):\n       print \"connection lost\"\nclass EchoFactory(protocol.ClientFactory):\n   def buildProtocol(self, addr):\n       return EchoClient()\n   def clientConnectionFailed(self, connector, reason):\n       print \"Connection failed - goodbye!\"\n       reactor.stop()\n   def clientConnectionLost(self, connector, reason):\n       print \"Connection lost - goodbye!\"\n       reactor.stop()\nreactor.connectTCP(\"localhost\", 8000, EchoFactory())\nreactor.run()\nRunning the server script starts a TCP server listening for connections on port 8000. The server uses the\nEcho  protocol, and data is written out over a TCP transport. Running the client makes a TCP\nconnection to the server, echoes the server response, and then terminates the connection and stops the\nreactor. Factories are used to produce instances of protocols for both sides of the connection. The\ncommunication is asynchronous on both sides; connectTCP  takes care of registering callbacks with the\nreactor to get notified when data is available to read from a socket.\n", "page": 228, "type": "text", "section": "Page 228"}
{"text": "Applications\nTwisted is an engine for producing scalable, cross-platform network servers and clients. Making it easy to\ndeploy these applications in a standardized fashion in production environments is an important part of a\nplatform like this getting wide-scale adoption.\nTo that end, Twisted developed the Twisted application infrastructure, a re-usable and configurable way\nto deploy a Twisted application. It allows a programmer to avoid boilerplate code by hooking an\napplication into existing tools for customizing the way it is run, including daemonization, logging, using a\ncustom reactor, profiling code, and more.\nThe application infrastructure has four main parts: Services, Applications, configuration management (via\nTAC files and plugins), and the twistd  command-line utility. To illustrate this infrastructure, we'll turn\nthe echo server from the previous section into an Application.\nService\nA Service is anything that can be started and stopped and which adheres to the IService  interface.\nTwisted comes with service implementations for TCP, FTP, HTTP, SSH, DNS, and many other protocols.\nMany Services can register with a single application.\nThe core of the IService  interface is:\nstartService\nStart the service. This might include loading configuration data, setting up database\nconnections, or listening on a port\nstopService\nShut down the service. This might include saving state to disk, closing database\nconnections, or stopping listening on a port\nOur echo service uses TCP, so we can use Twisted's default TCPServer  implementation of this\nIService  interface.\nApplication\nAn Application is the top-level service that represents the entire Twisted application. Services register\nthemselves with an Application, and the twistd  deployment utility described below searches for and\nruns Applications.\nWe'll create an echo Application with which the echo Service can register.\nTAC Files\nWhen managing Twisted applications in a regular Python file, the developer is responsible for writing\ncode to start and stop the reactor and to configure the application. Under the Twisted application\ninfrastructure, protocol implementations live in a module, Services using those protocols are registered in\na Twisted Application Configuration (TAC) file, and the reactor and configuration are managed by an\nexternal utility.\nTo turn our echo server into an echo application, we can follow a simple algorithm:\n1. Move the Protocol parts of the echo server into their own module.\n2. Inside a TAC file:\n1. Create an echo Application.\n2. Create an instance of the TCPServer  Service which will use our EchoFactory , and register it\nwith the Application.\nThe code for managing the reactor will be taken care of by twistd , discussed below. The application\ncode ends up looking like this:\nThe echo.py  file:\nfrom twisted.internet import protocol, reactor\nclass Echo(protocol.Protocol):\n    def dataReceived(self, data):\n        self.transport.write(data)\nclass EchoFactory(protocol.Factory):\n    def buildProtocol(self, addr):\n    return Echo()\nThe echo_server.tac  file:\nfrom twisted.application import internet, service\nfrom echo import EchoFactory\napplication = service.Application(\"echo\")\nechoService = internet.TCPServer(8000, EchoFactory())\nechoService.setServiceParent(application)\n", "page": 229, "type": "text", "section": "Page 229"}
{"text": "twistd\ntwistd  (pronounced \"twist-dee\") is a cross-platform utility for deploying Twisted applications. It runs\nTAC files and handles starting and stopping an application. As part of Twisted's batteries-included\napproach to network programming, twistd  comes with a number of useful configuration flags, including\ndaemonizing the application, the location of log files, dropping privileges, running in a chroot, running\nunder a non-default reactor, or even running the application under a profiler.\nWe can run our echo server Application with:\n$ twistd -y echo_server.tac\nIn this simplest case, twistd  starts a daemonized instance of the application, logging to twistd.log .\nAfter starting and stopping the application, the log looks like this:\n2011-11-19 22:23:07-0500 [-] Log opened.\n2011-11-19 22:23:07-0500 [-] twistd 11.0.0 (/usr/bin/python 2.7.1) starting up.\n2011-11-19 22:23:07-0500 [-] reactor class: twisted.internet.selectreactor.SelectReactor.\n2011-11-19 22:23:07-0500 [-] echo.EchoFactory starting on 8000\n2011-11-19 22:23:07-0500 [-] Starting factory <echo.EchoFactory instance at 0x12d8670>\n2011-11-19 22:23:20-0500 [-] Received SIGTERM, shutting down.\n2011-11-19 22:23:20-0500 [-] (TCP Port 8000 Closed)\n2011-11-19 22:23:20-0500 [-] Stopping factory <echo.EchoFactory instance at 0x12d8670>\n2011-11-19 22:23:20-0500 [-] Main loop terminated.\n2011-11-19 22:23:20-0500 [-] Server Shut Down.\nRunning a service using the Twisted application infrastructure allows developers to skip writing boilerplate\ncode for common service functionalities like logging and daemonization. It also establishes a standard\ncommand line interface for deploying applications.\nPlugins\nAn alternative to the TAC-based system for running Twisted applications is the plugin system. While the\nTAC system makes it easy to register simple hierarchies of pre-defined services within an application\nconfiguration file, the plugin system makes it easy to register custom services as subcommands of the\ntwistd  utility, and to extend the command-line interface to an application.\nUsing this system:\n1. Only the plugin API is required to remain stable, which makes it easy for third-party developers to\nextend the software.\n2. Plugin discoverability is codified. Plugins can be loaded and saved when a program is first run, re-\ndiscovered each time the program starts up, or polled for repeatedly at runtime, allowing the\ndiscovery of new plugins installed after the program has started.\nTo extend a program using the Twisted plugin system, all one has to do is create objects which\nimplement the IPlugin  interface and put them in a particular location where the plugin system knows\nto look for them.\nHaving already converted our echo server to a Twisted application, transformation into a Twisted plugin is\nstraightforward. Alongside the echo  module from before, which contains the Echo  protocol and\nEchoFactory  definitions, we add a directory called twisted , containing a subdirectory called\nplugins , containing our echo plugin definition. This plugin will allow us to start an echo server and\nspecify the port to use as arguments to the twistd  utility:\nfrom zope.interface import implements\nfrom twisted.python import usage\nfrom twisted.plugin import IPlugin\nfrom twisted.application.service import IServiceMaker\nfrom twisted.application import internet\nfrom echo import EchoFactory\nclass Options(usage.Options):\n    optParameters = [[\"port\", \"p\", 8000, \"The port number to listen on.\"]]\nclass EchoServiceMaker(object):\n    implements(IServiceMaker, IPlugin)\n    tapname = \"echo\"\n    description = \"A TCP-based echo server.\"\n    options = Options\n    def makeService(self, options):\n        \"\"\"\n", "page": 230, "type": "text", "section": "Page 230"}
{"text": "        Construct a TCPServer from a factory defined in myproject.\n        \"\"\"\n        return internet.TCPServer(int(options[\"port\"]), EchoFactory())\nserviceMaker = EchoServiceMaker()\nOur echo server will now show up as a server option in the output of twistd --help , and running\ntwistd echo --port=1235  will start an echo server on port 1235.\nTwisted comes with a pluggable authentication system for servers called twisted.cred , and a\ncommon use of the plugin system is to add an authentication pattern to an application. One can use\ntwisted.cred AuthOptionMixin  to add command-line support for various kinds of authentication off\nthe shelf, or to add a new kind. For example, one could add authentication via a local Unix password\ndatabase or an LDAP server using the plugin system.\ntwistd  comes with plugins for many of Twisted's supported protocols, which turns the work of spinning\nup a server into a single command. Here are some examples of twistd  servers that ship with Twisted:\ntwistd web --port 8080 --path .\nRun an HTTP server on port 8080, serving both static and dynamic content out of the current working\ndirectory.\ntwistd dns -p 5553 --hosts-file=hosts\nRun a DNS server on port 5553, resolving domains out of a file called hosts  in the format of\n/etc/hosts .\nsudo twistd conch -p tcp:2222\nRun an ssh server on port 2222. ssh keys must be set up independently.\ntwistd mail -E -H localhost -d localhost=emails\nRun an ESMTP POP3 server, accepting email for localhost and saving it to the emails  directory.\ntwistd  makes it easy to spin up a server for testing clients, but it is also pluggable, production-grade\ncode.\nIn that respect, Twisted's application deployment mechanisms via TAC files, plugins, and twistd  have\nbeen a success. However, anecdotally, most large Twisted deployments end up having to rewrite some of\nthese management and monitoring facilities; the architecture does not quite expose what system\nadministrators need. This is a reflection of the fact that Twisted has not historically had much architectural\ninput from system administrators\u2014the people who are experts at deploying and maintaining applications.\nTwisted would be well-served to more aggressively solicit feedback from expert end users when making\nfuture architectural decisions in this space.\n21.3. Retrospective and Lessons Learned\nTwisted recently celebrated its 10th anniversary. Since its inception, inspired by the networked game\nlandscape of the early 2000s, it has largely achieved its goal of being an extensible, cross-platform,\nevent-driven networking engine. Twisted is used in production environments at companies from Google\nand Lucasfilm to Justin.TV and the Launchpad software collaboration platform. Server implementations in\nTwisted are the core of numerous other open source applications, including BuildBot, BitTorrent, and\nTahoe-LAFS.\nTwisted has had few major architectural changes since its initial development. The one crucial addition\nwas Deferred , as discussed above, for managing pending results and their callback chains.\nThere was one important removal, which has almost no footprint in the current implementation: Twisted\nApplication Persistence.\nTwisted Application Persistence\nTwisted Application Persistence (TAP) was a way of keeping an application's configuration and state in a\npickle. Running an application using this scheme was a two-step process:\n1. Create the pickle that represents an Application, using the now defunct mktap  utility.\n2. Use twistd  to unpickle and run the Application.\nThis process was inspired by Smalltalk images, an aversion to the proliferation of seemingly ad hoc\nconfiguration languages that were hard to script, and a desire to express configuration details in Python.\nTAP files immediately introduced unwanted complexity. Classes would change in Twisted without\ninstances of those classes getting changed in the pickle. Trying to use class methods or attributes from a\nnewer version of Twisted on the pickled object would crash the application. The notion of \"upgraders\" that\nwould upgrade pickles to new API versions was introduced, but then a matrix of upgraders, pickle\nversions, and unit tests had to be maintained to cover all possible upgrade paths, and comprehensively\naccounting for all interface changes was still hard and error-prone.\nTAPs and their associated utilities were abandoned and then eventually removed from Twisted and\nreplaced with TAC files and plugins. TAP was backronymed to Twisted Application Plugin, and few traces\nof the failed pickling system exist in Twisted today.\n", "page": 231, "type": "text", "section": "Page 231"}
{"text": "This work is made available under the Creative Commons Attribution 3.0 Unported license. Please see\nthe full description of the license for details.\nThe lesson learned from the TAP fiasco was that to have reasonable maintainability, persistent data\nneeds an explicit schema. More generally, it was a lesson about adding complexity to a project: when\nconsidering introducing a novel system for solving a problem, make sure the complexity of that solution is\nwell understood and tested and that the benefits are clearly worth the added complexity before\ncommitting the project to it.\nweb2: a lesson on rewrites\nWhile not primarily an architectural decision, a project management decision about rewriting the Twisted\nWeb implementation has had long-term ramifications for Twisted's image and the maintainers' ability to\nmake architectural improvements to other parts of the code base, and it deserves a short discussion.\nIn the mid-2000s, the Twisted developers decided to do a full rewrite of the twisted.web  APIs as a\nseparate project in the Twisted code base called web2 . web2  would contain numerous improvements\nover twisted.web , including full HTTP 1.1 support and a streaming data API.\nweb2  was labelled as experimental, but ended up getting used by major projects anyway and was even\naccidentally released and packaged by Debian. Development on web  and web2  continued concurrently\nfor years, and new users were perennially frustrated by the side-by-side existence of both projects and a\nlack of clear messaging about which project to use. The switchover to web2  never happened, and in\n2011 web2  was finally removed from the code base and the website. Some of the improvements from\nweb2  are slowly getting ported back to web .\nPartially because of web2 , Twisted developed a reputation for being hard to navigate and structurally\nconfusing to newcomers. Years later, the Twisted community still works hard to combat this image.\nThe lesson learned from web2  was that rewriting a project from scratch is often a bad idea, but if it has\nto happen make sure that the developer community understands the long-term plan, and that the user\ncommunity has one clear choice of implementation to use during the rewrite.\nIf Twisted could go back and do web2  again, the developers would have done a series of backwards-\ncompatible changes and deprecations to twisted.web  instead of a rewrite.\nKeeping Up with the Internet\nThe way that we use the Internet continues to evolve. The decision to implement many protocols as part\nof the core software burdens Twisted with maintaining code for all of those protocols. Implementations\nhave to evolve with changing standards and the adoption of new protocols while maintaining a strict\nbackwards-compatibility policy.\nTwisted is primarily a volunteer-driven project, and the limiting factor for development is not community\nenthusiasm, but rather volunteer time. For example, RFC 2616 defining HTTP 1.1 was released in 1999,\nwork began on adding HTTP 1.1 support to Twisted's HTTP protocol implementations in 2005, and the\nwork was completed in 2009. Support for IPv6, defined in RFC 2460 in 1998, is in progress but\nunmerged as of 2011.\nImplementations also have to evolve as the interfaces exposed by supported operating systems change.\nFor example, the epoll  event notification facility was added to Linux 2.5.44 in 2002, and Twisted grew\nan epoll -based reactor to take advantage of this new API. In 2007, Apple released OS 10.5 Leopard\nwith a poll  implementation that didn't support devices, which was buggy enough behavior for Apple to\nnot expose select.poll  in its build of Python. Twisted has had to work around this issue and\ndocument it for users ever since.\nSometimes, Twisted development doesn't keep up with the changing networking landscape, and\nenhancements are moved to libraries outside of the core software. For example, the Wokkel project, a\ncollection of enhancements to Twisted's Jabber/XMPP support, has lived as a to-be-merged independent\nproject for years without a champion to oversee the merge. An attempt was made to add WebSockets to\nTwisted as browsers began to adopt support for the new protocol in 2009, but development moved to\nexternal projects after a decision not to include the protocol until it moved from an IETF draft to a\nstandard.\nAll of this being said, the proliferation of libraries and add-ons is a testament to Twisted's flexibility and\nextensibility. A strict test-driven development policy and accompanying documentation and coding\nstandards help the project avoid regressions and preserve backwards compatibility while maintaining a\nlarge matrix of supported protocols and platforms. It is a mature, stable project that continues to have\nvery active development and adoption.\nTwisted looks forward to being the engine of your Internet for another ten years.\nBack to top\nBack to The Architecture of Open Source Applications.\n", "page": 232, "type": "text", "section": "Page 232"}
{"text": "<>\nYesod is a web framework written in the Haskell programming language. While many popular web\nframeworks exploit the dynamic nature of their host languages, Yesod exploits the static nature of Haskell\nto produce safer, faster code.\nDevelopment began about two years ago and has been going strong ever since. Yesod cut its teeth on\nreal life projects, with all of its initial features born out of an actual, real-life need. At first, development\nwas almost entirely a one-man show. After about a year of development the community efforts kicked in,\nand Yesod has since blossomed into a thriving open source project.\nDuring the embryonic phase, when Yesod was incredibly ephemeral and ill-defined, it would have been\ncounter-productive to try and get a team to work on it. By the time it stabilized enough to be useful to\nothers, it was the right time to find out the downsides to some of the decisions that had been made.\nSince then, we have made major changes to the user-facing API to make it more useful, and are quickly\nsolidifying a 1.0 release.\nThe question you may ask is: Why another web framework? Let's instead redirect to a different question:\nWhy use Haskell? It seems that most of the world is happy with one of two styles of language:\nStatically typed languages, like Java, C# and C++. These languages provide speed and type safety,\nbut are more cumbersome to program with.\nDynamically typed languages, like Ruby and Python. These languages greatly increase productivity\n(at least in the short run), but run slowly and have very little support from the compiler to ensure\ncorrectness. (The solution to this last point is unit testing. We'll get to that later.)\nThis is a false dichotomy. There's no reason why statically typed languages need to be so clumsy.\nHaskell is able to capture a huge amount of the expressivity of Ruby and Python, while remaining a\nstrongly typed language. In fact, Haskell's type system catches many more bugs than Java and its ilk.\nNull pointer exceptions are completely eliminated; immutable data structures simplify reasoning about\nyour code and simplify parallel and concurrent programming.\nSo why Haskell? It is an efficient, developer-friendly language which provides many compile-time checks\nof program correctness.\nThe goal of Yesod is to extend Haskell's strengths into web development. Yesod strives to make your\ncode as concise as possible. As much as possible, every line of your code is checked for correctness at\ncompile time. Instead of requiring large libraries of unit tests to test basic properties, the compiler does it\nall for you. Under the surface, Yesod uses as many advanced performance techniques as we can muster\nto make your high-level code fly.\n22.1. Compared to Other Frameworks\nIn general terms, Yesod is more similar to than different than the leading frameworks such as Rails and\nDjango. It generally follows the Model-View-Controller (MVC) paradigm, has a templating system that\nseparates view from logic, provides an Object-Relational Mapping (ORM) system, and has a front\ncontroller approach to routing.\nThe devil is in the details. Yesod strives to push as much error catching to the compile phase instead of\nruntime, and to automatically catch both bugs and security flaws through the type system. While Yesod\ntries to maintain a user-friendly, high-level API, it uses a number of newer techniques from the functional\nprogramming world to achieve high performance, and is not afraid to expose these internals to\ndevelopers.\nThe main architectural challenge in Yesod is balancing these two seemingly conflicting goals. For\nexample, there is nothing revolutionary about Yesod's approach to routing (called type-safe URLs).\nHistorically, implementing such a solution was a tedious, error-prone process. Yesod's innovation is to\nuse Template Haskell (a form of code generation) to automate the boilerplate required to bootstrap the\nprocess. Similarly, type-safe HTML has been around for a long while; Yesod tries to keep the developer-\nfriendly aspect of common template languages while keeping the power of type safety.\nYesod\nMichael Snoyman\n", "page": 234, "type": "text", "section": "Page 234"}
{"text": "22.2. Web Application Interface\nA web application needs some way to communicate with a server. One possible approach is to bake the\nserver directly into the framework, but doing so necessarily limits your options for deployment and leads\nto poor interfaces. Many languages have created standard interfaces to address this issue: Python has\nWSGI and Ruby has Rack. In Haskell, we have WAI: Web Application Interface.\nWAI is not intended to be a high-level interface. It has two specific goals: generality and performance. By\nstaying general, WAI has been able to support backends for everything from standalone servers to old\nschool CGI and even works directly with Webkit to produce faux desktop applications. The performance\nside will introduce us to a number of the cool features of Haskell.\nFigure 22.1: Overall structure of a Yesod application\nDatatypes\nOne of the biggest advantages of Haskell\u2014and one of the things we make the most use of in Yesod\u2014is\nstrong static typing. Before we begin to write the code for how to solve something, we need to think\nabout what the data will look like. WAI is a perfect example of this paradigm. The core concept we want\nto express is that of an application. An application's most basic expression is a function that takes a\nrequest and returns a response. In Haskell lingo:\ntype Application = Request -> Response\nThis just raises the question: what do Request  and Response  look like? A request has a number of\npieces of information, but the most basic are the requested path, query string, request headers, and\nrequest body. And a response has just three components: a status code, response headers and\nresponse body.\nHow do we represent something like a query string? Haskell keeps a strict separation between binary\nand textual data. The former is represented by ByteString , the latter by Text . Both are highly\noptimized datatypes that provide a high-level, safe API. In the case of a query string we store the raw\nbytes transferred over the wire as a ByteString  and the parsed, decoded values as Text .\nStreaming\nA ByteString  represents a single memory buffer. If we were to naively use a plain ByteString  for\nholding the entire request or response bodies, our applications could never scale to large requests or\nresponses. Instead, we use a technique called enumerators, very similar in concept to generators in\nPython. Our Application  becomes a consumer of a stream of ByteString s representing the\nincoming request body, and a producer of a separate stream for the response.\nWe now need to slightly revise our definition of an Application . An Application  will take a\nRequest  value, containing headers, query string, etc., and will consume a stream of ByteString s,\nproducing a Response . So the revised definition of an Application  is:\ntype Application = Request -> Iteratee ByteString IO Response\nThe IO  simply explains what types of side effects an application can perform. In the case of IO , it can\nperform any kind of interaction with the outside world, an obvious necessity for the vast majority of web\napplications.\nBuilder\nThe trick in our arsenal is how we produce our response buffers. We have two competing desires here:\nminimizing system calls, and minimizing buffer copies. On the one hand, we want to minimize system\ncalls for sending data over the socket. To do this we need to store outgoing data in a buffer. However, if\nwe make this buffer too large, we will exhaust our memory and slow down the application's response\n", "page": 235, "type": "text", "section": "Page 235"}
{"text": "time. On the other hand, we want to minimize the number of times data is copied between buffers,\npreferably copying just once from the source to destination buffer.\nHaskell's solution is the builder. A builder is an instruction for how to fill a memory buffer, such as: place\nthe five bytes \"hello\" in the next open position. Instead of passing a stream of memory buffers to the\nserver, a WAI application passes a stream of these instructions. The server takes the stream and uses it\nto fill up optimally sized memory buffers. As each buffer is filled, the server makes a system call to send\nthe data over over the wire and then starts filling up the next buffer.\n(The optimal size for a buffer will depend on many factors such as cache size. The underlying blaze-\nbuilder library underwent significant performance testing to determine the best trade-off.)\nIn theory, this kind of optimization could be performed in the application itself. However, by encoding this\napproach in the interface, we are able to simply prepend the response headers to the response body.\nThe result is that, for small to medium-sized responses, the entire response can be sent with a single\nsystem call and memory is copied only once.\nHandlers\nNow that we have an application, we need some way to run it. In WAI parlance, this is a handler. WAI\nhas some basic, standard handlers, such as the standalone server Warp (discussed below), FastCGI,\nSCGI and CGI. This spectrum allows WAI applications to be run on anything from dedicated servers to\nshared hosting. But in addition to these, WAI has some more interesting backends:\nWebkit: This backend embeds a Warp server and calls out to QtWebkit. By launching a server, then\nlaunching a new standalone browser window, we have faux desktop applications.\nLaunch: This is a slight variant on Webkit. Having to deploy the Qt and Webkit libraries can be a bit\nburdensome, so instead we just launch the user's default browser.\nTest: Even testing counts as a handler. After all, testing is simply the act of running an application\nand inspecting the responses.\nMost developers will likely use Warp. It is lightweight enough to be used for testing. It requires no config\nfiles, no folder hierarchy and no long-running, administrator-owned process. It's a simple library that gets\ncompiled into your application or run via the Haskell interpreter. Warp is an incredibly fast server, with\nprotection from all kinds of attack vectors, such as Slowloris and infinite headers. Warp can be the only\nweb server you need, though it is also quite happy to sit behind a reverse HTTP proxy.\nThe PONG benchmark measures the requests per second of various servers for the 4-byte response\nbody \"PONG\". In the graph shown in Figure 22.2, Yesod is measured as a framework on top of Warp.\nAs can be seen, the Haskell servers (Warp, Happstack and Snap) lead the pack.\nFigure 22.2: Warp PONG benchmark\nMost of the reasons for Warp's speed have already been spelled out in the overall description of WAI:\nenumerators, builders and packed datatypes. The last piece in the puzzle is from the Glasgow Haskell\nCompiler's (GHC's) multithreaded runtime. GHC, Haskell's flagship compiler, has light-weight green\nthreads. Unlike system threads, it is possible to spin up thousands of these without serious performance\nhits. Therefore, in Warp each connection is handled by its own green thread.\nThe next trick is asynchronous I/O. Any web server hoping to scale to tens of thousands of requests per\nsecond will need some type of asynchronous communication. In most languages, this involves\ncomplicated programming involving callbacks. GHC lets us cheat: we program as if we're using a\nsynchronous API, and GHC automatically switches between different green threads waiting for activity.\nUnder the surface, GHC uses whatever system is provided by the host operating system, such as\n", "page": 236, "type": "text", "section": "Page 236"}
{"text": "kqueue , epoll  and select . This gives us all the performance of an event-based I/O system,\nwithout worrying about cross-platform issues or writing in a callback-oriented way.\nMiddleware\nIn between handlers and applications, we have middleware . Technically, middleware is an application\ntransformer: it takes one Application , and returns a new one. This is defined as:\ntype Middleware = Application -> Application\nThe best way to understand the purpose of middleware is to look at some common examples:\ngzip  automatically compresses the response from an application.\njsonp  automatically converts JSON responses to JSON-P responses when the client provided a\ncallback parameter.\nautohead  will generate appropriate HEAD responses based on the GET response of an\napplication.\ndebug  will print debug information to the console or a log on each request.\nThe idea here is to factor out common code from applications and let it be shared easily. Note that,\nbased on the definition of middleware, we can easily stack these things up. The general workflow of\nmiddleware is:\n1. Take the request value and apply some modifications.\n2. Pass the modified request to the application and receive a response.\n3. Modify the response and return it to the handler.\nIn the case of stacked middleware, instead of passing to the application or handler, the in-between\nmiddleware will actually be passing to the inner and outer middleware, respectively.\nWai-test\nNo amount of static typing will obviate the need for testing. We all know that automated testing is a\nnecessity for any serious applications. wai-test  is the recommended approach to testing a WAI\napplication. Since requests and responses are simple datatypes, it is easy to mock up a fake request,\npass it to an application, and test properties about the response. wai-test  simply provides some\nconvenience functions for testing common properties like the presence of a header or a status code.\n22.3. Templates\nIn the typical Model-View-Controller (MVC) paradigm, one of the goals is to separate logic from the view.\nPart of this separation is achieved through the use of a template language. However, there are many\ndifferent ways to approach this issue. At one end of the spectrum, for example, PHP/ASP/JSP will allow\nyou to embed any arbitrary code within your template. At the other end, you have systems like\nStringTemplate and QuickSilver, which are passed some arguments and have no other way of interacting\nwith the rest of the program.\nEach system has its pros and cons. Having a more powerful template system can be a huge\nconvenience. Need to show the contents of a database table? No problem, pull it in with the template.\nHowever, such an approach can quickly lead to convoluted code, interspersing database cursor updates\nwith HTML generation. This can be commonly seen in a poorly written ASP project.\nWhile weak template systems make for simple code, they also tend towards a lot of redundant work. You\nwill often need to not only keep your original values in datatypes, but also create dictionaries of values to\npass to the template. Maintaining such code is not easy, and usually there is no way for a compiler to\nhelp you out.\nYesod's family of template languages, the Shakespearean languages, strive for a middle ground. By\nleveraging Haskell's standard referential transparency, we can be assured that our templates produce no\nside effects. However, they still have full access to all the variables and functions available in your\nHaskell code. Also, since they are fully checked for both well-formedness, variable resolution and type\nsafety at compile time, typos are much less likely to have you searching through your code trying to pin\ndown a bug.\nWhy the Name Shakespeare?\nThe HTML language, Hamlet, was the first language written, and originally based its syntax on Haml.\nSince it was at the time a \"reduced\" Haml, Hamlet seemed appropriate. As we added CSS and\nJavascript options, we decided to keep the naming theme with Cassius and Julius. At this point,\nHamlet looks nothing like Haml, but the name stuck anyway.\nTypes\nOne of the overarching themes in Yesod is proper use of types to make developers' lives easier. In\nYesod templates, we have two main examples:\n1. All content embedded into a Hamlet template must have a type of Html . As we'll see later, this\nforces us to properly escape dangerous HTML when necessary, while avoiding accidental double-\nescaping as well.\n", "page": 237, "type": "text", "section": "Page 237"}
{"text": "2. Instead of concatenating URLs directly in our template, we have datatypes\u2014known as type-safe\nURLs\u2014which represent the routes in our application.\nAs a real-life example, suppose that a user submits his/her name to an application via a form. This data\nwould be represented with the Text  datatype. Now we would like to display this variable, called name ,\nin a page. The type system\u2014at compile time\u2014prevents it from being simply stuck into a Hamlet\ntemplate, since it's not of type Html . Instead we must convert it somehow. For this, there are two\nconversion functions:\n1. toHtml  will automatically escape any entities. So if a user submits the string <script\nsrc=\"http://example.com/evil.js\"></script> , the less-than signs will automatically be\nconverted to &lt; .\n2. preEscapedText , on the other hand, will leave the content precisely as it is now.\nSo in the case of untrusted input from a possibly nefarious user, toHtml  would be our recommended\napproach. On the other hand, let us say we have some static HTML stored on our server that we would\nlike to insert into some pages verbatim. In that case, we could load it into a Text  value and then apply\npreEscapedText , thereby avoiding any double-escaping.\nBy default, Hamlet will use the toHtml  function on any content you try to interpolate. Therefore, you\nonly need to explicitly perform a conversion if you want to avoid escaping. This follows the dictum of\nerring on the side of caution.\nname <- runInputPost $ ireq textField \"name\"\nsnippet <- readFile \"mysnippet.html\"\nreturn [hamlet|\n    <p>Welcome #{name}, you are on my site!\n    <div .copyright>#{preEscapedText snippet}\n|]\nThe first step in type-safe URLs is creating a datatype that represents all the routes in your site. Let us\nsay you have a site for displaying Fibonacci numbers. The site will have a separate page for each\nnumber in the sequence, plus the homepage. This could be modeled with the Haskell datatype:\ndata FibRoute = Home | Fib Int\nWe could then create a page like so:\n<p>You are currently viewing number #{show index} in the sequence. Its value is #{\nfib index}.\n<p>\n    <a href=@{Fib (index + 1)}>Next number\n<p>\n    <a href=@{Home}>Homepage\nThen all we need is some function to convert a type-safe URL into a string representation. In our case,\nthat could look something like this:\nrender :: FibRoute -> Text\nrender Home = \"/home\"\nrender (Fib i) = \"/fib/\" ++ show i\nFortunately, all of the boilerplate of defining and rendering type-safe URL datatypes is handled for the\ndeveloper automatically by Yesod. We will cover that in more depth later.\nThe Other Languages\nIn addition to Hamlet, there are three other languages: Julius, Cassius and Lucius. Julius is used for\nJavascript; however, it's a simple pass-through language, just allowing for interpolation. In other words,\nbarring accidental use of the interpolation syntax, any piece of Javascript could be dropped into Julius\nand be valid. For example, to test the performance of Julius, jQuery was run through the language\nwithout an issue.\nThe other two languages are alternate CSS syntaxes. Those familiar with the difference between Sass\nand Less will recognize this immediately: Cassius is whitespace delimited, while Lucius uses braces.\nLucius is in fact a superset of CSS, meaning all valid CSS files are valid Lucius files. In addition to\nallowing text interpolation, there are some helper datatypes provided to model unit sizes and colors. Also,\ntype-safe URLs work in these languages, making it convenient for specifying background images.\nAside from the type safety and compile-time checks mentioned above, having specialized languages for\nCSS and Javascript give us a few other advantages:\nFor production, all the CSS and Javascript is compiled into the final executable, increasing\nperformance (by avoiding file I/O) and simplifying deployment.\nBy being based around the efficient builder construct described earlier, the templates can be\n", "page": 238, "type": "text", "section": "Page 238"}
{"text": "rendered very quickly.\nThere is built-in support for automatically including these in final webpages. We will get into this in\nmore detail when describing widgets below.\n22.4. Persistent\nMost web applications will want to store information in a database. Traditionally, this has meant some\nkind of SQL database. In that regard, Yesod continues a long tradition, with PostgreSQL as our most\ncommonly used backend. But as we have been seeing in recent years, SQL isn't always the answer to\nthe persistence question. Therefore, Yesod was designed to work well with NoSQL databases as well,\nand ships with a MongoDB backend as a first-class citizen.\nThe result of this design decision is Persistent, Yesod's preferred storage option. There are really two\nguiding lights for Persistent: make it as back-end-agnostic as possible, and let user code be completely\ntype-checked.\nAt the same time, we fully recognize that it is impossible to completely shield the user from all details of\nthe backend. Therefore, we provide two types of escape routes:\nBack-end-specific functionality as necessary. For example, Persistent provides features for SQL joins\nand MongoDB lists and hashes. Proper portability warnings will apply, but if you want this\nfunctionality, it's there.\nEasy access to performing raw queries. We don't believe it's possible for any abstraction to cover\nevery use case of the underlying library. If you just have to write a 5-table, correlated subquery in\nSQL, go right ahead.\nTerminology\nThe most primitive datatype in Persistent is the PersistValue . This represents any raw data that can\nappear within the database, such as a number, a date, or a string. Of course, sometimes you'll have\nsome more user-friendly datatypes you want to store, like HTML. For that, we have the PersistField\nclass. Internally, a PersistField  expresses itself to the database in terms of a PersistValue .\nAll of this is very nice, but we will want to combine different fields together into a larger picture. For this,\nwe have a PersistEntity , which is basically a collection of PersistField s. And finally, we have a\nPersistBackend  that describes how to create, read, update and delete these entities.\nAs a practical example, consider storing a person in a database. We want to store the person's name,\nbirthday, and a profile image (a PNG file). We create a new entity Person  with three fields: a Text , a\nDay  and a PNG . Each of those gets stored in the database using a different PersistValue\nconstructor: PersistText , PersistDay  and PersistByteString , respectively.\nThere is nothing surprising about the first two mappings, but the last one is interesting. There is no\nspecific constructor for storing PNG content in a database, so instead we use a more generic type (a\nByteString , which is just a sequence of bytes). We could use the same mechanism to store other\ntypes of arbitrary data.\n(The commonly held best practice for storing images is to keep the data on the filesystem and just keep\na path to the image in the database. We do not advocate against using that approach, but are rather\nusing database-stored images as an illustrative example.)\nHow is all this represented in the database? Consider SQL as an example: the Person  entity becomes\na table with three columns (name, birthday, and picture). Each field is stored as a different SQL type:\nText  becomes a VARCHAR , Day  becomes a Date  and PNG  becomes a BLOB  (or BYTEA ).\nThe story for MongoDB is very similar. Person  becomes its own document, and its three fields each\nbecome a MongoDB field. There is no need for datatypes or creation of a schema in MongoDB.\nPersistent\nSQL\nMongoDB\nPersistEntity\nTable\nDocument\nPersistField\nColumn\nField\nPersistValue\nColumn type\nN/A\nType Safety\nPersistent handles all of the data marshaling concerns behind the scenes. As a user of Persistent, you\nget to completely ignore the fact that a Text  becomes a VARCHAR . You are able to simply declare\nyour datatypes and use them.\nEvery interaction with Persistent is strongly typed. This prevents you from accidentally putting a number\nin the date fields; the compiler will not accept it. Entire classes of subtle bugs simply disappear at this\npoint.\nNowhere is the power of strong typing more pronounced than in refactoring. Let's say you have been\nstoring users' ages in the database, and you realize that you really wanted to store birthdays instead.\nYou are able to make a single line change to your entities declaration file, hit compile, and automatically\n", "page": 239, "type": "text", "section": "Page 239"}
{"text": "find every single line of code that needs to be updated.\nIn most dynamically-typed languages, and their web frameworks, the recommended approach to solving\nthis issue is writing unit tests. If you have full test coverage, then running your tests will immediately\nreveal what code needs to be updated. This is all well and good, but it is a weaker solution than true\ntypes:\nIt is all predicated on having full test coverage. This takes extra time, and worse, is boilerplate code\nthat the compiler should be able to do for you.\nYou might be a perfect developer who never forgets to write a test, but can you say the same for\nevery person who will touch your codebase?\nEven 100% test coverage doesn't guarantee that you really have tested every case. All it's done is\nproven you've tested every line of code.\nCross-Database Syntax\nCreating an SQL schema that works for multiple SQL engines can be tricky enough. How do you create\na schema that will also work with a non-SQL database like MongoDB?\nPersistent allows you to define your entities in a high-level syntax, and will automatically create the SQL\nschema for you. In the case of MongoDB, we currently use a schema-less approach. This also allows\nPersistent to ensure that your Haskell datatypes match perfectly with the database's definitions.\nAdditionally, having all this information gives Persistent the ability to perform more advanced functions,\nsuch as migrations, for you automatically.\nMigrations\nPersistent not only creates schema files as necessary, but will also automatically apply database\nmigrations if possible. Database modification is one of the less-developed pieces of the SQL standard,\nand thus each engine has a different take on the process. As such, each Persistent backend defines its\nown set of migration rules. In PostgreSQL, which has a rich set of ALTER TABLE  rules, we use those\nextensively. Since SQLite lacks much of that functionality, we are reduced to creating temporary tables\nand copying rows. MongoDB's schema-less approach means no migration support is required.\nThis feature is purposely limited to prevent any kind of data loss. It will not remove any columns\nautomatically; instead, it will give you an error message, telling you the unsafe operations that are\nnecessary in order to continue. You will then have the option of either manually running the SQL it\nprovides you, or changing your data model to avoid the dangerous behavior.\nRelations\nPersistent is non-relational in nature, meaning it has no requirement for backends to support relations.\nHowever, in many use cases, we may want to use relations. In those cases, developers will have full\naccess to them.\nAssume we want to now store a list of skills with each user. If we were writing a MongoDB-specific app,\nwe could go ahead and just store that list as a new field in the original Person  entity. But that approach\nwould not work in SQL. In SQL, we call this kind of relationship a one-to-many relationship.\nThe idea is to store a reference to the \"one\" entity (person) with each \"many\" entity (skill). Then if we\nwant to find all the skills a person has, we simply find all skills that reference that person. For this\nreference, every entity has an ID. And as you might expect by now, these IDs are completely type-safe.\nThe datatype for a Person ID is PersonId . So to add our new skill, we would just add the following to\nour entity definition:\nSkill\n    person PersonId\n    name Text\n    description Text\n    UniqueSkill person name\nThis ID datatype concept comes up throughout Persistent and Yesod. You can dispatch based on an ID.\nIn such a case, Yesod will automatically marshal the textual representation of the ID to the internal one,\ncatching any parse errors along the way. These IDs are used for lookup and deletion with the get  and\ndelete  functions, and are returned by the insertion and query functions insert  and selectList .\n22.5. Yesod\nIf we are looking at the typical Model-View-Controller (MVC) paradigm, Persistent is the model and\nShakespeare is the view. This would leave Yesod as the controller.\nThe most basic feature of Yesod is routing. It features a declarative syntax and type-safe dispatch.\nLayered on top of this, Yesod provides many other features: streaming content generation, widgets, i18n,\nstatic files, forms and authentication. But the core feature added by Yesod is really routing.\nThis layered approach makes it simpler for users to swap different components of the system. Some\npeople are not interested in using Persistent. For them, nothing in the core system even mentions\nPersistent. Likewise, while they are commonly used features, not everyone needs authentication or static\n", "page": 240, "type": "text", "section": "Page 240"}
{"text": "file serving.\nOn the other hand, many users will want to integrate all of these features. And doing so, while enabling\nall the optimizations available in Yesod, is not always straightforward. To simplify the process, Yesod\nalso provides a scaffolding tool that sets up a basic site with the most commonly used features.\nRoutes\nGiven that routing is really the main function of Yesod, let's start there. The routing syntax is very simple:\na resource pattern, a name, and request methods. For example, a simple blog site might look like:\n/ HomepageR GET\n/add-entry AddEntryR GET POST\n/entry/#EntryId EntryR GET\nThe first line defines the homepage. This says \"I respond to the root path of the domain, I'm called\nHomepageR, and I answer GET requests.\" (The trailing \"R\" on the resource names is simply a\nconvention, it doesn't hold any special meaning besides giving a cue to the developer that something is a\nroute.)\nThe second line defines the add-entry page. This time, we answer both GET and POST requests. You\nmight be wondering why Yesod, as opposed to most frameworks, requires you to explicitly state your\nrequest methods. The reason is that Yesod tries to adhere to RESTful principles as much as possible,\nand GET and POST requests really have very different meanings. Not only do you state these two\nmethods separately, but later you will define their handler functions separately. (This is actually an\noptional feature in Yesod. If you want, you can leave off the list of methods and your handler function will\ndeal with all methods.)\nThe third line is a bit more interesting. After the second slash we have #EntryId . This defines a\nparameter of type EntryId . We already alluded to this feature in the Persistent section: Yesod will now\nautomatically marshal the path component into the relevant ID value. Assuming an SQL backend (Mongo\nis addressed later), if a user requests /entry/5 , the handler function will get called with an argument\nEntryId 5 . But if the user requests /entry/some-blog-post , Yesod will return a 404.\nThis is obviously possible in most other web frameworks as well. The approach taken by Django, for\ninstance, would use a regular expression for matching the routes, e.g. r\"/entry/(\\d+)\" . The Yesod\napproach, however, provides some advantages:\nTyping \"EntryId\" is much more semantic/developer-friendly than a regular expression.\nRegular expressions cannot express everything (or at least, can't do so succinctly). We can use\n/calendar/#Day  in Yesod; do you want to type a regex to match dates in your routes?\nYesod also automatically marshals the data for us. In our calendar case, our handler function would\nreceive a Day  value. In the Django equivalent, the function would receive a piece of text which it\nwould then have to marshal itself. This is tedious, repetitive and inefficient.\nSo far we've assumed that a database ID is just a string of digits. But what if it's more complicated?\nMongoDB uses GUIDs, for example. In Yesod, your #EntryId  will still work, and the type system\nwill instruct Yesod how to parse the route. In a regex system, you would have to go through all of\nyour routes and change the \\d+  to whatever monstrosity of regex is needed to match GUIDs.\nType-Safe URLs\nThis approach to routing gives birth to one of Yesod's most powerful features: type-safe URLs. Instead of\njust splicing together pieces of text to refer to a route, every route in your application can be represented\nby a Haskell value. This immediately eliminates a large number of 404 Not Found errors: it is simply not\npossible to produce an invalid URL. (It is still possible to produce a URL that would lead to a 404 error,\nsuch as by referring to a blog post that does not exist. However, all URLs will be formed correctly.)\nSo how does this magic work? Each site has a route datatype, and each resource pattern gets its own\nconstructor. In our previous example, we would get something that looks like:\ndata MySiteRoute = HomepageR\n                 | AddEntryR\n                 | EntryR EntryId\nIf you want to link to the homepage, you use HomepageR . To link to a specific entry, you would use the\nEntryR  constructor with an EntryId  parameter. For example, to create a new entry and redirect to it,\nyou could write:\nentryId <- insert (Entry \"My Entry\" \"Some content\")\nredirect RedirectTemporary (EntryR entryId)\nHamlet, Lucius and Julius all include built-in support for these type-safe URLs. Inside a Hamlet template\nyou can easily create a link to the add-entry page:\n<a href=@{AddEntryR}>Create a new entry.\n", "page": 241, "type": "text", "section": "Page 241"}
{"text": "The best part? Just like Persistent entities, the compiler will keep you honest. If you change any of your\nroutes (e.g., you want to include the year and month in your entry routes), Yesod will force you to update\nevery single reference throughout your codebase.\nHandlers\nOnce you define your routes, you need to tell Yesod how you want to respond to requests. This is where\nhandler functions come into play. The setup is simple: for each resource (e.g., HomepageR ) and request\nmethod, create a function named methodResourceR . For our previous example, we would need four\nfunctions: getHomepageR , getAddEntryR , postAddEntryR , and getEntryR .\nAll of the parameters collected from the route are passed in as arguments to the handler function.\ngetEntryR  will take a first argument of type EntryId , while all the other functions will take no\narguments.\nThe handler functions live in a Handler  monad, which provides a great deal of functionality, such as\nredirecting, accessing sessions, and running database queries. For the last one, a typical way to start off\nthe getEntryR  function would be:\ngetEntryR entryId = do\n    entry <- runDB $ get404 entryId\nThis will run a database action that will get the entry associated with the given ID from the database. If\nthere is no such entry, it will return a 404 response.\nEach handler function will return some value, which must be an instance of HasReps . This is another\nRESTful feature at play: instead of just returning some HTML or some JSON, you can return a value that\nwill return either one, depending on the HTTP Accept request header. In other words, in Yesod, a\nresource is a specific piece of data, and it can be returned in one of many representations.\nWidgets\nAssume you want to include a navbar on a few different pages of your site. This navbar will load up the\nfive most recent blog posts (stored in your database), generate some HTML, and then need some CSS\nand Javascript to style and enhance.\nWithout a higher-level interface to tie these components together, this could be a pain to implement. You\ncould add the CSS to the site-wide CSS file, but that's adding extra declarations you don't always need.\nLikewise with the Javascript, though a bit worse: having that extra Javascript might cause problems on a\npage it was not intended to live on. You will also be breaking modularity by having to generate the\ndatabase results from multiple handler functions.\nIn Yesod, we have a very simple solution: widgets. A widget is a piece of code that ties together HTML,\nCSS and Javascript, allowing you to add content to both the head and body, and can run any arbitrary\ncode that belongs in a handler. For example, to implement our navbar:\n-- Get last five blog posts. The \"lift\" says to run this code like we're in the handler.\nentries <- lift $ runDB $ selectList [] [LimitTo 5, Desc EntryPosted]\ntoWidget [hamlet|\n<ul .navbar>\n    $forall entry <- entries\n        <li>#{entryTitle entry}\n|]\ntoWidget [lucius| .navbar { color: red } |]\ntoWidget [julius|alert(\"Some special Javascript to play with my navbar\");|]\nBut there is even more power at work here. When you produce a page in Yesod, the standard approach\nis to combine a number of widgets together into a single widget containing all your page content, and\nthen apply defaultLayout . This function is defined per site, and applies the standard site layout.\nThere are two out-of-the-box approaches to handling where the CSS and Javascript go:\n1. Concatenate them and place them into style  and\nscript  tags, respectively, within your HTML.\n2. Place them in external files and refer to them with link  and script  tags, respectively.\nIn addition, your Javascript can be automatically minified. Option 2 is the preferred approach, since it\nallows a few extra optimizations:\n1. The files are created with names based on a hash of the contents. This means you can place\ncached values far in the future without worries of users receiving stale content.\n2. Your Javascript can be asynchronously loaded.\n", "page": 242, "type": "text", "section": "Page 242"}
{"text": "This work is made available under the Creative Commons Attribution 3.0 Unported license. Please see\nthe full description of the license for details.\nThe second point requires a bit of elaboration. Widgets not only contain raw Javascript, they also contain\na list of Javascript dependencies. For example, many sites will refer to the jQuery library and then add\nsome Javascript that uses it. Yesod is able to automatically turn all of that into an asynchronous load via\nyepnope.js .\nIn other words, widgets allow you to create modular, composable code that will result in incredibly\nefficient serving of your static resources.\nSubsites\nMany websites share common areas of functionality. Perhaps the two most common examples of this are\nserving static files and authentication. In Yesod, you can easily drop in this code using a subsite. All you\nneed to do is add an extra line to your routes. For example, to add the static subsite, you would write:\n/static StaticR Static getStatic\nThe first argument tells where in the site the subsite starts. The static subsite is usually used at\n/static , but you could use whatever you want. StaticR  is the name of the route; this is also entirely\nup to you, but convention is to use StaticR . Static  is the name of the static subsite; this is one you\ndo not have control over. getStatic  is a function that returns the settings for the static site, such as\nwhere the static files are located.\nLike all of your handlers, the subsite handlers also have access to the defaultLayout  function. This\nmeans that a well-designed subsite will automatically use your site skin without any extra intervention on\nyour part.\n22.6. Lessons Learned\nYesod has been a very rewarding project to work on. It has given me an opportunity to work on a large\nsystem with a diverse group of developers. One of the things that has truly shocked me is how different\nthe end product has become from what I had originally intended. I started off Yesod by creating a list of\ngoals. Very few of the main features we currently tout in Yesod are in that list, and a good portion of that\nlist is no longer something I plan to implement. The first lesson is:\nYou will have a better idea of the system you need after you start working on it. Do not tie yourself down\nto your initial ideas.\nAs this was my first major piece of Haskell code, I learned a lot about the language during Yesod's\ndevelopment. I'm sure others can relate to the feeling of \"How did I ever write code like this?\" Even\nthough that initial code was not of the same caliber as the code we have in Yesod at this point, it was\nsolid enough to kick-start the project. The second lesson is:\nDon't be deterred by supposed lack of mastery of the tools at hand. Write the best code you can, and\nkeep improving it.\nOne of the most difficult steps in Yesod's development was moving from a single-person team\u2014me\u2014to\ncollaborating with others. It started off simply, with merging pull requests on GitHub, and eventually\nmoved to having a number of core maintainers. I had established some of my own development patterns,\nwhich were nowhere explained or documented. As a result, contributors found it difficult to pull my latest\nunreleased changes and play around with them. This hindered others both when contributing and testing.\nWhen Greg Weber came aboard as another lead on Yesod, he put in place a lot of the coding standards\nthat were sorely lacking. To compound the problems, there were some inherent difficulties playing with\nthe Haskell development toolchain; specifically in dealing with Yesod's large number of packages. One of\nthe goals of the entire Yesod team has since been to create standard scripts and tools to automate\nbuilding. Many of these tools are making their way back into the general Haskell community. The final\nlesson is:\nConsider early on how to make your project approachable for others.\nBack to top\nBack to The Architecture of Open Source Applications.\n", "page": 243, "type": "text", "section": "Page 243"}
{"text": "<>\nThe Yocto Project is an open source project that provides a common starting point for developers of\nembedded Linux systems to create customized distributions for embedded products in a hardware-\nagnostic setting. Sponsored by the Linux Foundation, Yocto is more than a build system. It provides\ntools, processes, templates and methods so developers can rapidly create and deploy products for the\nembedded market. One of the core components of Yocto is the Poky Build system. As Poky is a large\nand complex system, we will be focusing on one of its core components, BitBake. BitBake is a Gentoo-\nPortage-inspired build tool, used by both the Yocto Project and OpenEmbedded communities to utilize\nmetadata to create Linux images from source.\nIn 2001, Sharp Corporation introduced the SL-5000 PDA, named Zaurus, which ran an embedded Linux\ndistribution, Lineo. Not long after the Zaurus's introduction, Chris Larson founded the OpenZaurus\nProject, a replacement Linux distribution for the SharpROM, based on a build system called buildroot.\nWith the founding of the project, people began contributing many more software packages, as well as\ntargets for other devices, and it wasn't long before the build system for OpenZaurus began to show\nfragility. In January 2003, the community began discussing a new build system to incorporate the\ncommunity usage model of a generic build system for embedded Linux distributions. This would\neventually become OpenEmbedded. Chris Larson, Michael Lauer, and Holger Schurig began work on\nOpenEmbedded by porting hundreds of OpenZaurus packages over to the new build system.\nThe Yocto Project springs from this work. At the project's core is the Poky build system, created by\nRichard Purdie. It began as a stabilized branch of OpenEmbedded using a core subset of the thousands\nof OpenEmbedded recipes, across a limited set of architectures. Over time, it slowly coalesced into more\nthan just an embedded build system, but into a complete software development platform, with an Eclipse\nplugin, a fakeroot replacement and QEMU based images. Around November 2010, the Linux Foundation\nannounced that this work would all continue under the heading of the Yocto Project as a Linux\nFoundation-sponsored project. It was then established that Yocto and OpenEmbedded would coordinate\non a core set of package metadata called OE-Core, combining the best of both Poky and\nOpenEmbedded with an increased use of layering for additional components.\n23.1. Introduction to the Poky Build System\nThe Poky build system is the core of the Yocto Project. In Poky's default configuration, it can provide a\nstarting image footprint that ranges from a shell-accessible minimal image all the way up to a Linux\nStandard Base-compliant image with a GNOME Mobile and Embedded (GMAE) based reference user\ninterface called Sato. From these base image types, metadata layers can be added to extend\nfunctionality; layers can provide an additional software stack for an image type, add a board support\npackage (BSP) for additional hardware or even represent a new image type. Using the 1.1 release of\nPoky, named \"edison\", we will show how BitBake uses these recipes and configuration files to generate\nan embedded image.\nFrom a very high level, the build process starts out by setting up the shell environment for the build run.\nThis is done by sourcing a file, oe-init-build-env , that exists in the root of the Poky source tree.\nThis sets up the shell environment, creates an initial customizable set of configuration files and wraps the\nBitBake runtime with a shell script that Poky uses to determine if the minimal system requirements have\nbeen met.\nFor example, one of the things it will look for is the existence of Pseudo, a fakeroot  replacement\ncontributed to the Yocto Project by Wind River Systems. At this point, bitbake core-image-minimal ,\nfor example, should be able to create a fully functional cross-compilation environment and then create a\nLinux image based on the image definition for core-image-minimal  from source as defined in the\nYocto metadata layer.\nYocto\nElizabeth Flanagan\n", "page": 244, "type": "text", "section": "Page 244"}
{"text": "Figure 23.1: High-level overview of Poky task execution\nDuring the creation of our image, BitBake will parse its configuration, include any additional layers,\nclasses, tasks or recipes defined, and begin by creating a weighted dependency chain. This process\nprovides an ordered and weighted task priority map. BitBake then uses this map to determine what\npackages must be built in which order so as to most efficiently fulfill compilation dependencies. Tasks\nneeded by the most other tasks are weighted higher, and thus run earlier during the build process. The\ntask execution queue for our build is created. BitBake also stores the parsed metadata summaries and if,\non subsequent runs, it determines that the metadata has changed, it can re-parse only what has\nchanged. The BitBake scheduler and parser are some of the more interesting architectural designs of\nBitBake and some of the decisions surrounding them and their implementation by BitBake contributors\nwill be discussed later.\nBitBake then runs through its weighted task queue, spawning threads (up to the number defined by\nBB_NUMBER_THREADS  in conf/local.conf ) that begin executing those tasks in the predetermined\norder. The tasks executed during a package's build may be modified, prepended- or appended-to\nthrough its recipe. The basic, default package task order of execution starts by fetching and unpacking\npackage source and then configuring and cross-compiling the unpacked source. The compiled source is\nthen split up into packages and various calculations are made on the compilation result such as the\ncreation of debug package information. The split packages are then packaged into a supported package\nformat; RPM, ipk and deb are supported. BitBake will then use these packages to build the root file\nsystem.\nPoky Build System Concepts\nOne of the most powerful properties of the Poky build system is that every aspect of a build is controlled\nby metadata. Metadata can be loosely grouped into configuration files or package recipes. A recipe is a\ncollection of non-executable metadata used by BitBake to set variables or define additional build-time\ntasks. A recipe contains fields such as the recipe description, the recipe version, the license of the\npackage and the upstream source repository. It may also indicate that the build process uses\nautotools , make , distutils  or any other build process, in which case the basic functionality can\nbe defined by classes it inherits from the OE-Core layer's class definitions in ./meta/classes .\nAdditional tasks can also be defined, as well as task prerequisites. BitBake also supports both\n_prepend  and _append  as a method of extending task functionality by injecting code indicated by\nusing prepend or append suffix into the beginning or end of a task.\nConfiguration files can be broken down into two types. There are those that configure BitBake and the\noverall build run, and those that configure the various layers Poky uses to create different configurations\n", "page": 245, "type": "text", "section": "Page 245"}
{"text": "of a target image. A layer is any grouping of metadata that provides some sort of additional functionality.\nThese can be BSP for new devices, additional image types or additional software outside of the core\nlayers. In fact, the core Yocto metadata, meta-yocto , is itself a layer applied on top of the OE-Core\nmetadata layer, meta  which adds additional software and image types to the OE-Core layer.\nAn example of how one would use layering is by creating a NAS device for the Intel n660 (Crownbay),\nusing x32, the new 32-bit native ABI for x86-64, with a custom software layer that adds a user interface.\nGiven the task at hand, we could split this functionality out into layers. At the lowest level we would utilize\na BSP layer for Crownbay that would enable Crownbay-specific hardware functionality, such as video\ndrivers. As we want x32, we would use the experimental meta-x32  layer. The NAS functionality would\nbe layered on top of this by adding the Yocto Project's example NAS layer, meta-baryon . And lastly,\nwe'll use an imaginary layer called meta-myproject , to provide the software and configuration to\ncreate a graphical user interface for configuration of the NAS.\nDuring the setup of the BitBake environment, some initial configuration files are generated by sourcing\noe-build-init-env . These configuration files allow us quite a bit of control over how and what Poky\ngenerates. The first of these configuration files is bblayers.conf . This file is what we will use to add\nadditional layers in order to build our example project.\nHere's an example of a bblayers.conf  file:\n# LAYER_CONF_VERSION is increased each time build/conf/bblayers.conf\n# changes incompatibly\nLCONF_VERSION = \"4\"\nBBFILES ?= \"\"\nBBLAYERS = \" \\\n/home/eflanagan/poky/meta \\\n/home/eflanagan/poky/meta-yocto \\\n/home/eflanagan/poky/meta-intel/crownbay \\\n/home/eflanagan/poky/meta-x32 \\\n/home/eflanagan/poky/meta-baryon\\\n/home/eflanagan/poky/meta-myproject \\\n\"\nThe BitBake layers file, bblayers , defines a variable BBLAYERS  that BitBake uses to look for BitBake\nlayers. In order to fully understand this, we should also look at how our layers are actually constructed.\nUsing meta-baryon  (git://git.yoctoproject.org/meta-baryon) as our example layer, we want to examine\nthe layer configuration file. This file, conf/layer.conf , is what BitBake parses after its initial parsing of\nbblayers.conf . From here it adds additional recipes, classes and configuration to the build.\nFigure 23.2: Example of BitBake layering\nHere's meta-baryon 's layer.conf :\n# Layer configuration for meta-baryon layer\n# Copyright 2011 Intel Corporation\n# We have a conf directory, prepend to BBPATH to prefer our versions\nBBPATH := \"${LAYERDIR}:${BBPATH}\"\n# We have recipes-* directories, add to BBFILES\nBBFILES := \"${BBFILES} ${LAYERDIR}/recipes-*/*/*.bb ${LAYERDIR}/recipes-*/*/*.bbapp\nend\"\n", "page": 246, "type": "text", "section": "Page 246"}
{"text": "BBFILE_COLLECTIONS += \"meta-baryon\"\nBBFILE_PATTERN_meta-baryon := \"^${LAYERDIR}/\"\nBBFILE_PRIORITY_meta-baryon = \"7\"\nAll of the BitBake configuration files help generate BitBake's datastore which is used during the creation\nof the task execution queue. During the beginning of a build, BitBake's BBCooker  class is started. The\ncooker manages the build task execution by baking the recipes. One of the first things the cooker does is\nattempt to load and parse configuration data. Remember, though, that BitBake is looking for two types of\nconfiguration data. In order to tell the build system where it should find this configuration data (and in\nturn where to find recipe metadata), the cooker's parseConfigurationFiles  method is called. With\nfew exceptions, the first configuration file that the cooker looks for is bblayers.conf . After this file is\nparsed, BitBake then parses each layer's layer.conf  file.\nOnce layer configuration files are parsed, parseConfigurationFiles  then parses bitbake.conf\nwhose main purpose is to set up global build time variables, such as directory structure naming for\nvarious rootfs  directories and the initial LDFLAGS  to be used during compile time. Most end users will\nnever touch this file as most anything needed to be changed here would be within a recipe context, as\nopposed to build wide or could be overridden in a configuration file such as local.conf .\nAs this file is parsed, BitBake also includes configuration files that are relative to each layer in\nBBLAYERS  and adds the variables found in those files to its data store.\nHere is a portion of a bitbake.conf  showing included configuration files:\ninclude conf/site.conf\ninclude conf/auto.conf\ninclude conf/local.conf\ninclude conf/build/${BUILD_SYS}.conf\ninclude conf/target/${TARGET_SYS}.conf\ninclude conf/machine/${MACHINE}.conf\nExample BitBake recipe for grep\nDESCRIPTION = \"GNU grep utility\"\nHOMEPAGE = \"http://savannah.gnu.org/projects/grep/\"\nBUGTRACKER = \"http://savannah.gnu.org/bugs/?group=grep\"\nSECTION = \"console/utils\"\nLICENSE = \"GPLv3\"\nLIC_FILES_CHKSUM = \"file://COPYING;md5=8006d9c814277c1bfc4ca22af94b59ee\"\nPR = \"r0\"\nSRC_URI = \"${GNU_MIRROR}/grep/grep-${PV}.tar.gz\"\nSRC_URI[md5sum] = \"03e3451a38b0d615cb113cbeaf252dc0\"\nSRC_URI[sha256sum]=\"e9118eac72ecc71191725a7566361ab7643edfd3364869a47b78dc934a357970\"\ninherit autotools gettext\nEXTRA_OECONF = \"--disable-perl-regexp\"\ndo_configure_prepend\n() {\nrm\n-f ${S}/m4/init.m4\n}\ndo_install () {\n  autotools_do_install\n  install -d ${D}${base_bindir}\n  mv ${D}${bindir}/grep ${D}${base_bindir}/grep.${PN}\n  mv ${D}${bindir}/egrep ${D}${base_bindir}/egrep.${PN}\n  mv ${D}${bindir}/fgrep ${D}${base_bindir}/fgrep.${PN}\n}\npkg_postinst_${PN}() {\n  update-alternatives --install ${base_bindir}/grep grep grep.${PN} 100\n", "page": 247, "type": "text", "section": "Page 247"}
{"text": "  update-alternatives --install ${base_bindir}/egrep egrep egrep.${PN} 100\n  update-alternatives --install ${base_bindir}/fgrep fgrep fgrep.${PN} 100\n}\npkg_prerm_${PN}() {\n  update-alternatives --remove grep grep.${PN}\n  update-alternatives --remove egrep egrep.${PN}\n  update-alternatives --remove fgrep fgrep.${PN}\n}\n23.2. BitBake Architecture\nBefore we delve into some of BitBake's current architectural design, it would help to understand how\nBitBake once worked. In order to fully appreciate how far BitBake has come, we will consider the initial\nversion, BitBake 1.0. In that first release of BitBake, a build's dependency chain was determined based\non recipe dependencies. If something failed during the build of an image, BitBake would move on to the\nnext task and try to build it again later. What this means, obviously, is that builds took a very long time.\nOne of the things BitBake also did is keep each and every variable that a recipe used in one very large\ndictionary. Given the number of recipes and the number of variables and tasks needed to accomplish a\nbuild, BitBake 1.0 was a memory hog. At a time when memory was expensive and systems had much\nless, builds could be painful affairs. It was not unheard of for a system to run out of memory (writing to\nswap!) as it slugged through a long running build. In its first incarnation, while it did the job (sometimes),\nit did it slowly while consuming an enormous amount of resources. Worse, as BitBake 1.0 had no\nconcept of a data persistence cache or shared state, it also had no ability to do incremental builds. If a\nbuild failed, one would have to restart it from scratch.\nA quick diff between the current BitBake version used in Poky \"edison\" 1.13.3 and 1.0 shows the\nimplementation of BitBake's client-server architecture, the data persistence cache, its datastore, a copy-\non-write improvement for the datastore, shared state implementation and drastic improvements in how it\ndetermines task and package dependency chains. This evolution has made it more reliable, more efficient\nand more dynamic. Much of this functionality came out of necessity for quicker, more reliable builds that\nused fewer resources. Three improvements to BitBake that we will examine are the implementation of a\nclient-server architecture, optimizations around BitBake's data storage and work done on how BitBake\ndetermines its build and task dependency chain.\nBitBake IPC\nSince we now know a good deal about how the Poky build system uses configurations, recipes and\nlayers to create embedded images, we're prepared to begin to look under the hood of BitBake and\nexamine how this is all combined. Starting with the core BitBake executable, bitbake/bin/bake , we\ncan begin to see the process BitBake follows as it begins to set up the infrastructure needed to begin a\nbuild. The first item of interest is BitBake's Interprocess Communications (IPC). Initially, BitBake had no\nconcept of a client-server. This functionality was factored into the BitBake design over a period of time in\norder to allow BitBake to run multiple processes during a build, as it was initially single-threaded, and to\nallow different user experiences.\n", "page": 248, "type": "text", "section": "Page 248"}
{"text": "Figure 23.3: Overview of BitBake IPC\nAll Poky builds are begun by starting a user interface instance. The user interface provides a mechanism\nfor logging of build output, build status and build progress, as well as for receiving events from build\ntasks through the BitBake event module. The default user interface used is knotty, BitBake's command\nline interface. Called knotty, or \"(no) tty\", since it handles both ttys and non-ttys, it is one of a few\ninterfaces that are supported. One of these additional user interfaces is Hob. Hob is the graphical\ninterface to BitBake, a kind of \"BitBake commander\". In addition to the typical functions you would see in\nthe knotty user interface, Hob (written by Joshua Lock) brings the ability to modify configuration files, add\nadditional layers and packages, and fully customize a build.\nBitBake user interfaces have the ability to send commands to the next module brought up by the BitBake\nexecutable, the BitBake server. Like the user interface, BitBake also supports multiple different server\ntypes, such as XMLRPC. The default server that most users use when executing BitBake from the knotty\nuser interface is BitBake's process server. After bringing up the server, the BitBake executable brings up\nthe cooker.\nThe cooker is a core portion of BitBake and is where most of the particularly interesting things that occur\nduring a Poky build are called from. The cooker is what manages the parsing of metadata, initiates the\ngeneration of the dependency and task trees, and manages the build. One of the functions of BitBake's\nserver architecture is allowing multiple ways of exposing the command API, indirectly, to the user\ninterface. The command module is the worker of BitBake, running build commands and triggering events\nthat get passed up to the user interface through BitBake's event handler. Once the cooker is brought up\nfrom the BitBake executable, it initializes the BitBake datastore and then begins to parse all of Poky's\nconfiguration files. It then creates the runqueue object, and triggers the build.\nBitBake DataSmart Copy-on-Write Data Storage\nIn BitBake 1.0, all BitBake variables were parsed and stored in one very large dictionary during the\ninitialization of that version's data class. As previously mentioned, this was problematic in that very large\nPython dictionaries are slow on writes and member access, and if the build host runs out of physical\nmemory during the build, we end up using swap. While this is less likely in most systems in late 2011,\nwhen OpenEmbedded and BitBake were first starting up, the average computer's specification usually\nhad less than one or two gigabytes of memory.\nThis was one of the major pain points in early BitBake. Two major issues needed to be worked out in\norder to help increase performance: one was precomputation of the build dependency chain; the other\nwas to reduce the size of data being stored in memory. Much of the data being stored for a recipe\n", "page": 249, "type": "text", "section": "Page 249"}
{"text": "doesn't change from recipe to recipe; for example, with TMPDIR , BB_NUMBER_THREADS  and other\nglobal BitBake variables, having a copy of the entire data environment per recipe stored in memory was\ninefficient. The solution was Tom Ansell's copy-on-write dictionary that \"abuses classes to be nice and\nfast\". BitBake's COW module is both an especially fearless and clever hack. Running python\nBitBake/lib/bb/COW.py  and examining the module will give you an idea of how this copy-on-write\nimplementation works and how BitBake uses it to store data efficiently\nThe DataSmart module, which uses the COW dictionary, stores the data from the initial Poky\nconfiguration, data from .conf  files and .bbclass  files, in a dict as a data object. Each of these\nobjects can contain another data object of just the diff of the data. So, if a recipe changes something\nfrom the initial data configuration, instead of copying the entire configuration in order to localize it, a diff\nof the parent data object is stored at the next layer down in the COW stack. When an attempt is made to\naccess a variable, the data module will use DataSmart to look into the top level of the stack. If the\nvariable is not found it will defer to a lower level of the stack until it does find the variable or throws an\nerror.\nOne of the other interesting things about the DataSmart module centers around variable expansion. As\nBitBake variables can contain executable Python code, one of the things that needs to be done is run the\nvariable through BitBake's bb.codeparser  to ensure that it's valid Python and that it contains no\ncircular references. An example of a variable containing Python code is this example taken from\n./meta/conf/distro/include/tclibc-eglibc.inc :\nLIBCEXTENSION = \"${@[\", '-gnu'][(d.getVar('ABIEXTENSION', True) or \") != \"]}\"\nThis variable is included from one of the OE-Core configuration files,\n./meta/conf/distro/include/defaultsetup.conf , and is used to provide a set of default options\nacross different distro configurations that one would want to lay on top of Poky or OpenEmbedded. This\nfile imports some eglibc -specific variables that are set dependent on the value of another BitBake\nvariable ABIEXTENSION . During the creation of the datastore, the Python code within this variable\nneeds to be parsed and validated to ensure tasks that use this variable will not fail.\nBitBake Scheduler\nOnce BitBake has parsed the configuration and created its datastore, it needs to parse the recipes\nrequired for the image and produce a build chain. This is one of the more substantial improvements to\nBitBake. Originally, BitBake took its build priorities from a recipe. If a recipe had a DEPENDS , it would try\nto figure out what to build in order to satisfy that dependency. If a task failed because it lacked a\nprerequisite needed for its buildout, it was simply put to the side and attempted later. This had obvious\ndrawbacks, both in efficiency and reliability.\nAs no precomputed dependency chain was established, task execution order was figured out during the\nbuild run. This limited BitBake to being single-threaded. To give an idea of how painful single-threaded\nBitBake builds can be, the smallest image \"core-image-minimal\" on a standard developer machine in\n2011 (Intel Core i7, 16 gigabytes of DDR3 memory) takes about three or four hours to build a complete\ncross-compilation toolchain and use it to produce packages that are then used to create an image. For\nreference, a build on the same machine with BB_NUMBER_THREADS  at 14 and PARALLEL_MAKE  set to \"-\nj 12\" takes about 30 to 40 minutes. As one could imagine, running single-threaded with no precomputed\norder of task execution on slower hardware that had less memory with a large portion wasted by\nduplicate copies of the entire datastore took much longer.\nDependencies\nWhen we talk of build dependencies, we need to make a distinction between the various types. A build\ndependency, or DEPENDS , is something we require as a prerequisite so that Poky can build the required\npackage, whereas a runtime dependency, RDEPENDS , requires that the image the package is to be\ninstalled on also contains the package listed as an RDEPENDS . Take, for example, the package task-\ncore-boot . If we look at the recipe for it in\nmeta/recipes-core/tasks/task-core-boot.bb\nwe will see two BitBake variables set: RDEPENDS  and DEPENDS . BitBake uses these two fields during\nthe creation of its dependency chain.\nHere is a portion of task-core-boot.bb  showing DEPENDS  and RDEPENDS :\nDEPENDS = \"virtual/kernel\"\n ...\nRDEPENDS_task-core-boot = \"\\\nbase-files \\\nbase-passwd \\\nbusybox \\\ninitscripts \\\n...\n", "page": 250, "type": "text", "section": "Page 250"}
{"text": "Packages aren't the only thing in BitBake with dependencies. Tasks also have their own dependencies.\nWithin the scope of BitBake's runqueue, we recognize four types: internally dependent, DEPENDS\ndependent, RDEPENDS dependent and inter-task dependent.\nInternally dependent tasks are set within a recipe and add a task before and/or after another task. For\nexample, in a recipe, we could add a task called do_deploy  by adding the line addtask deploy\nbefore do_build after do_compile . This would add a dependency for running the do_deploy  task\nprior to do_build  being started, but after do_compile  is completed. DEPENDS  and RDEPENDS\ndependent tasks are tasks that run after a denoted task. For example, if we wanted to run do_deploy\nof a package after the do_install  of its DEPENDS  or RDEPENDS , our recipe would include\ndo_deploy[deptask] = 'do_install'  or do_deploy[rdeptask] = 'do_install' . For inter-task\ndependencies, if we wanted a task to be dependent on a different package's task we would add, using\nthe above example of do_deploy , do_deploy[depends] = \"<target's name>:do_install\" .\nRunQueue\nAs an image build can have hundreds of recipes, each with multiple packages and task, each with its\nown dependency, BitBake is now tasked with trying to sort this out into something it can use as an order\nof execution. After the cooker has gotten the entire list of packages needed to be built from the\ninitialization of the bb.data object, it will begin to create a weighted task map from this data in order to\nproduce an ordered list of tasks it needs to run, called the runqueue. Once the runqueue is created,\nBitBake can begin executing it in order of priority, tasking out each portion to a different thread.\nWithin the provider module, BitBake will first look to see if there is a PREFERRED_PROVIDER for a\ngiven package or image. As more than one recipe can provide a given package and as tasks are defined\nin recipes, BitBake needs to decide which provider of a package it will use. It will sort all the providers of\nthe package, weighting each provider by various criteria. For example, preferred versions of software will\nget a higher priority than others. However, BitBake also takes into account package version as well as\nthe dependencies of other packages. Once it has selected the recipe from which it will derive its\npackage, BitBake will iterate over the DEPENDS and RDEPENDS of that recipe and proceed to compute\nthe providers for those packages. This chain reaction will produce a list of packages needed for image\ngeneration as well as providers for those packages.\nRunqueue now has a full list of all packages that need to be built and a dependency chain. In order to\nbegin execution of the build, the runqueue module now needs to create the TaskData object so it can\nbegin to sort out a weighted task map. It begins by taking each buildable package it has found, splitting\nthe tasks needed to generate that package and weighing each of those tasks based on the number of\npackages that require it. Tasks with a higher weight have more dependents, and therefore are generally\nrun earlier in the build. Once this is complete, the runqueue module then prepares to convert the\nTaskData object into a runqueue.\nThe creation of the runqueue is somewhat complex. BitBake first iterates through the list of task names\nwithin the TaskData object in order to determine task dependencies. As it iterates through TaskData, it\nbegins to build a weighted task map. When it is complete, if it has found no circular dependencies,\nunbuildable tasks or any such problems, it will then order the task map by weight and return a complete\nrunqueue object to the cooker. The cooker will begin to attempt to execute the runqueue, task by task.\nDepending upon image size and computing resources, Poky may take from a half-hour to hours to\ngenerate a cross-compilation toolchain, a package feed and the embedded Linux image specified. It is\nworth noting that from the time of executing bitbake <image_name>  from the command line, the entire\nprocess up to right before the execution of the task execution queue has taken less than a few seconds.\n23.3. Conclusion\nIn my discussions with community members and my own personal observations, I've identified a few\nareas where things should, perhaps, have been done differently, as well as a few valuable lessons. It is\nimportant to note that \"arm chair quarterbacking\" a decade-long development effort is not meant as a\ncriticism of those who've poured their time and effort into a wholly remarkable collection of software. As\ndevelopers, the most difficult part of our job is predicting what we will need years down the road and how\nwe can set up a framework to enable that work now. Few can achieve that without some road bumps.\nThe first lesson is to be sure to develop a written, agreed-upon standards document that is well\nunderstood by the community. It should be designed for maximum flexibility and growth.\nOne place where I've personally run into this issue is with my work in OE-Core's license manifest\ncreation class, especially with my experiences working with the LICENSE variable. As no clearly\ndocumented standard existed for what LICENSE should contain, a review of the many recipes available\nshowed many variations. The various LICENSE strings contained everything from Python abstract syntax\ntree-parsable values to values that one would have little hope of gaining meaningful data from. There was\na convention that was commonly used within the community; however, the convention had many\nvariations, some less correct than others. This wasn't the problem of the developer who wrote the recipe;\nit was a community failure to define a standard.\nAs little prior work was actually done with the LICENSE variable outside of checking for its existence,\nthere was no particular concern about a standard for that variable. Much trouble could have been\navoided had a project-wide agreed-upon standard been developed early on.\n", "page": 251, "type": "text", "section": "Page 251"}
{"text": "This work is made available under the Creative Commons Attribution 3.0 Unported license. Please see\nthe full description of the license for details.\nThe next lesson is a bit more general and speaks to an issue seen not only within the Yocto Project but\nin other large scale projects that are systems-design specific. It is the one of the most important things\ndevelopers can do to limit the amount of effort duplication, refactoring and churn their project encounters:\nspend time\u2014lots of time\u2014on front-end planning and architectural design.\nIf you think you've spent enough time on architectural design, you probably haven't. If you think you\nhaven't spent enough time on architectural design, you definitely haven't. Spending more time on front\nend planning won't stop you from later having to rip apart code or even do major architectural changes,\nbut it will certainly reduce the amount of duplicated effort in the long run.\nDesigning your software to be as modular as possible, knowing that you will end up revisiting areas for\nanything from minor tweaks to major rewrites, will make it so that when you do run into these issues,\ncode rewrites are less hair-raising.\nOne obvious place where this would have helped in the Yocto Project is identifying the needs of end\nusers with low memory systems. Had more thought been put into BitBake's datastore earlier, perhaps we\ncould have predicted the problems associated with the datastore taking up too much memory and dealt\nwith it earlier.\nThe lesson here is that while it is nearly impossible to identify every pain point your project will run into\nduring its lifetime, taking the time to do serious front-end planning will help reduce the effort needed later.\nBitBake, OE-Core and Yocto are all fortunate in this regard as there was a fair amount of architectural\nplanning done early. This enabled us to be able to make major changes to the architecture without too\nmuch pain and suffering.\n23.4. Acknowledgements\nFirst, thank you to Chris Larson, Michael Lauer, and Holger Schurig and the many, many people who\nhave contributed to BitBake, OpenEmbedded, OE-Core and Yocto over the years. Thank you also goes\nto Richard Purdie for his letting me pick his brain, both on historical and technical aspects of OE, and for\nhis constant encouragement and guidance, especially with some of the dark magic of BitBake.\nBack to top\nBack to The Architecture of Open Source Applications.\n", "page": 252, "type": "text", "section": "Page 252"}
{"text": "<>\n\u00d8MQ is a messaging system, or \"message-oriented middleware\", if you will. It's used in environments as\ndiverse as financial services, game development, embedded systems, academic research and aerospace.\nMessaging systems work basically as instant messaging for applications. An application decides to\ncommunicate an event to another application (or multiple applications), it assembles the data to be sent,\nhits the \"send\" button and there we go\u2014the messaging system takes care of the rest.\nUnlike instant messaging, though, messaging systems have no GUI and assume no human beings at the\nendpoints capable of intelligent intervention when something goes wrong. Messaging systems thus have\nto be both fault-tolerant and much faster than common instant messaging.\n\u00d8MQ was originally conceived as an ultra-fast messaging system for stock trading and so the focus was\non extreme optimization. The first year of the project was spent devising benchmarking methodology and\ntrying to define an architecture that was as efficient as possible.\nLater on, approximately in the second year of development, the focus shifted to providing a generic\nsystem for building distributed applications and supporting arbitrary messaging patterns, various transport\nmechanisms, arbitrary language bindings, etc.\nDuring the third year the focus was mainly on improving usability and flattening the learning curve. We've\nadopted the BSD Sockets API, tried to clean up the semantics of individual messaging patterns, and so\non.\nHopefully, this chapter will give an insight into how the three goals above translated into the internal\narchitecture of \u00d8MQ, and provide some tips for those who are struggling with the same problems.\nSince its third year \u00d8MQ has outgrown its codebase; there is an initiative to standardise the wire\nprotocols it uses, and an experimental implementation of a \u00d8MQ-like messaging system inside the Linux\nkernel, etc. These topics are not covered in this book. However, you can check online resources for\nfurther details: http://www.250bpm.com/concepts, http://groups.google.com/group/sp-discuss-group, and\nhttp://www.250bpm.com/hits.\n24.1. Application vs. Library\n\u00d8MQ is a library, not a messaging server. It took us several years working on AMQP protocol, a financial\nindustry attempt to standardise the wire protocol for business messaging\u2014writing a reference\nimplementation for it and participating in several large-scale projects heavily based on messaging\ntechnology\u2014to realise that there's something wrong with the classic client/server model of smart\nmessaging server (broker) and dumb messaging clients.\nOur primary concern at the time was with the performance: If there's a server in the middle, each\nmessage has to pass the network twice (from the sender to the broker and from the broker to the\nreceiver) inducing a penalty in terms of both latency and throughput. Moreover, if all the messages are\npassed through the broker, at some point it's bound to become the bottleneck.\nA secondary concern was related to large-scale deployments: when the deployment crosses\norganisational boundaries the concept of a central authority managing the whole message flow doesn't\napply any more. No company is willing to cede control to a server in different company; there are trade\nsecrets and there's legal liability. The result in practice is that there's one messaging server per company,\nwith hand-written bridges to connect it to messaging systems in other companies. The whole ecosystem\nis thus heavily fragmented, and maintaining a large number of bridges for every company involved\ndoesn't make the situation better. To solve this problem, we need a fully distributed architecture, an\narchitecture where every component can be possibly governed by a different business entity. Given that\nthe unit of management in server-based architecture is the server, we can solve the problem by installing\na separate server for each component. In such a case we can further optimize the design by making the\nserver and the component share the same processes. What we end up with is a messaging library.\n\u00d8MQ was started when we got an idea about how to make messaging work without a central server. It\nrequired turning the whole concept of messaging upside down and replacing the model of an autonomous\nZeroMQ\nMartin S\u00fastrik\n", "page": 253, "type": "text", "section": "Page 253"}
{"text": "centralised store of messages in the center of the network with a \"smart endpoint, dumb network\"\narchitecture based on the end-to-end principle. The technical consequence of that decision was that\n\u00d8MQ, from the very beginning, was a library, not an application.\nIn the meantime we've been able to prove that this architecture is both more efficient (lower latency,\nhigher throughput) and more flexible (it's easy to build arbitrary complex topologies instead of being tied\nto classic hub-and-spoke model).\nOne of the unintended consequences, however, was that opting for the library model improved the\nusability of the product. Over and over again users express their happiness about the fact that they don't\nhave to install and manage a stand-alone messaging server. It turns out that not having a server is a\npreferred option as it cuts operational cost (no need to have a messaging server admin) and improves\ntime-to-market (no need to negotiate the need to run the server with the client, the management or the\noperations team).\nThe lesson learned is that when starting a new project, you should opt for the library design if at all\npossible. It's pretty easy to create an application from a library by invoking it from a trivial program;\nhowever, it's almost impossible to create a library from an existing executable. A library offers much more\nflexibility to the users, at the same time sparing them non-trivial administrative effort.\n24.2. Global State\nGlobal variables don't play well with libraries. A library may be loaded several times in the process but\neven then there's only a single set of global variables. Figure 24.1 shows a \u00d8MQ library being used from\ntwo different and independent libraries. The application then uses both of those libraries.\nFigure 24.1: \u00d8MQ being used by different libraries\nWhen such a situation occurs, both instances of \u00d8MQ access the same variables, resulting in race\nconditions, strange failures and undefined behaviour.\nTo prevent this problem, the \u00d8MQ library has no global variables. Instead, a user of the library is\nresponsible for creating the global state explicitly. The object containing the global state is called context.\nWhile from the user's perspective context looks more or less like a pool of worker threads, from \u00d8MQ's\nperspective it's just an object to store any global state that we happen to need. In the picture above,\nlibA  would have its own context and libB  would have its own as well. There would be no way for\none of them to break or subvert the other one.\nThe lesson here is pretty obvious: Don't use global state in libraries. If you do, the library is likely to\nbreak when it happens to be instantiated twice in the same process.\n24.3. Performance\nWhen \u00d8MQ was started, its primary goal was to optimize performance. Performance of messaging\nsystems is expressed using two metrics: throughput\u2014how many messages can be passed during a given\namount of time; and latency\u2014how long it takes for a message to get from one endpoint to the other.\nWhich metric should we focus on? What's the relationship between the two? Isn't it obvious? Run the\ntest, divide the overall time of the test by number of messages passed and what you get is latency.\nDivide the number of messages by time and what you get is throughput. In other words, latency is the\ninverse value of throughput. Trivial, right?\nInstead of starting coding straight away we spent some weeks investigating the performance metrics in\ndetail and we found out that the relationship between throughput and latency is much more subtle than\nthat, and often the metrics are quite counter-intuitive.\nImagine A sending messages to B. (See Figure 24.2.) The overall time of the test is 6 seconds. There\nare 5 messages passed. Therefore the throughput is 0.83 msgs/sec (5/6) and the latency is 1.2 sec\n(6/5), right?\n", "page": 254, "type": "text", "section": "Page 254"}
{"text": "Figure 24.2: Sending messages from A to B\nHave a look at the diagram again. It takes a different time for each message to get from A to B: 2 sec,\n2.5 sec, 3 sec, 3.5 sec, 4 sec. The average is 3 seconds, which is pretty far away from our original\ncalculation of 1.2 second. This example shows the misconceptions people are intuitively inclined to make\nabout performance metrics.\nNow have a look at the throughput. The overall time of the test is 6 seconds. However, at A it takes just\n2 seconds to send all the messages. From A's perspective the throughput is 2.5 msgs/sec (5/2). At B it\ntakes 4 seconds to receive all messages. So from B's perspective the throughput is 1.25 msgs/sec (5/4).\nNeither of these numbers matches our original calculation of 1.2 msgs/sec.\nTo make a long story short, latency and throughput are two different metrics; that much is obvious. The\nimportant thing is to understand the difference between the two and their mutual relationship. Latency\ncan be measured only between two different points in the system; There's no such thing as latency at\npoint A. Each message has its own latency. You can average the latencies of multiple messages;\nhowever, there's no such thing as latency of a stream of messages.\nThroughput, on the other hand, can be measured only at a single point of the system. There's a\nthroughput at the sender, there's a throughput at the receiver, there's a throughput at any intermediate\npoint between the two, but there's no such thing as overall throughput of the whole system. And\nthroughput make sense only for a set of messages; there's no such thing as throughput of a single\nmessage.\nAs for the relationship between throughput and latency, it turns out there really is a relationship;\nhowever, the formula involves integrals and we won't discuss it here. For more information, read the\nliterature on queueing theory.\nThere are many more pitfalls in benchmarking the messaging systems that we won't go further into. The\nstress should rather be placed on the lesson learned: Make sure you understand the problem you are\nsolving. Even a problem as simple as \"make it fast\" can take lot of work to understand properly. What's\nmore, if you don't understand the problem, you are likely to build implicit assumptions and popular myths\ninto your code, making the solution either flawed or at least much more complex or much less useful than\nit could possibly be.\n24.4. Critical Path\nWe discovered during the optimization process that three factors have a crucial impact on performance:\nNumber of memory allocations\nNumber of system calls\nConcurrency model\nHowever, not every memory allocation or every system call has the same effect on performance. The\nperformance we are interested in in messaging systems is the number of messages we can transfer\nbetween two endpoints during a given amount of time. Alternatively, we may be interested in how long it\ntakes for a message to get from one endpoint to another.\nHowever, given that \u00d8MQ is designed for scenarios with long-lived connections, the time it takes to\nestablish a connection or the time needed to handle a connection error is basically irrelevant. These\nevents happen very rarely and so their impact on overall performance is negligible.\nThe part of a codebase that gets used very frequently, over and over again, is called the critical path;\noptimization should focus on the critical path.\nLet's have a look at an example: \u00d8MQ is not extremely optimized with respect to memory allocations.\nFor example, when manipulating strings, it often allocates a new string for each intermediate phase of the\ntransformation. However, if we look strictly at the critical path\u2014the actual message passing\u2014we'll find out\nthat it uses almost no memory allocations. If messages are small, it's just one memory allocation per 256\nmessages (these messages are held in a single large allocated memory chunk). If, in addition, the\n", "page": 255, "type": "text", "section": "Page 255"}
{"text": "stream of messages is steady, without huge traffic peaks, the number of memory allocations on the\ncritical path drops to zero (the allocated memory chunks are not returned to the system, but re-used over\nand over again).\nLesson learned: optimize where it makes difference. Optimizing pieces of code that are not on the critical\npath is wasted effort.\n24.5. Allocating Memory\nAssuming that all the infrastructure was initialised and a connection between two endpoints has been\nestablished, there's only one thing to allocate when sending a message: the message itself. Thus, to\noptimize the critical path we had to look into how messages are allocated and passed up and down the\nstack.\nIt's common knowledge in the high-performance networking field that the best performance is achieved\nby carefully balancing the cost of message allocation and the cost of message copying (for example,\nhttp://hal.inria.fr/docs/00/29/28/31/PDF/Open-MX-IOAT.pdf: see different handling of \"small\", \"medium\"\nand \"large\" messages). For small messages, copying is much cheaper than allocating memory. It makes\nsense to allocate no new memory chunks at all and instead to copy the message to preallocated memory\nwhenever needed. For large messages, on the other hand, copying is much more expensive than\nmemory allocation. It makes sense to allocate the message once and pass a pointer to the allocated\nblock, instead of copying the data. This approach is called \"zero-copy\".\n\u00d8MQ handles both cases in a transparent manner. A \u00d8MQ message is represented by an opaque\nhandle. The content of very small messages is encoded directly in the handle. So making a copy of the\nhandle actually copies the message data. When the message is larger, it's allocated in a separate buffer\nand the handle contains just a pointer to the buffer. Making a copy of the handle doesn't result in copying\nthe message data, which makes sense when the message is megabytes long (Figure 24.3). It should be\nnoted that in the latter case the buffer is reference-counted so that it can be referenced by multiple\nhandles without the need to copy the data.\nFigure 24.3: Message copying (or not)\nLesson learned: When thinking about performance, don't assume there's a single best solution. It may\nhappen that there are several subclasses of the problem (e.g., small messages vs. large messages),\neach having its own optimal algorithm.\n24.6. Batching\nIt has already been mentioned that the sheer number of system calls in a messaging system can result\nin a performance bottleneck. Actually, the problem is much more generic than that. There's a non-trivial\nperformance penalty associated with traversing the call stack and thus, when creating high-performance\napplications, it's wise to avoid as much stack traversing as possible.\nConsider Figure 24.4. To send four messages, you have to traverse the entire network stack four times\n(i.e., \u00d8MQ, glibc, user/kernel space boundary, TCP implementation, IP implementation, Ethernet layer,\nthe NIC itself and back up the stack again).\nFigure 24.4: Sending four messages\nHowever, if you decide to join those messages into a single batch, there would be only one traversal of\nthe stack (Figure 24.5). The impact on message throughput can be overwhelming: up to two orders of\nmagnitude, especially if the messages are small and hundreds of them can be packed into a single\nbatch.\n", "page": 256, "type": "text", "section": "Page 256"}
{"text": "Figure 24.5: Batching messages\nOn the other hand, batching can have negative impact on latency. Let's take, for example, the well-\nknown Nagle's algorithm, as implemented in TCP. It delays the outbound messages for a certain amount\nof time and merges all the accumulated data into a single packet. Obviously, the end-to-end latency of\nthe first message in the packet is much worse than the latency of the last one. Thus, it's common for\napplications that need consistently low latency to switch Nagle's algorithm off. It's even common to switch\noff batching on all levels of the stack (e.g., NIC's interrupt coalescing feature).\nBut again, no batching means extensive traversing of the stack and results in low message throughput.\nWe seem to be caught in a throughput versus latency dilemma.\n\u00d8MQ tries to deliver consistently low latencies combined with high throughput using the following\nstrategy: when message flow is sparse and doesn't exceed the network stack's bandwidth, \u00d8MQ turns all\nthe batching off to improve latency. The trade-off here is somewhat higher CPU usage\u2014we still have to\ntraverse the stack frequently. However, that isn't considered to be a problem in most cases.\nWhen the message rate exceeds the bandwidth of the network stack, the messages have to be queued\n\u2014stored in memory till the stack is ready to accept them. Queueing means the latency is going to grow.\nIf the message spends one second in the queue, end-to-end latency will be at least one second. What's\neven worse, as the size of the queue grows, latencies will increase gradually. If the size of the queue is\nnot bound, the latency can exceed any limit.\nIt has been observed that even though the network stack is tuned for lowest possible latency (Nagle's\nalgorithm switched off, NIC interrupt coalescing turned off, etc.) latencies can still be dismal because of\nthe queueing effect, as described above.\nIn such situations it makes sense to start batching aggressively. There's nothing to lose as the latencies\nare already high anyway. On the other hand, aggressive batching improves throughput and can empty\nthe queue of pending messages\u2014which in turn means the latency will gradually drop as the queueing\ndelay decreases. Once there are no outstanding messages in the queue, the batching can be turned off\nto improve the latency even further.\nOne additional observation is that the batching should only be done on the topmost level. If the\nmessages are batched there, the lower layers have nothing to batch anyway, and so all the batching\nalgorithms underneath do nothing except introduce additional latency.\nLesson learned: To get optimal throughput combined with optimal response time in an asynchronous\nsystem, turn off all the batching algorithms on the low layers of the stack and batch on the topmost level.\nBatch only when new data are arriving faster than they can be processed.\n24.7. Architecture Overview\nUp to this point we have focused on generic principles that make \u00d8MQ fast. From now on we'll have a\nlook at the actual architecture of the system (Figure 24.6).\n", "page": 257, "type": "text", "section": "Page 257"}
{"text": "Figure 24.6: \u00d8MQ architecture\nThe user interacts with \u00d8MQ using so-called \"sockets\". They are pretty similar to TCP sockets, the main\ndifference being that each socket can handle communication with multiple peers, a bit like unbound UDP\nsockets do.\nThe socket object lives in the user's thread (see the discussion of threading models in the next section).\nAside from that, \u00d8MQ is running multiple worker threads that handle the asynchronous part of the\ncommunication: reading data from the network, enqueueing messages, accepting incoming connections,\netc.\nThere are various objects living in the worker threads. Each of these objects is owned by exactly one\nparent object (ownership is denoted by a simple full line in the diagram). The parent can live in a different\nthread than the child. Most objects are owned directly by sockets; however, there are couple of cases\nwhere an object is owned by an object which is owned by the socket. What we get is a tree of objects,\nwith one such tree per socket. The tree is used during shut down; no object can shut itself down until it\ncloses all its children. This way we can ensure that the shut down process works as expected; for\nexample, that pending outbound messages are pushed to the network prior to terminating the sending\nprocess.\nRoughly speaking, there are two kinds of asynchronous objects; there are objects that are not involved in\nmessage passing and there are objects that are. The former have to do mainly with connection\nmanagement. For example, a TCP listener object listens for incoming TCP connections and creates an\nengine/session object for each new connection. Similarly, a TCP connector object tries to connect to the\nTCP peer and when it succeeds it creates an engine/session object to manage the connection. When\nsuch connection fails, the connector object tries to re-establish it.\nThe latter are objects that are handling data transfer itself. These objects are composed of two parts: the\nsession object is responsible for interacting with the \u00d8MQ socket, and the engine object is responsible\nfor communication with the network. There's only one kind of the session object, but there's a different\nengine type for each underlying protocol \u00d8MQ supports. Thus, we have TCP engines, IPC (inter-process\ncommunication) engines, PGM engines (a reliable multicast protocol, see RFC 3208), etc. The set of\nengines is extensible\u2014in the future we may choose to implement, say, a WebSocket engine or an SCTP\nengine.\nThe sessions are exchanging messages with the sockets. There are two directions to pass messages in\nand each direction is handled by a pipe object. Each pipe is basically a lock-free queue optimized for\nfast passing of messages between threads.\nFinally, there's a context object (discussed in the previous sections but not shown on the diagram) that\nholds the global state and is accessible by all the sockets and all the asynchronous objects.\n24.8. Concurrency Model\nOne of the requirements for \u00d8MQ was to take advantage of multi-core boxes; in other words, to scale\nthe throughput linearly with the number of available CPU cores.\nOur previous experience with messaging systems showed that using multiple threads in a classic way\n(critical sections, semaphores, etc.) doesn't yield much performance improvement. In fact, a multi-\n", "page": 258, "type": "text", "section": "Page 258"}
{"text": "threaded version of a messaging system can be slower than a single-threaded one, even if measured on\na multi-core box. Individual threads are simply spending too much time waiting for each other while, at\nthe same time, eliciting a lot of context switching that slows the system down.\nGiven these problems, we've decided to go for a different model. The goal was to avoid locking entirely\nand let each thread run at full speed. The communication between threads was to be provided via\nasynchronous messages (events) passed between the threads. This, as insiders know, is the classic\nactor model.\nThe idea was to launch one worker thread per CPU core\u2014having two threads sharing the same core\nwould only mean a lot of context switching for no particular advantage. Each internal \u00d8MQ object, such\nas say, a TCP engine, would be tightly bound to a particular worker thread. That, in turn, means that\nthere's no need for critical sections, mutexes, semaphores and the like. Additionally, these \u00d8MQ objects\nwon't be migrated between CPU cores so would thus avoid the negative performance impact of cache\npollution (Figure 24.7).\nFigure 24.7: Multiple worker threads\nThis design makes a lot of traditional multi-threading problems disappear. Nevertheless, there's a need to\nshare the worker thread among many objects, which in turn means there has to be some kind of\ncooperative multitasking. This means we need a scheduler; objects need to be event-driven rather than\nbeing in control of the entire event loop; we have to take care of arbitrary sequences of events, even very\nrare ones; we have to make sure that no object holds the CPU for too long; etc.\nIn short, the whole system has to become fully asynchronous. No object can afford to do a blocking\noperation, because it would not only block itself but also all the other objects sharing the same worker\nthread. All objects have to become, whether explicitly or implicitly, state machines. With hundreds or\nthousands of state machines running in parallel you have to take care of all the possible interactions\nbetween them and\u2014most importantly\u2014of the shutdown process.\nIt turns out that shutting down a fully asynchronous system in a clean way is a dauntingly complex task.\nTrying to shut down a thousand moving parts, some of them working, some idle, some in the process of\nbeing initiated, some of them already shutting down by themselves, is prone to all kinds of race\nconditions, resource leaks and similar. The shutdown subsystem is definitely the most complex part of\n\u00d8MQ. A quick check of the bug tracker indicates that some 30--50% of reported bugs are related to\nshutdown in one way or another.\nLesson learned: When striving for extreme performance and scalability, consider the actor model; it's\nalmost the only game in town in such cases. However, if you are not using a specialised system like\nErlang or \u00d8MQ itself, you'll have to write and debug a lot of infrastructure by hand. Additionally, think,\nfrom the very beginning, about the procedure to shut down the system. It's going to be the most complex\npart of the codebase and if you have no clear idea how to implement it, you should probably reconsider\nusing the actor model in the first place.\n24.9. Lock-Free Algorithms\nLock-free algorithms have been in vogue lately. They are simple mechanisms for inter-thread\ncommunication that don't rely on the kernel-provided synchronisation primitives, such as mutexes or\nsemaphores; rather, they do the synchronisation using atomic CPU operations, such as atomic compare-\nand-swap (CAS). It should be understood that they are not literally lock-free\u2014instead, locking is done\nbehind the scenes on the hardware level.\n\u00d8MQ uses a lock-free queue in pipe objects to pass messages between the user's threads and \u00d8MQ's\nworker threads. There are two interesting aspects to how \u00d8MQ uses the lock-free queue.\nFirst, each queue has exactly one writer thread and exactly one reader thread. If there's a need for 1-\nto- N  communication, multiple queues are created (Figure 24.8). Given that this way the queue doesn't\nhave to take care of synchronising the writers (there's only one writer) or readers (there's only one\nreader) it can be implemented in an extra-efficient way.\n", "page": 259, "type": "text", "section": "Page 259"}
{"text": "Figure 24.8: Queues\nSecond, we realised that while lock-free algorithms were more efficient than classic mutex-based\nalgorithms, atomic CPU operations are still rather expensive (especially when there's contention between\nCPU cores) and doing an atomic operation for each message written and/or each message read was\nslower than we were willing to accept.\nThe way to speed it up\u2014once again\u2014was batching. Imagine you had 10 messages to be written to the\nqueue. It can happen, for example, when you received a network packet containing 10 small messages.\nReceiving a packet is an atomic event; you cannot get half of it. This atomic event results in the need to\nwrite 10 messages to the lock-free queue. There's not much point in doing an atomic operation for each\nmessage. Instead, you can accumulate the messages in a \"pre-write\" portion of the queue that's\naccessed solely by the writer thread, and then flush it using a single atomic operation.\nThe same applies to reading from the queue. Imagine the 10 messages above were already flushed to\nthe queue. The reader thread can extract each message from the queue using an atomic operation.\nHowever, it's overkill; instead, it can move all the pending messages to a \"pre-read\" portion of the queue\nusing a single atomic operation. Afterwards, it can retrieve the messages from the \"pre-read\" buffer one\nby one. \"Pre-read\" is owned and accessed solely by the reader thread and thus no synchronisation\nwhatsoever is needed in that phase.\nThe arrow on the left of Figure 24.9 shows how the pre-write buffer can be flushed to the queue simply\nby modifying a single pointer. The arrow on the right shows how the whole content of the queue can be\nshifted to the pre-read by doing nothing but modifying another pointer.\nFigure 24.9: Lock-free queue\nLesson learned: Lock-free algorithms are hard to invent, troublesome to implement and almost impossible\nto debug. If at all possible, use an existing proven algorithm rather than inventing your own. When\nextreme performance is required, don't rely solely on lock-free algorithms. While they are fast, the\nperformance can be significantly improved by doing smart batching on top of them.\n24.10. API\nThe user interface is the most important part of any product. It's the only part of your program visible to\nthe outside world and if you get it wrong the world will hate you. In end-user products it's either the GUI\nor the command line interface. In libraries it's the API.\nIn early versions of \u00d8MQ the API was based on AMQP's model of exchanges and queues. (See the\nAMQP specification.) From a historical perspective it's interesting to have a look at the white paper from\n2007 that tries to reconcile AMQP with a brokerless model of messaging. I spent the end of 2009\nrewriting it almost from scratch to use the BSD Socket API instead. That was the turning point; \u00d8MQ\nadoption soared from that point on. While before it was a niche product used by a bunch of messaging\nexperts, afterwards it became a handy commonplace tool for anybody. In a year or so the size of the\ncommunity increased tenfold, some 20 bindings to different languages were implemented, etc.\nThe user interface defines the perception of a product. With basically no change to the functionality\u2014just\nby changing the API\u2014\u00d8MQ changed from an \"enterprise messaging\" product to a \"networking\" product.\nIn other words, the perception changed from \"a complex piece of infrastructure for big banks\" to \"hey, this\nhelps me to send my 10-byte-long message from application A to application B\".\nLesson learned: Understand what you want your project to be and design the user interface accordingly.\nHaving a user interface that doesn't align with the vision of the project is a 100% guaranteed way to fail.\nOne of the important aspects of the move to the BSD Sockets API was that it wasn't a revolutionary\nfreshly invented API, but an existing and well-known one. Actually, the BSD Sockets API is one of the\noldest APIs still in active use today; it dates back to 1983 and 4.2BSD Unix. It's been widely used and\nstable for literally decades.\n", "page": 260, "type": "text", "section": "Page 260"}
{"text": "The above fact brings a lot of advantages. Firstly, it's an API that everybody knows, so the learning curve\nis ludicrously flat. Even if you've never heard of \u00d8MQ, you can build your first application in couple of\nminutes thanks to the fact that you are able to reuse your BSD Sockets knowledge.\nSecondly, using a widely implemented API enables integration of \u00d8MQ with existing technologies. For\nexample, exposing \u00d8MQ objects as \"sockets\" or \"file descriptors\" allows for processing TCP, UDP, pipe,\nfile and \u00d8MQ events in the same event loop. Another example: the experimental project to bring \u00d8MQ-\nlike functionality to the Linux kernel turned out to be pretty simple to implement. By sharing the same\nconceptual framework it can re-use a lot of infrastructure already in place.\nThirdly and probably most importantly, the fact that the BSD Sockets API survived almost three decades\ndespite numerous attempts to replace it means that there is something inherently right in the design. BSD\nSockets API designers have\u2014whether deliberately or by chance\u2014made the right design decisions. By\nadopting the API we can automatically share those design decisions without even knowing what they\nwere and what problem they were solving.\nLesson learned: While code reuse has been promoted from time immemorial and pattern reuse joined in\nlater on, it's important to think of reuse in an even more generic way. When designing a product, have a\nlook at similar products. Check which have failed and which have succeeded; learn from the successful\nprojects. Don't succumb to Not Invented Here syndrome. Reuse the ideas, the APIs, the conceptual\nframeworks, whatever you find appropriate. By doing so you are allowing users to reuse their existing\nknowledge. At the same time you may be avoiding technical pitfalls you are not even aware of at the\nmoment.\n24.11. Messaging Patterns\nIn any messaging system, the most important design problem is that of how to provide a way for the user\nto specify which messages are routed to which destinations. There are two main approaches, and I\nbelieve this dichotomy is quite generic and applicable to basically any problem encountered in the\ndomain of software.\nOne approach is to adopt the Unix philosophy of \"do one thing and do it well\". What this means is that\nthe problem domain should be artificially restricted to a small and well-understood area. The program\nshould then solve this restricted problem in a correct and exhaustive way. An example of such approach\nin the messaging area is MQTT. It's a protocol for distributing messages to a set of consumers. It can't\nbe used for anything else (say for RPC) but it is easy to use and does message distribution well.\nThe other approach is to focus on generality and provide a powerful and highly configurable system.\nAMQP is an example of such a system. Its model of queues and exchanges provides the user with the\nmeans to programmatically define almost any routing algorithm they can think of. The trade-off, of course,\nis a lot of options to take care of.\n\u00d8MQ opts for the former model because it allows the resulting product to be used by basically anyone,\nwhile the generic model requires messaging experts to use it. To demonstrate the point, let's have a look\nhow the model affects the complexity of the API. What follows is implementation of RPC client on top of\na generic system (AMQP):\nconnect (\"192.168.0.111\")\nexchange.declare (exchange=\"requests\", type=\"direct\", passive=false,\n    durable=true, no-wait=true, arguments={})\nexchange.declare (exchange=\"replies\", type=\"direct\", passive=false,\n    durable=true, no-wait=true, arguments={})\nreply-queue = queue.declare (queue=\"\", passive=false, durable=false,\n    exclusive=true, auto-delete=true, no-wait=false, arguments={})\nqueue.bind (queue=reply-queue, exchange=\"replies\",\n    routing-key=reply-queue)\nqueue.consume (queue=reply-queue, consumer-tag=\"\", no-local=false,\n    no-ack=false, exclusive=true, no-wait=true, arguments={})\nrequest = new-message (\"Hello World!\")\nrequest.reply-to = reply-queue\nrequest.correlation-id = generate-unique-id ()\nbasic.publish (exchange=\"requests\", routing-key=\"my-service\",\n    mandatory=true, immediate=false)\nreply = get-message ()\nOn the other hand, \u00d8MQ splits the messaging landscape into so-called \"messaging patterns\". Examples\nof the patterns are \"publish/subscribe\", \"request/reply\" or \"parallelised pipeline\". Each messaging pattern\nis completely orthogonal to other patterns and can be thought of as a separate tool.\nWhat follows is the re-implementation of the above application using \u00d8MQ's request/reply pattern. Note\nhow all the option tweaking is reduced to the single step of choosing the right messaging pattern\n(\" REQ \"):\ns = socket (REQ)\ns.connect (\"tcp://192.168.0.111:5555\")\n", "page": 261, "type": "text", "section": "Page 261"}
{"text": "This work is made available under the Creative Commons Attribution 3.0 Unported license. Please see\nthe full description of the license for details.\ns.send (\"Hello World!\")\nreply = s.recv ()\nUp to this point we've argued that specific solutions are better than generic solutions. We want our\nsolution to be as specific as possible. However, at the same time we want to provide our customers with\nas wide a range of functionality as possible. How can we solve this apparent contradiction?\nThe answer consists of two steps:\n1. Define a layer of the stack to deal with a particular problem area (e.g. transport, routing,\npresentation, etc.).\n2. Provide multiple implementations of the layer. There should be a separate non-intersecting\nimplementation for each use case.\nLet's have a look at the example of the transport layer in the Internet stack. It's meant to provide\nservices such as transferring data streams, applying flow control, providing reliability, etc., on the top of\nthe network layer (IP). It does so by defining multiple non-intersecting solutions: TCP for connection-\noriented reliable stream transfer, UDP for connectionless unreliable packet transfer, SCTP for transfer of\nmultiple streams, DCCP for unreliable connections and so on.\nNote that each implementation is completely orthogonal: a UDP endpoint cannot speak to a TCP\nendpoint. Neither can a SCTP endpoint speak to a DCCP endpoint. It means that new implementations\ncan be added to the stack at any moment without affecting the existing portions of the stack. Conversely,\nfailed implementations can be forgotten and discarded without compromising the viability of the transport\nlayer as a whole.\nThe same principle applies to messaging patterns as defined by \u00d8MQ. Messaging patterns form a layer\n(the so-called \"scalability layer\") on top of the transport layer (TCP and friends). Individual messaging\npatterns are implementations of this layer. They are strictly orthogonal\u2014the publish/subscribe endpoint\ncan't speak to the request/reply endpoint, etc. Strict separation between the patterns in turn means that\nnew patterns can be added as needed and that failed experiments with new patterns won't hurt the\nexisting patterns.\nLesson learned: When solving a complex and multi-faceted problem it may turn out that a monolithic\ngeneral-purpose solution may not be the best way to go. Instead, we can think of the problem area as an\nabstract layer and provide multiple implementations of this layer, each focused on a specific well-defined\nuse case. When doing so, delineate the use case carefully. Be sure about what is in the scope and what\nis not. By restricting the use case too aggressively the application of your software may be limited. If you\ndefine the problem too broadly, however, the product may become too complex, blurry and confusing for\nthe users.\n24.12. Conclusion\nAs our world becomes populated with lots of small computers connected via the Internet\u2014mobile\nphones, RFID readers, tablets and laptops, GPS devices, etc.\u2014the problem of distributed computing\nceases to be the domain of academic science and becomes a common everyday problem for every\ndeveloper to tackle. The solutions, unfortunately, are mostly domain-specific hacks. This article\nsummarises our experience with building a large-scale distributed system in a systematic manner. It\nfocuses on problems that are interesting from a software architecture point of view, and we hope that\ndesigners and programmers in the open source community will find it useful.\nBack to top\nBack to The Architecture of Open Source Applications.\n", "page": 262, "type": "text", "section": "Page 262"}
{"text": "<>\n[aut12] Autoconf, 2012.\n[CKP05] Manuel M. T. Chakravarty, Gabriele Keller, and Simon Peyton Jones. Associated type synonyms. In Proceedings of the Tenth ACM SIGPLAN\nInternational Conference on Functional Programming, ICFP'05, pages 241\u2013253. ACM, 2005.\n[ecl12] Eclipse home page, 2012.\n[Ent11a] Enthought. Kiva source code, 2011.\n[Ent11b] Enthought. PDF reference and Adobe extensions to the PDF specification, 2011.\n[For09] Message Passing Interface Forum. MPI: A Message-Passing Interface Standard, Version 2.2, September 2009.\n[GFB+04] Edgar Gabriel, Graham E. Fagg, George Bosilca, Thara Angskun, Jack J. Dongarra, Jeffrey M. Squyres, Vishal Sahay, Prabhanjan Kambadur, Brian\nBarrett, Andrew Lumsdaine, Ralph H. Castain, David J. Daniel, Richard L. Graham, and Timothy S. Woodall. Open MPI: Goals, concept, and design of a next\ngeneration MPI implementation. In Proc. 11th European PVM/MPI Users' Group Meeting, pages 97\u2013104, September 2004.\n[GGHL+96] Al Geist, William Gropp, Steve Huss-Lederman, Andrew Lumsdaine, Ewin Lusk, William Saphir, Tony Skjellum, and Mark Snir. MPI-2: Extending\nthe Message-Passing Interface. In Euro-Par'96 Parallel Processing, pages 128\u2013135. Springer Verlag, 1996.\n[GLP93] A. Gill, J. Launchbury, and S. L. Peyton Jones. A Short Cut to Deforestation. In ACM Conference on Functional Programming and Computer\nArchitecture (FPCA'93). ACM, 1993.\n[HHPW07] Paul Hudak, John Hughes, Simon Peyton Jones, and Philip Wadler. A History of Haskell: being lazy with class. In Proceedings of the third ACM\nSIGPLAN Conference on the History of Programming Languages, pages 12-1\u201312-55. ACM, 2007.\n[Knu86] Donald E. Knuth. Computers & Typesetting B: TeX: The Program. Addison Wesley, 1986.\n[Mar10] The Haskell 2010 Report, 2010. http://www.haskell.org/haskellwiki/Language_and_library_specification#The_Haskell_2010_report.\n[McG07] Paul McGuire. Getting Started with Pyparsing. O'Reilly Media, 2007.\n[Mes93] Message Passing Interface Forum. MPI: A Message Passing Interface. In Proc. Supercomputing'93, pages 878\u2013883. IEEE Computer Society Press,\nNovember 1993.\n[MHJP08] Simon Marlow, Tim Harris, Roshan P. James, and Simon Peyton Jones. Parallel generational-copying garbage collection with a block-structured\nheap. In Proceedings of the 7th International Symposium on Memory Management, ISMM'08, pages 11\u201320. ACM, 2008.\n[MPT04] Simon Marlow, Simon Peyton Jones, and Wolfgang Thaller. Extending the Haskell Foreign Function Interface with Concurrency. In Proceedings of the\nACM SIGPLAN Workshop on Haskell, pages 57\u201368, September 2004.\n[PM02] Simon Peyton Jones and Simon Marlow. Secrets of the Glasgow Haskell Compiler inliner. Journal of Functional Programming, 12:393\u2013434, July 2002.\n[PTH01] Simon L. Peyton Jones, Andrew Tolmach, and Tony Hoare. Playing by the rules: rewriting as a practical optimisation technique in GHC. In Ralf Hinze,\neditor, 2001 Haskell Workshop. ACM SIGPLAN, September 2001.\n[PVWW06] Simon Peyton Jones, Dimitrios Vytiniotis, Stephanie Weirich, and Geoffrey Washburn. Simple unification-based type inference for GADTs. In\nProceedings of the Eleventh ACM SIGPLAN International Conference on Functional Programming, pages 50\u201361. ACM, 2006.\n[Ray] Why GPSes suck, and what to do about it. http://esr.ibiblio.org/?p=801.\n[SCPD07] Martin Sulzmann, Manuel Chakravarty, Simon Peyton Jones, and Kevin Donnelly. System F with type equality coercions. In ACM SIGPLAN\nInternational Workshop on Types in Language Design and Implementation (TLDI'07). ACM, 2007.\n[She06] Maxim Shemanarev. Anti-Grain Geometry: A high quality rendering engine for C++, 2002-2006.\n[SPS+00] Richard M. Stallman, Roland Pesch, Stan Shebs, et al. Debugging with GDB: The GNU Source-Level Debugger. Free Software Foundation, 2000.\nBibliography\n", "page": 263, "type": "text", "section": "Page 263"}
