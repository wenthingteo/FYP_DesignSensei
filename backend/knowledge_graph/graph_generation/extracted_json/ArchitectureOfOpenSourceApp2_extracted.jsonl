{"text": "The Architecture of Open Source Applications\n", "page": 1, "type": "text", "section": "Page 1"}
{"text": "The Architecture of Open Source Applications\nVolume II: Structure, Scale, and a Few More Fearless Hacks\nEdited by Amy Brown & Greg Wilson\n", "page": 3, "type": "text", "section": "Page 3"}
{"text": "The Architecture of Open Source Applications, Volume 2\nEdited by Amy Brown and Greg Wilson\nThis work is licensed under the Creative Commons Attribution 3.0 Unported license (CC BY 3.0). You are free:\n\u2022 to Share\u2014to copy, distribute and transmit the work\n\u2022 to Remix\u2014to adapt the work\nunder the following conditions:\n\u2022 Attribution\u2014you must attribute the work in the manner speci\ufb01ed by the author or licensor (but not in any\nway that suggests that they endorse you or your use of the work).\nwith the understanding that:\n\u2022 Waiver\u2014Any of the above conditions can be waived if you get permission from the copyright holder.\n\u2022 Public Domain\u2014Where the work or any of its elements is in the public domain under applicable law,\nthat status is in no way a\ufb00ected by the license.\n\u2022 Other Rights\u2014In no way are any of the following rights a\ufb00ected by the license:\n\u2013 Your fair dealing or fair use rights, or other applicable copyright exceptions and limitations;\n\u2013 The author\u2019s moral rights;\n\u2013 Rights other persons may have either in the work itself or in how the work is used, such as publicity\nor privacy rights.\n\u2022 Notice\u2014For any reuse or distribution, you must make clear to others the license terms of this work. The\nbest way to do this is with a link to http://creativecommons.org/licenses/by/3.0/.\nTo view a copy of this license, visit http://creativecommons.org/licenses/by/3.0/ or send a letter to\nCreative Commons, 444 Castro Street, Suite 900, Mountain View, California, 94041, USA.\nThe full text of this book is available online at http://www.aosabook.org/.\nAll royalties from its sale will be donated to Amnesty International.\nProduct and company names mentioned herein may be the trademarks of their respective owners.\nWhile every precaution has been taken in the preparation of this book, the editors and authors assume no\nresponsibility for errors or omissions, or for damages resulting from the use of the information contained herein.\nFront cover photo \u00a9James Howe.\nRevision Date: April 10, 2013\nISBN: 978-1-105-57181-7\n", "page": 4, "type": "text", "section": "Page 4"}
{"text": "In memory of Dennis Ritchie (1941-2011)\nWe hope he would have enjoyed reading\nwhat we have written.\n", "page": 5, "type": "text", "section": "Page 5"}
{"text": "Contents\nIntroduction\nix\nby Amy Brown and Greg Wilson\n1\nScalable Web Architecture and Distributed Systems\n1\nby Kate Matsudaira\n2\nFirefox Release Engineering\n23\nby Chris AtLee, Lukas Blakk, John O\u2019Duinn, and Armen Zambrano Gasparnian\n3\nFreeRTOS\n39\nby Christopher Svec\n4\nGDB\n53\nby Stan Shebs\n5\nThe Glasgow Haskell Compiler\n67\nby Simon Marlow and Simon Peyton Jones\n6\nGit\n89\nby Susan Potter\n7\nGPSD\n101\nby Eric Raymond\n8\nThe Dynamic Language Runtime and the Iron Languages\n113\nby Je\ufb00Hardy\n9\nITK\n127\nby Luis Ib\u00e1\u00f1ez and Brad King\n10 GNU Mailman\n149\nby Barry Warsaw\n11 matplotlib\n165\nby John Hunter and Michael Droettboom\n12 MediaWiki\n179\nby Sumana Harihareswara and Guillaume Paumier\n", "page": 7, "type": "text", "section": "Page 7"}
{"text": "13 Moodle\n195\nby Tim Hunt\n14 nginx\n211\nby Andrew Alexeev\n15 Open MPI\n225\nby Je\ufb00rey M. Squyres\n16 OSCAR\n239\nby Jennifer Ruttan\n17 Processing.js\n251\nby Mike Kamermans\n18 Puppet\n267\nby Luke Kanies\n19 PyPy\n279\nby Benjamin Peterson\n20 SQLAlchemy\n291\nby Michael Bayer\n21 Twisted\n315\nby Jessica McKellar\n22 Yesod\n331\nby Michael Snoyman\n23 Yocto\n347\nby Elizabeth Flanagan\n24 ZeroMQ\n359\nby Martin S\u00fastrik\nviii\nCONTENTS\n", "page": 8, "type": "text", "section": "Page 8"}
{"text": "Introduction\nAmy Brown and Greg Wilson\nIn the introduction to Volume 1 of this series, we wrote:\nBuilding architecture and software architecture have a lot in common, but there is\none crucial di\ufb00erence. While architects study thousands of buildings in their training\nand during their careers, most software developers only ever get to know a handful\nof large programs well... As a result, they repeat one another\u2019s mistakes rather than\nbuilding on one another\u2019s successes... This book is our attempt to change that.\nIn the year since that book appeared, over two dozen people have worked hard to create the sequel\nyou have in your hands. They have done so because they believe, as we do, that software design can\nand should be taught by example\u2014that the best way to learn how think like an expert is to study how\nexperts think. From web servers and compilers through health record management systems to the\ninfrastructure that Mozilla uses to get Firefox out the door, there are lessons all around us. We hope\nthat by collecting some of them together in this book, we can help you become a better developer.\n\u2014 Amy Brown and Greg Wilson\nContributors\nAndrew Alexeev (nginx): Andrew is a co-founder of Nginx, Inc.\u2014the company behind nginx.\nPrior to joining Nginx, Inc. at the beginning of 2011, Andrew worked in the Internet industry and in\na variety of ICT divisions for enterprises. Andrew holds a diploma in Electronics from St. Petersburg\nElectrotechnical University and an executive MBA from Antwerp Management School.\nChris AtLee (Firefox Release Engineering): Chris is loving his job managing Release Engineers\nat Mozilla. He has a BMath in Computer Science from the University of Waterloo. His online\nramblings can be found at http://atlee.ca.\nMichael Bayer (SQLAlchemy): Michael Bayer has been working with open source software and\ndatabases since the mid-1990s. Today he\u2019s active in the Python community, working to spread good\nsoftware practices to an ever wider audience. Follow Mike on Twitter at @zzzeek.\nLukas Blakk (Firefox Release Engineering): Lukas graduated from Toronto\u2019s Seneca College\nwith a bachelor of Software Development in 2009, but started working with Mozilla\u2019s Release\nEngineering team while still a student thanks to Dave Humphrey\u2019s (http://vocamus.net/dave/)\nTopics in Open Source classes. Lukas Blakk\u2019s adventures with open source can be followed on her\nblog at http://lukasblakk.com.\n", "page": 9, "type": "text", "section": "Page 9"}
{"text": "Amy Brown (editorial): Amy worked in the software industry for ten years before quitting to\ncreate a freelance editing and book production business. She has an underused degree in Math from\nthe University of Waterloo. She can be found online at http://www.amyrbrown.ca/.\nMichael Droettboom (matplotlib): Michael Droettboom works for STScI developing science\nand calibration software for the Hubble and James Webb Space Telescopes. He has worked on the\nmatplotlib project since 2007.\nElizabeth Flanagan (Yocto): Elizabeth Flanagan works for the Open Source Technologies Center\nat Intel Corp as the Yocto Project\u2019s Build and Release engineer. She is the maintainer of the Yocto\nAutobuilder and contributes to the Yocto Project and OE-Core. She lives in Portland, Oregon and\ncan be found online at http://www.hacklikeagirl.com.\nJe\ufb00Hardy (Iron Languages): Je\ufb00started programming in high school, which led to a bachelor\u2019s\ndegree in Software Engineering from the University of Alberta and his current position writing\nPython code for Amazon.com in Seattle. He has also led IronPython\u2019s development since 2010. You\ncan \ufb01nd more information about him at http://jdhardy.ca.\nSumana Harihareswara (MediaWiki): Sumana is the community manager for MediaWiki as the\nvolunteer development coordinator for the Wikimedia Foundation. She previously worked with the\nGNOME, Empathy, Telepathy, Miro, and AltLaw projects. Sumana is an advisory board member for\nthe Ada Initiative, which supports women in open technology and culture. She lives in New York\nCity. Her personal site is at http://www.harihareswara.net/.\nTim Hunt (Moodle): Tim Hunt started out as a mathematician, getting as far as a PhD in non-linear\ndynamics from the University of Cambridge before deciding to do something a bit less esoteric with\nhis life. He now works as a Leading Software Developer at the Open University in Milton Keynes,\nUK, working on their learning and teaching systems which are based on Moodle. Since 2006 he has\nbeen the maintainer of the Moodle quiz module and the question bank code, a role he still enjoys.\nFrom 2008 to 2009, Tim spent a year in Australia working at the Moodle HQ o\ufb03ces. He blogs at\nhttp://tjhunt.blogspot.com and can be found @tim_hunt on Twitter.\nJohn Hunter (matplotlib): John Hunter is a Quantitative Analyst at TradeLink Securities. He\nreceived his doctorate in neurobiology at the University of Chicago for experimental and numerical\nmodeling work on synchronization, and continued his work on synchronization processes as a postdoc\nin Neurology working on epilepsy. He left academia for quantitative \ufb01nance in 2005. An avid Python\nprogrammer and lecturer in scienti\ufb01c computing in Python, he is original author and lead developer\nof the scienti\ufb01c visualization package matplotlib.\nLuis Ib\u00e1\u00f1ez (ITK): Luis has worked for 12 years on the development of the Insight Toolkit (ITK),\nan open source library for medical imaging analysis. Luis is a strong supporter of open access and\nthe revival of reproducibility veri\ufb01cation in scienti\ufb01c publishing. Luis has been teaching a course on\nOpen Source Software Practices at Rensselaer Polytechnic Institute since 2007.\nMike Kamermans (Processing.js): Mike started his career in computer science by failing technical\nComputer Science and promptly moved on to getting a master\u2019s degree in Arti\ufb01cial Intelligence,\ninstead. He\u2019s been programming in order not to have to program since 1998, with a focus on getting\npeople the tools they need to get the jobs they need done, done. He has focussed on many other things\nas well, including writing a book on Japanese grammar, and writing a detailed explanation of the math\nbehind B\u00e9zier curves. His under-used home page is at http://pomax.nihongoresources.com.\nLuke Kanies (Puppet): Luke founded Puppet and Puppet Labs in 2005 out of fear and desperation,\nwith the goal of producing better operations tools and changing how we manage systems. He has been\npublishing and speaking on his work in Unix administration since 1997, focusing on development\nsince 2001. He has developed and published multiple simple sysadmin tools and contributed to\nestablished products like Cfengine, and has presented on Puppet and other tools around the world,\nx\nIntroduction\n", "page": 10, "type": "text", "section": "Page 10"}
{"text": "including at OSCON, LISA, Linux.Conf.au, and FOSS.in. His work with Puppet has been an\nimportant part of DevOps and delivering on the promise of cloud computing.\nBrad King (ITK): Brad King joined Kitware as a founding member of the Software Process\ngroup. He earned a PhD in Computer Science from Rensselaer Polytechnic Institute. He is one of the\noriginal developers of the Insight Toolkit (ITK), an open source library for medical imaging analysis.\nAt Kitware Dr. King\u2019s work focuses on methods and tools for open source software development.\nHe is a core developer of CMake and has made contributions to many open source projects including\nVTK and ParaView.\nSimon Marlow (The Glasgow Haskell Compiler): Simon Marlow is a developer at Microsoft\nResearch\u2019s Cambridge lab, and for the last 14 years has been doing research and development using\nHaskell. He is one of the lead developers of the Glasgow Haskell Compiler, and amongst other\nthings is responsible for its runtime system. Recently, Simon\u2019s main focus has been on providing\ngreat support for concurrent and parallel programming with Haskell. Simon can be reached via\n@simonmar on Twitter, or +Simon Marlow on Google+.\nKate Matsudaira (Scalable Web Architecture and Distributed Systems): Kate Matsudaira has\nworked as the VP Engineering/CTO at several technology startups, including currently at Decide,\nand formerly at SEOmoz and Delve Networks (acquired by Limelight). Prior to joining the startup\nworld she spent time as a software engineer and technical lead/manager at Amazon and Microsoft.\nKate has hands-on knowledge and experience with building large scale distributed web systems, big\ndata, cloud computing and technical leadership. Kate has a BS in Computer Science from Harvey\nMudd College, and has completed graduate work at the University of Washington in both Business\nand Computer Science (MS). You can read more on her blog and website http://katemats.com.\nJessica McKellar (Twisted): Jessica is a software engineer from Boston, MA. She is a Twisted\nmaintainer, Python Software Foundation member, and an organizer for the Boston Python user group.\nShe can be found online at http://jesstess.com.\nJohn O\u2019Duinn (Firefox Release Engineering): John has led Mozilla\u2019s Release Engineering group\nsince May 2007. In that time, he\u2019s led work to streamline Mozilla\u2019s release mechanics, improve\ndeveloper productivity\u2014and do it all while also making the lives of Release Engineers better. John\ngot involved in Release Engineering 19 years ago when he shipped software that reintroduced a bug\nthat had been \ufb01xed in a previous release. John\u2019s blog is at http://oduinn.com/.\nGuillaume Paumier (MediaWiki): Guillaume is Technical Communications Manager at the\nWikimedia Foundation, the nonpro\ufb01t behind Wikipedia and MediaWiki. A Wikipedia photographer\nand editor since 2005, Guillaume is the author of a Wikipedia handbook in French. He also holds an\nengineering degree in Physics and a PhD in microsystems for life sciences. His home online is at\nhttp://guillaumepaumier.com.\nBenjamin Peterson (PyPy): Benjamin contributes to CPython and PyPy as well as several Python\nlibraries. In general, he is interested in compilers and interpreters, particularly for dynamic languages.\nOutside of programming, he enjoys music (clarinet, piano, and composition), pure math, German\nliterature, and great food. His website is http://benjamin-peterson.org.\nSimon Peyton Jones (The Glasgow Haskell Compiler): Simon Peyton Jones is a researcher at\nMicrosoft Research Cambridge, before which he was a professor of computer science at Glasgow\nUniversity. Inspired by the elegance of purely-functional programming when he was a student, Simon\nhas focused nearly thirty years of research on pursuing that idea to see where it leads. Haskell is\nhis \ufb01rst baby, and still forms the platform for much of his research. http://research.microsoft.\ncom/~simonpj\nSusan Potter (Git): Susan is a polyglot software developer with a penchant for skepticism. She\nhas been designing, developing and deploying distributed trading services and applications since\nAmy Brown and Greg Wilson\nxi\n", "page": 11, "type": "text", "section": "Page 11"}
{"text": "1996, recently switching to building multi-tenant systems for software \ufb01rms. Susan is a passionate\npower user of Git, Linux, and Vim. You can \ufb01nd her tweeting random thoughts on Erlang, Haskell,\nScala, and (of course) Git @SusanPotter.\nEric Raymond (GPSD): Eric S. Raymond is a wandering anthropologist and trouble-making\nphilosopher. He\u2019s written some code, too. If you\u2019re not laughing by now, why are you reading this\nbook?\nJennifer Ruttan (OSCAR): Jennifer Ruttan lives in Toronto. Since graduating from the University\nof Toronto with a degree in Computer Science, she has worked as a software engineer for Indivica, a\ncompany devoted to improving patient health care through the use of new technology. Follow her on\nTwitter @jenruttan.\nStan Shebs (GDB): Stan has had open source as his day job since 1989, when a colleague at Apple\nneeded a compiler to generate code for an experimental VM and GCC 1.31 was conveniently at hand.\nAfter following up with the oft-disbelieved Mac System 7 port of GCC (it was the experiment\u2019s\ncontrol case), Stan went to Cygnus Support, where he maintained GDB for the FSF and helped on\nmany embedded tools projects. Returning to Apple in 2000, he worked on GCC and GDB for Mac\nOS X. A short time at Mozilla preceded a jump to CodeSourcery, now part of Mentor Graphics,\nwhere he continues to develop new features for GDB. Stan\u2019s professorial tone is explained by his\nPhD in Computer Science from the University of Utah.\nMichael Snoyman (Yesod): Michael Snoyman received his BS in Mathematics from UCLA. After\nworking as an actuary in the US, he moved to Israel and began a career in web development. In order\nto produce high-performance, robust sites quickly, he created the Yesod Web Framework and its\nassociated libraries.\nJe\ufb00rey M. Squyres (Open MPI): Je\ufb00works in the rack server division at Cisco; he is Cisco\u2019s\nrepresentative to the MPI Forum standards body and is a chapter author of the MPI-2 standard. Je\ufb00\nis Cisco\u2019s core software developer in the open source Open MPI project. He has worked in the\nHigh Performance Computing (HPC) \ufb01eld since his early graduate-student days in the mid-1990s.\nAfter some active duty tours in the military, Je\ufb00received his doctorate in Computer Science and\nEngineering from the University of Notre Dame in 2004.\nMartin S\u00fastrik (ZeroMQ): Martin S\u00fastrik is an expert in the \ufb01eld of messaging middleware,\nand participated in the creation and reference implementation of the AMQP standard. He has\nbeen involved in various messaging projects in the \ufb01nancial industry. He is a founder of the \u00d8MQ\nproject, and currently is working on integration of messaging technology with operating systems and\nthe Internet stack. He can be reached at sustrik@250bpm.com, http://www.250bpm.com and on\nTwitter as @sustrik.\nChristopher Svec (FreeRTOS): Chris is an embedded software engineer who currently develops\n\ufb01rmware for low-power wireless chips. In a previous life he designed x86 processors, which comes\nin handy more often than you\u2019d think when working on non-x86 processors. Chris has bachelor\u2019s and\nmaster\u2019s degrees in Electrical and Computer Engineering, both from Purdue University. He lives in\nBoston with his wife and golden retriever. You can \ufb01nd him on the web at http://saidsvec.com.\nBarry Warsaw (Mailman): Barry Warsaw is the project leader for GNU Mailman. He has\nbeen a core Python developer since 1995, and release manager for several Python versions. He\ncurrently works for Canonical as a software engineer on the Ubuntu Platform Foundations\nteam. He can be reached at barry@python.org or @pumpichank on Twitter. His home page is\nhttp://barry.warsaw.us.\nGreg Wilson (editorial): Greg has worked over the past 25 years in high-performance scienti\ufb01c\ncomputing, data visualization, and computer security, and is the author or editor of several computing\nxii\nIntroduction\n", "page": 12, "type": "text", "section": "Page 12"}
{"text": "books (including the 2008 Jolt Award winner Beautiful Code) and two books for children. Greg\nreceived a PhD in Computer Science from the University of Edinburgh in 1993.\nArmen Zambrano Gasparnian (Firefox Release Engineering): Armen has been working for\nMozilla since 2008 as a Release Engineer. He has worked on releases, developers\u2019 infrastructure\noptimization and localization. Armen works with youth at the Church on the Rock, Toronto, and\nhas worked with international Christian non-pro\ufb01ts for years. Armen has a bachelor in Software\nDevelopment from Seneca College and has taken a few years of Computer Science at the University\nof Malaga. He blogs at http://armenzg.blogspot.com.\nAcknowledgments\nWe would like to thank Google for their support of Amy Brown\u2019s work on this project, and Cat\nAllman for arranging it. We would also like to thank all of our technical reviewers:\nJohan Harjono\nJustin Sheehy\nNikita Pchelin\nLaurie McDougall Sookraj\nTom Plaskon\nGreg Lapouchnian\nWill Schroeder\nBill Ho\ufb00man\nAudrey Tang\nJames Crook\nTodd Ritchie\nJosh McCarthy\nAndrew Petersen\nPascal Rapicault\nEric Aderhold\nJonathan Deber\nTrevor Bekolay\nTaavi Burns\nTina Yee\nColin Morris\nChristian Muise\nDavid Scannell\nVictor Ng\nBlake Winton\nKim Moir\nSimon Stewart\nJonathan Dursi\nRichard Barry\nRic Holt\nMaria Khomenko\nErick Dransch\nIan Bull\nEllen Hsiang\nespecially Tavish Armstrong and Trevor Bekolay, without whose above-and-beyond assistance this\nbook would have taken a lot longer to produce. Thanks also to everyone who o\ufb00ered to review but\nwas unable to for various reasons, and to everyone else who helped and supported the production of\nthis book.\nThank you also to James Howe1, who kindly let us use his picture of New York\u2019s Equitable\nBuilding for the cover.\nContributing\nDozens of volunteers worked hard to create this book, but there is still a lot to do. You can help by\nreporting errors, helping to translate the content into other languages, or describing the architecture\nof other open source projects. Please contact us at aosa@aosabook.org if you would like to get\ninvolved.\n1http://jameshowephotography.com/\nxiii\n", "page": 13, "type": "text", "section": "Page 13"}
{"text": "xiv\nIntroduction\n", "page": 14, "type": "text", "section": "Page 14"}
{"text": "[chapter1]\nScalable Web Architecture and Distributed\nSystems\nKate Matsudaira\nOpen source software has become a fundamental building block for some of the biggest websites.\nAnd as those websites have grown, best practices and guiding principles around their architectures\nhave emerged. This chapter seeks to cover some of the key issues to consider when designing large\nwebsites, as well as some of the building blocks used to achieve these goals.\nThis chapter is largely focused on web systems, although some of the material is applicable to\nother distributed systems as well.\n1.1\nPrinciples of Web Distributed Systems Design\nWhat exactly does it mean to build and operate a scalable web site or application? At a primitive\nlevel it\u2019s just connecting users with remote resources via the Internet\u2014the part that makes it scalable\nis that the resources, or access to those resources, are distributed across multiple servers.\nLike most things in life, taking the time to plan ahead when building a web service can help in\nthe long run; understanding some of the considerations and tradeo\ufb00s behind big websites can result\nin smarter decisions at the creation of smaller web sites. Below are some of the key principles that\nin\ufb02uence the design of large-scale web systems:\nAvailability: The uptime of a website is absolutely critical to the reputation and functionality of\nmany companies. For some of the larger online retail sites, being unavailable for even minutes\ncan result in thousands or millions of dollars in lost revenue, so designing their systems to be\nconstantly available and resilient to failure is both a fundamental business and a technology\nrequirement. High availability in distributed systems requires the careful consideration of\nredundancy for key components, rapid recovery in the event of partial system failures, and\ngraceful degradation when problems occur.\nPerformance: Website performance has become an important consideration for most sites. The\nspeed of a website a\ufb00ects usage and user satisfaction, as well as search engine rankings, a\nfactor that directly correlates to revenue and retention. As a result, creating a system that is\noptimized for fast responses and low latency is key.\nReliability: A system needs to be reliable, such that a request for data will consistently return the\nsame data. In the event the data changes or is updated, then that same request should return\n", "page": 15, "type": "text", "section": "Page 15"}
{"text": "the new data. Users need to know that if something is written to the system, or stored, it will\npersist and can be relied on to be in place for future retrieval.\nScalability: When it comes to any large distributed system, size is just one aspect of scale that needs\nto be considered. Just as important is the e\ufb00ort required to increase capacity to handle greater\namounts of load, commonly referred to as the scalability of the system. Scalability can refer\nto many di\ufb00erent parameters of the system: how much additional tra\ufb03c can it handle, how\neasy is it to add more storage capacity, or even how many more transactions can be processed.\nManageability: Designing a system that is easy to operate is another important consideration. The\nmanageability of the system equates to the scalability of operations: maintenance and updates.\nThings to consider for manageability are the ease of diagnosing and understanding problems\nwhen they occur, ease of making updates or modi\ufb01cations, and how simple the system is to\noperate. (I.e., does it routinely operate without failure or exceptions?)\nCost: Cost is an important factor. This obviously can include hardware and software costs, but\nit is also important to consider other facets needed to deploy and maintain the system. The\namount of developer time the system takes to build, the amount of operational e\ufb00ort required\nto run the system, and even the amount of training required should all be considered. Cost is\nthe total cost of ownership.\nEach of these principles provides the basis for decisions in designing a distributed web architecture.\nHowever, they also can be at odds with one another, such that achieving one objective comes at\nthe cost of another. A basic example: choosing to address capacity by simply adding more servers\n(scalability) can come at the price of manageability (you have to operate an additional server) and\ncost (the price of the servers).\nWhen designing any sort of web application it is important to consider these key principles, even\nif it is to acknowledge that a design may sacri\ufb01ce one or more of them.\n1.2\nThe Basics\nWhen it comes to system architecture there are a few things to consider: what are the right pieces,\nhow these pieces \ufb01t together, and what are the right tradeo\ufb00s. Investing in scaling before it is needed\nis generally not a smart business proposition; however, some forethought into the design can save\nsubstantial time and resources in the future.\nThis section is focused on some of the core factors that are central to almost all large web\napplications: services, redundancy, partitions, and handling failure. Each of these factors involves\nchoices and compromises, particularly in the context of the principles described in the previous\nsection. In order to explain these in detail it is best to start with an example.\nExample: Image Hosting Application\nAt some point you have probably posted an image online. For big sites that host and deliver lots of\nimages, there are challenges in building an architecture that is cost-e\ufb00ective, highly available, and\nhas low latency (fast retrieval).\nImagine a system where users are able to upload their images to a central server, and the images\ncan be requested via a web link or API, just like Flickr or Picasa. For the sake of simplicity, let\u2019s\nassume that this application has two key parts: the ability to upload (write) an image to the server,\nand the ability to query for an image. While we certainly want the upload to be e\ufb03cient, we care\nmost about having very fast delivery when someone requests an image (for example, images could\n2\nScalable Web Architecture and Distributed Systems\n", "page": 16, "type": "text", "section": "Page 16"}
{"text": "be requested for a web page or other application). This is very similar functionality to what a web\nserver or Content Delivery Network (CDN) edge server (a server CDN uses to store content in many\nlocations so content is geographically/physically closer to users, resulting in faster performance)\nmight provide.\nOther important aspects of the system are:\n\u2022 There is no limit to the number of images that will be stored, so storage scalability, in terms of\nimage count needs to be considered.\n\u2022 There needs to be low latency for image downloads/requests.\n\u2022 If a user uploads an image, the image should always be there (data reliability for images).\n\u2022 The system should be easy to maintain (manageability).\n\u2022 Since image hosting doesn\u2019t have high pro\ufb01t margins, the system needs to be cost-e\ufb00ective.\nFigure 1.1 is a simpli\ufb01ed diagram of the functionality.\nFigure 1.1: Simpli\ufb01ed architecture diagram for image hosting application\nIn this image hosting example, the system must be perceivably fast, its data stored reliably and\nall of these attributes highly scalable. Building a small version of this application would be trivial\nand easily hosted on a single server; however, that would not be interesting for this chapter. Let\u2019s\nassume that we want to build something that could grow as big as Flickr.\nServices\nWhen considering scalable system design, it helps to decouple functionality and think about each\npart of the system as its own service with a clearly de\ufb01ned interface. In practice, systems designed\nin this way are said to have a Service-Oriented Architecture (SOA). For these types of systems, each\nservice has its own distinct functional context, and interaction with anything outside of that context\ntakes place through an abstract interface, typically the public-facing API of another service.\nKate Matsudaira\n3\n", "page": 17, "type": "text", "section": "Page 17"}
{"text": "Deconstructing a system into a set of complementary services decouples the operation of those\npieces from one another. This abstraction helps establish clear relationships between the service, its\nunderlying environment, and the consumers of that service. Creating these clear delineations can\nhelp isolate problems, but also allows each piece to scale independently of one another. This sort of\nservice-oriented design for systems is very similar to object-oriented design for programming.\nIn our example, all requests to upload and retrieve images are processed by the same server;\nhowever, as the system needs to scale it makes sense to break out these two functions into their own\nservices.\nFast-forward and assume that the service is in heavy use; such a scenario makes it easy to see\nhow longer writes will impact the time it takes to read the images (since they two functions will be\ncompeting for shared resources). Depending on the architecture this e\ufb00ect can be substantial. Even\nif the upload and download speeds are the same (which is not true of most IP networks, since most\nare designed for at least a 3:1 download-speed:upload-speed ratio), read \ufb01les will typically be read\nfrom cache, and writes will have to go to disk eventually (and perhaps be written several times in\neventually consistent situations). Even if everything is in memory or read from disks (like SSDs),\ndatabase writes will almost always be slower than reads1.\nAnother potential problem with this design is that a web server like Apache or lighttpd typically\nhas an upper limit on the number of simultaneous connections it can maintain (defaults are around\n500, but can go much higher) and in high tra\ufb03c, writes can quickly consume all of those. Since reads\ncan be asynchronous, or take advantage of other performance optimizations like gzip compression or\nchunked transfer encoding, the web server can switch serve reads faster and switch between clients\nquickly serving many more requests per second than the max number of connections (with Apache\nand max connections set to 500, it is not uncommon to serve several thousand read requests per\nsecond). Writes, on the other hand, tend to maintain an open connection for the duration for the\nupload, so uploading a 1MB \ufb01le could take more than 1 second on most home networks, so that web\nserver could only handle 500 such simultaneous writes.\nPlanning for this sort of bottleneck makes a good case to split out reads and writes of images\ninto their own services, shown in Figure 1.2. This allows us to scale each of them independently\n(since it is likely we will always do more reading than writing), but also helps clarify what is going\non at each point. Finally, this separates future concerns, which would make it easier to troubleshoot\nand scale a problem like slow reads.\nThe advantage of this approach is that we are able to solve problems independently of one\nanother\u2014we don\u2019t have to worry about writing and retrieving new images in the same context. Both\nof these services still leverage the global corpus of images, but they are free to optimize their own\nperformance with service-appropriate methods (for example, queuing up requests, or caching popular\nimages\u2014more on this below). And from a maintenance and cost perspective each service can scale\nindependently as needed, which is great because if they were combined and intermingled, one could\ninadvertently impact the performance of the other as in the scenario discussed above.\nOf course, the above example can work well when you have two di\ufb00erent endpoints (in fact this\nis very similar to several cloud storage providers\u2019 implementations and Content Delivery Networks).\nThere are lots of ways to address these types of bottlenecks though, and each has di\ufb00erent tradeo\ufb00s.\nFor example, Flickr solves this read/write issue by distributing users across di\ufb00erent shards such\nthat each shard can only handle a set number of users, and as users increase more shards are added to\n1Pole Position, an open source tool for DB benchmarking, http://polepos.org/ and results http://polepos.\nsourceforge.net/results/PolePositionClientServer.pdf.\n4\nScalable Web Architecture and Distributed Systems\n", "page": 18, "type": "text", "section": "Page 18"}
{"text": "Figure 1.2: Splitting out reads and writes\nthe cluster2. In the \ufb01rst example it is easier to scale hardware based on actual usage (the number of\nreads and writes across the whole system), whereas Flickr scales with their user base (but forces the\nassumption of equal usage across users so there can be extra capacity). In the former an outage or\nissue with one of the services brings down functionality across the whole system (no-one can write\n\ufb01les, for example), whereas an outage with one of Flickr\u2019s shards will only a\ufb00ect those users. In the\n\ufb01rst example it is easier to perform operations across the whole dataset\u2014for example, updating the\nwrite service to include new metadata or searching across all image metadata\u2014whereas with the\nFlickr architecture each shard would need to be updated or searched (or a search service would need\nto be created to collate that metadata\u2014which is in fact what they do).\nWhen it comes to these systems there is no right answer, but it helps to go back to the principles\nat the start of this chapter, determine the system needs (heavy reads or writes or both, level of concur-\nrency, queries across the data set, ranges, sorts, etc.), benchmark di\ufb00erent alternatives, understand\nhow the system will fail, and have a solid plan for when failure happens.\nRedundancy\nIn order to handle failure gracefully a web architecture must have redundancy of its services and\ndata. For example, if there is only one copy of a \ufb01le stored on a single server, then losing that server\nmeans losing that \ufb01le. Losing data is seldom a good thing, and a common way of handling it is to\ncreate multiple, or redundant, copies.\nThis same principle also applies to services. If there is a core piece of functionality for an\napplication, ensuring that multiple copies or versions are running simultaneously can secure against\nthe failure of a single node.\n2Presentation on Flickr\u2019s scaling: http://mysqldba.blogspot.com/2008/04/mysql-uc-2007-presentation-file.\nhtml\nKate Matsudaira\n5\n", "page": 19, "type": "text", "section": "Page 19"}
{"text": "Creating redundancy in a system can remove single points of failure and provide a backup or\nspare functionality if needed in a crisis. For example, if there are two instances of the same service\nrunning in production, and one fails or degrades, the system can failover to the healthy copy. Failover\ncan happen automatically or require manual intervention.\nAnother key part of service redundancy is creating a shared-nothing architecture. With this\narchitecture, each node is able to operate independently of one another and there is no central \u201cbrain\u201d\nmanaging state or coordinating activities for the other nodes. This helps a lot with scalability since\nnew nodes can be added without special conditions or knowledge. However, and most importantly,\nthere is no single point of failure in these systems, so they are much more resilient to failure.\nFor example, in our image server application, all images would have redundant copies on another\npiece of hardware somewhere (ideally in a di\ufb00erent geographic location in the event of a catastrophe\nlike an earthquake or \ufb01re in the data center), and the services to access the images would be redundant,\nall potentially servicing requests. (See Figure 1.3.) (Load balancers are a great way to make this\npossible, but there is more on that below).\nFigure 1.3: Image hosting application with redundancy\nPartitions\nThere may be very large data sets that are unable to \ufb01t on a single server. It may also be the case\nthat an operation requires too many computing resources, diminishing performance and making it\nnecessary to add capacity. In either case you have two choices: scale vertically or horizontally.\nScaling vertically means adding more resources to an individual server. So for a very large data\nset, this might mean adding more (or bigger) hard drives so a single server can contain the entire\ndata set. In the case of the compute operation, this could mean moving the computation to a bigger\nserver with a faster CPU or more memory. In each case, vertical scaling is accomplished by making\nthe individual resource capable of handling more on its own.\nTo scale horizontally, on the other hand, is to add more nodes. In the case of the large data\nset, this might be a second server to store parts of the data set, and for the computing resource it\nwould mean splitting the operation or load across some additional nodes. To take full advantage of\n6\nScalable Web Architecture and Distributed Systems\n", "page": 20, "type": "text", "section": "Page 20"}
{"text": "horizontal scaling, it should be included as an intrinsic design principle of the system architecture,\notherwise it can be quite cumbersome to modify and separate out the context to make this possible.\nWhen it comes to horizontal scaling, one of the more common techniques is to break up your\nservices into partitions, or shards. The partitions can be distributed such that each logical set of\nfunctionality is separate; this could be done by geographic boundaries, or by another criteria like\nnon-paying versus paying users. The advantage of these schemes is that they provide a service or\ndata store with added capacity.\nIn our image server example, it is possible that the single \ufb01le server used to store images could\nbe replaced by multiple \ufb01le servers, each containing its own unique set of images. (See Figure 1.4.)\nSuch an architecture would allow the system to \ufb01ll each \ufb01le server with images, adding additional\nservers as the disks become full. The design would require a naming scheme that tied an image\u2019s\n\ufb01lename to the server containing it. An image\u2019s name could be formed from a consistent hashing\nscheme mapped across the servers. Or alternatively, each image could be assigned an incremental\nID, so that when a client makes a request for an image, the image retrieval service only needs to\nmaintain the range of IDs that are mapped to each of the servers (like an index).\nFigure 1.4: Image hosting application with redundancy and partitioning\nOf course there are challenges distributing data or functionality across multiple servers. One of\nthe key issues is data locality; in distributed systems the closer the data to the operation or point of\ncomputation, the better the performance of the system. Therefore it is potentially problematic to\nhave data spread across multiple servers, as any time it is needed it may not be local, forcing the\nservers to perform a costly fetch of the required information across the network.\nAnother potential issue comes in the form of inconsistency. When there are di\ufb00erent services\nreading and writing from a shared resource, potentially another service or data store, there is the\nchance for race conditions\u2014where some data is supposed to be updated, but the read happens prior\nto the update\u2014and in those cases the data is inconsistent. For example, in the image hosting scenario,\na race condition could occur if one client sent a request to update the dog image with a new title,\nchanging it from \u201cDog\u201d to \u201cGizmo\u201d, but at the same time another client was reading the image. In\nthat circumstance it is unclear which title, \u201cDog\u201d or \u201cGizmo\u201d, would be the one received by the\nsecond client.\nKate Matsudaira\n7\n", "page": 21, "type": "text", "section": "Page 21"}
{"text": "There are certainly some obstacles associated with partitioning data, but partitioning allows each\nproblem to be split\u2014by data, load, usage patterns, etc.\u2014into manageable chunks. This can help\nwith scalability and manageability, but is not without risk. There are lots of ways to mitigate risk\nand handle failures; however, in the interest of brevity they are not covered in this chapter. If you are\ninterested in reading more, you can check out my blog post on fault tolerance and monitoring3.\n1.3\nThe Building Blocks of Fast and Scalable Data Access\nHaving covered some of the core considerations in designing distributed systems, let\u2019s now talk\nabout the hard part: scaling access to the data.\nMost simple web applications, for example, LAMP stack applications, look something like\nFigure 1.5.\nFigure 1.5: Simple web applications\nAs they grow, there are two main challenges: scaling access to the app server and to the database.\nIn a highly scalable application design, the app (or web) server is typically minimized and often\nembodies a shared-nothing architecture. This makes the app server layer of the system horizontally\nscalable. As a result of this design, the heavy lifting is pushed down the stack to the database server\nand supporting services; it\u2019s at this layer where the real scaling and performance challenges come\ninto play.\nThe rest of this chapter is devoted to some of the more common strategies and methods for\nmaking these types of services fast and scalable by providing fast access to data.\nFigure 1.6: Oversimpli\ufb01ed web application\nMost systems can be oversimpli\ufb01ed to Figure 1.6. This is a great place to start. If you have a lot\nof data, you want fast and easy access, like keeping a stash of candy in the top drawer of your desk.\nThough overly simpli\ufb01ed, the previous statement hints at two hard problems: scalability of storage\nand fast access of data.\nFor the sake of this section, let\u2019s assume you have many terabytes (TB) of data and you want\nto allow users to access small portions of that data at random. (See Figure 1.7.) This is similar to\nlocating an image \ufb01le somewhere on the \ufb01le server in the image application example.\n3http://katemats.com/2011/11/13/distributed-systems-basics-handling-failure-fault-tolerance-and-\nmonitoring/\n8\nScalable Web Architecture and Distributed Systems\n", "page": 22, "type": "text", "section": "Page 22"}
{"text": "Figure 1.7: Accessing speci\ufb01c data\nThis is particularly challenging because it can be very costly to load TBs of data into memory; this\ndirectly translates to disk IO. Reading from disk is many times slower than from memory\u2014memory\naccess is as fast as Chuck Norris, whereas disk access is slower than the line at the DMV. This\nspeed di\ufb00erence really adds up for large data sets; in real numbers memory access is as little as 6\ntimes faster for sequential reads, or 100,000 times faster for random reads4, than reading from disk.\nMoreover, even with unique IDs, solving the problem of knowing where to \ufb01nd that little bit of data\ncan be an arduous task. It\u2019s like trying to get that last Jolly Rancher from your candy stash without\nlooking.\nThankfully there are many options that you can employ to make this easier; four of the more\nimportant ones are caches, proxies, indexes and load balancers. The rest of this section discusses\nhow each of these concepts can be used to make data access a lot faster.\nCaches\nCaches take advantage of the locality of reference principle: recently requested data is likely to be\nrequested again. They are used in almost every layer of computing: hardware, operating systems,\nweb browsers, web applications and more. A cache is like short-term memory: it has a limited\namount of space, but is typically faster than the original data source and contains the most recently\naccessed items. Caches can exist at all levels in architecture, but are often found at the level nearest to\nthe front end, where they are implemented to return data quickly without taxing downstream levels.\nHow can a cache be used to make your data access faster in our API example? In this case, there\nare a couple of places you can insert a cache. One option is to insert a cache on your request layer\nnode, as in Figure 1.8.\nPlacing a cache directly on a request layer node enables the local storage of response data. Each\ntime a request is made to the service, the node will quickly return local, cached data if it exists. If it\nis not in the cache, the request node will query the data from disk. The cache on one request layer\nnode could also be located both in memory (which is very fast) and on the node\u2019s local disk (faster\nthan going to network storage).\nWhat happens when you expand this to many nodes? As you can see in Figure 1.9, if the request\nlayer is expanded to multiple nodes, it\u2019s still quite possible to have each node host its own cache.\nHowever, if your load balancer randomly distributes requests across the nodes, the same request\n4The Pathologies of Big Data, http://queue.acm.org/detail.cfm?id=1563874.\nKate Matsudaira\n9\n", "page": 23, "type": "text", "section": "Page 23"}
{"text": "Figure 1.8: Inserting a cache on your request layer node\nFigure 1.9: Multiple caches\nwill go to di\ufb00erent nodes, thus increasing cache misses. Two choices for overcoming this hurdle are\nglobal caches and distributed caches.\nGlobal Cache\nA global cache is just as it sounds: all the nodes use the same single cache space. This involves\nadding a server, or \ufb01le store of some sort, faster than your original store and accessible by all the\n10\nScalable Web Architecture and Distributed Systems\n", "page": 24, "type": "text", "section": "Page 24"}
{"text": "request layer nodes. Each of the request nodes queries the cache in the same way it would a local\none. This kind of caching scheme can get a bit complicated because it is very easy to overwhelm a\nsingle cache as the number of clients and requests increase, but is very e\ufb00ective in some architectures\n(particularly ones with specialized hardware that make this global cache very fast, or that have a\n\ufb01xed dataset that needs to be cached).\nThere are two common forms of global caches depicted in the diagrams. In Figure 1.10, when a\ncached response is not found in the cache, the cache itself becomes responsible for retrieving the\nmissing piece of data from the underlying store. In Figure 1.11 it is the responsibility of request\nnodes to retrieve any data that is not found in the cache.\nFigure 1.10: Global cache where cache is responsible for retrieval\nFigure 1.11: Global cache where request nodes are responsible for retrieval\nThe majority of applications leveraging global caches tend to use the \ufb01rst type, where the cache\nitself manages eviction and fetching data to prevent a \ufb02ood of requests for the same data from the\nKate Matsudaira\n11\n", "page": 25, "type": "text", "section": "Page 25"}
{"text": "clients. However, there are some cases where the second implementation makes more sense. For\nexample, if the cache is being used for very large \ufb01les, a low cache hit percentage would cause the\ncache bu\ufb00er to become overwhelmed with cache misses; in this situation it helps to have a large\npercentage of the total data set (or hot data set) in the cache. Another example is an architecture\nwhere the \ufb01les stored in the cache are static and shouldn\u2019t be evicted. (This could be because of\napplication requirements around that data latency\u2014certain pieces of data might need to be very fast\nfor large data sets\u2014where the application logic understands the eviction strategy or hot spots better\nthan the cache.)\nDistributed Cache\nIn a distributed cache (Figure 1.12), each of its nodes own part of the cached data, so if a refrigerator\nacts as a cache to the grocery store, a distributed cache is like putting your food in several locations\u2014\nyour fridge, cupboards, and lunch box\u2014convenient locations for retrieving snacks from, without a\ntrip to the store. Typically the cache is divided up using a consistent hashing function, such that if\na request node is looking for a certain piece of data it can quickly know where to look within the\ndistributed cache to determine if that data is available. In this case, each node has a small piece\nof the cache, and will then send a request to another node for the data before going to the origin.\nTherefore, one of the advantages of a distributed cache is the increased cache space that can be had\njust by adding nodes to the request pool.\nFigure 1.12: Distributed cache\nA disadvantage of distributed caching is remedying a missing node. Some distributed caches get\naround this by storing multiple copies of the data on di\ufb00erent nodes; however, you can imagine how\nthis logic can get complicated quickly, especially when you add or remove nodes from the request\n12\nScalable Web Architecture and Distributed Systems\n", "page": 26, "type": "text", "section": "Page 26"}
{"text": "layer. Although even if a node disappears and part of the cache is lost, the requests will just pull\nfrom the origin\u2014so it isn\u2019t necessarily catastrophic!\nThe great thing about caches is that they usually make things much faster (implemented correctly,\nof course!) The methodology you choose just allows you to make it faster for even more requests.\nHowever, all this caching comes at the cost of having to maintain additional storage space, typically\nin the form of expensive memory; nothing is free. Caches are wonderful for making things generally\nfaster, and moreover provide system functionality under high load conditions when otherwise there\nwould be complete service degradation.\nOne example of a popular open source cache is Memcached5 (which can work both as a local\ncache and distributed cache); however, there are many other options (including many language- or\nframework-speci\ufb01c options).\nMemcached is used in many large web sites, and even though it can be very powerful, it is simply\nan in-memory key value store, optimized for arbitrary data storage and fast lookups (O(1)).\nFacebook uses several di\ufb00erent types of caching to obtain their site performance6. They use\n$GLOBALS and APC caching at the language level (provided in PHP at the cost of a function call)\nwhich helps make intermediate function calls and results much faster. (Most languages have these\ntypes of libraries to improve web page performance and they should almost always be used.) Facebook\nthen use a global cache that is distributed across many servers7, such that one function call accessing\nthe cache could make many requests in parallel for data stored on di\ufb00erent Memcached servers.\nThis allows them to get much higher performance and throughput for their user pro\ufb01le data, and\nhave one central place to update data (which is important, since cache invalidation and maintaining\nconsistency can be challenging when you are running thousands of servers).\nNow let\u2019s talk about what to do when the data isn\u2019t in the cache...\nProxies\nAt a basic level, a proxy server is an intermediate piece of hardware/software that receives requests\nfrom clients and relays them to the backend origin servers. Typically, proxies are used to \ufb01lter requests,\nlog requests, or sometimes transform requests (by adding/removing headers, encrypting/decrypting,\nor compression).\nFigure 1.13: Proxy server\nProxies are also immensely helpful when coordinating requests from multiple servers, providing\nopportunities to optimize request tra\ufb03c from a system-wide perspective. One way to use a proxy to\nspeed up data access is to collapse the same (or similar) requests together into one request, and then\nreturn the single result to the requesting clients. This is known as collapsed forwarding.\nImagine there is a request for the same data (let\u2019s call it littleB) across several nodes, and that\npiece of data is not in the cache. If that request is routed thought the proxy, then all of those requests\n5http://memcached.org/\n6Facebook caching and performance, http://sizzo.org/talks/.\n7Scaling memcached at Facebook, http://www.facebook.com/note.php?note_id=39391378919.\nKate Matsudaira\n13\n", "page": 27, "type": "text", "section": "Page 27"}
{"text": "can be collapsed into one, which means we only have to read littleB o\ufb00disk once. (See Figure 1.14.)\nThere is some cost associated with this design, since each request can have slightly higher latency,\nand some requests may be slightly delayed to be grouped with similar ones. But it will improve\nperformance in high load situations, particularly when that same data is requested over and over.\nThis is similar to a cache, but instead of storing the data/document like a cache, it is optimizing the\nrequests or calls for those documents and acting as a proxy for those clients.\nIn a LAN proxy, for example, the clients do not need their own IPs to connect to the Internet, and\nthe LAN will collapse calls from the clients for the same content. It is easy to get confused here\nthough, since many proxies are also caches (as it is a very logical place to put a cache), but not all\ncaches act as proxies.\nFigure 1.14: Using a proxy server to collapse requests\nAnother great way to use the proxy is to not just collapse requests for the same data, but also to\ncollapse requests for data that is spatially close together in the origin store (consecutively on disk).\nEmploying such a strategy maximizes data locality for the requests, which can result in decreased\nrequest latency. For example, let\u2019s say a bunch of nodes request parts of B: partB1, partB2, etc. We\ncan set up our proxy to recognize the spatial locality of the individual requests, collapsing them into\na single request and returning only bigB, greatly minimizing the reads from the data origin. (See\nFigure 1.15.) This can make a really big di\ufb00erence in request time when you are randomly accessing\nacross TBs of data! Proxies are especially helpful under high load situations, or when you have\nlimited caching, since they can essentially batch several requests into one.\nFigure 1.15: Using a proxy to collapse requests for data that is spatially close together\nIt is worth noting that you can use proxies and caches together, but generally it is best to put the\ncache in front of the proxy, for the same reason that it is best to let the faster runners start \ufb01rst in a\n14\nScalable Web Architecture and Distributed Systems\n", "page": 28, "type": "text", "section": "Page 28"}
{"text": "crowded marathon race. This is because the cache is serving data from memory, it is very fast, and it\ndoesn\u2019t mind multiple requests for the same result. But if the cache was located on the other side of\nthe proxy server, then there would be additional latency with every request before the cache, and this\ncould hinder performance.\nIf you are looking at adding a proxy to your systems, there are many options to consider; Squid8\nand Varnish9 have both been road tested and are widely used in many production web sites. These\nproxy solutions o\ufb00er many optimizations to make the most of client-server communication. Installing\none of these as a reverse proxy (explained in the load balancer section below) at the web server layer\ncan improve web server performance considerably, reducing the amount of work required to handle\nincoming client requests.\nIndexes\nUsing an index to access your data quickly is a well-known strategy for optimizing data access\nperformance; probably the most well known when it comes to databases. An index makes the\ntrade-o\ufb00s of increased storage overhead and slower writes (since you must both write the data and\nupdate the index) for the bene\ufb01t of faster reads.\nFigure 1.16: Indexes\nJust as to a traditional relational data store, you can also apply this concept to larger data sets.\nThe trick with indexes is you must carefully consider how users will access your data. In the case of\ndata sets that are many TBs in size, but with very small payloads (e.g., 1 KB), indexes are a necessity\nfor optimizing data access. Finding a small payload in such a large data set can be a real challenge\nsince you can\u2019t possibly iterate over that much data in any reasonable time. Furthermore, it is very\nlikely that such a large data set is spread over several (or many!) physical devices\u2014this means you\n8http://www.squid-cache.org/\n9https://www.varnish-cache.org/\nKate Matsudaira\n15\n", "page": 29, "type": "text", "section": "Page 29"}
{"text": "need some way to \ufb01nd the correct physical location of the desired data. Indexes are the best way to\ndo this.\nAn index can be used like a table of contents that directs you to the location where your data\nlives. For example, let\u2019s say you are looking for a piece of data, part 2 of B\u2014how will you know\nwhere to \ufb01nd it? If you have an index that is sorted by data type\u2014say data A, B, C\u2014it would tell\nyou the location of data B at the origin. Then you just have to seek to that location and read the part\nof B you want. (See Figure 1.16.)\nThese indexes are often stored in memory, or somewhere very local to the incoming client request.\nBerkeley DBs (BDBs) and tree-like data structures are commonly used to store data in ordered lists,\nideal for access with an index.\nOften there are many layers of indexes that serve as a map, moving you from one location to the\nnext, and so forth, until you get the speci\ufb01c piece of data you want. (See Figure 1.17.)\nFigure 1.17: Many layers of indexes\nIndexes can also be used to create several di\ufb00erent views of the same data. For large data sets,\nthis is a great way to de\ufb01ne di\ufb00erent \ufb01lters and sorts without resorting to creating many additional\ncopies of the data.\nFor example, imagine that the image hosting system from earlier is actually hosting images of\nbook pages, and the service allows client queries across the text in those images, searching all the\nbook content about a topic, in the same way search engines allow you to search HTML content. In\nthis case, all those book images take many, many servers to store the \ufb01les, and \ufb01nding one page to\nrender to the user can be a bit involved. First, inverse indexes to query for arbitrary words and word\ntuples need to be easily accessible; then there is the challenge of navigating to the exact page and\nlocation within that book, and retrieving the right image for the results. So in this case the inverted\nindex would map to a location (such as book B), and then B may contain an index with all the words,\nlocations and number of occurrences in each part.\nAn inverted index, which could represent Index1 in the diagram above, might look something\nlike the following\u2014each word or tuple of words provide an index of what books contain them.\nWord(s)\nBook(s)\nbeing awesome\nBook B, Book C, Book D\nalways\nBook C, Book F\nbelieve\nBook B\n16\nScalable Web Architecture and Distributed Systems\n", "page": 30, "type": "text", "section": "Page 30"}
{"text": "The intermediate index would look similar but would contain just the words, location, and\ninformation for book B. This nested index architecture allows each of these indexes to take up less\nspace than if all of that info had to be stored into one big inverted index.\nAnd this is key in large-scale systems because even compressed, these indexes can get quite\nbig and expensive to store. In this system if we assume we have a lot of the books in the world\u2014\n100,000,00010\u2014and that each book is only 10 pages long (to make the math easier), with 250 words\nper page, that means there are 250 billion words. If we assume an average of 5 characters per word,\nand each character takes 8 bits (or 1 byte, even though some characters are 2 bytes), so 5 bytes per\nword, then an index containing only each word once is over a terabyte of storage. So you can see\ncreating indexes that have a lot of other information like tuples of words, locations for the data, and\ncounts of occurrences, can add up very quickly.\nCreating these intermediate indexes and representing the data in smaller sections makes big data\nproblems tractable. Data can be spread across many servers and still accessed quickly. Indexes are a\ncornerstone of information retrieval, and the basis for today\u2019s modern search engines. Of course, this\nsection only scratched the surface, and there is a lot of research being done on how to make indexes\nsmaller, faster, contain more information (like relevancy), and update seamlessly. (There are some\nmanageability challenges with race conditions, and with the sheer number of updates required to add\nnew data or change existing data, particularly in the event where relevancy or scoring is involved).\nBeing able to \ufb01nd your data quickly and easily is important; indexes are an e\ufb00ective and simple\ntool to achieve this.\nLoad Balancers\nFinally, another critical piece of any distributed system is a load balancer. Load balancers are a\nprincipal part of any architecture, as their role is to distribute load across a set of nodes responsible\nfor servicing requests. This allows multiple nodes to transparently service the same function in a\nsystem. (See Figure 1.18.) Their main purpose is to handle a lot of simultaneous connections and\nroute those connections to one of the request nodes, allowing the system to scale to service more\nrequests by just adding nodes.\nFigure 1.18: Load balancer\nThere are many di\ufb00erent algorithms that can be used to service requests, including picking a\nrandom node, round robin, or even selecting the node based on certain criteria, such as memory or\n10Inside Google Books blog post, http://booksearch.blogspot.com/2010/08/books-of-world-stand-up-and-be-\ncounted.html.\nKate Matsudaira\n17\n", "page": 31, "type": "text", "section": "Page 31"}
{"text": "CPU utilization. Load balancers can be implemented as software or hardware appliances. One open\nsource software load balancer that has received wide adoption is HAProxy11.\nIn a distributed system, load balancers are often found at the very front of the system, such that\nall incoming requests are routed accordingly. In a complex distributed system, it is not uncommon\nfor a request to be routed to multiple load balancers as shown in Figure 1.19.\nFigure 1.19: Multiple load balancers\nLike proxies, some load balancers can also route a request di\ufb00erently depending on the type of\nrequest it is. (Technically these are also known as reverse proxies.)\nOne of the challenges with load balancers is managing user-session-speci\ufb01c data. In an e-\ncommerce site, when you only have one client it is very easy to allow users to put things in their\nshopping cart and persist those contents between visits (which is important, because it is much\nmore likely you will sell the product if it is still in the user\u2019s cart when they return). However, if a\nuser is routed to one node for a session, and then a di\ufb00erent node on their next visit, there can be\ninconsistencies since the new node may be missing that user\u2019s cart contents. (Wouldn\u2019t you be upset\nif you put a 6 pack of Mountain Dew in your cart and then came back and it was empty?) One way\naround this can be to make sessions sticky so that the user is always routed to the same node, but\nthen it is very hard to take advantage of some reliability features like automatic failover. In this case,\nthe user\u2019s shopping cart would always have the contents, but if their sticky node became unavailable\nthere would need to be a special case and the assumption of the contents being there would no longer\nbe valid (although hopefully this assumption wouldn\u2019t be built into the application). Of course, this\nproblem can be solved using other strategies and tools in this chapter, like services, and many not\ncovered (like browser caches, cookies, and URL rewriting).\nIf a system only has a couple of a nodes, systems like round robin DNS may make more sense\nsince load balancers can be expensive and add an unneeded layer of complexity. Of course in larger\nsystems there are all sorts of di\ufb00erent scheduling and load-balancing algorithms, including simple\nones like random choice or round robin, and more sophisticated mechanisms that take things like\nutilization and capacity into consideration. All of these algorithms allow tra\ufb03c and requests to be\ndistributed, and can provide helpful reliability tools like automatic failover, or automatic removal of\na bad node (such as when it becomes unresponsive). However, these advanced features can make\nproblem diagnosis cumbersome. For example, when it comes to high load situations, load balancers\nwill remove nodes that may be slow or timing out (because of too many requests), but that only\nexacerbates the situation for the other nodes. In these cases extensive monitoring is important,\nbecause overall system tra\ufb03c and throughput may look like it is decreasing (since the nodes are\nserving less requests) but the individual nodes are becoming maxed out.\nLoad balancers are an easy way to allow you to expand system capacity, and like the other\ntechniques in this article, play an essential role in distributed system architecture. Load balancers\n11http://haproxy.1wt.eu/\n18\nScalable Web Architecture and Distributed Systems\n", "page": 32, "type": "text", "section": "Page 32"}
{"text": "also provide the critical function of being able to test the health of a node, such that if a node is\nunresponsive or over-loaded, it can be removed from the pool handling requests, taking advantage of\nthe redundancy of di\ufb00erent nodes in your system.\nQueues\nSo far we have covered a lot of ways to read data quickly, but another important part of scaling the\ndata layer is e\ufb00ective management of writes. When systems are simple, with minimal processing\nloads and small databases, writes can be predictably fast; however, in more complex systems writes\ncan take an almost non-deterministically long time. For example, data may have to be written several\nplaces on di\ufb00erent servers or indexes, or the system could just be under high load. In the cases where\nwrites, or any task for that matter, may take a long time, achieving performance and availability\nrequires building asynchrony into the system; a common way to do that is with queues.\nFigure 1.20: Synchronous request\nImagine a system where each client is requesting a task to be remotely serviced. Each of these\nclients sends their request to the server, where the server completes the tasks as quickly as possible\nand returns the results to their respective clients. In small systems where one server (or logical\nservice) can service incoming clients just as fast as they come, this sort of situation should work\njust \ufb01ne. However, when the server receives more requests than it can handle, then each client is\nforced to wait for the other clients\u2019 requests to complete before a response can be generated. This is\nan example of a synchronous request, depicted in Figure 1.20.\nThis kind of synchronous behavior can severely degrade client performance; the client is forced\nto wait, e\ufb00ectively performing zero work, until its request can be answered. Adding additional\nservers to address system load does not solve the problem either; even with e\ufb00ective load balancing\nKate Matsudaira\n19\n", "page": 33, "type": "text", "section": "Page 33"}
{"text": "in place it is extremely di\ufb03cult to ensure the even and fair distribution of work required to maximize\nclient performance. Further, if the server handling requests is unavailable, or fails, then the clients\nupstream will also fail. Solving this problem e\ufb00ectively requires abstraction between the client\u2019s\nrequest and the actual work performed to service it.\nFigure 1.21: Using queues to manage requests\nEnter queues. A queue is as simple as it sounds: a task comes in, is added to the queue and then\nworkers pick up the next task as they have the capacity to process it. (See Figure 1.21.) These tasks\ncould represent simple writes to a database, or something as complex as generating a thumbnail\npreview image for a document. When a client submits task requests to a queue they are no longer\nforced to wait for the results; instead they need only acknowledgement that the request was properly\nreceived. This acknowledgement can later serve as a reference for the results of the work when the\nclient requires it.\nQueues enable clients to work in an asynchronous manner, providing a strategic abstraction\nof a client\u2019s request and its response. On the other hand, in a synchronous system, there is no\ndi\ufb00erentiation between request and reply, and they therefore cannot be managed separately. In an\nasynchronous system the client requests a task, the service responds with a message acknowledging\nthe task was received, and then the client can periodically check the status of the task, only requesting\nthe result once it has completed. While the client is waiting for an asynchronous request to be\ncompleted it is free to perform other work, even making asynchronous requests of other services.\nThe latter is an example of how queues and messages are leveraged in distributed systems.\nQueues also provide some protection from service outages and failures. For instance, it is quite\neasy to create a highly robust queue that can retry service requests that have failed due to transient\nserver failures. It is more preferable to use a queue to enforce quality-of-service guarantees than to\nexpose clients directly to intermittent service outages, requiring complicated and often-inconsistent\nclient-side error handling.\nQueues are fundamental in managing distributed communication between di\ufb00erent parts of any\n20\nScalable Web Architecture and Distributed Systems\n", "page": 34, "type": "text", "section": "Page 34"}
{"text": "large-scale distributed system, and there are lots of ways to implement them. There are quite a few\nopen source queues like RabbitMQ12, ActiveMQ13, BeanstalkD14, but some also use services like\nZookeeper15, or even data stores like Redis16.\n1.4\nConclusion\nDesigning e\ufb03cient systems with fast access to lots of data is exciting, and there are lots of great\ntools that enable all kinds of new applications. This chapter covered just a few examples, barely\nscratching the surface, but there are many more\u2014and there will only continue to be more innovation\nin the space.\n12http://www.rabbitmq.com/\n13http://activemq.apache.org/\n14http://kr.github.com/beanstalkd/\n15http://zookeeper.apache.org/\n16http://redis.io/\n21\n", "page": 35, "type": "text", "section": "Page 35"}
{"text": "22\nScalable Web Architecture and Distributed Systems\n", "page": 36, "type": "text", "section": "Page 36"}
{"text": "[chapter2]\nFirefox Release Engineering\nChris AtLee, Lukas Blakk, John O\u2019Duinn, and Armen Zambrano\nGasparnian\nRecently, the Mozilla Release Engineering team has made numerous advances in release automation\nfor Firefox. We have reduced the requirements for human involvement during signing and sending\nnotices to stakeholders, and have automated many other small manual steps, because each manual step\nin the process is an opportunity for human error. While what we have now isn\u2019t perfect, we\u2019re always\nstriving to streamline and automate our release process. Our \ufb01nal goal is to be able to push a button\nand walk away; minimal human intervention will eliminate many of the headaches and do-overs we\nexperienced with our older part-manual, part-automated release processes. In this chapter, we will\nexplore and explain the scripts and infrastructure decisions that comprise the complete Firefox rapid\nrelease system, as of Firefox 10.\nYou\u2019ll follow the system from the perspective of a release-worthy Mercurial changeset as it\nis turned into a release candidate\u2014and then a public release\u2014available to over 450 million daily\nusers worldwide. We\u2019ll start with builds and code signing, then customized partner and localization\nrepacks, the QA process, and how we generate updates for every supported version, platform and\nlocalization. Each of these steps must be completed before the release can be pushed out to Mozilla\nCommunity\u2019s network of mirrors which provide the downloads to our users.\nWe\u2019ll look at some of the decisions that have been made to improve this process; for example, our\nsanity-checking script that helps eliminate much of what used to be vulnerable to human error; our\nautomated signing script; our integration of mobile releases into the desktop release process; patcher,\nwhere updates are created; and AUS (Application Update Service), where updates are served to our\nusers across multiple versions of the software.\nThis chapter describes the mechanics of how we generate release builds for Firefox. Most of this\nchapter details the signi\ufb01cant steps that occur in a release process once the builds start, but there is\nalso plenty of complex cross-group communication to deal with before Release Engineering even\nstarts to generate release builds, so let\u2019s start there.\n2.1\nLook N Ways Before You Start a Release\nWhen we started on the project to improve Mozilla\u2019s release process, we began with the premise that\nthe more popular Firefox became, the more users we would have, and the more attractive a target\nFirefox would become to blackhat hackers looking for security vulnerabilities to exploit. Also, the\nmore popular Firefox became, the more users we would have to protect from a newly discovered\n", "page": 37, "type": "text", "section": "Page 37"}
{"text": "Figure 2.1: Getting from code to \u201cGo to build\u201d\nsecurity vulnerability, so the more important it would be to be able to deliver a security \ufb01x as quickly\nas possible. We even have a term for this: a \u201cchemspill\u201d release1. Instead of being surprised by the\noccasional need for a chemspill release in between our regularly scheduled releases, we decided to\nplan as if every release could be a chemspill release, and designed our release automation accordingly.\nThis mindset has three important consequences:\n1. We do a postmortem after every release, and look to see where things could be made smoother,\neasier, and faster next time. If at all possible, we \ufb01nd and \ufb01x at least one thing, no matter\nhow small, immediately\u2014before the next release. This constant polishing of our release\nautomation means we\u2019re always looking for new ways to rely on less human involvement\nwhile also improving robustness and turnaround time. A lot of e\ufb00ort is spent making our\ntools and processes bulletproof so that \u201crare\u201d events like network hiccups, disk space issues\nor typos made by real live humans are caught and handled as early as possible. Even though\nwe\u2019re already fast enough for regular, non-chemspill releases, we want to reduce the risk of\nany human error in a future release. This is especially true in a chemspill release.\n2. When we do have a chemspill release, the more robust the release automation, the less stressed\nthe humans in Release Engineering are. We\u2019re used to the idea of going as fast as possible\nwith calm precision, and we\u2019ve built tools to do this as safely and robustly as we know how.\nLess stress means more calm and precise work within a well-rehearsed process, which in turn\nhelps chemspill releases go smoothly.\n1Short for \u201cchemical spill\u201d.\n24\nFirefox Release Engineering\n", "page": 38, "type": "text", "section": "Page 38"}
{"text": "3. We created a Mozilla-wide \u201cgo to build\u201d process. When doing a regular (non-chemspill)\nrelease, it\u2019s possible to have everyone look through the same bug triage queries, see clearly\nwhen the last \ufb01x was landed and tested successfully, and reach consensus on when to start\nbuilds. However, in a chemspill release\u2014where minutes matter\u2014keeping track of all the\ndetails of the issue as well as following up bug con\ufb01rmations and \ufb01xes gets very tricky very\nquickly. To reduce complexity and the risk of mistakes, Mozilla now has a full-time person\nto track the readiness of the code for a \u201cgo to build\u201d decision. Changing processes during\na chemspill is risky, so in order to make sure everyone is familiar with the process when\nminutes matter, we use this same process for chemspill and regular releases.\nQA does \nmanual testing \nof bug that \ncaused \nchemspill\n12:50   13:18   13:42     16:16   17:02   17:46   18:02  18:30   21:30      1:02   5:18   6:31         12:38   13:16   14:37   14:59       16:47 \nGo-to-\nbuild \nemail \nfrom \ndriver\nTaggine \nends, builds \nand source \nbundle \nbuilder \ntriggered\nLinux \nl10n \nrepacks \nstart\nQA does \nautomated \ntesting of \nbuilds\nWin32 \nrepacks \nstart\nMac, \nWin32, \nLinux64 \npartner \nrepacks \nstart\nUpdates \ncomplete \nand \ntrigger \nupdate \nverify\nDriver sends \nemail to \npush mobile \nFirefox to \nMarket\nQA signs off \non mobile \nupdates \nfrom Android \nMarket\nQA signs off \non release \nchannel \nupdates\nTagging \nstarts\nAutosign is \nstarted on \nkeymaster \nwhile builds \nare created\nLinux \npartner \nrepacks \nstart\nAndroid \nbuilds are \nsigned \nand \nveri\ufb01ed\nMac \nrepacks \nstart\nLinux64 \nrepacks \nstart\nSigning is \ncompleted \nand l10n \nveri\ufb01cation \nand updates \ntriggered\nQA signs off \non mobile \nbuilds\nUpdates are \npushed to \ninternal \nmirrors and \nQA runs \nautomated \nupdate \nveri\ufb01cation \ntests\nDriver sends \nemail \nrequest to \npush \ndesktop \nFirefox to \nrelease \nchannel\nFigure 2.2: Complete release timeline, using a chemspill as example\n2.2\n\"Go to Build\"\nWho Can Send the \u201cGo to Build\u201d?\nBefore the start of the release, one person is designated to assume responsibility for coordinating the\nentire release. This person needs to attend triage meetings, understand the background context on all\nthe work being landed, referee bug severity disputes fairly, approve landing of late-breaking changes,\nand make tough back-out decisions. Additionally, on the actual release day this person is on point\nfor all communications with the di\ufb00erent groups (developers, QA, Release Engineering, website\ndevelopers, PR, marketing, etc.).\nDi\ufb00erent companies use di\ufb00erent titles for this role. Some titles we\u2019ve heard include Release\nManager, Release Engineer, Program Manager, Project Manager, Product Manager, Product Czar,\nRelease Driver. In this chapter, we will use the term \u201cRelease Coordinator\u201d as we feel it most clearly\nde\ufb01nes the role in our process as described above. The important point here is that the role, and the\n\ufb01nal authority of the role, is clearly understood by everyone before the release starts, regardless of\ntheir background or previous work experiences elsewhere. In the heat of a release day, it is important\nthat everyone knows to abide by, and trust, the coordination decisions that this person makes.\nThe Release Coordinator is the only person outside of Release Engineering who is authorized\nto send \u201cstop builds\u201d emails if a show-stopper problem is discovered. Any reports of suspected\nChris AtLee, Lukas Blakk, John O\u2019Duinn, and Armen Zambrano Gasparnian\n25\n", "page": 39, "type": "text", "section": "Page 39"}
{"text": "show-stopper problems are redirected to the Release Coordinator, who will evaluate, make the \ufb01nal\ngo/no-go decision and communicate that decision to everyone in a timely manner. In the heat of\nthe moment of a release day, we all have to abide by, and trust, the coordination decisions that this\nperson makes.\nHow to Send the \u201cGo to Build\u201d?\nEarly experiments with sending \u201cgo to build\u201d in IRC channels or verbally over the phone led to\nmisunderstandings, occasionally causing problems for the release in progress. Therefore, we now\nrequire that the \u201cgo to build\u201d signal for every release is done by email, to a mailing list that includes\neveryone across all groups involved in release processes. The subject of the email includes \u201cgo to\nbuild\u201d and the explicit product name and version number; for example:\ngo to build Firefox 6.0.1\nSimilarly, if a problem is found in the release, the Release Coordinator will send a new \u201call stop\u201d\nemail to the same mailing list, with a new subject line. We found that it was not su\ufb03cient to just hit\nreply on the most recent email about the release; email threading in some email clients caused some\npeople to not notice the \u201call stop\u201d email if it was way down a long and unrelated thread.\nWhat Is In the \u201cGo to Build\u201d Email?\n1. The exact code to be used in the build; ideally, the URL to the speci\ufb01c change in the source\ncode repository that the release builds are to be created from.\n(a) Instructions like \u201cuse the latest code\u201d are never acceptable; in one release, after the\n\u201cgo to build\u201d email was sent and before builds started, a well-intentioned developer\nlanded a change, without approval, in the wrong branch. The release included that\nunwanted change in the builds. Thankfully the mistake was caught before we shipped,\nbut we did have to delay the release while we did a full stop and rebuilt everything.\n(b) In a time-based version control system like CVS, be fully explicit about the exact\ntime to use; give the time down to seconds, and specify timezone. In one release,\nwhen Firefox was still based on CVS, the Release Coordinator speci\ufb01ed the cuto\ufb00\ntime to be used for the builds but did not give the timezone. By the time Release\nEngineering noticed the missing timezone info, the Release Coordinator was asleep.\nRelease Engineering correctly guessed that the intent was local time (in California),\nbut in a late-night mixup over PDT instead of PST we ended up missing the last critical\nbug \ufb01x. This was caught by QA before we shipped, but we had to stop builds and start\nthe build over using the correct cuto\ufb00time.\n2. A clear sense of the urgency for this particular release. While it sounds obvious, it is important\nwhen handling some important edge cases, so here is a quick summary:\n(a) Some releases are \u201croutine\u201d, and can be worked on in normal working hours. They are\na pre-scheduled release, they are on schedule, and there is no emergency. Of course,\nall release builds need to be created in a timely manner, but there is no need for release\nengineers to pull all-nighters and burn out for a routine release. Instead, we schedule\nthem properly in advance so everything stays on schedule with people working normal\nhours. This keeps people fresh and better able to handle unscheduled urgent work if\nthe need arises.\n26\nFirefox Release Engineering\n", "page": 40, "type": "text", "section": "Page 40"}
{"text": "(b) Some releases are \u201cchemspills\u201d, and are urgent, where minutes matter. These are\ntypically to \ufb01x a published security exploit, or to \ufb01x a newly introduced top-crash\nproblem impacting a large percentage of our user base. Chemspills need to be created\nas quickly as possible and are typically not pre-scheduled releases.\n(c) Some releases change from routine to chemspill or from chemspill to routine. For\nexample, if a security \ufb01x in a routine release was accidentally leaked, it is now a\nchemspill release. If a business requirement like a \u201cspecial sneak preview\u201d release for\nan upcoming conference announcement was delayed for business reasons, the release\nnow changes from chemspill to routine.\n(d) Some releases have di\ufb00erent people holding di\ufb00erent opinions on whether the release\nis normal or urgent, depending on their perspective on the \ufb01xes being shipped in the\nrelease.\nIt is the role of the Release Coordinator to balance all the facts and opinions, reach a decision,\nand then communicate that decision about urgency consistently across all groups. If new information\narrives, the Release Coordinator reassesses, and then communicates the new urgency to all the same\ngroups. Having some groups believe a release is a chemspill, while other groups believe the same\nrelease is routine can be destructive to cross-group cohesion.\nFinally, these emails also became very useful to measure where time was spent during a release.\nWhile they are only accurate to wall-clock time resolution, this accuracy is really helpful when\n\ufb01guring out where next to focus our e\ufb00orts on making things faster. As the old adage goes, before\nyou can improve something, you have to be able to measure it.\nThroughout the beta cycle for Firefox we also do weekly releases from our mozilla-beta\nrepository2. Each one of these beta releases goes through our usual full release automation, and is\ntreated almost identically to our regular \ufb01nal releases. To minimize surprises during a release, our\nintent is to have no new untested changes to release automation or infrastructure by the time we start\nthe \ufb01nal release builds.\n2.3\nTagging, Building, and Source Tarballs\nIn preparation for starting automation, we recently started to use a script, release_sanity.py3, that\nwas originally written by a Release Engineering summer intern. This Python script assists a release\nengineer with double-checking that all con\ufb01gurations for a release match what is checked into our\ntools and con\ufb01guration repositories. It also checks what is in the speci\ufb01ed release code revisions for\nmozilla-release and all the (human) languages for this release, which will be what the builds and\nlanguage repacks are generated from.\nThe script accepts the buildbot con\ufb01g \ufb01les for any release con\ufb01gurations that will be used (such\nas desktop or mobile), the branch to look at (e.g., mozilla-release), the build and version number,\nand the names of the products that are to be built (such as \u201cfennec\u201d or \u201c\ufb01refox\u201d). It will fail if the\nrelease repositories do not match what\u2019s in the con\ufb01gurations, if locale repository changesets don\u2019t\nmatch our shipping locales and localization changeset \ufb01les, or if the release version and build number\ndon\u2019t match what has been given to our build tools with the tag generated using the product, version,\nand build number. If all the tests in the script pass, it will recon\ufb01gure the buildbot master where the\nscript is being run and where release builders will be triggered, and then generate the \u201csend change\u201d\nthat starts the automated release process.\n2http://hg.mozilla.org/releases/mozilla-beta/\n3http://mxr.mozilla.org/build/source/tools/buildbot-helpers/release_sanity.py\nChris AtLee, Lukas Blakk, John O\u2019Duinn, and Armen Zambrano Gasparnian\n27\n", "page": 41, "type": "text", "section": "Page 41"}
{"text": "Figure 2.3: Automated tagging\nAfter a release engineer kicks o\ufb00builders, the \ufb01rst automated step in the Firefox release process is\ntagging all the related source code repositories to record which revision of the source, language reposi-\ntories, and related tools are being used for this version and build number of a release candidate. These\ntags allow us to keep a history of Firefox and Fennec (mobile Firefox) releases\u2019 version and build num-\nbers in our release repositories. For Firefox releases, one example tag set is FIREFOX_10_0_RELEASE\nFIREFOX_10_0_BUILD1 FENNEC_10_0_RELEASE FENNEC_10_0_BUILD1.\nA single Firefox release uses code from about 85 version control repositories that host things\nsuch as the product code, localization strings, release automation code, and helper utilities. Tagging\nall these repositories is critical to ensure that future steps of the release automation are all executed\nusing the same set of revisions. It also has a number of other bene\ufb01ts: Linux distributions and other\ncontributors can reproduce builds with exactly the same code and tools that go into the o\ufb03cial builds,\nand it also records the revisions of source and tools used on a per-release basis for future comparison\nof what changed between releases.\nOnce all the repositories are branched and tagged, a series of dependent builders automatically\nstart up: one builder for each release platform plus a source bundle that includes all source used\nin the release. The source bundle and built installers are all uploaded to the release directory as\nthey become available. This allows anyone to see exactly what code is in a release, and gives a\nsnapshot that would allow us to re-create the builds if we ever needed to (for example, if our VCS\nfailed somehow).\nFor the Firefox build\u2019s source, sometimes we need to import code from an earlier repository.\nFor example, with a beta release this means pulling in the signed-o\ufb00revision from Mozilla-Aurora\n(our more-stable-than-Nightly repository) for Firefox 10.0b1. For a release it means pulling in\nthe approved changes from Mozilla-Beta (typically the same code used for 10.0b6) to the Mozilla-\nRelease repository. This release branch is then created as a named branch whose parent changeset\nis the signed-o\ufb00revision from the \u2018go to build\u2019 provided by the Release Coordinator. The release\nbranch can be used to make release-speci\ufb01c modi\ufb01cations to the source code, such as bumping\n28\nFirefox Release Engineering\n", "page": 42, "type": "text", "section": "Page 42"}
{"text": "version numbers or \ufb01nalizing the set of locales that will be built. If a critical security vulnerability is\ndiscovered in the future that requires an immediate \ufb01x\u2014a chemspill\u2014a minimal set of changes to\naddress the vulnerability will be landed on this relbranch and a new version of Firefox generated and\nreleased from it. When we have to do another round of builds for a particular release, buildN, we\nuse these relbranches to grab the same code that was signed o\ufb00on for \u2018go to build\u2019, which is where\nany changes to that release code will have been landed. The automation starts again and bumps the\ntagging to the new changeset on that relbranch.\nOur tagging process does a lot of operations with local and remote Mercurial repositories. To\nstreamline some of the most common operations we\u2019ve written a few tools to assist us: retry.py4\nand hgtool.py5. retry.py is a simple wrapper that can take a given command and run it, retrying\nseveral times if it fails. It can also watch for exceptional output conditions and retry or report failure\nin those cases. We\u2019ve found it useful to wrap retry.py around most of the commands which can\nfail due to external dependencies. For tagging, the Mercurial operations could fail due to temporary\nnetwork outages, web server issues, or the backend Mercurial server being temporarily overloaded.\nBeing able to automatically retry these operations and continue saves a lot of our time, since we don\u2019t\nhave to manually recover, clean up any fallout and then get the release automation running again.\nhgtool.py is a utility that encapsulates several common Mercurial operations, like cloning,\npulling, and updating with a single invocation. It also adds support for Mercurial\u2019s share extension,\nwhich we use extensively to avoid having several full clones of repositories in di\ufb00erent directories\non the same machine. Adding support for shared local repositories signi\ufb01cantly sped up our tagging\nprocess, since most full clones of the product and locale repositories could be avoided.\nAn important motivation for developing tools like these is making our automation as testable as\npossible. Because tools like hgtool.py are small, single-purpose utilities built on top of reusable\nlibraries, they\u2019re much easier to test in isolation.\nToday our tagging is done in two parallel jobs: one for desktop Firefox which takes around 20\nminutes to complete as it includes tagging 80+ locale repositories, and another for mobile Firefox\nwhich takes around 10 minutes to complete since we have fewer locales currently available for our\nmobile releases. In the future we would like to streamline our release automation process so that we\ntag all the various repositories in parallel. The initial builds can be started as soon as the product\ncode and tools requirement repository is tagged, without having to wait for all the locale repositories\nto be tagged. By the time these builds are \ufb01nished, the rest of the repositories will have been tagged\nso that localization repackages and future steps can be completed. We estimate this can reduce the\ntotal time to have builds ready by 15 minutes.\n2.4\nLocalization Repacks and Partner Repacks\nOnce the desktop builds are generated and uploaded to ftp.mozilla.org, our automation triggers\nthe localization repackaging jobs. A \u201clocalization repack\u201d takes the original build (which contains\nthe en-US locale), unpacks it, replaces the en-US strings with the strings for another locale that we\nare shipping in this release, then repackages all the \ufb01les back up again (this is why we call them\nrepacks). We repeat this for each locale shipping in the release. Originally, we did all repacks serially.\nHowever, as we added more locales, this took a long time to complete, and we had to restart from\nthe beginning if anything failed out mid-way through.\n4http://hg.mozilla.org/build/tools/file/7adc08bd1386/lib/python/util/retry.py\n5http://hg.mozilla.org/build/mozharness/file/a0fce0162fd5/scripts/hgtool.py\nChris AtLee, Lukas Blakk, John O\u2019Duinn, and Armen Zambrano Gasparnian\n29\n", "page": 43, "type": "text", "section": "Page 43"}
{"text": "Figure 2.4: Repacking Firefox for each localization\nNow, we instead split the entire set of repacks into six jobs, each processed concurrently on six\ndi\ufb00erent machines. This approach completes the work in approximately a sixth of the time. This\nalso allows us to redo a subset of repacks if an individual repack fails, without having to redo all\nrepacks. (We could split the repacks into even more, smaller, concurrent jobs, but we found it took\naway too many machines from the pool, which a\ufb00ected other unrelated jobs triggered by developers\non our continuous integration system.)\nThe process for mobile (on Android) is slightly di\ufb00erent, as we produce only two installers: an\nEnglish version and a multi-locale version with over a dozen languages built into the installer instead\nof a separate build per locale. The size of this multi-locale version is an issue, especially with slow\ndownload speeds onto small mobile devices. One proposal for the future is to have other languages\nbe requested on demand as add-ons from addons.mozilla.org.\nIn Figure 2.4, you can see that we currently rely on three di\ufb00erent sources for our locale in-\nformation: shipped_locales, l10_changesets and l10n-changesets_mobile-release.json.\n(There is a plan to move all three into a uni\ufb01ed JSON \ufb01le.) These \ufb01les contain information about the\ndi\ufb00erent localizations we have, and certain platform exceptions. Speci\ufb01cally, for a given localization\nwe need to know which revision of the repository to use for a given release and we need to know if\nthe localization can build on all of our supported platforms (e.g., Japanese for Mac comes from a\ndi\ufb00erent repository all together). Two of these \ufb01les are used for the Desktop releases and one for the\nMobile release (this JSON \ufb01le contains both the list of platforms and the changesets).\nWho decides which languages we ship? First of all, localizers themselves nominate their speci\ufb01c\n30\nFirefox Release Engineering\n", "page": 44, "type": "text", "section": "Page 44"}
{"text": "changeset for a given release. The nominated changeset gets reviewed by Mozilla\u2019s localization team\nand shows up in a web dashboard that lists the changesets needed for each language. The Release\nCoordinator reviews this before sending the \u201cgo to build\u201d email. On the day of a release, we retrieve\nthis list of changesets and we repackage them accordingly.\nBesides localization repackages we also generate partner repackages. These are customized\nbuilds for various partners we have who want to customize the experience for their customers. The\nmain type of changes are custom bookmarks, custom homepage and custom search engines but many\nother things can be changed. These customized builds are generated for the latest Firefox release and\nnot for betas.\n2.5\nSigning\nIn order for users to be sure that the copy of Firefox they have downloaded is indeed the unmodi\ufb01ed\nbuild from Mozilla, we apply a few di\ufb00erent types of digital signatures to the builds.\nThe \ufb01rst type of signing is for our Windows builds. We use a Microsoft Authenticode (signcode)\nsigning key to sign all our .exe and .dll \ufb01les. Windows can use these signatures to verify that\nthe application comes from a trusted source. We also sign the Firefox installer executable with the\nAuthenticode key.\nNext we use GPG to generate a set of MD5 and SHA1 checksums for all the builds on all\nplatforms, and generate detached GPG signatures for the checksum \ufb01les as well as all the builds\nand installers. These signatures are used by mirrors and other community members to validate their\ndownloads.\nFor security purposes, we sign on a dedicated signing machine that is blocked o\ufb00via \ufb01rewall\nand VPN from outside connections. Our keyphrases, passwords, and keystores are passed among\nrelease engineers only in secure channels, often in person, to minimize the risk of exposure as much\nas possible.\nUntil recently this signing process involved a release engineer working on a dedicated server (the\n\u201csigning master\u201d) for almost an hour manually downloading builds, signing them, and uploading them\nback to ftp.mozilla.org before the automation could continue. Once signing on the master was\ncompleted and all \ufb01les were uploaded, a log \ufb01le of all the signing activities was uploaded to the release\ncandidates directory on ftp.mozilla.org. The appearance of this log \ufb01le on ftp.mozilla.org\nsigni\ufb01ed the end of human signing work and from that point, dependent builders watching for\nthat log \ufb01le could resume automation. Recently we\u2019ve added an additional wrapper of automation\naround the signing steps. Now the release engineer opens a Cygwin shell on the signing master\nand sets up a few environment variables pertaining to the release, like VERSION, BUILD, TAG, and\nRELEASE_CONFIG, that help the script \ufb01nd the right directories on ftp.mozilla.org and know when\nall the deliverables for a release have been downloaded so that the signing can start. After checking\nout the most recent production version of our signing tools, the release engineer simply runs make\nautosign. The release engineer then enters two passphrases, one for gpg and one for signcode. Once\nthese passphrases are automatically veri\ufb01ed by the make scripts, the automation starts a download\nloop that watches for uploaded builds and repacks from the release automation and downloads them\nas they become available. Once all items have been downloaded, the automation begins signing\nimmediately, without human intervention.\nNot needing a human to sign is important for two reasons. Firstly, it reduces the risk of human\nerror. Secondly, it allows signing to proceed during non-work hours, without needing a release\nengineer awake at a computer at the time.\nChris AtLee, Lukas Blakk, John O\u2019Duinn, and Armen Zambrano Gasparnian\n31\n", "page": 45, "type": "text", "section": "Page 45"}
{"text": "Figure 2.5: Signing Firefox installers\nAll deliverables have an MD5SUM and SHA1SUM generated for them, and those hash values\nare written to \ufb01les of the same name. These \ufb01les will be uploaded back to the release-candidates\ndirectory as well as synced into the \ufb01nal location of the release on ftp.mozilla.org once it is\nlive, so that anyone who downloads a Firefox installer from one of our mirrors can ensure they\ngot the correct object. When all signed bits are available and veri\ufb01ed they are uploaded back to\nftp.mozilla.org along with the signing log \ufb01le, which the automation is waiting for to proceed.\nOur next planned round of improvements to the signing process will create a tool that allows\nus to sign bits at the time of build/repack. This work requires creating a signing server application\nthat can receive requests to sign \ufb01les from the release build machines. It will also require a signing\n32\nFirefox Release Engineering\n", "page": 46, "type": "text", "section": "Page 46"}
{"text": "client tool which would contact the signing server, authenticate itself as a machine trusted to request\nsigning, upload the \ufb01les to be signed, wait for the build to be signed, download the signed bits, and\nthen include them as part of the packaged build. Once these enhancements are in production, we\ncan discontinue our current all-at-once signing process, as well as our all-at-once generate-updates\nprocess (more on this below). We expect this work to trim a few hours o\ufb00our current end-to-end\ntimes for a release.\n2.6\nUpdates\nUpdates are created so users can update to the latest version of Firefox quickly and easily using\nour built-in updater, without having to download and run a standalone installer. From the user\u2019s\nperspective, the downloading of the update package happens quietly in the background. Only after\nthe update \ufb01les are downloaded, and ready to be applied, will Firefox prompt the user with the option\nto apply the update and restart.\nThe catch is, we generate a lot of updates. For a series of releases on a product line, we generate\nupdates from all supported previous releases in the series to the new latest release for that product\nline. For Firefox LATEST, that means generating updates for every platform, every locale, and every\ninstaller from Firefox LATEST-1, LATEST-2, LATEST-3, ... in both complete and partial forms. We\ndo all this for several di\ufb00erent product lines at a time.\nOur update generation automation modi\ufb01es the update con\ufb01guration \ufb01les of each release\u2019s build\no\ufb00a branch to maintain our canonical list of what version numbers, platforms, and localizations\nneed to have updates created to o\ufb00er users this newest release. We o\ufb00er updates as \u201csnippets\u201d. As\nyou can see in the example below, this snippet is simply an XML pointer \ufb01le hosted on our AUS\n(Application Update Service) that informs the user\u2019s Firefox browser where the complete and/or\npartial .mar (Mozilla Archive) \ufb01les are hosted.\nMajor Updates vs. Minor Updates\nAs you can see in Figure 2.6, update snippets have a type attribute which can be either major or\nminor. Minor updates keep people updated to the latest version available in their release train; for\nexample, it would update all 3.6.* release users to the latest 3.6 release, all rapid-release beta users to\nthe latest beta, all Nightly users to the latest Nightly build, etc. Most of the time, updates are minor\nand don\u2019t require any user interaction other than a con\ufb01rmation to apply the update and restart the\nbrowser.\nMajor updates are used when we need to advertise to our users that the latest and greatest release\nis available, prompting them that \u201cA new version of Firefox is available, would you like to update?\u201d\nand displaying a billboard showcasing the leading features in the new release. Our new rapid-release\nsystem means we no longer need to do as many major updates; we\u2019ll be able to stop generating major\nupdates once the 3.6.* release train is no longer supported.\nComplete Updates vs. Partial Updates\nAt build time we generate \u201ccomplete update\u201d .mar \ufb01les which contain all the \ufb01les for the new release,\ncompressed with bz2 and then archived into a .mar \ufb01le. Both complete and partial updates are\ndownloaded automatically through the update channel to which a user\u2019s Firefox is registered. We\nhave di\ufb00erent update channels (that is, release users look for updates on release channel, beta users\nChris AtLee, Lukas Blakk, John O\u2019Duinn, and Armen Zambrano Gasparnian\n33\n", "page": 47, "type": "text", "section": "Page 47"}
{"text": "<updates>\n<update type=\"minor\" version=\"7.0.1\" extensionVersion=\"7.0.1\"\nbuildID=\"20110928134238\"\ndetailsURL=\"https://www.mozilla.com/en-US/firefox/7.0.1/releasenotes/\">\n<patch type=\"complete\"\nURL=\"http://download.mozilla.org/?product=firefox-7.0.1-complete& os=osx&\\\nlang=en-US&force=1\"\nhashFunction=\"SHA512\"\nhashValue=\"7ecdbc110468b9b4627299794d793874436353dc36c80151550b08830f9d8c\\\n5afd7940c51df9270d54e11fd99806f41368c0f88721fa17e01ea959144f47\\\n3f9d\"\nsize=\"28680122\"/>\n<patch type=\"partial\"\nURL=\"http://download.mozilla.org/?product=firefox-7.0.1-partial-6.0.2&\\\nos=osx&lang=en-US&force=1\"\nhashFunction=\"SHA512\"\nhashValue=\"e9bb49bee862c7a8000de6508d006edf29778b5dbede4deaf3cfa05c22521f\\\nc775da126f5057621960d327615b5186b27d75a378b00981394716e93fc5cc\\\na11a\"\nsize=\"10469801\"/>\n</update>\n</updates>\nFigure 2.6: Sample update snippet\nlook on beta channel, etc.) so that we can serve updates to, for example, release users at a di\ufb00erent\ntime than we serve updates to beta users.\nPartial update .mar \ufb01les are created by comparing the complete .mar for the old release with the\ncomplete .mar for the new release to create a \u201cpartial-update\u201d .mar \ufb01le containing the binary di\ufb00of\nany changed \ufb01les, and a manifest \ufb01le. As you can see in the sample snippet in Figure 2.6, this results\nin a much smaller \ufb01le size for partial updates. This is very important for users with slower or dial-up\nInternet connections.\nIn older versions of our update automation the generation of partial updates for all locales and\nplatforms could take six to seven hours for one release, as the complete .mar \ufb01les were downloaded,\ndi\ufb00ed, and packaged into a partial-update .mar \ufb01le. Eventually it was discovered that even across\nplatforms, many component changes were identical, therefore many di\ufb00s could be re-used. With\na script that cached the hash for each part of the di\ufb00, our partial update creation time was brought\ndown to approximately 40 minutes.\nAfter the snippets are uploaded and are hosted on AUS, an update veri\ufb01cation step is run to a)\ntest downloading the snippets and b) run the updater with the downloaded .mar \ufb01le to con\ufb01rm that\nthe updates apply correctly.\nGeneration of partial-update .mar \ufb01les, as well as all the update snippets, is currently done after\nsigning is complete. We do this because generation of the partial updates must be done between\nsigned \ufb01les of the two releases, and therefore generation of the snippets must wait until the signed\nbuilds are available. Once we\u2019re able to integrate signing into the build process, we can generate\npartial updates immediately after completing a build or repack. Together with improvements to our\nAUS software, this means that once we\u2019re \ufb01nished builds and repacks we would be able to push\nimmediately to mirrors. This e\ufb00ectively parallelizes the creation of all the updates, trimming several\nhours from our total time.\n34\nFirefox Release Engineering\n", "page": 48, "type": "text", "section": "Page 48"}
{"text": "2.7\nPushing to Internal Mirrors and QA\nVerifying that the release process is producing the expected deliverables is key. This is accomplished\nby QA\u2019s veri\ufb01cation and sign o\ufb00s process.\nOnce the signed builds are available, QA starts manual and automated testing. QA relies on\na mix of community members, contractors and employees in di\ufb00erent timezones to speed up this\nveri\ufb01cation process. Meanwhile, our release automation generates updates for all languages and\nall platforms, for all supported releases. These update snippets are typically ready before QA has\n\ufb01nished verifying the signed builds. QA then veri\ufb01es that users can safely update from various\nprevious releases to the newest release using these updates.\nMechanically, our automation pushes the binaries to our \u201cinternal mirrors\u201d (Mozilla-hosted\nservers) in order for QA to verify updates. Only after QA has \ufb01nished veri\ufb01cation of the builds and\nthe updates will we push them to our community mirrors. These community mirrors are essential to\nhandle the global load of users, by allowing them to request their updates from local mirror nodes\ninstead of from ftp.mozilla.org directly. It\u2019s worth noting that we do not make builds and updates\navailable on the community mirrors until after QA signo\ufb00, because of complications that arise if QA\n\ufb01nds a last-minute showstopper and the candidate build needs to be withdrawn.\nThe validation process after builds and updates are generated is:\n\u2022 QA, along with community and contractors in other timezones, does manual testing.\n\u2022 QA triggers the automation systems to do functional testing.\n\u2022 QA independently veri\ufb01es that \ufb01xed problems and new features for that release are indeed\n\ufb01xed and of good enough quality to ship to users.\n\u2022 Meanwhile, release automation generates the updates.\n\u2022 QA signs o\ufb00the builds.\n\u2022 QA signs o\ufb00the updates.\nNote that users don\u2019t get updates until QA has signed o\ufb00and the Release Coordinator has sent\nthe email asking to push the builds and updates live.\n2.8\nPushing to Public Mirrors and AUS\nOnce the Release Coordinator gets signo\ufb00from QA and various other groups at Mozilla, they give\nRelease Engineering the go-ahead to push the \ufb01les to our community mirror network. We rely on\nour community mirrors to be able to handle a few hundred million users downloading updates over\nthe next few days. All the installers, as well as the complete and partial updates for all platforms and\nlocales, are already on our internal mirror network at this point. Publishing the \ufb01les to our external\nmirrors involves making a change to an rsync exclude \ufb01le for the public mirrors module. Once this\nchange is made, the mirrors will start to synchronize the new release \ufb01les. Each mirror has a score\nor weighting associated with it; we monitor which mirrors have synchronized the \ufb01les and sum their\nindividual scores to compute a total \u201cuptake\u201d score. Once a certain uptake threshold is reached, we\nnotify the Release Coordinator that the mirrors have enough uptake to handle the release.\nThis is the point at which the release becomes \u201co\ufb03cial\u201d. After the Release Coordinator sends\nthe \ufb01nal \u201cgo live\u201d email, Release Engineering will update the symlinks on the web server so that\nvisitors to our web and ftp sites can \ufb01nd the latest new version of Firefox. We also publish all the\nupdate snippets for users on past versions of Firefox to AUS.\nChris AtLee, Lukas Blakk, John O\u2019Duinn, and Armen Zambrano Gasparnian\n35\n", "page": 49, "type": "text", "section": "Page 49"}
{"text": "Firefox installed on users\u2019 machines regularly checks our AUS servers to see if there\u2019s an updated\nversion of Firefox available for them. Once we publish these update snippets, users are able to\nautomatically update Firefox to the latest version.\n2.9\nLessons Learned\nAs software engineers, our temptation is to jump to solve what we see as the immediate and obvious\ntechnical problem. However, Release Engineering spans across di\ufb00erent \ufb01elds\u2014both technical and\nnon-technical\u2014so being aware of technical and non-technical issues is important.\nThe Importance of Buy-in from Other Stakeholders\nIt was important to make sure that all stakeholders understood that our slow, fragile release engineering\nexposed the organization, and our users, to risks. This involved all levels of the organization\nacknowledging the lost business opportunities, and market risks, caused by slow fragile automation.\nFurther, Mozilla\u2019s ability to protect our users with super-fast turnaround on releases became more\nimportant as we grew to have more users, which in turn made us more attractive as a target.\nInterestingly, some people had only ever experienced fragile release automation in their careers,\nso came to Mozilla with low, \u201coh, it\u2019s always this bad\u201d expectations. Explaining the business\ngains expected with a robust, scalable release automation process helped everyone understand the\nimportance of the \u201cinvisible\u201d Release Engineering improvement work we were about to undertake.\nInvolving Other Groups\nTo make the release process more e\ufb03cient and more reliable required work, by Release Engineering\nand other groups across Mozilla. However, it was interesting to see how often \u201cit takes a long time\nto ship a release\u201d was mistranslated as \u201cit takes Release Engineering a long time to ship a release\u201d.\nThis misconception ignored the release work done by groups outside of Release Engineering, and\nwas demotivating to the Release Engineers. Fixing this misconception required educating people\nacross Mozilla on where time was actually spent by di\ufb00erent groups during a release. We did this\nwith low-tech \u201cwall-clock\u201d timestamps on emails of clear hando\ufb00s across groups, and a series of\n\u201cwall-clock\u201d blog posts detailing where time was spent.\n\u2022 These helped raise awareness of which di\ufb00erent groups were actually involved in a release.\n\u2022 These helped people appreciate whenever RelEng got processes to run faster, which in turn\nhelped motivate Release Engineers to make further improvements.\n\u2022 These helped other groups think about how they too could help improve the overall release\nprocess\u2014a big mindset shift for the entire organization.\n\u2022 Finally, these also eliminated all the unclear hando\ufb00communications across groups, which\nhad historically cost us many respins, false-starts, and other costly disruptions to the release\nprocess.\nEstablishing Clear Hando\ufb00s\nMany of our \u201crelease engineering\u201d problems were actually people problems: miscommunication\nbetween teams; lack of clear leadership; and the resulting stress, fatigue and anxiety during chemspill\nreleases. By having clear hando\ufb00s to eliminate these human miscommunications, our releases\nimmediately started to go more smoothly, and cross-group human interactions quickly improved.\n36\nFirefox Release Engineering\n", "page": 50, "type": "text", "section": "Page 50"}
{"text": "Managing Turnover\nWhen we started this project, we were losing team members too often. In itself, this is bad. However,\nthe lack of accurate up-to-date documentation meant that most of the technical understanding of the\nrelease process was documented by folklore and oral histories, which we lost whenever a person left.\nWe needed to turn this situation around, urgently.\nWe felt the best way to improve morale and show that things were getting better was to make sure\npeople could see that we had a plan to make things better, and that people had some control over their\nown destiny. We did this by making sure that we set aside time to \ufb01x at least one thing\u2014anything!\u2014\nafter each release. We implemented this by negotiating for a day or two of \u201cdo not disturb\u201d time\nimmediately after we shipped a release. Solving immediate small problems, while they were still\nfresh in people\u2019s minds, helped clear distractions, so people could focus on larger term problems\nin subsequent releases. More importantly, this gave people the feeling that we had regained some\ncontrol over our own fate, and that things were truly getting better.\nManaging Change\nBecause of market pressures, Mozilla\u2019s business and product needs from the release process changed\nwhile we were working on improving it. This is not unusual and should be expected.\nWe knew we had to continue shipping releases using the current release process, while we were\nbuilding the new process. We decided against attempting to build a separate \u201cgreen\ufb01eld project\u201d\nwhile also supporting the existing systems; we felt the current systems were so fragile that we literally\nwould not have the time to do anything new.\nWe also assumed from the outset that we didn\u2019t fully understand what was broken. Each incre-\nmental improvement allowed us to step back and check for new surprises, before starting work on\nthe next improvement. Phrases like \u201cdraining the swamp\u201d, \u201cpeeling the onion\u201d, and \u201chow did this\never work?\u201d were heard frequently whenever we discovered new surprises throughout this project.\nGiven all this, we decided to make lots of small, continuous improvements to the existing\nprocess. Each iterative improvement made the next release a little bit better. More importantly,\neach improvement freed up just a little bit more time during the next release, which allowed a\nrelease engineer a little more time to make the next improvement. These improvements snowballed\nuntil we found ourselves past the tipping point, and able to make time to work on signi\ufb01cant major\nimprovements. At that point, the gains from release optimizations really kicked in.\n2.10\nFor More Information\nWe\u2019re really proud of the work done so far, and the abilities that it has brought to Mozilla in a newly\nheated-up global browser market.\nFour years ago, doing two chemspill releases in a month would be a talking point within Mozilla.\nBy contrast, last week a published exploit in a third-party library caused Mozilla to ship eight\nchemspills releases in two low-fuss days.\nAs with everything, our release automation still has plenty of room for improvement, and our\nneeds and demands continue to change. For a look at our ongoing work, please see:\n\u2022 Chris AtLee\u2019s blog: http://atlee.ca/blog.\n\u2022 Lukas Blakk\u2019s blog: http://lukasblakk.com.\n\u2022 John O\u2019Duinn\u2019s blog: http://oduinn.com.\n\u2022 Armen Zambrano Gasparnian\u2019s blog: http://armenzg.blogspot.com.\nChris AtLee, Lukas Blakk, John O\u2019Duinn, and Armen Zambrano Gasparnian\n37\n", "page": 51, "type": "text", "section": "Page 51"}
{"text": "\u2022 Documentation on the design and \ufb02ow of our Mercurial-based release process: https://\nwiki.mozilla.org/Release:Release_Automation_on_Mercurial:Documentation.\n\u2022 Release Engineering\u2019s build repositories: http://hg.mozilla.org/build. In particular, the\nbuildbotcustom, buildbot-con\ufb01gs, and tools repositories are used heavily for releases.\n\u2022 The Firefox 7.0 Beta 4 Build Notes: https://wiki.mozilla.org/Releases/Firefox_7.\n0b4/BuildNotes. In addition to code, we document every aspect of a release. This link is to\nour 7.0b4 release notes, but you can \ufb01nd all our release notes if you edit the URL appropriately.\n38\nFirefox Release Engineering\n", "page": 52, "type": "text", "section": "Page 52"}
{"text": "[chapter3]\nFreeRTOS\nChristopher Svec\nFreeRTOS (pronounced \u201cfree-arr-toss\u201d) is an open source real-time operating system (RTOS) for\nembedded systems. FreeRTOS supports many di\ufb00erent architectures and compiler toolchains, and is\ndesigned to be \u201csmall, simple, and easy to use\u201d1.\nFreeRTOS is under active development, and has been since Richard Barry started work on it in\n2002. As for me, I\u2019m not a developer of or contributor to FreeRTOS, I\u2019m merely a user and a fan.\nAs a result, this chapter will favor the \u201cwhat\u201d and \u201chow\u201d of FreeRTOS\u2019s architecture, with less of\nthe \u201cwhy\u201d than other chapters in this book.\nLike all operating systems, FreeRTOS\u2019s main job is to run tasks. Most of FreeRTOS\u2019s code\ninvolves prioritizing, scheduling, and running user-de\ufb01ned tasks. Unlike all operating systems,\nFreeRTOS is a real-time operating system which runs on embedded systems.\nBy the end of this chapter I hope that you\u2019ll understand the basic architecture of FreeRTOS. Most\nof FreeRTOS is dedicated to running tasks, so you\u2019ll get a good look at exactly how FreeRTOS does\nthat.\nIf this is your \ufb01rst look under the hood of an operating system, I also hope that you\u2019ll learn\nthe basics about how any OS works. FreeRTOS is relatively simple, especially when compared to\nWindows, Linux, or OS X, but all operating systems share the same basic concepts and goals, so\nlooking at any OS can be instructive and interesting.\n3.1\nWhat is \u201cEmbedded\u201d and \u201cReal-Time\u201d?\n\u201cEmbedded\u201d and \u201creal-time\u201d can mean di\ufb00erent things to di\ufb00erent people, so let\u2019s de\ufb01ne them as\nFreeRTOS uses them.\nAn embedded system is a computer system that is designed to do only a few things, like the system\nin a TV remote control, in-car GPS, digital watch, or pacemaker. Embedded systems are typically\nsmaller and slower than general purpose computer systems, and are also usually less expensive. A\ntypical low-end embedded system may have an 8-bit CPU running at 25MHz, a few KB of RAM,\nand maybe 32KB of \ufb02ash memory. A higher-end embedded system may have a 32-bit CPU running\nat 750MHz, a GB of RAM, and multiple GB of \ufb02ash memory.\nReal-time systems are designed to do something within a certain amount of time; they guarantee\nthat stu\ufb00happens when it\u2019s supposed to.\n1http://www.freertos.org/index.html?http://www.freertos.org/FreeRTOS_Features.html\n", "page": 53, "type": "text", "section": "Page 53"}
{"text": "A pacemaker is an excellent example of a real-time embedded system. A pacemaker must\ncontract the heart muscle at the right time to keep you alive; it can\u2019t be too busy to respond in time.\nPacemakers and other real-time embedded systems are carefully designed to run their tasks on time,\nevery time.\n3.2\nArchitecture Overview\nFreeRTOS is a relatively small application. The minimum core of FreeRTOS is only three source\n(.c) \ufb01les and a handful of header \ufb01les, totalling just under 9000 lines of code, including comments\nand blank lines. A typical binary code image is less than 10KB.\nFreeRTOS\u2019s code breaks down into three main areas: tasks, communication, and hardware\ninterfacing.\n\u2022 Tasks: Almost half of FreeRTOS\u2019s core code deals with the central concern in many operating\nsystems: tasks. A task is a user-de\ufb01ned C function with a given priority. tasks.c and task.h\ndo all the heavy lifting for creating, scheduling, and maintaining tasks.\n\u2022 Communication: Tasks are good, but tasks that can communicate with each other are even bet-\nter! Which brings us to the second FreeRTOS job: communication. About 40% of FreeRTOS\u2019s\ncore code deals with communication. queue.c and queue.h handle FreeRTOS communica-\ntion. Tasks and interrupts use queues to send data to each other and to signal the use of critical\nresources using semaphores and mutexes.\n\u2022 The Hardware Whisperer: The approximately 9000 lines of code that make up the base of\nFreeRTOS are hardware-independent; the same code runs whether FreeRTOS is running on\nthe humble 8051 or the newest, shiniest ARM core. About 6% of FreeRTOS\u2019s core code acts\na shim between the hardware-independent FreeRTOS core and the hardware-dependent code.\nWe\u2019ll discuss the hardware-dependent code in the next section.\nHardware Considerations\nThe hardware-independent FreeRTOS layer sits on top of a hardware-dependent layer. This hardware-\ndependent layer knows how to talk to whatever chip architecture you choose. Figure 3.1 shows\nFreeRTOS\u2019s layers.\nHardware\nFreeRTOS Hardware-Dependent Code\nFreeRTOS Hardware-Independent Code\nFreeRTOS User Tasks and ISR Code\nFigure 3.1: FreeRTOS software layers\nFreeRTOS ships with all the hardware-independent as well as hardware-dependent code you\u2019ll\nneed to get a system up and running. It supports many compilers (CodeWarrior, GCC, IAR, etc.) as\n40\nFreeRTOS\n", "page": 54, "type": "text", "section": "Page 54"}
{"text": "well as many processor architectures (ARM7, ARM Cortex-M3, various PICs, Silicon Labs 8051,\nx86, etc.). See the FreeRTOS website for a list of supported architectures and compilers.\nFreeRTOS is highly con\ufb01gurable by design. FreeRTOS can be built as a single CPU, bare-bones\nRTOS, supporting only a few tasks, or it can be built as a highly functional multicore beast with\nTCP/IP, a \ufb01le system, and USB.\nCon\ufb01guration options are selected in FreeRTOSConfig.h by setting various #defines. Clock\nspeed, heap size, mutexes, and API subsets are all con\ufb01gurable in this \ufb01le, along with many other\noptions. Here are a few examples that set the maximum number of task priority levels, the CPU\nfrequency, the system tick frequency, the minimal stack size and the total heap size:\n#define configMAX_PRIORITIES\n( ( unsigned portBASE_TYPE ) 5 )\n#define configCPU_CLOCK_HZ\n( 12000000UL )\n#define configTICK_RATE_HZ\n( ( portTickType ) 1000 )\n#define configMINIMAL_STACK_SIZE\n( ( unsigned short ) 100 )\n#define configTOTAL_HEAP_SIZE\n( ( size_t ) ( 4 * 1024 ) )\nHardware-dependent code lives in separate \ufb01les for each compiler toolchain and CPU architecture.\nFor example, if you\u2019re working with the IAR compiler on an ARM Cortex-M3 chip, the hardware-\ndependent code lives in the FreeRTOS/Source/portable/IAR/ARM_CM3/ directory. portmacro.h\ndeclares all of the hardware-speci\ufb01c functions, while port.c and portasm.s contain all of the\nactual hardware-dependent code. The hardware-independent header \ufb01le portable.h #include\u2019s\nthe correct portmacro.h \ufb01le at compile time. FreeRTOS calls the hardware-speci\ufb01c functions using\n#define\u2019d functions declared in portmacro.h.\nLet\u2019s look at an example of how FreeRTOS calls a hardware-dependent function. The hardware-\nindependent \ufb01le tasks.c frequently needs to enter a critical section of code to prevent preemption.\nEntering a critical section happens di\ufb00erently on di\ufb00erent architectures, and the hardware-independent\ntasks.c does not want to have to understand the hardware-dependent details. So tasks.c calls\nthe global macro portENTER_CRITICAL(), glad to be ignorant of how it actually works. Assum-\ning we\u2019re using the IAR compiler on an ARM Cortex-M3 chip, FreeRTOS is built with the \ufb01le\nFreeRTOS/Source/portable/IAR/ARM_CM3/portmacro.h which de\ufb01nes portENTER_CRITICAL()\nlike this:\n#define portENTER_CRITICAL()\nvPortEnterCritical()\nvPortEnterCritical() is actually de\ufb01ned in FreeRTOS/Source/portable/IAR/ARM_CM3/port.c.\nThe port.c \ufb01le is hardware-dependent, and contains code that understands the IAR compiler and\nthe Cortex-M3 chip. vPortEnterCritical() enters the critical section using this hardware-speci\ufb01c\nknowledge and returns to the hardware-independent tasks.c.\nThe portmacro.h \ufb01le also de\ufb01nes an architecture\u2019s basic data types. Data types for basic integer\nvariables, pointers, and the system timer tick data type are de\ufb01ned like this for the IAR compiler on\nARM Cortex-M3 chips:\n#define portBASE_TYPE\nlong\n// Basic integer variable type\n#define portSTACK_TYPE unsigned long\n// Pointers to memory locations\ntypedef unsigned portLONG portTickType;\n// The system timer tick type\nThis method of using data types and functions through thin layers of #defines may seem a bit\ncomplicated, but it allows FreeRTOS to be recompiled for a completely di\ufb00erent system architecture\nby changing only the hardware-dependent \ufb01les. And if you want to run FreeRTOS on an architecture\nChristopher Svec\n41\n", "page": 55, "type": "text", "section": "Page 55"}
{"text": "it doesn\u2019t currently support, you only have to implement the hardware-dependent functionality which\nis much smaller than the hardware-independent part of FreeRTOS.\nAs we\u2019ve seen, FreeRTOS implements hardware-dependent functionality with C preprocessor\n#define macros. FreeRTOS also uses #define for plenty of hardware-independent code. For non-\nembedded applications this frequent use of #define is a cardinal sin, but in many smaller embedded\nsystems the overhead for calling a function is not worth the advantages that \u201creal\u201d functions o\ufb00er.\n3.3\nScheduling Tasks: A Quick Overview\nTask Priorities and the Ready List\nEach task has a user-assigned priority between 0 (the lowest priority) and the compile-time value of\nconfigMAX_PRIORITIES-1 (the highest priority). For instance, if configMAX_PRIORITIES is set to\n5, then FreeRTOS will use 5 priority levels: 0 (lowest priority), 1, 2, 3, and 4 (highest priority).\nFreeRTOS uses a \u201cready list\u201d to keep track of all tasks that are currently ready to run. It\nimplements the ready list as an array of task lists like this:\nstatic xList pxReadyTasksLists[ configMAX_PRIORITIES ];\n/* Prioritised ready tasks.\n*/\npxReadyTasksLists[0] is a list of all ready priority 0 tasks, pxReadyTasksLists[1] is a list of all\nready priority 1 tasks, and so on, all the way up to pxReadyTasksLists[configMAX_PRIORITIES-1].\nThe System Tick\nThe heartbeat of a FreeRTOS system is called the system tick. FreeRTOS con\ufb01gures the system\nto generate a periodic tick interrupt. The user can con\ufb01gure the tick interrupt frequency, which is\ntypically in the millisecond range. Every time the tick interrupt \ufb01res, the vTaskSwitchContext()\nfunction is called. vTaskSwitchContext() selects the highest-priority ready task and puts it in the\npxCurrentTCB variable like this:\n/* Find the highest-priority queue that contains ready tasks. */\nwhile( listLIST_IS_EMPTY( &( pxReadyTasksLists[ uxTopReadyPriority ] ) ) )\n{\nconfigASSERT( uxTopReadyPriority );\n--uxTopReadyPriority;\n}\n/* listGET_OWNER_OF_NEXT_ENTRY walks through the list, so the tasks of the same\npriority get an equal share of the processor time. */\nlistGET_OWNER_OF_NEXT_ENTRY( pxCurrentTCB, &( pxReadyTasksLists[ uxTopReadyPriority ] ) );\nBefore the while loop starts, uxTopReadyPriority is guaranteed to be greater than or equal to the\npriority of the highest-priority ready task. The while() loop starts at priority level uxTopReadyPriority\nand walks down through the pxReadyTasksLists[] array to \ufb01nd the highest-priority level with\nready tasks. listGET_OWNER_OF_NEXT_ENTRY() then grabs the next ready task from that priority\nlevel\u2019s ready list.\nNow pxCurrentTCB points to the highest-priority task, and when vTaskSwitchContext() re-\nturns the hardware-dependent code starts running that task.\n42\nFreeRTOS\n", "page": 56, "type": "text", "section": "Page 56"}
{"text": "Those nine lines of code are the absolute heart of FreeRTOS. The other 8900+ lines of FreeRTOS\nare there to make sure those nine lines are all that\u2019s needed to keep the highest-priority task running.\nFigure 3.2 is a high-level picture of what a ready list looks like. This example has three priority\nlevels, with one priority 0 task, no priority 1 tasks, and three priority 2 tasks. This picture is accurate\nbut not complete; it\u2019s missing a few details which we\u2019ll \ufb01ll in later.\npxReadyTaskLists[0] \npxReadyTaskLists[1] \npxReadyTaskLists[2] \nnext ptr == NULL\npointer to task \ntask A\n(empty list)\nnext ptr \npointer to task\ntask B\nnext ptr \npointer to task\ntask C\nnext ptr == NULL\npointer to task\ntask D\nFigure 3.2: Basic view of FreeRTOS Ready List\nNow that we have the high-level overview out of the way, let\u2019s dive in to the details. We\u2019ll look\nat the three main FreeRTOS data structures: tasks, lists, and queues.\n3.4\nTasks\nThe main job of all operating systems is to run and coordinate user tasks. Like many operating\nsystems, the basic unit of work in FreeRTOS is the task. FreeRTOS uses a Task Control Block (TCB)\nto represent each task.\nTask Control Block (TCB)\nThe TCB is de\ufb01ned in tasks.c like this:\ntypedef struct tskTaskControlBlock\n{\nvolatile portSTACK_TYPE *pxTopOfStack;\n/* Points to the location of\nthe last item placed on\nthe tasks stack.\nTHIS\nMUST BE THE FIRST MEMBER\nChristopher Svec\n43\n", "page": 57, "type": "text", "section": "Page 57"}
{"text": "OF THE STRUCT. */\nxListItem\nxGenericListItem;\n/* List item used to place\nthe TCB in ready and\nblocked queues. */\nxListItem\nxEventListItem;\n/* List item used to place\nthe TCB in event lists.*/\nunsigned portBASE_TYPE uxPriority;\n/* The priority of the task\npriority. */\nportSTACK_TYPE *pxStack;\n/* Points to the start of\nthe stack. */\nsigned char\npcTaskName[ configMAX_TASK_NAME_LEN ];\n/* Descriptive name given\nto the task when created.\nFacilitates debugging\nonly. */\n#if ( portSTACK_GROWTH > 0 )\nportSTACK_TYPE *pxEndOfStack;\n/* Used for stack overflow\nchecking on architectures\nwhere the stack grows up\nfrom low memory. */\n#endif\n#if ( configUSE_MUTEXES == 1 )\nunsigned portBASE_TYPE uxBasePriority;\n/* The priority last\nassigned to the task -\nused by the priority\ninheritance mechanism. */\n#endif\n} tskTCB;\nThe TCB stores the address of the stack start address in pxStack and the current top of stack in\npxTopOfStack. It also stores a pointer to the end of the stack in pxEndOfStack to check for stack\nover\ufb02ow if the stack grows \u201cup\u201d to higher addresses. If the stack grows \u201cdown\u201d to lower addresses\nthen stack over\ufb02ow is checked by comparing the current top of stack against the start of stack memory\nin pxStack.\nThe TCB stores the initial priority of the task in uxPriority and uxBasePriority. A task is\ngiven a priority when it is created, and a task\u2019s priority can be changed. If FreeRTOS implements\npriority inheritance then it uses uxBasePriority to remember the original priority while the task is\ntemporarily elevated to the \u201cinherited\u201d priority. (See the discussion about mutexes below for more\non priority inheritance.)\nEach task has two list items for use in FreeRTOS\u2019s various scheduling lists. When a task is\ninserted into a list FreeRTOS doesn\u2019t insert a pointer directly to the TCB. Instead, it inserts a pointer\nto either the TCB\u2019s xGenericListItem or xEventListItem. These xListItem variables let the\nFreeRTOS lists be smarter than if they merely held a pointer to the TCB. We\u2019ll see an example of\nthis when we discuss lists later.\nA task can be in one of four states: running, ready to run, suspended, or blocked. You might\nexpect each task to have a variable that tells FreeRTOS what state it\u2019s in, but it doesn\u2019t. Instead,\nFreeRTOS tracks task state implicitly by putting tasks in the appropriate list: ready list, suspended\n44\nFreeRTOS\n", "page": 58, "type": "text", "section": "Page 58"}
{"text": "list, etc. The presence of a task in a particular list indicates the task\u2019s state. As a task changes from\none state to another, FreeRTOS simply moves it from one list to another.\nTask Setup\nWe\u2019ve already touched on how a task is selected and scheduled with the pxReadyTasksLists array;\nnow let\u2019s look at how a task is initially created. A task is created when the xTaskCreate() function\nis called. FreeRTOS uses a newly allocated TCB object to store the name, priority, and other details\nfor a task, then allocates the amount of stack the user requests (assuming there\u2019s enough memory\navailable) and remembers the start of the stack memory in TCB\u2019s pxStack member.\nThe stack is initialized to look as if the new task is already running and was interrupted by a\ncontext switch. This way the scheduler can treat newly created tasks exactly the same way as it\ntreats tasks that have been running for a while; the scheduler doesn\u2019t need any special case code for\nhandling new tasks.\nThe way that a task\u2019s stack is made to look like it was interrupted by a context switch depends on\nthe architecture FreeRTOS is running on, but this ARM Cortex-M3 processor\u2019s implementation is a\ngood example:\nunsigned int *pxPortInitialiseStack( unsigned int *pxTopOfStack,\npdTASK_CODE pxCode,\nvoid *pvParameters )\n{\n/* Simulate the stack frame as it would be created by a context switch interrupt. */\npxTopOfStack--; /* Offset added to account for the way the MCU uses the stack on\nentry/exit of interrupts. */\n*pxTopOfStack = portINITIAL_XPSR;\n/* xPSR */\npxTopOfStack--;\n*pxTopOfStack = ( portSTACK_TYPE ) pxCode;\n/* PC */\npxTopOfStack--;\n*pxTopOfStack = 0;\n/* LR */\npxTopOfStack -= 5;\n/* R12, R3, R2 and R1. */\n*pxTopOfStack = ( portSTACK_TYPE ) pvParameters;\n/* R0 */\npxTopOfStack -= 8;\n/* R11, R10, R9, R8, R7, R6, R5 and R4. */\nreturn pxTopOfStack;\n}\nThe ARM Cortex-M3 processor pushes registers on the stack when a task is interrupted.\npxPortInitialiseStack() modi\ufb01es the stack to look like the registers were pushed even though\nthe task hasn\u2019t actually started running yet. Known values are stored to the stack for the ARM\nregisters xPSR, PC, LR, and R0. The remaining registers R1 \u2013 R12 get stack space allocated for\nthem by decrementing the top of stack pointer, but no speci\ufb01c data is stored in the stack for those\nregisters. The ARM architecture says that those registers are unde\ufb01ned at reset, so a (non-buggy)\nprogram will not rely on a known value.\nAfter the stack is prepared, the task is almost ready to run. First though, FreeRTOS disables\ninterrupts: We\u2019re about to start mucking with the ready lists and other scheduler structures and we\ndon\u2019t want anyone else changing them underneath us.\nIf this is the \ufb01rst task to ever be created, FreeRTOS initializes the scheduler\u2019s task lists. Free-\nRTOS\u2019s scheduler has an array of ready lists, pxReadyTasksLists[], which has one ready list for\nChristopher Svec\n45\n", "page": 59, "type": "text", "section": "Page 59"}
{"text": "each possible priority level. FreeRTOS also has a few other lists for tracking tasks that have been\nsuspended, killed, and delayed. These are all initialized now as well.\nAfter any \ufb01rst-time initialization is done, the new task is added to the ready list at its speci\ufb01ed\npriority level. Interrupts are re-enabled and new task creation is complete.\n3.5\nLists\nAfter tasks, the most used FreeRTOS data structure is the list. FreeRTOS uses its list structure to\nkeep track of tasks for scheduling, and also to implement queues.\nuxNumberOfItems = 3\npxReadyTaskLists[0] \nxListEnd\nxItemValue = \n0xffffffff\npxNext \npxPrevious  \npxIndex \nxItemValue = 0\npxNext \npxPrevious \npvOwner \npvContainer \nxItemValue = 0\npxNext \npxPrevious \npvOwner \npvContainer\nTCB: Task A\nTCB: Task C\nTCB: Task B\npxCurrentTCB\nstruct xListItem\nxItemValue = 0\npxNext \npxPrevious \npvOwner \npvContainer\nstruct xListItem\nstruct xListItem\nFigure 3.3: Full view of FreeRTOS Ready List\nThe FreeRTOS list is a standard circular doubly linked list with a couple of interesting additions.\nHere\u2019s a list element:\nstruct xLIST_ITEM\n{\nportTickType xItemValue;\n/* The value being listed.\nIn most cases\nthis is used to sort the list in\n46\nFreeRTOS\n", "page": 60, "type": "text", "section": "Page 60"}
{"text": "descending order. */\nvolatile struct xLIST_ITEM * pxNext;\n/* Pointer to the next xListItem in the\nlist.\n*/\nvolatile struct xLIST_ITEM * pxPrevious;\n/* Pointer to the previous xListItem in\nthe list. */\nvoid * pvOwner;\n/* Pointer to the object (normally a TCB)\nthat contains the list item.\nThere is\ntherefore a two-way link between the\nobject containing the list item and\nthe list item itself. */\nvoid * pvContainer;\n/* Pointer to the list in which this list\nitem is placed (if any). */\n};\nEach list element holds a number, xItemValue, that is the usually the priority of the task being\ntracked or a timer value for event scheduling. Lists are kept in high-to-low priority order, meaning\nthat the highest-priority xItemValue (the largest number) is at the front of the list and the lowest\npriority xItemValue (the smallest number) is at the end of the list.\nThe pxNext and pxPrevious pointers are standard linked list pointers. pvOwner is a pointer to\nthe owner of the list element. This is usually a pointer to a task\u2019s TCB object. pvOwner is used to\nmake task switching fast in vTaskSwitchContext(): once the highest-priority task\u2019s list element is\nfound in pxReadyTasksLists[], that list element\u2019s pvOwner pointer leads us directly to the TCB\nneeded to schedule the task.\npvContainer points to the list that this item is in. It is used to quickly determine if a list item is\nin a particular list. Each list element can be put in a list, which is de\ufb01ned as:\ntypedef struct xLIST\n{\nvolatile unsigned portBASE_TYPE uxNumberOfItems;\nvolatile xListItem * pxIndex;\n/* Used to walk through the list.\nPoints to\nthe last item returned by a call to\npvListGetOwnerOfNextEntry (). */\nvolatile xMiniListItem xListEnd;\n/* List item that contains the maximum\npossible item value, meaning it is always\nat the end of the list and is therefore\nused as a marker. */\n} xList;\nThe size of a list at any time is stored in uxNumberOfItems, for fast list-size operations. All\nnew lists are initialized to contain a single element: the xListEnd element. xListEnd.xItemValue\nis a sentinel value which is equal to the largest value for the xItemValue variable: 0xffff when\nportTickType is a 16-bit value and 0xffffffff when portTickType is a 32-bit value. Other list\nelements may also have the same value; the insertion algorithm ensures that xListEnd is always the\nlast item in the list.\nSince lists are sorted high-to-low, the xListEnd element is used as a marker for the start of the\nlist. And since the list is circular, this xListEnd element is also a marker for the end of the list.\nChristopher Svec\n47\n", "page": 61, "type": "text", "section": "Page 61"}
{"text": "Most \u201ctraditional\u201d list accesses you\u2019ve used probably do all of their work within a single for()\nloop or function call like this:\nfor (listPtr = listStart; listPtr != NULL; listPtr = listPtr->next) {\n// Do something with listPtr here...\n}\nFreeRTOS frequently needs to access a list across multiple for() and while() loops as well as\nfunction calls, and so it uses list functions that manipulate the pxIndex pointer to walk the list. The\nlist function listGET_OWNER_OF_NEXT_ENTRY() does pxIndex = pxIndex->pxNext; and returns\npxIndex. (Of course it does the proper end-of-list-wraparound detection too.) This way the list itself\nis responsible for keeping track of \u201cwhere you are\u201d while walking it using pxIndex, allowing the\nrest of FreeRTOS to not worry about it.\nuxNumberOfItems = 3\npxReadyTaskLists[0] \nxListEnd\nxItemValue = \n0xffffffff\npxNext \npxPrevious  \npxIndex \nxItemValue = 0\npxNext \npxPrevious \npvOwner \npvContainer \nxItemValue = 0\npxNext \npxPrevious \npvOwner \npvContainer\nTCB: Task A\nTCB: Task C\nTCB: Task B\nstruct xListItem\nxItemValue = 0\npxNext \npxPrevious \npvOwner \npvContainer\nstruct xListItem\nstruct xListItem\npxCurrentTCB\nFigure 3.4: Full view of FreeRTOS Ready List after a system timer tick\nThe pxReadyTasksLists[] list manipulation done in vTaskSwitchContext() is a good exam-\nple of how pxIndex is used. Let\u2019s assume we have only one priority level, priority 0, and there are\nthree tasks at that priority level. This is similar to the basic ready list picture we looked at earlier, but\nthis time we\u2019ll include all of the data structures and \ufb01elds.\n48\nFreeRTOS\n", "page": 62, "type": "text", "section": "Page 62"}
{"text": "As you can see in Figure 3.3, pxCurrentTCB indicates that we\u2019re currently running Task B. The\nnext time vTaskSwitchContext() runs, it calls listGET_OWNER_OF_NEXT_ENTRY() to get the next\ntask to run. This function uses pxIndex->pxNext to \ufb01gure out the next task is Task C, and now\npxIndex points to Task C\u2019s list element and pxCurrentTCB points to Task C\u2019s TCB, as shown in\nFigure 3.4.\nNote that each struct xListItem object is actually the xGenericListItem object from the\nassociated TCB.\n3.6\nQueues\nFreeRTOS allows tasks to communicate and synchronize with each other using queues. Interrupt\nservice routines (ISRs) also use queues for communication and synchronization.\nThe basic queue data structure is:\ntypedef struct QueueDefinition\n{\nsigned char *pcHead;\n/* Points to the beginning of the queue\nstorage area. */\nsigned char *pcTail;\n/* Points to the byte at the end of the\nqueue storage area. One more byte is\nallocated than necessary to store the\nqueue items; this is used as a marker. */\nsigned char *pcWriteTo;\n/* Points to the free next place in the\nstorage area. */\nsigned char *pcReadFrom;\n/* Points to the last place that a queued\nitem was read from. */\nxList xTasksWaitingToSend;\n/* List of tasks that are blocked waiting\nto post onto this queue.\nStored in\npriority order. */\nxList xTasksWaitingToReceive;\n/* List of tasks that are blocked waiting\nto read from this queue. Stored in\npriority order. */\nvolatile unsigned portBASE_TYPE uxMessagesWaiting;\n/* The number of items currently\nin the queue. */\nunsigned portBASE_TYPE uxLength;\n/* The length of the queue\ndefined as the number of\nitems it will hold, not the\nnumber of bytes. */\nunsigned portBASE_TYPE uxItemSize;\n/* The size of each items that\nthe queue will hold. */\n} xQUEUE;\nThis is a fairly standard queue with head and tail pointers, as well as pointers to keep track of\nwhere we\u2019ve just read from and written to.\nWhen creating a queue, the user speci\ufb01es the length of the queue and the size of each item to\nbe tracked by the queue. pcHead and pcTail are used to keep track of the queue\u2019s internal storage.\nAdding an item into a queue does a deep copy of the item into the queue\u2019s internal storage.\nChristopher Svec\n49\n", "page": 63, "type": "text", "section": "Page 63"}
{"text": "FreeRTOS makes a deep copy instead of storing a pointer to the item because the lifetime of the\nitem inserted may be much shorter than the lifetime of the queue. For instance, consider a queue of\nsimple integers inserted and removed using local variables across several function calls. If the queue\nstored pointers to the integers\u2019 local variables, the pointers would be invalid as soon as the integers\u2019\nlocal variables went out of scope and the local variables\u2019 memory was used for some new value.\nThe user chooses what to queue. The user can queue copies of items if the items are small, like\nin the simple integer example in the previous paragraph, or the user can queue pointers to the items\nif the items are large. Note that in both cases FreeRTOS does a deep copy: if the user chooses to\nqueue copies of items then the queue stores a deep copy of each item; if the user chooses to queue\npointers then the queue stores a deep copy of the pointer. Of course, if the user stores pointers in the\nqueue then the user is responsible for managing the memory associated with the pointers. The queue\ndoesn\u2019t care what data you\u2019re storing in it, it just needs to know the data\u2019s size.\nFreeRTOS supports blocking and non-blocking queue insertions and removals. Non-blocking\noperations return immediately with a \u201cDid the queue insertion work?\u201d or \u201cDid the queue removal\nwork?\u201d status. Blocking operations are speci\ufb01ed with a timeout. A task can block inde\ufb01nitely or for\na limited amount of time.\nA blocked task\u2014call it Task A\u2014will remain blocked as long as its insert/remove operation cannot\ncomplete and its timeout (if any) has not expired. If an interrupt or another task modi\ufb01es the queue\nso that Task A\u2019s operation could complete, Task A will be unblocked. If Task A\u2019s queue operation is\nstill possible by the time it actually runs then Task A will complete its queue operation and return\n\u201csuccess\u201d. However, by the time Task A actually runs, it is possible that a higher-priority task or\ninterrupt has performed yet another operation on the queue that prevents Task A from performing its\noperation. In this case Task A will check its timeout and either resume blocking if the timeout hasn\u2019t\nexpired, or return with a queue operation \u201cfailed\u201d status.\nIt\u2019s important to note that the rest of the system keeps going while a task is blocking on a queue;\nother tasks and interrupts continue to run. This way the blocked task doesn\u2019t waste CPU cycles that\ncould be used productively by other tasks and interrupts.\nFreeRTOS uses the xTasksWaitingToSend list to keep track of tasks that are blocking on\ninserting into a queue. Each time an element is removed from a queue the xTasksWaitingToSend\nlist is checked. If a task is waiting in that list the task is unblocked.\nSimilarly, xTasksWaitingToReceive keeps track of tasks that are blocking on removing from\na queue. Each time a new element is inserted into a queue the xTasksWaitingToReceive list is\nchecked. If a task is waiting in that list the task is unblocked.\nSemaphores and Mutexes\nFreeRTOS uses its queues for communication between and within tasks. FreeRTOS also uses its\nqueues to implement semaphores and mutexes.\nWhat\u2019s The Di\ufb00erence?\nSemaphores and mutexes may sound like the same thing, but they\u2019re not. FreeRTOS implements\nthem similarly, but they\u2019re intended to be used in di\ufb00erent ways. How should they be used di\ufb00er-\nently? Embedded systems guru Michael Barr says it best in his article, \u201cMutexes and Semaphores\nDemysti\ufb01ed\u201d2:\n2http://www.barrgroup.com/Embedded-Systems/How-To/RTOS-Mutex-Semaphore\n50\nFreeRTOS\n", "page": 64, "type": "text", "section": "Page 64"}
{"text": "The correct use of a semaphore is for signaling from one task to another. A mutex is\nmeant to be taken and released, always in that order, by each task that uses the shared\nresource it protects. By contrast, tasks that use semaphores either signal [\u201csend\u201d in\nFreeRTOS terms] or wait [\u201creceive\u201d in FreeRTOS terms] - not both.\nA mutex is used to protect a shared resource. A task acquires a mutex, uses the shared resource,\nthen releases the mutex. No task can acquire a mutex while the mutex is being held by another task.\nThis guarantees that only one task is allowed to use a shared resource at a time.\nSemaphores are used by one task to signal another task. To quote Barr\u2019s article:\nFor example, Task 1 may contain code to post (i.e., signal or increment) a particular\nsemaphore when the \u201cpower\u201d button is pressed and Task 2, which wakes the display,\npends on that same semaphore. In this scenario, one task is the producer of the event\nsignal; the other the consumer.\nIf you\u2019re at all in doubt about semaphores and mutexes, please check out Michael\u2019s article.\nImplementation\nFreeRTOS implements an N-element semaphore as a queue that can hold N items. It doesn\u2019t\nstore any actual data for the queue items; the semaphore just cares how many queue entries are\ncurrently occupied, which is tracked in the queue\u2019s uxMessagesWaiting \ufb01eld. It\u2019s doing \u201cpure\nsynchronization\u201d, as the FreeRTOS header \ufb01le semphr.h calls it. Therefore the queue has a item\nsize of zero bytes (uxItemSize == 0). Each semaphore access increments or decrements the\nuxMessagesWaiting \ufb01eld; no item or data copying is needed.\nLike a semaphore, a mutex is also implemented as a queue, but several of the xQUEUE struct \ufb01elds\nare overloaded using #defines:\n/* Effectively make a union out of the xQUEUE structure. */\n#define uxQueueType\npcHead\n#define pxMutexHolder\npcTail\nSince a mutex doesn\u2019t store any data in the queue, it doesn\u2019t need any internal storage, and so the\npcHead and pcTail \ufb01elds aren\u2019t needed. FreeRTOS sets the uxQueueType \ufb01eld (really the pcHead\n\ufb01eld) to 0 to note that this queue is being used for a mutex. FreeRTOS uses the overloaded pcTail\n\ufb01elds to implement priority inheritance for mutexes.\nIn case you\u2019re not familiar with priority inheritance, I\u2019ll quote Michael Barr again to de\ufb01ne it,\nthis time from his article, \u201cIntroduction to Priority Inversion\u201d3:\n[Priority inheritance] mandates that a lower-priority task inherit the priority of any\nhigher-priority task pending on a resource they share. This priority change should take\nplace as soon as the high-priority task begins to pend; it should end when the resource\nis released.\nFreeRTOS implements priority inheritance using the pxMutexHolder \ufb01eld (which is really just\nthe overloaded-by-#define pcTail \ufb01eld). FreeRTOS records the task that holds a mutex in the\npxMutexHolder \ufb01eld. When a higher-priority task is found to be waiting on a mutex currently\ntaken by a lower-priority task, FreeRTOS \u201cupgrades\u201d the lower-priority task to the priority of the\nhigher-priority task until the mutex is available again.\n3http://www.eetimes.com/discussion/beginner-s-corner/4023947/Introduction-to-Priority-Inversion\nChristopher Svec\n51\n", "page": 65, "type": "text", "section": "Page 65"}
{"text": "3.7\nConclusion\nWe\u2019ve completed our look at the FreeRTOS architecture. Hopefully you now have a good feel for\nhow FreeRTOS\u2019s tasks run and communicate. And if you\u2019ve never looked at any OS\u2019s internals\nbefore, I hope you now have a basic idea of how they work.\nObviously this chapter did not cover all of FreeRTOS\u2019s architecture. Notably, I didn\u2019t mention\nmemory allocation, ISRs, debugging, or MPU support. This chapter also did not discuss how to set\nup or use FreeRTOS. Richard Barry has written an excellent book4, Using the FreeRTOS Real Time\nKernel: A Practical Guide, which discusses exactly that; I highly recommend it if you\u2019re going to\nuse FreeRTOS.\n3.8\nAcknowledgements\nI would like to thank Richard Barry for creating and maintaining FreeRTOS, and for choosing to\nmake it open source. Richard was very helpful in writing this chapter, providing some FreeRTOS\nhistory as well as a very valuable technical review.\nThanks also to Amy Brown and Greg Wilson for pulling this whole AOSA thing together.\nLast and most (the opposite of \u201cnot least\u201d), thanks to my wife Sarah for sharing me with the\nresearch and writing for this chapter. Luckily she knew I was a geek when she married me!\n4http://www.freertos.org/Documentation/FreeRTOS-documentation-and-book.html\n52\nFreeRTOS\n", "page": 66, "type": "text", "section": "Page 66"}
{"text": "[chapter4]\nGDB\nStan Shebs\nGDB, the GNU Debugger, was among the \ufb01rst programs to be written for the Free Software Foun-\ndation, and it has been a staple of free and open source software systems ever since. Originally\ndesigned as a plain Unix source-level debugger, it has since been expanded to a wide range of uses,\nincluding use with many embedded systems, and has grown from a few thousand lines of C to over\nhalf a million.\nThis chapter will delve into the overall internal structure of GDB, showing how it has gradually\ndeveloped as new user needs and new features have come in over time.\n4.1\nThe Goal\nGDB is designed to be a symbolic debugger for programs written in compiled imperative languages\nsuch as C, C++, Ada, and Fortran. Using its original command-line interface, a typical usage looks\nsomething like this:\n% gdb myprog\n[...]\n(gdb) break buggy_function\nBreakpoint 1 at 0x12345678: file myprog.c, line 232.\n(gdb) run 45 92\nStarting program: myprog\nBreakpoint 1, buggy_function (arg1=45, arg2=92) at myprog.c:232\n232\nresult = positive_variable * arg1 + arg2;\n(gdb) print positive_variable\n$$1 = -34\n(gdb)\nGDB shows something that is not right, the developer says \u201caha\u201d or \u201chmmm\u201d, and then has to decide\nboth what the mistake is and how to \ufb01x it.\nThe important point for design is that a tool like GDB is basically an interactive toolbox for\npoking around in a program, and as such it needs to be responsive to an unpredictable series of\nrequests. In addition, it will be used with programs that have been optimized by the compiler, and\nprograms that exploit every hardware option for performance, so it needs to have detailed knowledge\ndown to the lowest levels of a system.\nGDB also needs to be able to debug programs compiled by di\ufb00erent compilers (not just the GNU\nC compiler), to debug programs compiled years earlier by long-obsolete versions of compilers, and to\n", "page": 67, "type": "text", "section": "Page 67"}
{"text": "debug programs whose symbolic info is missing, out of date, or simply incorrect; so, another design\nconsideration is that GDB should continue to work and be useful even if data about the program is\nmissing, or corrupted, or simply incomprehensible.\nThe following sections assume a passing familiarity with using GDB from the command line. If\nyou\u2019re new to GDB, give it a try and peruse the manual.[SPS+00]\n4.2\nOrigins of GDB\nGDB is an old program. It came into existence around 1985, written by Richard Stallman along with\nGCC, GNU Emacs, and other early components of GNU. (In those days, there were no public source\ncontrol repositories, and much of the detailed development history is now lost.)\nThe earliest readily available releases are from 1988, and comparison with present-day sources\nshows that only a handful of lines bear much resemblance; nearly all of GDB has been rewritten at\nleast once. Another striking thing about early versions of GDB is that the original goals were rather\nmodest, and much of the work since then has been extension of GDB into environments and usages\nthat were not part of the original plan.\n4.3\nBlock Diagram\nAt the largest scale, GDB can be said to have two sides to it:\n1. The \u201csymbol side\u201d is concerned with symbolic information about the program. Symbolic\ninformation includes function and variable names and types, line numbers, machine register\nusage, and so on. The symbol side extracts symbolic information from the program\u2019s exe-\ncutable \ufb01le, parses expressions, \ufb01nds the memory address of a given line number, lists source\ncode, and in general works with the program as the programmer wrote it.\n2. The \u201ctarget side\u201d is concerned with the manipulation of the target system. It has facilities to\nstart and stop the program, to read memory and registers, to modify them, to catch signals,\nand so on. The speci\ufb01cs of how this is done can vary drastically between systems; most\nUnix-type systems provide a special system call named ptrace that gives one process the\nability to read and write the state of a di\ufb00erent process. Thus, GDB\u2019s target side is mostly\nabout making ptrace calls and interpreting the results. For cross-debugging an embedded\nsystem, however, the target side constructs message packets to send over a wire, and waits for\nresponse packets in return.\nThe two sides are somewhat independent of each other; you can look around your program\u2019s\ncode, display variable types, etc., without actually running the program. Conversely, it is possible to\ndo pure machine-language debugging even if no symbols are available.\nIn the middle, tying the two sides together, is the command interpreter and the main execution\ncontrol loop.\n4.4\nExamples of Operation\nTo take a simple case of how it all ties together, consider the print command from above. The\ncommand interpreter \ufb01nds the print command function, which parses the expression into a simple\ntree structure and then evaluates it by walking the tree. At some point the evaluator will consult the\n54\nGDB\n", "page": 68, "type": "text", "section": "Page 68"}
{"text": "Figure 4.1: Overall structure of GDB\nsymbol table to \ufb01nd out that positive_variable is an integer global variable that is stored at, say,\nmemory address 0x601028. It then calls a target-side function to read the four bytes of memory at\nthat address, and hands the bytes to a formatting function that displays them as a decimal number.\nTo display source code and its compiled version, GDB does a combination of reads from the\nsource \ufb01le and the target system, then uses compiler-generated line number information to connect\nthe two. In the example here, line 232 has the address 0x4004be, line 233 is at 0x4004ce, and so on.\n[...]\n232\nresult = positive_variable * arg1 + arg2;\n0x4004be <+10>:\nmov\n0x200b64(%rip),%eax\n# 0x601028 <positive_variable>\n0x4004c4 <+16>:\nimul -0x14(%rbp),%eax\n0x4004c8 <+20>:\nadd\n-0x18(%rbp),%eax\n0x4004cb <+23>:\nmov\n%eax,-0x4(%rbp)\n233\nreturn result;\n0x4004ce <+26>:\nmov\n-0x4(%rbp),%eax\n[...]\nStan Shebs\n55\n", "page": 69, "type": "text", "section": "Page 69"}
{"text": "The single-stepping command step conceals a complicated dance going on behind the scenes.\nWhen the user asks to step to the next line in the program, the target side is asked to execute only a\nsingle instruction of the program and then stop it again (this is one of the things that ptrace can do).\nUpon being informed that the program has stopped, GDB asks for the program counter (PC) register\n(another target side operation) and then compares it with the range of addresses that the symbol side\nsays is associated with the current line. If the PC is outside that range, then GDB leaves the program\nstopped, \ufb01gures out the new source line, and reports that to the user. If the PC is still in the range of\nthe current line, then GDB steps by another instruction and checks again, repeating until the PC gets\nto a di\ufb00erent line. This basic algorithm has the advantage that it always does the right thing, whether\nthe line has jumps, subroutine calls, etc., and does not require GDB to interpret all the details of the\nmachine\u2019s instruction set. A disadvantage is that there are many interactions with the target for each\nsingle-step which, for some embedded targets, results in noticeably slow stepping.\n4.5\nPortability\nAs a program needing extensive access all the way down to the physical registers on a chip, GDB\nwas designed from the beginning to be portable across a variety of systems. However, its portability\nstrategy has changed considerably over the years.\nOriginally, GDB started out similar to the other GNU programs of the time; coded in a minimal\ncommon subset of C, and using a combination of preprocessor macros and Make\ufb01le fragments to\nadapt to a speci\ufb01c architecture and operating system. Although the stated goal of the GNU project\nwas a self-contained \u201cGNU operating system\u201d, bootstrapping would have to be done on a variety of\nexisting systems; the Linux kernel was still years in the future. The configure shell script is the\n\ufb01rst key step of the process. It can do a variety of things, such as making a symbolic link from a\nsystem-speci\ufb01c \ufb01le to a generic header name, or constructing \ufb01les from pieces, more importantly the\nMake\ufb01le used to build the program.\nPrograms like GCC and GDB have additional portability needs over something like cat or diff,\nand over time, GDB\u2019s portability bits came to be separated into three classes, each with its own\nMake\ufb01le fragment and header \ufb01le.\n\u2022 \u201cHost\u201d de\ufb01nitions are for the machine that GDB itself runs on, and might include things\nlike the sizes of the host\u2019s integer types. Originally done as human-written header \ufb01les, it\neventually occurred to people that they could be calculated by having configure run little test\nprograms, using the same compiler that was going to be used to build the tool. This is what\nautoconf[aut12] is all about, and today nearly all GNU tools and many (if not most) Unix\nprograms use autoconf-generated con\ufb01gure scripts.\n\u2022 \u201cTarget\u201d de\ufb01nitions are speci\ufb01c to the machine running the program being debugged. If the\ntarget is the same as the host, then we are doing \u201cnative\u201d debugging, otherwise it is \u201ccross\u201d\ndebugging, using some kind of wire connecting the two systems. Target de\ufb01nitions fall in turn\ninto two main classes:\n\u2013 \u201cArchitecture\u201d de\ufb01nitions: These de\ufb01ne how to disassemble machine code, how to walk\nthrough the call stack, and which trap instruction to insert at breakpoints. Originally\ndone with macros, they were migrated to regular C accessed by via \u201cgdbarch\u201d objects,\ndescribed in more depth below.\n\u2013 \u201cNative\u201d de\ufb01nitions: These de\ufb01ne the speci\ufb01cs of arguments to ptrace (which vary\nconsiderably between \ufb02avors of Unix), how to \ufb01nd shared libraries that have been loaded,\n56\nGDB\n", "page": 70, "type": "text", "section": "Page 70"}
{"text": "and so forth, which only apply to the native debugging case. Native de\ufb01nitions are a last\nholdout of 1980s-style macros, although most are now \ufb01gured out using autoconf.\n4.6\nData Structures\nBefore drilling down into the parts of GDB, let\u2019s take a look at the major data structures that GDB\nworks with. As GDB is a C program, these are implemented as structs rather than as C++-style\nobjects, but in most cases they are treated as objects, and here we follow GDBers\u2019 frequent practice\nin calling them objects.\nBreakpoints\nA breakpoint is the main kind of object that is directly accessible to the user. The user creates a\nbreakpoint with the break command, whose argument speci\ufb01es a location, which can be a function\nname, a source line number, or a machine address. GDB assigns a small positive integer to the\nbreakpoint object, which the user subsequently uses to operate on the breakpoint. Within GDB, the\nbreakpoint is a C struct with a number of \ufb01elds. The location gets translated to a machine address,\nbut is also saved in its original form, since the address may change and need recomputation, for\ninstance if the program is recompiled and reloaded into a session.\nSeveral kinds of breakpoint-like objects actually share the breakpoint struct, including watch-\npoints, catchpoints, and tracepoints. This helps ensure that creation, manipulation, and deletion\nfacilities are consistently available.\nThe term \u201clocation\u201d also refers to the memory addresses at which the breakpoint is to be installed.\nIn the cases of inline functions and C++ templates, it may be that a single user-speci\ufb01ed breakpoint\nmay correspond to several addresses; for instance, each inlined copy of a function entails a separate\nlocation for a breakpoint that is set on a source line in the function\u2019s body.\nSymbols and Symbol Tables\nSymbol tables are a key data structure to GDB, and can be quite large, sometimes growing to occupy\nmultiple gigabytes of RAM. To some extent, this is unavoidable; a large application in C++ can have\nmillions of symbols in its own right, and it pulls in system header \ufb01les which can have millions more\nsymbols. Each local variable, each named type, each value of an enum\u2014all of these are separate\nsymbols.\nGDB uses a number of tricks to reduce symbol table space, such as partial symbol tables (more\nabout those later), bit \ufb01elds in structs, etc.\nIn addition to symbol tables that basically map character strings to address and type information,\nGDB builds line tables that support lookup in two directions; from source lines to addresses, and\nthen from addresses back to source lines. (For instance, the single-stepping algorithm described\nearlier crucially depends on the address-to-source mapping.)\nStack Frames\nThe procedural languages for which GDB was designed share a common runtime architecture, in\nthat function calls cause the program counter to be pushed on a stack, along with some combination\nof function arguments and local arguments. The assemblage is called a stack frame, or \u201cframe\u201d for\nStan Shebs\n57\n", "page": 71, "type": "text", "section": "Page 71"}
{"text": "short, and at any moment in a program\u2019s execution, the stack consists of a sequence of frames chained\ntogether. The details of a stack frame vary radically from one chip architecture to the next, and is\nalso dependent on the operating system, compiler, and optimization options.\nA port of GDB to a new chip may need a considerable volume of code to analyze the stack,\nas programs (especially buggy ones, which are the ones debugger users are mostly interested in)\ncan stop anywhere, with frames possibly incomplete, or partly overwritten by the program. Worse,\nconstructing a stack frame for each function call slows down the application, and a good optimizing\ncompiler will take every opportunity to simplify stack frames, or even eliminate them altogether,\nsuch as for tail calls.\nThe result of GDB\u2019s chip-speci\ufb01c stack analysis is recorded in a series of frame objects. Originally\nGDB kept track of frames by using the literal value of a \ufb01xed-frame pointer register. This approach\nbreaks down for inlined function calls and other kinds of compiler optimizations, and starting in\n2002, GDBers introduced explicit frame objects that recorded what had been \ufb01gured out about each\nframe, and were linked together, mirroring the program\u2019s stack frames.\nExpressions\nAs with stack frames, GDB assumes a degree of commonality among the expressions of the various\nlanguages it supports, and represents them all as a tree structure built out of node objects. The set of\nnode types is e\ufb00ectively a union of all the types of expressions possible in all the di\ufb00erent languages;\nunlike in the compiler, there is no reason to prevent the user from trying to subtract a Fortran variable\nfrom a C variable\u2014perhaps the di\ufb00erence of the two is an obvious power of two, and that gives us\nthe \u201caha\u201d moment.\nValues\nThe result of evaluation may itself be more complex than an integer or memory address, and GDB\nalso retains evaluation results in a numbered history list, which can then be referred to in later\nexpressions. To make all this work, GDB has a data structure for values. Value structs have a\nnumber of \ufb01elds recording various properties; important ones include an indication of whether the\nvalue is an r-value or l-value (l-values can be assigned to, as in C), and whether the value is to be\nconstructed lazily.\n4.7\nThe Symbol Side\nThe symbol side of GDB is mainly responsible for reading the executable \ufb01le, extracting any symbolic\ninformation it \ufb01nds, and building it into a symbol table.\nThe reading process starts with the BFD library. BFD is a sort of universal library for handling\nbinary and object \ufb01les; running on any host, it can read and write the original Unix a.out format,\nCOFF (used on System V Unix and MS Windows), ELF (modern Unix, GNU/Linux, and most\nembedded systems), and some other \ufb01le formats. Internally, the library has a complicated structure\nof C macros that expand into code incorporating the arcane details of object \ufb01le formats for dozens\nof di\ufb00erent systems. Introduced in 1990, BFD is also used by the GNU assembler and linker, and its\nability to produce object \ufb01les for any target is key to cross-development using GNU tools. (Porting\nBFD is also a key \ufb01rst step in porting the tools to a new target.)\n58\nGDB\n", "page": 72, "type": "text", "section": "Page 72"}
{"text": "GDB only uses BFD to read \ufb01les, using it to pull blocks of data from the executable \ufb01le into\nGDB\u2019s memory. GDB then has two levels of reader functions of its own. The \ufb01rst level is for basic\nsymbols, or \u201cminimal symbols\u201d, which are just the names that the linker needs to do its work. These\nare strings with addresses and not much else; we assume that addresses in text sections are functions,\naddresses in data sections are data, and so forth.\nThe second level is detailed symbolic information, which typically has its own format di\ufb00erent\nfrom the basic executable \ufb01le format; for instance, information in the DWARF debug format is\ncontained in specially named sections of an ELF \ufb01le. By contrast, the old stabs debug format of\nBerkeley Unix used specially \ufb02agged symbols stored in the general symbol table.\nThe code for reading symbolic information is somewhat tedious, as the di\ufb00erent symbolic formats\nencode every kind of type information that could be in a source program, but each goes about it in\nits own idiosyncratic way. A GDB reader just walks through the format, constructing GDB symbols\nthat we think correspond to what the symbolic format intends.\nPartial Symbol Tables\nFor a program of signi\ufb01cant size (such as Emacs or Firefox), construction of the symbol table can\ntake quite a while, maybe even several minutes. Measurements consistently show that the time is not\nin \ufb01le reading as one might expect, but in the in-memory construction of GDB symbols. There are\nliterally millions of small interconnected objects involved, and the time adds up.\nMost of the symbolic information will never be looked at in a session, since it is local to functions\nthat the user may never examine. So, when GDB \ufb01rst pulls in a program\u2019s symbols, it does a cursory\nscan through the symbolic information, looking for just the globally visible symbols and recording\nonly them in the symbol table. Complete symbolic info for a function or method is \ufb01lled in only if\nthe user stops inside it.\nPartial symbol tables allow GDB to start up in only a few seconds, even for large programs.\n(Shared library symbols are also dynamically loaded, but the process is rather di\ufb00erent. Typically\nGDB uses a system-speci\ufb01c technique to be noti\ufb01ed when the library is loaded, then builds a symbol\ntable with functions at the addresses that were decided on by the dynamic linker.)\nLanguage Support\nSource language support mainly consists of expression parsing and value printing. The details of\nexpression parsing are left up to each language, but in the general the parser is based on a Yacc\ngrammar fed by a hand-crafted lexical analyzer. In keeping with GDB\u2019s goal of providing more\n\ufb02exibility to the interactive user, the parser is not expected to be especially stringent; for instance,\nif it can guess at a reasonable type for an expression, it will simply assume that type, rather than\nrequire the user to add a cast or type conversion.\nSince the parser need not handle statements or type declarations, it is much simpler than the full\nlanguage parser. Similarly, for printing, there are just a handful of types of values that need to be\ndisplayed, and oftentimes the language-speci\ufb01c print function can call out to generic code to \ufb01nish\nthe job.\n4.8\nTarget Side\nThe target side is all about manipulation of program execution and raw data. In a sense, the target side\nis a complete low-level debugger; if you are content to step by instructions and dump raw memory,\nStan Shebs\n59\n", "page": 73, "type": "text", "section": "Page 73"}
{"text": "you can use GDB without needing any symbols at all. (You may end up operating in this mode\nanyway, if the program happens to stop in a library whose symbols have been stripped out.)\nTarget Vectors and the Target Stack\nOriginally the target side of GDB was composed of a handful of platform-speci\ufb01c \ufb01les that handled\nthe details of calling ptrace, launching executables, and so on. This is not su\ufb03ciently \ufb02exible for\nlong-running debugging sessions, in which the user might switch from local to remote debugging,\nswitch from \ufb01les to core dumps to live programs, attach and detach, etc., so in 1990 John Gilmore\nredesigned the target side of GDB to send all target-speci\ufb01c operations through the target vector,\nwhich is basically a class of objects, each of which de\ufb01nes the the speci\ufb01cs of a type of target system.\nEach target vector is implemented as a structure of several dozen function pointers (often called\n\u201cmethods\u201d), whose purposes range from the reading and writing of memory and registers, to resuming\nprogram execution, to setting parameters for the handling of shared libraries. There are about 40\ntarget vectors in GDB, ranging from the well-used target vector for Linux to obscure vectors such as\nthe one that operates a Xilinx MicroBlaze. Core dump support uses a target vector that gets data by\nreading a core\ufb01le, and there is another target vector that reads data from the executable.\nIt is often useful to blend methods from several target vectors. Consider the printing of an\ninitialized global variable on Unix; before the program starts running, printing the variable should\nwork, but at that point there is no process to read, and bytes need to come from the executable\u2019s .data\nsection. So, GDB uses the target vector for executables and reads from the binary \ufb01le. But while the\nprogram is running, the bytes should instead come from the process\u2019s address space. So, GDB has a\n\u201ctarget stack\u201d where the target vector for live processes is pushed on top of the executable\u2019s target\nvector when the process starts running, and is popped when it exits.\nIn reality, the target stack turns out not to be quite as stack-like as one might think. Target vectors\nare not really orthogonal to each other; if you have both an executable and a live process in the\nsession, while it makes sense to have the live process\u2019s methods override the executable\u2019s methods,\nit almost never makes sense to do the reverse. So GDB has ended up with a notion of a stratum in\nwhich \u201cprocess-like\u201d target vectors are all at one stratum, while \u201c\ufb01le-like\u201d target vectors get assigned\nto a lower stratum, and target vectors can get inserted as well as pushed and popped.\n(Although GDB maintainers don\u2019t like the target stack much, no one has proposed\u2014or prototyped\u2014\nany better alternative.)\nGdbarch\nAs a program that works directly with the instructions of a CPU, GDB needs in-depth knowledge\nabout the details of the chip. It needs to know about all the registers, the sizes of the di\ufb00erent kinds\nof data, the size and shape of the address space, how the calling convention works, what instruction\nwill cause a trap exception, and so on. GDB\u2019s code for all this typically ranges from 1,000 to over\n10,000 lines of C, depending on the architecture\u2019s complexity.\nOriginally this was handled using target-speci\ufb01c preprocessor macros, but as the debugger\nbecame more sophisticated, these got larger and larger, and over time long macro de\ufb01nitions were\nmade into regular C functions called from the macros. While this helped, it did not help much with\narchitectural variants (ARM vs. Thumb, 32-bit vs. 64-bit versions of MIPS or x86, etc.), and worse,\nmultiple-architecture designs were on the horizon, for which macros would not work at all. In 1995,\n60\nGDB\n", "page": 74, "type": "text", "section": "Page 74"}
{"text": "I proposed solving this with an object-based design, and starting in 1998 Cygnus Solutions1 funded\nAndrew Cagney to start the changeover. It took several years and contributions from dozens of\nhackers to \ufb01nish the job, a\ufb00ecting perhaps 80,000 lines of code in all.\nThe introduced constructs are called gdbarch objects, and at this point may contain as many as\n130 methods and variables de\ufb01ning a target architecture, although a simple target might only need a\ndozen or so of these.\nTo get a sense of how the old and new ways compare, see the declaration that x86 long doubles\nare 96 bits in size from gdb/config/i386/tm-i386.h, circa 2002:\n#define TARGET_LONG_DOUBLE_BIT 96\nand from gdb/i386-tdep.c, in 2012:\ni386_gdbarch_init( [...] )\n{\n[...]\nset_gdbarch_long_double_bit (gdbarch, 96);\n[...]\n}\nExecution Control\nThe heart of GDB is its execution control loop. We touched on it earlier when describing single-\nstepping over a line; the algorithm entailed looping over multiple instructions until \ufb01nding one\nassociated with a di\ufb00erent source line. The loop is called wait_for_inferior, or \u201cw\ufb01\u201d for short.\nConceptually it is inside the main command loop, and is only entered for commands that cause\nthe program to resume execution. When the user types continue or step and then waits while\nnothing seems to be happening, GDB may in fact be quite busy. In addition to the single-stepping\nloop mentioned above, the program may be hitting trap instructions and reporting the exception\nto GDB. If the exception is due to the trap being a breakpoint inserted by GDB, it then tests the\nbreakpoint\u2019s condition, and if false, it removes the trap, single-steps the original instruction, re-inserts\nthe trap, and then lets the program resume. Similarly, if a signal is raised, GDB may choose to ignore\nit, or handle it one of several ways speci\ufb01ed in advance.\nAll of this activity is managed by wait_for_inferior. Originally this was a simple loop,\nwaiting for the target to stop and then deciding what to do about it, but as ports to various systems\nneeded special handling, it grew to a thousand lines, with goto statements criss-crossing it for poorly\nunderstood reasons. For instance, with the proliferation of Unix variants, there was no one person\nwho understood all their \ufb01ne points, nor did we have access to all of them for regression testing, so\nthere was a strong incentive to modify the code in a way that exactly preserved behavior for existing\nports\u2014and a goto skipping over part of the loop was an all-too-easy tactic.\nThe single big loop was also a problem for any kind of asynchronous handling or debugging of\nthreaded programs, in which the user wants to start and stop a single thread while allowing the rest\nof the program to continue running.\nThe conversion to an event-oriented model took several years. I broke up wait_for_inferior in\n1999, introducing an execution control state structure to replace the pile of local and global variables,\n1Cygnus Solutions was a company founded in 1989 to provide commercial support for free software. It was acquired in 2000\nby Red Hat.\nStan Shebs\n61\n", "page": 75, "type": "text", "section": "Page 75"}
{"text": "and converting the tangle of jumps into smaller independent functions. At the same time Elena\nZannoni and others introduced event queues that included both input from the user and noti\ufb01cations\nfrom the inferior.\nThe Remote Protocol\nAlthough GDB\u2019s target vector architecture allows for a broad variety of ways to control a program\nrunning on a di\ufb00erent computer, we have a single preferred protocol. It does not have a distinguishing\nname, and is typically called just the \u201cremote protocol\u201d, \u201cGDB remote protocol\u201d, \u201cremote serial\nprotocol\u201d (abbreviating to \u201cRSP\u201d), \u201cremote.c protocol\u201d (after the source \ufb01le that implements it), or\nsometimes the \u201cstub protocol\u201d, referring to the target\u2019s implementation of the protocol.\nThe basic protocol is simple, re\ufb02ecting the desire to have it work on small embedded systems of\nthe 1980s, whose memories were measured in kilobytes. For instance, the protocol packet $g requests\nall registers, and expects a reply consisting of all the bytes of all the registers, all run together\u2014the\nassumption being that their number, size, and order will match what GDB knows about.\nThe protocol expects a single reply to each packet sent, and assumes the connection is reliable,\nadding only a checksum to packets sent (so $g is really sent as $g#67 over the wire).\nAlthough there are only a handful of required packet types (corresponding to the half-dozen\ntarget vector methods that are most important), scores of additional optional packets have been added\nover the years, to support everything from hardware breakpoints, to tracepoints, to shared libraries.\nOn the target itself, the implementation of the remote protocol can take a wide variety of\nforms. The protocol is fully documented in the GDB manual, which means that it is possible to\nwrite an implementation that is not encumbered with a GNU license, and indeed many equipment\nmanufacturers have incorporated code that speaks the GDB remote protocol, both in the lab and in\nthe \ufb01eld. Cisco\u2019s IOS, which runs much of their networking equipment, is one well-known example.\nA target\u2019s implementation of the protocol is often referred to as a \u201cdebugging stub\u201d, or just \u201cstub\u201d,\nconnoting that it is not expected to do very much work on its own. The GDB sources include a few\nexample stubs, which are typically about 1,000 lines of low-level C. On a totally bare board with\nno OS, the stub must install its own handlers for hardware exceptions, most importantly to catch\ntrap instructions. It will also need serial driver code if the hardware link is a serial line. The actual\nprotocol handling is simple, since all the required packets are single characters that can be decoded\nwith a switch statement.\nAnother approach to remote protocol is to build a \u201csprite\u201d that interfaces between GDB and\ndedicated debugging hardware, including JTAG devices, \u201cwigglers\u201d, etc. Oftentimes these devices\nhave a library that must run on the computer that is physically connected to a target board, and often\nthe library API is not architecturally compatible with GDB\u2019s internals. So, while con\ufb01gurations of\nGDB have called hardware control libraries directly, it has proven simpler to run the sprite as an\nindependent program that understands remote protocol and translates the packets into device library\ncalls.\nGDBserver\nThe GDB sources do include one complete and working implementation of the target side of the\nremote protocol: GDBserver. GDBserver is a native program that runs under the target\u2019s operating\nsystem, and controls other programs on the target OS using its native debugging support, in response\nto packets received via remote protocol. In other words, it acts as a sort of proxy for native debugging.\n62\nGDB\n", "page": 76, "type": "text", "section": "Page 76"}
{"text": "GDBserver doesn\u2019t do anything that native GDB can\u2019t do; if your target system can run GDBserver,\nthen theoretically it can run GDB. However, GDBserver is 10 times smaller and doesn\u2019t need to\nmanage symbol tables, so it is very convenient for embedded GNU/Linux usages and the like.\nFigure 4.2: GDBserver\nGDB and GDBserver share some code, but while it is an obvious idea to encapsulate OS-speci\ufb01c\nprocess control, there are practical di\ufb03culties with separating out tacit dependencies in native GDB,\nand the transition has gone slowly.\n4.9\nInterfaces to GDB\nGDB is fundamentally a command-line debugger. Over time people have tried various schemes to\nmake it into a graphical windowed debugger but, despite all the time and e\ufb00ort, none of these are\nuniversally accepted.\nCommand-Line Interface\nThe command-line interface uses the standard GNU library readline to handle the character-by-\ncharacter interaction with the user. Readline takes care of things like line editing and command\ncompletion; the user can do things like use cursor keys to go back in a line and \ufb01x a character.\nGDB then takes the command returned by readline and looks it up using a cascaded structure\nof command tables, where each successive word of the command selects an additional table. For\ninstance set print elements 80 involves three tables; the \ufb01rst is the table of all commands, the\nsecond is a table of options that can be set, and the third is a table of value printing options, of\nwhich elements is the one that limits the number of objects printed from an aggregate like a string\nor array. Once the cascaded tables have called an actual command-handling function, it takes control,\nand argument parsing is completely up to the function. Some commands, such as run, handle their\narguments similarly to traditional C argc/argv standards, while others, such as print, assume that\nthe remainder of the line is a single programming language expression, and give the entire line over\nto a language-speci\ufb01c parser.\nStan Shebs\n63\n", "page": 77, "type": "text", "section": "Page 77"}
{"text": "Machine Interface\nOne way to provide a debugging GUI is to use GDB as a sort of \u201cbackend\u201d to a graphical interface\nprogram, translating mouse clicks into commands and formatting print results into windows. This\nhas been made to work several times, including KDbg and DDD (Data Display Debugger), but it\u2019s\nnot the ideal approach because sometimes results are formatted for human readability, omitting\ndetails and relying on human ability to supply context.\nTo solve this problem, GDB has an alternate \u201cuser\u201d interface, known as the Machine Interface\nor MI for short. It is still fundamentally a command-line interface, but both commands and results\nhave additional syntax that makes everything explicit\u2014each argument is bounded by quotes, and\ncomplex output has delimiters for subgroups and parameter names for component pieces. In addition,\nMI commands can be pre\ufb01xed with sequence identi\ufb01ers that are echoed back in results, ensuring\nreported results are matched up with the right commands.\nTo see how the two forms compare, here is a normal step command and GDB\u2019s response:\n(gdb) step\nbuggy_function (arg1=45, arg2=92) at ex.c:232\n232\nresult = positive_variable * arg1 + arg2;\nWith the MI, the input and output are more verbose, but easier for other software to parse accurately:\n4321-exec-step\n4321^done,reason=\"end-stepping-range\",\nframe={addr=\"0x00000000004004be\",\nfunc=\"buggy_function\",\nargs=[{name=\"arg1\",value=\"45\"},\n{name=\"arg2\",value=\"92\"}],\nfile=\"ex.c\",\nfullname=\"/home/sshebs/ex.c\",\nline=\"232\"}\nThe Eclipse[ecl12] development environment is the most notable client of the MI.\nOther User Interfaces\nAdditional frontends include a tcl/tk-based version called GDBtk or Insight, and a curses-based\ninterface called the TUI, originally contributed by Hewlett-Packard. GDBtk is a conventional\nmulti-paned graphical interface built using the tk library, while the TUI is a split-screen interface.\n4.10\nDevelopment Process\nMaintainers\nAs an original GNU program, GDB development started out following the \u201ccathedral\u201d model of\ndevelopment. Originally written by Stallman, GDB then went through a succession of \u201cmaintainers\u201d,\neach of whom was a combination of architect, patch reviewer, and release manager, with access to\nthe source repository limited to a handful of Cygnus employees.\n64\nGDB\n", "page": 78, "type": "text", "section": "Page 78"}
{"text": "In 1999, GDB migrated to a public source repository and expanded to a team of several dozen\nmaintainers, aided by scores of individuals with commit privileges. This has accelerated development\nconsiderably, with the 10-odd commits each week growing to 100 or more.\nTesting Testing\nAs GDB is highly system-speci\ufb01c, has a great many ports to systems ranging from the smallest to\nthe largest in computerdom, and has hundreds of commands, options, and usage styles, it is di\ufb03cult\nfor even an experienced GDB hacker to anticipate all the e\ufb00ects of a change.\nThis is where the test suite comes in. The test suite consists of a number of test programs\ncombined with expect scripts, using a tcl-based testing framework called DejaGNU. The basic\nmodel is that each script drives GDB as it debugs a test program, sending commands and then\npattern-matching the output against regular expressions.\nThe test suite also has the ability to run cross-debugging to both live hardware and simulators,\nand to have tests that are speci\ufb01c to a single architecture or con\ufb01guration.\nAt the end of 2011, the test suite includes some 18,000 test cases, which include tests of basic\nfunctionality, language-speci\ufb01c tests, architecture-speci\ufb01c tests, and MI tests. Most of these are\ngeneric and are run for any con\ufb01guration. GDB contributors are expected to run the test suite on\npatched sources and observe no regressions, and new tests are expected to accompany each new\nfeature. However, as no one has access to all platforms that might be a\ufb00ected by a change, it is rare to\nget all the way to zero failures; 10\u201320 failures is usually reasonable for a trunk snapshot con\ufb01gured\nfor native debugging, and some embedded targets will have more failures.\n4.11\nLessons Learned\nOpen Development Wins\nGDB started out as an exemplar of the \u201ccathedral\u201d development process, in which the maintainer\nkeeps close control of the sources, with the outside world only seeing progress via periodic snapshots.\nThis was rationalized by the relative infrequence of patch submissions, but the closed process was\nactually discouraging patches. Since the open process has been adopted, the number of patches is\nmuch larger than ever before, and quality is just as good or better.\nMake a Plan, but Expect It to Change\nThe open source development process is intrinsically somewhat chaotic, as di\ufb00erent individuals work\non the code for a while, then fall away, leaving others to continue on.\nHowever, it still makes sense to make a development plan and publish it. It helps guide developers\nas they work on related tasks, it can be shown to potential funders, and it lets volunteers think about\nwhat they can do to advance it.\nBut don\u2019t try to force dates or time frames; even if everyone is enthusiastic about a direction, it is\nunlikely that people can guarantee full-time e\ufb00ort for long enough to \ufb01nish by a chosen date.\nFor that matter, don\u2019t cling to the plan itself if it has become outdated. For a long time, GDB had\na plan to restructure as a library, libgdb, with a well-de\ufb01ned API, that could be linked into other\nprograms (in particular ones with GUIs); the build process was even changed to build a libgdb.a as\nan intermediate step. Although the idea has come up periodically since then, the primacy of Eclipse\nStan Shebs\n65\n", "page": 79, "type": "text", "section": "Page 79"}
{"text": "and MI meant that the library\u2019s main rationale has been sidestepped, and as of January 2012 we have\nabandoned the library concept and are expunging the now-pointless bits of code.\nThings Would Be Great If We Were In\ufb01nitely Intelligent\nAfter seeing some of the changes we made, you might be thinking: Why didn\u2019t we do things right in\nthe \ufb01rst place? Well, we just weren\u2019t smart enough.\nCertainly we could have anticipated that GDB was going to be tremendously popular, and was\ngoing to be ported to dozens and dozens of architectures, both native and cross. If we had known\nthat, we could have started with the gdbarch objects, instead of spending years upgrading old macros\nand global variables; ditto for the target vector.\nCertainly we could have anticipated GDB was going to be used with GUIs. After all in 1986\nboth the Mac and the X Window System had already been out for two years! Instead of designing a\ntraditional command interface, we could have set it up to handle events asynchronously.\nThe real lesson though is that not that GDBers were dumb, but that we couldn\u2019t possibly have\nbeen smart enough to anticipate how GDB would need to evolve. In 1986 it was not at all clear that\nthe windows-and-mouse interface was going to become ubiquitous; if the \ufb01rst version of GDB was\nperfectly adapted for GUI use, we\u2019d have looked like geniuses, but it would have been sheer luck.\nInstead, by making GDB useful in a more limited scope, we built a user base that enabled more\nextensive development and re-engineering later.\nLearn to Live with Incomplete Transitions\nTry to complete transitions, but they may take a while; expect to live with them being incomplete.\nAt the GCC Summit in 2003, Zack Weinberg lamented the \u201cincomplete transitions\u201d in GCC,\nwhere new infrastructure had been introduced, but the old infrastructure could not be removed. GDB\nhas these also, but we can point to a number of transitions that have been completed, such as the\ntarget vector and gdbarch. Even so, they can take a number of years to complete, and in the meantime\none has to keep the debugger running.\nDon\u2019t Get Too Attached to the Code\nWhen you spend a long time with a single body of code, and it\u2019s an important program that also pays\nthe bills, it\u2019s easy to get attached to it, and even to mold your thinking to \ufb01t the code, rather than the\nother way around.\nDon\u2019t.\nEverything in the code originated with a series of conscious decisions: some inspired, some less\nso. The clever space-saving trick of 1991 is a pointless complexity with the multi-gigabyte RAMs of\n2011.\nGDB once supported the Gould supercomputer. When they turned o\ufb00the last machine, around\n2000, there really wasn\u2019t any point in keeping those bits around. That episode was the genesis of an\nobsoletion process for GDB, and most releases now include the retirement of some piece or another.\nIn fact, there are a number of radical changes on the table or already underway, ranging from the\nadoption of Python for scripting, to support for debugging of highly parallel multicore systems, to\nrecoding into C++. The changes may take years to complete; all the more reason to get started on\nthem now.\n66\nGDB\n", "page": 80, "type": "text", "section": "Page 80"}
{"text": "[chapter5]\nThe Glasgow Haskell Compiler\nSimon Marlow and Simon Peyton Jones\nThe Glasgow Haskell Compiler (GHC) started as part of an academic research project funded by the\nUK government at the beginning of the 1990s, with several goals in mind:\n\u2022 To make freely available a robust and portable compiler for Haskell that generates high\nperformance code;\n\u2022 To provide a modular foundation that other researchers can extend and develop;\n\u2022 To learn how real programs behave, so that we can design and build better compilers.\nGHC is now over 20 years old, and has been under continuous active development since its\ninception. Today, GHC releases are downloaded by hundreds of thousands of people, the online\nrepository of Haskell libraries has over 3,000 packages, GHC is used to teach Haskell in many\nundergraduate courses, and there are a growing number of instances of Haskell being depended upon\ncommercially.\nOver its lifetime GHC has generally had around two or three active developers, although the\nnumber of people who have contributed some code to GHC is in the hundreds. While the ultimate\ngoal for us, the main developers of GHC, is to produce research rather than code, we consider\ndeveloping GHC to be an essential prerequisite: the artifacts of research are fed back into GHC,\nso that GHC can then be used as the basis for further research that builds on these previous ideas.\nMoreover, it is important that GHC is an industrial-strength product, since this gives greater credence\nto research results produced with it. So while GHC is stu\ufb00ed full of cutting-edge research ideas, a\ngreat deal of e\ufb00ort is put into ensuring that it can be relied on for production use. There has often\nbeen some tension between these two seemingly contradictory goals, but by and large we have found\na path that is satisfactory both from the research and the production-use angles.\nIn this chapter we want to give an overview of the architecture of GHC, and focus on a handful\nof the key ideas that have been successful in GHC (and a few that haven\u2019t). Hopefully throughout the\nfollowing pages you will gain some insight into how we managed to keep a large software project\nactive for over 20 years without it collapsing under its own weight, with what is generally considered\nto be a very small development team.\n5.1\nWhat is Haskell?\nHaskell is a functional programming language, de\ufb01ned by a document known as the \u201cHaskell Report\u201d\nof which the latest revision is Haskell 2010 [Mar10]. Haskell was created in 1990 by several members\n", "page": 81, "type": "text", "section": "Page 81"}
{"text": "of the academic research community interested in functional languages, to address the lack of a\ncommon language that could be used as a focus for their research.\nTwo features of Haskell stand out amongst the programming languages crowd:\n\u2022 It is purely functional. That is, functions cannot have side e\ufb00ects or mutate data; for a given\nset of inputs (arguments) a function always gives the same result. The bene\ufb01ts of this model\nfor reasoning about code (and, we believe, writing code) are clear, but integrating input/output\ninto the purely functional setting proved to be a signi\ufb01cant challenge. Fortunately an elegant\nsolution in the form of monads was discovered, which not only allowed input/output to be\nneatly integrated with purely functional code, but introduced a powerful new abstraction that\nrevolutionised coding in Haskell (and subsequently had an impact on other languages too).\n\u2022 It is lazy. This refers to the evaluation strategy of the language: most languages use strict\nevaluation in which the arguments to a function are evaluated before the function is called,\nwhereas in Haskell the arguments to a function are passed unevaluated, and only evaluated on\ndemand. This aspect of Haskell also has bene\ufb01ts for reasoning about programs, but more than\nanything else serves as a barrier to prevent the leakage of impure non-functional features into\nthe language: such features fundamentally cannot work in conjunction with lazy semantics.\nHaskell is also strongly-typed, while supporting type inference which means that type annotations\nare rarely necessary.\nThose interested in a complete history of Haskell should read [HHPW07].\n5.2\nHigh-Level Structure\nAt the highest level, GHC can be divided into three distinct chunks:\n\u2022 The compiler itself. This is essentially a Haskell program whose job is to convert Haskell\nsource code into executable machine code.\n\u2022 The Boot Libraries. GHC comes with a set of libraries that we call the boot libraries, because\nthey constitute the libraries that the compiler itself depends on. Having these libraries in\nthe source tree means that GHC can bootstrap itself. Some of these libraries are very tightly\ncoupled to GHC, because they implement low-level functionality such as the Int type in terms\nof primitives de\ufb01ned by the compiler and runtime system. Other libraries are more high-level\nand compiler-independent, such as the Data.Map library.\n\u2022 The Runtime System (RTS). This is a large library of C code that handles all the tasks associated\nwith running the compiled Haskell code, including garbage collection, thread scheduling,\npro\ufb01ling, exception handling and so on. The RTS is linked into every compiled Haskell\nprogram. The RTS represents a signi\ufb01cant chunk of the development e\ufb00ort put into GHC, and\nthe design decisions made there are responsible for some of Haskell\u2019s key strengths, such as\nits e\ufb03cient support for concurrency and parallelism. We\u2019ll describe the RTS in more detail in\nSection 5.5.\nIn fact, these three divisions correspond exactly to three subdirectories of a GHC source tree:\ncompiler, libraries, and rts respectively.\nWe won\u2019t spend much time here discussing the boot libraries, as they are largely uninteresting\nfrom an architecture standpoint. All the key design decisions are embodied in the compiler and\nruntime system, so we will devote the rest of this chapter to discussing these two components.\n68\nThe Glasgow Haskell Compiler\n", "page": 82, "type": "text", "section": "Page 82"}
{"text": "Code Metrics\nThe last time we measured the number of lines in GHC was in 19921, so it is interesting to look at\nhow things have changed since then. Figure 5.1 gives a breakdown of the number of lines of code in\nGHC divided up into the major components, comparing the current tallies with those from 1992.\nModule\nLines (1992)\nLines (2011)\nIncrease\nCompiler\nMain\n997\n11,150\n11.2\nParser\n1,055\n4,098\n3.9\nRenamer\n2,828\n4,630\n1.6\nType checking\n3,352\n24,097\n7.2\nDesugaring\n1,381\n7,091\n5.1\nCore transformations\n1,631\n9,480\n5.8\nSTG transformations\n814\n840\n1\nData-Parallel Haskell\n\u2014\n3,718\n\u2014\nCode generation\n2913\n11,003\n3.8\nNative code generation\n\u2014\n14,138\n\u2014\nLLVM code generation\n\u2014\n2,266\n\u2014\nGHCi\n\u2014\n7,474\n\u2014\nHaskell abstract syntax\n2,546\n3,700\n1.5\nCore language\n1,075\n4,798\n4.5\nSTG language\n517\n693\n1.3\nC-- (was Abstract C)\n1,416\n7,591\n5.4\nIdenti\ufb01er representations\n1,831\n3,120\n1.7\nType representations\n1,628\n3,808\n2.3\nPrelude de\ufb01nitions\n3,111\n2,692\n0.9\nUtilities\n1,989\n7,878\n3.96\nPro\ufb01ling\n191\n367\n1.92\nCompiler Total\n28,275\n139,955\n4.9\nRuntime System\nAll C and C-- code\n43,865\n48,450\n1.10\nFigure 5.1: Lines of code in GHC, past and present\nThere are some notable aspects of these \ufb01gures:\n\u2022 Despite nearly 20 years of non-stop development the compiler has only increased in size by\na factor of 5, from around 28,000 to around 140,000 lines of Haskell code. We obsessively\nrefactor while adding new code, keeping the code base as fresh as possible.\n\u2022 There are several new components, although these only account for about 28,000 new lines.\nMuch of the new components are concerned with code generation: native code generators\nfor various processors, and an LLVM2 code generator. The infrastructure for the interactive\ninterpreter GHCi also added over 7,000 lines.\n1\u201cThe Glasgow Haskell compiler: a technical overview\u201d, JFIT technical conference digest, 1992\n2Formerly the \u201cLow Level Virtual Machine\u201d, the LLVM project includes a generic code-generator with targets for many\ndi\ufb00erent processors. For more information see http://llvm.org/, and the chapter on LLVM in Volume 1 of The Architecture\nof Open Source Applications.\nSimon Marlow and Simon Peyton Jones\n69\n", "page": 83, "type": "text", "section": "Page 83"}
{"text": "\u2022 The biggest increase in a single component is the type checker, where over 20,000 lines were\nadded. This is unsurprising given that much of the recent research using GHC has been into\nnew type system extensions (for example GADTs [PVWW06] and Type Families [CKP05]).\n\u2022 A lot of code has been added to the Main component; this is partly because there was previously\na 3,000-line Perl script called the \u201cdriver\u201d that was rewritten in Haskell and moved into GHC\nproper, and also because support for compiling multiple modules was added.\n\u2022 The runtime system has barely grown: it is only 10% larger, despite having accumulated a lot\nof new functionality and being ported to more platforms. We rewrote it completely around\n1997.\n\u2022 GHC has a complex build system, which today comprises about 6,000 lines of GNU make code.\nIt is on its fourth complete rewrite, the latest being about two years ago, and each successive\niteration has reduced the amount of code.\nThe Compiler\nWe can divide the compiler into three:\n\u2022 The compilation manager, which is responsible for the compilation of multiple Haskell source\n\ufb01les. The job of the compilation manager is to \ufb01gure out in which order to compile the\ndi\ufb00erent \ufb01les, and to decide which modules do not need to be recompiled because none of\ntheir dependencies have changed since the last time they were compiled.\n\u2022 The Haskell compiler (we abbreviate this as Hsc inside GHC), which handles the compilation\nof a single Haskell source \ufb01le. As you might imagine, most of the action happens in here. The\noutput of Hsc depends on what backend is selected: assembly, LLVM code, or bytecode.\n\u2022 The pipeline, which is responsible for composing together any necessary external programs\nwith Hsc to compile a Haskell source \ufb01le to object code. For example, a Haskell source \ufb01le\nmay need preprocessing with the C preprocessor before feeding to Hsc, and the output of Hsc\nis usually an assembly \ufb01le that must be fed into the assembler to create an object \ufb01le.\nThe compiler is not simply an executable that performs these functions; it is itself a library with\na large API that can be used to build other tools that work with Haskell source code, such as IDEs\nand analysis tools. More about this later in Section 5.4.\nCompiling Haskell Code\nAs with most compilers, compiling a Haskell source \ufb01le proceeds in a sequence of phases, with\nthe output of each phase becoming the input of the subsequent phase. The overall structure of the\ndi\ufb00erent phases is illustrated in Figure 5.2.\nParsing\nWe start in the traditional way with parsing, which takes as input a Haskell source \ufb01le and produces\nas output abstract syntax. In GHC the abstract syntax datatype HsSyn is parameterised by the types\nof the identi\ufb01ers it contains, so an abstract syntax tree has type HsSyn t for some type of identi\ufb01ers t.\nThis enables us to add more information to identi\ufb01ers as the program passes through the various\nstages of the compiler, while reusing the same type of abstract syntax trees.\nThe output of the parser is an abstract syntax tree in which the identi\ufb01ers are simple strings,\nwhich we call RdrName. Hence, the abstract syntax produced by the parser has type HsSyn RdrName.\n70\nThe Glasgow Haskell Compiler\n", "page": 84, "type": "text", "section": "Page 84"}
{"text": "Cmm (C--)\nCoreExpr\n(in A-normal form)\nCoreExpr\n(with tidy names)\nHsSyn Id\nHsSyn RdrName\nHsSyn Name\nCoreExpr\nCoreExpr\nIfaceSyn\nSTG\n\uff65 The Simpli\ufb01er\n\uff65 Rewrite rules\n\uff65 Strictness analysis\n\uff65 Let-\ufb02oating (inwards and outwards)\n\uff65 Specialise overloaded functions\n\uff65 Constructor specialisation\nM.hs\nParse\nRename\nTypecheck\nDesugar\nSimplify\nCoreTidy\nCorePrep\nConvert to IfaceSyn\nSerialise\nM.hi\n(interface file)\nConvert to STG\nCode generation\nPretty-print \nC code\nGenerate \nmachine code\nGenerate \nLLVM code\nM.hc\n(C code)\nM.s\n(asm code)\nM.ll\n(LLVM code)\nFigure 5.2: The compiler phases\nGHC uses the tools Alex and Happy to generate its lexical analysis and parsing code respectively,\nwhich are analogous to the tools lex and yacc for C.\nGHC\u2019s parser is purely functional. In fact, the API of the GHC library provides a pure function\ncalled parser that takes a String (and a few other things) and returns either the parsed abstract\nsyntax or an error message.\nSimon Marlow and Simon Peyton Jones\n71\n", "page": 85, "type": "text", "section": "Page 85"}
{"text": "Renaming\nRenaming is the process of resolving all of the identi\ufb01ers in the Haskell source code into fully quali\ufb01ed\nnames, at the same time identifying any out-of-scope identi\ufb01ers and \ufb02agging errors appropriately.\nIn Haskell it is possible for a module to re-export an identi\ufb01er that it imported from another\nmodule. For example, suppose module A de\ufb01nes a function called f, and module B imports module\nA and re-exports f. Now, if a module C imports module B, it can refer to f by the name B.f\u2014even\nthough f is originally de\ufb01ned in module A. This is a useful form of namespace manipulation; it\nmeans that a library can use whatever module structure it likes internally, but expose a nice clean\nAPI via a few interface modules that re-export identi\ufb01ers from the internal modules.\nThe compiler however has to resolve all this, so that it knows what each name in the source\ncode corresponds to. We make a clean distinction between the entities, the \u201cthings themselves\u201d (in\nour example, A.f), and the names by which the entities can be referred to (e.g., B.f). At any given\npoint in the source code, there are a set of entities in scope, and each may be known by one or more\ndi\ufb00erent names. The job of the renamer is to replace each of the names in the compiler\u2019s internal\nrepresentation of the code by a reference to a particular entity. Sometimes a name can refer to several\ndi\ufb00erent entities; by itself that is not an error, but if the name is actually used, then the renamer will\n\ufb02ag an ambiguity error and reject the program.\nRenaming takes Haskell abstract syntax (HsSyn RdrName) as input, and also produces abstract\nsyntax as output (HsSyn Name). Here a Name is a reference to a particular entity.\nResolving names is the main job of the renamer, but it performs a plethora of other tasks too:\ncollecting the equations of a function together and \ufb02agging an error if they have di\ufb00ering numbers of\narguments; rearranging in\ufb01x expressions according to the \ufb01xity of the operators; spotting duplicate\ndeclarations; generating warnings for unused identi\ufb01ers, and so on.\nType Checking\nType checking, as one might imagine, is the process of checking that the Haskell program is type-\ncorrect. If the program passes the type checker, then it is guaranteed to not crash at runtime.3\nThe input to the type checker is HsSyn Name (Haskell source with quali\ufb01ed names), and the\noutput is HsSyn Id. An Id is a Name with extra information: notably a type. In fact, the Haskell\nsyntax produced by the type checker is fully decorated with type information: every identi\ufb01er has its\ntype attached, and there is enough information to reconstruct the type of any subexpression (which\nmight be useful for an IDE, for example).\nIn practice, type checking and renaming may be interleaved, because the Template Haskell feature\ngenerates code at runtime that itself needs to be renamed and type checked.\nDesugaring, and the Core language\nHaskell is a rather large language, containing many di\ufb00erent syntactic forms. It is intended to\nbe easy for humans to read and write\u2014there is a wide range of syntactic constructs which gives\nthe programmer plenty of \ufb02exibility in choosing the most appropriate construct for the situation\nat hand. However, this \ufb02exibility means that there are often several ways to write the same code;\nfor example, an if expression is identical in meaning to a case expression with True and False\nbranches, and list-comprehension notation can be translated into calls to map, filter, and concat.\n3The term \u201ccrash\u201d here has a formal de\ufb01nition that includes hard crashes like \u201csegmentation fault\u201d, but not things like\npattern-matching failure. The non-crash guarantee can be subverted by using certain unsafe language features, such as the\nForeign Function Interface.\n72\nThe Glasgow Haskell Compiler\n", "page": 86, "type": "text", "section": "Page 86"}
{"text": "In fact, the de\ufb01nition of the Haskell language de\ufb01nes all these constructs by their translation into\nsimpler constructs; the constructs that can be translated away like this are called \u201csyntactic sugar\u201d.\nIt is much simpler for the compiler if all the syntactic sugar is removed, because the subsequent\noptimisation passes that need to work with the Haskell program have a smaller language to deal\nwith. The process of desugaring therefore removes all the syntactic sugar, translating the full Haskell\nsyntax into a much smaller language that we call Core. We\u2019ll talk about Core in detail in Section 5.3.\nOptimisation\nNow that the program is in Core, the process of optimisation begins. One of GHC\u2019s great strengths\nis in optimising away layers of abstraction, and all of this work happens at the Core level. Core is\na tiny functional language, but it is a tremendously \ufb02exible medium for expressing optimisations,\nranging from the very high-level, such as strictness analysis, to the very low-level, such as strength\nreduction.\nEach of the optimisation passes takes Core and produces Core. The main pass here is called the\nSimpli\ufb01er, whose job it is to perform a large collection of correctness-preserving transformations,\nwith the goal of producing a more e\ufb03cient program. Some of these transformations are simple\nand obvious, such as eliminating dead code or reducing a case expression when the value being\nscrutinised is known, and some are more involved, such as function inlining and applying rewrite\nrules (Section 5.4).\nThe simpli\ufb01er is normally run between the other optimisation passes, of which there are about\nsix; which passes are actually run and in which order depends on the optimisation level selected by\nthe user.\nCode Generation\nOnce the Core program has been optimised, the process of code generation begins. After a couple of\nadministrative passes, the code takes one of two routes: either it is turned into byte code for execution\nby the interactive interpreter, or it is passed to the code generator for eventual translation to machine\ncode.\nThe code generator \ufb01rst converts the Core into a language called STG, which is essentially just\nCore annotated with more information required by the code generator. Then, STG is translated to Cmm,\na low-level imperative language with an explicit stack. From here, the code takes one of three routes:\n\u2022 Native code generation: GHC contains simple native code generators for a few processor\narchitectures. This route is fast, and generates reasonable code in most cases.\n\u2022 LLVM code generation: The Cmm is converted to LLVM code and passed to the LLVM\ncompiler. This route can produce signi\ufb01cantly better code in some cases, although it takes\nlonger than the native code generator.\n\u2022 C code generation: GHC can produce ordinary C code. This route produces signi\ufb01cantly\nslower code than the other two routes, but can be useful for porting GHC to new platforms.\n5.3\nKey Design Choices\nIn this section we focus on a handful of the design choices that have been particularly e\ufb00ective in\nGHC.\nSimon Marlow and Simon Peyton Jones\n73\n", "page": 87, "type": "text", "section": "Page 87"}
{"text": "The Intermediate Language\nExpressions\nt, e, u\n::=\nx\nVariables\n|\nK\nData constructors\n|\nk\nLiterals\n|\n\u03bbx:\u03c3.e | e u\nValue abstraction and application\n|\n\u039ba:\u03b7.e | e \u03c6\nType abstraction and application\n|\nlet x : \u03c4 = e in u\nLocal bindings\n|\ncase e of p \u2192u\nCase expressions\n|\ne\u25b7\u03b3\nCasts\n|\n\u230a\u03b3\u230b\nCoercions\np\n::=\nK c:\u03b7 x:\u03c4\nPatterns\nFigure 5.3: The syntax of Core\nA typical structure for a compiler for a statically-typed language is this: the program is type\nchecked, and transformed to some untyped intermediate language, before being optimised. GHC is\ndi\ufb00erent: it has a statically-typed intermediate language. As it turns out, this design choice has had\na pervasive e\ufb00ect on the design and development of GHC.\nGHC\u2019s intermediate language is called Core (when thinking of the implementation) or System FC\n(when thinking about the theory). Its syntax is given in Figure 5.3. The exact details are not important\nhere; the interested reader can consult [SCPD07] for more details. For our present purposes, however,\nthe following points are the key ones:\n\u2022 Haskell is a very large source language. The data type representing its syntax tree has literally\nhundreds of constructors.\nIn contrast Core is a tiny, principled, lambda calculus. It has extremely few syntactic forms,\nyet we can translate all of Haskell into Core.\n\u2022 Haskell is an implicitly-typed source language. A program may have few or no type annotations;\ninstead it is up to the type inference algorithm to \ufb01gure out the type of every binder and sub-\nexpressions. This type inference algorithm is complex, and occasionally somewhat ad hoc,\nre\ufb02ecting the design compromises that every real programming language embodies.\nIn contrast Core is an explicitly-typed language. Every binder has an explicit type, and terms\ninclude explicit type abstractions and applications. Core enjoys a very simple, fast type\nchecking algorithm that checks that the program is type correct. The algorithm is entirely\nstraightforward; there are no ad hoc compromises.\nAll of GHC\u2019s analysis and optimisation passes work on Core. This is great: because Core is\nsuch a tiny language an optimisation has only a few cases to deal with. Although Core is small, it is\nextremely expressive\u2014System F was, after all, originally developed as a foundational calculus for\ntyped computation. When new language features are added to the source language (and that happens\nall the time) the changes are usually restricted to the front end; Core stays unchanged, and hence so\ndoes most of the compiler.\nBut why is Core typed? After all, if the type inference engine accepts the source program,\nthat program is presumably well typed, and each optimisation pass presumably maintains that\ntype-correctness. Core may enjoy a fast type checking algorithm, but why would you ever want\n74\nThe Glasgow Haskell Compiler\n", "page": 88, "type": "text", "section": "Page 88"}
{"text": "to run it? Moreover, making Core typed carries signi\ufb01cant costs, because every transformation or\noptimisation pass must produce a well-typed program, and generating all those type annotations is\noften non-trivial.\nNevertheless, it has been a huge win to have an explicitly-typed intermediate language, for several\nreasons:\n\u2022 Running the Core type checker (we call it Lint) is a very powerful consistency check on the\ncompiler itself. Imagine that you write an \u201coptimisation\u201d that accidentally generates code that\ntreats an integer value as a function, and tries to call it. The chances are that the program will\nsegmentation fault, or fail at runtime in a bizarre way. Tracing a seg-fault back to the particular\noptimisation pass that broke the program is a long road.\nNow imagine instead that we run Lint after every optimisation pass (and we do, if you use\nthe \ufb02ag -dcore-lint): it will report a precisely located error immediately after the o\ufb00ending\noptimisation. What a blessing.\nOf course, type soundness is not the same as correctness: Lint will not signal an error if you\n\u201coptimise\u201d (x \u22171) to 1 instead of to x. But if the program passes Lint, it will guarantee to\nrun without seg-faults; and moreover in practice we have found that it is surprisingly hard to\naccidentally write optimisations that are type-correct but not semantically correct.\n\u2022 The type inference algorithm for Haskell is very large and very complex: a glance at Figure 5.1\ncon\ufb01rms that the type checker is by far the largest single component of GHC. Large and complex\nmeans error-prone. But Lint serves as an 100% independent check on the type inference\nengine; if the type inference engine accepts a program that is not, in fact, type-correct, Lint\nwill reject it. So Lint serves as a powerful auditor of the type inference engine.\n\u2022 The existence of Core has also proved to be a tremendous sanity check on the design of\nthe source language. Our users constantly suggest new features that they would like in the\nlanguage. Sometimes these features are manifestly \u201csyntactic sugar\u201d, convenient new syntax\nfor something you can do already. But sometimes they are deeper, and it can be hard to tell\nhow far-reaching the feature is.\nCore gives us a precise way to evaluate such features. If the feature can readily be translated\ninto Core, that reassures us that nothing fundamentally new is going on: the new feature is\nsyntactic-sugar-like. On the other hand, if it would require an extension to Core, then we think\nmuch, much more carefully.\nIn practice Core has been incredibly stable: over a 20-year time period we have added exactly\none new major feature to Core (namely coercions and their associated casts). Over the same period,\nthe source language has evolved enormously. We attribute this stability not to our own brilliance, but\nrather to the fact that Core is based directly on foundational mathematics: bravo Girard!\nType Checking the Source Language\nOne interesting design decision is whether type checking should be done before or after desugaring.\nThe trade-o\ufb00s are these:\n\u2022 Type checking before desugaring means that the type checker must deal directly with Haskell\u2019s\nvery large syntax, so the type checker has many cases to consider. If we desugared into (an\nuntyped variant of) Core \ufb01rst, one might hope that the type checker would become much\nsmaller.\nSimon Marlow and Simon Peyton Jones\n75\n", "page": 89, "type": "text", "section": "Page 89"}
{"text": "\u2022 On the other hand, type checking after desugaring would impose a signi\ufb01cant new obligation:\nthat desugaring does not a\ufb00ect which programs are type-correct. After all, desugaring implies\na deliberate loss of information. It is probably the case that in 95% of the cases there is no\nproblem, but any problem here would force some compromise in the design of Core to preserve\nsome extra information.\n\u2022 Most seriously of all, type checking a desugared program would make it much harder to report\nerrors that relate to the original program text, and not to its (sometimes elaborate) desugared\nversion.\nMost compilers type check after desugaring, but for GHC we made the opposite choice: we type\ncheck the full original Haskell syntax, and then desugar the result. It sounds as if adding a new\nsyntactic construct might be complicated, but (following the French school) we have structured the\ntype inference engine in a way that makes it easy. Type inference is split into two parts:\n1. Constraint generation: walk over the source syntax tree, generating a collection of type\nconstraints. This step deals with the full syntax of Haskell, but it is very straightforward code,\nand it is easy to add new cases.\n2. Constraint solving: solve the gathered constraints. This is where the subtlety of the type\ninference engine lies, but it is independent of the source language syntax, and would be the\nsame for a much smaller or much larger language.\nOn the whole, the type-check-before-desugar design choice has turned out to be a big win. Yes,\nit adds lines of code to the type checker, but they are simple lines. It avoids giving two con\ufb02icting\nroles to the same data type, and makes the type inference engine less complex, and easier to modify.\nMoreover, GHC\u2019s type error messages are pretty good.\nNo Symbol Table\nCompilers usually have one or more data structures known as symbol tables, which are mappings\nfrom symbols (e.g., variables) to some information about the variable, such as its type, or where in\nthe source code it was de\ufb01ned.\nIn GHC we use symbol tables quite sparingly; mainly in the renamer and type checker. As far as\npossible, we use an alternative strategy: a variable is a data structure that contains all the information\nabout itself. Indeed, a large amount of information is reachable by traversing the data structure of a\nvariable: from a variable we can see its type, which contains type constructors, which contain their\ndata constructors, which themselves contain types, and so on. For example, here are some data types\nfrom GHC (heavily abbreviated and simpli\ufb01ed):\ndata Id\n= MkId Name Type\ndata Type\n= TyConApp TyCon [Type]\n| ....\ndata TyCon\n= AlgTyCon Name [DataCon]\n| ...\ndata DataCon = MkDataCon Name Type ...\nAn Id contains its Type. A Type might be an application of a type constructor to some arguments\n(e.g., Maybe Int), in which case it contains the TyCon. A TyCon can be an algebraic data type, in\nwhich case it includes a list of its data constructors. Each DataCon includes its Type, which of course\nmentions the TyCon. And so on. The whole structure is highly interconnected. Indeed it is cyclic;\nfor example, a TyCon may contain a DataCon which contains a Type, which contains the very TyCon\nwe started with.\n76\nThe Glasgow Haskell Compiler\n", "page": 90, "type": "text", "section": "Page 90"}
{"text": "This approach has some advantages and disadvantages:\n\u2022 Many queries that would require a lookup in a symbol table are reduced to a simple \ufb01eld\naccess, which is great for e\ufb03ciency and code clarity.\n\u2022 There is no need to carry around extra symbol tables, the abstract syntax tree already contains\nall the information.\n\u2022 The space overheads are better: all instances of the same variable share the same data structure,\nand there is no space needed for the table.\n\u2022 The only di\ufb03culties arise when we need to change any of the information associated with a\nvariable. This is where a symbol table has the advantage: we would just change the entry in the\nsymbol table. In GHC we have to traverse the abstract syntax tree and replace all the instances\nof the old variable with the new one; indeed the simpli\ufb01er does this regularly, as it needs to\nupdate certain optimisation-related information about each variable.\nIt is hard to know whether it would be better or worse overall to use symbol tables, because this\naspect of the design is so fundamental that it is almost impossible to change. Still, avoiding symbol\ntables is a natural choice in the purely functional setting, so it seems likely that this approach is a\ngood choice for Haskell.\nInter-Module Optimisation\nFunctional languages encourage the programmer to write small de\ufb01nitions. For example, here is the\nde\ufb01nition of && from the standard library:\n(&&) :: Bool -> Bool -> Bool\nTrue && True = True\n_\n&& _\n= False\nIf every use of such a function really required a function call, e\ufb03ciency would be terrible. One\nsolution is to make the compiler treat certain functions specially; another is to use a pre-processor to\nreplace a \u201ccall\u201d with the desired inline code. All of these solutions are unsatisfactory in one way or\nanother, especially as another solution is so obvious: simply inline the function. To \u201cinline a function\u201d\nmeans to replace the call by a copy of the function body, suitably instantiating its parameters.\nIn GHC we have systematically adopted this approach [PM02]. Virtually nothing is built into the\ncompiler. Instead, we de\ufb01ne as much as possible in libraries, and use aggressive inlining to eliminate\nthe overheads. This means that programmers can de\ufb01ne their own libraries that will be inlined and\noptimised as well as the ones that come with GHC.\nA consequence is that GHC must be able to do cross-module, and indeed cross-package, inlining.\nThe idea is simple:\n\u2022 When compiling a Haskell module Lib.hs, GHC produces object code in Lib.o and an\n\u201cinterface \ufb01le\u201d in Lib.hi. This interface \ufb01le contains information about all the functions that\nLib exports, including both their types and, for su\ufb03ciently small functions, their de\ufb01nitions.\n\u2022 When compiling a module Client.hs that imports Lib, GHC reads the interface Lib.hi. So\nif Client calls a function Lib.f de\ufb01ned in Lib, GHC can use the information in Lib.hi to\ninline Lib.f.\nBy default GHC will expose the de\ufb01nition of a function in the interface \ufb01le only if the function\nis \u201csmall\u201d (there are \ufb02ags to control this size threshold). But we also support an INLINE pragma, to\ninstruct GHC to inline the de\ufb01nition aggressively at call sites, regardless of size, thus:\nSimon Marlow and Simon Peyton Jones\n77\n", "page": 91, "type": "text", "section": "Page 91"}
{"text": "foo :: Int -> Int\n{-# INLINE foo #-}\nfoo x = <some big expression>\nCross-module inlining is absolutely essential for de\ufb01ning super-e\ufb03cient libraries, but it does\ncome with a cost. If the author upgrades his library, it is not enough to re-link Client.o with the new\nLib.o, because Client.o contains inlined fragments of the old Lib.hs, and they may well not be\ncompatible with the new one. Another way to say this is that the ABI (Application Binary Interface)\nof Lib.o has changed in a way that requires recompilation of its clients.\nIn fact, the only way for compilation to generate code with a \ufb01xed, predictable ABI is to disable\ncross-module optimisation, and this is typically too high a price to pay for ABI compatibility. Users\nworking with GHC will usually have the source code to their entire stack available, so recompiling is\nnot normally an issue (and, as we will describe later, the package system is designed around this\nmode of working). However, there are situations where recompiling is not practical: distributing bug\n\ufb01xes to libraries in a binary OS distribution, for example. In the future we hope it may be possible\nto \ufb01nd a compromise solution that allows retaining ABI compatibility while still allowing some\ncross-module optimisation to take place.\n5.4\nExtensibility\nIt is often the case that a project lives or dies according to how extensible it is. A monolithic piece of\nsoftware that is not extensible has to do everything and do it right, whereas an extensible piece of\nsoftware can be a useful base even if it doesn\u2019t provide all the required functionality out of the box.\nOpen source projects are of course extensible by de\ufb01nition, in that anyone can take the code and\nadd their own features. But modifying the original source code of a project maintained by someone\nelse is not only a high-overhead approach, it is also not conducive to sharing your extension with\nothers. Therefore successful projects tend to o\ufb00er forms of extensibility that do not involve modifying\nthe core code, and GHC is no exception in this respect.\nUser-De\ufb01ned Rewrite Rules\nThe core of GHC is a long sequence of optimisation passes, each of which performs some semantics-\npreserving transformation, Core into Core. But the author of a library de\ufb01nes functions that often\nhave some non-trivial, domain-speci\ufb01c transformations of their own, ones that cannot possibly be\npredicted by GHC. So GHC allows library authors to de\ufb01ne rewrite rules that are used to rewrite the\nprogram during optimisation [PTH01]. In this way, programmers can, in e\ufb00ect, extend GHC with\ndomain-speci\ufb01c optimisations.\nOne example is the foldr/build rule, which is expressed like this:\n{-# RULES \"fold/build\"\nforall k z (g::forall b. (a->b->b) -> b -> b) .\nfoldr k z (build g) = g k z\n#-}\nThe entire rule is a pragma, introduced by {-# RULES. The rule says that whenever GHC sees\nthe expression (foldr k z (build g)) it should rewrite it to (g k z). This transformation is\nsemantics-preserving, but it takes a research paper to argue that it is [GLP93], so there is no chance\nof GHC performing it automatically. Together with a handful of other rules, and some INLINE\n78\nThe Glasgow Haskell Compiler\n", "page": 92, "type": "text", "section": "Page 92"}
{"text": "pragmas, GHC is able to fuse together list-transforming functions. For example, the two loops in\n(map f (map g xs)) are fused into one.\nAlthough rewrite rules are simple and easy to use, they have proved to be a very powerful\nextension mechanism. When we \ufb01rst introduced the feature into GHC ten years ago we expected\nit to be an occasionally useful facility. But in practice it has turned out to be useful in very many\nlibraries, whose e\ufb03ciency often depends crucially on rewrite rules. For example, GHC\u2019s own base\nlibrary contains upward of 100 rules, while the popular vector library uses several dozen.\nCompiler Plugins\nOne way in which a compiler can o\ufb00er extensibility is to allow programmers to write a pass that is\ninserted directly into the compiler\u2019s pipeline. Such passes are often called \u201cplugins\u201d. GHC supports\nplugins in the following way:\n\u2022 The programmer writes a Core to Core pass, as an ordinary Haskell function in a module P.hs,\nsay, and compiles it to object code.\n\u2022 When compiling some module, the programmer uses the command-line \ufb02ag -plugin P.\n(Alternatively, he can give the \ufb02ag in a pragma at the start of the module.)\n\u2022 GHC searches for P.o, dynamically links it into the running GHC binary, and calls it at the\nappropriate point in the pipeline.\nBut what is \u201cthe appropriate point in the pipeline\u201d? GHC does not know, and so it allows the\nplugin to make that decision. As a result of this and other matters, the API that the plugin must o\ufb00er\nis a bit more complicated than a single Core to Core function\u2014but not much.\nPlugins sometimes require, or produce, auxiliary plugin-speci\ufb01c data. For example, a plugin\nmight perform some analysis on the functions in the module being compiled (M.hs, say), and might\nwant to put that information in the interface \ufb01le M.hi, so that the plugin has access to that information\nwhen compiling modules that import M. GHC o\ufb00ers an annotation mechanism to support this.\nPlugins and annotations are relatively new to GHC. They have a higher barrier to entry than\nrewrite rules, because the plugin is manipulating GHC\u2019s internal data structures, but of course they\ncan do much more. It remains to be seen how widely they will be used.\nGHC as a Library: The GHC API\nOne of GHC\u2019s original goals was to be a modular foundation that others could build on. We wanted\nthe code of GHC to be as transparent and well-documented as possible, so that it could be used as\nthe basis for research projects by others; we imagined that people would want to make their own\nmodi\ufb01cations to GHC to add new experimental features or optimisations. Indeed, there have been\nsome examples of this: for example, there exists a version of GHC with a Lisp front-end, and a\nversion of GHC that generates Java code, both developed entirely separately by individuals with little\nor no contact with the GHC team.\nHowever, producing modi\ufb01ed versions of GHC represents only a small subset of the ways in\nwhich the code of GHC can be re-used. As the popularity of the Haskell language has grown, there\nhas been an increasing need for tools and infrastructure that understand Haskell source code, and\nGHC of course contains a lot of the functionality necessary for building these tools: a Haskell parser,\nabstract syntax, type checker and so on.\nWith this in mind, we made a simple change to GHC: rather than building GHC as a monolithic\nprogram, we build GHC as a library, that is then linked with a small Main module to make the GHC\nexecutable itself, but also shipped in library form so that users can call it from their own programs.\nSimon Marlow and Simon Peyton Jones\n79\n", "page": 93, "type": "text", "section": "Page 93"}
{"text": "At the same time we built an API to expose GHC\u2019s functionality to clients. The API provides enough\nfunctionality to implement the GHC batch compiler and the GHCi interactive environment, but it\nalso provides access to individual passes such as the parser and type checker, and allows the data\nstructures produced by these passes to be inspected. This change has given rise to a wide range of\ntools built using the GHC API, including:\n\u2022 A documentation tool, Haddock4, which reads Haskell source code and produces HTML\ndocumentation.\n\u2022 New versions of the GHCi front end with additional features; e.g., ghci-haskeline5 which was\nsubsequently merged back into GHC.\n\u2022 IDEs that o\ufb00er advanced navigation of Haskell source code; e.g., Leksah6.\n\u2022 hint7, a simpler API for on-the-\ufb02y evaluation of Haskell source code.\nThe Package System\nThe package system has been a key factor in the growth in use of the Haskell language in recent\nyears. Its main purpose is to enable Haskell programmers to share code with each other, and as such\nit is an important aspect of extensibility: the package system extends the shared codebase beyond\nGHC itself.\nThe package system embodies various pieces of infrastructure that together make sharing code\neasy. With the package system as the enabler, the community has built a large body of shared\ncode; rather than relying on libraries from a single source, Haskell programmers draw on libraries\ndeveloped by the whole community. This model has worked well for other languages; CPAN for\nPerl, for example, although Haskell being a predominantly compiled rather than interpreted language\npresents a somewhat di\ufb00erent set of challenges.\nBasically, the package system lets a user manage libraries of Haskell code written by other people,\nand use them in their own programs and libraries. Installing a Haskell library is as simple as uttering\na single command, for example:\n$ cabal install zlib\ndownloads the code for the zlib package from http://hackage.haskell.org, compiles it using\nGHC, installs the compiled code somewhere on your system (e.g., in your home directory on a Unix\nsystem), and registers the installation with GHC. Furthermore, if zlib depends on any other packages\nthat are not yet installed, those will also be downloaded, compiled and installed automatically before\nzlib itself is compiled. It is a tremendously smooth way to work with libraries of Haskell code\nshared by others.\nThe package system is made of four components, only the \ufb01rst of which is strictly part of the\nGHC project:\n\u2022 Tools for managing the package database, which is simply a repository for information about\nthe packages installed on your system. GHC reads the package database when it starts up, so\nthat it knows which packages are available and where to \ufb01nd them.\n\u2022 A library called Cabal (Common Architecture for Building Applications and Libraries), which\nimplements functionality for building, installing and registering individual packages.\n4http://www.haskell.org/haddock/\n5http://hackage.haskell.org/package/ghci-haskeline\n6http://hackage.haskell.org/package/leksah\n7http://hackage.haskell.org/package/hint\n80\nThe Glasgow Haskell Compiler\n", "page": 94, "type": "text", "section": "Page 94"}
{"text": "\u2022 A website at http://hackage.haskell.org which hosts packages written and uploaded by\nusers. The website automatically builds documentation for the packages which can be browsed\nonline. At the time of writing, Hackage is hosting over 3,000 packages covering functionality\nincluding database libraries, web frameworks, GUI toolkits, data structures, and networking.\n\u2022 The cabal tool which ties together the Hackage website and the Cabal library: it downloads\npackages from Hackage, resolves dependencies, and builds and installs packages in the right\norder. New packages can also be uploaded to Hackage using cabal from the command line.\nThese components have been developed over several years by members of the Haskell commu-\nnity and the GHC team, and together they make a system that \ufb01ts perfectly with the open source\ndevelopment model. There are no barriers to sharing code or using code that others have shared\n(provided you respect the relevant licenses, of course). You can be using a package that someone\nelse has written literally within seconds of \ufb01nding it on Hackage.\nHackage has been so successful that the remaining problems it has are now those of scale: users\n\ufb01nd it di\ufb03cult to choose amongst the four di\ufb00erent database frameworks, for example. Ongoing\ndevelopments are aimed at solving these problems in ways that leverage the community. For example,\nallowing users to comment and vote on packages will make it easier to \ufb01nd the best and most popular\npackages, and collecting data on build success or failures from users and reporting the results will\nhelp users avoid packages that are unmaintained or have problems.\n5.5\nThe Runtime System\nThe Runtime System is a library of mostly C code that is linked into every Haskell program. It\nprovides the support infrastructure needed for running the compiled Haskell code, including the\nfollowing main components:\n\u2022 Memory management, including a parallel, generational, garbage collector;\n\u2022 Thread management and scheduling;\n\u2022 The primitive operations provided by GHC;\n\u2022 A bytecode interpreter and dynamic linker for GHCi.\nThe rest of this section is divided into two: \ufb01rst we focus on a couple of the aspects of the design\nof the RTS that we consider to have been successful and instrumental in making it work so well, and\nsecondly we talk about the coding practices and infrastructure we have built in the RTS for coping\nwith what is a rather hostile programming environment.\nKey Design Decisions\nIn this section we describe two of the design decisions in the RTS that we consider to have been\nparticularly successful.\nThe Block Layer\nThe garbage collector is built on top of a block layer that manages memory in units of blocks, where\na block is a multiple of 4 KB in size. The block layer has a very simple API:\ntypedef struct bdescr_ {\nvoid *\nstart;\nstruct bdescr_ *\nlink;\nSimon Marlow and Simon Peyton Jones\n81\n", "page": 95, "type": "text", "section": "Page 95"}
{"text": "struct generation_ * gen;\n// generation\n// .. various other fields\n} bdescr;\nbdescr * allocGroup (int n);\nvoid\nfreeGroup\n(bdescr *p);\nbdescr * Bdescr\n(void *p);\n// a macro\nThis is the only API used by the garbage collector for allocating and deallocating memory. Blocks\nof memory are allocated with allocGroup and freed with freeGroup. Every block has a small\nstructure associated with it called a block descriptor (bdescr). The operation Bdescr(p) returns the\nblock descriptor associated with an arbitrary address p; this is purely an address calculation based\non the value of p and compiles to a handful of arithmetic and bit-manipulation instructions.\nBlocks may be linked together into chains using the link \ufb01eld of the bdescr, and this is the real\npower of the technique. The garbage collector needs to manage several distinct areas of memory\nsuch as generations, and each of these areas may need to grow or shrink over time. By representing\nmemory areas as linked lists of blocks, the GC is freed from the di\ufb03culties of \ufb01tting multiple resizable\nmemory areas into a \ufb02at address space.\nThe implementation of the block layer uses techniques that are well-known from C\u2019s\nmalloc()/free() API; it maintains lists of free blocks of various sizes, and coalesces free ar-\neas. The operations freeGroup() and allocGroup() are carefully designed to be O(1).\nOne major advantage of this design is that it needs very little support from the OS, and hence is\ngreat for portability. The block layer needs to allocate memory in units of 1 MB, aligned to a 1 MB\nboundary. While none of the common OSs provide this functionality directly, it is implementable\nwithout much di\ufb03culty in terms of the facilities they do provide. The payo\ufb00is that GHC has no\ndependence on the particular details of the address-space layout used by the OS, and it coexists\npeacefully with other users of the address space, such as shared libraries and operating system\nthreads.\nThere is a small up-front complexity cost for the block layer, in terms of managing chains of\nblocks rather than contiguous memory. However, we have found that this cost is more than repaid in\n\ufb02exibility and portability; for example, the block layer enabled a particularly simple algorithm for\nparallel GC to be implemented [MHJP08].\nLightweight Threads and Parallelism\nWe consider concurrency to be a vitally important programming abstraction, particularly for building\napplications like web servers that need to interact with large numbers of external agents simultaneously.\nIf concurrency is an important abstraction, then it should not be so expensive that programmers\nare forced to avoid it, or build elaborate infrastructure to amortise its cost (e.g., thread pools). We\nbelieve that concurrency should just work, and be cheap enough that you don\u2019t worry about forking\nthreads for small tasks.\nAll operating systems provide threads that work perfectly well, the problem is that they are far\ntoo expensive. Typical OSs struggle to handle thousands of threads, whereas we want to manage\nthreads by the million.\nGreen threads, otherwise known as lightweight threads or user-space threads, are a well-known\ntechnique for avoiding the overhead of operating system threads. The idea is that threads are managed\nby the program itself, or a library (in our case, the RTS), rather than by the operating system.\n82\nThe Glasgow Haskell Compiler\n", "page": 96, "type": "text", "section": "Page 96"}
{"text": "Managing threads in user space should be cheaper, because fewer traps into the operating system are\nrequired.\nIn the GHC RTS we take full advantage of this idea. A context switch only occurs when the\nthread is at a safe point, where very little additional state needs to be saved. Because we use accurate\nGC, the stack of the thread can be moved and expanded or shrunk on demand. Contrast these with\nOS threads, where every context switch must save the entire processor state, and where stacks are\nimmovable so a large chunk of address space has to be reserved up front for each thread.\nGreen threads can be vastly more e\ufb03cient than OS threads, so why would anyone want to use\nOS threads? It comes down to three main problems:\n\u2022 Blocking and foreign calls. A thread should be able to make a call to an OS API or a foreign\nlibrary that blocks, without blocking all the other threads in the system.\n\u2022 Parallelism. Threads should automatically run in parallel if there are multiple processor cores\non the system.\n\u2022 Some external libraries (notably OpenGL and some GUI libraries) have APIs that must be\ncalled from the same OS thread each time, because they use thread-local state.\nIt turns out that all of these are di\ufb03cult to arrange with green threads. Nevertheless, we persevered\nwith green threads in GHC and found solutions to all three:\n\u2022 When a Haskell thread makes a foreign call, another OS thread takes over the execution of\nthe remaining Haskell threads [MPT04]. A small pool of OS threads are maintained for this\npurpose, and new ones are created on demand.\n\u2022 GHC\u2019s scheduler multiplexes many lightweight Haskell threads onto a few heavyweight OS\nthreads; it implements a transparent M:N threading model. Typically N is chosen to be the\nsame as the number of processor cores in the machine, allowing real parallelism to take place\nbut without the overhead of having a full OS thread for each lightweight Haskell thread.\nIn order to run Haskell code, an OS thread must hold a Capability8: a data structure that\nholds the resources required to execute Haskell code, such as the nursery (memory where new\nobjects are created). Only one OS thread may hold a given Capability at a time.\n\u2022 We provide an API for creating a bound thread: a Haskell thread that is tied to one speci\ufb01c\nOS thread, such that any foreign calls made by this Haskell thread are guaranteed to be made\nby that OS thread.\nSo in the vast majority of cases, Haskell\u2019s threads behave exactly like OS threads: they can make\nblocking OS calls without a\ufb00ecting other threads, and they run in parallel on a multicore machine.\nBut they are orders of magnitude more e\ufb03cient, in terms of both time and space.\nHaving said that, the implementation does have one problem that users occasionally run into,\nespecially when running benchmarks. We mentioned above that lightweight threads derive some\nof their e\ufb03ciency by only context-switching at \u201csafe points\u201d, points in the code that the compiler\ndesignates as safe, where the internal state of the virtual machine (stack, heap, registers, etc.) is in\na tidy state and garbage collection could take place. In GHC, a safe point is whenever memory is\nallocated, which in almost all Haskell programs happens regularly enough that the program never\nexecutes more than a few tens of instructions without hitting a safe point. However, it is possible in\nhighly optimised code to \ufb01nd loops that run for many iterations without allocating memory. This\ntends to happen often in benchmarks (e.g., functions like factorial and Fibonacci). It occurs less\noften in real code, although it does happen. The lack of safe points prevents the scheduler from\nrunning, which can have detrimental e\ufb00ects. It is possible to solve this problem, but not without\n8We have also called it a \u201cHaskell Execution Context\u201d, but the code currently uses the Capability terminology.\nSimon Marlow and Simon Peyton Jones\n83\n", "page": 97, "type": "text", "section": "Page 97"}
{"text": "impacting the performance of these loops, and often people care about saving every cycle in their\ninner loops. This may just be a compromise we have to live with.\n5.6\nDeveloping GHC\nGHC is a single project with a twenty-year life span, and is still in a ferment of innovation and\ndevelopment. For the most part our infrastructure and tooling has been conventional. For example,\nwe use a bug tracker (Trac), a wiki (also Trac), and Git for revision control. (This revision-control\nmechanism evolved from purely manual, then CVS, then Darcs, before \ufb01nally moving to Git in 2010.)\nThere are a few points that may be less universal, and we o\ufb00er them here.\nComments and Notes\nOne of the most serious di\ufb03culties in a large, long-lived project is keeping technical documentation\nup to date. We have no silver bullet, but we o\ufb00er one low-tech mechanism that has served us\nparticularly well: Notes.\nWhen writing code, there is often a moment when a careful programmer will mentally say\nsomething like \u201cThis data type has an important invariant\u201d. She is faced with two choices, both\nunsatisfactory. She can add the invariant as a comment, but that can make the data type declaration\ntoo long, so that it is hard to see what the constructors are. Alternatively, she can document the\ninvariant elsewhere, and risk it going out of date. Over twenty years, everything goes out of date!\nThus motivated, we developed the following very simple convention:\n\u2022 Comments of any signi\ufb01cant size are not interleaved with code, but instead set o\ufb00by themselves,\nwith a heading in standard form, thus:\nNote [Equality-constrained types]\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nThe type\nforall ab. (a ~ [b]) => blah\nis encoded like this:\nForAllTy (a:*) $ ForAllTy (b:*) $\nFunTy (TyConApp (~) [a, [b]]) $\nblah\n\u2022 A the point where the comment is relevant, we add a short comment referring to the Note:\ndata Type\n= FunTy Type Type -- See Note [Equality-constrained types]\n| ...\nThe comment highlights that something interesting is going on, and gives a precise reference to\nthe comment that explains. It sounds trivial, but the precision is vastly better than our previous\nhabit of saying \u201csee the comment above\u201d, because it often was not clear which of the many\ncomments above was intended, and after a few years the comment was not even above (it was\nbelow, or gone altogether).\n84\nThe Glasgow Haskell Compiler\n", "page": 98, "type": "text", "section": "Page 98"}
{"text": "Not only is it possible to go from the code that refers to the Note to the Note itself, but the reverse\nis also possible, and that is often useful. Moreover, the same Note may be referred to from multiple\npoints in the code.\nThis simple, ASCII-only technique, with no automated support, has transformed our lives: GHC\nhas around 800 Notes, and the number grows daily.\nHow to Keep On Refactoring\nThe code of GHC is churning just as quickly as it was ten years ago, if not more so. There is no\ndoubt that the complexity of the system has increased manyfold over that same time period; we saw\nmeasures of the amount of code in GHC earlier. Yet, the system remains manageable. We attribute\nthis to three main factors:\n\u2022 There\u2019s no substitute for good software engineering. Modularity always pays o\ufb00: making\nthe APIs between components as small as possible makes the individual components more\n\ufb02exible because they have fewer interdependencies. For example, GHC\u2019s Core{} datatype\nbeing small reduces the coupling between Core-to-Core passes, to the extent that they are\nalmost completely independent and can be run in arbitrary order.\n\u2022 Developing in a strongly-typed language makes refactoring a breeze. Whenever we need to\nchange a data type, or change the number of arguments or type of a function, the compiler\nimmediately tells us what other places in the code need to be \ufb01xed. Simply having an absolute\nguarantee that a large class of errors have been statically ruled out saves a huge amount of\ntime, especially when refactoring. It is scary to imagine how many hand-written test cases we\nwould need to provide the same level of coverage that the type system provides.\n\u2022 When programming in a purely functional language, it is hard to introduce accidental depen-\ndencies via state. If you decide that you suddenly need access to a piece of state deep in an\nalgorithm, in an imperative language you might be tempted to just make the state globally\nvisible rather than explicitly pass it down to the place that needs it. This way eventually leads\nto a tangle of invisible dependencies, and brittle code: code that breaks easily when modi\ufb01ed.\nPure functional programming forces you to make all the dependencies explicit, which exerts\nsome negative pressure on adding new dependencies, and fewer dependencies means greater\nmodularity. Certainly when it is necessary to add a new dependency then purity makes you\nwrite more code to express the dependency, but in our view it is a worthwhile price to pay for\nthe long-term health of the code base.\nAs an added bene\ufb01t, purely functional code is thread-safe by construction and tends to be\neasier to parallelise.\nCrime Doesn\u2019t Pay\nLooking back over the changes we\u2019ve had to make to GHC as it has grown, a common lesson emerges:\nbeing less than purely functional, whether for the purposes of e\ufb03ciency or convenience, tends to\nhave negative consequences down the road. We have a couple of great examples of this:\n\u2022 GHC uses a few data structures that rely on mutation internally. One is the FastString type,\nwhich uses a single global hash table; another is a global NameCache that ensures all external\nnames are assigned a unique number. When we tried to parallelise GHC (that is, make GHC\ncompile multiple modules in parallel on a multicore processor), these data structures based on\nSimon Marlow and Simon Peyton Jones\n85\n", "page": 99, "type": "text", "section": "Page 99"}
{"text": "mutation were the only sticking points. Had we not resorted to mutation in these places, GHC\nwould have been almost trivial to parallelise.\nIn fact, although we did build a prototype parallel version of GHC, GHC does not currently\ncontain support for parallel compilation, but that is largely because we have not yet invested\nthe e\ufb00ort required to make these mutable data structures thread-safe.\n\u2022 GHC\u2019s behaviour is governed to a large extent by command-line \ufb02ags. These command-line\n\ufb02ags are by de\ufb01nition constant over a given run of GHC, so in early versions of GHC we made\nthe values of these \ufb02ags available as top-level constants. For example, there was a top-level\nvalue opt_GlasgowExts of type Bool, that governed whether certain language extensions\nshould be enabled or not. Top-level constants are highly convenient, because their values don\u2019t\nhave to be explicitly passed as arguments to all the code that needs access to them.\nOf course these options are not really constants, because they change from run to run, and\nthe de\ufb01nition of opt_GlasgowExts involves calling unsafePerformIO because it hides a side\ne\ufb00ect. Nevertheless, this trick is normally considered \u201csafe enough\u201d because the value is\nconstant within any given run; it doesn\u2019t invalidate compiler optimisations, for example.\nHowever, GHC was later extended from a single-module compiler to a multi-module compiler.\nAt this point the trick of using top-level constants for \ufb02ags broke, because the \ufb02ags may have\ndi\ufb00erent values when compiling di\ufb00erent modules. So we had to refactor large amounts of\ncode to pass around the \ufb02ags explicitly.\nPerhaps you might argue that treating the \ufb02ags as state in the \ufb01rst place, as would be natural\nin an imperative language, would have sidestepped the problem. To some extent this is true,\nalthough purely functional code has a number of other bene\ufb01ts, not least of which is that\nrepresenting the \ufb02ags by an immutable data structure means that the resulting code is already\nthread-safe and will run in parallel without modi\ufb01cation.\nDeveloping the RTS\nGHC\u2019s runtime system presents a stark contrast to the compiler in many ways. There is the obvious\ndi\ufb00erence that the runtime system is written in C rather than Haskell, but there are also considerations\nunique to the RTS that give rise to a di\ufb00erent design philosophy:\n1. Every Haskell program spends a lot of time executing code in the RTS: 20\u201330% is typical,\nbut characteristics of Haskell programs vary a lot and so \ufb01gures greater or less than this range\nare also common. Every cycle saved by optimising the RTS is multiplied many times over, so\nit is worth spending a lot of time and e\ufb00ort to save those cycles.\n2. The runtime system is statically linked into every Haskell program9, so there is an incentive\nto keep it small.\n3. Bugs in the runtime system are often inscrutable to the user (e.g., \u201csegmentation fault\u201d) and\nare hard to work around. For example, bugs in the garbage collector tend not to be tied\nto the use of a particular language feature, but arise when some complex combination of\nfactors emerges at runtime. Furthermore, bugs of this kind tend to be non-deterministic (only\noccurring in some runs), and highly sensitive (tiny changes to the program make the bug\ndisappear). Bugs in the multithreaded version of the runtime system present even greater\nchallenges. It is therefore worth going to extra lengths to prevent these bugs, and also to build\ninfrastructure to make identifying them easier.\n9That is, unless dynamic linking is being used.\n86\nThe Glasgow Haskell Compiler\n", "page": 100, "type": "text", "section": "Page 100"}
{"text": "The symptoms of an RTS bug are often indistinguishable from two other kinds of failure:\nhardware failure, which is more common than you might think, and misuse of unsafe Haskell\nfeatures like the FFI (Foreign Function Interface). The \ufb01rst job in diagnosing a runtime crash\nis to rule out these two other causes.\n4. The RTS is low-level code that runs on several di\ufb00erent architectures and operating systems,\nand is regularly ported to new ones. Portability is important.\nEvery cycle and every byte is important, but correctness is even more so. Moreover, the tasks\nperformed by the runtime system are inherently complex, so correctness is hard to begin with.\nReconciling these has lead us to some interesting defensive techniques, which we describe in the\nfollowing sections.\nCoping With Complexity\nThe RTS is a complex and hostile programming environment. In contrast to the compiler, the RTS\nhas almost no type safety. In fact, it has even less type safety than most other C programs, because it\nis managing data structures whose types live at the Haskell level and not at the C level. For example,\nthe RTS has no idea that the object pointed to by the tail of a cons cell is either [] or another cons:\nthis information is simply not present at the C level. Moreover, the process of compiling Haskell\ncode erases types, so even if we told the RTS that the tail of a cons cell is a list, it would still have no\ninformation about the pointer in the head of the cons cell. So the RTS code has to do a lot of casting\nof C pointer types, and it gets very little help in terms of type safety from the C compiler.\nSo our \ufb01rst weapon in this battle is to avoid putting code in the RTS. Wherever possible, we put\nthe minimum amount of functionality into the RTS and write the rest in a Haskell library. This has\nrarely turned out badly; Haskell code is far more robust and concise than C, and performance is\nusually perfectly acceptable. Deciding where to draw the line is not an exact science, although in\nmany cases it is reasonably clear. For example, while it might be theoretically possible to implement\nthe garbage collector in Haskell, in practice it is extremely di\ufb03cult because Haskell does not allow\nthe programmer precise control of memory allocation, and so dropping down to C for this kind of\nlow-level task makes practical sense.\nThere is plenty of functionality that can\u2019t be (easily) implemented in Haskell, and writing code\nin the RTS is not pleasant. In the next section we focus on one aspect of managing complexity and\ncorrectness in the RTS: maintaining invariants.\nInvariants, and Checking Them\nThe RTS is full of invariants. Many of them are trivial and easy to check: for example, if the pointer\nto the head of a queue is NULL, then the pointer to the tail should also be NULL. The code of the RTS\nis littered with assertions to check these kinds of things. Assertions are our go-to tool for \ufb01nding\nbugs before they manifest; in fact, when a new invariant is added, we often add the assertion before\nwriting the code that implements the invariant.\nSome of the invariants in the runtime are far more di\ufb03cult to satisfy, and to check. One invariant\nof this kind that pervades more of the RTS than any other is the following: the heap has no dangling\npointers.\nDangling pointers are easy to introduce, and there are many places both in the compiler and the\nRTS itself that can violate this invariant. The code generator could generate code that creates invalid\nheap objects; the garbage collector might forget to update the pointers of some object when it scans\nSimon Marlow and Simon Peyton Jones\n87\n", "page": 101, "type": "text", "section": "Page 101"}
{"text": "the heap. Tracking down these kinds of bugs can be extremely time-consuming10 because by the\ntime the program eventually crashes, execution might have progressed a long way from where the\ndangling pointer was originally introduced. There are good debugging tools available, but they tend\nnot to be good at executing the program in reverse.11\nThe general principle is: if a program is going to crash, it should crash as soon, as noisily, and\nas often as possible.12\nThe problem is, the no-dangling-pointer invariant is not something that can be checked with a\nconstant-time assertion. The assertion that checks it must do a full traversal of the heap! Clearly we\ncannot run this assertion after every heap allocation, or every time the GC scans an object (indeed,\nthis would not even be enough, as dangling pointers don\u2019t appear until the end of GC, when memory\nis freed).\nSo, the debug RTS has an optional mode that we call sanity checking. Sanity checking enables all\nkinds of expensive assertions, and can make the program run many times more slowly. In particular,\nsanity checking runs a full scan of the heap to check for dangling pointers (amongst other things),\nbefore and after every GC. The \ufb01rst job when investigating a runtime crash is to run the program\nwith sanity checking turned on; sometimes this will catch the invariant violation well before the\nprogram actually crashes.\n5.7\nConclusion\nGHC has consumed a signi\ufb01cant portion of the authors\u2019 lives over the last 20 years, and we are rather\nproud of how far it has come. It is not the only Haskell implementation, but it is the only one in\nregular use by hundreds of thousands of people to get real work done. We are constantly surprised\nwhen Haskell turns up being used in unusual places; one recent example is Haskell being used to\ncontrol the systems in a garbage truck13.\nFor many, Haskell and GHC are synonymous: it was never intended to be so, and indeed in\nmany ways it is counterproductive to have just one implementation of a standard, but the fact is that\nmaintaining a good implementation of a programming language is a lot of work. We hope that our\ne\ufb00orts in GHC, to support the standard and to clearly delimit each separate language extension, will\nmake it feasible for more implementations to emerge and to integrate with the the package system\nand other infrastructure. Competition is good for everyone!\nWe are deeply indebted to Microsoft in particular for giving us the opportunity to develop GHC\nas part of our research and to distribute it as open source.\n10It is, however, one of the author\u2019s favourite activities!\n11Recent versions of GDB and the Microsoft Visual Studio debugger do have some support for reverse execution, however.\n12This quote comes from the GHC coding style guidelines, and was originally written by Alastair Reid, who worked on an\nearly version of the RTS.\n13http://www.haskell.org/pipermail/haskell-cafe/2010-April/075647.html\n88\nThe Glasgow Haskell Compiler\n", "page": 102, "type": "text", "section": "Page 102"}
{"text": "[chapter6]\nGit\nSusan Potter\n6.1\nGit in a Nutshell\nGit enables the maintenance of a digital body of work (often, but not limited to, code) by many\ncollaborators using a peer-to-peer network of repositories. It supports distributed work\ufb02ows, allowing\na body of work to either eventually converge or temporarily diverge.\nThis chapter will show how various aspects of Git work under the covers to enable this, and how\nit di\ufb00ers from other version control systems (VCSs).\n6.2\nGit\u2019s Origin\nTo understand Git\u2019s design philosophy better it is helpful to understand the circumstances in which\nthe Git project was started in the Linux Kernel Community.\nThe Linux kernel was unusual, compared to most commercial software projects at that time,\nbecause of the large number of committers and the high variance of contributor involvement and\nknowledge of the existing codebase. The kernel had been maintained via tarballs and patches for\nyears, and the core development community struggled to \ufb01nd a VCS that satis\ufb01ed most of their needs.\nGit is an open source project that was born out of those needs and frustrations in 2005. At that\ntime the Linux kernel codebase was managed across two VCSs, BitKeeper and CVS, by di\ufb00erent\ncore developers. BitKeeper o\ufb00ered a di\ufb00erent view of VCS history lineage than that o\ufb00ered by the\npopular open source VCSs at this time.\nDays after BitMover, the maker of BitKeeper, announced it would revoke the licenses of some\ncore Linux kernel developers, Linus Torvalds began development, in haste, of what was to become\nGit. He began by writing a collection of scripts to help him manage email patches to apply one after\nthe other. The aim of this initial collection of scripts was to be able to abort merges quickly so the\nmaintainer could modify the codebase mid-patch-stream to manually merge, then continue merging\nsubsequent patches.\nFrom the outset, Torvalds had one philosophical goal for Git\u2014to be the anti-CVS\u2014plus three\nusability design goals:\n\u2022 Support distributed work\ufb02ows similar to those enabled by BitKeeper\n\u2022 O\ufb00er safeguards against content corruption\n\u2022 O\ufb00er high performance\n", "page": 103, "type": "text", "section": "Page 103"}
{"text": "These design goals have been accomplished and maintained, to a degree, as I will attempt to\nshow by dissecting Git\u2019s use of directed acyclic graphs (DAGs) for content storage, reference pointers\nfor heads, object model representation, and remote protocol; and \ufb01nally how Git tracks the merging\nof trees.\nDespite BitKeeper in\ufb02uencing the original design of Git, it is implemented in fundamentally\ndi\ufb00erent ways and allows even more distributed plus local-only work\ufb02ows, which were not possible\nwith BitKeeper. Monotone1, an open source distributed VCS started in 2003, was likely another\ninspiration during Git\u2019s early development.\nDistributed version control systems o\ufb00er great work\ufb02ow \ufb02exibility, often at the expense of\nsimplicity. Speci\ufb01c bene\ufb01ts of a distributed model include:\n\u2022 Providing the ability for collaborators to work o\ufb04ine and commit incrementally.\n\u2022 Allowing a collaborator to determine when his/her work is ready to share.\n\u2022 O\ufb00ering the collaborator access to the repository history when o\ufb04ine.\n\u2022 Allowing the managed work to be published to multiple repositories, potentially with di\ufb00erent\nbranches or granularity of changes visible.\nAround the time the Git project started, three other open source distributed VCS projects were\ninitiated. (One of them, Mercurial, is discussed in Volume 1 of The Architecture of Open Source\nApplications.) All of these dVCS tools o\ufb00er slightly di\ufb00erent ways to enable highly \ufb02exible work\ufb02ows,\nwhich centralized VCSs before them were not capable of handling directly. Note: Subversion has an\nextension named SVK maintained by di\ufb00erent developers to support server-to-server synchronization.\nToday popular and actively maintained open source dVCS projects include Bazaar, Darcs, Fossil,\nGit, Mercurial, and Veracity.\n6.3\nVersion Control System Design\nNow is a good time to take a step back and look at the alternative VCS solutions to Git. Understanding\ntheir di\ufb00erences will allow us to explore the architectural choices faced while developing Git.\nA version control system usually has three core functional requirements, namely:\n\u2022 Storing content\n\u2022 Tracking changes to the content (history including merge metadata)\n\u2022 Distributing the content and history with collaborators\nNote: The third requirement above is not a functional requirement for all VCSs.\nContent Storage\nThe most common design choices for storing content in the VCS world are with a delta-based\nchangeset, or with directed acyclic graph (DAG) content representation.\nDelta-based changesets encapsulate the di\ufb00erences between two versions of the \ufb02attened content,\nplus some metadata. Representing content as a directed acyclic graph involves objects forming\na hierarchy which mirrors the content\u2019s \ufb01lesystem tree as a snapshot of the commit (reusing the\nunchanged objects inside the tree where possible). Git stores content as a directed acyclic graph\nusing di\ufb00erent types of objects. The \u201cObject Database\u201d section later in this chapter describes the\ndi\ufb00erent types of objects that can form DAGs inside the Git repository.\n1http://www.monotone.ca/\n90\nGit\n", "page": 104, "type": "text", "section": "Page 104"}
{"text": "Commit and Merge Histories\nOn the history and change-tracking front most VCS software uses one of the following approaches:\n\u2022 Linear history\n\u2022 Directed acyclic graph for history\nAgain Git uses a DAG, this time to store its history. Each commit contains metadata about its\nancestors; a commit in Git can have zero or many (theoretically unlimited) parent commits. For\nexample, the \ufb01rst commit in a Git repository would have zero parents, while the result of a three-way\nmerge would have three parents.\nAnother primary di\ufb00erence between Git and Subversion and its linear history ancestors is its\nability to directly support branching that will record most merge history cases.\nFigure 6.1: Example of a DAG representation in Git\nGit enables full branching capability using directed acyclic graphs to store content. The history\nof a \ufb01le is linked all the way up its directory structure (via nodes representing directories) to the root\ndirectory, which is then linked to a commit node. This commit node, in turn, can have one or more\nparents. This a\ufb00ords Git two properties that allow us to reason about history and content in more\nde\ufb01nite ways than the family of VCSs derived from RCS do, namely:\n\u2022 When a content (i.e., \ufb01le or directory) node in the graph has the same reference identity (the\nSHA in Git) as that in a di\ufb00erent commit, the two nodes are guaranteed to contain the same\ncontent, allowing Git to short-circuit content di\ufb03ng e\ufb03ciently.\n\u2022 When merging two branches we are merging the content of two nodes in a DAG. The DAG\nallows Git to \"e\ufb03ciently\" (as compared to the RCS family of VCS) determine common\nancestors.\nSusan Potter\n91\n", "page": 105, "type": "text", "section": "Page 105"}
{"text": "Distribution\nVCS solutions have handled content distribution of a working copy to collaborators on a project in\none of three ways:\n\u2022 Local-only: for VCS solutions that do not have the third functional requirement above.\n\u2022 Central server: where all changes to the repository must transact via one speci\ufb01c repository\nfor it to be recorded in history at all.\n\u2022 Distributed model: where there will often be publicly accessible repositories for collaborators\nto \"push\" to, but commits can be made locally and pushed to these public nodes later, allowing\no\ufb04ine work.\nTo demonstrate the bene\ufb01ts and limitations of each major design choice, we will consider a\nSubversion repository and a Git repository (on a server), with equivalent content (i.e., the HEAD of\nthe default branch in the Git repository has the same content as the Subversion repository\u2019s latest\nrevision on trunk). A developer, named Alex, has a local checkout of the Subversion repository and\na local clone of the Git repository.\nLet us say Alex makes a change to a 1 MB \ufb01le in the local Subversion checkout, then commits\nthe change. Locally, the checkout of the \ufb01le mimics the latest change and local metadata is updated.\nDuring Alex\u2019s commit in the centralized Subversion repository, a di\ufb00is generated between the\nprevious snapshot of the \ufb01les and the new changes, and this di\ufb00is stored in the repository.\nContrast this with the way Git works. When Alex makes the same modi\ufb01cation to the equivalent\n\ufb01le in the local Git clone, the change will be recorded locally \ufb01rst, then Alex can \"push\" the local\npending commits to a public repository so the work can be shared with other collaborators on the\nproject. The content changes are stored identically for each Git repository that the commit exists\nin. Upon the local commit (the simplest case), the local Git repository will create a new object\nrepresenting a \ufb01le for the changed \ufb01le (with all its content inside). For each directory above the\nchanged \ufb01le (plus the repository root directory), a new tree object is created with a new identi\ufb01er. A\nDAG is created starting from the newly created root tree object pointing to blobs (reusing existing\nblob references where the \ufb01les content has not changed in this commit) and referencing the newly\ncreated blob in place of that \ufb01le\u2019s previous blob object in the previous tree hierarchy. (A blob\nrepresents a \ufb01le stored in the repository.)\nAt this point the commit is still local to the current Git clone on Alex\u2019s local device. When Alex\n\"pushes\" the commit to a publicly accessible Git repository this commit gets sent to that repository.\nAfter the public repository veri\ufb01es that the commit can apply to the branch, the same objects are\nstored in the public repository as were originally created in the local Git repository.\nThere are a lot more moving parts in the Git scenario, both under the covers and for the user,\nrequiring them to explicitly express intent to share changes with the remote repository separately\nfrom tracking the change as a commit locally. However, both levels of added complexity o\ufb00er the\nteam greater \ufb02exibility in terms of their work\ufb02ow and publishing capabilities, as described in the\n\u201cGit\u2019s Origin\u201d section above.\nIn the Subversion scenario, the collaborator did not have to remember to push to the public\nremote repository when ready for others to view the changes made. When a small modi\ufb01cation\nto a larger \ufb01le is sent to the central Subversion repository the delta stored is much more e\ufb03cient\nthan storing the complete \ufb01le contents for each version. However, as we will see later, there is a\nworkaround for this that Git takes advantage of in certain scenarios.\n92\nGit\n", "page": 106, "type": "text", "section": "Page 106"}
{"text": "6.4\nThe Toolkit\nToday the Git ecosystem includes many command-line and UI tools on a number of operating systems\n(including Windows, which was originally barely supported). Most of these tools are mostly built on\ntop of the Git core toolkit.\nDue to the way Git was originally written by Linus, and its inception within the Linux community,\nit was written with a toolkit design philosophy very much in the Unix tradition of command line\ntools.\nThe Git toolkit is divided into two parts: the plumbing and the porcelain. The plumbing consists\nof low-level commands that enable basic content tracking and the manipulation of directed acyclic\ngraphs (DAG). The porcelain is the smaller subset of git commands that most Git end users are likely\nto need to use for maintaining repositories and communicating between repositories for collaboration.\nWhile the toolkit design has provided enough commands to o\ufb00er \ufb01ne-grained access to function-\nality for many scripters, application developers complained about the lack of a linkable library for\nGit. Since the Git binary calls die(), it is not reentrant and GUIs, web interfaces or longer running\nservices would have to fork/exec a call to the Git binary, which can be slow.\nWork is being done to improve the situation for application developers; see the \u201cCurrent And\nFuture Work\u201d section for more information.\n6.5\nThe Repository, Index and Working Areas\nLet\u2019s get our hands dirty and dive into using Git locally, if only to understand a few fundamental\nconcepts.\nFirst to create a new initialized Git repository on our local \ufb01lesystem (using a Unix inspired\noperating system) we can do:\n$ mkdir testgit\n$ cd testgit\n$ git init\nNow we have an empty, but initialized, Git repository sitting in our testgit directory. We can branch,\ncommit, tag and even communicate with other local and remote Git repositories. Even communication\nwith other types of VCS repositories is possible with just a handful of git commands.\nThe git init command creates a .git subdirectory inside of testgit. Let\u2019s have a peek inside it:\ntree .git/\n.git/\n|-- HEAD\n|-- config\n|-- description\n|-- hooks\n|\n|-- applypatch-msg.sample\n|\n|-- commit-msg.sample\n|\n|-- post-commit.sample\n|\n|-- post-receive.sample\n|\n|-- post-update.sample\n|\n|-- pre-applypatch.sample\n|\n|-- pre-commit.sample\n|\n|-- pre-rebase.sample\nSusan Potter\n93\n", "page": 107, "type": "text", "section": "Page 107"}
{"text": "|\n|-- prepare-commit-msg.sample\n|\n|-- update.sample\n|-- info\n|\n|-- exclude\n|-- objects\n|\n|-- info\n|\n|-- pack\n|-- refs\n|-- heads\n|-- tags\nThe .git directory above is, by default, a subdirectory of the root working directory, testgit.\nIt contains a few di\ufb00erent types of \ufb01les and directories:\n\u2022 Con\ufb01guration: the .git/config, .git/description and .git/info/exclude \ufb01les essen-\ntially help con\ufb01gure the local repository.\n\u2022 Hooks: the .git/hooks directory contains scripts that can be run on certain lifecycle events\nof the repository.\n\u2022 Staging Area: the .git/index \ufb01le (which is not yet present in our tree listing above) will\nprovide a staging area for our working directory.\n\u2022 Object Database: the .git/objects directory is the default Git object database, which\ncontains all content or pointers to local content. All objects are immutable once created.\n\u2022 References: the .git/refs directory is the default location for storing reference pointers for\nboth local and remote branches, tags and heads. A reference is a pointer to an object, usually\nof type tag or commit. References are managed outside of the Object Database to allow the\nreferences to change where they point to as the repository evolves. Special cases of references\nmay point to other references, e.g. HEAD.\nThe .git directory is the actual repository. The directory that contains the working set of \ufb01les\nis the working directory, which is typically the parent of the .git directory (or repository). If you\nwere creating a Git remote repository that would not have a working directory, you could initialize it\nusing the git init --bare command. This would create just the pared-down repository \ufb01les at the\nroot, instead of creating the repository as a subdirectory under the working tree.\nAnother \ufb01le of great importance is the Git index: .git/index. It provides the staging area\nbetween the local working directory and the local repository. The index is used to stage speci\ufb01c\nchanges within one \ufb01le (or more), to be committed all together. Even if you make changes related to\nvarious types of features, the commits can be made with like changes together, to more logically\ndescribe them in the commit message. To selectively stage speci\ufb01c changes in a \ufb01le or set of \ufb01les\nyou can using git add -p.\nThe Git index, by default, is stored as a single \ufb01le inside the repository directory. The paths to\nthese three areas can be customized using environment variables.\nIt is helpful to understand the interactions that take place between these three areas (the repository,\nindex and working areas) during the execution of a few core Git commands:\n\u2022 git checkout [branch]\nThis will move the HEAD reference of the local repository to branch reference path (e.g.\nrefs/heads/master), populate the index with this head data and refresh the working directory\nto represent the tree at that head.\n\u2022 git add [files]\n94\nGit\n", "page": 108, "type": "text", "section": "Page 108"}
{"text": "This will cross reference the checksums of the \ufb01les speci\ufb01ed with the corresponding entries in\nthe Git index to see if the index for staged \ufb01les needs updating with the working directory\u2019s\nversion. Nothing changes in the Git directory (or repository).\nLet us explore what this means more concretely by inspecting the contents of \ufb01les under the\n.git directory (or repository).\n$ GIT_DIR=$PWD/.git\n$ cat $GIT_DIR/HEAD\nref: refs/heads/master\n$ MY_CURRENT_BRANCH=$(cat .git/HEAD | sed \u2019s/ref: //g\u2019)\n$ cat $GIT_DIR/$MY_CURRENT_BRANCH\ncat: .git/refs/heads/master: No such file or directory\nWe get an error because, before making any commits to a Git repository at all, no branches exist\nexcept the default branch in Git which is master, whether it exists yet or not.\nNow if we make a new commit, the master branch is created by default for this commit. Let us\ndo this (continuing in the same shell, retaining history and context):\n$ git commit -m \"Initial empty commit\" --allow-empty\n$ git branch\n* master\n$ cat $GIT_DIR/$MY_CURRENT_BRANCH\n3bce5b130b17b7ce2f98d17b2998e32b1bc29d68\n$ git cat-file -p $(cat $GIT_DIR/$MY_CURRENT_BRANCH)\nWhat we are starting to see here is the content representation inside Git\u2019s object database.\n6.6\nThe Object Database\nGit has four basic primitive objects that every type of content in the local repository is built around.\nEach object type has the following attributes: type, size and content. The primitive object types are:\n\u2022 Tree: an element in a tree can be another tree or a blob, when representing a content directory.\n\u2022 Blob: a blob represents a \ufb01le stored in the repository.\n\u2022 Commit: a commit points to a tree representing the top-level directory for that commit as well\nas parent commits and standard attributes.\n\u2022 Tag: a tag has a name and points to a commit at the point in the repository history that the tag\nrepresents.\nAll object primitives are referenced by a SHA, a 40-digit object identity, which has the following\nproperties:\n\u2022 If two objects are identical they will have the same SHA.\n\u2022 if two objects are di\ufb00erent they will have di\ufb00erent SHAs.\nSusan Potter\n95\n", "page": 109, "type": "text", "section": "Page 109"}
{"text": "Figure 6.2: Git objects\n\u2022 If an object was only copied partially or another form of data corruption occurred, recalculating\nthe SHA of the current object will identify such corruption.\nThe \ufb01rst two properties of the SHA, relating to identity of the objects, is most useful in enabling\nGit\u2019s distributed model (the second goal of Git). The latter property enables some safeguards against\ncorruption (the third goal of Git).\nDespite the desirable results of using DAG-based storage for content storage and merge histories,\nfor many repositories delta storage will be more space-e\ufb03cient than using loose DAG objects.\n6.7\nStorage and Compression Techniques\nGit tackles the storage space problem by packing objects in a compressed format, using an index \ufb01le\nwhich points to o\ufb00sets to locate speci\ufb01c objects in the corresponding packed \ufb01le.\nWe can count the number of loose (or unpacked) objects in the local Git repository using git\ncount-objects. Now we can have Git pack loose objects in the object database, remove loose\nobjects already packed, and \ufb01nd redundant pack \ufb01les with Git plumbing commands if desired.\nThe pack \ufb01le format in Git has evolved, with the initial format storing CRC checksums for the\npack \ufb01le and index \ufb01le in the index \ufb01le itself. However, this meant there was the possibility of\nundetectable corruption in the compressed data since the repacking phase did not involve any further\nchecks. Version 2 of the pack \ufb01le format overcomes this problem by including the CRC checksums\n96\nGit\n", "page": 110, "type": "text", "section": "Page 110"}
{"text": "Figure 6.3: Diagram of a pack \ufb01le with corresponding index \ufb01le\nof each compressed object in the pack index \ufb01le. Version 2 also allows pack\ufb01les larger than 4 GB,\nwhich the initial format did not support. As a way to quickly detect pack \ufb01le corruption the end\nof the pack \ufb01le contains a 20-byte SHA1 sum of the ordered list of all the SHAs in that \ufb01le. The\nemphasis of the newer pack \ufb01le format is on helping ful\ufb01ll Git\u2019s second usability design goal of\nsafeguarding against data corruption.\nFor remote communication Git calculates the commits and content that need to be sent over the\nwire to synchronize repositories (or just a branch), and generates the pack \ufb01le format on the \ufb02y to\nsend back using the desired protocol of the client.\n6.8\nMerge Histories\nAs mentioned previously, Git di\ufb00ers fundamentally in merge history approach than the RCS family\nof VCSs. Subversion, for example, represents \ufb01le or tree history in a linear progression; whatever\nhas a higher revision number will supercede anything before it. Branching is not supported directly,\nonly through an unenforced directory structure within the repository.\nLet us \ufb01rst use an example to show how this can be problematic when maintaining multiple\nbranches of a work. Then we will look at a scenario to show its limitations.\nWhen working on a \"branch\" in Subversion at the typical root branches/branch-name, we are\nworking on directory subtree adjacent to the trunk (typically where the live or master equivalent\ncode resides within). Let us say this branch is to represent parallel development of the trunk tree.\nFor example, we might be rewriting a codebase to use a di\ufb00erent database. Part of the way\nthrough our rewrite we wish to merge in upstream changes from another branch subtree (not trunk).\nWe merge in these changes, manually if necessary, and proceed with our rewrite. Later that day\nwe \ufb01nish our database vendor migration code changes on our branches/branch-name branch and\nmerge our changes into trunk. The problem with the way linear-history VCSs like Subversion handle\nthis is that there is no way to know that the changesets from the other branch are now contained\nwithin the trunk.\nSusan Potter\n97\n", "page": 111, "type": "text", "section": "Page 111"}
{"text": "Figure 6.4: Diagram showing merge history lineage\nDAG-based merge history VCSs, like Git, handle this case reasonably well. Assuming the other\nbranch does not contain commits that have not been merged into our database vendor migration\nbranch (say, db-migration in our Git repository), we can determine\u2014from the commit object parent\nrelationships\u2014that a commit on the db-migration branch contained the tip (or HEAD) of the other\nupstream branch. Note that a commit object can have zero or more (bounded by only the abilities\nof the merger) parents. Therefore the merge commit on the db-migration branch knows it merged\nin the current HEAD of the current branch and the HEAD of the other upstream branch through\nthe SHA hashes of the parents. The same is true of the merge commit in the master (the trunk\nequivalent in Git).\nA question that is hard to answer de\ufb01nitively using DAG-based (and linear-based) merge histories\nis which commits are contained within each branch. For example, in the above scenario we assumed\nwe merged into each branch all the changes from both branches. This may not be the case.\nFor simpler cases Git has the ability to cherry pick commits from other branches in to the current\nbranch, assuming the commit can cleanly be applied to the branch.\n6.9\nWhat\u2019s Next?\nAs mentioned previously, Git core as we know it today is based on a toolkit design philosophy\nfrom the Unix world, which is very handy for scripting but less useful for embedding inside or\nlinking with longer running applications or services. While there is Git support in many popular\n98\nGit\n", "page": 112, "type": "text", "section": "Page 112"}
{"text": "Integrated Development Environments today, adding this support and maintaining it has been more\nchallenging than integrating support for VCSs that provide an easy-to-link-and-share library for\nmultiple platforms.\nTo combat this, Shawn Pearce (of Google\u2019s Open Source Programs O\ufb03ce) spearheaded an\ne\ufb00ort to create a linkable Git library with more permissive licensing that did not inhibit use of the\nlibrary. This was called libgit22. It did not \ufb01nd much traction until a student named Vincent Marti\nchose it for his Google Summer of Code project last year. Since then Vincent and Github engineers\nhave continued contributing to the libgit2 project, and created bindings for numerous other popular\nlanguages such as Ruby, Python, PHP, .NET languages, Lua, and Objective-C.\nShawn Pearce also started a BSD-licensed pure Java library called JGit that supports many\ncommon operations on Git repositories3. It is now maintained by the Eclipse Foundation for use in\nthe Eclipse IDE Git integration.\nOther interesting and experimental open source endeavours outside of the Git core project are a\nnumber of implementations using alternative datastores as backends for the Git object database such\nas:\n\u2022 jgit_cassandra4, which o\ufb00ers Git object persistence using Apache Cassandra, a hybrid datastore\nusing Dynamo-style distribution with BigTable column family data model semantics.\n\u2022 jgit_hbase5, which enables read and write operations to Git objects stored in HBase, a dis-\ntributed key-value datastore.\n\u2022 libgit2-backends6, which emerged from the libgit2 e\ufb00ort to create Git object database backends\nfor multiple popular datastores such as Memcached, Redis, SQLite, and MySQL.\nAll of these open source projects are maintained independently of the Git core project.\nAs you can see, today there are a large number of ways to use the Git format. The face of Git is\nno longer just the toolkit command line interface of the Git Core project; rather it is the repository\nformat and protocol to share between repositories.\nAs of this writing, most of these projects, according to their developers, have not reached a stable\nrelease, so work in the area still needs to be done but the future of Git appears bright.\n6.10\nLessons Learned\nIn software, every design decision is ultimately a trade-o\ufb00. As a power user of Git for version control\nand as someone who has developed software around the Git object database model, I have a deep\nfondness for Git in its present form. Therefore, these lessons learned are more of a re\ufb02ection of\ncommon recurring complaints about Git that are due to design decisions and focus of the Git core\ndevelopers.\nOne of the most common complaints by developers and managers who evaluate Git has been the\nlack of IDE integration on par with other VCS tools. The toolkit design of Git has made this more\nchallenging than integrating other modern VCS tools into IDEs and related tools.\nEarlier in Git\u2019s history some of the commands were implemented as shell scripts. These shell\nscript command implementations made Git less portable, especially to Windows. I am sure the\nGit core developers did not lose sleep over this fact, but it has negatively impacted adoption of\n2https://github.com/libgit2/libgit2\n3https://github.com/eclipse/jgit\n4https://github.com/spearce/jgit_cassandra\n5https://github.com/spearce/jgit_hbase\n6https://github.com/libgit2/libgit2-backends\nSusan Potter\n99\n", "page": 113, "type": "text", "section": "Page 113"}
{"text": "Git in larger organizations due to portability issues that were prevalent in the early days of Git\u2019s\ndevelopment. Today a project named Git for Windows has been started by volunteers to ensure new\nversions of Git are ported to Windows in a timely manner.\nAn indirect consequence of designing Git around a toolkit design with a lot of plumbing com-\nmands is that new users get lost quickly; from confusion about all the available subcommands to not\nbeing able to understand error messages because a low level plumbing task failed, there are many\nplaces for new users to go astray. This has made adopting Git harder for some developer teams.\nEven with these complaints about Git, I am excited about the possibilities of future development\non the Git Core project, plus all the related open source projects that have been launched from it.\n100\nGit\n", "page": 114, "type": "text", "section": "Page 114"}
{"text": "[chapter7]\nGPSD\nEric Raymond\nGPSD is a suite of tools for managing collections of GPS devices and other sensors related to\nnavigation and precision timekeeping, including marine AIS (Automatic Identi\ufb01cation System)\nradios and digital compasses. The main program, a service daemon named gpsd, manages a\ncollection of sensors and makes reports from all of them available as a JSON object stream on a\nwell-known TCP/IP port. Other programs in the suite include demonstration clients usable as code\nmodels and various diagnostic tools.\nGPSD is widely deployed on laptops, smartphones, and autonomous vehicles including self-\ndriving automobiles and robot submarines. It features in embedded systems used for navigation,\nprecision agriculture, location-sensitive scienti\ufb01c telemetry, and network time service. It\u2019s even\nused in the Identi\ufb01cation-Friend-or-Foe system of armored \ufb01ghting vehicles including the M1\n\u201cAbrams\u201dmain battle tank.\nGPSD is a mid-sized project\u2014about 43 KLOC, mainly in C and Python\u2014with a history under\nits current lead going back to 2005 and a prehistory going back to 1997. The core team has been\nstable at about three developers, with semi-regular contributions from about two dozen more and the\nusual one-o\ufb00patches from hundreds of others.\nGPSD has historically had an exceptionally low defect rate, as measured both by auditing\ntools such as splint, valgrind, and Coverity and by the incidence of bug reports on its tracker\nand elsewhere. This did not come about by accident; the project has been very aggressive about\nincorporating technology for automated testing, and that e\ufb00ort has paid o\ufb00handsomely.\nGPSD is su\ufb03ciently good at what it does that it has coopted or e\ufb00ectively wiped out all of its\napproximate predecessors and at least one direct attempt to compete with it. In 2010, GPSD won the\n\ufb01rst Good Code Grant from the Alliance for Code Excellence. By the time you \ufb01nish this chapter\nyou should understand why.\n7.1\nWhy GPSD Exists\nGPSD exists because the application protocols shipped with GPSs and other navigation-related\nsensors are badly designed, poorly documented, and highly variable by sensor type and model. See\n[Ray] for a detailed discussion; in particular, you\u2019ll learn there about the vagaries of NMEA 0183\n(the sort-of standard for GPS reporting packets) and the messy pile of poorly documented vendor\nprotocols that compete with it.\n", "page": 115, "type": "text", "section": "Page 115"}
{"text": "If applications had to handle all this complexity themselves the result would be huge amounts of\nbrittle and duplicative code, leading to high rates of user-visible defects and constant problems as\nhardware gradually mutated out from under the applications.\nGPSD isolates location-aware applications from hardware interface details by knowing about\nall the protocols itself (at time of writing we support about 20 di\ufb00erent ones), managing serial and\nUSB devices so the applications don\u2019t have to, and reporting sensor payload information in a simple\ndevice-independent JSON format. GPSD further simpli\ufb01es life by providing client libraries so client\napplications need not even know about that reporting format. Instead, getting sensor information\nbecomes a simple procedure call.\nGPSD also supports precision timekeeping; it can act as a time source for ntpd (the Network\nTime Protocol Daemon) if any of its attached sensors have PPS (pulse-per-second) capability. The\nGPSD developers cooperate closely with the ntpd project in improving the network time service.\nWe are presently (mid-2011) working on completing support for the AIS network of marine\nnavigational receivers. In the future, we expect to support new kinds of location-aware sensors\u2014such\nas receivers for second-generation aircraft transponders\u2014as protocol documentation and test devices\nbecome available.\nTo sum up, the single most important theme in GPSD\u2019s design is hiding all the device-dependent\nugliness behind a simple client interface talking to a zero-con\ufb01guration service.\n7.2\nThe External View\nThe main program in the GPSD suite is the gpsd service daemon. It can collect the take from a set of\nattached sensor devices over RS232, USB, Bluetooth, TCP/IP, and UDP links. Reports are normally\nshipped to TCP/IP port 2947, but can also go out via a shared-memory or D-BUS interface.\nThe GPSD distribution ships with client libraries for C, C++, and Python. It includes sample\nclients in C, C++, Python, and PHP. A Perl client binding is available via CPAN. These client libraries\nare not merely a convenience for application developers; they save GPSD\u2019s developers headaches\ntoo, by isolating applications from the details of GPSD\u2019s JSON reporting protocol. Thus, the API\nexposed to clients can remain the same even as the protocol grows new features for new sensor types.\nOther programs in the suite include a utility for low-level device monitoring (gpsmon), a pro\ufb01ler\nthat produces reports on error statistics and device timing (gpsprof), a utility for tweaking device\nsettings (gpsctl), and a program for batch-converting sensor logs into readable JSON (gpsdecode).\nTogether, they help technically savvy users look as deeply into the operation of the attached sensors\nas they care to.\nOf course, these tools also help GPSD\u2019s own developers verify the correct operation of gpsd.\nThe single most important test tool is gpsfake, a test harness for gpsd which can connect it to any\nnumber of sensor logs as though they were live devices. With gpsfake, we can re-run a sensor\nlog shipped with a bug report to reproduce speci\ufb01c problems. gpsfake is also the engine of our\nextensive regression-test suite, which lowers the cost of modifying the software by making it easy to\nspot changes that break things.\nOne of the most important lessons we think we have for future projects is that it is not enough for\na software suite to be correct, it should also be able to demonstrate its own correctness. We have\nfound that when this goal is pursued properly it is not a hair shirt but rather a pair of wings\u2014the\ntime we\u2019ve take to write test harnesses and regression tests has paid for itself many times over in the\nfreedom it gives us to modify code without fearing that we are wreaking subtle havoc on existing\nfunctionality.\n102\nGPSD\n", "page": 116, "type": "text", "section": "Page 116"}
{"text": "7.3\nThe Software Layers\nThere is a lot more going on inside GPSD than the \u201cplug a sensor in and it just works\u201d experience\nmight lead people to assume. gpsd\u2019s internals break naturally into four pieces: the drivers, the packet\nsni\ufb00er, the core library and the multiplexer. We\u2019ll describe these from the bottom up.\nMultiplexer\nCore library\nPacket Sniffer\nDriver\nDriver\nDriver\nDriver\n.  .  .\nsensor\nsensor\nsensor\nsensor\nclient \nsession\nclient \nsession\nclient \nsession\nclient \nsession\nclient \nsession\nFigure 7.1: Software layers\nThe drivers are essentially user-space device drivers for each kind of sensor chipset we support.\nThe key entry points are methods to parse a data packet into time-position-velocity or status informa-\ntion, change its mode or baud rate, probe for device subtype, etc. Auxiliary methods may support\ndriver control operations, such as changing the serial speed of the device. The entire interface to a\ndriver is a C structure full of data and method pointers, deliberately modeled on a Unix device driver\nstructure.\nThe packet sni\ufb00er is responsible for mining data packets out of serial input streams. It\u2019s basically\na state machine that watches for anything that looks like one of our 20 or so known packet types\n(most of which are checksummed, so we can have high con\ufb01dence when we think we have identi\ufb01ed\none). Because devices can hotplug or change modes, the type of packet that will come up the wire\nfrom a serial or USB port isn\u2019t necessarily \ufb01xed forever by the \ufb01rst one recognized.\nThe core library manages a session with a sensor device. The key entry points are:\n\u2022 starting a session by opening the device and reading data from it, hunting through baud rates\nand parity/stopbit combinations until the packet sni\ufb00er achieves synchronization lock with a\nknown packet type;\n\u2022 polling the device for a packet; and\n\u2022 closing the device and wrapping up the session.\nA key feature of the core library is that it is responsible for switching each GPS connection to\nusing the correct device driver depending on the packet type that the sni\ufb00er returns. This is not\ncon\ufb01gured in advance and may change over time, notably if the device switches between di\ufb00erent\nreporting protocols. (Most GPS chipsets support NMEA and one or more vendor binary protocols,\nand devices like AIS receivers may report packets in two di\ufb00erent protocols on the same wire.)\nEric Raymond\n103\n", "page": 117, "type": "text", "section": "Page 117"}
{"text": "Finally, the multiplexer is the part of the daemon that handles client sessions and device assign-\nment. It is responsible for passing reports up to clients, accepting client commands, and responding\nto hotplug noti\ufb01cations. It is essentially all contained in one source \ufb01le, gpsd.c, and never talks to\nthe device drivers directly.\nThe \ufb01rst three components (other than the multiplexer) are linked together in a library called\nlibgpsd and can be used separately from the multiplexer. Our other tools that talk to sensors directly,\nsuch as gpsmon and gpsctl, do it by calling into the core library and driver layer directly.\nThe most complex single component is the packet sni\ufb00er at about two thousand lines of code.\nThis is irreducible; a state machine that can recognize as many di\ufb00erent protocols as it does is bound\nto be large and gnarly. Fortunately, the packet sni\ufb00er is also easy to isolate and test; problems in it\ndo not tend to be coupled to other parts of the code.\nThe multiplexer layer is about same size, but somewhat less gnarly. The device drivers make up\nthe bulk of the daemon code at around 15 KLOC. All the rest of the code\u2014all the support tools and\nlibraries and test clients together\u2014adds up to about the size of the daemon (some code, notably the\nJSON parser, is shared between the daemon and the client libraries).\nThe success of this layering approach is demonstrated in a couple of di\ufb00erent ways. One is that\nnew device drivers are so easy to write that several have been contributed by people not on the core\nteam: the driver API is documented, and the individual drivers are coupled to the core library only\nvia pointers in a master device types table.\nAnother bene\ufb01t is that system integrators can drastically reduce GPSD\u2019s footprint for embedded\ndeployment simply by electing not to compile in unused drivers. The daemon is not large to begin\nwith, and a suitably stripped-down build runs quite happily on low-power, low-speed, small-memory\nARM devices1.\nA third bene\ufb01t of the layering is that the daemon multiplexer can be detached from atop the core\nlibrary and replaced with simpler logic, such as the straight batch conversion of sensor log\ufb01les to\nJSON reports that the gpsdecode utility does.\nThere is nothing novel about this part of the GPSD architecture. Its lesson is that conscious and\nrigorous application of the design pattern of Unix device handling is bene\ufb01cial not just in OS kernels\nbut also in userspace programs that are similarly required to deal with varied hardware and protocols.\n7.4\nThe Data\ufb02ow View\nNow we\u2019ll consider GPSD\u2019s architecture from a data\ufb02ow view. In normal operation, gpsd spins in a\nloop waiting for input from one of these sources:\n1. A set of clients making requests over a TCP/IP port.\n2. A set of navigation sensors connected via serial or USB devices.\n3. The special control socket used by hotplug scripts and some con\ufb01guration tools.\n4. A set of servers issuing periodic di\ufb00erential-GPS correction updates (DGPS and NTRIP).\nThese are handled as though they are navigation sensors.\nWhen a USB port goes active with a device that might be a navigation sensor, a hotplug script\n(shipped with GPSD) sends a noti\ufb01cation to the control socket. This is the cue for the multiplexer\nlayer to put the device on its internal list of sensors. Conversely, a device-removal event can remove\na device from that list.\n1ARM is a 32-bit RISC instruction set architecture used in mobile and embedded electronics. See http://en.wikipedia.\norg/wiki/ARM_architecture.\n104\nGPSD\n", "page": 118, "type": "text", "section": "Page 118"}
{"text": "When a client issues a watch request, the multiplexer layer opens the navigation sensors in its list\nand begins accepting data from them (by adding their \ufb01le descriptors to the set in the main select\ncall). Otherwise all GPS devices are closed (but remain in the list) and the daemon is quiescent.\nDevices that stop sending data get timed out of the device list.\nsensor\nPacket \nsniffer\nCore \nlibrary\nDriver\nExporter \nSniffer\nclient \nsession\nFigure 7.2: Data\ufb02ow\nWhen data comes in from a navigation sensor, it\u2019s fed to the packet sni\ufb00er, a \ufb01nite-state machine\nthat works like the lexical analyzer in a compiler. The packet sni\ufb00er\u2019s job is to accumulate data from\neach port (separately), recognizing when it has accumulated a packet of a known type.\nA packet may contain a position \ufb01x from a GPS, a marine AIS datagram, a sensor reading from a\nmagnetic compass, a DGPS (Di\ufb00erential GPS) broadcast packet, or any of several other things. The\npacket sni\ufb00er doesn\u2019t care about the content of the packet; all it does is tell the core library when it\nhas accumulated one and pass back the payload and the packet type.\nThe core library then hands the packet to the driver associated with its type. The driver\u2019s job is\nto mine data out of the packet payload into a per-device session structure and set some status bits\ntelling the multiplexer layer what kind data it got.\nOne of those bits is an indication that the daemon has accumulated enough data to ship a report\nto its clients. When this bit is raised after a data read from a sensor device, it means we\u2019ve seen the\nend of a packet, the end of a packet group (which may be one or more packets), and the data in the\ndevice\u2019s session structure should be passed to one of the exporters.\nThe main exporter is the \u201csocket\u201d one; it generates a report object in JSON and ships it to\nall the clients watching the device. There\u2019s a shared-memory exporter that copies the data to a\nshared-memory segment instead. In either of these cases, it is expected that a client library will\nunmarshal the data into a structure in the client program\u2019s memory space. A third exporter, which\nships position updates via DBUS, is also available.\nThe GPSD code is as carefully partitioned horizontally as it vertically. The packet sni\ufb00er neither\nknows nor needs to know anything about packet payloads, and doesn\u2019t care whether its input source\nis a USB port, an RS232 device, a Bluetooth radio link, a pseudo-tty, a TCP socket connection, or\na UDP packet stream. The drivers know how to analyze packet payloads, but know nothing about\neither the packet-sni\ufb00er internals nor the exporters. The exporters look only at the session data\nstructure updated by the drivers.\nThis separation of function has served GPSD very well. For example, when we got a request\nin early 2010 to adapt the code to accept sensor data coming in as UDP packets for the on-board\nnavigation system of a robot submarine, it was easy to implement that in a handful of lines of code\nwithout disturbing later stages in the data pipeline.\nMore generally, careful layering and modularization has made it relatively easy to add new sensor\ntypes. We incorporate new drivers every six months or so; some have been written by people who\nare not core developers.\nEric Raymond\n105\n", "page": 119, "type": "text", "section": "Page 119"}
{"text": "7.5\nDefending the Architecture\nAs an open source program like gpsd evolves, one of the recurring themes is that each contributor\nwill do things to solve his or her particular problem case which gradually leak more information\nbetween layers or stages that were originally designed with clean separation.\nOne that we\u2019re concerned about at the time of writing is that some information about input source\ntype (USB, RS232, pty, Bluetooth, TCP, UDP) seems to need to be passed up to the multiplexer\nlayer, to tell it, for example, whether probe strings should be sent to an unidenti\ufb01ed device. Such\nprobes are sometimes required to wake up RS232C sensors, but there are good reasons not to ship\nthem to any more devices than we have to. Many GPSs and other sensor devices are designed on low\nbudgets and in a hurry; some can be confused to the point of catatonia by unexpected control strings.\nFor a similar reason, the daemon has a -b option that prevents it from attempting baud-rate\nchanges during the packet-sni\ufb00er hunt loop. Some poorly made Bluetooth devices handle these so\npoorly that they have to be power-cycled to function again; in one extreme case a user actually had to\nunsolder the backup battery to unwedge his!\nBoth these cases are necessary exceptions to the project\u2019s design rules. Much more usually,\nthough, such exceptions are a bad thing. For example, we\u2019ve had some patches contributed to make\nPPS time service work better that messed up the vertical layering, making it impossible for PPS to\nwork properly with more than the one driver they were intended to help. We rejected these in favor\nof working harder at device-type-independent improvement.\nOn one occasion some years ago, we had a request to support a GPS with the odd property that\nthe checksums in its NMEA packets may be invalid when the device doesn\u2019t have a location \ufb01x.\nTo support this device, we would have had to either (a) give up on validating the checksum on any\nincoming data that looked like an NMEA packet, risking that the packet-sni\ufb00er would hand garbage\nto the NMEA driver, or (b) add a command-line option to force the sensor type.\nThe project lead (the author of this chapter) refused to do either. Giving up on NMEA packet\nvalidation was an obvious bad idea. But a switch to force the sensor type would have been an\ninvitation to get lazy about proper autocon\ufb01guration, which would cause problems all the way up to\nGPSD\u2019s client applications and their users. The next step down that road paved with good intentions\nwould surely have been a baud-rate switch. Instead, we declined to support this broken device.\nOne of the most important duties of a project\u2019s lead architect is to defend the architecture\nagainst expedient \u201c\ufb01xes\u201d that would break it and cause functional problems or severe maintenance\nheadaches down the road. Arguments over this can get quite heated, especially when defending\narchitecture con\ufb02icts against something that a developer or user considers a must-have feature. But\nthese arguments are necessary, because the easiest choice is often the wrong one for the longer term.\n7.6\nZero Con\ufb01guration, Zero Hassles\nAn extremely important feature of gpsd is that it is a zero-con\ufb01guration service2. It has no dot\ufb01le!\nThe daemon deduces the sensor types it\u2019s talking to by sni\ufb03ng the incoming data. For RS232 and\nUSB devices gpsd even autobauds (that is, automatically detects the serial line speed), so it is not\nnecessary for the daemon to know in advance the speed/parity/stopbits at which the sensor is shipping\ninformation.\nWhen the host operating system has a hotplug capability, hotplug scripts can ship device-activation\nand deactivation messages to a control socket to notify the daemon of the change in its environment.\n2With one minor exception for Bluetooth devices with broken \ufb01rmware.\n106\nGPSD\n", "page": 120, "type": "text", "section": "Page 120"}
{"text": "The GPSD distribution supplies these scripts for Linux. The result is that end users can plug a\nUSB GPS into their laptop and expect it to immediately begin supplying reports that location-aware\napplications can read\u2014no muss, no fuss, and no editing a dot\ufb01le or preferences registry.\nThe bene\ufb01ts of this ripple all the way up the application stack. Among other things, it means that\nlocation-aware applications don\u2019t have to have a con\ufb01guration panel dedicated to tweaking the GPS\nand port settings until the whole mess works. This saves a lot of e\ufb00ort for application writers as well\nas users: they get to treat location as a service that is nearly as simple as the system clock.\nOne consequence of the zero-con\ufb01guration philosophy is that we do not look favorably on\nproposals to add a con\ufb01g \ufb01le or additional command-line options. The trouble with this is that\ncon\ufb01guration which can be edited, must be edited. This implies adding setup hassle for end users,\nwhich is precisely what a well-designed service daemon should avoid.\nThe GPSD developers are Unix hackers working from deep inside the Unix tradition, in which\ncon\ufb01gurability and having lots of knobs is close to being a religion. Nevertheless, we think open\nsource projects could be trying a lot harder to throw away their dot\ufb01les and autocon\ufb01gure to what\nthe running environment is actually doing.\n7.7\nEmbedded Constraints Considered Helpful\nDesigning for embedded deployment has been a major goal of GPSD since 2005. This was originally\nbecause we got a lot of interest from system integrators working with single-board computers, but\nit has since paid o\ufb00in an unexpected way: deployment on GPS-enabled smartphones. (Our very\nfavorite embedded-deployment reports are still the ones from the robot submarines, though.)\nDesigning for embedded deployment has in\ufb02uenced GPSD in important ways. We think a lot\nabout ways to keep memory footprint and CPU usage low so the code will run well on low-speed,\nsmall-memory, power-constrained systems.\nOne important attack on this issue, as previously mentioned, is to ensure that gpsd builds don\u2019t\nhave to carry any deadweight over the speci\ufb01c set of sensor protocols that a system integrator needs\nto support. In June 2011 a minimum static build of gpsd on an x86 system has a memory footprint\nof about 69K (that is with all required standard C libraries linked in) on 64-bit x86. For comparison,\nthe static build with all drivers is about 418K.\nAnother is that we pro\ufb01le for CPU hotspots with a slightly di\ufb00erent emphasis than most projects.\nBecause location sensors tend to report only small amounts of data at intervals on the order of 1\nsecond, performance in the normal sense isn\u2019t a GPSD issue\u2014even grossly ine\ufb03cient code would\nbe unlikely to introduce enough latency to be visible at the application level. Instead, our focus is\non decreasing processor usage and power consumption. We\u2019ve been quite successful at this: even\non low-power ARM systems without an FPU, gpsd\u2019s fraction of CPU is down around the level of\npro\ufb01ler noise.\nWhile designing the core code for low footprint and good power e\ufb03ciency is at this point largely\na solved problem, there is one respect in which targeting embedded deployments still produces\ntension in the GPSD architecture: use of scripting languages. On the one hand, we want to minimize\ndefects due to low-level resource management by moving as much code as possible out of C. On the\nother hand, Python (our preferred scripting language) is simply too heavyweight and slow for most\nembedded deployments.\nWe\u2019ve split the di\ufb00erence in the obvious way: the gpsd service daemon is C, while the test\nframework and several of the support utilities are written in Python. Over time, we hope to migrate\nEric Raymond\n107\n", "page": 121, "type": "text", "section": "Page 121"}
{"text": "more of the auxiliary code out of C and into Python, but embedded deployment makes those choices\na continuing source of controversy and discomfort.\nStill, on the whole we \ufb01nd the pressures from embedded deployment quite bracing. It feels good\nto write code that is lean, tight, and sparing of processor resources. It has been said that art comes\nfrom creativity under constraints; to the extent that\u2019s true, GPSD is better art for the pressure.\nThat feeling doesn\u2019t translate directly into advice for other projects, but something else de\ufb01nitely\ndoes: don\u2019t guess, measure! There is nothing like regular pro\ufb01ling and footprint measurements to\nwarn you when you\u2019re straying into committing bloat\u2014and to reassure you that you\u2019re not.\n7.8\nJSON and the Architecturenauts\nOne of the most signi\ufb01cant transitions in the history of the project was when we switched over from\nthe original reporting protocol to using JSON as a metaprotocol and passing reports up to clients as\nJSON objects. The original protocol had used one-letter keys for commands and responses, and we\nliterally ran out of keyspace as the daemon\u2019s capabilities gradually increased.\nSwitching to JSON was a big, big win. JSON combines the traditional Unix virtues of a purely\ntextual format\u2014easy to examine with a Mark 1 Eyeball, easy to edit with standard tools, easy to\ngenerate programmatically\u2014with the ability to pass structured information in rich and \ufb02exible ways.\nBy mapping report types to JSON objects, we ensured that any report could contain mixes of\nstring, numeric, and Boolean data with structure (a capability the old protocol lacked). By identifying\nreport types with a \"class\" attribute, we guaranteed that we would always be able to add new report\ntypes without stepping on old ones.\nThis decision was not without cost. A JSON parser is a bit more computationally expensive than\nthe very simple and limited parser it replaced, and certainly requires more lines of code (implying\nmore places for defects to occur). Also, conventional JSON parsers require dynamic storage allocation\nin order to cope with the variable-length arrays and dictionaries that JSON describes, and dynamic\nstorage allocation is a notorious defect attractor.\nWe coped with these problems in several ways. The \ufb01rst step was to write a C parser for a\n(su\ufb03ciently) large subset of JSON that uses entirely static storage. This required accepting some\nminor restrictions; for example, objects in our dialect cannot contain the JSON null value, and\narrays always have a \ufb01xed maximum length. Accepting these restrictions allowed us to \ufb01t the parser\ninto 600 lines of C.\nWe then built a comprehensive set of unit tests for the parser in order to verify error-free operation.\nFinally, for very tight embedded deployments where the overhead of JSON might be too high, we\nwrote a shared-memory exporter that bypasses the need to ship and parse JSON entirely if the daemon\nand its client have access to common memory.\nJSON isn\u2019t just for web applications anymore. We think anyone designing an application protocol\nshould consider an approach like GPSD\u2019s. Of course the idea of building your protocol on top of a\nstandard metaprotocol is not new; XML fans have been pushing it for many years, and that makes\nsense for protocols with a document-like structure. JSON has the advantages of being lower-overhead\nthan XML and better \ufb01tted to passing around array and record structures.\n7.9\nDesigning for Zero Defects\nBecause of its use in navigational systems, any software that lives between the user and a GPS or\nother location sensor is potentially life-critical, especially at sea or when airborne. Open source\n108\nGPSD\n", "page": 122, "type": "text", "section": "Page 122"}
{"text": "navigation software has a tendency to try to evade this problem by shipping with disclaimers that\nsay, \u201cDon\u2019t rely on this if doing so might put lives at risk.\u201d\nWe think such disclaimers are futile and dangerous: futile because system integrators are quite\nlikely to treat them as pro-forma and ignore them, and dangerous because they encourage developers\nto fool themselves that code defects won\u2019t have serious consequences, and that cutting corners in\nquality assurance is acceptable.\nThe GPSD project developers believe that the only acceptable policy is to design for zero defects.\nSoftware complexity being what it is, we have not quite achieved this\u2014but for a project GPSD\u2019s size\nand age and complexity we come very close.\nOur strategy for doing this is a combination of architecture and coding policies that aim to exclude\nthe possibility of defects in shipped code.\nOne important policy is this: the gpsd daemon never uses dynamic storage allocation\u2014no malloc\nor calloc, and no calls to any functions or libraries that require it. At a stroke this banishes the\nsingle most notorious defect attractor in C coding. We have no memory leaks and no double-malloc\nor double-free bugs, and we never will.\nWe get away with this because all of the sensors we handle emit packets with relatively small\n\ufb01xed maximum lengths, and the daemon\u2019s job is to digest them and ship them to clients with minimal\nbu\ufb00ering. Still, banishing malloc requires coding discipline and some design compromises, a few\nof which we previously noted in discussing the JSON parser. We pay these costs willingly to reduce\nour defect rate.\nA useful side e\ufb00ect of this policy is that it increases the e\ufb00ectiveness of static code checkers such\nas splint, cppcheck, and Coverity. This feeds into another major policy choice; we make extremely\nheavy use of both these code-auditing tools and a custom framework for regression testing. (We\ndo not know of any program suite larger than GPSD that is fully splint-annotated, and strongly\nsuspect that none such yet exist.)\nThe highly modular architecture of GPSD aids us here as well. The module boundaries serve as\ncut points where we can rig test harnesses, and we have very systematically done so. Our normal\nregression test checks everything from the \ufb02oating-point behavior of the host hardware up through\nJSON parsing to correct reporting behavior on over seventy di\ufb00erent sensor logs.\nAdmittedly, we have a slightly easier time being rigorous than many applications would because\nthe daemon has no user-facing interfaces; the environment around it is just a bunch of serial data\nstreams and is relatively easy to simulate. Still, as with banishing malloc, actually exploiting that\nadvantage requires the right attitude, which very speci\ufb01cally means being willing to spend as much\ndesign and coding time on test tools and harnesses as we do on the production code. This is a policy\nwe think other open-source projects can and should emulate.\nAs I write (July 2011), GPSD\u2019s project bug tracker is empty. It has been empty for weeks, and\nbased on past rates of bug submissions we can expect it to stay that way for a good many more. We\nhaven\u2019t shipped code with a crash bug in six years. When we do have bugs, they tend to be the sort of\nminor missing feature or mismatch with speci\ufb01cation that is readily \ufb01xed in a few minutes of work.\nThis is not to say that the project has been an uninterrupted idyll. Next, we\u2019ll review some of our\nmistakes...\n7.10\nLessons Learned\nSoftware design is di\ufb03cult; mistakes and blind alleys are all too normal a part of it, and GPSD has\nbeen no exception to that rule. The largest mistake in this project\u2019s history was the design of the\nEric Raymond\n109\n", "page": 123, "type": "text", "section": "Page 123"}
{"text": "original pre-JSON protocol for requesting and reporting GPS information. Recovering from it took\nyears of e\ufb00ort, and there are lessons in both the original mis-design and the recovery.\nThere were two serious problems with the original protocol:\n1. Poor extensibility. It used requests and response tags consisting of a single letter each, case-\ninsensitive. Thus, for example, the request to report longitude and latitude was \"P\" and a\nresponse looked like \"P -75.32 40.05\". Furthermore, the parser interpreted a request like\n\"PA\" as a \"P\" request followed by an \"A\" (altitude) request. As the daemon\u2019s capabilities\ngradually broadened, we literally ran out of command space.\n2. A mismatch between the protocol\u2019s implicit model of sensor behavior and how they actually\nbehave. The old protocol was request/response: send a request for position (or altitude, or\nwhatever) get back a report sometime later. In reality, it is usually not possible to request a\nreport from a GPS or other navigation-related sensors; they stream out reports, and the best\na request can do is query a cache. This mismatch encouraged sloppy data-handling from\napplications; too often, they would ask for location data without also requesting a timestamp\nor any check information about the \ufb01x quality, a practice which could easily result in stale or\ninvalid data getting presented to the user.\nIt became clear as early as 2006 that the old protocol design was inadequate, but it took nearly\nthree years of design sketches and false starts to design a new one. The transition took two years\nafter that, and caused some pain for developers of client applications. It would have cost a lot more\nif the project had not shipped client-side libraries that insulated users from most of the protocol\ndetails\u2014but we didn\u2019t get the API of those libraries quite right either at \ufb01rst.\nIf we had known then what we know now, the JSON-based protocol would have been introduced\n\ufb01ve years sooner, and the API design of the client libraries would have required many fewer revisions.\nBut there are some kinds of lessons only experience and experiment can teach.\nThere are at least two design guidelines that future service daemons could bear in mind to avoid\nreplicating our mistakes:\n1. Design for extensibility. If your daemon\u2019s application protocol can run out of namespace\nthe way our old one did, you\u2019re doing it wrong. Overestimating the short-term costs and\nunderestimating the long-term bene\ufb01ts of metaprotocols like XML and JSON is an error\nthat\u2019s all too common.\n2. Client-side libraries are a better idea than exposing the application protocol details. A library\nmay be able to adapt its internals to multiple versions of the application protocol, substantially\nreducing both interface complexity and defect rates compared to the alternative, in which\neach application writer needs to develop an ad hoc binding. This di\ufb00erence will translate\ndirectly into fewer bug reports on your project\u2019s tracker.\nOne possible reply to our emphasis on extensibility, not just in GPSD\u2019s application protocol\nbut in other aspects of the project architecture like the packet-driver interface, is to dismiss it as an\nover-elaboration brought about by mission creep. Unix programmers schooled in the tradition of\n\u201cdo one thing well\u201d may ask whether gpsd\u2019s command set really needs to be larger in 2011 than it\nwas in 2006, why gpsd now handles non-GPS sensors like magnetic compasses and Marine AIS\nreceivers, and why we contemplate possibilities like ADS-B aircraft tracking.\nThese are fair questions. We can approach an answer by looking at the actual complexity cost\nof adding a new device type. For very good reasons, including relatively low data volumes and the\nhigh electrical-noise levels historically associated with serial wires to sensors, almost all reporting\nprotocols for GPSs and other navigation-related sensors look broadly similar: small packets with\na validation checksum of some sort. Such protocols are \ufb01ddly to handle but not really di\ufb03cult to\n110\nGPSD\n", "page": 124, "type": "text", "section": "Page 124"}
{"text": "distinguish from each other and parse, and the incremental cost of adding a new one tends to be\nless than a KLOC each. Even the most complex of our supported protocols with their own report\ngenerators attached, such as Marine AIS, only cost on the order of 3 KLOC each. In aggregate, the\ndrivers plus the packet-sni\ufb00er and their associated JSON report generators are about 18 KLOC total.\nComparing this with 43 KLOC for the project as a whole, we see that most of the complexity\ncost of GPSD is actually in the framework code around the drivers\u2014and (importantly) in the test\ntools and framework for verifying the daemon\u2019s correctness. Duplicating these would be a much\nlarger project than writing any individual packet parser. So writing a GPSD-equivalent for a packet\nprotocol that GPSD doesn\u2019t handle would be a great deal more work than adding another driver\nand test set to GPSD itself. Conversely, the most economical outcome (and the one with the lowest\nexpected cumulative rate of defects) is for GPSD to grow packet drivers for many di\ufb00erent sensor\ntypes.\nThe \u201cone thing\u201d that GPSD has evolved to do well is handle any collection of sensors that ship\ndistinguishable checksummed packets. What looks like mission creep is actually preventing many\ndi\ufb00erent and duplicative handler daemons from having to be written. Instead, application developers\nget one relatively simple API and the bene\ufb01t of our hard-won expertise at design and testing across\nan increasing range of sensor types.\nWhat distinguishes GPSD from a mere mission-creepy pile of features is not luck or black magic\nbut careful application of known best practices in software engineering. The payo\ufb00from these begins\nwith a low defect rate in the present, and continues with the ability to support new features with little\ne\ufb00ort or expected impact on defect rates in the future.\nPerhaps the most important lesson we have for other open-source projects is this: reducing\ndefect rates asymptotically close to zero is di\ufb03cult, but it\u2019s not impossible\u2014not even for a project as\nwidely and variously deployed as GPSD is. Sound architecture, good coding practice, and a really\ndetermined focus on testing can achieve it\u2014and the most important prerequisite is the discipline to\npursue all three.\n111\n", "page": 125, "type": "text", "section": "Page 125"}
{"text": "112\nGPSD\n", "page": 126, "type": "text", "section": "Page 126"}
{"text": "[chapter8]\nThe Dynamic Language Runtime and the\nIron Languages\nJe\ufb00Hardy\nThe Iron languages are an informal group of language implementations with \u201cIron\u201d in their names, in\nhonour of the \ufb01rst one, IronPython. All of these languages have at least one thing in common\u2014they\nare dynamic languages that target the Common Language Runtime (CLR), which is more commonly\nknown as the .NET Framework1, and they are built on top of the Dynamic Language Runtime (DLR).\nThe DLR is a set of libraries for the CLR that provide much better support for dynamic languages on\nthe CLR. IronPython and IronRuby are both used in a few dozen closed and open source projects,\nand are both under active development; the DLR, which started as an open-source project, is included\nas part of the .NET Framework and Mono.\nArchitecturally, IronPython, IronRuby, and the DLR are both simple and devilishly complex.\nFrom a high level, the designs are similar to many other language implementations, with parsers and\ncompilers and code generators; however, look a little closer and the interesting details begin to emerge:\ncall sites, binders, adaptive compilation, and other techniques are used to make dynamic languages\nperform nearly as fast as static languages on a platform that was designed for static languages.\n8.1\nHistory\nThe history of the Iron languages begins in 2003. Jim Hugunin had already written an implementation\nof Python, called Jython, for the Java Virtual Machine (JVM). At the time, the then-new .NET\nFramework Common Language Runtime (CLR) was considered by some (exactly who, I\u2019m not\nsure) to be poorly suited for implementing dynamic languages such as Python. Having already\nimplemented Python on the JVM, Jim was curious as to how Microsoft could have made .NET so\nmuch worse than Java. In a September 2006 blog post2, he wrote:\nI wanted to understand how Microsoft could have screwed up so badly that the CLR\nwas a worse platform for dynamic languages than the JVM. My plan was to take a couple\nof weeks to build a prototype implementation of Python on the CLR and then to use\nthat work to write a short pithy article called, \u201cWhy the CLR is a terrible platform for\ndynamic languages\u201d. My plans quickly changed as I worked on the prototype, because I\n1\u201cCLR\u201d is the generic term; the .NET Framework is Microsoft\u2019s implementation, and there is also the open-source Mono\nimplementation.\n2http://blogs.msdn.com/b/hugunin/archive/2006/09/05/741605.aspx\n", "page": 127, "type": "text", "section": "Page 127"}
{"text": "found that Python could run extremely well on the CLR\u2014in many cases noticeably faster\nthan the C-based implementation. For the standard pystone3 benchmark, IronPython on\nthe CLR was about 1.7x faster than the C-based implementation.\n(The \u201cIron\u201d part of the name was a play on the name of Jim\u2019s company at the time, Want of a Nail\nSoftware.)\nShortly afterwards, Jim was hired by Microsoft to make .NET an even better platform for dynamic\nlanguages. Jim (and several others) developed the DLR by factoring the language-neutral parts out of\nthe original IronPython code. The DLR was designed to provide a common core for implementing\ndynamic languages for .NET, and was a major new feature of .NET 4.\nAt the same time as the DLR was announced (April 2007), Microsoft also announced that, in\naddition to a new version of IronPython built on top of the DLR (IronPython 2.0), they would be\ndeveloping IronRuby on top of the DLR to demonstrate the DLR\u2019s adaptability to multiple languages4.\nIntegration with dynamic languages using the DLR would also be a major part of C# and Visual\nBasic, with a new keyword (dynamic) that allowed those languages to easily call into any language\nimplemented on the DLR, or any other dynamic data source. The CLR was already a good platform\nfor implementing static languages, and the DLR makes dynamic languages a \ufb01rst-class citizen.\nOther language implementations from outside of Microsoft also use the DLR, including Iron-\nScheme5 and IronJS6. In addition, Microsoft\u2019s PowerShell v3 will use the DLR instead of its own\ndynamic object system.\n8.2\nDynamic Language Runtime Principles\nThe CLR is designed with statically-typed languages in mind; the knowledge of types is baked very\ndeeply into the runtime, and one of its key assumptions is that those types do not change\u2014that a\nvariable never changes its type, or that a type never has any \ufb01elds or members added or removed\nwhile the program is running. This is \ufb01ne for languages like C# or Java, but dynamic languages,\nby de\ufb01nition, do not follow those rules. The CLR also provides a common object system for static\ntypes, which means that any .NET language can call objects written in any other .NET language\nwith no extra e\ufb00ort.\nWithout the DLR, every dynamic language would have to provide its own object model; the\nvarious dynamic languages would not be able to call objects in another dynamic language, and C#\nwould not be able to treat IronPython and IronRuby equally. Thus, the heart of the DLR is a standard\nway of implementing dynamic objects while still allowing an object\u2019s behaviour to be customized for\na particular language by using binders. It also includes a mechanism known as call-site caching for\nensuring that dynamic operations are as fast as possible, and a set of classes for building expression\ntrees, which allow code to be stored as data and easily manipulated.\nThe CLR also provides several other features that are useful to dynamic languages, including a\nsophisticated garbage collector; a Just-in-Time (JIT) compiler that converts Common Intermediate\nLanguage (IL) bytecode, which is what .NET compilers output, into machine code at runtime; a\nruntime introspection system, which allows dynamic languages to call objects written in any static\nlanguage; and \ufb01nally, dynamic methods (also known as lightweight code generation) that allow code\n3http://ironpython.codeplex.com/wikipage?title=IP26RC1VsCPy26Perf\n4In October of 2010, Microsoft stopped developing IronPython and IronRuby and they became independent open-source\nprojects.\n5http://ironscheme.codeplex.com/\n6https://github.com/fholm/IronJS/\n114\nThe Dynamic Language Runtime and the Iron Languages\n", "page": 128, "type": "text", "section": "Page 128"}
{"text": "to be generated at runtime and then executed with only sightly more overhead than a static method\ncall7.\nThe result of the DLR design is that languages like IronPython and IronRuby can call each\nother\u2019s objects (and those of any other DLR language), because they have a common dynamic object\nmodel. Support for this object model was also added to C# 4 (with the dynamic keyword) and\nVisual Basic 10 (in addition to VB\u2019s existing method of \u201clate binding\u201d) so that they can perform\ndynamic calls on objects as well. The DLR thus makes dynamic languages \ufb01rst-class citizens on\n.NET.\nInterestingly, the DLR is entirely implemented as a set of libraries and can be built and run on\n.NET 2.0 as well. No changes to the CLR are required to implement it.\n8.3\nLanguage Implementation Details\nEvery language implementation has two basic stages\u2014parsing (the front end) and code generation\n(the backend). In the DLR, each language implements its own front end, which contains the language\nparser and syntax tree generator; the DLR provides a common backend that takes expression trees\nto produce Intermediate Language (IL) for the CLR to consume; the CLR will pass the IL to a\nJust-In-Time (JIT) compiler, which produces machine code to run on the processor. Code that is\nde\ufb01ned at runtime (and run using eval) is handled similarly, except that everything happens at the\neval call site instead of when the \ufb01le is loaded.\nThere are a few di\ufb00erent way to implement the key pieces of a language front end, and while\nIronPython and IronRuby are very similar (they were developed side-by-side, after all) they di\ufb00er\nin a few key areas. Both IronPython and IronRuby have fairly standard parser designs\u2014both use\na tokenizer (also known as a lexer) to split the text into tokens, and then the parser turns those\ntokens into an abstract syntax tree (AST) that represents the program. However, the languages have\ncompletely di\ufb00erent implementations of these pieces.\n8.4\nParsing\nIronPython\u2019s tokenizer is in the IronPython.Compiler.Tokenizer class and the parser is in the\nIronPython.Compiler.Parser class. The tokenizer is a hand-written state machine that recognizes\nPython keywords, operators, and names and produces the corresponding tokens. Each token also\ncarries with it any additional information (such as the value of a constant or name), as well as where\nin the source the token was found, to aid in debugging. The parser then takes this set of tokens and\ncompares them to the Python grammar to see if it matches legal Python constructs.\nIronPython\u2019s parser is an LL(1) recursive descent parser. The parser will look at the incoming\ntoken, call a function if the token is allowed and return an error if it is not. A recursive descent\nparser is built from a set of mutually recursive functions; these functions ultimately implement a\nstate machine, with each new token triggering a state transition. Like the tokenizer, IronPython\u2019s\nparser is written by hand.\nIronRuby, on the other hand, has a tokenizer and parser generated by the Gardens Point Parser\nGenerator (GPPG). The parser is is described in the Parser.y \ufb01le8, which is a yacc-format \ufb01le that\ndescribes the grammar of IronRuby at a high level using rules that describe the grammar. GPPG then\n7The JVM acquired a similar mechanism with invokedynamic in Java 7.\n8Languages/Ruby/Ruby/Compiler/Parser/Parser.y\nJe\ufb00Hardy\n115\n", "page": 129, "type": "text", "section": "Page 129"}
{"text": "takes Parser.y and creates the actual parser functions and tables; the result is a table-based LALR(1)\nparser. The generated tables are long arrays of integers, where each integer represents a state; based\non the current state and the current token, the tables determine which state should be transitioned to\nnext. While IronPython\u2019s recursive descent parser is quite easy to read, IronRuby\u2019s generated parser\nis not. The transition table is enormous (540 distinct states and over 45,000 transitions) and it is next\nto impossible to modify it by hand.\nUltimately, this is an engineering tradeo\ufb00\u2014IronPython\u2019s parser is simple enough to modify by\nhand, but complex enough that it obscures the structure of the language. The IronRuby parser, on the\nother hand, makes it much easier to understand the structure of the language in the Parser.y \ufb01le,\nbut it is now dependent on a third-party tool that uses a custom (albeit well-known) domain-speci\ufb01c\nlanguage and may have its own bugs or quirks. In this case, the IronPython team didn\u2019t want to\ncommit to a dependency on an external tool, while the IronRuby team didn\u2019t mind.\nWhat is clear, however, is how important state machines are to parsing, at every phase. For any\nparsing task, no matter how simple, a state machine is always the right answer.\nThe output of the parser for either language is an abstract syntax tree (AST). This describes the\nstructure of the program at a high level, with each node mapping directly to a language construct\u2014a\nstatement or expression. These trees can be manipulated at runtime, often to make optimizations to\nthe program before compilation. However, a language\u2019s AST is tied to the language; the DLR needs\nto operate on trees that do not contain any language-speci\ufb01c constructs, only general ones.\n8.5\nExpression Trees\nAn expression tree is also a representation of a program that can be manipulated at\nruntime, but in a lower-level, language-independent form. In .NET, the node types are in the\nSystem.Linq.Expressions namespace9, and all of the node types are derived from the abstract\nExpression class. These expression trees cover more than just expressions, however, as there are\nnode types for if statements, try blocks, and loops as well; in some languages (Ruby, for one) these\nare expressions and not statements.\nThere are nodes to cover almost every feature a programming language could want. However,\nthey tend to be de\ufb01ned at a fairly low level\u2014instead of having ForExpression, WhileExpression,\netc., there is a single LoopExpression which, when combined with a GotoExpression, can describe\nany type of loop. To describe a language at a higher level, languages can de\ufb01ne their own node\ntypes by deriving from Expression and overriding the Reduce() method, which returns another\nexpression tree. In IronPython, the parse tree is also a DLR expression tree, but it contains many\ncustom nodes that the DLR would not normally understand (such as ForStatement). These custom\nnodes can be reduced to expression trees that the DLR does understand (such as a combination of\nLoopExpressions and GotoExpressions). A custom expression node can reduce to other custom\nexpression nodes, so the reduction proceeds recursively until only the intrinsic DLR nodes remain.\nOne key di\ufb00erence between IronPython and IronRuby is that while IronPython\u2019s AST is also an\nexpression tree, IronRuby\u2019s is not. Instead, IronRuby\u2019s AST is transformed into an expression tree\nbefore moving onto the next stage. It\u2019s arguable whether having the AST also be an expression tree\nis actually useful, so IronRuby did not implement it that way.\nEach node type knows how to reduce itself, and it can usually only be reduced in one way. For\ntransformations that come from code outside the tree\u2014optimizations such as constant folding, for ex-\n9The namespace is a historical artifact; expression trees were originally added in .NET 3.5 to implement LINQ\u2014Language\nIntegrated Query\u2014and the DLR expression trees extended that.\n116\nThe Dynamic Language Runtime and the Iron Languages\n", "page": 130, "type": "text", "section": "Page 130"}
{"text": "ample, or IronPython\u2019s implementation of Python generators\u2014a subclass of the ExpressionVisitor\nclass is used. ExpressionVisitor has a Visit() method that calls the Accept() method on\nExpression, and subclasses of Expression override Accept() to call a speci\ufb01c Visit() method\non ExpressionVisitor, such as VisitBinary(). This is a textbook implementation of the Visitor\npattern from Gamma et al.\u2014there\u2019s a \ufb01xed set of node types to visit, and an in\ufb01nite number of\noperations that could be performed upon them. When the expression visitor visits a node, it usu-\nally recursively visits its children as well, and its children, and so on down the tree. However, an\nExpressionVisitor can\u2019t actually modify the expression tree it is visiting, because expression trees\nare immutable. If the expression visitor needs to modify a node (such as removing children), it must\nproduce a new node that replaces the old one instead, and all of its parents as well.\nOnce an expression tree has been created, reduced, and visited, it ultimately needs to be executed.\nWhile expression trees can be compiled directly to IL code, IronPython and IronRuby pass them to\nan interpreter \ufb01rst, because compiling directly to IL is expensive for code that may only be executed\na handful of times.\n8.6\nInterpreting and Compilation\nOne of the downsides to using a JIT compiler, like .NET does, is that it imposes a time penalty when\nstarting up because it takes time to convert the IL bytecode into machine code that the processor\ncan run. JIT compilation makes the code much faster while running than using an interpreter, but\nthe initial cost can be prohibitive, depending on what is being done. For example, a long-lived\nserver process such as a web application will bene\ufb01t from the JIT because the startup time is mostly\nirrelevant but the per-request time is critical, and it tends to run the same code repeatedly. On the\nother hand, a program that is run often but only for short periods of time, such as the Mercurial\ncommand-line client, would be better o\ufb00with a short startup time because it likely only runs each\nchunk of code once, and the fact that the JIT\u2019d code is faster doesn\u2019t overcome the fact that it takes\nlonger to start running.\n.NET can\u2019t execute IL code directly; it always gets JIT compiled into machine code, and this takes\ntime. In particular, program startup times are one of the weak spots of the .NET Framework because\nmuch of the code needs to be JIT compiled. While there are ways to avoid the JIT penalty in static .NET\nprograms10, they don\u2019t work for dynamic programs. Rather than always compile directly to IL, Iron-\nRuby and IronPython will use their own interpreter (found in Microsoft.Scripting.Interpreter)\nthat isn\u2019t as fast as JIT-compiled code but takes much less time to get started. The interpreter is also\nuseful in situations where dynamic code generation is not allowed, such as on mobile platforms;\notherwise the DLR languages would not be able to run at all.\nBefore execution, the entire expression tree must be wrapped in a function so that it can be\nexecuted. In the DLR, functions are represented as LambdaExpression nodes. While in most\nlanguages a lambda is an anonymous function, the DLR has no concept of names; all functions are\nanonymous. The LambdaExpression is unique in that it is the only node type that can be converted\nto a delegate, which is what .NET calls \ufb01rst-class functions, using its Compile() method. A delegate\nis similar to a C function pointer\u2014it is simply a handle to a piece of code that can be called.\nInitially, the expression tree is wrapped in a LightLambdaExpression, which can also produce\na delegate that can be executed, but rather than generate IL code (which would then invoke the\nJIT), it instead compiles the expression tree to a list of instructions that are then executed on the\ninterpreter\u2019s simple VM. The interpreter is a simple stack-based one; instructions pop values o\ufb00\n10Native Image Generation, or NGEN\u2014http://msdn.microsoft.com/en-us/library/6t9t5wcf.aspx.\nJe\ufb00Hardy\n117\n", "page": 131, "type": "text", "section": "Page 131"}
{"text": "of the stack, perform an operation, and then push the result back on the stack. Each instruction is\nan instance of a class derived from Microsoft.Scripting.Interpreter.Instruction (such as\nAddInstruction or BranchTrueInstruction) that has properties describing how many items it\ntakes o\ufb00of the stack, how many it will put on, and a Run() method that executes the instruction\nby popping and pushing values on the stack and returning the o\ufb00set of the next instruction. The\ninterpreter takes the list of instructions and executes them one by one, jumping forward or backwards\ndepending on the return value of the Run() method.\nOnce a a piece of code has been executed a certain number of times, it will be converted\nto a full LambdaExpression by calling LightLambdaExpression.Reduce(), then compiled to a\nDynamicMethod delegate (on a background thread for a bit of parallelism), and the old delegate\ncall site will be replaced with the newer, faster one. This greatly reduces the cost of executing\nfunctions that may only be called a few times, such as the main function of a program, while making\ncommonly called functions run as fast as possible. By default, the compilation threshold is set at 32\nexecutions, but this can be changed with a command-line option or by the host program, and can\ninclude disabling either compilation or the interpreter entirely.\nWhether running through the interpreter or compiled to IL, the language\u2019s operations are not\nhard-coded by the expression tree compiler. Instead, the compiler generates call sites for each\noperation that may be dynamic (which is nearly all of them). These call sites give the objects a\nchance to implement dynamic behaviour while still keeping performance high.\n8.7\nDynamic Call Sites\nIn a static .NET language, all of the decisions about what code should be called are made at compile\ntime. For example, consider the following line of C#:\nvar z = x + y;\nThe compiler knows what the types of \u2018x\u2019 and \u2018y\u2019 are and whether or not they can be added. The\ncompiler can emit the proper code for handling overloaded operators, type conversions, or whatever\nelse might be needed to make the code run properly, based solely on the static information it knows\nabout the types involved. Now, consider the following line of Python code:\nz = x + y\nThe IronPython compiler has no idea what this might do when it encounters it, because it doesn\u2019t\nknow what the types of x and y are11, and even if it did know, the ability of x and y to be added\ncould change at runtime anyway. Instead of emitting the IL code for adding numbers, the IronPython\nemits a call site that will be resolved at runtime.\nA call site is a placeholder for an operation to be determined at runtime; they are implemented\nas instances of the System.Runtime.CompilerServices.CallSite class. In a dynamic language\nlike Ruby or Python, just about every operation has a dynamic component; these dynamic operations\nare represented in the expression trees as DynamicExpression nodes, which the expression tree\ncompiler knows to convert to a call site. When a call site is created, it is does not yet know how to\nperform the desired operation; however, it is created with an instance of the proper call site binder\nthat is speci\ufb01c to the language in use, and contains all of the necessary information about how to\nperform the operation.\n11In principle it could, but neither IronPython nor IronRuby do type inference.\n118\nThe Dynamic Language Runtime and the Iron Languages\n", "page": 132, "type": "text", "section": "Page 132"}
{"text": "+Target\t\r \u00a0:\t\r \u00a0Delegate\n+Update\t\r \u00a0:\t\r \u00a0Delegate\n-\u00ad\u2010Rules\t\r \u00a0:\t\r \u00a0Delegate[]\n-\u00ad\u2010_binder\t\r \u00a0:\t\r \u00a0CallSiteBinder\nCallSite\n+Bind()\t\r \u00a0:\t\r \u00a0Expression\n+BindDelegate()\t\r \u00a0:\t\r \u00a0Delegate\n-\u00ad\u2010Cache\t\r \u00a0:\t\r \u00a0Delegate[]\nCallSiteBinder\nFigure 8.1: CallSite class diagram\nEach language will have a di\ufb00erent call site binder for each operation, and the binders often\nknow many di\ufb00erent ways to perform an operation depending on the arguments given to the call\nsite. However, generating these rules is expensive (in particular, compiling them to a delegate for\nexecution, which involves invoking the .NET JIT), so the call site has a multi-level call site cache\nthat stores the rules that have already been created for later use.\nThe \ufb01rst level, L0, is the CallSite.Target property on the call site instance itself. This stores\nthe most-recently-used rule for this call site; for a vast number of call sites, this is all that will ever\nbe needed as they are only ever called with one set of argument types. The call site also has another\ncache, L1, that stores a further 10 rules. If Target is not valid for this call (for example, if the\narguments types are di\ufb00erent), the call site \ufb01rst checks its rules cache to see if it has already created\nthe proper delegate from a previous call, and reuses that rule instead of creating a new one.\nStoring rules in the cache is driven by the time it takes to actually compile a new rule compared\nto the time it takes to check the existing rules. Roughly speaking, it takes about 10 ns for .NET to\nexecute a type check on a variable (checking a binary function takes 20 ns, etc.), which is the most\ncommon type of rule predicate. Compiling a simple method to add doubles, on the other hand, takes\nabout 80 \u00b5s, or three orders of magnitude longer. The size of the caches is limited to prevent wasting\nmemory storing every rule that gets used at a call site; for a simple addition, each variation requires\nabout 1 KB of memory. However, pro\ufb01ling showed that very few call sites ever had more than 10\nvariations.\nFinally, there is the L2 cache, which is stored on the binder instance itself. The binder instance\nthat is associated with a call site may store some extra information with it that makes it speci\ufb01c to\na call site, but a large number of call sites aren\u2019t unique in any way and can share the same binder\ninstance. For example, in Python, the basic rules for addition are the same throughout the program;\nit depends on the two types on the either side of the +, and that\u2019s it. All of the addition operations in\nthe program can share the same binder, and if both the L0 and L1 caches miss, the L2 cache contains\na much larger number of recent rules (128) collected from across the entire program. Even if a call\nsite is on its \ufb01rst execution, there\u2019s a good chance it might already \ufb01nd an appropriate rule in the\nL2 cache. To ensure that this works most e\ufb00ectively, IronPython and IronRuby both have a set of\ncanonical binder instances that are used for common operations like addition.\nIf the L2 cache misses, the binder is asked to create an implementation for the call site, taking\ninto account the types (and possibly even the values) of the arguments. In the above example, if x\nand y are doubles (or another native type), then the implementation simply casts them to doubles and\ncalls the IL add instruction. The binder also produces a test that checks the arguments and ensures\nthey are valid for the implementation. Together, the implementation and the test make a rule. In most\ncases, both the implementation and the test are created and stored as expression trees12.\n12The call site infrastructure does not depend on expression trees, however; it can be used with delegates alone.\nJe\ufb00Hardy\n119\n", "page": 133, "type": "text", "section": "Page 133"}
{"text": "CallSite\t\r \u00a0reached\nUpdate()\nIn\t\r \u00a0CallSite\t\r \u00a0rules\t\r \u00a0\n(L1)?\nIn\t\r \u00a0Binder\t\r \u00a0rules\t\r \u00a0\n(L2)?\nNo\nGenerate\t\r \u00a0Rule\nTarget\t\r \u00a0Valid?\nNo\nSet\t\r \u00a0Target\t\r \u00a0to\t\r \u00a0rule\nYes\nYes\nExecute\t\r \u00a0Target\t\r \u00a0rule\nYes\nFigure 8.2: CallSite \ufb02owchart\nIf the expression trees were expressed in C#, the code would be similar to:\nif(x is double && y is double) {\n// check for doubles\nreturn (double)x + (double)y;\n// execute if doubles\n}\nreturn site.Update(site, x, y);\n// not doubles, so find/create another rule\n// for these types\n120\nThe Dynamic Language Runtime and the Iron Languages\n", "page": 134, "type": "text", "section": "Page 134"}
{"text": "The binder then produces a delegate from the expression trees, which means the rule is compiled\nto IL and then to machine code. In the case of adding two numbers, this will likely become a quick\ntype check and then a machine instruction to add the numbers. Even with all of the machinery\ninvolved, the ultimate end result is only marginally slower than static code. IronPython and IronRuby\nalso include a set of precompiled rules for common operations like addition of primitive types, which\nsaves time because they don\u2019t have to be created at runtime, but does cost some extra space on disk.\n8.8\nMeta-Object Protocol\nBesides the language infrastructure, the other key part of the DLR is the ability for a language\n(the host language) to make dynamic calls on objects de\ufb01ned in another language (the source\nlanguage). To make this possible, the DLR must be able to understand what operations are valid\non an object, no matter the language it was written in. Python and Ruby have fairly similar object\nmodels, but JavaScript has a radically di\ufb00erent prototype-based (as opposed class-based) type system.\nInstead of trying to unify the various type systems, the DLR treats them all as if they were based on\nSmalltalk-style message passing.\nIn a message-passing object-oriented system, objects send messages to other objects (with\nparameters, usually), and the object can return another object as a result. Thus, while each language\nhas its own idea of what an object is, they can almost all be made equivalent by viewing method\ncalls as messages that are sent between objects. Of course, even static OO languages \ufb01t this model to\nsome extent; what makes dynamic languages di\ufb00erent is that the method being called does not have\nto be known at compile time, or even exist on the object at all (e.g., Ruby\u2019s method_missing), and\nthe target object usually has a chance to intercept the message and process it di\ufb00erently if necessary\n(e.g., Python\u2019s __getattr__).\nThe DLR de\ufb01nes the following messages:\n\u2022 Get|Set|DeleteMember: operations for manipulating an object\u2019s members\n\u2022 Get|Set|DeleteIndex: operations for indexed objects (such as arrays or dictionaries)\n\u2022 Invoke, InvokeMember: invoke an object or member of an object\n\u2022 CreateInstance: create an instance of an object\n\u2022 Convert: convert an object from one type to another\n\u2022 UnaryOperation, BinaryOperation: perform operator-based operations, such as negate (!)\nor add (+)\nTaken together, these operations should be su\ufb03cient for implementing just about any language\u2019s\nobject model.\nBecause the CLR is inherently statically typed, dynamic language objects must still be rep-\nresented by static classes. The usual technique is to have a static class such as PythonObject\nand have the actual Python objects be instances of this class or its subclasses. For reasons of\ninteroperability and performance, the DLR\u2019s mechanism is a lot more complicated. Instead of\ndealing with language-speci\ufb01c objects the DLR deals with meta-objects, which are subclasses of\nSystem.Dynamic.DynamicMetaObject and have methods for handling all of the above messages.\nEach language has its own subclasses of DynamicMetaObject that implement the language\u2019s object\nmodel, such as IronPython\u2019s MetaPythonObject. The meta classes also have corresponding concrete\nclasses that implement the System.Dynamic.IDynamicMetaObjectProtocol interface, which is\nhow the DLR identi\ufb01es dynamic objects.\nFrom a class that implements IDynamicMetaObjectProtocol, the DLR can get a Dynamic-\nMetaObject by calling GetMetaObject(). This DynamicMetaObject is provided by the language\nJe\ufb00Hardy\n121\n", "page": 135, "type": "text", "section": "Page 135"}
{"text": "+GetMetaObject(in\t\r \u00a0parameter\t\r \u00a0:\t\r \u00a0Expression)\t\r \u00a0:\t\r \u00a0DynamicMetaObject\n\u00abinterface\u00bb\nIDynamicMetaObjectProvider\n+BindInvoke(in\t\r \u00a0binder\t\r \u00a0:\t\r \u00a0InvokeMemberBinder,\t\r \u00a0in\t\r \u00a0args\t\r \u00a0:\t\r \u00a0DynamicMetaObject[])\t\r \u00a0:\t\r \u00a0DynamicMetaObject\n+BindGetMember(in\t\r \u00a0binder\t\r \u00a0:\t\r \u00a0GetMemberBinder,\t\r \u00a0in\t\r \u00a0args\t\r \u00a0:\t\r \u00a0DynamicMetaObject[])\t\r \u00a0:\t\r \u00a0DynamicMetaObject\n-\u00ad\u2010...()\n+Expression\t\r \u00a0:\t\r \u00a0Expression\n+Value\t\r \u00a0:\t\r \u00a0object\n+RunLmeType\t\r \u00a0:\t\r \u00a0Type\n-\u00ad\u2010...\nDynamicMetaObject\n+GetMetaObject(in\t\r \u00a0parameter\t\r \u00a0:\t\r \u00a0Expression)\t\r \u00a0:\t\r \u00a0DynamicMetaObject\nOldInstance\n+BindInvoke(in\t\r \u00a0binder\t\r \u00a0:\t\r \u00a0InvokeMemberBinder,\t\r \u00a0in\t\r \u00a0args\t\r \u00a0:\t\r \u00a0DynamicMetaObject[])\t\r \u00a0:\t\r \u00a0DynamicMetaObject\n+BindGetMember(in\t\r \u00a0binder\t\r \u00a0:\t\r \u00a0GetMemberBinder,\t\r \u00a0in\t\r \u00a0args\t\r \u00a0:\t\r \u00a0DynamicMetaObject[])\t\r \u00a0:\t\r \u00a0DynamicMetaObject\n+...()\nMetaOldInstance\nMetaPythonObject\nFigure 8.3: IDMOP class diagram\nand implements the binding functions as required by that object. Each DynamicMetaObject also\nhas the value and type, if available, of the underlying object. Finally, a DynamicMetaObject stores\nan expression tree representing the call site so far and any restrictions on that expression, similar to\nthe call site binders.\nWhen the DLR is compiling a call to a method on a user-de\ufb01ned class, it \ufb01rst creates a call\nsite (i.e., an instance of the CallSite class). The call site initiates the binding process as de-\nscribed above in \u201cDynamic Call Sites\u201d, which results in it eventually calling GetMetaObject()\non an instance of OldInstance13, which returns a MetaOldInstance. Next, a binder is called\n(PythonGetMemberBinder.Bind()) which in turn calls MetaOldInstance.BindGetMember();\nit returns a new DynamicMetaObject that describes how to look up the method name on\nthe object.\nThen another binder, PythonInvokeBinder.Bind(), is called, which calls\nMetaOldInstance.BindInvoke(), wrapping the \ufb01rst DynamicMetaObject with a new one rep-\nresenting how to call the method that was looked up. This includes the original object, the expression\ntree for looking up the method name, and DynamicMetaObjects representing the arguments to the\nmethod.\nOnce the \ufb01nal DynamicMetaObject in an expression has been built, its expression tree and\n13Python has old-style and new-style classes, but that\u2019s not relevant here.\n122\nThe Dynamic Language Runtime and the Iron Languages\n", "page": 136, "type": "text", "section": "Page 136"}
{"text": "restrictions are used to build a delegate which is then returned to the call site that initiated the binding.\nFrom there the code can be stored in the call site caches, making operations on objects as fast as\nother dynamic calls, and almost as fast as static calls.\nHost languages that want to perform dynamic operations on dynamic languages must derive\ntheir binders from DynamicMetaObjectBinder. The DynamicMetaObjectBinder will \ufb01rst ask the\ntarget object to bind the operation (by calling GetMetaObject() and going through the binding\nprocess described above) before falling back on the host language\u2019s binding semantics. As a result,\nif an IronRuby object is accessed from an IronPython program, the binding is \ufb01rst attempted with\nRuby (target language) semantics; if that fails, the DynamicMetaObjectBinder will fall back on\nthe Python (host language) semantics. If the object being bound is not dynamic (i.e., it does not\nimplement IDynamicMetaObjectProvider), such as classes from the .NET base class library, then\nit is accessed with the host language\u2019s semantics using .NET re\ufb02ection.\nLanguages do have some freedom in how they implement this; IronPython\u2019s PythonInvokeBinder\ndoes not derive from InvokeBinder because it needs to do some extra processing speci\ufb01c to Python\nobjects. As long as it only deals with Python objects, there are no issues; if it encounters an ob-\nject that implements IDynamicMetaObjectProvider but is not a Python object, it forwards to a\nCompatibilityInvokeBinder class that does inherit from InvokeBinder and can handle foreign\nobjects correctly.\nIf the fallback cannot bind the operation, it doesn\u2019t throw an exception; instead, it returns a\nDynamicMetaObject representing the error. The host language\u2019s binder will then handle this in an\nappropriate manner for the host language; for example, accessing a missing member on an IronPython\nobject from a hypothetical JavaScript implementation could return undefined, while doing the same\nto a JavaScript object from IronPython would raise an AttributeError.\nThe ability for languages to work with dynamic objects is rather useless without the ability to\n\ufb01rst load and execute code written in other languages, and for this the DLR provides a common\nmechanism for hosting other languages.\n8.9\nHosting\nIn addition to providing common language implementation details, the DLR also provides a shared\nhosting interface. The hosting interface is used by the host language (usually a static language like\nC#) to execute code written in another language such as Python or Ruby. This is a common technique\nthat allows end users to extend an application, and the DLR takes it step further by making it trivial\nto use any scripting language that has a DLR implementation. There are four key parts to the hosting\ninterface: the runtime, engines, sources, and scopes.\nThe ScriptRuntime is generally shared amongst all dynamic languages in an application. The\nruntime handles all of the current assembly references that are presented to the loaded languages,\nprovides methods for quick execution of a \ufb01le, and provides the methods for creating new engines.\nFor simple scripting tasks, the runtime is the only interface that needs to be used, but the DLR also\nprovides classes with more control over how scripts are run.\nUsually, only one ScriptEngine is used for each scripting language. The DLR\u2019s meta-object pro-\ntocol means that a program can load scripts from multiple languages, and the objects created by each\nlanguage can all seamlessly interoperate. The engine wraps a language-speci\ufb01c LanguageContext\n(such as PythonContext or RubyContext) and is used for executing code from \ufb01les or strings and\nperforming operations on dynamic objects from languages that don\u2019t natively support the DLR (such\nas C# prior to .NET 4). Engines are thread-safe, and can execute multiple scripts in parallel, as long\nJe\ufb00Hardy\n123\n", "page": 137, "type": "text", "section": "Page 137"}
{"text": "as each thread has its own scope. It also provides methods for creating script sources, which allow\nfor more \ufb01ne-grained control of script execution.\nA ScriptSource holds a chunk of code to be executed; it binds a SourceUnit object, which\nholds the actual code, to the ScriptEngine that created the source. This class allows code to be\ncompiled (which produces a CompiledCode object that can be cached) or executed directly. If a\nchunk of code is going to be executed repeatedly, it\u2019s best to compile \ufb01rst, and then execute the\ncompiled code; for scripts that will only be executed once, it\u2019s best to just execute it directly.\nFinally, however the code gets to be executed, a ScriptScope must be provided for the code to\nexecute in. The scope is used to hold all of script\u2019s variables, and can be pre-loaded with variables\nfrom the host, if necessary. This allows a host to provide custom objects to the script when it starts\nrunning\u2014for example, an image editor may provide a method to access the pixels of the image\nthe script is working on. Once a script has executed, any variables it created can be read from the\nscope. The other main use of scopes is to provide isolation, so that multiple scripts can be loaded\nand executed at the same time without interfering with each other.\nIt\u2019s important to note that all of these classes are provided by the DLR, not the language; only\nthe LanguageContext used by the engine comes from the language implementation. The language\ncontext provides all of the functionality\u2014loading code, creating scopes, compilation, execution, and\noperations on dynamic objects\u2014that is needed by a host, and the DLR hosting classes provide a\nmore usable interface to access that functionality. Because of this, the same hosting code can be\nused to host any DLR-based language.\nFor dynamic language implementations written in C (such as the original Python and Ruby),\nspecial wrapper code must be written to access code not written in the dynamic language, and\nit must be repeated for each supported scripting language. While software like SWIG exists to\nmake this easier, it\u2019s still not trivial to add a Python or Ruby scripting interface to a program and\nexpose its object model for manipulation by external scripts. For .NET programs, however, adding\nscripting is as simple as setting up a runtime, loading the program\u2019s assemblies into the runtime, and\nusing ScriptScope.SetVariable() to make the program\u2019s objects available to the scripts. Adding\nsupport for scripting to a .NET application can be done in a matter of minutes, which is a huge bonus\nof the DLR.\n8.10\nAssembly Layout\nBecause of how the DLR evolved from a separate library into part of the CLR, there are parts that\nare in the CLR (call sites, expression trees, binders, code generation, and dynamic meta objects) and\nparts that are part of IronLanguages open-source project (hosting, the interpreter, and a few other bits\nnot discussed here). The parts that are in the CLR are also included in the IronLanguages project in\nMicrosoft.Scripting.Core. The DLR parts are split into two assemblies, Microsoft.Scripting\nand Microsoft.Dynamic\u2014the former contains the hosting APIs and the latter contains code for\nCOM interop, the interpreter, and some other pieces common to dynamic languages.\nThe languages themselves are split in two as well: IronPython.dll and IronRuby.dll im-\nplement the languages themselves (parsers, binders, etc.) while IronPython.Modules.dll and\nIronRuby.Libraries.dll implement the portions of the standard library that are implemented in\nC in the classic Python and Ruby implementations.\n124\nThe Dynamic Language Runtime and the Iron Languages\n", "page": 138, "type": "text", "section": "Page 138"}
{"text": "8.11\nLessons Learned\nThe DLR is a useful example of a language-neutral platform for dynamic languages built on top of\na static runtime. The techniques it uses to achieve high-performance dynamic code are tricky to\nimplement properly, so the DLR takes these techniques and makes them available to every dynamic\nlanguage implementation.\nIronPython and IronRuby are good examples of how to build a language on top of the DLR. The\nimplementations are very similar because they were developed at the same time by close teams,\nyet they still have signi\ufb01cant di\ufb00erences in implementation. Having multiple di\ufb00erent languages\nco-developed14, along with C#\u2019s and VB\u2019s dynamic features, made sure that the DLR design got\nplenty of testing during development.\nThe actual development of IronPython, IronRuby, and the DLR was handled very di\ufb00erently\nthan most projects within Microsoft at the time\u2014it was a very agile, iterative development model\nwith continuous integration running from day one. This enabled them to change very quickly when\nthey had to, which was good because the DLR became tied into C#\u2019s dynamic features early in its\ndevelopment. While the DLR tests are very quick, only taking a dozen seconds or so, the language\ntests take far too long to run (the IronPython test suite takes about 45 minutes, even with parallel\nexecution); improving this would have improved the iteration speed. Ultimately, these iterations\nconverged on the current DLR design, which seems overly complicated in parts but \ufb01ts together quite\nnicely in total.\nHaving the DLR tied to C# was critically important because it made sure the DLR had a place and\na \u201cpurpose\u201d, but once the C# dynamic features were done the political climate changed (coinciding\nwith an economic downturn) and the Iron languages lost their support within the company. The\nhosting APIs, for example, never made it into the .NET Framework (and it\u2019s highly unlikely they\never will); this means that PowerShell 3, which is also based on the DLR, uses a completely di\ufb00erent\nset of hosting APIs than IronPython and IronRuby, although all of their objects can still interact as\ndescribed above15. But, thanks to the wonder of open source licensing, they will continue to survive\nand even thrive.\n14IronPython, IronRuby, a prototype JavaScript, and the mysterious VBx\u2014a fully dynamic version of VB.\n15Some of the DLR team members went on to work on the C# compiler-as-a-service library code-named \u201cRoslyn\u201d, which\nbears a striking resemblance to the IronPython and IronRuby hosting APIs.\n125\n", "page": 139, "type": "text", "section": "Page 139"}
{"text": "126\nThe Dynamic Language Runtime and the Iron Languages\n", "page": 140, "type": "text", "section": "Page 140"}
{"text": "[chapter9]\nITK\nLuis Ib\u00e1\u00f1ez and Brad King\n9.1\nWhat Is ITK?\nITK (the Insight Toolkit)1 is a library for image analysis that was developed by the initiative, and\nmainly with the funding, of the US National Library of Medicine2. ITK can be thought of as a usable\nencyclopedia of image analysis algorithms, in particular for image \ufb01ltering, image segmentation and\nimage registration. The library was developed by a consortium involving universities, commercial\ncompanies, and many individual contributors from around the world. Development of ITK started in\n1999, and recently after its 10th anniversary the library underwent a refactoring process intended to\nremove crusty code and to reshape it for the next decade.\n9.2\nArchitectural Features\nSoftware toolkits have a very synergistic relationship with their communities. They shape one another\nin a continuous iterative cycle. The software is continuously modi\ufb01ed until it satis\ufb01es the needs of\nthe community, while the community behaviors themselves are adapted based on what the software\nempowers or restricts them to do. In order to better understand the nature of ITK\u2019s architecture,\nit is therefore very useful to get a sense of what kind of problems the ITK community is usually\naddressing, and how they tend to go about solving them.\nThe Nature of the Beast\nIf you did not understand the nature of the beasts,\nit would be of little use to know the mechanics of their anatomy.\nDee Hock, One from Many: VISA and the Rise of Chaordic Organization\nIn a typical image analysis problem, a researcher or an engineer will take an input image, improve\nsome characteristics of the image by, let\u2019s say, reducing noise or increasing contrast, and then proceed\nto identify some features in the image, such as corners and strong edges. This type of processing is\nnaturally well-suited for a data pipeline architecture, as shown in Figure 9.1.\n1http://www.itk.org\n2http://www.nlm.nih.gov\n", "page": 141, "type": "text", "section": "Page 141"}
{"text": "File\nReader\nNoise\nReducion\nEdge\nDetection\nFile\nWriter\nFigure 9.1: Image processing pipeline\nTo illustrate this point, Figure 9.2 shows an image of a brain from a magnetic resonance image\n(MRI), and the result of processing it with a median \ufb01lter to reduce its level of noise, as well as the\noutcome of an edge detection \ufb01lter used to identify the borders of anatomical structures.\nFigure 9.2: From left to right: MRI brain image, median \ufb01lter, edge detection \ufb01lter\nFor each one of these tasks, the image analysis community has developed a variety of algorithms,\nand continue developing new ones. Why do they continue doing this?, you may ask, and the answer\nis that image processing is a combination of science, engineering, art, and \u201ccooking\u201d skills. Claiming\nthat there is an algorithmic combination that is the \u201cright\u201d answer to an image processing task is as\nmisleading as claiming that there is such a thing as the \u201cright\u201d type of chocolate dessert for a dinner.\nInstead of pursuing perfection, the community strives to produce a rich set of tools that ensures that\nthere will be no shortage of options to try when facing a given image processing challenge. This\nstate of a\ufb00airs, of course, comes at a price. The cost is that the image analyst has the di\ufb03cult task\nof choosing among dozens of di\ufb00erent tools that can be used in di\ufb00erent combinations to achieve\nsimilar results.\nThe image analysis community is closely integrated with the research community. It is common\nto \ufb01nd that speci\ufb01c research groups become attached to the algorithmic families they have developed.\nThis custom of \u201cbranding\u201d, and up to some level \u201cmarketing\u201d, leads to a situation where the best\nthat the software toolkit can do for the community is to o\ufb00er a very complete set of algorithmic\nimplementations that they can try, and then mix and match to create a recipe that satis\ufb01es their needs.\nThese are some of the reasons why ITK was designed and implemented as a large collection\nof somewhat independent but coherent tools, the image \ufb01lters, many of which can be used to solve\nsimilar problems. In this context, a certain level of \u201credundancy\u201d\u2014for example, o\ufb00ering three\ndi\ufb00erent implementations of the Gaussian \ufb01lter\u2014is not seen as a problem but as a valuable feature,\nbecause di\ufb00erent implementations can be used interchangeably to satisfy constraints and exploit\ne\ufb03ciencies with respect to image size, number of processors, and Gaussian kernel size that might be\nspeci\ufb01c to a given imaging application.\n128\nITK\n", "page": 142, "type": "text", "section": "Page 142"}
{"text": "The toolkit was also conceived as a resource that grows and renews itself continuously as new\nalgorithms and better implementations become available, superseding existing ones, and as new\ntools are developed in response to the emerging needs of new medical imaging technologies.\nArmed with this quick insight into the daily routine of the image analysts in the ITK community,\nwe can now dive into the main features of the architecture:\n\u2022 Modularity\n\u2022 Data Pipeline\n\u2022 Factories\n\u2022 IO Factories\n\u2022 Streaming\n\u2022 Reusability\n\u2022 Maintainability\nModularity\nModularity is one of the main characteristics of ITK. This is a requirement that emerges from\nthe way people in the image analysis community work when solving their problems. Most image\nanalysis problems put one or more input images through a combination of processing \ufb01lters that\nenhance or extract particular pieces of information from the images. Therefore there is no single\nlarge processing object, but rather myriad small ones. This structural nature of the image processing\nproblem logically implies implementing the software as a large collection of image processing \ufb01lters\nthat can be combined in many di\ufb00erent ways.\nIt is also the case that certain types of processing \ufb01lters are clustered into families, inside which\nsome of their implementation features can be factorized. This leads to natural grouping of the image\n\ufb01lters into modules and groups of modules.\nModularity, therefore occurs at three natural levels in ITK:\n\u2022 Filter Level\n\u2022 Filter Family Level\n\u2022 Filter Family Group Level\nAt the image \ufb01lter level, ITK has a collection of about 700 \ufb01lters. Given that ITK is implemented\nin C++, this is a natural level at which every one of those \ufb01lters is implemented by a C++ Class\nfollowing object-oriented design patterns. At the \ufb01lter family level, ITK groups \ufb01lters together\naccording to the nature of the processing that they perform. For example, all \ufb01lters that are related to\nFourier transforms will be put together into a Module. At the C++ level, Modules map to directories\nin the source tree, and to libraries once the software is compiled to its binary form. ITK has about\n120 of these Modules. Each module contains:\n1. The source code of the image \ufb01lters that belong to that family.\n2. A set of con\ufb01guration \ufb01les that describe how to build the module and list dependencies\nbetween this module and other modules.\n3. The set of unit tests corresponding to each one of the \ufb01lters.\nThe group level is mostly a conceptual division that has been drawn on top of the software to\nhelp locate \ufb01lters in the source tree. Groups are associated with high-level concepts such as Filtering,\nSegmentation, Registration and IO. This hierarchical structure is illustrated in Figure 9.3. ITK\ncurrently has 124 modules, which are in turn aggregated into 13 major groups. The modules have a\nvariety of di\ufb00erent sizes. This size distribution, in bytes, is presented in Figure 9.4.\nLuis Ib\u00e1\u00f1ez and Brad King\n129\n", "page": 143, "type": "text", "section": "Page 143"}
{"text": "ITK\nModules\nCore\nSegmentation\nCommon\nitkObject.h\nitkObject.hxx\nitkObjectTest.cxx\n...\nTransforms\nitkAffineTransform.h\nitkAffineTransform.hxx\nitkAffineTransformTest.cxx\n...\nLevelSets\nitkLevelSetFunction.h\nitkLevelSetFunction.hxx\nitkLevelSetFunctionTest.cxx\n...\nClassifiers\nitkBayesianClassifier...h\nitkBayesianClassifier...hxx\nitkBayesianClassifier...Test.cxx\n...\n...\n...\n...\nFigure 9.3: Hierarchical structure of groups, modules and classes\n130\nITK\n", "page": 144, "type": "text", "section": "Page 144"}
{"text": " 0\n 5000\n 10000\n 15000\n 20000\nSize (KB)\nSize distribution of 50 largest ITK Modules\nITK Native\nCommon\nReview\nStatistics\nFEM\nTransform\nLabelMap\nCommon\nImageGrid\nLevelSets\nSpatialObjects\nImageIntensity\nLevelSetsv4\nMathematicalMorphology\nMesh\nQuadEdgeMesh\nImageFunction\nDeprecated\nOptimizers\nMesh\nBase\nImageStatistics\nNeuralNetworks\nImageFeature\nFastMarching\nQuadEdgeMeshFiltering\nPath\nPDEDeformable\nClassifiers\nDistanceMap\nBinaryMathematicalMorphology\nWatersheds\nIntegratedTest\nPhilipsREC\nDisplacementField\nImageFilterBase\nFFT\nThird Party\nVNL\nHDF5\nGDCM\nTIFF\nNIFTI\nJPEG\nPNG\nKWSys\nOpenJPEG\nMetaIO\nNrrdIO\nZLIB\nGIFTI\nExpat\nFigure 9.4: Size distribution of 50 largest ITK modules in KB\nThe modularization in ITK also applies to a set of third-party libraries that are not directly part\nof the toolkit, but that the toolkit depends upon, and that are distributed along with the rest of the\ncode for the convenience of users. Particular examples of these third-party libraries are the image \ufb01le\nformat libraries: HDF5, PNG, TIFF, JPEG and OpenJPEG among others. The third party libraries\nare highlighted here because they account for about 56 percent of the size of ITK. This re\ufb02ects the\nusual nature of open source applications that build upon existing platforms. The size distribution\nof the third-party libraries does not necessarily re\ufb02ect the architectural organization of ITK, since\nwe have adopted these useful libraries just as they have been developed upstream. However, the\nthird-party code is distributed along with the toolkit, and partitioning it was one of the key driving\ndirectives for the modularization process.\nThe module size distribution is presented here because it is a measure of the proper modularization\nof the code. One can see the modularization of the code as a continuous spectrum that ranges from\nthe extremes of having all the code in a single module, the monolithic version, to partitioning the\ncode in a very large collection of equally sized modules. This size distribution was a tool used to\nmonitor the progression of the modularization process, particularly to ensure that no big blocks of\ncode were left in the same module unless true logical dependencies called for such grouping.\nThe modular architecture of ITK enables and facilitates:\n\u2022 Reduction and clari\ufb01cation of cross-dependencies\n\u2022 Adoption of code contributed by the community\n\u2022 Evaluation of quality metrics per module (for example, code coverage)\n\u2022 Building selected subsets of the toolkit\n\u2022 Packaging selected subsets of the toolkit for redistribution\n\u2022 Continued growth by progressive addition of new modules\nLuis Ib\u00e1\u00f1ez and Brad King\n131\n", "page": 145, "type": "text", "section": "Page 145"}
{"text": "The modularization process made it possible to explicitly identify and declare the dependencies\nbetween di\ufb00erent portions of the toolkit as they were put into modules. In many cases, this exercise\nrevealed arti\ufb01cial or incorrect dependencies that had been introduced in the toolkit over time, and\nthat passed unnoticed when most of the code was put together in a few large groups.\nThe usefulness of evaluating quality metrics per module is twofold. First, it makes it easier to\nhold developers accountable for the modules which they maintain. Second, it makes it possible to\nengage in clean-up initiatives in which a few developers focus for a short period of time on raising\nthe quality of a speci\ufb01c module. When concentrating on a small portion of the toolkit, it is easier to\nsee the e\ufb00ect of the e\ufb00ort and to keep developers engaged and motivated.\nTo reiterate, we note that the structure of the toolkit re\ufb02ects the organization of the community\nand in some cases the processes that have been adopted for the continuous growth and quality control\nof the software.\nData Pipeline\nThe staged nature of most image analysis tasks led naturally to the selection of a Data Pipeline\narchitecture as the backbone infrastructure for data processing. The Data Pipeline enables:\n\u2022 Filter Concatenation: A set of image \ufb01lters can be concatenated one after another, composing\na processing chain in which a sequence of operations are applied to the input images.\n\u2022 Parameter Exploration: Once a processing chain is put together, it is easy to change the\nparameters of any \ufb01lter in the chain, and to explore the e\ufb00ects that such change will have on\nthe \ufb01nal output image.\n\u2022 Memory Streaming: Large images can be managed by processing only sub-blocks of the image\nat a time. In this way, it becomes possible to process large images that otherwise would not\nhave \ufb01t into main memory.\nFigures 9.1 and 9.2 have already presented a simpli\ufb01ed representation of a data pipeline from the\nimage processing point of view. Image \ufb01lters typically have numeric parameters that are used to\nregulate the behavior of the \ufb01lter. Every time one of the numeric parameters is modi\ufb01ed, the data\npipeline marks its output as \u201cdirty\u201d and knows that this particular \ufb01lter, and all the downstream ones\nthat use its output, should be executed again. This feature of the pipeline facilitates the exploration\nof parameter space while using a minimum amount of processing power for each instance of an\nexperiment.\nThe process of updating the pipeline can be driven in such a way that only sub-pieces of the\nimages are processed at a time. This is a mechanism necessary to support the functionality of\nstreaming. In practice, the process is controlled by the internal passing of a RequestedRegion\nspeci\ufb01cation from one \ufb01lter downstream to its provider \ufb01lter upstream. This communication is done\nthrough an internal API and it is not exposed to the application developers.\nFor a more concrete example, if a Gaussian blur image \ufb01lter is expecting to use as input a\n100x100-pixel image that is produced by a median image \ufb01lter, the blur \ufb01lter can ask the median\n\ufb01lter to produce only a quarter of the image, that is, an image region of size 100x25 pixels. This\nrequest can be further propagated upstream, with the caveat that every intermediate \ufb01lter may have\nto add an extra border to the image region size in order to produce that requested output region size.\nThere is more on data streaming in Section 9.2.\nBoth a change in the parameters of a given \ufb01lter, or a change in the speci\ufb01c requested region to\nbe processed by that \ufb01lter, will have the e\ufb00ect of marking the pipeline as \u201cdirty\u201d and indicating the\nneed for a reexecution of that \ufb01lter through the downstream \ufb01lters in the pipeline.\n132\nITK\n", "page": 146, "type": "text", "section": "Page 146"}
{"text": "Process and Data Objects\nTwo main types of objects were designed to hold the basic structure of the pipeline. They are the\nDataObject and the ProcessObject. The DataObject is the abstraction of classes that carry data;\nfor example, images and geometrical meshes. The ProcessObject provides an abstraction for the\nimage \ufb01lters and mesh \ufb01lters that process such data. ProcessObjects take DataObjects as input and\nperform some type of algorithmic transformation on them, such as the ones illustrated in Figure 9.2.\nDataObjects are generated by ProcessObjects. This chain typically starts by reading a DataObject\nfrom disk, for example by using a ImageFileReader which is a type of ProcessObject. The\nProcessObject that created a given DataObject is the only one that should modify such DataObject.\nThis output DataObject is typically connected as input to another ProcessObject downstream in\nthe pipeline.\nFile\nReader\nImage\nMedian\nFilter\nImage\nCanny\nFilter\nImage\nFile\nWriter\nGradient\nFilter\nImage\nSubtract\nFilter\nImage\nFile\nWriter\nFigure 9.5: Relationship between ProcessObjects and DataObjects\nThis sequence is illustrated in Figure 9.5. The same DataObject may be passed as input to\nmultiple ProcessObjects, as it is shown in the \ufb01gure, where the DataObject is produced by the\n\ufb01le reader at the beginning of the pipeline. In this particular case, the \ufb01le reader is an instance of the\nImageFileReader class, and the DataObject that it produces as output is an instance of the Image\nclass. It is also common for some \ufb01lters to require two DataObjects as input, as it is the case of the\nsubtract \ufb01lter indicated in the right side of the same \ufb01gure.\nThe ProcessObjects and DataObjects are connected together as a side e\ufb00ect of constructing\nthe pipeline. From the application developer\u2019s point of view, the pipeline is linked together by\ninvoking a sequence of calls involving the ProcessObjects such as:\nwriter->SetInput( canny->GetOutput() );\ncanny->SetInput( median->GetOutput() );\nmedian->SetInput( reader->GetOutput() );\nInternally, however, what is connected as a consequence of these calls is not one ProcessObject\nto the next ProcessObject, but the downstream ProcessObject to the DataObject that is produced\nby the upstream ProcessObject.\nThe internal chained structure of the pipeline is held together by three types of connections:\n\u2022 The ProcessObject holds a list of pointers to its output DataObjects. Output DataObjects\nare owned and controlled by the ProcessObject that produces them.\n\u2022 The ProcessObject holds a list of pointers to its input DataObjects. Input DataObjects are\nowned by the upstream ProcessObject.\n\u2022 The DataObject holds a pointer to its producer ProcessObject. That happens to be the\nProcessObject that also owns and control this DataObject.\nThis collection of internal links is later exploited to propagate calls upstream and downstream\nin the pipeline. During all these interactions, the ProcessObject retains control and ownership of\nLuis Ib\u00e1\u00f1ez and Brad King\n133\n", "page": 147, "type": "text", "section": "Page 147"}
{"text": "the DataObject that it generates. The \ufb01lters downstream gain access to the information about a\ngiven DataObject through the pointer links that are established as a consequence of the calls to the\nSetInput() and GetOutput() methods, without ever taking control of that input data. For practical\npurposes, \ufb01lters should treat their input data as read-only objects. This is enforced in the API by\nusing the C++ const keyword in the arguments of SetInput() methods. As a general rule, ITK\nembraces a const-correct external API, even though internally this const-correctness is overridden by\nsome of the pipeline operations.\nThe Pipeline Class Hierarchy\nLightObject\nObject\nProcessObject\nDataObject\nImageSource\nFileReader\nMedianFilter\nImage\nMesh\n...\nFigure 9.6: Hierarchy of ProcessObjects and DataObjects\nThe initial design and implementation of the Data Pipeline in ITK was derived from the Visual-\nization Toolkit (VTK)3, a mature project at the time when ITK development began.\nFigure 9.6 shows the object-oriented hierarchy of the pipeline objects in ITK. In particular, note\nthe relationship between the basic Object, ProcessObject, DataObject, and some of the classes in\nthe \ufb01lter family and the data family. In this abstraction, any object that is expected to be passed as input\nto a \ufb01lter, or to be produced as output by a \ufb01lter, must derive from the DataObject. All \ufb01lters that\nproduce and consume data are expected to derive from the ProcessObject. The data negotiations\nrequired to move data through the pipeline are implemented partly in the ProcessObject and partly\nin the DataObject.\nThe LightObject and Object classes are above the dichotomy of the ProcessObject and\nDataObject. The LightObject and Object classes provide common functionalities such as the\nAPI for communications of Events, and the support for multi-threading.\n3See The Architecture of Open Source Applications, Volume 1\n134\nITK\n", "page": 148, "type": "text", "section": "Page 148"}
{"text": "The Inner Workings of the Pipeline\nFigure 9.7 presents a UML sequence diagram describing the interactions between ProcessObjects\nand DataObjects in a minimal pipeline composed of an ImageFileReader, MedianImageFilter\nand ImageFileWriter.\nThe full interaction consist of four passes:\n\u2022 Update Output Information (upstream call sequence)\n\u2022 Update Requested Region (upstream call sequence)\n\u2022 Update Output Data (upstream call sequence)\n\u2022 Generate Data (downstream call sequence)\n\uf001\uf002\uf003\uf004\uf005\uf006\uf007\uf008\uf005\uf009\uf005\uf003\uf00a\uf005\uf00b\n\uf00c\uf00b\uf00d\uf00e\uf005\uf00f\uf00f\n\uf010\uf011\uf012\uf005\uf00e\uf013\n\uf014\uf003\uf013\uf003\n\uf010\uf011\uf012\uf005\uf00e\uf013\n\uf015\uf005\uf00a\uf007\uf003\uf016\uf001\uf002\uf003\uf004\uf005\uf006\uf007\uf008\uf013\uf005\uf00b\n\uf001\uf002\uf003\uf004\uf005\n\uf00c\uf00b\uf00d\uf00e\uf005\uf00f\uf00f\n\uf010\uf011\uf012\uf005\uf00e\uf013\n\uf014\uf003\uf013\uf003\n\uf010\uf011\uf012\uf005\uf00e\uf013\n\uf001\uf002\uf003\uf004\uf005\n\uf001\uf002\uf003\uf004\uf005\uf006\uf007\uf008\uf005\uf017\uf00b\uf007\uf013\uf005\uf00b\n\uf00c\uf00b\uf00d\uf00e\uf005\uf00f\uf00f\n\uf010\uf011\uf012\uf005\uf00e\uf013\n\uf018\uf019\uf00a\uf003\uf013\uf005\uf01a\uf01b\n\uf018\uf019\uf00a\uf003\uf013\uf005\uf010\uf01c\uf013\uf019\uf01c\uf013\uf001\uf016\uf01d\uf00d\uf00b\uf002\uf003\uf013\uf007\uf00d\uf016\uf01a\uf01b\n\uf00c\uf00b\uf00d\uf019\uf003\uf004\uf003\uf013\uf005\uf009\uf005\uf01e\uf01c\uf005\uf00f\uf013\uf005\uf00a\uf009\uf005\uf004\uf007\uf00d\uf016\uf01a\uf01b\n\uf018\uf019\uf00a\uf003\uf013\uf005\uf010\uf01c\uf013\uf019\uf01c\uf013\uf014\uf003\uf013\uf003\uf01a\uf01b\n\uf018\uf019\uf00a\uf003\uf013\uf005\uf010\uf01c\uf013\uf019\uf01c\uf013\uf001\uf016\uf01d\uf00d\uf00b\uf002\uf003\uf013\uf007\uf00d\uf016\uf01a\uf01b\n\uf018\uf019\uf00a\uf003\uf013\uf005\uf010\uf01c\uf013\uf019\uf01c\uf013\uf001\uf016\uf01d\uf00d\uf00b\uf002\uf003\uf013\uf007\uf00d\uf016\uf01a\uf01b\n\uf018\uf019\uf00a\uf003\uf013\uf005\uf010\uf01c\uf013\uf019\uf01c\uf013\uf001\uf016\uf01d\uf00d\uf00b\uf002\uf003\uf013\uf007\uf00d\uf016\uf01a\uf01b\n\uf01f\uf005\uf013\uf01f\uf019\uf003\uf00e\uf007\uf016\uf004\uf01a\uf01b\n\uf01f\uf005\uf013\uf010\uf00b\uf007\uf004\uf007\uf016\uf01a\uf01b\n\uf01f\uf005\uf013\uf01f\uf007\uf020\uf005\uf01a\uf01b\n\uf01f\uf005\uf013\uf014\uf007\uf00b\uf005\uf00e\uf013\uf007\uf00d\uf016\uf01a\uf01b\n\uf01f\uf005\uf013\uf021\uf003\uf00b\uf004\uf005\uf00f\uf013\uf00c\uf00d\uf00f\uf00f\uf007\uf011\uf008\uf005\uf009\uf005\uf004\uf007\uf00d\uf016\uf01a\uf01b\n\uf00c\uf00b\uf00d\uf019\uf003\uf004\uf003\uf013\uf005\uf009\uf005\uf01e\uf01c\uf005\uf00f\uf013\uf005\uf00a\uf009\uf005\uf004\uf007\uf00d\uf016\uf01a\uf01b\n\uf00c\uf00b\uf00d\uf019\uf003\uf004\uf003\uf013\uf005\uf009\uf005\uf01e\uf01c\uf005\uf00f\uf013\uf005\uf00a\uf009\uf005\uf004\uf007\uf00d\uf016\uf01a\uf01b\n\uf022\uf016\uf008\uf003\uf00b\uf004\uf005\uf010\uf01c\uf013\uf019\uf01c\uf013\uf009\uf005\uf01e\uf01c\uf005\uf00f\uf013\uf005\uf00a\uf009\uf005\uf004\uf007\uf00d\uf016\uf01a\uf01b\n\uf023\uf005\uf016\uf005\uf00b\uf003\uf013\uf005\uf00d\uf01c\uf013\uf019\uf01c\uf013\uf009\uf005\uf01e\uf01c\uf005\uf00f\uf013\uf005\uf00a\uf009\uf005\uf004\uf007\uf00d\uf016\uf01a\uf01b\n\uf023\uf005\uf016\uf005\uf00b\uf003\uf013\uf005\uf001\uf016\uf019\uf01c\uf013\uf009\uf005\uf01e\uf01c\uf005\uf00f\uf013\uf005\uf00a\uf009\uf005\uf004\uf007\uf00d\uf016\uf01a\uf01b\n\uf022\uf016\uf008\uf003\uf00b\uf004\uf005\uf010\uf01c\uf013\uf019\uf01c\uf013\uf009\uf005\uf01e\uf01c\uf005\uf00f\uf013\uf005\uf00a\uf009\uf005\uf004\uf007\uf00d\uf016\uf01a\uf01b\n\uf023\uf005\uf016\uf005\uf00b\uf003\uf013\uf005\uf00d\uf01c\uf013\uf019\uf01c\uf013\uf009\uf005\uf01e\uf01c\uf005\uf00f\uf013\uf005\uf00a\uf009\uf005\uf004\uf007\uf00d\uf016\uf01a\uf01b\n\uf023\uf005\uf016\uf005\uf00b\uf003\uf013\uf005\uf001\uf016\uf019\uf01c\uf013\uf009\uf005\uf01e\uf01c\uf005\uf00f\uf013\uf005\uf00a\uf009\uf005\uf004\uf007\uf00d\uf016\uf01a\uf01b\n\uf00c\uf00b\uf00d\uf019\uf003\uf004\uf003\uf013\uf005\uf009\uf005\uf01e\uf01c\uf005\uf00f\uf013\uf005\uf00a\uf009\uf005\uf004\uf007\uf00d\uf016\uf01a\uf01b\n\uf018\uf019\uf00a\uf003\uf013\uf005\uf010\uf01c\uf013\uf019\uf01c\uf013\uf014\uf003\uf013\uf003\uf01a\uf01b\n\uf018\uf019\uf00a\uf003\uf013\uf005\uf010\uf01c\uf013\uf019\uf01c\uf013\uf014\uf003\uf013\uf003\uf01a\uf01b\n\uf018\uf019\uf00a\uf003\uf013\uf005\uf010\uf01c\uf013\uf019\uf01c\uf013\uf014\uf003\uf013\uf003\uf01a\uf01b\n\uf023\uf005\uf016\uf005\uf00b\uf003\uf013\uf005\uf014\uf003\uf013\uf003\uf01a\uf01b\n\uf023\uf005\uf016\uf005\uf00b\uf003\uf013\uf005\uf014\uf003\uf013\uf003\uf01a\uf01b\n\uf023\uf005\uf016\uf005\uf00b\uf003\uf013\uf005\uf014\uf003\uf013\uf003\uf01a\uf01b\n\uf017\uf00b\uf007\uf013\uf005\uf01a\uf01b\n\uf01f\uf005\uf013\uf009\uf005\uf01e\uf01c\uf005\uf00f\uf013\uf005\uf00a\uf009\uf005\uf004\uf007\uf00d\uf016\uf01a\uf01b\n\uf01f\uf005\uf013\uf00c\uf007\uf024\uf005\uf008\uf00f\uf01a\uf01b\n\uf01f\uf005\uf013\uf00c\uf007\uf024\uf005\uf008\uf00f\uf01a\uf01b\n\uf001\uf002\uf003\uf004\uf005\uf001\uf010\uf025\uf026\uf009\uf005\uf003\uf00a\uf01a\uf01b\nFigure 9.7: UML sequence diagram\nThe whole process is triggered when an application invokes the Update() method in the last\n\ufb01lter of the pipeline; in this concrete example this is the ImageFileWriter. The Update() call\ninitiates the \ufb01rst pass that goes in the upstream direction. That is, from the last \ufb01lter in the pipeline,\ntowards the \ufb01rst \ufb01lter in the pipeline.\nThe goal of this \ufb01rst pass is to ask the question, \u201cHow much data can you generate for me?\u201d\nThis question is codi\ufb01ed in the method UpdateOutputInformation(). In this method, every \ufb01lter\nLuis Ib\u00e1\u00f1ez and Brad King\n135\n", "page": 149, "type": "text", "section": "Page 149"}
{"text": "computes the amount of image data that can be produced as output with the given amount of data\navailable to it as input. Given that the amount of data input must be known \ufb01rst before the \ufb01lter\ncan answer the question about the amount of data output, the question has to propagate to the \ufb01lter\nupstream, until it reaches a source \ufb01lter that can answer the \ufb01rst question by itself. In this concrete\nexample, that source \ufb01lter is the ImageFileReader. This \ufb01lter can \ufb01gure out the size of its output by\ngathering information from the image \ufb01le that it has been assigned to read. Once the \ufb01rst \ufb01lter of the\npipeline answers the question, then the subsequent \ufb01lters downstream can compute their respective\namount of output one after another, until they make it to the last \ufb01lter of the pipeline.\nThe second pass, which also travels in the upstream direction, informs \ufb01lters as to the amount\nof output that they are requested to produce during pipeline execution. The concept of Requested\nRegion is essential in supporting the streaming capabilities of ITK. It makes it possible to tell the\n\ufb01lters in the pipeline not to generate the entire full image, but to focus instead in a subregion of\nthe image, the Requested Region. This is very useful when the image at hand is larger than the\nRAM available in the system. The call propagates from the last \ufb01lter to the \ufb01rst one, and at every\nintermediate \ufb01lter the requested region size is modi\ufb01ed to take into account any extra borders that a\n\ufb01lter may need in the input so it can generate a given region size as output. In our concrete example,\nthe median \ufb01lter will typically have to add a 2-pixel border to the size of its own input. That is, if the\nwriter requests a region of size 500 x 500 pixels to the median \ufb01lter, the median \ufb01lter in its turn will\nrequest a region of 502 x 502 pixels to the reader, because the median \ufb01lter by default needs a 3 x\n3 pixel neighborhood region to compute the value of one output pixel. The pass is encoded in the\nPropagateRequestedRegion() method.\nThe third pass is intended to trigger the computation on the data inside the Requested Region.\nThis pass also goes in the upstream direction and it is codi\ufb01ed in the UpdateOutputData() method.\nSince every \ufb01lter needs its input data before it can compute its output data, the call is passed to\nthe respective upstream \ufb01lter \ufb01rst, hence the upstream propagation. Upon return the current \ufb01lter\nactually proceeds to computes its data.\nThe fourth and \ufb01nal pass proceeds downstream, and consists of the actual execution of computa-\ntion by every \ufb01lter. The call is codi\ufb01ed in the GenerateData() method. The downstream direction\nis not a consequence of one \ufb01lter making calls on its downstream partner, but rather of the fact that\nthe UpdateOutputData() calls are executing in order from the \ufb01rst \ufb01lter to the last \ufb01lter. That is,\nthe sequence happens downstream due to timing of the calls, and not due to what \ufb01lter is driving the\ncalls. This clari\ufb01cation is important because the ITK pipeline is by nature a Pull Pipeline, in which\ndata is requested from the end, and the logic is also controlled from the end.\nFactories\nOne of the fundamental design requirements of ITK is to provide support for multiple platforms. This\nrequirement emerges from the desire to maximize the impact of the toolkit by making it usable to a\nbroad community regardless of their platform of choice. ITK adopted the Factory design pattern to\naddress the challenge of supporting fundamental di\ufb00erences among the many hardware and software\nplatforms, without sacri\ufb01cing the \ufb01tness of a solution to each one of the individual platforms.\nThe Factory pattern in ITK uses class names as keys to a registry of class constructors. The\nregistration of factories happens at run time, and can be done by simply placing dynamic libraries in\nspeci\ufb01c directories that ITK applications search at start-up time. This last feature provides a natural\nmechanism for implementing a plugin architecture in a clean and transparent way. The outcome is to\nfacilitate the development of extensible image analysis applications, satisfying the need to provide\nan ever-growing set of image analysis capabilities.\n136\nITK\n", "page": 150, "type": "text", "section": "Page 150"}
{"text": "IO Factories\nThe factory mechanism is particularly important when performing IO.\nEmbracing Diversity with Facades\nThe image analysis community has developed a very large set of \ufb01le formats to store image data.\nMany of these \ufb01le formats are designed and implemented with speci\ufb01c uses in mind, and therefore are\n\ufb01ne-tuned to speci\ufb01c types of images. As a consequence, on a regular basis, new image \ufb01le formats\nare conceived and promoted across the community. Aware of this situation, the ITK development\nteam designed an IO architecture suitable for ease of extensibility, in which it is easy to add support\nfor more and more \ufb01le formats on a regular basis.\nImageIOFactory\nPNGImageIO\nPNGImageIOFactory\nJPEGImageIO\nJPEGImageIOFactory\nGDCMImageIO\nGDCMImageIOFactory\n...\nImageFileReader\nImageFileReader\nFigure 9.8: IO Factories dependencies\nThis IO extensible architecture is built upon the Factory mechanism described in the previous\nsection. The main di\ufb00erence is that in the case of IO, the IO Factories are registered in a specialized\nregistry that is managed by the ImageIOFactory base class, shown on the upper left corner of Fig-\nure 9.8. The actual functionality of reading and writing data from image \ufb01le formats is implemented\nin a family of ImageIO classes, shown on the right side of Figure 9.8. These service classes are\nintended to be instantiated on demand when the user requests to read or write an image. The service\nclasses are not exposed to the application code. Instead, applications are expected to interact with\nthe facade classes:\n\u2022 ImageFileReader\n\u2022 ImageFileWriter\nThese are the two classes with which the application will invoke code such as:\nreader->SetFileName(\u2018\u2018image1.png\u2019\u2019);\nreader->Update();\nor\nwriter->SetFileName(\u2018\u2018image2.jpg\u2019\u2019);\nwriter->Update();\nLuis Ib\u00e1\u00f1ez and Brad King\n137\n", "page": 151, "type": "text", "section": "Page 151"}
{"text": "In both cases the call to Update() triggers the execution of the upstream pipeline to which these\nProcessObjects are connected. Both the reader and the writer behave as one \ufb01lter more in a pipeline.\nIn the particular case of the reader, the call to Update() triggers the reading of the corresponding\nimage \ufb01le into memory. In the case of the writer, the call to Update() triggers the execution of\nthe upstream pipeline that is providing the input to the writer, and \ufb01nally results in an image being\nwritten out to disk into a particular \ufb01le format.\nThese facade classes hide from the application developer the internal di\ufb00erences that are inherent\nto the particularities of each \ufb01le format. They even hide the existence of the \ufb01le format itself. The\nfacades are designed in such a way that most of the time application developers do not need to know\nwhat \ufb01le formats are expected to be read by the application. The typical application will simply\ninvoke code such as\nstd::string filename = this->GetFileNameFromGUI();\nwriter->SetFileName( filename );\nwriter->Update();\nThese calls will work \ufb01ne regardless of whether the content of the filename variable is any of the\nfollowing strings:\n\u2022 image1.png\n\u2022 image1.jpeg\n\u2022 image1.ti\ufb00\n\u2022 image1.dcm\n\u2022 image1.mha\n\u2022 image1.nii\n\u2022 image1.nii.gz\nwhere the \ufb01le name extensions identify a di\ufb00erent image \ufb01le format in every case.\nKnow Thy Pixel Type\nDespite the assistance that the \ufb01le reader and writer facades provide, it is still up to the application\ndeveloper to be aware of the pixel type that the application needs to process. In the context of\nmedical imaging, it is reasonable to expect that the application developer will know whether the\ninput image will contain a MRI, a mammogram or a CT scan, and therefore be mindful of selecting\nthe appropriate pixel type and image dimensionality for each one of these di\ufb00erent image modalities.\nThis speci\ufb01city of image type might not be convenient for application settings where users wants\nto read any image type, which are most commonly found in the scenarios of rapid prototyping and\nteaching. In the context of deploying a medical image application for production in a clinical setting,\nhowever, it is expected that the pixel type and dimension of the images will be clearly de\ufb01ned and\nspeci\ufb01ed based on the image modality to be processed. A concrete example, where an application\nmanages 3D MRI scans, looks like:\ntypedef itk::Image< signed short, 3 >\nMRImageType;\ntypedef itk::ImageFileWriter< MRImageType > MRIWriterType;\nMRIWriterType::Pointer writer = MRIWriterType::New();\nwriter->Update();\nThere is a limit, however, to how much the particularities of the image \ufb01le formats can be hidden\nfrom the application developer. For example, when reading images from DICOM \ufb01les, or when\n138\nITK\n", "page": 152, "type": "text", "section": "Page 152"}
{"text": "reading RAW images, the application developer may have to insert extra calls to further specify\nthe characteristics of the \ufb01le format at hand. DICOM \ufb01les will be the most commonly found in\nclinical environments, while RAW images are still a necessary evil for exchanging data in the research\nenvironment.\nTogether But Separate\nThe self-contained nature of every IO Factory and ImageIO service class is also re\ufb02ected in the\nmodularization. Typically, an ImageIO class depends on a specialized library that is dedicated to\nmanaging a speci\ufb01c \ufb01le format. That is the case for PNG, JPEG, TIFF and DICOM, for example.\nIn those cases, the third-party library is managed as a self-contained module, and the specialized\nImageIO code that interfaces ITK to that third-party library is also put in a module by itself. In\nthis way, speci\ufb01c applications may disable many \ufb01le formats that are not relevant to their domain,\nand can focus on o\ufb00ering only those \ufb01le formats that are useful for the anticipated scenarios of that\napplication.\nJust as with standard factories, the IO factories can be loaded at run-time from dynamic libraries.\nThis \ufb02exibility facilitates the use of specialized and in-house developed \ufb01le formats without requiring\nall such \ufb01le formats to be incorporated directly into the ITK toolkit itself. The loadable IO factories\nhas been one of the most successful features in the architectural design of ITK. It has made it possible\nto easily manage a challenging situation without placing a burden on the code or obscuring its\nimplementation. More recently, the same IO architecture has been adapted to manage the process\nof reading and writing \ufb01les containing spatial transformations represented by the Transform class\nfamily.\nStreaming\nITK was initially conceived as a set of tools for processing the images acquired by the Visible Human\nProject4. At the time, it was clear that such a large dataset would not \ufb01t in the RAM of computers\nthat were typically available to the medical imaging research community. It is still the case that\nthe dataset will not \ufb01t in the typical desktop computers that we use today. Therefore, one of the\nrequirements for developing the Insight Toolkit was to enable the streaming of image data through\nthe data pipeline. More speci\ufb01cally, to be able to process large images by pushing sub-blocks of the\nimage throughout the data pipeline, and then assembling the resulting blocks on the output side of\nthe pipeline.\nThis partitioning of the image domain is illustrated in Figure 9.9 for the concrete example of a\nmedian \ufb01lter. The median \ufb01lter computes the value of one output pixel as the statistical median of the\npixel values from the input image in a neighborhood around the pixel. The size of that neighborhood\nis a numerical parameter of the \ufb01lter. In this case we set it to 2 pixels, which means that we will\ntake a neighborhood with a 2-pixel radius around our output pixel. This leads to a neighborhood of\n5x5 pixels with the position of the output pixel in the middle, and a rectangular border of 2 pixels\naround it. This is usually called a Manhattan radius. When the median \ufb01lter is asked to computed\na particular Requested Region of the output image, it turns around and asks its upstream \ufb01lter to\nprovide a larger region that is formed by the Requested Region enlarged by a border of, in this case,\n2 pixels. In the speci\ufb01c case of Figure 9.9, when asked for Region 2, of size 100x25 pixels, the\nmedian \ufb01lter passes along that request to its upstream \ufb01lter for a region of size 100x29 pixels. The\n29-pixel size in the vertical direction is computed as 25 pixels plus two borders of 2-pixel radius\n4http://www.nlm.nih.gov/research/visible/visible_human.html\nLuis Ib\u00e1\u00f1ez and Brad King\n139\n", "page": 153, "type": "text", "section": "Page 153"}
{"text": "Median\nImage\nFilter\nInput Image\nOutput Image\nRegion 1\nRegion 2\nRegion 3\nRegion 4\nRegion 1\nRegion 2\nRegion 3\nRegion 4\nNeighborhood Radius = 2\nSize = 100x25 pixels\nSize = 100x29 pixels\nFigure 9.9: Illustration of image streaming process\neach. Note that the horizontal dimension is not enlarged in this case because it is already at the\nmaximum that the input image can provide; therefore, the enlarged request of 104 pixels (100 pixels\nplus two borders of 2 pixels) gets cropped to the maximum size of the image, which is 100 pixels in\nthe horizontal dimension.\nITK \ufb01lters that operate on neighborhoods will take care of the boundary conditions by using one\nof the three typical approaches: considering a null value outside of the image, mirroring the pixels\u2019\nvalues across the border, or repeating the border value on the outside. In the case of the median \ufb01lter,\na zero-\ufb02ux Neumann boundary condition is used, which simply means that the pixels outside of the\nregion border are assumed to be a repetition of the pixel values found in the last pixels inside the\nborder.\nIt is a well-kept dirty little secret of the image processing literature that most of the implementation\ndi\ufb03culties with image \ufb01lters are related to proper management of boundary conditions. This is a\nparticular symptom of the disconnection between the theoretical training found in many textbooks\nand the software practice of image processing. In ITK this was managed by implementing a collection\nof image iterator classes and an associated family of boundary condition calculators. These two\nhelper classes families hide from image \ufb01lters the complexities of managing boundary conditions in\nN-dimensions.\nThe streaming process is driven from outside the \ufb01lter, typically by the ImageFileWriter or\nthe StreamingImageFilter. These two classes implement the streaming functionality of taking the\ntotal size of the image and partitioning it into a number of divisions requested by the application\ndeveloper. Then, during their Update() call, they go in an iteration loop asking for each one of the\nintermediate pieces of the image. At that stage, they take advantage of the SetRequestedRegion()\nAPI described in Figure 9.7 in Section 9.2. That constrains the computation of the upstream pipeline\nto a subregion of the image.\nThe application code driving the streaming process looks like\nmedian->SetInput( reader->GetOutput() );\nmedian->SetNeighborhoodRadius( 2 );\nwriter->SetInput( median->GetOutput() );\nwriter->SetFileName( filename );\nwriter->SetNumberOfStreamDivisions( 4 );\nwriter->Update();\nwhere the only new element is the SetNumberOfStreamDivisions() call that de\ufb01nes the number\nof pieces into which the image will be split for the purpose of streaming it through the pipeline. To\n140\nITK\n", "page": 154, "type": "text", "section": "Page 154"}
{"text": "match the example of Figure 9.9 we have used four as the number of regions to split the image into.\nThis means that the writer is going to trigger the execution of the median \ufb01lter four times, each\ntime with a di\ufb00erent Requested Region.\nThere are interesting similarities between the process of streaming and the process of parallelizing\nthe execution of a given \ufb01lter. Both of them rely on the possibility of dividing the image processing\nwork into image chunks that are processed separately. In the streaming case, the image chunks are\nprocessed across time, one after another, while in the parallelization case the image chunks are\nassigned to di\ufb00erent threads, which in turn are assigned to separate processor cores. At the end, it is\nthe algorithmic nature of the \ufb01lter that will determine whether it is possible to split the output image\ninto chunks that can be computed independently based on a corresponding set of image chunks from\nthe input image. In ITK, streaming and parallelization are actually orthogonal, in the sense that\nthere is an API to take care of the streaming process, and a separate API dedicated to support the\nimplementation of parallel computation base on multiple-threads and shared memory.\nStreaming, unfortunately, can not be applied to all types of algorithms. Speci\ufb01c cases that are\nnot suitable for streaming are:\n\u2022 Iterative algorithms that, to compute a pixel value at every iteration, require as input the pixel\nvalues of its neighbors. This is the case for most PDE-solving-based algorithms, such as\nanisotropic di\ufb00usion, demons deformable registration, and dense level sets.\n\u2022 Algorithms that require the full set of input pixel values in order to compute the value of one\nof the output pixels. Fourier transform and In\ufb01nite Impulse Response (IIR) \ufb01lters, such as the\nRecursive Gaussian \ufb01lter, are examples of this class.\n\u2022 Region propagation or front propagation algorithms in which the modi\ufb01cation of pixels also\nhappens in an iterative way but for which the location of the regions or fronts can not be\nsystematically partitioned in blocks in a predictable way. Region growing segmentation, sparse\nlevel sets, some implementations of mathematical morphology operations and some forms of\nwatersheds are typical examples here.\n\u2022 Image registration algorithms, given that they require access to the full input image data for\ncomputing metric values at every iteration of their optimization cycles.\nFortunately, on the other hand, the data pipeline structure of ITK enables support for streaming\nin a variety of transformation \ufb01lters by taking advantage of the fact that all \ufb01lters create their own\noutput, and therefore they do not overwrite the memory of the input image. This comes at the price\nof memory consumption, since the pipeline has to allocate both the input and output images in\nmemory simultaneously. Filters such as \ufb02ipping, axes permutation, and geometric resampling fall\nin this category. In these cases, the data pipeline manages the matching of input regions to output\nregions by requiring that every \ufb01lter provide a method called GenerateInputRequestedRegion()\nthat takes as an argument a rectangular output region. This method computes the rectangular input\nregion that will be needed by this \ufb01lter to compute that speci\ufb01c rectangular output region. This\ncontinuous negotiation in the data pipeline makes it possible to associate, for every output block, the\ncorresponding section of input image that is required for computation.\nTo be more precise here, we must say therefore that ITK supports streaming\u2014but only in\nalgorithms that are \u201cstreamable\u201d in nature. That said, in the spirit of being progressive regarding\nthe remaining algorithms, we should qualify this statement not by claiming that \u201cit is impossible to\nstream such algorithms\u201d, but rather that \u201cour typical approach to streaming is not suitable for these\nalgorithms\u201d at this point, and that hopefully new techniques will be devised by the community in the\nfuture to address these cases.\nLuis Ib\u00e1\u00f1ez and Brad King\n141\n", "page": 155, "type": "text", "section": "Page 155"}
{"text": "9.3\nLessons Learned\nReusability\nThe principle of reusability can also be read as \u201cavoidance of redundancy\u201d. In the case of ITK, this\nhas been achieved with a three-pronged approach.\n\u2022 First, the adoption of object-oriented programming, and in particular the proper creation of\nclass hierarchies where common functionalities are factorized in base classes.\n\u2022 Second, the adoption of generic programming, implemented via the heavy use of C++ templates,\nfactorizing behaviors that are identi\ufb01ed as patterns.\n\u2022 Third, the generous use of C++ macros has also permitted reuse of standard snippets of code\nthat are needed in myriad places across the toolkit.\nMany of these items may sound like platitudes and appear obvious today, but when ITK develop-\nment started in 1999 some of them were not that obvious. In particular, at the time the support most\nC++ compilers o\ufb00ered for templates did not quite follow a consistent standard. Even today, decisions\nsuch as the adoption of generic programming and the use of a widely templated implementation\ncontinue to be controversial in the community. This is manifested in the communities that prefer to\nuse ITK via the wrapping layers to Python, Tcl or Java.\nGeneric Programming\nThe adoption of generic programming was one of the de\ufb01ning implementation features of ITK. It\nwas a di\ufb03cult decision in 1999 when the compiler support for C++ templates was rather fragmented,\nand the Standard Template Library (STL) was still considered a bit exotic.\nGeneric programming was adopted in ITK by embracing the use of C++ templates for imple-\nmenting generalization of concepts and in this way increasing code reuse. The typical example of\nC++ template parameterization in ITK is the Image class, that can be instantiated in the following\nway:\ntypedef unsigned char PixelType;\nconst unsigned int Dimension = 3;\ntypedef itk::Image< PixelType, Dimension > ImageType;\nImageType::Pointer image = ImageType::New();\nIn this expression, the application developer chooses the type to be used to represent pixels in\nthe image, as well as the dimension of the image as a grid in space. In this particular example, we\nchose to use an 8-bit pixel represented in an unsigned char type, for a 3D image. Thanks to the\nunderlying generic implementation, it is possible to instantiate images of any pixel type and any\ndimension in ITK.\nTo make it possible to write these expressions, ITK developers had to implement the Image\nclass by being very careful with the assumptions made about the pixel type. Once the application\ndeveloper has instantiated the image type, the developer can create objects of that type, or proceed to\ninstantiate image \ufb01lters whose types, in turn, depend on the image type. For example:\ntypedef itk::MedianImageFilter< ImageType, ImageType> FilterType;\nFilterType::Pointer median = FilterType::New();\nThe algorithmic speci\ufb01city of di\ufb00erent image \ufb01lters restricts the actual pixel types that they\ncan support. For example, some image \ufb01lters expect the image pixel type to be an integer scalar\n142\nITK\n", "page": 156, "type": "text", "section": "Page 156"}
{"text": "type while some other \ufb01lters expect the pixel type to be a vector of \ufb02oating point numbers. When\ninstantiated with inappropriate pixel types, these \ufb01lters will produce compilation errors or will\nresult in erroneous computational results. To prevent incorrect instantiations and to facilitate the\ntroubleshooting of compilation errors, ITK adopted the use of concept checking that is based on\nforcing the exercise of certain expected features of the types, with the goal of producing early failures\ncombined with human-readable error messages.\nC++ templates are also exploited in certain sections of the toolkit in the form of Template\nMetaprogramming, with the goal of increasing run-time speed performance of the code, in particular\nfor unrolling loops that control the computation of low-dimensional vectors and matrices. Ironically,\nwe have found over time that certain compilers have become smarter at \ufb01guring out when to unroll\nloops, and no longer need the help of Template MetaProgramming expressions in some cases.\nKnowing When to Stop\nThere is also the general risk of doing \u201ctoo much of a good thing\u201d, meaning, there is a risk of\noverusing templates, or overusing macros. It is easy to go overboard and end up creating a new\nlanguage on top of C++ that is essentially based on the use of templates and macros. This is a \ufb01ne\nline, and it demands continuous attention from the development team to make sure that the language\nfeatures are properly used without being abused.\nAs a concrete example, the widespread use of explicitly naming types via C++ typedefs has\nproved to be particularly important. This practice plays two roles: on the one hand it provides a\nhuman-readable informative name describing the nature of the type and its purpose; on the other\nhand, it ensures that the type is used consistently across the toolkit. As an example, during the\nrefactoring of the toolkit for its 4.0 version, a massive e\ufb00ort was invested in collecting the cases where\nC++ integer types such as int, unsigned int, long and unsigned long were used and to replace\nthem with types named after the proper concept that the associated variables were representing. This\nwas the most costly part of the task of ensuring that the toolkit was able to take advantage of 64-bit\ntypes for managing images larger than four gigabytes in all platforms. This task was of the utmost\nimportance for promoting the use of ITK in the \ufb01elds of microscopy and remote sensing, where\nimage of tens of gigabytes in size are common.\nMaintainability\nThe architecture satis\ufb01es the constraints that minimize maintenance cost.\n\u2022 Modularity (at the class level)\n\u2022 Many small \ufb01les\n\u2022 Code reuse\n\u2022 Repeated patterns\nThese characteristics reduce maintenance cost in the following ways:\n\u2022 Modularity (at the class level) makes it possible to enforce test-driven development techniques\nat the image \ufb01lter level, or in general the ITK class level. Stringent testing discipline applied to\nsmall and modular pieces of code has the advantage of reducing the pools of code where bugs\ncan hide, and with the natural decoupling that results from modularization, it is a lot easier to\nlocate defects and eliminate them.\n\u2022 Many small \ufb01les facilitate the assignment of portions of the code to speci\ufb01c developers, and\nsimplify the tracking of defects when they are associated with speci\ufb01c commits in the revision\nLuis Ib\u00e1\u00f1ez and Brad King\n143\n", "page": 157, "type": "text", "section": "Page 157"}
{"text": "control system. The discipline of keeping small \ufb01les also leads to the enforcement of the\ngolden rule of functions and classes: Do one thing, and do it right.\n\u2022 Code reuse: When code is reused (instead of being copy-pasted and reimplemented) the code\nitself bene\ufb01ts from the higher level of scrutiny that results from being exercised in many\ndi\ufb00erent circumstances. It leads more eyes to look at the code, or at least at the e\ufb00ects of\nthe code, and so the code bene\ufb01ts from Linus\u2019s Law: \u201cGiven enough eyeballs, all bugs are\nshallow.\u201d\n\u2022 Repeated patterns simplify the work of maintainers, who in reality account for more than 75%\nof the cost of software development over the lifetime of a project. Using coding patterns that\nare consistently repeated in di\ufb00erent places in the code makes it a lot easier for a developer to\nopen a \ufb01le and quickly understand what the code is doing, or what it is intended to do.\nAs the developers got involved in regular maintenance activities they were exposed to some\n\u201ccommon failures\u201d, in particular:\n\u2022 Assumptions that some \ufb01lters make regarding speci\ufb01c pixel types for their input or output\nimages, but that are not enforced via types or concept checking, and that are not speci\ufb01ed in\nthe documentation.\n\u2022 Not writing for readability. This is one of the most common challenges for any software whose\nnew algorithm implementations originate in the research community. It is common in that\nenvironment to write code that \u201cjust works\u201d, and to forget that the purpose of code is not just\nto be executed at run time, but to be easily read by the next developer. Typical good rules\nof \u201cclean code\u201d writing\u2014for example, write small functions that do one thing and one thing\nonly (the Single Responsibility Principle and the Principle of Least Surprise), adhere to proper\nnaming of variables and functions\u2014tend to be ignored when researchers are excited about\ngetting their new shiny algorithm to work.\n\u2022 Ignoring failure cases and error management. It is common to focus on the \u201cnice cases\u201d of data\nprocessing and to fail to provide code for managing all the cases that can go wrong. Adopters\nof the toolkit quickly run into such cases once they start developing and deploying applications\nin the real world.\n\u2022 Insu\ufb03cient testing. It requires a lot of discipline to follow the practice of test-driven develop-\nment, especially the notion of writing the tests \ufb01rst and only implementing functionalities as\nyou test them. It is almost always the case that bugs in the code are hiding behind the cases\nthat were skipped while implementing the testing code.\nThanks to the communication practices of open source communities, many of these items end\nup being exposed through questions that are commonly asked in the mailing lists, or are directly\nreported as bugs by users. After dealing with many such issues, developers learn to write code that is\n\u201cgood for maintenance\u201d. Some of these traits apply to both coding style and the actual organization\nof the code. It is our view that a developer only reaches mastery after spending some time\u2014at least\na year\u2014doing maintenance and getting exposed to \u201call the things that can go wrong\u201d.\nThe Invisible Hand\nSoftware should look like it was written by a single person. The best developers are the ones who\nwrite code that, should they be hit by the proverbial bus, can be taken over by anybody else. We have\ngrown to recognize that any trace of a \u201cpersonal touch\u201d is an indication of a defect introduced in the\nsoftware.\nIn order to enforce and promote code style uniformity, the following tools have proved to be very\ne\ufb00ective:\n144\nITK\n", "page": 158, "type": "text", "section": "Page 158"}
{"text": "\u2022 KWStyle5 for automatic source code style checking. This is a simpli\ufb01ed C++ parser that checks\ncoding style and \ufb02ags any violations.\n\u2022 Gerrit6 for regular code reviews. This tools serves two purposes: On one hand, it prevents im-\nmature code from entering the code base by distilling its errors, de\ufb01ciencies and imperfections\nduring iterative review cycles where other developers contribute to improve the code. On the\nother hand, it provides a virtual training camp in which new developers get to learn from more\nexperienced developers (read \u201cexperienced\u201d as have made all the mistakes and know where\nthe bodies are buried...) how to improve the code and avoid known problems that have been\nobserved during maintenance cycles.\n\u2022 Git hooks that enforce the use of the KWStyle and Gerrit and that also perform some of their\nown checks. For example, ITK uses Git hooks that prevent commits of code with tabs or with\ntrailing blank spaces.\n\u2022 The team has also explored the use of Uncrustify7 as a tool for enforcing a consistent style.\nIt is worth emphasizing that uniformity of style is not a simple matter of aesthetic appeal, it is\nreally a matter of economics. Studies on the Total Cost of Ownership (TCO) of software projects\nhave estimated that in the life-cycle of a project, the cost of maintenance will be about 75% of the\nTCO, and given that maintenance cost is applied on an annual basis, it typically surpasses the cost of\ninitial development costs by the \ufb01rst \ufb01ve years of the life-cycle of a software project8. Maintenance\nis estimated to be about 80% of what software developers actually do, and when engaged in that\nactivity the large majority of the developer\u2019s time is dedicated to reading someone else\u2019s code, trying\nto \ufb01gure out what it was supposed to do9. Uniform style does wonders for reducing the time it takes\nfor developers to immerse themselves in a newly open source \ufb01le and understand the code before\nthey make any modi\ufb01cations to it. By the same token, it reduces the chances that developers will\nmisinterpret the code and make modi\ufb01cations that end up introducing new bugs when they were\nhonestly trying to \ufb01x old bugs10.\nThe key for making these tools e\ufb00ective is to make sure that they are:\n\u2022 Available to all developers, hence our preference for Open Source tools.\n\u2022 Run on a regular basis. In the case of ITK, these tools have been integrated in the Nightly and\nContinuous Dashboard builds managed by CDash11.\n\u2022 Run as close as possible to the point where the code is being written, so that deviations can be\n\ufb01xed immediately, and so developers quickly learn what kind of practices break style rules.\nRefactoring\nITK started in 2000 and grew continuously until 2010. In 2011, thanks to an infusion of federal\nfunding investment, the development team had the truly unique opportunity to embark on a refactoring\ne\ufb00ort. The funding was provided by the National Library of Medicine as part of the initiative of the\nAmerican Recovery and Reinvestment Act (ARRA). This was not a minor undertaking. Imagine you\nhave been working on a piece of software for over a decade, and you are o\ufb00ered the opportunity to\nclean it up; what would you change?\n5http://public.kitware.com/KWStyle\n6http://code.google.com/p/gerrit\n7http://uncrustify.sourceforge.net\n8\u201cSoftware Development Cost Estimating Handbook\u201d, Volume I, Naval Center for Cost Analysis, Air Force Cost Analysis\nAgency, 2008.\n9\u201cClean Code, A Handbook of Agile Software Craftsmanship\u201d, Robert C. Martin, Prentice Hall, 2009\n10\u201cThe Art of Readable Code\u201d, Dustin Boswell, Trevor Foucher, O\u2019Reilly, 2012\n11http://www.cdash.org/CDash/index.php?project=Insight\nLuis Ib\u00e1\u00f1ez and Brad King\n145\n", "page": 159, "type": "text", "section": "Page 159"}
{"text": "This opportunity for widespread refactoring is very rare. For the previous ten years, we had\nrelied on the daily e\ufb00ort of performing small local refactorings, cleaning up speci\ufb01c corners of the\ntoolkit as we ran into them. This continuous process of clean up and improvement takes advantage\nof the massive collaboration of open source communities, and it is safely enabled by the testing\ninfrastructure driven by CDash, which regularly exercises about 84% of the code in the toolkit. Note\nthat in contrast, the average code coverage of software industry testing is estimated to be only 50%.\nAmong the many things that were changed in the refactoring e\ufb00ort, the ones that are most relevant\nto the architecture are:\n\u2022 Modularization was introduced in the toolkit\n\u2022 Integer types were standardized\n\u2022 Typedefs were \ufb01xed to allow management of images larger than 4 GB on all platforms\n\u2022 The software process was revised:\n\u2013 Migrated from CVS to Git\n\u2013 Introduced code review with Gerrit\n\u2013 Introduced testing on demand with CDash@home\n\u2013 Improved method for downloading data for unit testing\n\u2022 Deprecated support for obsolete compilers\n\u2022 Improved support for many IO image \ufb01le formats including:\n\u2013 DICOM\n\u2013 JPEG2000\n\u2013 TIFF (BigTIFF)\n\u2013 HDF5\n\u2022 Introduced a framework for supporting GPU computation\n\u2022 Introduced support for video processing\n\u2013 Added a bridge to OpenCV\n\u2013 Added a bridge to VXL\nMaintenance based on incremental modi\ufb01cations\u2014tasks such as adding features to an image \ufb01lter,\nimproving performance of a given algorithm, addressing bug reports, and improving documentation\nof speci\ufb01c image \ufb01lters\u2014works \ufb01ne for the local improvement of speci\ufb01c C++ classes. However, a\nmassive refactoring is needed for infrastructure modi\ufb01cations that a\ufb00ect a large number of classes\nacross the board, such as the ones listed above. For example, the set of changes needed to support\nimages larger than 4 GB was probably one of the largest patches ever applied to ITK. It required the\nmodi\ufb01cation of hundreds of classes and could not have been done incrementally without incurring in\na great deal of pain. The modularization is another example of a task that could not have been done\nincrementally. It truly a\ufb00ected the entire organization of the toolkit, how its testing infrastructure\nworks, how testing data is managed, how the toolkit is packaged and distributed, and how new\ncontributions will be encapsulated to be added to the toolkit in the future.\nReproducibility\nOne of the early lessons learned in ITK was that the many papers published in the \ufb01eld were not\nas easy to implement as we were led to believe. The computational \ufb01eld tends to over-celebrate\nalgorithms and to dismiss the practical work of writing software as \u201cjust an implementation detail\u201d.\nThat dismissive attitude is quite damaging to the \ufb01eld, since it diminishes the importance of the\n\ufb01rst-hand experience with the code and its proper use. The outcome is that most published papers\nare simply not reproducible, and when researchers and students attempt to use such techniques they\n146\nITK\n", "page": 160, "type": "text", "section": "Page 160"}
{"text": "end up spending a lot of time in the process and deliver variations of the original work. It is actually\nquite di\ufb03cult, in practice, to verify if an implementation matches what was described in a paper.\nITK disrupted, for the good, that environment and restored a culture of DIY to a \ufb01eld that had\ngrown accustomed to theoretical reasoning, and that had learned to dismiss experimental work. The\nnew culture brought by ITK is a practical and pragmatic one in which the virtues of the software are\njudged by its practical results and not by the appearance of complexity that is celebrated in some\nscienti\ufb01c publications. It turns out that in practice the most e\ufb00ective processing methods are those\nthat would appear to be too simple to be accepted for a scienti\ufb01c paper.\nThe culture of reproducibility is a continuation of the philosophy of test driven development, and\nsystematically results in better software; higher clarity, readability, robustness and focus.\nIn order to \ufb01ll the gap of lack of reproducible publications, the ITK community created the\nInsight Journal12. It is an open-access, fully online publication in which contributions are required to\ninclude code, data, parameters, and tests in order to enable veri\ufb01cation by reproducibility. Articles are\npublished online less than 24 hours after submission. Then they are made available for peer-review by\nany member of the community. Readers get full access to all the materials accompanying the article,\nnamely source code, data, parameters, and testing scripts. The Journal has provided a productive\nspace for sharing new code contributions which from there make their way into the code base. The\nJournal recently received its 500th article, and continues to be used as the o\ufb03cial gateway for new\ncode to be added to ITK.\n12http://www.insight-journal.org\n147\n", "page": 161, "type": "text", "section": "Page 161"}
{"text": "148\nITK\n", "page": 162, "type": "text", "section": "Page 162"}
{"text": "[chapter10]\nGNU Mailman\nBarry Warsaw\nGNU Mailman1 is free software for managing mailing lists. Almost everybody who writes or uses\nfree and open source software has encountered a mailing list. Mailing lists can be discussion-based or\nannouncement-based, with all kinds of variations in between. Sometimes mailing lists are gatewayed\nto newsgroups on Usenet or similar services such as Gmane2. Mailing lists typically have archives\nwhich contain the historical record of all the messages that have been posted to the mailing list.\nGNU Mailman has been around since the early 1990s, when John Viega wrote the \ufb01rst version\nto connect fans with the nascent Dave Matthews Band, the members of which he was friends with in\ncollege. This early version came to the attention of the Python community in the mid-\u201990s, when the\ncenter of the Python universe had moved from CWI3, a scienti\ufb01c research institute in the Netherlands,\nto CNRI4, the Corporation for National Research Initiatives in Reston, Virginia, USA. At CNRI\nwe were running various Python-related mailing lists using Majordomo, a Perl-based mailing list\nmanager. Of course, it just wouldn\u2019t do for the Python world to be maintaining so much Perl code.\nMore importantly, because of its design, we found that modifying Majordomo for our purposes (such\nas to add minimal anti-spam measures) was too di\ufb03cult.\nKen Manheimer was instrumental in much of the early GNU Mailman work, and many excellent\ndevelopers have contributed to Mailman since then. Today, Mark Sapiro is maintaining the stable 2.1\nbranch, while Barry Warsaw, the author of this chapter, concentrates on the new 3.0 version.\nMany of the original architectural decisions John made have lived on in the code right up until the\nMailman 3 branch, and can still be seen in the stable branch. In the sections that follow, I\u2019ll describe\nsome of the more problematic design decisions in Mailman 1 and 2, and how we\u2019ve addressed them\nin Mailman 3.\nIn the early Mailman 1 days, we had a lot of problems with messages getting lost, or bugs causing\nmessages to be re-delivered over and over again. This prompted us to articulate two overriding\nprinciples that are critical to Mailman\u2019s ongoing success:\n\u2022 No message should ever be lost.\n\u2022 No message should ever be delivered more than once.\nIn Mailman 2 we re-designed the message handling system to ensure that these two principles\nwould always be of prime importance. This part of the system has been stable for at least a decade\n1http://www.list.org\n2http://gmane.org/\n3http://www.cwi.nl/\n4http://www.cnri.reston.va.us/\n", "page": 163, "type": "text", "section": "Page 163"}
{"text": "now, and is one of the key reasons that Mailman is as ubiquitous as it is today. Despite modernizing\nthis subsystem in Mailman 3, the design and implementation remains largely unchanged.\n10.1\nThe Anatomy of a Message\nOne of the core data structures in Mailman is the email message, represented by a message object.\nMany of the interfaces, functions, and methods in the system take three arguments: the mailing list\nobject, the message object, and a metadata dictionary that is used to record and communicate state\nwhile a message is processed through the system.\nFigure 10.1: A MIME multipart/mixed message containing text, images, and an audio \ufb01le\nOn the face of it, an email message is a simple object. It consists of a number of colon-separated\nkey-value pairs, called the headers, followed by an empty line which separates the headers from\nthe message body. This textural representation should be easy to parse, generate, reason about, and\nmanipulate, but in fact it quickly gets quite complicated. There are countless RFCs that describe\nall the variations that can occur, such as handling complex data types like images, audio, and more.\nEmail can contain ASCII English, or just about any language and character set in existence. The\n150\nGNU Mailman\n", "page": 164, "type": "text", "section": "Page 164"}
{"text": "basic structure of an email message has been borrowed over and over again for other protocols,\nsuch as NNTP and HTTP, yet each is slightly di\ufb00erent. Our work on Mailman has spawned several\nlibraries just to deal with the vagaries of this format (often called \u201cRFC822\u201d for the founding\n1982 IETF standard5). The email libraries originally developed for use by GNU Mailman have\nfound their way into the Python standard library, where development continues to make them more\nstandards-compliant and robust.\nEmail messages can act as containers for other types of data, as de\ufb01ned in the various MIME\nstandards. A container message part can encode an image, some audio, or just about any type of\nbinary or text data, including other container parts. In mail reader applications, these are known as\nattachments. Figure 10.1 shows the structure of a complex MIME message. The boxes with solid\nborders are the container parts, the boxes with dashed borders are Base64 encoded binary data, and\nthe box with a dotted border is a plain text message.\nContainer parts can also be arbitrarily nested; these are called multiparts and can in fact get quite\ndeep. But any email message, regardless of its complexity, can be modeled as a tree with a single\nmessage object at its root. Within Mailman, we often refer to this as the message object tree, and we\npass this tree around by reference to the root message object. Figure 10.2 shows the object tree of\nthe multipart message in Figure 10.1.\nFigure 10.2: Message object tree of a complex MIME email message\nMailman will almost always modify the original message in some way. Sometimes the trans-\nformations can be fairly benign, such as adding or removing headers. Sometimes we\u2019ll completely\nchange the structure of the message object tree, such as when the content \ufb01lter removes certain\ncontent types like HTML, images, or other non-text parts. Mailman might even collapse \u201cmulti-\npart/alternatives\u201d, where a message appears as both plain text and as some rich text type, or add\nadditional parts containing information about the mailing list itself.\nMailman generally parses the on the wire bytes representation of a message just once, when it\n\ufb01rst comes into the system. From then on, it deals only with the message object tree until it\u2019s ready to\nsend it back out to the outgoing mail server. It\u2019s at that point that Mailman \ufb02attens the tree back into\na bytes representation. Along the way, Mailman pickles6 the message object tree for quick storage\nto, and reconstruction from, the \ufb01le system. Pickles are a Python technology for serializing any\n5http://www.faqs.org/rfcs/rfc822.html\n6http://docs.python.org/library/pickle.html\nBarry Warsaw\n151\n", "page": 165, "type": "text", "section": "Page 165"}
{"text": "Python object, including all its subobjects, to a byte stream, and it\u2019s perfectly suited to optimizing\nthe handling of email message object trees. Unpickling is deserializing this byte stream back into a\nlive object. By storing these byte streams in a \ufb01le, Python programs gain low-cost persistence.\n10.2\nThe Mailing List\nThe mailing list is obviously another core object in the Mailman system, and most of the operations\nin Mailman are mailing list-centric, such as:\n\u2022 Membership is de\ufb01ned in terms of a user or address being subscribed to a speci\ufb01c mailing list.\n\u2022 Mailing lists have a large number of con\ufb01guration options that are stored in the database, and\nwhich control everything from posting privileges to how messages are modi\ufb01ed before \ufb01nal\ndelivery.\n\u2022 Mailing lists have owners and moderators which have greater permission to change aspects of\nthe list, or to approve and reject questionable postings.\n\u2022 Every mailing list has its own archive.\n\u2022 Users post new messages to a speci\ufb01c mailing list.\nand so on. Almost every operation in Mailman takes a mailing list as an argument\u2014it\u2019s that\nfundamental. Mailing list objects have undergone a radical redesign in Mailman 3 to make them\nmore e\ufb03cient and to expand their \ufb02exibility.\nOne of John\u2019s earliest design decisions was how to represent a mailing list object inside the\nsystem. For this central data type, he chose a Python class with multiple base classes, each of\nwhich implements a small part of the mailing list\u2019s responsibility. These cooperating base classes,\ncalled mixin classes, were a clever way to organize the code so that it was easy to add entirely\nnew functionality. By grafting on a new mixin base class, the core MailList class could easily\naccommodate something new and cool.\nFor example, to add an auto-responder to Mailman 2, a mixin class was created that held the data\nspeci\ufb01c to that feature. The data would get automatically initialized when a new mailing list was\ncreated. The mixin class also provided the methods necessary to support the auto-responder feature.\nThis structure was even more useful when it came to the design of the mailing MailList object\u2019s\npersistence.\nAnother of John\u2019s early design decisions was to use Python pickles for storing MailList state\npersistence.\nIn Mailman 2, the MailList object\u2019s state is stored in a \ufb01le called config.pck, which is just\nthe pickled representation of the MailList object\u2019s dictionary. Every Python object has an attribute\ndictionary called __dict__. So saving a mailing list object then is simply a matter of pickling its\n__dict__ to a \ufb01le, and loading it just involves reading the pickle from the \ufb01le and reconstituting its\n__dict__.\nThus, when a new mixin class was added to implement some new functionality, all the attributes\nof the mixin were automatically pickled and unpickled appropriately. The only extra work we had\nto do was to maintain a schema version number to automatically upgrade older mailing list objects\nwhen new attributes were added via the mixin, since the pickled representation of older MailList\nobjects would be missing the new attributes.\nAs convenient as this was, both the mixin architecture and pickle persistence eventually crumbled\nunder their own weight. Site administrators often requested ways to access the mailing list con\ufb01gura-\ntion variables via external, non-Python systems. But the pickle protocol is entirely Python-speci\ufb01c,\nso sequestering all that useful data inside a pickle wouldn\u2019t work for them. Also, because the entire\n152\nGNU Mailman\n", "page": 166, "type": "text", "section": "Page 166"}
{"text": "state of a mailing list was contained in the config.pck, and Mailman has multiple processes that\nneed to read, modify, and write the mailing list state, we had to implement exclusive \ufb01le-based and\nNFS-safe locks to ensure data consistency. Every time some part of Mailman wants to change the\nstate of a mailing list, it must acquire the lock, write out the change, then release the lock. Even read\noperations can require a re-load of the list\u2019s config.pck \ufb01le, since some other process may have\nchanged it before the read operation. This serialization of operations on a mailing list turned out to\nbe horribly slow and ine\ufb03cient.\nFor these reasons, Mailman 3 stores all of its data in a SQL database. By default SQLite3 is\nused, though this is easily changed since Mailman 3 uses the Object Relational Mapper called Storm,\nwhich supports a wide variety of databases. PostgreSQL support was added with just a few lines of\ncode, and a site administrator can enable it by changing one con\ufb01guration variable.\nAnother, bigger problem is that in Mailman 2, each mailing list is a silo. Often operations span\nacross many mailing lists, or even all of them. For example, a user might want to temporarily suspend\nall their subscriptions when they go on vacation. Or a site administrator might want to add some\ndisclaimer to the welcome message of all of the mailing lists on her system. Even the simple matter\nof \ufb01guring out which mailing lists a single address is subscribed to required unpickling the state of\nevery mailing list on the system, since membership information was kept in the config.pck \ufb01le too.\nAnother problem was that each config.pck \ufb01le lived in a directory named after the mailing list,\nbut Mailman was originally designed without consideration for virtual domains. This lead to a very\nunfortunate problem where two mailing lists could not have the same name in di\ufb00erent domains.\nFor example, if you owned both the example.com and example.org domains, and you wanted them\nto act independently and allow for a di\ufb00erent support mailing list in each, you cannot do this in\nMailman 2, without modi\ufb01cations to the code, a barely supported hook, or conventional workarounds\nthat forced a di\ufb00erent list name under the covers, which is the approach used by large sites such as\nSourceForge.\nThis has been solved in Mailman 3 by changing the way mailing lists are identi\ufb01ed, along with mov-\ning all the data into a traditional database. The primary key for the mailing list table is the fully qual-\ni\ufb01ed list name or as you\u2019d probably recognize it, the posting address. Thus support@example.com\nand support@example.org are now completely independent rows in the mailing list table, and can\neasily co-exist in a single Mailman system.\n10.3\nRunners\nMessages \ufb02ow through the system by way of a set of independent processes called runners. Originally\nconceived as a way of predictably processing all the queued message \ufb01les found in a particular\ndirectory, there are now a few runners which are simply independent, long-running processes that\nperform a speci\ufb01c task and are managed by a master process; more on that later. When a runner does\nmanage \ufb01les in a directory, it is called a queue runner.\nMailman is religiously single-threaded, even though there is signi\ufb01cant parallelism to exploit. For\nexample, Mailman can accept messages from the mail server at the same time it\u2019s sending messages\nout to recipients, or processing bounces, or archiving a message. Parallelism in Mailman is achieved\nthrough the use of multiple processes, in the form of these runners. For example, there is an incoming\nqueue runner with the sole job of accepting (or rejecting) messages from the upstream mail server.\nThere is an outgoing queue runner with the sole job of communicating with the upstream mail server\nover SMTP in order to send messages out to the \ufb01nal recipients. There\u2019s an archiver queue runner,\na bounce processing queue runner, a queue runner for forwarding messages to an NNTP server, a\nBarry Warsaw\n153\n", "page": 167, "type": "text", "section": "Page 167"}
{"text": "runner for composing digests, and several others. Runners which don\u2019t manage a queue include a\nLocal Mail Transfer Protocol (LMTP)7 server and an administrative HTTP server.\nEach queue runner is responsible for a single directory, i.e., its queue. While the typical Mailman\nsystem can perform perfectly well with a single process per queue, we use a clever algorithm for\nallowing parallelism within a single queue directory, without requiring any kind of cooperation or\nlocking. The secret is in the way we name the \ufb01les within the queue directory.\nAs mentioned above, every message that \ufb02ows through the system is also accompanied by a\nmetadata dictionary that accumulates state and allows independent components of Mailman to\ncommunicate with each other. Python\u2019s pickle library is able to serialize and deserialize multiple\nobjects to a single \ufb01le, so we can pickle both the message object tree and metadata dictionary into\none \ufb01le.\nThere is a core Mailman class called Switchboard which provides an interface for enqueuing\n(i.e., writing) and dequeuing (i.e., reading) the message object tree and metadata dictionary to \ufb01les\nin a speci\ufb01c queue directory. Every queue directory has at least one switchboard instance, and every\nqueue runner instance has exactly one switchboard.\nPickle \ufb01les all end in the .pck su\ufb03x, though you may also see .bak, .tmp, and .psv \ufb01les in a\nqueue. These are used to ensure the two sacrosanct tenets of Mailman: no \ufb01le should ever get lost,\nand no message should ever be delivered more than once. But things usually work properly and these\n\ufb01les can be pretty rare.\nAs indicated, for really busy sites Mailman supports running more than one runner process per\nqueue directory, completely in parallel, with no communication between them or locking necessary\nto process the \ufb01les. It does this by naming the pickle \ufb01les with a SHA1 hash, and then allowing a\nsingle queue runner to manage just a slice of the hash space. So if a site wants to run two runners on\nthe bounces queue, one would process \ufb01les from the top half of the hash space, and the other would\nprocess \ufb01les from the bottom half of the hash space. The hashes are calculated using the contents of\nthe pickled message object tree, the name of the mailing list that the message is destined for, and\na time stamp. The SHA1 hashes are e\ufb00ectively random, and thus on average a two-runner queue\ndirectory will have about equal amounts of work per process. And because the hash space can be\nstatically divided, these processes can operate on the same queue directory with no interference or\ncommunication necessary.\nThere\u2019s an interesting limitation to this algorithm. Since the splitting algorithm allots one or\nmore bits of the hash to each space, the number of runners per queue directory must be a power of 2.\nThis means there can be 1, 2, 4, or 8 runner processes per queue, but not, for example, 5. In practice\nthis has never been a problem, since few sites will ever need more than 4 processes to handle their\nload.\nThere\u2019s another side e\ufb00ect of this algorithm that did cause problems during the early design of\nthis system. Despite the unpredictability of email delivery in general, the best user experience is\nprovided by processing the queue \ufb01les in FIFO order, so that replies to a mailing list get sent out in\nroughly chronological order. Not making a best e\ufb00ort attempt at doing so can cause confusion for\nmembers. But using SHA1 hashes as \ufb01le names obliterates any timestamps, and for performance\nreasons stat() calls on queue \ufb01les, or unpickling the contents (e.g., to read a time stamp in the\nmetadata) should be avoided.\nMailman\u2019s solution was to extend the \ufb01le naming algorithm to include a time stamp pre\ufb01x, as\nthe number of seconds since the epoch (e.g., <timestamp>+<sha1hash>.pck). Each loop through\nthe queue runner starts by doing an os.listdir(), which returns all the \ufb01les in the queue directory.\n7http://tools.ietf.org/html/rfc2033\n154\nGNU Mailman\n", "page": 168, "type": "text", "section": "Page 168"}
{"text": "Then for each \ufb01le, it splits the \ufb01le name and ignores any \ufb01le names where the SHA1 hash doesn\u2019t\nmatch its slice of responsibility. The runner then sorts the remaining \ufb01les based on the timestamp\npart of the \ufb01le name. It\u2019s true that with multiple queue runners each managing di\ufb00erent slices of the\nhash space, this could lead to ordering problems between the parallel runners, but in practice, the\ntimestamp ordering is enough to preserve end-user perception of best-e\ufb00ort sequential delivery.\nIn practice this has worked extremely well for at least a decade, with only the occasional minor\nbug \ufb01x or elaboration to handle obscure corner cases and failure modes. It\u2019s one of the most stable\nparts of Mailman and was largely ported untouched from Mailman 2 to Mailman 3.\n10.4\nThe Master Runner\nWith all these runner processes, Mailman needed a simple way to start and stop them consistently;\nthus the master watcher process was born. It must be able to handle both queue runners and runners\nwhich do not manage a queue. For example, in Mailman 3, we accept messages from the incoming\nupstream mail server via LMTP, which is a protocol similar to SMTP, but which operates only for\nlocal delivery and thus can be much simpler as it doesn\u2019t need to deal with the vagaries of delivering\nmail over an unpredictable Internet. The LMTP runner simply listens on a port, waiting for its\nupstream mail server to connect and send it a byte stream. It then parses this byte stream into a\nmessage object tree, creates an initial metadata dictionary, and enqueues this into a processing queue\ndirectory.\nMailman also has a runner that listens on another port and processes REST requests over HTTP.\nThis process doesn\u2019t handle queue \ufb01les at all.\nA typical running Mailman system might have eight or ten processes, and they all need to be\nstopped and started appropriately and conveniently. They can also crash occasionally; for example,\nwhen a bug in Mailman causes an unexpected exception to occur. When this happens, the message\nbeing delivered is shunted to a holding area, with the state of the system at the time of the exception\npreserved in the message metadata. This ensures that an uncaught exception does not cause multiple\ndeliveries of the message. In theory, the Mailman site administrator could \ufb01x the problem, and\nthen unshunt the o\ufb00ending messages for redelivery, picking up where it left o\ufb00. After shunting the\nproblematic message, the master restarts the crashed queue runner, which begins processing the\nremaining messages in its queue.\nWhen the master watcher starts, it looks in a con\ufb01guration \ufb01le to determine how many and which\ntypes of child runners to start. For the LMTP and REST runners, there is usually a single process.\nFor the queue runners, as mentioned above, there can be a power-of-2 number of parallel processes.\nThe master fork()s and exec()s all the runner processes based on the con\ufb01guration \ufb01le, passing in\nthe appropriate command line arguments to each (e.g., to tell the subprocess which slice of the hash\nspace to look at). Then the master basically sits in an in\ufb01nite loop, blocking until one of its child\nprocesses exits. It keeps track of the process ID for each child, along with a count of the number of\ntimes the child has been restarted. This count prevents a catastrophic bug from causing a cascade of\nunstoppable restarts. There\u2019s a con\ufb01guration variable which speci\ufb01es how many restarts are allowed,\nafter which an error is logged and the runner is not restarted.\nWhen a child does exit, the master looks at both the exit code and the signal that killed the\nsubprocess. Each runner process installs a number of signal handlers with the following semantics:\n\u2022 SIGTERM: intentionally stop the subprocess. It is not restarted. SIGTERM is what init will kill\nthe process with when changing run levels, and it\u2019s also the signal that Mailman itself uses to\nstop the subprocess.\nBarry Warsaw\n155\n", "page": 169, "type": "text", "section": "Page 169"}
{"text": "\u2022 SIGINT: also used to intentionally stop the subprocess, it\u2019s the signal that occurs when control-C\nis used in a shell. The runner is not restarted.\n\u2022 SIGHUP: tells the process to close and reopen its log \ufb01les, but to keep running. This is used\nwhen rotating log \ufb01les.\n\u2022 SIGUSR1: initially stop the subprocess, but allow the master to restart the process. This is used\nin the restart command of init scripts.\nThe master also responds to all four of these signals, but it doesn\u2019t do much more than forward\nthem to all its subprocesses. So if you sent SIGTERM to the master, all the subprocesses would get\nSIGTERM\u2019d and exit. The master would know that the subprocess exited because of SIGTERM and it\nwould know that this was an intentional stoppage, so it would not restart the runner.\nTo ensure that only one master is running at any time, it acquires a lock with a lifetime of about a\nday and a half. The master installs a SIGALRM handler, which wakes the master up once per day so\nthat it can refresh the lock. Because the lock\u2019s lifetime is longer than the wake up interval, the lock\nshould never time out or be broken while Mailman is running, unless of course the system crashes or\nthe master is killed with an uncatchable signal. In those cases, the command line interface to the\nmaster process provides an option to override a stale lock.\nThis leads to the last bit of the master watcher story, the command line interface to it. The\nactual master script takes very few command line options. Both it and the queue runner scripts are\nintentionally kept simple. This wasn\u2019t the case in Mailman 2, where the master script was fairly\ncomplex and tried to do too much, which made it more di\ufb03cult to understand and debug. In Mailman\n3, the real command line interface for the master process is in the bin/mailman script, a kind of\nmeta-script that contains a number of subcommands, in a style made popular by programs like\nSubversion. This reduces the number of programs that need to be installed on your shell\u2019s PATH.\nbin/mailman has subcommands to start, stop, and restart the master, as well as all the subprocesses,\nand also to cause all the log \ufb01les to be reopened. The start subcommand fork()s and exec()s\nthe master process, while the others simply send the appropriate signal to the master, which then\npropagates it to its subprocesses as described above. This improved separation of responsibility\nmake it much easier to understand each individual piece.\n10.5\nRules, Links, and Chains\nA mailing list posting goes through several phases from the time it\u2019s \ufb01rst received until the time it\u2019s\nsent out to the list\u2019s membership. In Mailman 2, each processing step was represented by a handler,\nand a string of handlers were put together into a pipeline. So, when a message came into the system,\nMailman would \ufb01rst determine which pipeline would be used to process it, and then each handler in\nthe pipeline would be called in turn. Some handlers would do moderation functions (e.g., \u201cIs this\nperson allowed to post to the mailing list?\u201d), others would do modi\ufb01cation functions (e.g., \u201cWhich\nheaders should I remove or add?\u201d), and others would copy the message to other queues. A few\nexamples of the latter are:\n\u2022 A message accepted for posting would be copied to the archiver queue at some point, so that\nits queue runner would add the message to the archive.\n\u2022 A copy of the message eventually had to end up in the outgoing queue so that it could be\ndelivered to the upstream mail server, which has the ultimate responsibility of delivery to a list\nmember.\n\u2022 A copy of the message had to get put into a digest for people who wanted only occasional,\nregular tra\ufb03c from the list, rather than an individual message whenever someone sent it.\n156\nGNU Mailman\n", "page": 170, "type": "text", "section": "Page 170"}
{"text": "The pipeline-of-handlers architecture proved to be quite powerful. It provided an easy way that\npeople could extend and modify Mailman to do custom operations. The interface for a handler was\nfairly straightforward, and it was a simple matter to implement a new handler, ensuring it got added\nto the right pipeline in the right location to accomplish the custom operation.\nOne problem with this was that mixing moderation and modi\ufb01cation in the same pipeline became\nproblematic. The handlers had to be sequenced in the pipeline just so, or unpredictable or undesirable\nthings would happen. For example, if the handler that added the RFC 23698 List-* headers came\nafter the handler to copy the message to the digest collator, then folks receiving digests would get\nincorrect copies of the list posts. In di\ufb00erent cases, it might be bene\ufb01cial to moderate the message\nbefore or after modifying it. In Mailman 3, the moderation and modi\ufb01cation operations have been\nsplit into separate subsystems for better control over the sequencing.\nAs described previously, the LMTP runner parses an incoming byte stream into a message object\ntree and creates an initial metadata dictionary for the message. It then enqueues these to one or\nanother queue directory. Some messages may be email commands (e.g., to join or leave a mailing\nlist, to get automated help, etc.) which are handled by a separate queue. Most messages are postings\nto the mailing list, and these get put in the incoming queue. The incoming queue runner processes\neach message sequentially through a chain consisting of any number of links. There is a built-in\nchain that most mailing lists use, but even this is con\ufb01gurable.\nFigure 10.3 illustrates the default set of chains in the Mailman 3 system. Each link in the chain is\nillustrated by a rounded rectangle. The built-in chain is where the initial rules of moderation are\napplied to the incoming message, and in this chain, each link is associated with a rule. Rules are\nsimply pieces of code that get passed the three typical parameters: the mailing list, the message\nobject tree, and the metadata dictionary. Rules are not supposed to modify the message; they just\nmake a binary decision and return a Boolean answering the question, \u201cDid the rule match or not?\u201d.\nRules can also record information in the metadata dictionary.\nIn the \ufb01gure, solid arrows indicates message \ufb02ow when the rule matches, while dotted arrows\nindicate message \ufb02ow when the rule does not match. The outcome of each rule is recorded in\nthe metadata dictionary so that later on, Mailman will know (and be able to report) exactly which\nrules matched and which ones missed. The dashed arrows indication transitions which are taken\nunconditionally, regardless of whether the rule matches or not.\nIt\u2019s important to note that the rules themselves do not dispatch based on outcome. In the built-in\nchain, each link is associated with an action which is performed when the rule matches. So for\nexample, when the \u201cloop\u201d rule matches (meaning, the mailing list has seen this message before), the\nmessage is immediate handed o\ufb00to the \u201cdiscard\u201d chain, which throws the message away after some\nbookkeeping. If the \u201cloop\u201d rule does not match, the next link in the chain will process the message.\nIn Figure 10.3, the links associated with \u201cadministrivia\u201d, \u201cmax-size\u201d, and \u201ctruth\u201d rules have\nno binary decision. In case of the \ufb01rst two, this is because their action is deferred, so they simply\nrecord the match outcome and processing continues to the next link. The \u201cany\u201d rule then matches if\nany previous rule matches. This way, Mailman can report on all the reasons why a message is not\nallowed to be posted, instead of just the \ufb01rst reason. There are several more such rules not illustrated\nhere for simplicity.\nThe \u201ctruth\u201d rule is a bit di\ufb00erent. It\u2019s always associated with the last link in the chain, and it\nalways matches. With the combination of the penultimate \u201cany\u201d rule sweeping aside all previously\nmatching messages, the last link then knows that any message making it this far is allowed to be\nposted to the mailing list, so it unconditionally moves the message to the \u201caccept\u201d chain.\n8http://www.faqs.org/rfcs/rfc2369.html\nBarry Warsaw\n157\n", "page": 171, "type": "text", "section": "Page 171"}
{"text": "Figure 10.3: Simpli\ufb01ed view of default chains with their links\nThere are a few other details of chain processing not described here, but the architecture is very\n\ufb02exible and extensible so that just about any type of message processing can be implemented, and\nsites can customize and extend rules, links, and chains.\nWhat happens to the message when it hits the \u201caccept\u201d chain? The message, which is now\ndeemed appropriate for the mailing list, is sent o\ufb00to the pipeline queue for some modi\ufb01cations\nbefore it is delivered to the end recipients. This process is described in more detail in the following\nsection.\nThe \u201chold\u201d chain puts the message into a special bucket for the human moderator to review.\nThe \u201cmoderation\u201d chain does a little additional processing to decide whether the message should\nbe accepted, held for moderator approval, discarded, or rejected. In order to reduce clutter in the\ndiagram, the \u201creject\u201d chain, which is used to bounce messages back to the original sender, is not\nillustrated.\n158\nGNU Mailman\n", "page": 172, "type": "text", "section": "Page 172"}
{"text": "10.6\nHandlers and Pipelines\nOnce a message has made its way through the chains and rules and is accepted for posting, the\nmessage must be further processed before it can be delivered to the \ufb01nal recipients. For example, some\nheaders may get added or deleted, and some messages may get some extra decorations that provide\nimportant disclaimers or information, such as how to leave the mailing list. These modi\ufb01cations\nare performed by a pipeline which contains a sequence of handlers. In a manner similar to chains\nand rules, pipelines and handlers are extensible, but there are a number of built-in pipelines for the\ncommon cases. Handlers have a similar interface as rules, accepting a mailing list, message object,\nand metadata dictionary. However, unlike rules, handlers can and do modify the message. Figure 10.4\nillustrates the default pipeline and set of handlers (some handlers are omitted for simplicity).\nFigure 10.4: Pipeline queue handlers\nFor example, a posted message needs to have a Precedence: header added, which tells other\nautomated software that this message came from a mailing list. This header is a de facto standard to\nprevent vacation programs from responding back to the mailing list. Adding this header (among other\nheader modi\ufb01cations) is done by the \u201cadd headers\u201d handler. Unlike rules, handler order generally\ndoesn\u2019t matter, and messages always \ufb02ow through all handlers in the pipeline.\nSome handlers send copies of the message to other queues. As shown in Figure 10.4, there is a\nhandler that makes a copy of the message for folks who want to receive digests. Copies are also sent\nBarry Warsaw\n159\n", "page": 173, "type": "text", "section": "Page 173"}
{"text": "to the archive queue for eventual delivery to the mailing list archives. Finally, the message is copied\nto the outgoing queue for \ufb01nal delivery to the mailing list\u2019s members.\n10.7\nVERP\n\u201cVERP\u201d stands for \u201cVariable Envelope Return Path\u201d, and it is a well-known technique9 that mailing\nlists use to unambiguously determine bouncing recipient addresses. When an address on a mailing\nlist is no longer active, the recipient\u2019s mail server will send a noti\ufb01cation back to the sender. In the\ncase of a mailing list, you want this bounce to go back to the mailing list, not to the original author\nof the message; the author can\u2019t do anything about the bounce, and worse, sending the bounce back\nto the author can leak information about who is subscribed to the mailing list. When the mailing\nlist gets the bounce, however, it can do something useful, such as disable the bouncing address or\nremove it from the list\u2019s membership.\nThere are two general problems with this. First, even though there is a standard format for these\nbounces10 (called delivery status noti\ufb01cations) many deployed mail servers do not conform to it.\nInstead, the body of their bounce messages can contain just about any amount of di\ufb03cult-to-machine-\nparse gobbledygook, which makes automated parsing di\ufb03cult. In fact, Mailman uses a library that\ncontains dozens of bounce format heuristics, all of which have been seen in the wild during the 15\nyears of Mailman\u2019s existence.\nSecond, imagine the situation where a member of a mailing list has several forwards. She\nmight be subscribed to the list with her anne@example.com address, but this might forward to\nperson@example.org, which might further forward the message to me@example.net. When the\n\ufb01nal destination server at example.net receives the message, it will usually just send a bounce\nsaying that me@example.net is no longer valid. But the Mailman server that sent the message only\nknows the member as anne@example.com, so a bounce \ufb02agging me@example.net will not contain\na subscribed address, and Mailman will ignore it.\nAlong comes VERP, which exploits a requirement of the fundamental SMTP protocol11 to\nprovide unambiguous bounce detection, by returning such bounce messages to the envelope sender.\nThis is not the From: \ufb01eld in the message body, but in fact the MAIL FROM value set during the SMTP\ndialog. This is preserved along the delivery route, and the ultimate receiving mail server is required,\nby the standards, to send the bounces to this address. Mailman uses this fact to encode the original\nrecipient email address into the MAIL FROM value.\nIf the Mailman server is mylist@example.org, then the VERP-encoded envelope sender for a\nmailing list posting sent to anne@example.com will be:\nmylist-bounce+anne=example.com@example.org\nHere, the + is a local address separator, which is a format supported by most modern mail servers.\nSo when the bounce comes back, it will actually be delivered to mylist-bounce@example.com but\nwith the To: header still set to VERP-encoded recipient address. Mailman can then parse this To:\nheader to decode the original recipient as anne@example.com.\nWhile VERP is an extremely powerful tool for culling bad addresses from the mailing list, it does\nhave one potentially important disadvantage. Using VERP requires that Mailman send out exactly\none copy of the message per recipient. Without VERP, Mailman can bundle up identical copies of an\n9http://cr.yp.to/proto/verp.txt\n10http://www.faqs.org/rfcs/rfc5337.html\n11http://www.faqs.org/rfcs/rfc5321.html\n160\nGNU Mailman\n", "page": 174, "type": "text", "section": "Page 174"}
{"text": "outgoing message for multiple recipients, thus reducing overall bandwidth and processing time. But\nVERP requires a unique MAIL FROM for each recipient, and the only way to do that is to send a unique\ncopy of the message. Generally this is an acceptable trade-o\ufb00, and in fact, once these individualized\nmessages are being sent for VERP anyway, there are a lot of useful things Mailman can also do. For\nexample, it can embed a URL in the footer of the message customized for each recipient which gives\nthem a direct link to unsubscribe from the list. You could even imagine various types of mail-merge\noperations for customizing the body of the message for each individual recipient.\n10.8\nREST\nOne of the key architectural changes in Mailman 3 addresses a common request over the years: to\nallow Mailman to be more easily integrated with external systems. When I was hired by Canonical,\nthe corporate sponsor of the Ubuntu project, in 2007 my job was originally to add mailing lists to\nLaunchpad, a collaboration and hosting platform for software projects. I knew that Mailman 2 could\ndo the job, but there was a requirement to use Launchpad\u2019s web user interface instead of Mailman\u2019s\ndefault user interface. Since Launchpad mailing lists were almost always going to be discussion\nlists, we wanted very little variability in the way they operated. List administrators would not need\nthe plethora of options available in the typical Mailman site, and what few options they would need\nwould be exposed through the Launchpad web user interface.\nAt the time, Launchpad was not free software (this changed in 2009), so we had to design the\nintegration in such a way that Mailman 2\u2019s GPLv2 code could not infect Launchpad. This led\nto a number of architectural decisions during that integration design that were quite tricky and\nsomewhat ine\ufb03cient. Because Launchpad is now free software licensed under the AGPLv3, these\nhacks wouldn\u2019t be necessary today, but having to do it this way did provide some very valuable\nlessons on how a web-user-interface-less Mailman could be integrated with external systems. The\nvision that emerged was of a core engine that implemented mailing list operations e\ufb03ciently and\nreliably, and that could be managed by any kind of web front-end, including ones written in Zope,\nDjango, or PHP, or with no web user interface at all.\nThere were a number of technologies at the time that would allow this, and in fact Mailman\u2019s\nintegration with Launchpad is based on XMLRPC. But XMLRPC has a number of problems that\nmake it a less-than-ideal protocol.\nMailman 3 has adopted the Representational State Transfer (REST) model for external admin-\nistrative control. REST is based on HTTP, and Mailman\u2019s default object representation is JSON.\nThese protocols are ubiquitous and well-supported in a large variety of programming languages and\nenvironments, making it fairly easy to integrate Mailman with third party systems. REST was the\nperfect \ufb01t for Mailman 3, and now much of its functionality is exposed through a REST API.\nThis is a powerful paradigm that more applications should adopt: deliver a core engine that\nimplements its basic functionality well, exposing a REST API to query and control it. The REST API\nprovides yet another way of integrating with Mailman, the others being utilizing the command line\ninterface, and writing Python code to access the internal API. This architecture is extremely \ufb02exible\nand can be used and integrated in ways that are beyond the initial vision of the system designers.\nNot only does this design allow for much greater choices for deployment, but it even allowed the\no\ufb03cial components of the system to be designed and implemented independently. For example, the\nnew o\ufb03cial web user interface for Mailman 3 is technically a separate project with its own code base,\ndriven primarily by experienced web designers. These outstanding developers are empowered to\nmake decisions, create designs, and execute implementations without the core engine development\nBarry Warsaw\n161\n", "page": 175, "type": "text", "section": "Page 175"}
{"text": "being a bottleneck. The web user interface work feeds back into the core engine implementation\nby requesting additional functionality, exposed through the REST API, but they needn\u2019t wait for it,\nsince they can mock up the server side on their end and continue experimenting and developing the\nweb user interface while the core engine catches up.\nWe plan to use the REST API for many more things, including allowing the scripting of common\noperations and integration with IMAP or NNTP servers for alternative access to the archives.\n10.9\nInternationalization\nGNU Mailman was one of the \ufb01rst Python programs to embrace internationalization. Of course,\nbecause Mailman does not usually modify the contents of email messages posted through it, those\nmessages can be in any language of the original author\u2019s choosing. However, when interacting\ndirectly with Mailman, either through the web interface or via email commands, users would prefer\nto use their own natural language.\nMailman pioneered many of the internationalization technologies used in the Python world, but it\nis actually much more complex than most applications. In a typical desktop environment, the natural\nlanguage is chosen when the user logs in, and remains static throughout the desktop session. However,\nMailman is a server application, so it must be able to handle dozens of languages, separate from the\nlanguage of the system on which it runs. In fact, Mailman must somehow determine the language\ncontext that a response is to be returned under, and translate its text to that language. Sometimes a\nresponse may even involve multiple languages; for example, if a bounce message from a Japanese\nuser is to be forwarded to list administrators who speak German, Italian, and Catalan.\nAgain, Mailman pioneered some key Python technologies to handle complex language contexts\nsuch as these. It utilizes a library that manages a stack of languages which can be pushed onto\nand popped from as the context changes, even within the processing of a single message. It also\nimplements an elaborate scheme for customizing its response templates based on site preferences, list\nowner preferences, and language choice. For example, if a list owner wants to customize a response\ntemplate for one of her lists, but only for Japanese users, she would place the speci\ufb01c template in the\nappropriate place on the \ufb01le system, and this would override more generic defaults.\n10.10\nLessons Learned\nWhile this article has provided an overview of Mailman 3\u2019s architecture and insight into how that\narchitecture has evolved over the 15 years of its existence (through three major rewrites), there are\nlots of other interesting architectural decisions in Mailman which I can\u2019t cover. These include the\ncon\ufb01guration subsystem, the testing infrastructure, the database layer, the programmatic use of\nformal interfaces, archiving, mailing list styles, the email commands and command-line interface,\nand integration with the outgoing mail server. Contact us on the mailman-developers mailing list12\nif you\u2019re interested in more details.\nHere are some lessons we\u2019ve learned while rewriting a popular, established, and stable piece of\nthe open source ecosystem.\n\u2022 Use test driven development (TDD). There really is no other way! Mailman 2 largely lacks an\nautomated test suite, and while it\u2019s true that not all of the Mailman 3 code base is covered by\nits test suite, most of it is, and all new code is required to be accompanied by tests, using either\n12http://mail.python.org/mailman/listinfo/mailman-developers\n162\nGNU Mailman\n", "page": 176, "type": "text", "section": "Page 176"}
{"text": "unittests or doctests. Doing TDD is the only way to gain the con\ufb01dence that the changes\nyou make today do not introduce regressions in existing code. Yes, TDD can sometimes take\nlonger, but think of it as an investment in the future quality of your code. In that way, not\nhaving a good test suite means you\u2019re just wasting your time. Remember the mantra: untested\ncode is broken code.\n\u2022 Get your bytes/strings story straight from the beginning. In Python 3, a sharp distinction is\nmade between Unicode text strings and byte arrays, which, while initially painful, is a huge\nbene\ufb01t to writing correct code. Python 2 blurred this line by having both Unicode and 8-bit\nASCII strings, with some automated coercions between them. While appearing to be a useful\nconvenience, problems with this fuzzy line are the number one cause of bugs in Mailman 2.\nThis is not helped by the fact that email is notoriously di\ufb03cult to classify into strings and\nbytes. Technically, the on-the-wire representation of an email is as a sequence of bytes, but\nthese bytes are almost always ASCII, and there is a strong temptation to manipulate message\ncomponents as text. The email standards themselves describe how human-readable, non-ASCII\ntext can be safely encoded, so even things like \ufb01nding a Re: pre\ufb01x in a Subject: header will\nbe text operations, not byte operations. Mailman\u2019s principle is to convert all incoming data\nfrom bytes to Unicode as early as possible, deal with the text as Unicode internally, and only\nconvert it back to bytes on the way out. It\u2019s critical to be crystal clear from the start when\nyou\u2019re dealing with bytes and when you\u2019re dealing with text, since it\u2019s very di\ufb03cult to retro\ufb01t\nthis fundamental model shift later.\n\u2022 Internationalize your application from the start. Do you want your application to be used only\nby the minority of the world that speaks English? Think about how many fantastic users this\nignores! It\u2019s not hard to set up internationalization, and there are lots of good tools for making\nthis easy, many of which were pioneered in Mailman. Don\u2019t worry about the translations to\nstart with; if your application is accessible to the world\u2019s wealth of languages, you will have\nvolunteer translators knocking down your door to help.\nGNU Mailman is a vibrant project with a healthy user base, and lots of opportunities for contri-\nbutions. Here are some resources you can use if you think you\u2019d like to help us out, which I hope\nyou do!\nPrimary web site\nhttp://www.list.org\nProject wiki\nhttp://wiki.list.org\nDeveloper mailing list\nmailman-developers@python.org\nUsers mailing list\nmailman-users@python.org\nFreenode IRC channel\n#mailman\nA Final Note\nWhile this chapter was being written, we learned with sadness of the passing of Tokio Kikuchi13, a\nJapanese professor who contributed heavily to Mailman, and was especially knowledgeable about\ninternationalization and the idiosyncrasies of Japanese mail user agents. He will be greatly missed.\n13http://wiki.list.org/display/COM/TokioKikuchi\n163\n", "page": 177, "type": "text", "section": "Page 177"}
{"text": "164\nGNU Mailman\n", "page": 178, "type": "text", "section": "Page 178"}
{"text": "[chapter11]\nmatplotlib\nJohn Hunter and Michael Droettboom\nmatplotlib is a Python-based plotting library with full support for 2D and limited support for 3D\ngraphics, widely used in the Python scienti\ufb01c computing community. The library targets a broad\nrange of use cases. It can embed graphics in the user interface toolkit of your choice, and currently\nsupports interactive graphics on all major desktop operating systems using the GTK+, Qt, Tk, FLTK,\nwxWidgets and Cocoa toolkits. It can be called interactively from the interactive Python shell to\nproduce graphics with simple, procedural commands, much like Mathematica, IDL or MATLAB.\nmatplotlib can also be embedded in a headless webserver to provide hardcopy in both raster-based\nformats like Portable Network Graphics (PNG) and vector formats like PostScript, Portable Document\nFormat (PDF) and Scalable Vector Graphics (SVG) that look great on paper.\n11.1\nThe Dongle Problem\nmatplotlib\u2019s origin dates to an attempt by one of us (John Hunter) to free himself and his fellow\nepilepsy researchers from a proprietary software package for doing electrocorticography (ECoG)\nanalysis. The laboratory in which he worked had only one license for the software, and the various\ngraduate students, medical students, postdocs, interns, and investigators took turns sharing the\nhardware key dongle. MATLAB is widely used in the biomedical community for data analysis\nand visualization, so Hunter set out, with some success, to replace the proprietary software with a\nMATLAB-based version that could be utilized and extended by multiple investigators. MATLAB,\nhowever, naturally views the world as an array of \ufb02oating point numbers, and the complexities of\nreal-world hospital records for epilepsy surgery patients with multiple data modalities (CT, MRI,\nECoG, EEG) warehoused on di\ufb00erent servers pushed MATLAB to its limits as a data management\nsystem. Unsatis\ufb01ed with the suitability of MATLAB for this task, Hunter began working on a new\nPython application built on top of the user interface toolkit GTK+, which was at the time the leading\ndesktop windowing system for Linux.\nmatplotlib was thus originally developed as an EEG/ECoG visualization tool for this GTK+\napplication, and this use case directed its original architecture. matplotlib was originally designed to\nserve a second purpose as well: as a replacement for interactive command-driven graphics generation,\nsomething that MATLAB does very well. The MATLAB design makes the simple task of loading a\ndata \ufb01le and plotting very straightforward, where a full object-oriented API would be too syntactically\nheavy. So matplotlib also provides a stateful scripting interface for quick and easy generation of\ngraphics similar to MATLAB\u2019s. Because matplotlib is a library, users have access to all of the rich\nbuilt-in Python data structures such as lists, dictionaries, sets and more.\n", "page": 179, "type": "text", "section": "Page 179"}
{"text": "Figure 11.1: The original matplotlib application: an ECoG viewer\n11.2\nOverview of matplotlib Architecture\nThe top-level matplotlib object that contains and manages all of the elements in a given graphic\nis called the Figure. One of the core architectural tasks matplotlib must solve is implementing a\nframework for representing and manipulating the Figure that is segregated from the act of rendering\nthe Figure to a user interface window or hardcopy. This enables us to build increasingly sophisticated\nfeatures and logic into the Figures, while keeping the \u201cbackends\u201d, or output devices, relatively\nsimple. matplotlib encapsulates not just the drawing interfaces to allow rendering to multiple devices,\nbut also the basic event handling and windowing of most popular user interface toolkits. Because of\nthis, users can create fairly rich interactive graphics and toolkits incorporating mouse and keyboard\ninput that can be plugged without modi\ufb01cation into the six user interface toolkits we support.\nThe architecture to accomplish this is logically separated into three layers, which can be viewed\nas a stack. Each layer that sits above another layer knows how to talk to the layer below it, but the\nlower layer is not aware of the layers above it. The three layers from bottom to top are: backend,\nartist, and scripting.\n166\nmatplotlib\n", "page": 180, "type": "text", "section": "Page 180"}
{"text": "Backend Layer\nAt the bottom of the stack is the backend layer, which provides concrete implementations of the\nabstract interface classes:\n\u2022 FigureCanvas encapsulates the concept of a surface to draw onto (e.g. \u201cthe paper\u201d).\n\u2022 Renderer does the drawing (e.g. \u201cthe paintbrush\u201d).\n\u2022 Event handles user inputs such as keyboard and mouse events.\nFor a user interface toolkit such as Qt, the FigureCanvas has a concrete implementation which\nknows how to insert itself into a native Qt window (QtGui.QMainWindow), transfer the matplotlib\nRenderer commands onto the canvas (QtGui.QPainter), and translate native Qt events into the\nmatplotlib Event framework, which signals the callback dispatcher to generate the events so upstream\nlisteners can handle them. The abstract base classes reside in matplotlib.backend_bases and\nall of the derived classes live in dedicated modules like matplotlib.backends.backend_qt4agg.\nFor a pure image backend dedicated to producing hardcopy output like PDF, PNG, SVG, or PS, the\nFigureCanvas implementation might simply set up a \ufb01le-like object into which the default headers,\nfonts, and macro functions are de\ufb01ned, as well as the individual objects (lines, text, rectangles, etc.)\nthat the Renderer creates.\nThe job of the Renderer is to provide a low-level drawing interface for putting ink onto the\ncanvas. As mentioned above, the original matplotlib application was an ECoG viewer in a GTK+\napplication, and much of the original design was inspired by the GDK/GTK+ API available at\nthat time. The original Renderer API was motivated by the GDK Drawable interface, which\nimplements such primitive methods as draw_point, draw_line, draw_rectangle, draw_image,\ndraw_polygon, and draw_glyphs. Each additional backend we implemented\u2014the earliest were\nthe PostScript backend and the GD backend\u2014implemented the GDK Drawable API and translated\nthese into native backend-dependent drawing commands. As we discuss below, this unnecessarily\ncomplicated the implementation of new backends with a large proliferation of methods, and this API\nhas subsequently been dramatically simpli\ufb01ed, resulting in a simple process for porting matplotlib to\na new user interface toolkit or \ufb01le speci\ufb01cation.\nOne of the design decisions that has worked quite well for matplotlib is support for a core\npixel-based renderer using the C++ template library Anti-Grain Geometry or \u201cagg\u201d [She06]. This is\na high-performance library for rendering anti-aliased 2D graphics that produces attractive images.\nmatplotlib provides support for inserting pixel bu\ufb00ers rendered by the agg backend into each user\ninterface toolkit we support, so one can get pixel-exact graphics across UIs and operating systems.\nBecause the PNG output matplotlib produces also uses the agg renderer, the hardcopy is identical to\nthe screen display, so what you see is what you get across UIs, operating systems and PNG output.\nThe matplotlib Event framework maps underlying UI events like key-press-event or\nmouse-motion-event to the matplotlib classes KeyEvent or MouseEvent. Users can connect to\nthese events to callback functions and interact with their \ufb01gure and data; for example, to pick a data\npoint or group of points, or manipulate some aspect of the \ufb01gure or its constituents. The following\ncode sample illustrates how to toggle all of the lines in an Axes window when the user types \u2018t\u2019.\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef on_press(event):\nif event.inaxes is None: return\nfor line in event.inaxes.lines:\nif event.key==\u2019t\u2019:\nJohn Hunter and Michael Droettboom\n167\n", "page": 181, "type": "text", "section": "Page 181"}
{"text": "visible = line.get_visible()\nline.set_visible(not visible)\nevent.inaxes.figure.canvas.draw()\nfig, ax = plt.subplots(1)\nfig.canvas.mpl_connect(\u2019key_press_event\u2019, on_press)\nax.plot(np.random.rand(2, 20))\nplt.show()\nThe abstraction of the underlying UI toolkit\u2019s event framework allows both matplotlib developers\nand end-users to write UI event-handling code in a \u201cwrite once run everywhere\u201d fashion. For example,\nthe interactive panning and zooming of matplotlib \ufb01gures that works across all user interface toolkits\nis implemented in the matplotlib event framework.\nArtist Layer\nThe Artist hierarchy is the middle layer of the matplotlib stack, and is the place where much of the\nheavy lifting happens. Continuing with the analogy that the FigureCanvas from the backend is the\npaper, the Artist is the object that knows how to take the Renderer (the paintbrush) and put ink on\nthe canvas. Everything you see in a matplotlib Figure is an Artist instance; the title, the lines, the\ntick labels, the images, and so on all correspond to individual Artist instances (see Figure 11.3).\nThe base class is matplotlib.artist.Artist, which contains attributes that every Artist shares:\nthe transformation which translates the artist coordinate system to the canvas coordinate system\n(discussed in more detail below), the visibility, the clip box which de\ufb01nes the region the artist can\npaint into, the label, and the interface to handle user interaction such as \u201cpicking\u201d; that is, detecting\nwhen a mouse click happens over the artist.\nFigure 11.2: A \ufb01gure\n168\nmatplotlib\n", "page": 182, "type": "text", "section": "Page 182"}
{"text": "\u0001\u0002\u0003\u0004\u0005\u0006\u0007\u0001\n\b\t\u0006\n\u0007\u0002\n\u000b\u0002\f\u0006\r\u000e\u0007\u0003\n\u000f\u0006\t\u0010\u0007\u0004\n\u0011\b\t\u0002\n\u0007\u0005\n\u0012\b\t\u0002\n\u0007\u0006\n\t\u0013\u0014\u0015\u0016\u0006\u0014\n\u0017\u0013\u0014\u0015\u0016\u0006\u0014\n\t\u0013\u0010\u0002\u0018\u0019\n\u0010\u0002\u0018\u0019\n\u0007\u001a\n\u0017\u0013\u0010\u0002\u0018\u0019\n\u0010\u0002\u0018\u0019\n\u0007\u001a\nFigure 11.3: The hierarchy of artist instances used to draw Figure 11.2.\nThe coupling between the Artist hierarchy and the backend happens in the draw method. For\nexample, in the mockup class below where we create SomeArtist which subclasses Artist, the\nessential method that SomeArtist must implement is draw, which is passed a renderer from the\nbackend. The Artist doesn\u2019t know what kind of backend the renderer is going to draw onto (PDF,\nSVG, GTK+ DrawingArea, etc.) but it does know the Renderer API and will call the appropriate\nmethod (draw_text or draw_path). Since the Renderer has a pointer to its canvas and knows how\nto paint onto it, the draw method transforms the abstract representation of the Artist to colors in a\npixel bu\ufb00er, paths in an SVG \ufb01le, or any other concrete representation.\nclass SomeArtist(Artist):\n\u2019An example Artist that implements the draw method\u2019\ndef draw(self, renderer):\n\"\"\"Call the appropriate renderer methods to paint self onto canvas\"\"\"\nif not self.get_visible():\nreturn\n# create some objects and use renderer to draw self here\nrenderer.draw_path(graphics_context, path, transform)\nThere are two types of Artists in the hierarchy. Primitive artists represent the kinds of objects you\nsee in a plot: Line2D, Rectangle, Circle, and Text. Composite artists are collections of Artists\nsuch as the Axis, Tick, Axes, and Figure. Each composite artist may contain other composite\nartists as well as primitive artists. For example, the Figure contains one or more composite Axes\nand the background of the Figure is a primitive Rectangle.\nThe most important composite artist is the Axes, which is where most of the matplotlib API\nplotting methods are de\ufb01ned. Not only does the Axes contain most of the graphical elements that\nmake up the background of the plot\u2014the ticks, the axis lines, the grid, the patch of color which is\nthe plot background\u2014it contains numerous helper methods that create primitive artists and add them\nto the Axes instance. For example, Table 11.1 shows a small sampling of Axes methods that create\nplot objects and store them in the Axes instance.\nJohn Hunter and Michael Droettboom\n169\n", "page": 183, "type": "text", "section": "Page 183"}
{"text": "method\ncreates\nstored in\nAxes.imshow\none or more matplotlib.image.AxesImages\nAxes.images\nAxes.hist\nmany matplotlib.patch.Rectangles\nAxes.patches\nAxes.plot\none or more matplotlib.lines.Line2Ds\nAxes.lines\nTable 11.1: Sampling of Axes methods and the Artist instances they create\nBelow is a simple Python script illustrating the architecture above. It de\ufb01nes the backend,\nconnects a Figure to it, uses the array library numpy to create 10,000 normally distributed random\nnumbers, and plots a histogram of these.\n# Import the FigureCanvas from the backend of your choice\n#\nand attach the Figure artist to it.\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\nfrom matplotlib.figure import Figure\nfig = Figure()\ncanvas = FigureCanvas(fig)\n# Import the numpy library to generate the random numbers.\nimport numpy as np\nx = np.random.randn(10000)\n# Now use a figure method to create an Axes artist; the Axes artist is\n#\nadded automatically to the figure container fig.axes.\n# Here \"111\" is from the MATLAB convention: create a grid with 1 row and 1\n#\ncolumn, and use the first cell in that grid for the location of the new\n#\nAxes.\nax = fig.add_subplot(111)\n# Call the Axes method hist to generate the histogram; hist creates a\n#\nsequence of Rectangle artists for each histogram bar and adds them\n#\nto the Axes container.\nHere \"100\" means create 100 bins.\nax.hist(x, 100)\n# Decorate the figure with a title and save it.\nax.set_title(\u2019Normal distribution with $\\mu=0,\\ \\sigma=1$\u2019)\nfig.savefig(\u2019matplotlib_histogram.png\u2019)\nScripting Layer (pyplot)\nThe script using the API above works very well, especially for programmers, and is usually the\nappropriate programming paradigm when writing a web application server, a UI application, or\nperhaps a script to be shared with other developers. For everyday purposes, particularly for interactive\nexploratory work by bench scientists who are not professional programmers, it is a bit syntactically\nheavy. Most special-purpose languages for data analysis and visualization provide a lighter scripting\ninterface to simplify common tasks, and matplotlib does so as well in its matplotlib.pyplot\ninterface. The same code above, using pyplot, reads\nimport matplotlib.pyplot as plt\nimport numpy as np\n170\nmatplotlib\n", "page": 184, "type": "text", "section": "Page 184"}
{"text": "x = np.random.randn(10000)\nplt.hist(x, 100)\nplt.title(r\u2019Normal distribution with $\\mu=0, \\sigma=1$\u2019)\nplt.savefig(\u2019matplotlib_histogram.png\u2019)\nplt.show()\nFigure 11.4: A histogram created using pyplot\npyplot is a stateful interface that handles much of the boilerplate for creating \ufb01gures and axes and\nconnecting them to the backend of your choice, and maintains module-level internal data structures\nrepresenting the current \ufb01gure and axes to which to direct plotting commands.\nLet\u2019s dissect the important lines in the script to see how this internal state is managed.\n\u2022 import matplotlib.pyplot as plt: When the pyplot module is loaded, it parses a local\ncon\ufb01guration \ufb01le in which the user states, among many other things, their preference for a\ndefault backend. This might be a user interface backend like QtAgg, in which case the script\nabove will import the GUI framework and launch a Qt window with the plot embedded, or it\nmight be a pure image backend like Agg, in which case the script will generate the hard-copy\noutput and exit.\n\u2022 plt.hist(x, 100): This is the \ufb01rst plotting command in the script. pyplot will check its\ninternal data structures to see if there is a current Figure instance. If so, it will extract the\ncurrent Axes and direct plotting to the Axes.hist API call. In this case there is none, so it\nwill create a Figure and Axes, set these as current, and direct the plotting to Axes.hist.\n\u2022 plt.title(r\u2019Normal distribution with $\\mu=0, \\sigma=1$\u2019): As above, pyplot will\nlook to see if there is a current Figure and Axes. Finding that there is, it will not create new\ninstances but will direct the call to the existing Axes instance method Axes.set_title.\n\u2022 plt.show(): This will force the Figure to render, and if the user has indicated a default GUI\nbackend in their con\ufb01guration \ufb01le, will start the GUI mainloop and raise any \ufb01gures created to\nthe screen.\nJohn Hunter and Michael Droettboom\n171\n", "page": 185, "type": "text", "section": "Page 185"}
{"text": "A somewhat stripped-down and simpli\ufb01ed version of pyplot\u2019s frequently used line plotting\nfunction matplotlib.pyplot.plot is shown below to illustrate how a pyplot function wraps func-\ntionality in matplotlib\u2019s object-oriented core. All other pyplot scripting interface functions follow\nthe same design.\n@autogen_docstring(Axes.plot)\ndef plot(*args, **kwargs):\nax = gca()\nret = ax.plot(*args, **kwargs)\ndraw_if_interactive()\nreturn ret\nThe Python decorator @autogen_docstring(Axes.plot) extracts the documentation string\nfrom the corresponding API method and attaches a properly formatted version to the pyplot.plot\nmethod; we have a dedicated module matplotlib.docstring to handle this docstring magic. The\n*args and **kwargs in the documentation signature are special conventions in Python to mean all\nthe arguments and keyword arguments that are passed to the method. This allows us to forward them\non to the corresponding API method. The call ax = gca() invokes the stateful machinery to \u201cget\ncurrent Axes\u201d (each Python interpreter can have only one \u201ccurrent axes\u201d), and will create the Figure\nand Axes if necessary. The call to ret = ax.plot(*args, **kwargs) forwards the function call\nand its arguments to the appropriate Axes method, and stores the return value to be returned later.\nThus the pyplot interface is a fairly thin wrapper around the core Artist API which tries to avoid\nas much code duplication as possible by exposing the API function, call signature and docstring in\nthe scripting interface with a minimal amount of boilerplate code.\n11.3\nBackend Refactoring\nOver time, the drawing API of the output backends grew a large number of methods, including:\ndraw_arc, draw_image, draw_line_collection, draw_line, draw_lines, draw_point,\ndraw_quad_mesh, draw_polygon_collection, draw_polygon, draw_rectangle,\ndraw_regpoly_collection\nUnfortunately, having more backend methods meant it took much longer to write a new backend,\nand as new features were added to the core, updating the existing backends took considerable work.\nSince each of the backends was implemented by a single developer who was expert in a particular\noutput \ufb01le format, it sometimes took a long time for a new feature to arrive in all of the backends,\ncausing confusion for the user about which features were available where.\nFor matplotlib version 0.98, the backends were refactored to require only the minimum necessary\nfunctionality in the backends themselves, with everything else moved into the core. The number of\nrequired methods in the backend API was reduced considerably, to only:\n\u2022 draw_path: Draws compound polygons, made up of line and B\u00e9zier segments. This interfaces\nreplaces many of the old methods: draw_arc, draw_line, draw_lines, and draw_rectangle.\n\u2022 draw_image: Draws raster images.\n\u2022 draw_text: Draws text with the given font properties.\n\u2022 get_text_width_height_descent: Given a string of text, return its metrics.\n172\nmatplotlib\n", "page": 186, "type": "text", "section": "Page 186"}
{"text": "It\u2019s possible to implement all of the drawing necessary for a new backend using only these\nmethods.1 This is useful for getting a new backend up and running more easily. However, in some\ncases, a backend may want to override the behavior of the core in order to create more e\ufb03cient\noutput. For example, when drawing markers (small symbols used to indicate the vertices in a line\nplot), it is more space-e\ufb03cient to write the marker\u2019s shape only once to the \ufb01le, and then repeat it as\na \u201cstamp\u201d everywhere it is used. In that case, the backend can implement a draw_markers method.\nIf it\u2019s implemented, the backend writes out the marker shape once and then writes out a much shorter\ncommand to reuse it in a number of locations. If it\u2019s not implemented, the core simply draws the\nmarker multiple times using multiple calls to draw_path.\nThe full list of optional backend API methods is:\n\u2022 draw_markers: Draws a set of markers.\n\u2022 draw_path_collection: Draws a collection of paths.\n\u2022 draw_quad_mesh: Draws a quadrilateral mesh.\n11.4\nTransforms\nmatplotlib spends a lot of time transforming coordinates from one system to another. These coordinate\nsystems include:\n\u2022 data: the original raw data values\n\u2022 axes: the space de\ufb01ned by a particular axes rectangle\n\u2022 \ufb01gure: the space containing the entire \ufb01gure\n\u2022 display: the physical coordinates used in the output (e.g. points in PostScript, pixels in PNG)\nEvery Artist has a transformation node that knows how to transform from one coordinate system\nto another. These transformation nodes are connected together in a directed graph, where each node is\ndependent on its parent. By following the edges to the root of the graph, coordinates in data space can\nbe transformed all the way to coordinates in the \ufb01nal output \ufb01le. Most transformations are invertible,\nas well. This makes it possible to click on an element of the plot and return its coordinate in data\nspace. The transform graph sets up dependencies between transformation nodes: when a parent\nnode\u2019s transformation changes, such as when an Axes\u2019s limits are changed, any transformations\nrelated to that Axes are invalidated since they will need to be redrawn. Transformations related to\nother Axes in the \ufb01gure, of course, may be left alone, preventing unnecessary recomputations and\ncontributing to better interactive performance.\nTransform nodes may be either simple a\ufb03ne transformations and non-a\ufb03ne transformations.\nA\ufb03ne transformations are the family of transformations that preserve straight lines and ratios of\ndistances, including rotation, translation, scale and skew. Two-dimensional a\ufb03ne transformations\nare represented using a 3 \u00d7 3 a\ufb03ne transformation matrix. The transformed point (x\u2032, y\u2032) is obtained\nby matrix-multiplying the original point (x, y) by this matrix:\n\uf8ee\n\uf8f0\nx\u2032\ny\u2032\n1\n\uf8f9\n\uf8fb=\n\uf8ee\n\uf8f0\nsx\n\u03b8x\ntx\n\u03b8y\nsy\nty\n0\n0\n1\n\uf8f9\n\uf8fb\n\uf8ee\n\uf8f0\nx\ny\n1\n\uf8f9\n\uf8fb\nTwo-dimensional coordinates can then easily be transformed by simply multiplying them by the\ntransformation matrix. A\ufb03ne transformations also have the useful property that they can be composed\n1We could also go one step further and draw text using draw_path, removing the need for the draw_text method, but we\nhaven\u2019t gotten around to making that simpli\ufb01cation. Of course, a backend would still be free to implement its own draw_text\nmethod to output \u201creal\u201d text.\nJohn Hunter and Michael Droettboom\n173\n", "page": 187, "type": "text", "section": "Page 187"}
{"text": "together using matrix multiplication. This means that to perform a series of a\ufb03ne transformations,\nthe transformation matrices can \ufb01rst be multiplied together only once, and the resulting matrix can\nbe used to transform coordinates. matplotlib\u2019s transformation framework automatically composes\n(freezes) a\ufb03ne transformation matrices together before transforming coordinates to reduce the\namount of computation. Having fast a\ufb03ne transformations is important, because it makes interactive\npanning and zooming in a GUI window more e\ufb03cient.\nFigure 11.5: The same data plotted with three di\ufb00erent non-a\ufb03ne transformations: logarithmic, polar and\nLambert\nNon-a\ufb03ne transformations in matplotlib are de\ufb01ned using Python functions, so they are truly\narbitrary. Within the matplotlib core, non-a\ufb03ne transformations are used for logarithmic scaling,\npolar plots and geographical projections (Figure 11.5). These non-a\ufb03ne transformations can be\nfreely mixed with a\ufb03ne ones in the transformation graph. matplotlib will automatically simplify the\na\ufb03ne portion and only fall back to the arbitrary functions for the non-a\ufb03ne portion.\nFrom these simple pieces, matplotlib can do some pretty advanced things. A blended transforma-\ntion is a special transformation node that uses one transformation for the x axis and another for the y\naxis. This is of course only possible if the given transformations are \u201cseparable\u201d, meaning the x and\ny coordinates are independent, but the transformations themselves may be either a\ufb03ne or non-a\ufb03ne.\nThis is used, for example, to plot logarithmic plots where either or both of the x and y axes may have\na logarithmic scale. Having a blended transformation node allow the available scales to be combined\nin arbitrary ways. Another thing the transform graph allows is the sharing of axes. It is possible to\n\u201clink\u201d the limits of one plot to another and ensure that when one is panned or zoomed, the other is\nupdated to match. In this case, the same transform node is simply shared between two axes, which\nmay even be on two di\ufb00erent \ufb01gures. Figure 11.6 shows an example transformation graph with some\nof these advanced features at work. axes1 has a logarithmic x axis; axes1 and axes2 share the same\ny axis.\n\uf001\uf002\uf003\uf004\uf005\n\uf006\uf005\uf007\uf008\uf009\n\uf00a\uf003\uf00b\uf00c\uf007\n\uf001\uf00d\uf00e\uf007\uf006\uf00c\uf004\uf00f\n\uf006\uf005\uf007\uf008\uf010\n\uf001\uf00d\uf00e\uf007\uf006\uf00c\uf004\uf005\n\uf011\uf001\uf007\uf00e\uf012\uf007\uf012\uf009\n\uf011\uf001\uf007\uf00e\uf012\uf007\uf012\uf010\n\uf012\uf006\uf013\uf006\uf009\n\uf012\uf006\uf013\uf006\uf010\n\uf012\uf00d\uf008\uf014\uf001\uf006\uf00f\nFigure 11.6: An example transformation graph\n174\nmatplotlib\n", "page": 188, "type": "text", "section": "Page 188"}
{"text": "11.5\nThe Polyline Pipeline\nWhen plotting line plots, there are a number of steps that are performed to get from the raw data to\nthe line drawn on screen. In an earlier version of matplotlib, all of these steps were tangled together.\nThey have since been refactored so they are discrete steps in a \u201cpath conversion\u201d pipeline. This\nallows each backend to choose which parts of the pipeline to perform, since some are only useful in\ncertain contexts.\nFigure 11.7: A close-up view of the e\ufb00ect of pixel snapping. On the left, without pixel snapping; on the right,\nwith pixel snapping.\n\u2022 Transformation: The coordinates are transformed from data coordinates to \ufb01gure coordinates.\nIf this is a purely a\ufb03ne transformation, as described above, this is as simple as a matrix\nmultiplication. If this involves arbitrary transformations, transformation functions are called to\ntransform the coordinates into \ufb01gure space.\n\u2022 Handle missing data: The data array may have portions where the data is missing or invalid.\nThe user may indicate this either by setting those values to NaN, or using numpy masked arrays.\nVector output formats, such as PDF, and rendering libraries, such as Agg, do not often have a\nconcept of missing data when plotting a polyline, so this step of the pipeline must skip over\nthe missing data segments using MOVETO commands, which tell the renderer to pick up the pen\nand begin drawing again at a new point.\n\u2022 Clipping: Points outside of the boundaries of the \ufb01gure can increase the \ufb01le size by including\nmany invisible points. More importantly, very large or very small coordinate values can cause\nover\ufb02ow errors in the rendering of the output \ufb01le, which results in completely garbled output.\nThis step of the pipeline clips the polyline as it enters and exits the edges of the \ufb01gure to prevent\nboth of these problems.\n\u2022 Snapping: Perfectly vertical and horizontal lines can look fuzzy due to antialiasing when\ntheir centers are not aligned to the center of a pixel (see Figure 11.7). The snapping step of\nthe pipeline \ufb01rst determines whether the entire polyline is made up of horizontal and vertical\nsegments (such as an axis-aligned rectangle), and if so, rounds each resulting vertex to the\nnearest pixel center. This step is only used for raster backends, since vector backends should\ncontinue to have exact data points. Some renderers of vector \ufb01le formats, such as Adobe\nAcrobat, perform pixel snapping when viewed on screen.\n\u2022 Simpli\ufb01cation: When plotting really dense plots, many of the points on the line may not\nactually be visible. This is particularly true of plots representing a noisy waveform. Including\nthese points in the plot increases \ufb01le size, and may even hit limits on the number of points\nallowed in the \ufb01le format. Therefore, any points that lie exactly on the line between their two\nneighboring points are removed (see Figure 11.8). The determination depends on a threshold\nbased on what would be visible at a given resolution speci\ufb01ed by the user.\nJohn Hunter and Michael Droettboom\n175\n", "page": 189, "type": "text", "section": "Page 189"}
{"text": "Figure 11.8: The \ufb01gure on the right is a close-up of the \ufb01gure on the left. The circled vertex is automatically\nremoved by the path simpli\ufb01cation algorithm, since it lies exactly on the line between its neighboring vertices,\nand therefore is redundant.\n11.6\nMath Text\nSince the users of matplotlib are often scientists, it is useful to put richly formatted math expressions\ndirectly on the plot. Perhaps the most widely used syntax for math expressions is from Donald\nKnuth\u2019s TeX typesetting system. It\u2019s a way to turn input in a plain-text language like this:\n\\sqrt{\\frac{\\delta x}{\\delta y}}\ninto a properly formatted math expression like this:\nq\n\u03b4x\n\u03b4y\nmatplotlib provides two ways to render math expressions. The \ufb01rst, usetex, uses a full copy of\nTeX on the user\u2019s machine to render the math expression. TeX outputs the location of the characters\nand lines in the expression in its native DVI (device independent) format. matplotlib then parses the\nDVI \ufb01le and converts it to a set of drawing commands that one of its output backends then renders\ndirectly onto the plot. This approach handles a great deal of obscure math syntax. However, it\nrequires that the user have a full and working installation of TeX. Therefore, matplotlib also includes\nits own internal math rendering engine, called mathtext.\nmathtext is a direct port of the TeX math-rendering engine, glued onto a much simpler parser\nwritten using the pyparsing [McG07] parsing framework. This port was written based on the\npublished copy of the TeX source code [Knu86]. The simple parser builds up a tree of boxes and\nglue (in TeX nomenclature), that are then laid out by the layout engine. While the complete TeX\nmath rendering engine is included, the large set of third-party TeX and LaTeX math libraries is not.\nFeatures in such libraries are ported on an as-needed basis, with an emphasis on frequently used and\nnon-discipline-speci\ufb01c features \ufb01rst. This makes for a nice, lightweight way to render most math\nexpressions.\n176\nmatplotlib\n", "page": 190, "type": "text", "section": "Page 190"}
{"text": "11.7\nRegression Testing\nHistorically, matplotlib has not had a large number of low-level unit tests. Occasionally, if a serious\nbug was reported, a script to reproduce it would be added to a directory of such \ufb01les in the source\ntree. The lack of automated tests created all of the usual problems, most importantly regressions\nin features that previously worked. (We probably don\u2019t need to sell you on the idea that automated\ntesting is a good thing.) Of course, with so much code and so many con\ufb01guration options and\ninterchangeable pieces (e.g., the backends), it is arguable that low-level unit tests alone would ever\nbe enough; instead we\u2019ve followed the belief that it is most cost-e\ufb00ective to test all of the pieces\nworking together in concert.\nTo this end, as a \ufb01rst e\ufb00ort, a script was written that generated a number of plots exercising\nvarious features of matplotlib, particularly those that were hard to get right. This made it a little\neasier to detect when a new change caused inadvertent breakage, but the correctness of the images\nstill needed to be veri\ufb01ed by hand. Since this required a lot of manual e\ufb00ort, it wasn\u2019t done very\noften.\nAs a second pass, this general approach was automated. The current matplotlib testing script\ngenerates a number of plots, but instead of requiring manual intervention, those plots are automatically\ncompared to baseline images. All of the tests are run inside of the nose testing framework, which\nmakes it very easy to generate a report of which tests failed.\nComplicating matters is that the image comparison cannot be exact. Subtle changes in versions\nof the Freetype font-rendering library can make the output of text slightly di\ufb00erent across di\ufb00erent\nmachines. These di\ufb00erences are not enough to be considered \u201cwrong\u201d, but are enough to throw o\ufb00\nany exact bit-for-bit comparison. Instead, the testing framework computes the histogram of both\nimages, and calculates the root-mean-square of their di\ufb00erence. If that di\ufb00erence is greater than a\ngiven threshold, the images are considered too di\ufb00erent and the comparison test fails. When tests\nfail, di\ufb00erence images are generated which show where on the plot a change has occurred (see\nFigure 11.9). The developer can then decide whether the failure is due to an intentional change and\nupdate the baseline image to match the new image, or decide the image is in fact incorrect and track\ndown and \ufb01x the bug that caused the change.\nFigure 11.9: A regression test image comparison. From left to right: a) The expected image, b) the result of\nbroken legend placement, c) the di\ufb00erence between the two images.\nSince di\ufb00erent backends can contribute di\ufb00erent bugs, the testing framework tests multiple\nbackends for each plot: PNG, PDF and SVG. For the vector formats, we don\u2019t compare the vector\ninformation directly, since there are multiple ways to represent something that has the same end\nresult when rasterized. The vector backends should be free to change the speci\ufb01cs of their output to\nincrease e\ufb03ciency without causing all of the tests to fail. Therefore, for vector backends, the testing\nframework \ufb01rst renders the \ufb01le to a raster using an external tool (Ghostscript for PDF and Inkscape\nfor SVG) and then uses those rasters for comparison.\nJohn Hunter and Michael Droettboom\n177\n", "page": 191, "type": "text", "section": "Page 191"}
{"text": "Using this approach, we were able to bootstrap a reasonably e\ufb00ective testing framework from\nscratch more easily than if we had gone on to write many low-level unit tests. Still, it is not perfect;\nthe code coverage of the tests is not very complete, and it takes a long time to run all of the tests.2\nTherefore, some regressions do still fall through the cracks, but overall the quality of the releases has\nimproved considerably since the testing framework was implemented.\n11.8\nLessons Learned\nOne of the important lessons from the development of matplotlib is, as Le Corbusier said, \u201cGood\narchitects borrow\u201d. The early authors of matplotlib were largely scientists, self-taught programmers\ntrying to get their work done, not formally trained computer scientists. Thus we did not get the\ninternal design right on the \ufb01rst try. The decision to implement a user-facing scripting layer largely\ncompatible with the MATLAB API bene\ufb01ted the project in three signi\ufb01cant ways: it provided a\ntime-tested interface to create and customize graphics, it made for an easy transition to matplotlib\nfrom the large base of MATLAB users, and\u2014most importantly for us in the context of matplotlib\narchitecture\u2014it freed developers to refactor the internal object-oriented API several times with\nminimal impact to most users because the scripting interface was unchanged. While we have had\nAPI users (as opposed to scripting users) from the outset, most of them are power users or developers\nable to adapt to API changes. The scripting users, on the other hand, can write code once and pretty\nmuch assume it is stable for all subsequent releases.\nFor the internal drawing API, while we did borrow from GDK, we did not spend enough\ne\ufb00ort determining whether this was the right drawing API, and had to expend considerable e\ufb00ort\nsubsequently after many backends were written around this API to extend the functionality around\na simpler and more \ufb02exible drawing API. We would have been well-served by adopting the PDF\ndrawing speci\ufb01cation [Ent11b], which itself was developed from decades of experience Adobe had\nwith its PostScript speci\ufb01cation; it would have given us mostly out-of-the-box compatibility with PDF\nitself, the Quartz Core Graphics framework, and the Enthought Enable Kiva drawing kit [Ent11a].\nOne of the curses of Python is that it is such an easy and expressive language that developers\noften \ufb01nd it easier to re-invent and re-implement functionality that exists in other packages than work\nto integrate code from other packages. matplotlib could have bene\ufb01ted in early development from\nexpending more e\ufb00ort on integration with existing modules and APIs such as Enthought\u2019s Kiva and\nEnable toolkits which solve many similar problems, rather than reinventing functionality. Integration\nwith existing functionality is, however, a double-edged sword, as it can make builds and releases\nmore complex and reduce \ufb02exibility in internal development.\n2Around 15 minutes on a 2.33 GHz Intel Core 2 E6550.\n178\nmatplotlib\n", "page": 192, "type": "text", "section": "Page 192"}
{"text": "[chapter12]\nMediaWiki\nSumana Harihareswara and Guillaume Paumier\nFrom the start, MediaWiki was developed speci\ufb01cally to be Wikipedia\u2019s software. Developers\nhave worked to facilitate reuse by third-party users, but Wikipedia\u2019s in\ufb02uence and bias have shaped\nMediaWiki\u2019s architecture throughout its history.\nWikipedia is one of the top ten websites in the world, currently getting about 400 million unique\nvisitors a month. It gets over 100,000 hits per second. Wikipedia isn\u2019t commercially supported by\nads; it is entirely supported by a non-pro\ufb01t organization, the Wikimedia Foundation, which relies on\ndonations as its primary funding model. This means that MediaWiki must not only run a top-ten\nwebsite, but also do so on a shoestring budget. To meet these demands, MediaWiki has a heavy\nbias towards performance, caching and optimization. Expensive features that can\u2019t be enabled on\nWikipedia are either reverted or disabled through a con\ufb01guration variable; there is an endless balance\nbetween performance and features.\nThe in\ufb02uence of Wikipedia on MediaWiki\u2019s architecture isn\u2019t limited to performance. Unlike\ngeneric content management systems (CMSes), MediaWiki was originally written for a very speci\ufb01c\npurpose: supporting a community that creates and curates freely reusable knowledge on an open\nplatform. This means, for example, that MediaWiki doesn\u2019t include regular features found in corporate\nCMSes, like a publication work\ufb02ow or access control lists, but does o\ufb00er a variety of tools to handle\nspam and vandalism.\nSo, from the start, the needs and actions of a constantly evolving community of Wikipedia\nparticipants have a\ufb00ected MediaWiki\u2019s development, and vice versa. The architecture of MediaWiki\nhas been driven many times by initiatives started or requested by the community, such as the creation\nof Wikimedia Commons, or the Flagged Revisions feature. Developers made major architectural\nchanges because the way that MediaWiki was used by Wikipedians made it necessary.\nMediaWiki has also gained a solid external user base by being open source software from the\nbeginning. Third-party reusers know that, as long as such a high-pro\ufb01le website as Wikipedia uses\nMediaWiki, the software will be maintained and improved. MediaWiki used to be really focused on\nWikimedia sites, but e\ufb00orts have been made to make it more generic and better accommodate the\nneeds of these third-party users. For example, MediaWiki now ships with an excellent web-based\ninstaller, making the installation process much less painful than when everything had to be done via\nthe command line and the software contained hardcoded paths for Wikipedia.\nStill, MediaWiki is and remains Wikipedia\u2019s software, and this shows throughout its history and\narchitecture.\n", "page": 193, "type": "text", "section": "Page 193"}
{"text": "This chapter is organized as follows:\n\u2022 Historical Overview gives a short overview of the history of MediaWiki, or rather its prehistory,\nand the circumstances of its creation.\n\u2022 MediaWiki Code Base and Practices explains the choice of PHP, the importance and imple-\nmentation of secure code, and how general con\ufb01guration is handled.\n\u2022 Database and Text Storage dives into the distributed data storage system, and how its structure\nevolved to accommodate growth.\n\u2022 Requests, Caching and Delivery follows the execution of a web request through the components\nof MediaWiki it activates. This section includes a description of the di\ufb00erent caching layers,\nand the asset delivery system.\n\u2022 Languages details the pervasive internationalization and localization system, why it matters,\nand how it is implemented.\n\u2022 Users presents how users are represented in the software, and how user permissions work.\n\u2022 Content details how content is structured, formatted and processed to generate the \ufb01nal HTML.\nA subsection focuses on how MediaWiki handles media \ufb01les.\n\u2022 Customizing and Extending MediaWiki explains how JavaScript, CSS, extensions, and skins can\nbe used to customize a wiki, and how they modify its appearance and behavior. A subsection\npresents the software\u2019s machine-readable web API.\n12.1\nHistorical Overview\nPhase I: UseModWiki\nWikipedia was launched in January 2001. At the time, it was mostly an experiment to try to boost\nthe production of content for Nupedia, a free-content, but peer-reviewed, encyclopedia created by\nJimmy Wales. Because it was an experiment, Wikipedia was originally powered by UseModWiki,\nan existing GPL wiki engine written in Perl, using CamelCase and storing all pages in individual\ntext \ufb01les with no history of changes made.\nIt soon appeared that CamelCase wasn\u2019t really appropriate for naming encyclopedia articles. In\nlate January 2001, UseModWiki developer and Wikipedia participant Cli\ufb00ord Adams added a new\nfeature to UseModWiki: free links; i.e., the ability to link to pages with a special syntax (double\nsquare brackets), instead of automatic CamelCase linking. A few weeks later, Wikipedia upgraded\nto the new version of UseModWiki supporting free links, and enabled them.\nWhile this initial phase isn\u2019t about MediaWiki per se, it provides some context and shows that,\neven before MediaWiki was created, Wikipedia started to shape the features of the software that\npowered it. UseModWiki also in\ufb02uenced some of MediaWiki\u2019s features; for example, its markup\nlanguage. The Nostalgia Wikipedia1 contains a complete copy of the Wikipedia database from\nDecember 2001, when Wikipedia still used UseModWiki.\nPhase II: The PHP Script\nIn 2001, Wikipedia was not yet a top ten website; it was an obscure project sitting in a dark corner of\nthe Interwebs, unknown to most search engines, and hosted on a single server. Still, performance\nwas already an issue, notably because UseModWiki stored its content in a \ufb02at \ufb01le database. At the\n1http://nostalgia.wikipedia.org\n180\nMediaWiki\n", "page": 194, "type": "text", "section": "Page 194"}
{"text": "time, Wikipedians were worried about being inundated with tra\ufb03c following articles in the New\nYork Times, Slashdot and Wired.\nSo in summer 2001, Wikipedia participant Magnus Manske (then a university student) started\nto work on a dedicated Wikipedia wiki engine in his free time. He aimed to improve Wikipedia\u2019s\nperformance using a database-driven app, and to develop Wikipedia-speci\ufb01c features that couldn\u2019t\nbe provided by a \u201cgeneric\u201d wiki engine. Written in PHP and MySQL-backed, the new engine was\nsimply called the \u201cPHP script\u201d, \u201cPHP wiki\u201d, \u201cWikipedia software\u201d or \u201cphase II\u201d.\nThe PHP script was made available in August 2001, shared on SourceForge in September, and\ntested until late 2001. As Wikipedia su\ufb00ered from recurring performance issues because of increasing\ntra\ufb03c, the English language Wikipedia eventually switched from UseModWiki to the PHP script in\nJanuary 2002. Other language versions also created in 2001 were slowly upgraded as well, although\nsome of them would remain powered by UseModWiki until 2004.\nAs PHP software using a MySQL database, the PHP script was the \ufb01rst iteration of what would\nlater become MediaWiki. It introduced many critical features still in use today, like namespaces to\norganize content (including talk pages), skins, and special pages (including maintenance reports, a\ncontributions list and a user watchlist).\nPhase III: MediaWiki\nDespite the improvements from the PHP script and database backend, the combination of increasing\ntra\ufb03c, expensive features and limited hardware continued to cause performance issues on Wikipedia.\nIn 2002, Lee Daniel Crocker rewrote the code again, calling the new software \u201cPhase III\u201d2. Because\nthe site was experiencing frequent di\ufb03culties, Lee thought there \u201cwasn\u2019t much time to sit down and\nproperly architect and develop a solution\u201d, so he \u201cjust reorganized the existing architecture for better\nperformance and hacked all the code\u201d. Pro\ufb01ling features were added to track down slow functions.\nThe Phase III software kept the same basic interface, and was designed to look and behave as\nmuch like the Phase II software as possible. A few new features were also added, like a new \ufb01le\nupload system, side-by-side di\ufb00s of content changes, and interwiki links.\nOther features were added over 2002, like new maintenance special pages, and the \u201cedit on\ndouble click\u201d option. Performance issues quickly reappeared, though. For example, in November\n2002, administrators had to temporarily disable the \u201cview count\u201d and \u201csite\u201d statistics which were\ncausing two database writes on every page view. They would also occasionally switch the site to\nread-only mode to maintain the service for readers, and disable expensive maintenance pages during\nhigh-access times because of table locking problems.\nIn early 2003, developers discussed whether they should properly re-engineer and re-architect\nthe software from scratch, before the \ufb01re-\ufb01ghting became unmanageable, or continue to tweak and\nimprove the existing code base. They chose the latter solution, mostly because most developers\nwere su\ufb03ciently happy with the code base, and con\ufb01dent enough that further iterative improvements\nwould be enough to keep up with the growth of the site.\nIn June 2003, administrators added a second server, the \ufb01rst database server separate from the web\nserver. (The new machine was also the web server for non-English Wikipedia sites.) Load-balancing\nbetween the two servers would be set up later that year. Admins also enabled a new page-caching\nsystem that used the \ufb01le system to cache rendered, ready-to-output pages for anonymous users.\nJune 2003 is also when Jimmy Wales created the non-pro\ufb01t Wikimedia Foundation to support\nWikipedia and manage its infrastructure and day-to-day operations. The \u201cWikipedia software\u201d was\n2http://article.gmane.org/gmane.science.linguistics.wikipedia.technical/2794\nSumana Harihareswara and Guillaume Paumier\n181\n", "page": 195, "type": "text", "section": "Page 195"}
{"text": "o\ufb03cially named \u201cMediaWiki\u201d in July, as wordplay on the Wikimedia Foundation\u2019s name. What was\nthought at the time to be a clever pun would confuse generations of users and developers.\nNew features were added in July, like the automatically generated table of contents and the ability\nto edit page sections, both still in use today. The \ufb01rst release under the name \u201cMediaWiki\u201d happened\nin August 2003, concluding the long genesis of an application whose overall structure would remain\nfairly stable from there on.\n12.2\nMediaWiki Code Base and Practices\nPHP\nPHP was chosen as the framework for Wikipedia\u2019s \u201cPhase II\u201d software in 2001; MediaWiki has\ngrown organically since then, and is still evolving. Most MediaWiki developers are volunteers\ncontributing in their free time, and there were very few of them in the early years. Some software\ndesign decisions or omissions may seem wrong in retrospect, but it\u2019s hard to criticize the founders\nfor not implementing some abstraction which is now found to be critical, when the initial code base\nwas so small, and the time taken to develop it so short.\nFor example, MediaWiki uses unpre\ufb01xed class names, which can cause con\ufb02icts when PHP core\nand PECL (PHP Extension Community Library) developers add new classes: MediaWiki Namespace\nclass had to be renamed to MWNamespace to be compatible with PHP 5.3. Consistently using a pre\ufb01x\nfor all classes (e.g., \u201cMW\u201d) would have made it easier to embed MediaWiki inside another application\nor library.\nRelying on PHP was probably not the best choice for performance, since it has not bene\ufb01tted\nfrom improvements that some other dynamic languages have seen. Using Java would have been\nmuch better for performance, and simpli\ufb01ed execution scaling for back-end maintenance tasks. On\nthe other hand, PHP is very popular, which facilitates recruiting new developers.\nEven if MediaWiki still contains \u201cugly\u201d legacy code, major improvements have been made over\nthe years, and new architectural elements have been introduced to MediaWiki throughout its history.\nThey include the Parser, SpecialPage, and Database classes, the Image class and the FileRepo\nclass hierarchy, ResourceLoader, and the Action hierarchy. MediaWiki started without any of these\nthings, but all of them support features that have been around since the beginning. Many developers\nare interested primarily in feature development and architecture is often left behind, only to catch up\nlater as the cost of working within an inadequate architecture becomes apparent.\nSecurity\nBecause MediaWiki is the platform for high-pro\ufb01le sites such as Wikipedia, core developers and code\nreviewers have enforced strict security rules3. To make it easier to write secure code, MediaWiki\ngives developers wrappers around HTML output and database queries to handle escaping. To sanitize\nuser input, a develop uses the WebRequest class, which analyzes data passed in the URL or via a\nPOSTed form. It removes \u201cmagic quotes\u201d and slashes, strips illegal input characters and normalizes\nUnicode sequences. Cross-site request forgery (CSRF) is avoided by using tokens, and cross-site\nscripting (XSS) by validating inputs and escaping outputs, usually with PHP\u2019s htmlspecialchars()\nfunction. MediaWiki also provides (and uses) an XHTML sanitizer with the Sanitizer class, and\ndatabase functions that prevent SQL injection.\n3See https://www.mediawiki.org/wiki/Security_for_developers for a detailed guide.\n182\nMediaWiki\n", "page": 196, "type": "text", "section": "Page 196"}
{"text": "Con\ufb01guration\nMediaWiki o\ufb00ers hundreds of con\ufb01guration settings, stored in global PHP variables. Their default\nvalue is set in DefaultSettings.php, and the system administrator can override them by editing\nLocalSettings.php.\nMediaWiki used to over-depend on global variables, including for con\ufb01guration and context\nprocessing. Globals cause serious security implications with PHP\u2019s register_globals function\n(which MediaWiki hasn\u2019t needed since version 1.2). This system also limits potential abstractions for\ncon\ufb01guration, and makes it more di\ufb03cult to optimize the start-up process. Moreover, the con\ufb01guration\nnamespace is shared with variables used for registration and object context, leading to potential\ncon\ufb02icts. From a user perspective, global con\ufb01guration variables have also made MediaWiki seem\ndi\ufb03cult to con\ufb01gure and maintain. MediaWiki development has been a story of slowly moving\ncontext out of global variables and into objects. Storing processing context in object member\nvariables allows those objects to be reused in a much more \ufb02exible way.\n12.3\nDatabase and Text Storage\nMediaWiki has been using a relational database backend since the Phase II software. The default\n(and best-supported) database management system (DBMS) for MediaWiki is MySQL, which is\nthe one that all Wikimedia sites use, but other DBMSes (such as PostgreSQL, Oracle, and SQLite)\nhave community-supported implementations. A sysadmin can choose a DBMS while installing\nMediaWiki, and MediaWiki provides both a database abstraction and a query abstraction layer that\nsimplify database access for developers.\nThe current layout contains dozens of tables. Many are about the wiki\u2019s content (e.g., page,\nrevision, category, and recentchanges).\nOther tables include data about users (user,\nuser_groups),\nmedia \ufb01les (image,\nfilearchive),\ncaching (objectcache,\nl10n_cache,\nquerycache) and internal tools (job for the job queue), among others4. (See Figure 12.1.) In-\ndices and summary tables are used extensively in MediaWiki, since SQL queries that scan huge\nnumbers of rows can be very expensive, particularly on Wikimedia sites. Unindexed queries are\nusually discouraged.\nThe database went through dozens of schema changes over the years, the most notable being the\ndecoupling of text storage and revision tracking in MediaWiki 1.5.\nIn the 1.4 model, the content was stored in two important tables, cur (containing the text and\nmetadata of the current revision of the page) and old (containing previous revisions); deleted pages\nwere kept in archive. When an edit was made, the previously current revision was copied to the\nold table, and the new edit was saved to cur. When a page was renamed, the page title had to be\nupdated in the metadata of all the old revisions, which could be a long operation. When a page was\ndeleted, its entries in both the cur and old tables had to be copied to the archive table before being\ndeleted; this meant moving the text of all revisions, which could be very large and thus take time.\nIn the 1.5 model, revision metadata and revision text were split: the cur and old tables were\nreplaced with page (pages\u2019 metadata), revision (metadata for all revisions, old or current) and\ntext (text of all revisions, old, current or deleted). Now, when an edit is made, revision metadata\ndon\u2019t need to be copied around tables: inserting a new entry and updating the page_latest pointer\nis enough. Also, the revision metadata don\u2019t include the page title anymore, only its ID: this removes\nthe need for renaming all revisions when a page is renamed\n4Complete documentation of the database layout in MediaWiki is available at https://www.mediawiki.org/wiki/Manual:\nDatabase_layout.\nSumana Harihareswara and Guillaume Paumier\n183\n", "page": 197, "type": "text", "section": "Page 197"}
{"text": "Figure 12.1: Main content tables in MediaWiki 1.4 and 1.5\nThe revision table stores metadata for each revision, but not their text; instead, they contain a\ntext ID pointing to the text table, which contains the actual text. When a page is deleted, the text of\nall revisions of the page stays there and doesn\u2019t need to be moved to another table. The text table\nis composed of a mapping of IDs to text blobs; a flags \ufb01eld indicates if the text blob is gzipped\n(for space savings) or if the text blob is only a pointer to external text storage. Wikimedia sites use a\nMySQL-backed external storage cluster with blobs of a few dozen revisions. The \ufb01rst revision of\nthe blob is stored in full, and following revisions to the same page are stored as di\ufb00s relative to the\nprevious revision; the blobs are then gzipped. Because the revisions are grouped per page, they tend\nto be similar, so the di\ufb00s are relatively small and gzip works well. The compression ratio achieved\non Wikimedia sites nears 98%.\nOn the hardware side, MediaWiki has built-in support for load balancing, added as early as 2004\nin MediaWiki 1.2 (when Wikipedia got its second server\u2014a big deal at the time). The load balancer\n(MediaWiki\u2019s PHP code that decides which server to connect to) is now a critical part of Wikimedia\u2019s\ninfrastructure, which explains its in\ufb02uence on some algorithm decisions in the code. The system\nadministrator can specify, in MediaWiki\u2019s con\ufb01guration, that there is one master database server and\nany number of slave database servers; a weight can be assigned to each server. The load balancer\nwill send all writes to the master, and will balance reads according to the weights. It also keeps track\nof the replication lag of each slave. If a slave\u2019s replication lag exceeds 30 seconds, it will not receive\nany read queries to allow it to catch up; if all slaves are lagged more than 30 seconds, MediaWiki\nwill automatically put itself in read-only mode.\nMediaWiki\u2019s \u201cchronology protector\u201d ensures that replication lag never causes a user to see a\npage that claims an action they\u2019ve just performed hasn\u2019t happened yet: for instance, if a user renames\na page, another user may still see the old name, but the one who renamed will always see the new\n184\nMediaWiki\n", "page": 198, "type": "text", "section": "Page 198"}
{"text": "name, because he\u2019s the one who renamed it. This is done by storing the master\u2019s position in the\nuser\u2019s session if a request they made resulted in a write query. The next time the user makes a read\nrequest, the load balancer reads this position from the session, and tries to select a slave that has\ncaught up to that replication position to serve the request. If none is available, it will wait until one\nis. It may appear to other users as though the action hasn\u2019t happened yet, but the chronology remains\nconsistent for each user.\n12.4\nRequests, Caching and Delivery\nExecution Work\ufb02ow of a Web Request\nindex.php is the main entry point for MediaWiki, and handles most requests processed by the\napplication servers (i.e., requests that were not served by the caching infrastructure; see below).\nThe code executed from index.php performs security checks, loads default con\ufb01guration settings\nfrom includes/DefaultSettings.php, guesses con\ufb01guration with includes/Setup.php and\nthen applies site settings contained in LocalSettings.php. Next it instantiates a MediaWiki object\n($mediawiki), and creates a Title object ($wgTitle) depending on the title and action parameters\nfrom the request.\nindex.php can take a variety of action parameters in the URL request; the default action is\nview, which shows the regular view of an article\u2019s content. For example, the request https://en.\nwikipedia.org/w/index.php?title=Apple\\&action=view displays the content of the article\n\u201cApple\u201d on the English Wikipedia5. Other frequent actions include edit (to open an article for\nediting), submit (to preview or save an article), history (to show an article\u2019s history) and watch\n(to add an article to the user\u2019s watchlist). Administrative actions include delete (to delete an article)\nand protect (to prevent edits to an article).\nMediaWiki::performRequest() is then called to handle most of the URL request. It checks\nfor bad titles, read restrictions, local interwiki redirects, and redirect loops, and determines whether\nthe request is for a normal or a special page.\nNormal page requests are handed over to MediaWiki::initializeArticle(), to create an\nArticle object for the page ($wgArticle), and then to MediaWiki::performAction(), which\nhandles \u201cstandard\u201d actions. Once the action has been completed, MediaWiki::finalCleanup()\n\ufb01nalizes the request by committing database transactions, outputting the HTML and launching\ndeferred updates through the job queue. MediaWiki::restInPeace() commits the deferred updates\nand closes the task gracefully.\nIf the page requested is a Special page (i.e., not a regular wiki content page, but a special\nsoftware-related page such as Statistics), SpecialPageFactory::executePath is called instead\nof initializeArticle(); the corresponding PHP script is then called. Special pages can do all\nsorts of magical things, and each has a speci\ufb01c purpose, usually independent of any one article or its\ncontent. Special pages include various kinds of reports (recent changes, logs, uncategorized pages)\nand wiki administration tools (user blocks, user rights changes), among others. Their execution\nwork\ufb02ow depends on their function.\nMany functions contain pro\ufb01ling code, which makes it possible to follow the execution work-\n\ufb02ow for debugging if pro\ufb01ling is enabled. Pro\ufb01ling is done by calling the wfProfileIn and\nwfProfileOut functions to respectively start and stop pro\ufb01ling a function; both functions take\nthe function\u2019s name as a parameter. On Wikimedia sites, pro\ufb01ling is done for a percentage of all\n5View requests are usually pretti\ufb01ed with URL rewriting, in this example to https://en.wikipedia.org/wiki/Apple.\nSumana Harihareswara and Guillaume Paumier\n185\n", "page": 199, "type": "text", "section": "Page 199"}
{"text": "requests, to preserve performance. MediaWiki sends UDP packets to a central server that collects\nthem and produces pro\ufb01ling data.\nCaching\nMediaWiki itself is improved for performance because it plays a central role on Wikimedia sites, but it\nis also part of a larger operational ecosystem that has in\ufb02uenced its architecture. Wikimedia\u2019s caching\ninfrastructure (structured in layers) has imposed limitations in MediaWiki; developers worked around\nthe issues, not by trying to shape Wikimedia\u2019s extensively optimized caching infrastructure around\nMediaWiki, but rather by making MediaWiki more \ufb02exible, so it could work within that infrastructure\nwithout compromising on performance and caching needs. For example, by default MediaWiki\ndisplays the user\u2019s IP in the top-right corner of the interface (for left-to-right languages) as a reminder\nthat that\u2019s how they\u2019re known to the software when they\u2019re not logged in. The $wgShowIPinHeader\ncon\ufb01guration variable allows the system administrator to disable this feature, thus making the page\ncontent independent of the user: all anonymous visitors can then be served the exact same version of\neach page.\nThe \ufb01rst level of caching (used on Wikimedia sites) consists of reverse caching proxies (Squids)\nthat intercept and serve most requests before they make it to the MediaWiki application servers.\nSquids contain static versions of entire rendered pages, served for simple reads to users who aren\u2019t\nlogged in to the site. MediaWiki natively supports Squid and Varnish, and integrates with this caching\nlayer by, for example, notifying them to purge a page from the cache when it has been changed. For\nlogged-in users, and other requests that can\u2019t be served by Squids, Squid forwards the requests to the\nweb server (Apache).\nThe second level of caching happens when MediaWiki renders and assembles the page from\nmultiple objects, many of which can be cached to minimize future calls. Such objects include\nthe page\u2019s interface (sidebar, menus, UI text) and the content proper, parsed from wikitext. The\nin-memory object cache has been available in MediaWiki since the early 1.1 version (2003), and is\nparticularly important to avoid re-parsing long and complex pages.\nLogin session data can also be stored in memcached, which lets sessions work transparently on\nmultiple front-end web servers in a load-balancing setup (Wikimedia heavily relies on load balancing,\nusing LVS with PyBal).\nSince version 1.16, MediaWiki uses a dedicated object cache for localized UI text; this was\nadded after noticing that a large part of the objects cached in memcached consisted of UI messages\nlocalized into the user\u2019s language. The system is based on fast fetches of individual messages from\nconstant databases (CDB), e.g., \ufb01les with key-value pairs. CDBs minimize memory overhead and\nstart-up time in the typical case; they\u2019re also used for the interwiki cache.\nThe last caching layer consists of the PHP opcode cache, commonly enabled to speed up PHP\napplications. Compilation can be a lengthy process; to avoid compiling PHP scripts into opcode\nevery time they\u2019re invoked, a PHP accelerator can be used to store the compiled opcode and execute\nit directly without compilation. MediaWiki will \u201cjust work\u201d with many accelerators such as APC,\nPHP accelerator and eAccelerator.\nBecause of its Wikimedia bias, MediaWiki is optimized for this complete, multi-layer, distributed\ncaching infrastructure. Nonetheless, it also natively supports alternate setups for smaller sites. For\nexample, it o\ufb00ers an optional simplistic \ufb01le caching system that stores the output of fully rendered\npages, like Squid does. Also, MediaWiki\u2019s abstract object caching layer lets it store the cached\nobjects in several places, including the \ufb01le system, the database, or the opcode cache.\n186\nMediaWiki\n", "page": 200, "type": "text", "section": "Page 200"}
{"text": "ResourceLoader\nAs in many web applications, MediaWiki\u2019s interface has become more interactive and responsive\nover the years, mostly through the use of JavaScript. Usability e\ufb00orts initiated in 2008, as well\nas advanced media handling (e.g., online editing of video \ufb01les), called for dedicated front-end\nperformance improvements.\nTo optimize the delivery of JavaScript and CSS assets, the ResourceLoader module was developed\nto optimize delivery of JS and CSS. Started in 2009, it was completed in 2011 and has been a core\nfeature of MediaWiki since version 1.17. ResourceLoader works by loading JS and CSS assets on\ndemand, thus reducing loading and parsing time when features are unused, for example by older\nbrowsers. It also mini\ufb01es the code, groups resources to save requests, and can embed images as data\nURIs6.\n12.5\nLanguages\nContext and Rationale\nA central part of e\ufb00ectively contributing and disseminating free knowledge to all is to provide it in as\nmany languages as possible. Wikipedia is available in more than 280 languages, and encyclopedia\narticles in English represent less than 20% of all articles. Because Wikipedia and its sister sites exist\nin so many languages, it is important not only to provide the content in the readers\u2019 native language,\nbut also to provide a localized interface, and e\ufb00ective input and conversion tools, so that participants\ncan contribute content.\nFor this reason, localization and internationalization (l10n and i18n) are central components of\nMediaWiki. The i18n system is pervasive, and impacts many parts of the software; it\u2019s also one of the\nmost \ufb02exible and feature-rich7. Translator convenience is usually preferred to developer convenience,\nbut this is believed to be an acceptable cost.\nMediaWiki is currently localized in more than 350 languages, including non-Latin and right-\nto-left (RTL) languages, with varying levels of completion. The interface and content can be in\ndi\ufb00erent languages, and have mixed directionality.\nContent Language\nMediaWiki originally used per-language encoding, which led to a lot of issues; for example, foreign\nscripts could not be used in page titles. UTF-8 was adopted instead. Support for character sets other\nthan UTF-8 was dropped in 2005, along with the major database schema change in MediaWiki 1.5;\ncontent must now be encoded in UTF-8.\nCharacters not available on the editor\u2019s keyboard can be customized and inserted via Medi-\naWiki\u2019s Edittools, an interface message that appears below the edit window; its JavaScript version\nautomatically inserts the character clicked into the edit window. The WikiEditor extension for\nMediaWiki, developed as part of a usability e\ufb00ort, merges special characters with the edit toolbar.\nAnother extension, called Narayam, provides additional input methods and key mapping features for\nnon-ASCII characters.\n6For more on ResourceLoader, see https://www.mediawiki.org/wiki/ResourceLoader for the o\ufb03cial documentation,\nand the talk Low Hanging Fruit vs. Micro-optimization: Creative Techniques for Loading Web Pages Faster given by Trevor\nParscal and Roan Kattouw at OSCON 2011.\n7For an exhaustive guide to internationalization and localization in MediaWiki, see https://www.mediawiki.org/wiki/\nLocalisation.\nSumana Harihareswara and Guillaume Paumier\n187\n", "page": 201, "type": "text", "section": "Page 201"}
{"text": "Interface Language\nInterface messages have been stored in PHP arrays of key-values pairs since the Phase III software\nwas created. Each message is identi\ufb01ed by a unique key, which is assigned di\ufb00erent values across\nlanguages. Keys are determined by developers, who are encouraged to use pre\ufb01xes for extensions;\nfor example, message keys for the UploadWizard extension will start with mwe-upwiz-, where mwe\nstands for MediaWiki extension.\nMediaWiki messages can embed parameters provided by the software, which will often in\ufb02uence\nthe grammar of the message. In order to support virtually any possible language, MediaWiki\u2019s\nlocalization system has been improved and complexi\ufb01ed over time to accommodate languages\u2019\nspeci\ufb01c traits and exceptions, often considered oddities by English speakers.\nFor example, adjectives are invariable words in English, but languages like French require adjec-\ntive agreement with nouns. If the user speci\ufb01ed their gender in their preferences, the GENDER: switch\ncan be used in interface messages to appropriately address them. Other switches include PLURAL:,\nfor \u201csimple\u201d plurals and languages like Arabic with dual, trial or paucal numbers, and GRAMMAR:,\nproviding grammatical transformation functions for languages like Finnish whose grammatical cases\ncause alterations or in\ufb02ections.\nLocalizing Messages\nLocalized interface messages for MediaWiki reside in MessagesXx.php \ufb01les, where Xx is the ISO-\n639 code of the language (e.g. MessagesFr.php for French); default messages are in English\nand stored in MessagesEn.php. MediaWiki extensions use a similar system, or host all localized\nmessages in an <Extension-name>.i18n.php \ufb01le. Along with translations, Message \ufb01les also\ninclude language-dependent information such as date formats.\nContributing translations used to be done by submitting PHP patches for the MessagesXx.php\n\ufb01les. In December 2003, MediaWiki 1.1 introduced \u201cdatabase messages\u201d, a subset of wiki pages in the\nMediaWiki namespace containing interface messages.\nThe content of the wiki page\nMediaWiki:<Message-key> is the message\u2019s text, and overrides its value in the PHP \ufb01le. Local-\nized versions of the message are at MediaWiki:<Message-key>/<language-code>; for example,\nMediaWiki:Rollbacklink/de.\nThis feature has allowed power users to translate (and customize) interface messages locally\non their wiki, but the process doesn\u2019t update i18n \ufb01les shipping with MediaWiki. In 2006, Niklas\nLaxstr\u00f6m\ncreated\na\nspecial,\nheavily\nhacked\nMediaWiki\nwebsite\n(now\nhosted\nat\nhttp://translatewiki.net) where translators can easily localize interface messages in all lan-\nguages simply by editing a wiki page. The MessagesXx.php \ufb01les are then updated in the MediaWiki\ncode repository, where they can be automatically fetched by any wiki, and updated using the Localisa-\ntionUpdate extension. On Wikimedia sites, database messages are now only used for customization,\nand not for localization any more. MediaWiki extensions and some related programs, such as bots,\nare also localized at translatewiki.net.\nTo help translators understand the context and meaning of an interface message, it is consid-\nered a good practice in MediaWiki to provide documentation for every message. This documen-\ntation is stored in a special Message \ufb01le, with the qqq language code which doesn\u2019t correspond\nto a real language. The documentation for each message is then displayed in the translation in-\nterface on translatewiki.net. Another helpful tool is the qqx language code; when used with the\n&uselang parameter to display a wiki page (e.g., https://en.wikipedia.org/wiki/Special:\n188\nMediaWiki\n", "page": 202, "type": "text", "section": "Page 202"}
{"text": "RecentChanges?uselang=qqx), MediaWiki will display the message keys instead of their values in\nthe user interface; this is very useful to identify which message to translate or change.\nRegistered users can set their own interface language in their preferences, to override the site\u2019s\ndefault interface language. MediaWiki also supports fallback languages: if a message isn\u2019t available\nin the chosen language, it will be displayed in the closest possible language, and not necessarily in\nEnglish. For example, the fallback language for Breton is French.\n12.6\nUsers\nUsers are represented in the code using instances of the User class, which encapsulates all of the user-\nspeci\ufb01c settings (user id, name, rights, password, email address, etc.). Client classes use accessors to\naccess these \ufb01elds; they do all the work of determining whether the user is logged in, and whether\nthe requested option can be satis\ufb01ed from cookies or whether a database query is needed. Most of\nthe settings needed for rendering normal pages are set in the cookie to minimize use of the database.\nMediaWiki provides a very granular permissions system, with a user permission for, basically, ev-\nery possible action. For example, to perform the \u201cRollback\u201d action (i.e., to \u201cquickly rollback the edits\nof the last user who edited a particular page\u201d), a user needs the rollback permission, included by\ndefault in MediaWiki\u2019s sysop user group. But it can also be added to other user groups, or have a ded-\nicated user group only providing this permission (this is the case on the English Wikipedia, with the\nRollbackers group). Customization of user rights is done by editing the $wgGroupPermissions ar-\nray in LocalSettings.php; for instance, $wgGroupPermissions[\u2019user\u2019][\u2019movefile\u2019] = true;\nallows all registered users to rename \ufb01les. A user can belong to several groups, and inherits the\nhighest rights associated with each of them.\nHowever, MediaWiki\u2019s user permissions system was really designed with Wikipedia in mind: a\nsite whose content is accessible to all, and where only certain actions are restricted to some users.\nMediaWiki lacks a uni\ufb01ed, pervasive permissions concept; it doesn\u2019t provide traditional CMS features\nlike restricting read or write access by topic or type of content. A few MediaWiki extensions provide\nsuch features to some extent.\n12.7\nContent\nContent Structure\nThe concept of namespaces was used in the UseModWiki era of Wikipedia, where talk pages were\nat the title \u201c<article name>/Talk\u201d. Namespaces were formally introduced in Magnus Manske\u2019s \ufb01rst\n\u201cPHP script\u201d. They were reimplemented a few times over the years, but have kept the same function:\nto separate di\ufb00erent kinds of content. They consist of a pre\ufb01x separated from the page title by a colon\n(e.g. Talk: or File: and Template:); the main content namespace has no pre\ufb01x. Wikipedia users\nquickly adopted them, and they provided the community with di\ufb00erent spaces to evolve. Namespaces\nhave proven to be an important feature of MediaWiki, as they create the necessary preconditions for\na wiki\u2019s community and set up meta-level discussions, community processes, portals, user pro\ufb01les,\netc.\nThe default con\ufb01guration for MediaWiki\u2019s main content namespace is to be \ufb02at (no subpages),\nbecause it\u2019s how Wikipedia works, but it is trivial to enable subpages. They are enabled in other\nnamespaces (e.g., User:, where people can, for instance, work on draft articles) and display bread-\ncrumbs.\nSumana Harihareswara and Guillaume Paumier\n189\n", "page": 203, "type": "text", "section": "Page 203"}
{"text": "Namespaces separate content by type; within the same namespace, pages can be organized by\ntopic using categories, a pseudo-hierarchical organization scheme introduced in MediaWiki 1.3.\nContent Processing: MediaWiki Markup Language and Parser\nThe user-generated content stored by MediaWiki isn\u2019t in HTML, but in a markup language speci\ufb01c\nto MediaWiki, sometimes called \u201cwikitext\u201d. It allows users to make formatting changes (e.g. bold,\nitalic using quotes), add links (using square brackets), include templates, insert context-dependent\ncontent (like a date or signature), and make an incredible number of other magical things happen8.\nTo display a page, this content needs to be parsed, assembled from all the external or dynamic\npieces it calls, and converted to proper HTML. The parser is one of the most essential parts of\nMediaWiki, which makes it di\ufb03cult to change or improve. Because hundreds of millions of wiki\npages worldwide depend on the parser to continue outputting HTML the way it always has, it has to\nremain extremely stable.\nThe markup language wasn\u2019t formally specced from the beginning; it started based on Use-\nModWiki\u2019s markup, then morphed and evolved as needs demanded. In the absence of a formal\nspeci\ufb01cation, the MediaWiki markup language has become a complex and idiosyncratic language,\nbasically only compatible with MediaWiki\u2019s parser; it can\u2019t be represented as a formal grammar. The\ncurrent parser\u2019s speci\ufb01cation is jokingly referred to as \u201cwhatever the parser spits out from wikitext,\nplus a few hundred test cases\u201d.\nThere have been many attempts at alternative parsers, but none has succeeded so far. In 2004 an\nexperimental tokenizer was written by Jens Frank to parse wikitext, and enabled on Wikipedia; it had\nto be disabled three days later because of the poor performance of PHP array memory allocations.\nSince then, most of the parsing has been done with a huge pile of regular expressions, and a ton of\nhelper functions. The wiki markup, and all the special cases the parser needs to support, have also\nbecome considerably more complex, making future attempts even more di\ufb03cult.\nA notable improvement was Tim Starling\u2019s preprocessor rewrite in MediaWiki 1.12, whose\nmain motivation was to improve the parsing performance on pages with complex templates. The\npreprocessor converts wikitext to an XML DOM tree representing parts of the document (template\ninvocations, parser functions, tag hooks, section headings, and a few other structures), but can skip\n\u201cdead branches\u201d, such as unfollowed #switch cases and unused defaults for template arguments, in\ntemplate expansion. The parser then iterates through the DOM structure and converts its content to\nHTML.\nRecent work on a visual editor for MediaWiki has made it necessary to improve the parsing\nprocess (and make it faster), so work has resumed on the parser and intermediate layers between\nMediaWiki markup and \ufb01nal HTML (see Future, below).\nMagic Words and Templates\nMediaWiki o\ufb00ers \u201cmagic words\u201d that modify the general behavior of the page or include dynamic\ncontent into it. They consist of: behavior switches like __NOTOC__ (to hide the automatic table of\ncontent) or __NOINDEX__ (to tell search engines not to index the page); variables like CURRENTTIME\nor SITENAME; and parser functions, i.e., magic words that can take parameters, like lc:<string> (to\noutput <string> in lowercase). Constructs like GENDER:, PLURAL: and GRAMMAR:, used to localize\nthe UI, are parser functions.\n8Detailed documentation is available at https://www.mediawiki.org/wiki/Markup_spec and the associated pages.\n190\nMediaWiki\n", "page": 204, "type": "text", "section": "Page 204"}
{"text": "The most common way to include content from other pages in a MediaWiki page is to use\ntemplates. Templates were really intended to be used to include the same content on di\ufb00erent pages,\ne.g., navigation panels or maintenance banners on Wikipedia articles; having the ability to create\npartial page layouts and reuse them in thousands of articles with central maintenance made a huge\nimpact on sites like Wikipedia.\nHowever, templates have also been used (and abused) by users for a completely di\ufb00erent purpose.\nMediaWiki 1.3 made it possible for templates to take parameters that change their output; the ability\nto add a default parameter (introduced in MediaWiki 1.6) enabled the construction of a functional\nprogramming language implemented on top of PHP, which was ultimately one of the most costly\nfeatures in terms of performance.\nTim Starling then developed additional parser functions (the ParserFunctions extension), as a\nstopgap measure against insane constructs created by Wikipedia users with templates. This set\nof functions included logical structures like #if and #switch, and other functions like #expr (to\nevaluate mathematical expressions) and #time (for time formatting).\nSoon enough, Wikipedia users started to create even more complex templates using the new\nfunctions, which considerably degraded the parsing performance on template-heavy pages. The\nnew preprocessor introduced in MediaWiki 1.12 (a major architectural change) was implemented to\npartly remedy this issue. Recently, MediaWiki developers have discussed the possibility of using an\nactual scripting language, perhaps Lua, to improve performance.\nMedia Files\nUsers upload \ufb01les through the Special:Upload page; administrators can con\ufb01gure the allowed \ufb01le\ntypes through an extension whitelist. Once uploaded, \ufb01les are stored in a folder on the \ufb01le system,\nand thumbnails in a dedicated thumb directory.\nBecause of Wikimedia\u2019s educational mission, MediaWiki supports \ufb01le types that may be uncom-\nmon in other web applications or CMSes, like SVG vector images, and multipage PDFs and DjVus.\nThey are rendered as PNG \ufb01les, and can be thumbnailed and displayed inline, as are more common\nimage \ufb01les like GIFs, JPGs and PNGs.\nWhen a \ufb01le is uploaded, it is assigned a File: page containing information entered by the\nuploader; this is free text and usually includes copyright information (author, license) and items\ndescribing or classifying the content of the \ufb01le (description, location, date, categories, etc.). While\nprivate wikis may not care much about this information, on media libraries like Wikimedia Commons\nit are critical to organise the collection and ensure the legality of sharing these \ufb01les. It has been\nargued that most of these metadata should, in fact, be stored in a queryable structure like a database\ntable. This would considerably facilitate search, but also attribution and reuse by third parties\u2014for\nexample, through the API.\nMost Wikimedia sites also allow \u201clocal\u201d uploads to each wiki, but the community tries to store\nfreely licensed media \ufb01les in Wikimedia\u2019s free media library, Wikimedia Commons. Any Wikimedia\nsite can display a \ufb01le hosted on Commons as if it were hosted locally. This custom avoids having to\nupload a \ufb01le to every wiki to use it there.\nAs a consequence, MediaWiki natively supports foreign media repositories, i.e., the ability to\naccess media \ufb01les hosted on another wiki through its API and the ForeignAPIRepo system. Since\nversion 1.16, any MediaWiki website can easily use \ufb01les from Wikimedia Commons through the\nInstantCommons feature. When using a foreign repository, thumbnails are stored locally to save\nbandwidth. However, it is not (yet) possible to upload to a foreign media repository from another\nwiki.\nSumana Harihareswara and Guillaume Paumier\n191\n", "page": 205, "type": "text", "section": "Page 205"}
{"text": "12.8\nCustomizing and Extending MediaWiki\nLevels\nMediaWiki\u2019s architecture provides di\ufb00erent ways to customize and extend the software. This can be\ndone at di\ufb00erent levels of access:\n\u2022 System administrators can install extensions and skins, and con\ufb01gure the wiki\u2019s separate\nhelper programs (e.g., for image thumbnailing and TeX rendering) and global settings (see\nCon\ufb01guration above).\n\u2022 Wiki sysops (sometimes called \u201cadministrators\u201d too) can edit site-wide gadgets, JavaScript\nand CSS settings.\n\u2022 Any registered user can customize their own experience and interface using their preferences\n(for existing settings, skins and gadgets) or make their own modi\ufb01cations (using their personal\nJS and CSS pages).\nExternal programs can also communicate with MediaWiki through its machine API, if it\u2019s\nenabled, basically making any feature and data accessible to the user.\nJavaScript and CSS\nMediaWiki can read and apply site-wide or skin-wide JavaScript and CSS using custom wiki\npages; these pages are in the MediaWiki: namespace, and thus can only be edited by sysops;\nfor example, JavaScript modi\ufb01cations from MediaWiki:Common.js apply to all skins, CSS from\nMediaWiki:Common.css applies to all skins, but MediaWiki:Vector.css only applies to users with\nthe Vector skin.\nUsers can do the same types of changes, which will only apply to their own interface, by\nediting subpages of their user page (e.g. User:<Username>/common.js for JavaScript on all skins,\nUser:<Username>/common.css for CSS on all skins, or User:<Username>/vector.css for CSS\nmodi\ufb01cations that only apply to the Vector skin).\nIf the Gadgets extension is installed, sysops can also edit gadgets, i.e., snippets of JavaScript code,\nproviding features that can be turned on and o\ufb00by users in their preferences. Upcoming developments\non gadgets will make it possible to share gadgets across wikis, thus avoiding duplication.\nThis set of tools has had a huge impact and greatly increased the democratization of MediaWiki\u2019s\nsoftware development. Individual users are empowered to add features for themselves; power users\ncan share them with others, both informally and through globally con\ufb01gurable sysop-controlled\nsystems. This framework is ideal for small, self-contained modi\ufb01cations, and presents a lower barrier\nto entry than heavier code modi\ufb01cations done through hooks and extensions.\nExtensions and Skins\nWhen JavaScript and CSS modi\ufb01cations are not enough, MediaWiki provides a system of hooks\nthat let third-party developers run custom PHP code before, after, or instead of MediaWiki code for\nparticular events9. MediaWiki extensions use hooks to plug into the code.\nBefore hooks existed in MediaWiki, adding custom PHP code meant modifying the core code,\nwhich was neither easy nor recommended. The \ufb01rst hooks were proposed and added in 2004 by\nEvan Prodromou; many more have been added over the years when needed. Using hooks, it is even\npossible to extend MediaWiki\u2019s wiki markup with additional capabilities using tag extensions.\n9MediaWiki hooks are referenced at https://www.mediawiki.org/wiki/Manual:Hooks.\n192\nMediaWiki\n", "page": 206, "type": "text", "section": "Page 206"}
{"text": "The extension system isn\u2019t perfect; extension registration is based on code execution at startup,\nrather than cacheable data, which limits abstraction and optimization and hurts MediaWiki\u2019s perfor-\nmance. But overall, the extension architecture is now a fairly \ufb02exible infrastructure that has helped\nmake specialized code more modular, keeping the core software from expanding (too) much, and\nmaking it easier for third-party users to build custom functionality on top of MediaWiki.\nConversely, it\u2019s very di\ufb03cult to write a new skin for MediaWiki without reinventing the wheel.\nIn MediaWiki, skins are PHP classes each extending the parent Skin class; they contain functions\nthat gather the information needed to generate the HTML. The long-lived \u201cMonoBook\u201d skin was\ndi\ufb03cult to customize because it contained a lot of browser-speci\ufb01c CSS to support old browsers;\nediting the template or CSS required many subsequent changes to re\ufb02ect the change for all browsers\nand platforms.\nAPI\nThe other main entry point for MediaWiki, besides index.php, is api.php, used to access its\nmachine-readable web query API (Application Programming Interface).\nWikipedia users originally created \u201cbots\u201d that worked by screen scraping the HTML content\nserved by MediaWiki; this method was very unreliable and broke many times. To improve this\nsituation, developers introduced a read-only interface (located at query.php), which then evolved into\na full-\ufb02edged read and write machine API providing direct, high-level access to the data contained\nin the MediaWiki database10.\nClient programs can use the API to login, get data, and post changes. The API supports thin\nweb-based JavaScript clients and end-user applications. Almost anything that can be done via the\nweb interface can basically be done through the API. Client libraries implementing the MediaWiki\nAPI are available in many languages, including Python and .NET.\n12.9\nFuture\nWhat started as a summer project done by a single volunteer PHP developer has grown into MediaWiki,\na mature, stable wiki engine powering a top-ten website with a ridiculously small operational\ninfrastructure. This has been made possible by constant optimization for performance, iterative\narchitectural changes and a team of awesome developers.\nThe evolution of web technologies, and the growth of Wikipedia, call for ongoing improvements\nand new features, some of which require major changes to MediaWiki\u2019s architecture. This is, for\nexample, the case for the ongoing visual editor project, which has prompted renewed work on the\nparser and on the wiki markup language, the DOM and \ufb01nal HTML conversion.\nMediaWiki is a tool used for very di\ufb00erent purposes. Within Wikimedia projects, for instance,\nit\u2019s used to create and curate an encyclopedia (Wikipedia), to power a huge media library (Wiki-\nmedia Commons), to transcribe scanned reference texts (Wikisource), and so on. In other contexts,\nMediaWiki is used as a corporate CMS, or as a data repository, sometimes combined with a semantic\nframework. These specialized uses that weren\u2019t planned for will probably continue to drive constant\nadjustments to the software\u2019s internal structure. As such, MediaWiki\u2019s architecture is very much\nalive, just like the immense community of users it supports.\n10Exhaustive documentation of the API is available at https://www.mediawiki.org/wiki/API.\nSumana Harihareswara and Guillaume Paumier\n193\n", "page": 207, "type": "text", "section": "Page 207"}
{"text": "12.10\nFurther Reading\n\u2022 MediaWiki documentation and support: https://www.mediawiki.org.\n\u2022 Automatically generated MediaWiki documentation: http://svn.wikimedia.org/doc/.\n\u2022 Domas Mituzas, Wikipedia: site internals, con\ufb01guration, code examples and management\nissues, MySQL Users conference, 2007. Full text available at http://dom.as/talks/.\n12.11\nAcknowledgments\nThis chapter was created collaboratively. Guillaume Paumier wrote most of the content by organizing\nthe input provided by MediaWiki users and core developers. Sumana Harihareswara coordinated\nthe interviews and input-gathering phases. Many thanks to Antoine Musso, Brion Vibber, Chad\nHorohoe, Tim Starling, Roan Kattouw, Sam Reed, Siebrand Mazeland, Erik M\u00f6ller, Magnus Manske,\nRob Lanphier, Amir Aharoni, Federico Leva, Graham Pearce and others for providing input and/or\nreviewing the content.\n194\nMediaWiki\n", "page": 208, "type": "text", "section": "Page 208"}
{"text": "[chapter13]\nMoodle\nTim Hunt\nMoodle is a web application used in educational settings. While this chapter will try to give an\noverview of all aspects of how Moodle works, it focuses on those areas where Moodle\u2019s design is\nparticularly interesting:\n\u2022 The way the application is divided into plugins;\n\u2022 The permission system, which controls which users can perform which actions in di\ufb00erent\nparts of the system;\n\u2022 The way output is generated, so that di\ufb00erent themes (skins) can be used to give di\ufb00erent\nappearances, and so that the interface can be localised.\n\u2022 The database abstraction layer.\nMoodle1 provides a place online where students and teachers can come together to teach and\nlearn. A Moodle site is divided into courses. A course has users enrolled in it with di\ufb00erent roles,\nsuch as Student or Teacher. Each course comprises a number of resources and activities. A resource\nmight be a PDF \ufb01le, a page of HTML within Moodle, or a link to something elsewhere on the web.\nAn activity might be a forum, a quiz or a wiki. Within the course, these resources and activities will\nbe structured in some way. For example they may be grouped into logical topics, or into weeks on a\ncalendar.\nFigure 13.1: Moodle course\n1http://moodle.org/\n", "page": 209, "type": "text", "section": "Page 209"}
{"text": "Moodle can be used as a standalone application. Should you wish to teach courses on software\narchitecture (for example) you could download Moodle to your web host, install it, start creating\ncourses, and wait for students to come and self-register. Alternatively, if you are a large institution,\nMoodle would be just one of the systems you run. You would probably also have the infrastructure\nshown in Figure 13.2.\nFigure 13.2: Typical university systems architecture\n\u2022 An authentication/identity provider (for example LDAP) to control user accounts across all\nyour systems.\n\u2022 A student information system; that is, a database of all your students, which program of study\nthey are on, and hence which courses they need to complete; and their transcript\u2014a high-level\nsummary of the results of the courses they have completed. This would also deal with other\nadministrative functions, like tracking whether they have paid their fees.\n\u2022 A document repository (for example, Alfresco); to store \ufb01les, and track work\ufb02ow as users\ncollaborate to create \ufb01les.\n\u2022 An ePortfolio; this is a place where students can assemble assets, either to build a CV (resume),\nor to provide evidence that they have met the requirements of a practice-based course.\n\u2022 A reporting or analytics tool; to generate high-level information about what is going on in your\ninstitution.\nMoodle focuses on providing an online space for teaching and learning, rather than any of the\nother systems that an educational organisation might need. Moodle provides a basic implementation\nof the other functionalities, so that it can function either as a stand-alone system or integrated with\nother systems. The role Moodle plays is normally called a virtual learning environment (VLE), or\nlearning or course management system (LMS, CMS or even LCMS).\nMoodle is open source or free software (GPL). It is written in PHP. It will run on most common\nweb servers, on common platforms. It requires a database, and will work with MySQL, PostgreSQL,\nMicrosoft SQL Server or Oracle.\nThe Moodle project was started by Martin Dougiamas in 1999, while he was working at Curtin\nUniversity, Australia. Version 1.0 was released in 2002, at which time PHP4.2 and MySQL 3.23\nwere the technologies available. This limited the kind of architecture that was possible initially, but\nmuch has changed since then. The current release is the Moodle 2.2.x series.\n196\nMoodle\n", "page": 210, "type": "text", "section": "Page 210"}
{"text": "13.1\nAn Overview of How Moodle Works\nA Moodle installation comprises three parts:\n1. The code, typically in a folder like /var/www/moodle or ~/htdocs/moodle. This should not\nbe writable by the web server.\n2. The database, managed by one of the supported RDMSs. In fact, Moodle adds a pre\ufb01x to all\nthe table names, so it can share a database with other applications if desired.\n3. The moodledata folder. This is a folder where Moodle stores uploaded and generated \ufb01les,\nand so needs to be writable by the web server. For security reasons, the should be outside the\nweb root.\nThese can all be on a single server. Alternatively, in a load-balanced set-up, there will be multiple\ncopies of the code on each web server, but just one shared copy of the database and moodledata,\nprobably on other servers.\nThe con\ufb01guration information about these three parts is stored in a \ufb01le called config.php in the\nroot of the moodle folder when Moodle is installed.\nRequest Dispatching\nMoodle is a web applications, so users interact with it using their web browser. From Moodle\u2019s\npoint of view that means responding to HTTP requests. An important aspect of Moodle\u2019s design is,\ntherefore, the URL namespace, and how URLs get dispatched to di\ufb00erent scripts.\nMoodle uses the standard PHP approach to this.\nTo view the main page for a course,\nthe URL would be .../course/view.php?id=123, where 123 is the unique id of the course\nin the database.\nTo view a forum discussion,\nthe URL would be something like\n.../mod/forum/discuss.php?id=456789. That is, these particular scripts, course/view.php\nor mod/forum/discuss.php, would handle these requests.\nThis is simple for the developer. To understand how Moodle handles a particular request, you\nlook at the URL and start reading code there. It is ugly from the user\u2019s point of view. These URLs\nare, however, permanent. The URLs do not change if the course is renamed, or if a moderator moves\na discussion to a di\ufb00erent forum.2\nThe alternative approach one could take is to have a single entry point .../index.php/[extra-\ninformation-to-make-the-request-unique]. The single script index.php would then dispatch\nthe requests in some way. This approach adds a layer of indirection, which is something software\ndevelopers always like to do. The lack of this layer of indirection does not seem to hurt Moodle.\nPlugins\nLike many successful open source projects, Moodle is built out of many plugins, working together\nwith the core of the system. This is a good approach because at allows people to change and enhance\nMoodle in de\ufb01ned ways. An important advantage of an open source system is that you can tailor\nit to your particular needs. Making extensive customisations to the code can, however, lead to big\nproblems when the time comes to upgrade, even when using a good version control system. By\nallowing as many customisations and new features as possible to be implemented as self-contained\nplugins that interact with the Moodle core through a de\ufb01ned API, it is easier for people to customise\n2This is a good property for URLs to have, as explained in Tim Berners-Lee\u2019s article Cool URIs don\u2019t change http:\n//www.w3.org/Provider/Style/URI.html\nTim Hunt\n197\n", "page": 211, "type": "text", "section": "Page 211"}
{"text": "Moodle to their needs, and to share customisations, while still being able to upgrade the core Moodle\nsystem.\nThere are various ways a system can be built as a core surrounded by plugins. Moodle has a\nrelatively fat core, and the plugins are strongly-typed. When I say a fat core, I mean that there is a lot\nof functionality in the core. This contrasts with the kind of architecture where just about everything,\nexcept for a small plugin-loader stub, is a plugin.\nWhen I say plugins are strongly typed, I mean that depending on which type of functionality\nyou want to implement, you have to write a di\ufb00erent type of plugin, and implement a di\ufb00erent API.\nFor example, a new Activity module plugin would be very di\ufb00erent from a new Authentication\nplugin or a new Question type. At the last count there are about 35 di\ufb00erent types of plugin3. This\ncontrasts with the kind of architecture where all plugins use basically the same API and then, perhaps,\nsubscribe to the subset of hooks or events they are interested in.\nGenerally, the trend in Moodle has been to try to shrink the core, by moving more functionality\ninto plugins. This e\ufb00ort has only been somewhat successful, however, because an increasing feature-\nset tends to expand the core. The other trend has been to try to standardise the di\ufb00erent types of\nplugin as much as possible, so that in areas of common functionality, like install and upgrade, all\ntypes of plugins work the same way.\nA plugin in Moodle takes the form of a folder containing \ufb01les. The plugin has a type and a name,\nwhich together make up the \u201cFrankenstyle\u201d component name of the plugin4. The plugin type and\nname determine the path to the plugin folder. The plugin type gives a pre\ufb01x, and the foldername is\nthe plugin name. Here are some examples:\nPlugin type\nPlugin name\nFrankenstyle\nFolder\nmod (Activity module)\nforum\nmod_forum\nmod/forum\nmod (Activity module)\nquiz\nmod_quiz\nmod/quiz\nblock (Side-block)\nnavigation\nblock_navigation\nblocks/navigation\nqtype (Question type)\nshortanswer\nqtype_shortanswer\nquestion/type/shortanswer\nquiz (Quiz report)\nstatistics\nquiz_statistics\nmod/quiz/report/statistics\nThe last example shows that each activity module is allowed to declare sub-plugin types. At the\nmoment only activity modules can do this, for two reasons. If all plugins could have sub-plugins that\nmight cause performance problems. Activity modules are the main educational activities in Moodle,\nand so are the most important type of plugin, thus they get special privileges.\nAn Example Plugin\nI will explain a lot of details of the Moodle architecture by considering a speci\ufb01c example plugin.\nAs is traditional, I have chosen to implement a plugin that displays \u201cHello world\u201d.\nThis plugin does not really \ufb01t naturally into any of the standard Moodle plugin types. It is just a\nscript, with no connection to anything else, so I will choose to implement it as a \u201clocal\u201d plugin. This\nis a catch-all plugin type for miscellaneous functionality that does not \ufb01t anywhere better. I will name\nmy plugin greet, to give a Frankensyle name of local_greet, and a folder path of local/greet.5\nEach plugin must contain a \ufb01le called version.php which de\ufb01nes some basic metadata about\nthe plugin. This is used by the Moodle\u2019s plugin installer system to install and upgrade the plugin.\nFor example, local/greet/version.php contains:\n3For a full list of Moodle plugin types see http://docs.moodle.org/dev/Plugins.\n4The word \u201cFrankenstyle\u201d arose out of an argument in the developers\u2019 Jabber channel, but everyone liked it and it stuck.\n5The plugin code can be downloaded from https://github.com/timhunt/moodle-local_greet.\n198\nMoodle\n", "page": 212, "type": "text", "section": "Page 212"}
{"text": "<?php\n$plugin->component\n= \u2019local_greet\u2019;\n$plugin->version\n= 2011102900;\n$plugin->requires\n= 2011102700;\n$plugin->maturity\n= MATURITY_STABLE;\nIt may seem redundant to include the component name, since this can be deduced from the path,\nbut the installer uses this to verify that the plugin has been installed in the right place. The version\n\ufb01eld is the version of this plugin. Maturity is ALPHA, BETA, RC (release candidate), or STABLE.\nRequires is the minimum version of Moodle that this plugin is compatible with. If necessary, one\ncan also document other plugins that this one depends on.\nHere is the main script for this simple plugin (stored in local/greet/index.php):\n<?php\nrequire_once(dirname(__FILE__) . \u2019/../../config.php\u2019);\n// 1\nrequire_login();\n// 2\n$context = context_system::instance();\n// 3\nrequire_capability(\u2019local/greet:begreeted\u2019, $context);\n// 4\n$name = optional_param(\u2019name\u2019, \u2019\u2019, PARAM_TEXT);\n// 5\nif (!$name) {\n$name = fullname($USER);\n// 6\n}\nadd_to_log(SITEID, \u2019local_greet\u2019, \u2019begreeted\u2019,\n\u2019local/greet/index.php?name=\u2019 . urlencode($name));\n// 7\n$PAGE->set_context($context);\n// 8\n$PAGE->set_url(new moodle_url(\u2019/local/greet/index.php\u2019),\narray(\u2019name\u2019 => $name));\n// 9\n$PAGE->set_title(get_string(\u2019welcome\u2019, \u2019local_greet\u2019));\n// 10\necho $OUTPUT->header();\n// 11\necho $OUTPUT->box(get_string(\u2019greet\u2019, \u2019local_greet\u2019,\nformat_string($name)));\n// 12\necho $OUTPUT->footer();\n// 13\nLine 1: Bootstrapping Moodle\nrequire_once(dirname(__FILE__) . \u2019/../../config.php\u2019);\n// 1\nThe single line of this script that does the most work is the \ufb01rst. I said above that config.php\ncontains the details Moodle needs to connect to the database and \ufb01nd the moodledata folder. It ends,\nhowever, with the line require_once(\u2019lib/setup.php\u2019). This:\n1. loads all the standard Moodle libraries using require_once;\n2. starts the session handling;\n3. connects to the database; and\n4. sets up a number of global variables, which we shall meet later.\nTim Hunt\n199\n", "page": 213, "type": "text", "section": "Page 213"}
{"text": "Line 2: Checking the User Is Logged In\nrequire_login();\n// 2\nThis line causes Moodle to check that the current user is logged in, using whatever authentication\nplugin the administrator has con\ufb01gured. If not, the user will be redirected to the log-in form, and\nthis function will never return.\nA script that was more integrated into Moodle would pass more arguments here, to say which\ncourse or activity this page is part of, and then require_login would also verify that the user is\nenrolled in, or otherwise allowed to access this course, and is allowed to see this activity. If not, an\nappropriate error would be displayed.\n13.2\nMoodle\u2019s Roles and Permissions System\nThe next two lines of code show how to check that the user has permission to do something. As you\ncan see, from the developer\u2019s point of view, the API is very simple. Behind the scenes, however,\nthere is a sophisticated access system which gives the administrator great \ufb02exibility to control who\ncan do what.\nLine 3: Getting the Context\n$context = context_system::instance();\n// 3\nIn Moodle, users can have di\ufb00erent permissions in di\ufb00erent places. For example, a user might be\na Teacher in one course, and a Student in another, and so have di\ufb00erent permissions in each place.\nThese places are called contexts. Contexts in Moodle form a hierarchy rather like a folder hierarchy in\na \ufb01le-system. At the top level is the System context (and, since this script is not very well integrated\ninto Moodle, it uses that context).\nWithin the System context are a number of contexts for the di\ufb00erent categories that have been\ncreated to organise courses. These can be nested, with one category containing other categories.\nCategory contexts can also contain Course contexts. Finally, each activity in a course will have its\nown Module context.\nLine 4: Checking the User Has Permission to Use This Script\nrequire_capability(\u2019local/greet:begreeted\u2019, $context);\n// 4\nHaving got the context\u2014the relevant area of Moodle\u2014the permission can be checked. Each\nbit of functionality that a user may or may not have is called a capability. Checking a capability\nprovides more \ufb01ne-grained access control than the basic checks performed by require_login. Our\nsimple example plugin has just one capability: local/greet:begreeted.\nThe check is done using the require_capability function, which takes the capability name\nand the context. Like other require_... functions, it will not return if the user does not have the\ncapability. It will display an error instead. In other places the non-fatal has_capability function,\nwhich returns a Boolean would be used, for example, to determine whether to display a link to this\nscript from another page.\nHow does the administrator con\ufb01gure which user has which permission? Here is the calculation\nthat has_capability performs (at least conceptually):\n200\nMoodle\n", "page": 214, "type": "text", "section": "Page 214"}
{"text": "Figure 13.3: Contexts\n1. Start from the current Context.\n2. Get a list of the Roles that the user has in this Context.\n3. Then work out what the Permission is for each Role in this Context.\n4. Aggregate those permissions to get a \ufb01nal answer.\nDe\ufb01ning Capabilities\nAs the example shows, a plugin can de\ufb01ne new capabilities relating to the particular functionality it\nprovides. Inside each Moodle plugin there is a sub-folder of the code called db. This contains all the\ninformation required to install or upgrade the plugin. One of those bits of information is a \ufb01le called\naccess.php that de\ufb01nes the capabilities. Here is the access.php \ufb01le for our plugin, which lives in\nlocal/greet/db/access.php:\n<?php\n$capabilities = array(\u2019local/greet:begreeted\u2019 => array(\n\u2019captype\u2019 => \u2019read\u2019,\n\u2019contextlevel\u2019 => CONTEXT_SYSTEM,\n\u2019archetypes\u2019 => array(\u2019guest\u2019 => CAP_ALLOW, \u2019user\u2019 => CAP_ALLOW)\n));\nThis gives some metadata about each capability which are used when constructing the permissions\nmanagement user interface. It also give default permissions for common types of role.\nRoles\nThe next part of the Moodle permissions system is roles. A role is really just a named set of\npermissions. When you are logged into Moodle, you will have the \u201cAuthenticated user\u201d role in\nthe System context, and since the System context is the root of the hierarchy, that role will apply\neverywhere.\nWithin a particular course, you may be a Student, and that role assignment will apply in the\nCourse context and all the Module contexts within it. In another course, however, you may have a\nTim Hunt\n201\n", "page": 215, "type": "text", "section": "Page 215"}
{"text": "di\ufb00erent role. For example, Mr Gradgrind may be Teacher in the \u201cFacts, Facts, Facts\u201d course, but a\nStudent in the professional development course \u201cFacts Aren\u2019t Everything\u201d. Finally, a user might be\ngiven the Moderator role in one particular forum (Module context).\nPermissions\nA role de\ufb01nes a permission for each capability. For example the Teacher role will probably ALLOW\nmoodle/course:manage, but the Student role will not. However, both Student and Teacher will\nallow mod/forum:startdiscussion.\nThe roles are normally de\ufb01ned globally, but they can be re-de\ufb01ned in each context. For exam-\nple, one particular wiki can be made read-only to students by overriding the permission for the\nmod/wiki:edit capability for the Student role in that wiki (Module) context, to PREVENT.\nThere are four Permissions:\n\u2022 NOT SET/INHERIT (default)\n\u2022 ALLOW\n\u2022 PREVENT\n\u2022 PROHIBIT\nIn a given context, a role will have one of these four permissions for each capability. One di\ufb00erence\nbetween PROHIBIT and PREVENT is that a PROHIBIT cannot be overridden in sub-contexts.\nPermission Aggregation\nFinally the permissions for all the roles the user has in this context are aggregated.\n\u2022 If any role gives the permission PROHIBIT for this capability, return false.\n\u2022 Otherwise, if any role gives ALLOW for this capability, return true.\n\u2022 Otherwise return false.\nA use case for PROHIBIT is this: Suppose a user has been making abusive posts in a number\nof forums, and we want to stop them immediately. We can create a Naughty user role, which sets\nmod/forum:post and other such capabilities to PROHIBIT. We can then assign this role to the\nabusive user in the System context. That way, we can be sure that the user will not be able to post any\nmore in any forum. (We would then talk to the student, and having reached a satisfactory outcome,\nremove that role assignment so that they may use the system again.)\nSo, Moodle\u2019s permissions system gives administrators a huge amount of \ufb02exibility. They can\nde\ufb01ne whichever roles they like with di\ufb00erent permissions for each capability; they can alter the role\nde\ufb01nitions in sub-contexts; and then they can assign di\ufb00erent roles to users in di\ufb00erent contexts.\n13.3\nBack to Our Example Script\nThe next part of the script illustrates some miscellaneous points:\nLine 5: Get Data From the Request\n$name = optional_param(\u2019name\u2019, \u2019\u2019, PARAM_TEXT);\n// 5\n202\nMoodle\n", "page": 216, "type": "text", "section": "Page 216"}
{"text": "Something that every web application has to do is get data from a request (GET or POST variables)\nwithout being susceptible to SQL injection or cross-site scripting attacks. Moodle provides two ways\nto do this.\nThe simple method is the one shown here. It gets a single variable given the parameter name\n(here name) a default value, and the expected type. The expected type is used to clean the input of all\nunexpected characters. There are numerous types like PARAM_INT, PARAM_ALPHANUM, PARAM_EMAIL,\nand so on.\nThere is also a similar required_param function, which like other require_... functions stops\nexecution and displays an error message if the expected parameter is not found.\nThe other mechanism Moodle has for getting data from the request is a fully \ufb02edged forms library.\nThis is a wrapper around the HTML QuickForm library from PEAR6. This seemed like a good choice\nwhen it was selected, but is now no longer maintained. At some time in the future we will have\nto tackle moving to a new forms library, which many of us look forwards to, because QuickForm\nhas several irritating design issues. For now, however, it is adequate. Forms can be de\ufb01ned as a\ncollection of \ufb01elds of various types (e.g. text box, select drop-down, date-selector) with client- and\nserver- side validation (including use of the same PARAM_... types).\nLine 6: Global Variables\nif (!$name) {\n$name = fullname($USER);\n// 6\n}\nThis snippet shows the \ufb01rst of the global variables Moodle provides. $USER makes accessible\nthe information about the user accessing this script. Other globals include:\n\u2022 $CFG: holds the commonly used con\ufb01guration settings.\n\u2022 $DB: the database connection.\n\u2022 $SESSION: a wrapper around the PHP session.\n\u2022 $COURSE: the course the current request relates to.\nand several others, some of which we will encounter below.\nYou may have read the words \u201cglobal variable\u201d with horror. Note, however, that PHP processes a\nsingle request at a time. Therefore these variables are not as global as all that. In fact, PHP global\nvariables can be seen as an implementation of the thread-scoped registry pattern7 and this is the way\nin which Moodle uses them. It is very convenient in that it makes commonly used objects available\nthroughout the code, without requiring them to be passed to every function and method. It is only\ninfrequently abused.\nNothing is Simple\nThis line also serves to make a point about the problem domain: nothing is ever simple. To dis-\nplay a user\u2019s name is more complicated than simply concatenating $USER->firstname, \u2019 \u2019, and\n$USER->lastname. The school may have policies about showing either of those parts, and di\ufb00erent\ncultures have di\ufb00erent conventions for which order to show names. Therefore, there are several\ncon\ufb01gurations settings and a function to assemble the full name according to the rules.\n6For non-PHP programmers, PEAR is PHP\u2019s equivalent of CPAN.\n7See Martin Fowler\u2019s Patterns of Enterprise Application Architecture.\nTim Hunt\n203\n", "page": 217, "type": "text", "section": "Page 217"}
{"text": "Dates are a similar problem. Di\ufb00erent users may be in di\ufb00erent time-zones. Moodle stores\nall dates as Unix time-stamps, which are integers, and so work in all databases. There is then a\nuserdate function to display the time-stamp to the user using the appropriate timezone and locale\nsettings.\nLine 7: Logging\nadd_to_log(SITEID, \u2019local_greet\u2019, \u2019begreeted\u2019,\n\u2019local/greet/index.php?name=\u2019 . urlencode($name));\n// 7\nAll signi\ufb01cant actions in Moodle are logged. Logs are written to a table in the database. This is\na trade-o\ufb00. It makes sophisticated analysis quite easy, and indeed various reports based on the logs\nare included with Moodle. On a large and busy site, however, it is a performance problem. The log\ntable gets huge, which makes backing up the database more di\ufb03cult, and makes queries on the log\ntable slow. There can also be write contention on the log table. These problems can be mitigated in\nvarious ways, for example by batching writes, or archiving or deleting old records to remove them\nfrom the main database.\n13.4\nGenerating Output\nOutput is mainly handled via two global objects.\nLine 8: The $PAGE Global\n$PAGE->set_context($context);\n// 8\n$PAGE stores the information about the page to be output. This information is then readily\navailable to the code that generates the HTML. This script needs to explicitly specify the current\ncontext. (In other situations, this might have been set automatically by require_login.) The URL\nfor this page must also be set explicitly. This may seem redundant, but the rationale for requiring it is\nthat you might get to a particular page using any number of di\ufb00erent URLs, but the URL passed to\nset_url should be the canonical URL for the page\u2014a good permalink, if you like. The page title is\nalso set. This will end up in the head element of the HTML.\nLine 9: Moodle URL\n$PAGE->set_url(new moodle_url(\u2019/local/greet/index.php\u2019),\narray(\u2019name\u2019 => $name));\n// 9\nI just wanted to \ufb02ag this nice little helper class which makes manipulating URLs much easier. As\nan aside, recall that the add_to_log function call above did not use this helper class. Indeed, the log\nAPI cannot accept moodle_url objects. This sort of inconsistency is a typical sign of a code-base as\nold as Moodle\u2019s.\nLine 10: Internationalisation\n$PAGE->set_title(get_string(\u2019welcome\u2019, \u2019local_greet\u2019));\n// 10\n204\nMoodle\n", "page": 218, "type": "text", "section": "Page 218"}
{"text": "Moodle uses its own system to allow the interface to be translated into any language. There may\nnow be good PHP internationalisation libraries, but in 2002 when it was \ufb01rst implemented there was\nnot one available that was adequate. The system is based around the get_string function. Strings\nare identi\ufb01ed by a key and the plugin Frankenstyle name. As can be seen on line 12, it is possible to\ninterpolate values into the string. (Multiple values are handled using PHP arrays or objects.)\nThe strings are looked up in language \ufb01les that are just plain PHP arrays. Here is the language\n\ufb01le local/greet/lang/en/local_greet.php for our plugin:\n<?php\n$string[\u2019greet:begreeted\u2019] = \u2019Be greeted by the hello world example\u2019;\n$string[\u2019welcome\u2019] = \u2019Welcome\u2019;\n$string[\u2019greet\u2019] = \u2019Hello, {$a}!\u2019;\n$string[\u2019pluginname\u2019] = \u2019Hello world example\u2019;\nNote that, as well as the two string used in our script, there are also strings to give a name to the\ncapability, and the name of the plugin as it appears in the user interface.\nThe di\ufb00erent languages are identi\ufb01ed by the two-letter country code (en here). Languages packs\nmay derive from other language packs. For example the fr_ca (French Canadian) language pack\ndeclares fr (French) as the parent language, and thus only has to de\ufb01ne those strings that di\ufb00er from\nthe French. Since Moodle originated in Australia, en means British English, and en_us (American\nEnglish) is derived from it.\nAgain, the simple get_string API for plugin developers hides a lot of complexity, including\nworking out the current language (which may depend on the current user\u2019s preferences, or the settings\nfor the particular course they are currently in), and then searching through all the language packs and\nparent language packs to \ufb01nd the string.\nProducing the language pack \ufb01les, and co-ordinating the translation e\ufb00ort is managed at http:\n//lang.moodle.org/, which is Moodle with a custom plugin8. It uses both Git and the database as\na backend to store the language \ufb01les with full version history.\nLine 11: Starting Output\necho $OUTPUT->header();\n// 11\nThis is another innocuous-looking line that does much more than it seems. The point is that\nbefore any output can be done, the applicable theme (skin) must be worked out. This may depend on\na combination of the page context and the user\u2019s preferences. $PAGE->context was, however, only\nset on line 8, so the $OUTPUT global could not have been initialised at the start of the script. In order\nto solve this problem, some PHP magic is used to create the proper $OUTPUT object based on the\ninformation in $PAGE the \ufb01rst time any output method is called.\nAnother thing to consider is that every page in Moodle may contain blocks. These are extra\ncon\ufb01gurable bits of content that are normally displayed to the left or right of the main content. (They\nare a type of plugin.) Again, the exact collection of blocks to display depends, in a \ufb02exible way\n(that the administrator can control) on the page context and some other aspects of the page identity.\nTherefore, another part of preparing for output is a call to $PAGE->blocks->load_blocks().\nOnce all the necessary information has been worked out, the theme plugin (that controls the\noverall look of the page) is called to generate the overall page layout, including whatever standard\nheader and footer is desired. This call is also responsible for adding the output from the blocks\n8local_amos, http://docs.moodle.org/22/en/AMOS.\nTim Hunt\n205\n", "page": 219, "type": "text", "section": "Page 219"}
{"text": "at the appropriate place in the HTML. In the middle of the layout there will be a div where the\nspeci\ufb01c content for this page goes. The HTML of this layout is generated, and then split in half after\nthe start of the main content div. The \ufb01rst half is returned, and the rest is stored to be returned by\n$OUTPUT->footer().\nLine 12: Outputting the Body of the Page\necho $OUTPUT->box(get_string(\u2019greet\u2019, \u2019local_greet\u2019,\nformat_string($name)));\n// 12\nThis line outputs the body of the page. Here it simply displays the greeting in a box. The\ngreeting is, again, a localised string, this time with a value substituted into a placeholder. The core\nrenderer $OUTPUT provides many convenience methods like box to describe the required output in\nquite high-level terms. Di\ufb00erent themes can control what HTML is actually output to make the box.\nThe content that originally came from the user ($name) is output though the format_string\nfunction. This is the other part of providing XSS protection. It also enables the user of text \ufb01lters\n(another plugin type). An example \ufb01lter would be the LaTeX \ufb01lter, which replaces input like\n$$x + 1$$ with an image of the equation. I will mention, but not explain, that there are actually\nthree di\ufb00erent functions (s, format_string, and format_text) depending on the particular type\nof content being output.\nLine 13: Finishing Output\necho $OUTPUT->footer();\n// 13\nFinally, the footer of the page is output. This example does not show it, but Moodle tracks all the\nJavaScript that is required by the page, and outputs all the necessary script tags in the footer. This is\nstandard good practice. It allows users to see the page without waiting for all the JavaScript to load. A\ndeveloper would include JavaScript using API calls like $PAGE->requires->js(\u2019/local/greet/\ncooleffect.js\u2019).\nShould This Script Mix Logic and Output?\nObviously, putting the output code directly in index.php, even if at a high level of abstraction, limits\nthe \ufb02exibility that themes have to control the output. This is another sign of the age of the Moodle\ncode-base. The $OUTPUT global was introduced in 2010 as a stepping stone on the way from the old\ncode, where the output and controller code were in the same \ufb01le, to a design where all the view code\nwas properly separated. This also explains the rather ugly way that the entire page layout is generated,\nthen split in half, so that any output from the script itself can be placed between the header and the\nfooter. Once the view code has been separated out of the script, into what Moodle calls a renderer,\nthe theme can then choose to completely (or partially) override the view code for a given script.\nA small refactoring can move all the output code out of our index.php and into a renderer. The\nend of index.php (lines 11 to 13) would change to:\n$output = $PAGE->get_renderer(\u2019local_greet\u2019);\necho $output->greeting_page($name);\nand there would be a new \ufb01le local/greet/renderer.php:\n206\nMoodle\n", "page": 220, "type": "text", "section": "Page 220"}
{"text": "<?php\nclass local_greet_renderer extends plugin_renderer_base {\npublic function greeting_page($name) {\n$output = \u2019\u2019;\n$output .= $this->header();\n$output .= $this->box(get_string(\u2019greet\u2019, \u2019local_greet\u2019, $name));\n$output .= $this->footer();\nreturn $output;\n}\n}\nIf the theme wished to completely change this output, it would de\ufb01ne a subclass of this renderer\nthat overrides the greeting_page method. $PAGE->get_renderer() determines the appropriate\nrenderer class to instantiate depending on the current theme. Thus, the output (view) code is fully\nseparated from the controller code in index.php, and the plugin has been refactored from typical\nlegacy Moodle code to a clean MVC architecture.\n13.5\nDatabase Abstraction\nThe \"Hello world\" script was su\ufb03ciently simple that it did not need to access the database, although\nseveral of the Moodle library calls used did do database queries. I will now brie\ufb02y describe the\nMoodle database layer.\nMoodle used to use the ADOdb library as the basis of its database abstraction layer, but there were\nissues for us, and the extra layer of library code had a noticeable impact on performance. Therefore,\nin Moodle 2.0 we switched to our own abstraction layer, which is a thin wrapper around the various\nPHP database libraries.\nThe moodle_database Class\nThe heart of the library is the moodle_database class. This de\ufb01nes the interface provided by the\n$DB global variable, which gives access to the database connection. A typical usage might be:\n$course = $DB->get_record(\u2019course\u2019, array(\u2019id\u2019 => $courseid));\nThat translates into the SQL:\nSELECT * FROM mdl_course WHERE id = $courseid;\nand returns the data as a plain PHP object with public \ufb01elds, so you could access $course->id,\n$course->fullname, etc.\nSimple methods like this deal with basic queries, and simple updates and inserts. Sometimes it\nis necessary to do more complex SQL, for example to run reports. In that case, there are methods to\nexecute arbitrary SQL:\n$courseswithactivitycounts = $DB->get_records_sql(\n\u2019SELECT c.id, \u2019 . $DB->sql_concat(\u2019shortname\u2019, \"\u2019 \u2019\", \u2019fullname\u2019) . \u2019 AS coursename,\nCOUNT(1) AS activitycount\nFROM {course} c\nJOIN {course_modules} cm ON cm.course = c.id\nTim Hunt\n207\n", "page": 221, "type": "text", "section": "Page 221"}
{"text": "WHERE c.category = :categoryid\nGROUP BY c.id, c.shortname, c.fullname ORDER BY c.shortname, c.fullname\u2019,\narray(\u2019categoryid\u2019 => $category));\nSome things to note there:\n\u2022 The table names are wrapped in {} so that the library can \ufb01nd them and prepend the table\nname pre\ufb01x.\n\u2022 The library uses placeholders to insert values into the SQL. In some cases this uses the facilities\nof the underlying database driver. In other cases the values have to be escaped and inserted\ninto the SQL using string manipulation. The library supports both named placeholders (as\nabove) and anonymous ones, using ? as the placeholder.\n\u2022 For queries to work on all our supported databases a safe subset of standard SQL must be used.\nFor example, you can see that I have used the AS keyword for column aliases, but not for table\naliases. Both of these usage rules are necessary.\n\u2022 Even so, there are some situations where no subset of standard SQL will work on all our\nsupported databases; for example, every database has a di\ufb00erent way to concatenate strings. In\nthese cases there are compatibility functions to generate the correct SQL.\nDe\ufb01ning the Database Structure\nAnother area where database management systems di\ufb00er a lot is in the SQL syntax required to de\ufb01ne\ntables. To get around this problem, each Moodle plugin (and Moodle core) de\ufb01nes the required\ndatabase tables in an XML \ufb01le. The Moodle install system parses the install.xml \ufb01les and uses\nthe information they contain to create the required tables and indexes. There is a developer tool\ncalled XMLDB built into Moodle to help create and edit these install \ufb01les.\nIf the database structure needs to change between two releases of Moodle (or of a plugin) then\nthe developer is responsible for writing code (using an additional database object that provides DDL\nmethods) to update the database structure, while preserving all the users\u2019 data. Thus, Moodle will\nalways self-update from one release to the next, simplifying maintenance for administrators.\nOne contentious point, stemming from the fact that Moodle started out using MySQL 3, is that the\nMoodle database does not use foreign keys. This allows some buggy behaviour to remain undetected\neven though modern databases would be capable of detecting the problem. The di\ufb03culty is that\npeople have been running Moodle sites without foreign keys for years, so there is almost certainly\ninconsistent data present. Adding the keys now would be impossible, without a very di\ufb03cult clean-up\njob. Even so, since the XMLDB system was added to Moodle 1.7 (in 2006!) the install.xml \ufb01les\nhave contained the de\ufb01nitions of the foreign keys that should exist, and we are still hoping, one day,\nto do all the work necessary to allow us to create them during the install process.\n13.6\nWhat Has Not Been Covered\nI hope I have given you a good overview of how Moodle works. Due to lack of space I have had to\nomit several interesting topics, including how authentication, enrolment and grade plugins allow\nMoodle to interoperate with student information systems, and the interesting content-addressed way\nthat Moodle stores uploaded \ufb01les. Details of these, and other aspects of Moodle\u2019s design, can be\nfound in the developer documentation9.\n9http://docs.moodle.org/dev/\n208\nMoodle\n", "page": 222, "type": "text", "section": "Page 222"}
{"text": "13.7\nLessons Learned\nOne interesting aspect of working on Moodle is that it came out of a research project. Moodle enables\n(but does not enforce) a social constructivist pedagogy10. That is, we learn best by actually creating\nsomething, and we learn from each other as a community. Martin Dougiamas\u2019s PhD question did\nnot ask whether this was an e\ufb00ective model for education, but rather whether it is an e\ufb00ective model\nfor running an open source project. That is, can we view the Moodle project as an attempt to learn\nhow to build and use a VLE, and an attempt to learn that by actually building and using Moodle as a\ncommunity where teachers, developers, administrators and students all teach and learn from each\nother? I \ufb01nd this a good model for thinking about an open source software development project. The\nmain place where developers and users learn from each other is in discussions in the Moodle project\nforums, and in the bug database.\nPerhaps the most important consequence of this learning approach is that you should not be\nafraid to start by implementing the simplest possible solution \ufb01rst. For example, early versions of\nMoodle had just a few hard-coded roles like Teacher, Student and Administrator. That was enough\nfor many years, but eventually the limitations had to be addressed. When the time came to design\nthe Roles system for Moodle 1.7, there was a lot of experience in the community about how people\nwere using Moodle, and many little feature requests that showed what people needed to be able to\nadjust using a more \ufb02exible access control system. This all helped design the Roles system to be as\nsimple as possible, but as complex as necessary. (In fact, the \ufb01rst version of the roles system ended\nup slightly too complex, and it was subsequently simpli\ufb01ed a little in Moodle 2.0.)\nIf you take the view that programming is a problem-solving exercise, then you might think that\nMoodle got the design wrong the \ufb01rst time, and later had to waste time correcting it. I suggest that\nis an unhelpful viewpoint when trying to solve complex real-world problems. At the time Moodle\nstarted, no-one knew enough to design the roles system we now have. If you take the learning\nviewpoint, then the various stages Moodle went through to reach the current design were necessary\nand inevitable.\nFor this perspective to work, it must be possible to change almost any aspect of a system\u2019s\narchitecture once you have learned more. I think Moodle shows that this is possible. For example,\nwe found a way for code to be gradually refactored from legacy scripts to a cleaner MVC architecture.\nThis requires e\ufb00ort, but it seems that when necessary, the resources to implement these changes can\nbe found in open source projects. From the user\u2019s point of view, the system gradually evolves with\neach major release.\n10http://docs.moodle.org/22/en/Pedagogy\n209\n", "page": 223, "type": "text", "section": "Page 223"}
{"text": "210\nMoodle\n", "page": 224, "type": "text", "section": "Page 224"}
{"text": "[chapter14]\nnginx\nAndrew Alexeev\nnginx (pronounced \u201cengine x\u201d) is a free open source web server written by Igor Sysoev, a Russian\nsoftware engineer. Since its public launch in 2004, nginx has focused on high performance, high\nconcurrency and low memory usage. Additional features on top of the web server functionality, like\nload balancing, caching, access and bandwidth control, and the ability to integrate e\ufb03ciently with a\nvariety of applications, have helped to make nginx a good choice for modern website architectures.\nCurrently nginx is the second most popular open source web server on the Internet.\n14.1\nWhy Is High Concurrency Important?\nThese days the Internet is so widespread and ubiquitous it\u2019s hard to imagine it wasn\u2019t exactly there,\nas we know it, a decade ago. It has greatly evolved, from simple HTML producing clickable text,\nbased on NCSA and then on Apache web servers, to an always-on communication medium used by\nmore than 2 billion users worldwide. With the proliferation of permanently connected PCs, mobile\ndevices and recently tablets, the Internet landscape is rapidly changing and entire economies have\nbecome digitally wired. Online services have become much more elaborate with a clear bias towards\ninstantly available live information and entertainment. Security aspects of running online business\nhave also signi\ufb01cantly changed. Accordingly, websites are now much more complex than before,\nand generally require a lot more engineering e\ufb00orts to be robust and scalable.\nOne of the biggest challenges for a website architect has always been concurrency. Since\nthe beginning of web services, the level of concurrency has been continuously growing. It\u2019s not\nuncommon for a popular website to serve hundreds of thousands and even millions of simultaneous\nusers. A decade ago, the major cause of concurrency was slow clients\u2014users with ADSL or dial-\nup connections. Nowadays, concurrency is caused by a combination of mobile clients and newer\napplication architectures which are typically based on maintaining a persistent connection that\nallows the client to be updated with news, tweets, friend feeds, and so on. Another important factor\ncontributing to increased concurrency is the changed behavior of modern browsers, which open four\nto six simultaneous connections to a website to improve page load speed.\nTo illustrate the problem with slow clients, imagine a simple Apache-based web server which\nproduces a relatively short 100 KB response\u2014a web page with text or an image. It can be merely a\nfraction of a second to generate or retrieve this page, but it takes 10 seconds to transmit it to a client\nwith a bandwidth of 80 kbps (10 KB/s). Essentially, the web server would relatively quickly pull 100\nKB of content, and then it would be busy for 10 seconds slowly sending this content to the client\nbefore freeing its connection. Now imagine that you have 1,000 simultaneously connected clients\n", "page": 225, "type": "text", "section": "Page 225"}
{"text": "who have requested similar content. If only 1 MB of additional memory is allocated per client, it\nwould result in 1000 MB (about 1 GB) of extra memory devoted to serving just 1000 clients 100 KB\nof content. In reality, a typical web server based on Apache commonly allocates more than 1 MB of\nadditional memory per connection, and regrettably tens of kbps is still often the e\ufb00ective speed of\nmobile communications. Although the situation with sending content to a slow client might be, to\nsome extent, improved by increasing the size of operating system kernel socket bu\ufb00ers, it\u2019s not a\ngeneral solution to the problem and can have undesirable side e\ufb00ects.\nWith persistent connections the problem of handling concurrency is even more pronounced,\nbecause to avoid latency associated with establishing new HTTP connections, clients would stay\nconnected, and for each connected client there\u2019s a certain amount of memory allocated by the web\nserver.\nConsequently, to handle the increased workloads associated with growing audiences and hence\nhigher levels of concurrency\u2014and to be able to continuously do so\u2014a website should be based on a\nnumber of very e\ufb03cient building blocks. While the other parts of the equation such as hardware (CPU,\nmemory, disks), network capacity, application and data storage architectures are obviously important,\nit is in the web server software that client connections are accepted and processed. Thus, the web\nserver should be able to scale nonlinearly with the growing number of simultaneous connections and\nrequests per second.\nIsn\u2019t Apache Suitable?\nApache, the web server software that still largely dominates the Internet today, has its roots in the\nbeginning of the 1990s. Originally, its architecture matched the then-existing operating systems\nand hardware, but also the state of the Internet, where a website was typically a standalone physical\nserver running a single instance of Apache. By the beginning of the 2000s it was obvious that the\nstandalone web server model could not be easily replicated to satisfy the needs of growing web\nservices. Although Apache provided a solid foundation for future development, it was architected\nto spawn a copy of itself for each new connection, which was not suitable for nonlinear scalability\nof a website. Eventually Apache became a general purpose web server focusing on having many\ndi\ufb00erent features, a variety of third-party extensions, and universal applicability to practically any\nkind of web application development. However, nothing comes without a price and the downside to\nhaving such a rich and universal combination of tools in a single piece of software is less scalability\nbecause of increased CPU and memory usage per connection.\nThus, when server hardware, operating systems and network resources ceased to be major\nconstraints for website growth, web developers worldwide started to look around for a more e\ufb03cient\nmeans of running web servers. Around ten years ago, Daniel Kegel, a prominent software engineer,\nproclaimed that \u201cit\u2019s time for web servers to handle ten thousand clients simultaneously\u201d1 and\npredicted what we now call Internet cloud services. Kegel\u2019s C10K manifest spurred a number of\nattempts to solve the problem of web server optimization to handle a large number of clients at the\nsame time, and nginx turned out to be one of the most successful ones.\nAimed at solving the C10K problem of 10,000 simultaneous connections, nginx was written\nwith a di\ufb00erent architecture in mind\u2014one which is much more suitable for nonlinear scalability in\nboth the number of simultaneous connections and requests per second. nginx is event-based, so it\ndoes not follow Apache\u2019s style of spawning new processes or threads for each web page request. The\nend result is that even as load increases, memory and CPU usage remain manageable. nginx can\nnow deliver tens of thousands of concurrent connections on a server with typical hardware.\n1http://www.kegel.com/c10k.html\n212\nnginx\n", "page": 226, "type": "text", "section": "Page 226"}
{"text": "When the \ufb01rst version of nginx was released, it was meant to be deployed alongside Apache\nsuch that static content like HTML, CSS, JavaScript and images were handled by nginx to o\ufb04oad\nconcurrency and latency processing from Apache-based application servers. Over the course of\nits development, nginx has added integration with applications through the use of FastCGI, uswgi\nor SCGI protocols, and with distributed memory object caching systems like memcached. Other\nuseful functionality like reverse proxy with load balancing and caching was added as well. These\nadditional features have shaped nginx into an e\ufb03cient combination of tools to build a scalable web\ninfrastructure upon.\nIn February 2012, the Apache 2.4.x branch was released to the public. Although this latest\nrelease of Apache has added new multi-processing core modules and new proxy modules aimed\nat enhancing scalability and performance, it\u2019s too soon to tell if its performance, concurrency and\nresource utilization are now on par with, or better than, pure event-driven web servers. It would be\nvery nice to see Apache application servers scale better with the new version, though, as it could\npotentially alleviate bottlenecks on the backend side which still often remain unsolved in typical\nnginx-plus-Apache web con\ufb01gurations.\nAre There More Advantages to Using nginx?\nHandling high concurrency with high performance and e\ufb03ciency has always been the key bene\ufb01t of\ndeploying nginx. However, there are now even more interesting bene\ufb01ts.\nIn the last few years, web architects have embraced the idea of decoupling and separating their\napplication infrastructure from the web server. However, what would previously exist in the form\nof a LAMP (Linux, Apache, MySQL, PHP, Python or Perl)-based website, might now become not\nmerely a LEMP-based one (\u2018E\u2019 standing for \u2018Engine x\u2019), but more and more often an exercise in\npushing the web server to the edge of the infrastructure and integrating the same or a revamped set\nof applications and database tools around it in a di\ufb00erent way.\nnginx is very well suited for this, as it provides the key features necessary to conveniently o\ufb04oad\nconcurrency, latency processing, SSL (secure sockets layer), static content, compression and caching,\nconnections and requests throttling, and even HTTP media streaming from the application layer to a\nmuch more e\ufb03cient edge web server layer. It also allows integrating directly with memcached/Redis\nor other \u201cNoSQL\u201d solutions, to boost performance when serving a large number of concurrent users.\nWith recent \ufb02avors of development kits and programming languages gaining wide use, more\nand more companies are changing their application development and deployment habits. nginx has\nbecome one of the most important components of these changing paradigms, and it has already\nhelped many companies start and develop their web services quickly and within their budgets.\nThe \ufb01rst lines of nginx were written in 2002. In 2004 it was released to the public under the\ntwo-clause BSD license. The number of nginx users has been growing ever since, contributing ideas,\nand submitting bug reports, suggestions and observations that have been immensely helpful and\nbene\ufb01cial for the entire community.\nThe nginx codebase is original and was written entirely from scratch in the C programming\nlanguage. nginx has been ported to many architectures and operating systems, including Linux,\nFreeBSD, Solaris, Mac OS X, AIX and Microsoft Windows. nginx has its own libraries and with\nits standard modules does not use much beyond the system\u2019s C library, except for zlib, PCRE and\nOpenSSL which can be optionally excluded from a build if not needed or because of potential license\ncon\ufb02icts.\nA few words about the Windows version of nginx. While nginx works in a Windows environment,\nthe Windows version of nginx is more like a proof-of-concept rather than a fully functional port.\nAndrew Alexeev\n213\n", "page": 227, "type": "text", "section": "Page 227"}
{"text": "There are certain limitations of the nginx and Windows kernel architectures that do not interact well\nat this time. The known issues of the nginx version for Windows include a much lower number\nof concurrent connections, decreased performance, no caching and no bandwidth policing. Future\nversions of nginx for Windows will match the mainstream functionality more closely.\n14.2\nOverview of nginx Architecture\nTraditional process- or thread-based models of handling concurrent connections involve handling\neach connection with a separate process or thread, and blocking on network or input/output operations.\nDepending on the application, it can be very ine\ufb03cient in terms of memory and CPU consumption.\nSpawning a separate process or thread requires preparation of a new runtime environment, including\nallocation of heap and stack memory, and the creation of a new execution context. Additional CPU\ntime is also spent creating these items, which can eventually lead to poor performance due to thread\nthrashing on excessive context switching. All of these complications manifest themselves in older\nweb server architectures like Apache\u2019s. This is a tradeo\ufb00between o\ufb00ering a rich set of generally\napplicable features and optimized usage of server resources.\nFrom the very beginning, nginx was meant to be a specialized tool to achieve more performance,\ndensity and economical use of server resources while enabling dynamic growth of a website, so it\nhas followed a di\ufb00erent model. It was actually inspired by the ongoing development of advanced\nevent-based mechanisms in a variety of operating systems. What resulted is a modular, event-driven,\nasynchronous, single-threaded, non-blocking architecture which became the foundation of nginx\ncode.\nnginx uses multiplexing and event noti\ufb01cations heavily, and dedicates speci\ufb01c tasks to separate\nprocesses. Connections are processed in a highly e\ufb03cient run-loop in a limited number of single-\nthreaded processes called workers. Within each worker nginx can handle many thousands of\nconcurrent connections and requests per second.\nCode Structure\nThe nginx worker code includes the core and the functional modules. The core of nginx is responsible\nfor maintaining a tight run-loop and executing appropriate sections of modules\u2019 code on each stage of\nrequest processing. Modules constitute most of the presentation and application layer functionality.\nModules read from and write to the network and storage, transform content, do outbound \ufb01ltering,\napply server-side include actions and pass the requests to the upstream servers when proxying is\nactivated.\nnginx\u2019s modular architecture generally allows developers to extend the set of web server features\nwithout modifying the nginx core. nginx modules come in slightly di\ufb00erent incarnations, namely\ncore modules, event modules, phase handlers, protocols, variable handlers, \ufb01lters, upstreams and\nload balancers. At this time, nginx doesn\u2019t support dynamically loaded modules; i.e., modules are\ncompiled along with the core at build stage. However, support for loadable modules and ABI is\nplanned for the future major releases. More detailed information about the roles of di\ufb00erent modules\ncan be found in Section 14.4.\nWhile handling a variety of actions associated with accepting, processing and managing network\nconnections and content retrieval, nginx uses event noti\ufb01cation mechanisms and a number of disk I/O\nperformance enhancements in Linux, Solaris and BSD-based operating systems, like kqueue, epoll,\nand event ports. The goal is to provide as many hints to the operating system as possible, in regards\n214\nnginx\n", "page": 228, "type": "text", "section": "Page 228"}
{"text": "to obtaining timely asynchronous feedback for inbound and outbound tra\ufb03c, disk operations, reading\nfrom or writing to sockets, timeouts and so on. The usage of di\ufb00erent methods for multiplexing and\nadvanced I/O operations is heavily optimized for every Unix-based operating system nginx runs on.\nA high-level overview of nginx architecture is presented in Figure 14.1.\nFigure 14.1: Diagram of nginx\u2019s architecture\nWorkers Model\nAs previously mentioned, nginx doesn\u2019t spawn a process or thread for every connection. Instead,\nworker processes accept new requests from a shared \u201clisten\u201d socket and execute a highly e\ufb03cient\nrun-loop inside each worker to process thousands of connections per worker. There\u2019s no specialized\narbitration or distribution of connections to the workers in nginx; this work is done by the OS kernel\nmechanisms. Upon startup, an initial set of listening sockets is created. workers then continuously\naccept, read from and write to the sockets while processing HTTP requests and responses.\nThe run-loop is the most complicated part of the nginx worker code. It includes comprehensive\ninner calls and relies heavily on the idea of asynchronous task handling. Asynchronous operations\nare implemented through modularity, event noti\ufb01cations, extensive use of callback functions and\n\ufb01ne-tuned timers. Overall, the key principle is to be as non-blocking as possible. The only situation\nwhere nginx can still block is when there\u2019s not enough disk storage performance for a worker process.\nBecause nginx does not fork a process or thread per connection, memory usage is very conserva-\ntive and extremely e\ufb03cient in the vast majority of cases. nginx conserves CPU cycles as well because\nthere\u2019s no ongoing create-destroy pattern for processes or threads. What nginx does is check the\nstate of the network and storage, initialize new connections, add them to the run-loop, and process\nasynchronously until completion, at which point the connection is deallocated and removed from the\nrun-loop. Combined with the careful use of syscalls and an accurate implementation of supporting\nAndrew Alexeev\n215\n", "page": 229, "type": "text", "section": "Page 229"}
{"text": "interfaces like pool and slab memory allocators, nginx typically achieves moderate-to-low CPU\nusage even under extreme workloads.\nBecause nginx spawns several workers to handle connections, it scales well across multiple\ncores. Generally, a separate worker per core allows full utilization of multicore architectures, and\nprevents thread thrashing and lock-ups. There\u2019s no resource starvation and the resource controlling\nmechanisms are isolated within single-threaded worker processes. This model also allows more\nscalability across physical storage devices, facilitates more disk utilization and avoids blocking on\ndisk I/O. As a result, server resources are utilized more e\ufb03ciently with the workload shared across\nseveral workers.\nWith some disk use and CPU load patterns, the number of nginx workers should be adjusted.\nThe rules are somewhat basic here, and system administrators should try a couple of con\ufb01gurations\nfor their workloads. General recommendations might be the following: if the load pattern is CPU\nintensive\u2014for instance, handling a lot of TCP/IP, doing SSL, or compression\u2014the number of nginx\nworkers should match the number of CPU cores; if the load is mostly disk I/O bound\u2014for instance,\nserving di\ufb00erent sets of content from storage, or heavy proxying\u2014the number of workers might be\none and a half to two times the number of cores. Some engineers choose the number of workers\nbased on the number of individual storage units instead, though e\ufb03ciency of this approach depends\non the type and con\ufb01guration of disk storage.\nOne major problem that the developers of nginx will be solving in upcoming versions is how to\navoid most of the blocking on disk I/O. At the moment, if there\u2019s not enough storage performance to\nserve disk operations generated by a particular worker, that worker may still block on reading/writing\nfrom disk. A number of mechanisms and con\ufb01guration \ufb01le directives exist to mitigate such disk I/O\nblocking scenarios. Most notably, combinations of options like send\ufb01le and AIO typically produce a\nlot of headroom for disk performance. An nginx installation should be planned based on the data set,\nthe amount of memory available for nginx, and the underlying storage architecture.\nAnother problem with the existing worker model is related to limited support for embedded\nscripting. For one, with the standard nginx distribution, only embedding Perl scripts is supported.\nThere is a simple explanation for that: the key problem is the possibility of an embedded script to\nblock on any operation or exit unexpectedly. Both types of behavior would immediately lead to a\nsituation where the worker is hung, a\ufb00ecting many thousands of connections at once. More work is\nplanned to make embedded scripting with nginx simpler, more reliable and suitable for a broader\nrange of applications.\nnginx Process Roles\nnginx runs several processes in memory; there is a single master process and several worker processes.\nThere are also a couple of special purpose processes, speci\ufb01cally a cache loader and cache manager.\nAll processes are single-threaded in version 1.x of nginx. All processes primarily use shared-memory\nmechanisms for inter-process communication. The master process is run as the root user. The cache\nloader, cache manager and workers run as an unprivileged user.\nThe master process is responsible for the following tasks:\n\u2022 Reading and validating con\ufb01guration\n\u2022 Creating, binding and closing sockets\n\u2022 Starting, terminating and maintaining the con\ufb01gured number of worker processes\n\u2022 Recon\ufb01guring without service interruption\n\u2022 Controlling non-stop binary upgrades (starting new binary and rolling back if necessary)\n\u2022 Re-opening log \ufb01les\n216\nnginx\n", "page": 230, "type": "text", "section": "Page 230"}
{"text": "\u2022 Compiling embedded Perl scripts\nThe worker processes accept, handle and process connections from clients, provide reverse\nproxying and \ufb01ltering functionality and do almost everything else that nginx is capable of. In regards\nto monitoring the behavior of an nginx instance, a system administrator should keep an eye on\nworkers as they are the processes re\ufb02ecting the actual day-to-day operations of a web server.\nThe cache loader process is responsible for checking the on-disk cache items and populating\nnginx\u2019s in-memory database with cache metadata. Essentially, the cache loader prepares nginx\ninstances to work with \ufb01les already stored on disk in a specially allocated directory structure. It\ntraverses the directories, checks cache content metadata, updates the relevant entries in shared\nmemory and then exits when everything is clean and ready for use.\nThe cache manager is mostly responsible for cache expiration and invalidation. It stays in memory\nduring normal nginx operation and it is restarted by the master process in the case of failure.\nBrief Overview of nginx Caching\nCaching in nginx is implemented in the form of hierarchical data storage on a \ufb01lesystem. Cache\nkeys are con\ufb01gurable, and di\ufb00erent request-speci\ufb01c parameters can be used to control what gets into\nthe cache. Cache keys and cache metadata are stored in the shared memory segments, which the\ncache loader, cache manager and workers can access. Currently there is not any in-memory caching\nof \ufb01les, other than optimizations implied by the operating system\u2019s virtual \ufb01lesystem mechanisms.\nEach cached response is placed in a di\ufb00erent \ufb01le on the \ufb01lesystem. The hierarchy (levels and naming\ndetails) are controlled through nginx con\ufb01guration directives. When a response is written to the\ncache directory structure, the path and the name of the \ufb01le are derived from an MD5 hash of the\nproxy URL.\nThe process for placing content in the cache is as follows: When nginx reads the response from\nan upstream server, the content is \ufb01rst written to a temporary \ufb01le outside of the cache directory\nstructure. When nginx \ufb01nishes processing the request it renames the temporary \ufb01le and moves it to\nthe cache directory. If the temporary \ufb01les directory for proxying is on another \ufb01le system, the \ufb01le\nwill be copied, thus it\u2019s recommended to keep both temporary and cache directories on the same \ufb01le\nsystem. It is also quite safe to delete \ufb01les from the cache directory structure when they need to be\nexplicitly purged. There are third-party extensions for nginx which make it possible to control cached\ncontent remotely, and more work is planned to integrate this functionality in the main distribution.\n14.3\nnginx Con\ufb01guration\nnginx\u2019s con\ufb01guration system was inspired by Igor Sysoev\u2019s experiences with Apache. His main\ninsight was that a scalable con\ufb01guration system is essential for a web server. The main scaling\nproblem was encountered when maintaining large complicated con\ufb01gurations with lots of virtual\nservers, directories, locations and datasets. In a relatively big web setup it can be a nightmare if not\ndone properly both at the application level and by the system engineer himself.\nAs a result, nginx con\ufb01guration was designed to simplify day-to-day operations and to provide\nan easy means for further expansion of web server con\ufb01guration.\nnginx con\ufb01guration is kept in a number of plain text \ufb01les which typically reside in /usr/local-\n/etc/nginx or /etc/nginx. The main con\ufb01guration \ufb01le is usually called nginx.conf. To keep\nit uncluttered, parts of the con\ufb01guration can be put in separate \ufb01les which can be automatically\nincluded in the main one. However, it should be noted here that nginx does not currently support\nAndrew Alexeev\n217\n", "page": 231, "type": "text", "section": "Page 231"}
{"text": "Apache-style distributed con\ufb01gurations (i.e., .htaccess \ufb01les). All of the con\ufb01guration relevant to\nnginx web server behavior should reside in a centralized set of con\ufb01guration \ufb01les.\nThe con\ufb01guration \ufb01les are initially read and veri\ufb01ed by the master process. A compiled read-only\nform of the nginx con\ufb01guration is available to the worker processes as they are forked from the\nmaster process. Con\ufb01guration structures are automatically shared by the usual virtual memory\nmanagement mechanisms.\nnginx con\ufb01guration has several di\ufb00erent contexts for main, http, server, upstream, location\n(and also mail for mail proxy) blocks of directives. Contexts never overlap. For instance, there is no\nsuch thing as putting a location block in the main block of directives. Also, to avoid unnecessary\nambiguity there isn\u2019t anything like a \u201cglobal web server\u201d con\ufb01guration. nginx con\ufb01guration is meant\nto be clean and logical, allowing users to maintain complicated con\ufb01guration \ufb01les that comprise\nthousands of directives. In a private conversation, Sysoev said, \u201cLocations, directories, and other\nblocks in the global server con\ufb01guration are the features I never liked in Apache, so this is the reason\nwhy they were never implemented in nginx.\u201d\nCon\ufb01guration syntax, formatting and de\ufb01nitions follow a so-called C-style convention. This\nparticular approach to making con\ufb01guration \ufb01les is already being used by a variety of open source\nand commercial software applications. By design, C-style con\ufb01guration is well-suited for nested\ndescriptions, being logical and easy to create, read and maintain, and liked by many engineers.\nC-style con\ufb01guration of nginx can also be easily automated.\nWhile some of the nginx directives resemble certain parts of Apache con\ufb01guration, setting up an\nnginx instance is quite a di\ufb00erent experience. For instance, rewrite rules are supported by nginx,\nthough it would require an administrator to manually adapt a legacy Apache rewrite con\ufb01guration to\nmatch nginx style. The implementation of the rewrite engine di\ufb00ers too.\nIn general, nginx settings also provide support for several original mechanisms that can be very\nuseful as part of a lean web server con\ufb01guration. It makes sense to brie\ufb02y mention variables and the\ntry_files directive, which are somewhat unique to nginx. Variables in nginx were developed to\nprovide an additional even-more-powerful mechanism to control run-time con\ufb01guration of a web\nserver. Variables are optimized for quick evaluation and are internally pre-compiled to indices.\nEvaluation is done on demand; i.e., the value of a variable is typically calculated only once and\ncached for the lifetime of a particular request. Variables can be used with di\ufb00erent con\ufb01guration\ndirectives, providing additional \ufb02exibility for describing conditional request processing behavior.\nThe try_files directive was initially meant to gradually replace conditional if con\ufb01guration\nstatements in a more proper way, and it was designed to quickly and e\ufb03ciently try/match against\ndi\ufb00erent URI-to-content mappings. Overall, the try_files directive works well and can be extremely\ne\ufb03cient and useful. It is recommended that the reader thoroughly check the try_files directive\nand adopt its use whenever applicable2.\n14.4\nnginx Internals\nAs was mentioned before, the nginx codebase consists of a core and a number of modules. The core\nof nginx is responsible for providing the foundation of the web server, web and mail reverse proxy\nfunctionalities; it enables the use of underlying network protocols, builds the necessary run-time\nenvironment, and ensures seamless interaction between di\ufb00erent modules. However, most of the\nprotocol- and application-speci\ufb01c features are done by nginx modules, not the core.\n2See http://nginx.org/en/docs/http/ngx_http_core_module.html#try_files for more details.\n218\nnginx\n", "page": 232, "type": "text", "section": "Page 232"}
{"text": "Internally, nginx processes connections through a pipeline, or chain, of modules. In other words,\nfor every operation there\u2019s a module which is doing the relevant work; e.g., compression, modifying\ncontent, executing server-side includes, communicating to the upstream application servers through\nFastCGI or uwsgi protocols, or talking to memcached.\nThere are a couple of nginx modules that sit somewhere between the core and the real \u201cfunctional\u201d\nmodules. These modules are http and mail. These two modules provide an additional level of\nabstraction between the core and lower-level components. In these modules, the handling of the\nsequence of events associated with a respective application layer protocol like HTTP, SMTP or IMAP\nis implemented. In combination with the nginx core, these upper-level modules are responsible for\nmaintaining the right order of calls to the respective functional modules. While the HTTP protocol\nis currently implemented as part of the http module, there are plans to separate it into a functional\nmodule in the future, due to the need to support other protocols like SPDY3.\nThe functional modules can be divided into event modules, phase handlers, output \ufb01lters, variable\nhandlers, protocols, upstreams and load balancers. Most of these modules complement the HTTP\nfunctionality of nginx, though event modules and protocols are also used for mail. Event modules\nprovide a particular OS-dependent event noti\ufb01cation mechanism like kqueue or epoll. The event\nmodule that nginx uses depends on the operating system capabilities and build con\ufb01guration. Protocol\nmodules allow nginx to communicate through HTTPS, TLS/SSL, SMTP, POP3 and IMAP.\nA typical HTTP request processing cycle looks like the following:\n1. Client sends HTTP request\n2. nginx core chooses the appropriate phase handler based on the con\ufb01gured location matching\nthe request\n3. If con\ufb01gured to do so, a load balancer picks an upstream server for proxying\n4. Phase handler does its job and passes each output bu\ufb00er to the \ufb01rst \ufb01lter\n5. First \ufb01lter passes the output to the second \ufb01lter\n6. Second \ufb01lter passes the output to third (and so on)\n7. Final response is sent to the client\nnginx module invocation is extremely customizable. It is performed through a series of callbacks\nusing pointers to the executable functions. However, the downside of this is that it may place a big\nburden on programmers who would like to write their own modules, because they must de\ufb01ne exactly\nhow and when the module should run. Both the nginx API and developers\u2019 documentation are being\nimproved and made more available to alleviate this.\nSome examples of where a module can attach are:\n\u2022 Before the con\ufb01guration \ufb01le is read and processed\n\u2022 For each con\ufb01guration directive for the location and the server where it appears\n\u2022 When the main con\ufb01guration is initialized\n\u2022 When the server (i.e., host/port) is initialized\n\u2022 When the server con\ufb01guration is merged with the main con\ufb01guration\n\u2022 When the location con\ufb01guration is initialized or merged with its parent server con\ufb01guration\n\u2022 When the master process starts or exits\n\u2022 When a new worker process starts or exits\n\u2022 When handling a request\n\u2022 When \ufb01ltering the response header and the body\n\u2022 When picking, initiating and re-initiating a request to an upstream server\n3See \"SPDY: An experimental protocol for a faster web\" at http://www.chromium.org/spdy/spdy-whitepaper\nAndrew Alexeev\n219\n", "page": 233, "type": "text", "section": "Page 233"}
{"text": "\u2022 When processing the response from an upstream server\n\u2022 When \ufb01nishing an interaction with an upstream server\nInside a worker, the sequence of actions leading to the run-loop where the response is generated\nlooks like the following:\n1. Begin ngx_worker_process_cycle()\n2. Process events with OS speci\ufb01c mechanisms (such as epoll or kqueue)\n3. Accept events and dispatch the relevant actions\n4. Process/proxy request header and body\n5. Generate response content (header, body) and stream it to the client\n6. Finalize request\n7. Re-initialize timers and events\nThe run-loop itself (steps 5 and 6) ensures incremental generation of a response and streaming it\nto the client.\nA more detailed view of processing an HTTP request might look like this:\n1. Initialize request processing\n2. Process header\n3. Process body\n4. Call the associated handler\n5. Run through the processing phases\nWhich brings us to the phases. When nginx handles an HTTP request, it passes it through a\nnumber of processing phases. At each phase there are handlers to call. In general, phase handlers\nprocess a request and produce the relevant output. Phase handlers are attached to the locations\nde\ufb01ned in the con\ufb01guration \ufb01le.\nPhase handlers typically do four things: get the location con\ufb01guration, generate an appropriate\nresponse, send the header, and send the body. A handler has one argument: a speci\ufb01c structure\ndescribing the request. A request structure has a lot of useful information about the client request,\nsuch as the request method, URI, and header.\nWhen the HTTP request header is read, nginx does a lookup of the associated virtual server\ncon\ufb01guration. If the virtual server is found, the request goes through six phases:\n1. Server rewrite phase\n2. Location phase\n3. Location rewrite phase (which can bring the request back to the previous phase)\n4. Access control phase\n5. try_files phase\n6. Log phase\nIn an attempt to generate the necessary content in response to the request, nginx passes the\nrequest to a suitable content handler. Depending on the exact location con\ufb01guration, nginx may try\nso-called unconditional handlers \ufb01rst, like perl, proxy_pass, flv, mp4, etc. If the request does not\nmatch any of the above content handlers, it is picked by one of the following handlers, in this exact\norder: random index, index, autoindex, gzip_static, static.\nIndexing module details can be found in the nginx documentation, but these are the modules\nwhich handle requests with a trailing slash. If a specialized module like mp4 or autoindex isn\u2019t\nappropriate, the content is considered to be just a \ufb01le or directory on disk (that is, static) and is served\nby the static content handler. For a directory it would automatically rewrite the URI so that the\ntrailing slash is always there (and then issue an HTTP redirect).\n220\nnginx\n", "page": 234, "type": "text", "section": "Page 234"}
{"text": "The content handlers\u2019 content is then passed to the \ufb01lters. Filters are also attached to locations,\nand there can be several \ufb01lters con\ufb01gured for a location. Filters do the task of manipulating the\noutput produced by a handler. The order of \ufb01lter execution is determined at compile time. For the\nout-of-the-box \ufb01lters it\u2019s prede\ufb01ned, and for a third-party \ufb01lter it can be con\ufb01gured at the build stage.\nIn the existing nginx implementation, \ufb01lters can only do outbound changes and there is currently no\nmechanism to write and attach \ufb01lters to do input content transformation. Input \ufb01ltering will appear\nin future versions of nginx.\nFilters follow a particular design pattern. A \ufb01lter gets called, starts working, and calls the next\n\ufb01lter until the \ufb01nal \ufb01lter in the chain is called. After that, nginx \ufb01nalizes the response. Filters don\u2019t\nhave to wait for the previous \ufb01lter to \ufb01nish. The next \ufb01lter in a chain can start its own work as soon\nas the input from the previous one is available (functionally much like the Unix pipeline). In turn,\nthe output response being generated can be passed to the client before the entire response from the\nupstream server is received.\nThere are header \ufb01lters and body \ufb01lters; nginx feeds the header and the body of the response to\nthe associated \ufb01lters separately.\nA header \ufb01lter consists of three basic steps:\n1. Decide whether to operate on this response\n2. Operate on the response\n3. Call the next \ufb01lter\nBody \ufb01lters transform the generated content. Examples of body \ufb01lters include:\n\u2022 Server-side includes\n\u2022 XSLT \ufb01ltering\n\u2022 Image \ufb01ltering (for instance, resizing images on the \ufb02y)\n\u2022 Charset modi\ufb01cation\n\u2022 gzip compression\n\u2022 Chunked encoding\nAfter the \ufb01lter chain, the response is passed to the writer. Along with the writer there are a couple\nof additional special purpose \ufb01lters, namely the copy \ufb01lter, and the postpone \ufb01lter. The copy \ufb01lter\nis responsible for \ufb01lling memory bu\ufb00ers with the relevant response content which might be stored in\na proxy temporary directory. The postpone \ufb01lter is used for subrequests.\nSubrequests are a very important mechanism for request/response processing. Subrequests are\nalso one of the most powerful aspects of nginx. With subrequests nginx can return the results from\na di\ufb00erent URL than the one the client originally requested. Some web frameworks call this an\ninternal redirect. However, nginx goes further\u2014not only can \ufb01lters perform multiple subrequests and\ncombine the outputs into a single response, but subrequests can also be nested and hierarchical. A\nsubrequest can perform its own sub-subrequest, and a sub-subrequest can initiate sub-sub-subrequests.\nSubrequests can map to \ufb01les on the hard disk, other handlers, or upstream servers. Subrequests are\nmost useful for inserting additional content based on data from the original response. For example,\nthe SSI (server-side include) module uses a \ufb01lter to parse the contents of the returned document,\nand then replaces include directives with the contents of speci\ufb01ed URLs. Or, it can be an example\nof making a \ufb01lter that treats the entire contents of a document as a URL to be retrieved, and then\nappends the new document to the URL itself.\nUpstream and load balancers are also worth describing brie\ufb02y. Upstreams are used to implement\nwhat can be identi\ufb01ed as a content handler which is a reverse proxy (proxy_pass handler). Upstream\nmodules mostly prepare the request to be sent to an upstream server (or \u201cbackend\u201d) and receive\nthe response from the upstream server. There are no calls to output \ufb01lters here. What an upstream\nAndrew Alexeev\n221\n", "page": 235, "type": "text", "section": "Page 235"}
{"text": "module does exactly is set callbacks to be invoked when the upstream server is ready to be written to\nand read from. Callbacks implementing the following functionality exist:\n\u2022 Crafting a request bu\ufb00er (or a chain of them) to be sent to the upstream server\n\u2022 Re-initializing/resetting the connection to the upstream server (which happens right before\ncreating the request again)\n\u2022 Processing the \ufb01rst bits of an upstream response and saving pointers to the payload received\nfrom the upstream server\n\u2022 Aborting requests (which happens when the client terminates prematurely)\n\u2022 Finalizing the request when nginx \ufb01nishes reading from the upstream server\n\u2022 Trimming the response body (e.g. removing a trailer)\nLoad balancer modules attach to the proxy_pass handler to provide the ability to choose an\nupstream server when more than one upstream server is eligible. A load balancer registers an enabling\ncon\ufb01guration \ufb01le directive, provides additional upstream initialization functions (to resolve upstream\nnames in DNS, etc.), initializes the connection structures, decides where to route the requests, updates\nstats information. Currently nginx supports two standard disciplines for load balancing to upstream\nservers: round-robin and ip-hash.\nUpstream and load balancing handling mechanisms include algorithms to detect failed upstream\nservers and to re-route new requests to the remaining ones\u2014though a lot of additional work is\nplanned to enhance this functionality. In general, more work on load balancers is planned, and in the\nnext versions of nginx the mechanisms for distributing the load across di\ufb00erent upstream servers as\nwell as health checks will be greatly improved.\nThere are also a couple of other interesting modules which provide an additional set of variables\nfor use in the con\ufb01guration \ufb01le. While the variables in nginx are created and updated across di\ufb00erent\nmodules, there are two modules that are entirely dedicated to variables: geo and map. The geo\nmodule is used to facilitate tracking of clients based on their IP addresses. This module can create\narbitrary variables that depend on the client\u2019s IP address. The other module, map, allows for the\ncreation of variables from other variables, essentially providing the ability to do \ufb02exible mappings\nof hostnames and other run-time variables. This kind of module may be called the variable handler.\nMemory allocation mechanisms implemented inside a single nginx worker were, to some extent,\ninspired by Apache. A high-level description of nginx memory management would be the following:\nFor each connection, the necessary memory bu\ufb00ers are dynamically allocated, linked, used for\nstoring and manipulating the header and body of the request and the response, and then freed upon\nconnection release. It is very important to note that nginx tries to avoid copying data in memory as\nmuch as possible and most of the data is passed along by pointer values, not by calling memcpy.\nGoing a bit deeper, when the response is generated by a module, the retrieved content is put in a\nmemory bu\ufb00er which is then added to a bu\ufb00er chain link. Subsequent processing works with this\nbu\ufb00er chain link as well. Bu\ufb00er chains are quite complicated in nginx because there are several\nprocessing scenarios which di\ufb00er depending on the module type. For instance, it can be quite tricky\nto manage the bu\ufb00ers precisely while implementing a body \ufb01lter module. Such a module can only\noperate on one bu\ufb00er (chain link) at a time and it must decide whether to overwrite the input bu\ufb00er,\nreplace the bu\ufb00er with a newly allocated bu\ufb00er, or insert a new bu\ufb00er before or after the bu\ufb00er in\nquestion. To complicate things, sometimes a module will receive several bu\ufb00ers so that it has an\nincomplete bu\ufb00er chain that it must operate on. However, at this time nginx provides only a low-level\nAPI for manipulating bu\ufb00er chains, so before doing any actual implementation a third-party module\ndeveloper should become really \ufb02uent with this arcane part of nginx.\nA note on the above approach is that there are memory bu\ufb00ers allocated for the entire life of a\nconnection, thus for long-lived connections some extra memory is kept. At the same time, on an idle\n222\nnginx\n", "page": 236, "type": "text", "section": "Page 236"}
{"text": "keepalive connection, nginx spends just 550 bytes of memory. A possible optimization for future\nreleases of nginx would be to reuse and share memory bu\ufb00ers for long-lived connections.\nThe task of managing memory allocation is done by the nginx pool allocator. Shared memory\nareas are used to accept mutex, cache metadata, the SSL session cache and the information associated\nwith bandwidth policing and management (limits). There is a slab allocator implemented in nginx to\nmanage shared memory allocation. To allow simultaneous safe use of shared memory, a number\nof locking mechanisms are available (mutexes and semaphores). In order to organize complex data\nstructures, nginx also provides a red-black tree implementation. Red-black trees are used to keep\ncache metadata in shared memory, track non-regex location de\ufb01nitions and for a couple of other\ntasks.\nUnfortunately, all of the above was never described in a consistent and simple manner, making\nthe job of developing third-party extensions for nginx quite complicated. Although some good\ndocuments on nginx internals exist\u2014for instance, those produced by Evan Miller\u2014such documents\nrequired a huge reverse engineering e\ufb00ort, and the implementation of nginx modules is still a black\nart for many.\nDespite certain di\ufb03culties associated with third-party module development, the nginx user\ncommunity recently saw a lot of useful third-party modules. There is, for instance, an embedded Lua\ninterpreter module for nginx, additional modules for load balancing, full WebDAV support, advanced\ncache control and other interesting third-party work that the authors of this chapter encourage and\nwill support in the future.\n14.5\nLessons Learned\nWhen Igor Sysoev started to write nginx, most of the software enabling the Internet already existed,\nand the architecture of such software typically followed de\ufb01nitions of legacy server and network\nhardware, operating systems, and old Internet architecture in general. However, this didn\u2019t prevent\nIgor from thinking he might be able to improve things in the web servers area. So, while the \ufb01rst\nlesson might seem obvious, it is this: there is always room for improvement.\nWith the idea of better web software in mind, Igor spent a lot of time developing the initial code\nstructure and studying di\ufb00erent ways of optimizing the code for a variety of operating systems. Ten\nyears later he is developing a prototype of nginx version 2.0, taking into account the years of active\ndevelopment on version 1. It is clear that the initial prototype of a new architecture, and the initial\ncode structure, are vitally important for the future of a software product.\nAnother point worth mentioning is that development should be focused. The Windows version\nof nginx is probably a good example of how it is worth avoiding the dilution of development e\ufb00orts\non something that is neither the developer\u2019s core competence or the target application. It is equally\napplicable to the rewrite engine that appeared during several attempts to enhance nginx with more\nfeatures for backward compatibility with the existing legacy setups.\nLast but not least, it is worth mentioning that despite the fact that the nginx developer community\nis not very large, third-party modules and extensions for nginx have always been a very important\npart of its popularity. The work done by Evan Miller, Piotr Sikora, Valery Kholodkov, Zhang\nYichun (agentzh) and other talented software engineers has been much appreciated by the nginx user\ncommunity and its original developers.\n223\n", "page": 237, "type": "text", "section": "Page 237"}
{"text": "224\nnginx\n", "page": 238, "type": "text", "section": "Page 238"}
{"text": "[chapter15]\nOpen MPI\nJe\ufb00rey M. Squyres\n15.1\nBackground\nOpen MPI [GFB+04] is an open source software implementation of The Message Passing Interface\n(MPI) standard. Before the architecture and innards of Open MPI will make any sense, a little\nbackground on the MPI standard must be discussed.\nThe Message Passing Interface (MPI)\nThe MPI standard is created and maintained by the MPI Forum1, an open group consisting of\nparallel computing experts from both industry and academia. MPI de\ufb01nes an API that is used for a\nspeci\ufb01c type of portable, high-performance inter-process communication (IPC): message passing.\nSpeci\ufb01cally, the MPI document describes the reliable transfer of discrete, typed messages between\nMPI processes. Although the de\ufb01nition of an \u201cMPI process\u201d is subject to interpretation on a given\nplatform, it usually corresponds to the operating system\u2019s concept of a process (e.g., a POSIX\nprocess). MPI is speci\ufb01cally intended to be implemented as middleware, meaning that upper-level\napplications call MPI functions to perform message passing.\nMPI de\ufb01nes a high-level API, meaning that it abstracts away whatever underlying transport is\nactually used to pass messages between processes. The idea is that sending-process X can e\ufb00ectively\nsay \u201ctake this array of 1,073 double precision values and send them to process Y \u201d. The corresponding\nreceiving-process Y e\ufb00ectively says \u201creceive an array of 1,073 double precision values from process\nX.\u201d A miracle occurs, and the array of 1,073 double precision values arrives in Y \u2019s waiting bu\ufb00er.\nNotice what is absent in this exchange: there is no concept of a connection occurring, no stream\nof bytes to interpret, and no network addresses exchanged. MPI abstracts all of that away, not only\nto hide such complexity from the upper-level application, but also to make the application portable\nacross di\ufb00erent environments and underlying message passing transports. Speci\ufb01cally, a correct\nMPI application is source-compatible across a wide variety of platforms and network types.\nMPI de\ufb01nes not only point-to-point communication (e.g., send and receive), it also de\ufb01nes other\ncommunication patterns, such as collective communication. Collective operations are where multiple\nprocesses are involved in a single communication action. Reliable broadcast, for example, is where\none process has a message at the beginning of the operation, and at the end of the operation, all\n1http://www.mpi-forum.org/\n", "page": 239, "type": "text", "section": "Page 239"}
{"text": "processes in a group have the message. MPI also de\ufb01nes other concepts and communications patterns\nthat are not described here.2\nUses of MPI\nThere are many implementations of the MPI standard that support a wide variety of platforms,\noperating systems, and network types. Some implementations are open source, some are closed\nsource. Open MPI, as its name implies, is one of the open source implementations. Typical MPI\ntransport networks include (but are not limited to): various protocols over Ethernet (e.g., TCP,\niWARP, UDP, raw Ethernet frames, etc.), shared memory, and In\ufb01niBand.\nMPI implementations are typically used in so-called \u201chigh-performance computing\u201d (HPC)\nenvironments. MPI essentially provides the IPC for simulation codes, computational algorithms,\nand other \u201cbig number crunching\u201d types of applications. The input data sets on which these codes\noperate typically represent too much computational work for just one server; MPI jobs are spread out\nacross tens, hundreds, or even thousands of servers, all working in concert to solve one computational\nproblem.\nThat is, the applications using MPI are both parallel in nature and highly compute-intensive.\nIt is not unusual for all the processor cores in an MPI job to run at 100% utilization. To be clear,\nMPI jobs typically run in dedicated environments where the MPI processes are the only application\nrunning on the machine (in addition to bare-bones operating system functionality, of course).\nAs such, MPI implementations are typically focused on providing extremely high performance,\nmeasured by metrics such as:\n\u2022 Extremely low latency for short message passing. As an example, a 1-byte message can be\nsent from a user-level Linux process on one server, through an In\ufb01niBand switch, and received\nat the target user-level Linux process on a di\ufb00erent server in a little over 1 microsecond (i.e.,\n0.000001 second).\n\u2022 Extremely high message network injection rate for short messages. Some vendors have MPI\nimplementations (paired with speci\ufb01ed hardware) that can inject up to 28 million messages\nper second into the network.\n\u2022 Quick ramp-up (as a function of message size) to the maximum bandwidth supported by the\nunderlying transport.\n\u2022 Low resource utilization. All resources used by MPI (e.g., memory, cache, and bus bandwidth)\ncannot be used by the application. MPI implementations therefore try to maintain a balance of\nlow resource utilization while still providing high performance.\nOpen MPI\nThe \ufb01rst version of the MPI standard, MPI-1.0, was published in 1994 [Mes93]. MPI-2.0, a set of\nadditions on top of MPI-1, was completed in 1996 [GGHL+96].\nIn the \ufb01rst decade after MPI-1 was published, a variety of MPI implementations sprung up. Many\nwere provided by vendors for their proprietary network interconnects. Many other implementations\narose from the research and academic communities. Such implementations were typically \u201cresearch-\nquality,\u201d meaning that their purpose was to investigate various high-performance networking concepts\nand provide proofs-of-concept of their work. However, some were high enough quality that they\ngained popularity and a number of users.\n2As of this writing, the most recent version of the MPI standard is MPI-2.2 [For09]. Draft versions of the upcoming MPI-3\nstandard have been published; it may be \ufb01nalized as early as late 2012.\n226\nOpen MPI\n", "page": 240, "type": "text", "section": "Page 240"}
{"text": "Open MPI represents the union of four research/academic, open source MPI implementations:\nLAM/MPI, LA/MPI (Los Alamos MPI), and FT-MPI (Fault-Tolerant MPI). The members of the\nPACX-MPI team joined the Open MPI group shortly after its inception.\nThe members of these four development teams decided to collaborate when we had the collective\nrealization that, aside from minor di\ufb00erences in optimizations and features, our software code bases\nwere quite similar. Each of the four code bases had their own strengths and weaknesses, but on the\nwhole, they more-or-less did the same things. So why compete? Why not pool our resources, work\ntogether, and make an even better MPI implementation?\nAfter much discussion, the decision was made to abandon our four existing code bases and take\nonly the best ideas from the prior projects. This decision was mainly predicated upon the following\npremises:\n\u2022 Even though many of the underlying algorithms and techniques were similar among the four\ncode bases, they each had radically di\ufb00erent implementation architectures, and would be\nincredible di\ufb03cult (if not impossible) to merge.\n\u2022 Each of the four also had their own (signi\ufb01cant) strengths and (signi\ufb01cant) weaknesses. Specif-\nically, there were features and architecture decisions from each of the four that were desirable\nto carry forward. Likewise, there were poorly optimized and badly designed code in each of\nthe four that were desirable to leave behind.\n\u2022 The members of the four developer groups had not worked directly together before. Starting\nwith an entirely new code base (rather than advancing one of the existing code bases) put all\ndevelopers on equal ground.\nThus, Open MPI was born. Its \ufb01rst Subversion commit was on November 22, 2003.\n15.2\nArchitecture\nFor a variety of reasons (mostly related to either performance or portability), C and C++ were the only\ntwo possibilities for the primary implementation language. C++ was eventually discarded because\ndi\ufb00erent C++ compilers tend to lay out structs/classes in memory according to di\ufb00erent optimization\nalgorithms, leading to di\ufb00erent on-the-wire network representations. C was therefore chosen as the\nprimary implementation language, which in\ufb02uenced several architectural design decisions.\nWhen Open MPI was started, we knew that it would be a large, complex code base:\n\u2022 In 2003, the current version of the MPI standard, MPI-2.0, de\ufb01ned over 300 API functions.\n\u2022 Each of the four prior projects were large in themselves. For example, LAM/MPI had over\n1,900 \ufb01les of source code, comprising over 300,000 lines of code (including comments and\nblanks).\n\u2022 We wanted Open MPI to support more features, environments, and networks than all four prior\nprojects put together.\nWe therefore spent a good deal of time designing an architecture that focused on three things:\n1. Grouping similar functionality together in distinct abstraction layers\n2. Using run-time loadable plugins and run-time parameters to choose between multiple di\ufb00erent\nimplementations of the same behavior\n3. Not allowing abstraction to get in the way of performance\nJe\ufb00rey M. Squyres\n227\n", "page": 241, "type": "text", "section": "Page 241"}
{"text": "!\"#$%\n!&'(%\n!)*+,-./%0123*4%\n5,+67,+*%\n&'(%8))9:;,-<.%\n!'8=%\nFigure 15.1: Abstraction layer architectural view of Open MPI showing its three main layers: OPAL, ORTE,\nand OMPI\nAbstraction Layer Architecture\nOpen MPI has three main abstraction layers, shown in Figure 15.1:\n\u2022 Open, Portable Access Layer (OPAL): OPAL is the bottom layer of Open MPI\u2019s abstractions.\nIts abstractions are focused on individual processes (versus parallel jobs). It provides utility\nand glue code such as generic linked lists, string manipulation, debugging controls, and other\nmundane\u2014yet necessary\u2014functionality.\nOPAL also provides Open MPI\u2019s core portability between di\ufb00erent operating systems, such as\ndiscovering IP interfaces, sharing memory between processes on the same server, processor\nand memory a\ufb03nity, high-precision timers, etc.\n\u2022 Open MPI Run-Time Environment (ORTE)3: An MPI implementation must provide not only the\nrequired message passing API, but also an accompanying run-time system to launch, monitor,\nand kill parallel jobs. In Open MPI\u2019s case, a parallel job is comprised of one or more processes\nthat may span multiple operating system instances, and are bound together to act as a single,\ncohesive unit.\nIn simple environments with little or no distributed computational support, ORTE uses rsh\nor ssh to launch the individual processes in parallel jobs. More advanced, HPC-dedicated\nenvironments typically have schedulers and resource managers for fairly sharing computational\nresources between many users. Such environments usually provide specialized APIs to launch\nand regulate processes on compute servers. ORTE supports a wide variety of such managed\nenvironments, such as (but not limited to): Torque/PBS Pro, SLURM, Oracle Grid Engine,\nand LSF.\n\u2022 Open MPI (OMPI): The MPI layer is the highest abstraction layer, and is the only one exposed\nto applications. The MPI API is implemented in this layer, as are all the message passing\nsemantics de\ufb01ned by the MPI standard.\nSince portability is a primary requirement, the MPI layer supports a wide variety of network\ntypes and underlying protocols. Some networks are similar in their underlying characteristics\nand abstractions; some are not.\nAlthough each abstraction is layered on top of the one below it, for performance reasons the\nORTE and OMPI layers can bypass the underlying abstraction layers and interact directly with the\n3Pronounced \u201cor-tay\u201d.\n228\nOpen MPI\n", "page": 242, "type": "text", "section": "Page 242"}
{"text": "operating system and/or hardware when needed (as depicted in Figure 15.1). For example, the\nOMPI layer uses OS-bypass methods to communicate with certain types of NIC hardware to obtain\nmaximum networking performance.\nEach layer is built into a standalone library. The ORTE library depends on the OPAL library; the\nOMPI library depends on the ORTE library. Separating the layers into their own libraries has acted\nas a wonderful tool for preventing abstraction violations. Speci\ufb01cally, applications will fail to link\nif one layer incorrectly attempts to use a symbol in a higher layer. Over the years, this abstraction\nenforcement mechanism has saved many developers from inadvertently blurring the lines between\nthe three layers.\nPlugin Architecture\nAlthough the initial members of the Open MPI collaboration shared a similar core goal (produce a\nportable, high-performance implementation of the MPI standard), our organizational backgrounds,\nopinions, and agendas were\u2014and still are\u2014wildly di\ufb00erent. We therefore spent a considerable\namount of time designing an architecture that would allow us to be di\ufb00erent, even while sharing a\ncommon code base.\nRun-time loadable components were a natural choice (a.k.a., dynamic shared objects, or \u201cDSOs\u201d,\nor \u201cplugins\u201d). Components enforce a common API but place few limitations on the implementation of\nthat API. Speci\ufb01cally: the same interface behavior can be implemented multiple di\ufb00erent ways. Users\ncan then choose, at run time, which plugin(s) to use. This even allows third parties to independently\ndevelop and distribute their own Open MPI plugins outside of the core Open MPI package. Allowing\narbitrary extensibility is quite a liberating policy, both within the immediate set of Open MPI\ndevelopers and in the greater Open MPI community.\nThis run-time \ufb02exibility is a key component of the Open MPI design philosophy and is deeply\nintegrated throughout the entire architecture. Case in point: the Open MPI v1.5 series includes\n155 plugins. To list just a few examples, there are plugins for di\ufb00erent memcpy() implementations,\nplugins for how to launch processes on remote servers, and plugins for how to communicate on\ndi\ufb00erent types of underlying networks.\nOne of the major bene\ufb01ts of using plugins is that multiple groups of developers have freedom\nto experiment with alternate implementations without a\ufb00ecting the core of Open MPI. This was a\ncritical feature, particularly in the early days of the Open MPI project. Sometimes the developers\ndidn\u2019t always know what was the right way to implement something, or sometimes they just disagreed.\nIn both cases, each party would implement their solution in a component, allowing the rest of the\ndeveloper community to easily compare and contrast. Code comparisons can be done without\ncomponents, of course, but the component concept helps guarantee that all implementations expose\nexactly the same external API, and therefore provide exactly the same required semantics.\nAs a direct result of the \ufb02exibility that it provides, the component concept is utilized heavily\nthroughout all three layers of Open MPI; in each layer there are many di\ufb00erent types of components.\nEach type of component is enclosed in a framework. A component belongs to exactly one framework,\nand a framework supports exactly one kind of component. Figure 15.2 is a template of Open MPI\u2019s\narchitectural layout; it shows a few of Open MPI\u2019s frameworks and some of the components that\nthey contain. (The rest of Open MPI\u2019s frameworks and components are laid out in the same manner.)\nOpen MPI\u2019s set of layers, frameworks, and components is referred to as the Modular Component\nArchitecture (MCA).\nFinally, another major advantage of using frameworks and components is their inherent compos-\nability. With over 40 frameworks in Open MPI v1.5, giving users the ability to mix-n-match di\ufb00erent\nJe\ufb00rey M. Squyres\n229\n", "page": 243, "type": "text", "section": "Page 243"}
{"text": "!\"#$%&'(%)*+#%,!'-./%!012/%3$4%!&'(%536#+78%\n&'(%96:#%\n:+3$7;#+%536#+%\n,9:58%\n&'(%)*55#)<=#%\n*\"#+3<*$7%\n,)*558%\n>?@A%\n+#7*5B<*$%\n<C#+7%,<C#+8%\nD37#%\n1E'%\nFA3+#4%C#C*+6%\n!\"#$G39+?)7%\nH%\nD37#%\n1B$#4%\nFA3+#4%C#C*+6%\n.**\"93)I%\nH%\nD37#%\n+7A%%J%77A%\nF.K0&%\n-.'F%,E+368%\nH%\nD37#%\n.?$BL%\nM3+N?$%\nO?$4*N7%\nH%\nH%\nG+3C#N*+I7%\nD37#%3$4%\n)*C\"*$#$:7%\n,?P#P/%\"5B@?$78%\n'+*)#77%53B$)A?$@%\n3$4%C*$?:*+?$@%\n,\"5C8%\nFigure 15.2: Framework architectural view of Open MPI, showing just a few of Open MPI\u2019s frameworks and\ncomponents (i.e., plugins). Each framework contains a base and one or more components. This structure is\nreplicated in each of the layers shown in Figure 15.1. The sample frameworks listed in this \ufb01gure are spread\nacross all three layers: btl and coll are in the OMPI layer, plm is in the ORTE layer, and timer is in the OPAL\nlayer.\nplugins of di\ufb00erent types allows them to create a software stack that is e\ufb00ectively tailored to their\nindividual system.\nPlugin Frameworks\nEach framework is fully self-contained in its own subdirectory in the Open MPI source code tree. The\nname of the subdirectory is the same name as the framework; for example, the memory framework is\nin the memory directory. Framework directories contain at least the following three items:\n1. Component interface de\ufb01nition: A header \ufb01le named <framework>.h will be located in the\ntop-level framework directory (e.g., the Memory framework contains memory/memory.h).\nThis well-known header \ufb01le de\ufb01nes the interfaces that each component in the framework must\nsupport. This header includes function pointer typedefs for the interface functions, structs for\nmarshaling these function pointers, and any other necessary types, attribute \ufb01elds, macros,\ndeclarations, etc.\n2. Base code: The base subdirectory contains the glue code that provides the core functionality\nof the framework. For example, the memory framework\u2019s base directory is memory/base. The\nbase is typically comprised of logistical grunt work such as \ufb01nding and opening components\nat run-time, common utility functionality that may be utilized by multiple components, etc.\n3. Components: All other subdirectories in the framework directory are assumed to be compo-\nnents. Just like the framework, the names of the components are the same names as their\nsubdirectories (e.g., the memory/posix subdirectory contains the POSIX component in the\nMemory framework).\nSimilar to how each framework de\ufb01nes the interfaces to which its components must adhere,\nframeworks also de\ufb01ne other operational aspects, such as how they bootstrap themselves, how they\npick components to use, and how they are shut down. Two common examples of how frameworks\ndi\ufb00er in their setup are many-of-many versus one-of-many frameworks, and static versus dynamic\nframeworks.\n230\nOpen MPI\n", "page": 244, "type": "text", "section": "Page 244"}
{"text": "Many-of-many frameworks.\nSome frameworks have functionality that can be implemented\nmultiple di\ufb00erent ways in the same process. For example, Open MPI\u2019s point-to-point network\nframework will load multiple driver plugins to allow a single process to send and receive messages\non multiple network types.\nSuch frameworks will typically open all components that they can \ufb01nd and then query each\ncomponent, e\ufb00ectively asking, \u201cDo you want to run?\u201d The components determine whether they want\nto run by examining the system on which they are running. For example, a point-to-point network\ncomponent will look to see if the network type it supports is both available and active on the system.\nIf it is not, the component will reply \u201cNo, I do not want to run\u201d, causing the framework to close and\nunload that component. If that network type is available, the component will reply \u201cYes, I want to\nrun\u201d, causing the framework to keep the component open for further use.\nOne-of-many frameworks.\nOther frameworks provide functionality for which it does not make\nsense to have more than one implementation available at run-time. For example, the creation of\na consistent checkpoint of a parallel job\u2014meaning that the job is e\ufb00ectively \u201cfrozen\u201d and can be\narbitrarily resumed later\u2014must be performed using the same back-end checkpointing system for\neach process in the job. The plugin that interfaces to the desired back-end checkpointing system is\nthe only checkpoint plugin that must be loaded in each process\u2014all others are unnecessary.\nDynamic frameworks.\nMost frameworks allow their components to be loaded at run-time via\nDSOs. This is the most \ufb02exible method of \ufb01nding and loading components; it allows features such\nas explicitly not loading certain components, loading third-party components that were not included\nin the main-line Open MPI distribution, etc.\nStatic frameworks.\nSome one-of-many frameworks have additional constraints that force their\none-and-only-one component to be selected at compile time (versus run time). Statically linking one-\nof-many components allows direct invocation of its member functions (versus invocation via function\npointer), which may be important in highly performance-sensitive functionality. One example is the\nmemcpy framework, which provides platform-optimized memcpy() implementations.\nAdditionally, some frameworks provide functionality that may need to be utilized before Open\nMPI is fully initialized. For example, the use of some network stacks require complicated memory\nregistration models, which, in turn, require replacing the C library\u2019s default memory management\nroutines. Since memory management is intrinsic to an entire process, replacing the default scheme\ncan only be done pre-main. Therefore, such components must be statically linked into Open MPI\nprocesses so that they can be available for pre-main hooks, long before MPI has even been initialized.\nPlugin Components\nOpen MPI plugins are divided into two parts: a component struct and a module struct. The component\nstruct and the functions to which it refers are typically collectively referred to as \u201cthe component.\u201d\nSimilarly, \u201cthe module\u201d collectively refers to the module struct and its functions. The division\nis somewhat analogous to C++ classes and objects. There is only one component per process; it\ndescribes the overall plugin with some \ufb01elds that are common to all components (regardless of\nframework). If the component elects to run, it is used to generate one or more modules, which\ntypically perform the bulk of the functionality required by the framework.\nJe\ufb00rey M. Squyres\n231\n", "page": 245, "type": "text", "section": "Page 245"}
{"text": "Throughout the next few sections, we\u2019ll build up the structures necessary for the TCP component\nin the BTL (byte transfer layer) framework. The BTL framework e\ufb00ects point-to-point message\ntransfers; the TCP component, not surprisingly, uses TCP as its underlying transport for message\npassing.\nComponent struct.\nRegardless of framework, each component contains a well-known, statically\nallocated and initialized component struct. The struct must be named according to the template\nmca_<framework>_<component>_component. For example, the TCP network driver component\u2019s\nstruct in the BTL framework is named mca_btl_tcp_component.\nHaving templated component symbols both guarantees that there will be no name collisions\nbetween components, and allows the MCA core to \ufb01nd any arbitrary component struct via dlsym(2)\n(or the appropriate equivalent in each supported operating system).\nThe base component struct contains some logistical information, such as the component\u2019s formal\nname, version, framework version adherence, etc. This data is used for debugging purposes, inventory\nlisting, and run-time compliance and compatibility checking.\nstruct mca_base_component_2_0_0_t {\n/* Component struct version number */\nint mca_major_version, mca_minor_version, mca_release_version;\n/* The string name of the framework that this component belongs to,\nand the framework\u2019s API version that this component adheres to */\nchar mca_type_name[MCA_BASE_MAX_TYPE_NAME_LEN + 1];\nint mca_type_major_version, mca_type_minor_version,\nmca_type_release_version;\n/* This component\u2019s name and version number */\nchar mca_component_name[MCA_BASE_MAX_COMPONENT_NAME_LEN + 1];\nint mca_component_major_version, mca_component_minor_version,\nmca_component_release_version;\n/* Function pointers */\nmca_base_open_component_1_0_0_fn_t mca_open_component;\nmca_base_close_component_1_0_0_fn_t mca_close_component;\nmca_base_query_component_2_0_0_fn_t mca_query_component;\nmca_base_register_component_params_2_0_0_fn_t\nmca_register_component_params;\n};\nThe base component struct is the core of the TCP BTL component; it contains the following\nfunction pointers:\n\u2022 Open. The open call is the initial query function invoked on a component. It allows a component\nto initialize itself, look around the system where it is running, and determine whether it wants\nto run. If a component can always be run, it can provide a NULL open function pointer.\nThe TCP BTL component open function mainly initializes some data structures and ensures\nthat invalid parameters were not set by the user.\n\u2022 Close. When a framework decides that a component is no longer needed, it calls the close\nfunction to allow the component to release any resources that it has allocated. The close\nfunction is invoked on all remaining components when processes are shutting down. However,\n232\nOpen MPI\n", "page": 246, "type": "text", "section": "Page 246"}
{"text": "close can also be invoked on components that are rejected at run time so that they can be closed\nand ignored for the duration of the process.\nThe TCP BTL component close function closes listening sockets and frees resources (e.g.,\nreceiving bu\ufb00ers).\n\u2022 Query. This call is a generalized \u201cDo you want to run?\u201d function. Not all frameworks utilize\nthis speci\ufb01c call\u2014some need more specialized query functions.\nThe BTL framework does not use the generic query function (it de\ufb01nes its own; see below), so\nthe TCP BTL does not \ufb01ll it in.\n\u2022 Parameter registration. This function is typically the \ufb01rst function called on a component. It\nallows the component to register any relevant run-time, user-settable parameters. Run-time\nparameters are discussed further below.\nThe TCP BTL component register function creates a variety of user-settable run-time parame-\nters, such as one which allows the user to specify which IP interface(s) to use.\nThe component structure can also be extended on a per-framework and/or per-component basis.\nFrameworks typically create a new component struct with the component base struct as the \ufb01rst\nmember. This nesting allows frameworks to add their own attributes and function pointers. For\nexample, a framework that needs a more specialized query function (as compared to the query\nfunction provided on the basic component) can add a function pointer in its framework-speci\ufb01c\ncomponent struct.\nThe MPI btl framework, which provides point-to-point MPI messaging functionality, uses this\ntechnique.\nstruct mca_btl_base_component_2_0_0_t {\n/* Base component struct */\nmca_base_component_t btl_version;\n/* Base component data block */\nmca_base_component_data_t btl_data;\n/* btl-framework specific query functions */\nmca_btl_base_component_init_fn_t btl_init;\nmca_btl_base_component_progress_fn_t btl_progress;\n};\nAs an example of the TCP BTL framework query functions, the TCP BTL component btl_init\nfunction does several things:\n\u2022 Creates a listening socket for each \u201cup\u201d IPv4 and IPv6 interface\n\u2022 Creates a module for each \u201cup\u201d IP interface\n\u2022 Registers the tuple (IP address, port) for each \u201cup\u201d IP interface with a central repository\nso that other MPI processes know how to contact it\nSimilarly, plugins can extend the framework-speci\ufb01c component struct with their own members.\nThe tcp component in the btl framework does this; it caches many data members in its component\nstruct.\nstruct mca_btl_tcp_component_t {\n/* btl framework-specific component struct */\nmca_btl_base_component_2_0_0_t super;\nJe\ufb00rey M. Squyres\n233\n", "page": 247, "type": "text", "section": "Page 247"}
{"text": "/* Some of the TCP BTL component\u2019s specific data members */\n/* Number of TCP interfaces on this server */\nuint32_t tcp_addr_count;\n/* IPv4 listening socket descriptor */\nint tcp_listen_sd;\n/* ...and many more not shown here */\n};\nThis struct-nesting technique is e\ufb00ectively a simple emulation of C++ single inheritance: a\npointer to an instance of a struct mca_btl_tcp_component_t can be cast to any of the three types\nsuch that it can be used by an abstraction layer than does not understand the \u201cderived\u201d types.\nThat being said, casting is generally frowned upon in Open MPI because it can lead to incredibly\nsubtle, di\ufb03cult-to-\ufb01nd bugs. An exception was made for this C++-emulation technique because it\nhas well-de\ufb01ned behaviors and helps enforce abstraction barriers.\nModule struct.\nModule structs are individually de\ufb01ned by each framework; there is little com-\nmonality between them. Depending on the framework, components generate one or more module\nstruct instances to indicate that they want to be used.\nFor example, in the BTL framework, one module usually corresponds to a single network device.\nIf an MPI process is running on a Linux server with three \u201cup\u201d Ethernet devices, the TCP BTL\ncomponent will generate three TCP BTL modules; one corresponding to each Linux Ethernet device.\nEach module will then be wholly responsible for all sending and receiving to and from its Ethernet\ndevice.\nTying it all together.\nFigure 15.3 shows the nesting of the structures in the TCP BTL component,\nand how it generates one module for each of the three Ethernet devices.\n!\"#$%\"&'%()*\"+)\"%,)%-',-./.\")\"&\n!\"#$%\"&'%()*\"+)*(!/)%-',-./.\")0)1)1)\"&&\n!\"#$%\"&'%()*(!/)%-',-./.\")0)1)1)\"&&\n!\"#$%\"&'%()*\"+)\"%,)'-2$+/)\"&\n3-$.2&\"-&/\"41&\n5-',-./.\"&\n%#/(\"/!&\n&\n'-2$+/&\n!\"#$%\"!&\n!\"#$%\"&'%()*\"+)\"%,)'-2$+/)\"&\n3-$.2&\"-&/\"46&\n!\"#$%\"&'%()*\"+)\"%,)'-2$+/)\"&\n3-$.2&\"-&/\"40&\nFigure 15.3: The left side shows the nesting of structures in the TCP BTL component. The right side shows\nhow the component generates one module struct for each \u201cup\u201d Ethernet interface.\nComposing BTL modules this way allows the upper-layer MPI progression engine both to treat\nall network devices equally, and to perform user-level channel bonding.\nFor example, consider sending a large message across the three-device con\ufb01guration described\nabove. Assume that each of the three Ethernet devices can be used to reach the intended receiver\n(reachability is determined by TCP networks and netmasks, and some well-de\ufb01ned heuristics). In\nthis case, the sender will split the large message into multiple fragments. Each fragment will be\n234\nOpen MPI\n", "page": 248, "type": "text", "section": "Page 248"}
{"text": "assigned\u2014in a round-robin fashion\u2014to one of the TCP BTL modules (each module will therefore\nbe assigned roughly one third of the fragments). Each module then sends its fragments over its\ncorresponding Ethernet device.\nThis may seem like a complex scheme, but it is surprisingly e\ufb00ective. By pipelining the sends\nof a large message across the multiple TCP BTL modules, typical HPC environments (e.g., where\neach Ethernet device is on a separate PCI bus) can sustain nearly maximum bandwidth speeds across\nmultiple Ethernet devices.\nRun-Time Parameters\nDevelopers commonly make decisions when writing code, such as:\n\u2022 Should I use algorithm A or algorithm B?\n\u2022 How large of a bu\ufb00er should I preallocate?\n\u2022 How long should the timeout be?\n\u2022 At what message size should I change network protocols?\n\u2022 ...and so on.\nUsers tend to assume that the developers will answer such questions in a way that is generally\nsuitable for most types of systems. However, the HPC community is full of scientist and engineer\npower users who want to aggressively tweak their hardware and software stacks to eke out every\npossible compute cycle. Although these users typically do not want to tinker with the actual code\nof their MPI implementation, they do want to tinker by selecting di\ufb00erent internal algorithms,\nchoosing di\ufb00erent resource consumption patterns, or forcing speci\ufb01c network protocols in di\ufb00erent\ncircumstances.\nTherefore, the MCA parameter system was included when designing Open MPI; the system is\na \ufb02exible mechanism that allows users to change internal Open MPI parameter values at run time.\nSpeci\ufb01cally, developers register string and integer MCA parameters throughout the Open MPI code\nbase, along with an associated default value and descriptive string de\ufb01ning what the parameter is\nand how it is used. The general rule of thumb is that rather than hard-coding constants, developers\nuse run-time-settable MCA parameters, thereby allowing power users to tweak run-time behavior.\nThere are a number of MCA parameters in the base code of the three abstraction layers, but the\nbulk of Open MPI\u2019s MCA parameters are located in individual components. For example, the TCL\nBTL plugin has a parameter that speci\ufb01es whether only TCPv4 interfaces, only TCPv6 interfaces, or\nboth types of interfaces should be used. Alternatively, another TCP BTL parameter can be set to\nspecify exactly which Ethernet devices to use.\nUsers can discover what parameters are available via a user-level command line tool (ompi_info).\nParameter values can be set in multiple ways: on the command line, via environment variables, via\nthe Windows registry, or in system- or user-level INI-style \ufb01les.\nThe MCA parameter system complements the idea of run-time plugin selection \ufb02exibility, and has\nproved to be quite valuable to users. Although Open MPI developers try hard to choose reasonable\ndefaults for a wide variety of situations, every HPC environment is di\ufb00erent. There are inevitably\nenvironments where Open MPI\u2019s default parameter values will be unsuitable\u2014and possibly even\ndetrimental to performance. The MCA parameter system allows users to be proactive and tweak\nOpen MPI\u2019s behavior for their environment. Not only does this alleviate many upstream requests for\nchanges and/or bug reports, it allows users to experiment with the parameter space to \ufb01nd the best\ncon\ufb01guration for their speci\ufb01c system.\nJe\ufb00rey M. Squyres\n235\n", "page": 249, "type": "text", "section": "Page 249"}
{"text": "15.3\nLessons Learned\nWith such a varied group of core Open MPI members, it is inevitable that we would each learn\nsomething, and that as a group, we would learn many things. The following list describes just a few\nof these lessons.\nPerformance\nMessage-passing performance and resource utilization are the king and queen of high-performance\ncomputing. Open MPI was speci\ufb01cally designed in such a way that it could operate at the very\nbleeding edge of high performance: incredibly low latencies for sending short messages, extremely\nhigh short message injection rates on supported networks, fast ramp-ups to maximum bandwidth for\nlarge messages, etc. Abstraction is good (for many reasons), but it must be designed with care so\nthat it does not get in the way of performance. Or, put di\ufb00erently: carefully choose abstractions that\nlend themselves to shallow, performant call stacks (versus deep, feature-rich API call stacks).\nThat being said, we also had to accept that in some cases, abstraction\u2014not architecture\u2014must\nbe thrown out the window. Case in point: Open MPI has hand-coded assembly for some of its most\nperformance-critical operations, such as shared memory locking and atomic operations.\nIt is worth noting that Figures 15.1 and 15.2 show two di\ufb00erent architectural views of Open MPI.\nThey do not represent the run-time call stacks or calling invocation layering for the high performance\ncode sections.\nLesson learned:\nIt is acceptable (albeit undesirable) and unfortunately sometimes necessary to\nhave gross, complex code in the name of performance (e.g., the aforementioned assembly code).\nHowever, it is always preferable to spend time trying to \ufb01gure out how to have good abstractions to\ndiscretize and hide complexity whenever possible. A few weeks of design can save literally hundreds\nor thousands of developer-hours of maintenance on tangled, subtle, spaghetti code.\nStanding on the Shoulders of Giants\nWe actively tried to avoid re-inventing code in Open MPI that someone else has already written (when\nsuch code is compatible with Open MPI\u2019s BSD licensing). Speci\ufb01cally, we have no compunctions\nabout either directly re-using or interfacing to someone else\u2019s code.\nThere is no place for the \u201cnot invented here\u201d religion when trying to solve highly complex\nengineering problems; it only makes good logistical sense to re-use external code whenever possible.\nSuch re-use frees developers to focus on the problems unique to Open MPI; there is no sense\nre-solving a problem that someone else has solved already.\nA good example of this kind of code re-use is the GNU Libtool Libltdl package. Libltdl is a\nsmall library that provides a portable API for opening DSOs and \ufb01nding symbols in them. Libltdl is\nsupported on a wide variety of operating systems and environments, including Microsoft Windows.\nOpen MPI could have provided this functionality itself\u2014but why? Libltdl is a \ufb01ne piece of\nsoftware, is actively maintained, is compatible with Open MPI\u2019s license, and provides exactly the\nfunctionality that was needed. Given these points, there is no realistic gain for Open MPI developers\nto re-write this functionality.\nLesson learned:\nWhen a suitable solution exists elsewhere, do not hesitate to integrate it and stop\nwasting time trying to re-invent it.\n236\nOpen MPI\n", "page": 250, "type": "text", "section": "Page 250"}
{"text": "Optimize for the Common Case\nAnother guiding architectural principle has been to optimize for the common case. For example,\nemphasis is placed on splitting many operations into two phases: setup and repeated action. The\nassumption is that setup may be expensive (meaning: slow). So do it once and get it over with.\nOptimize for the much more common case: repeated operation.\nFor example, malloc() can be slow, especially if pages need to be allocated from the operating\nsystem. So instead of allocating just enough bytes for a single incoming network message, allocate\nenough space for a bunch of incoming messages, divide the result up into individual message bu\ufb00ers,\nand set up a freelist to maintain them. In this way, the \ufb01rst request for a message bu\ufb00er may be slow,\nbut successive requests will be much faster because they will just be de-queues from a freelist.\nLesson learned:\nSplit common operations into (at least) two phases: setup and repeated action.\nNot only will the code perform better, it may be easier to maintain over time because the distinct\nactions are separated.\nMiscellaneous\nThere are too many more lessons learned to describe in detail here; the following are a few more\nlessons that can be summed up brie\ufb02y:\n\u2022 We were fortunate to draw upon 15+ years of HPC research and make designs that have (mostly)\nsuccessfully carried us for more than eight years. When embarking on a new software project,\nlook to the past. Be sure to understand what has already been done, why it was done, and what\nits strengths and weaknesses were.\n\u2022 The concept of components\u2014allowing multiple di\ufb00erent implementations of the same functionality\u2014\nhas saved us many times, both technically and politically. Plugins are good.\n\u2022 Similarly, we continually add and remove frameworks as necessary. When developers start\narguing about the \u201cright\u201d way to implement a new feature, add a framework that fronts\ncomponents that implement that feature. Or when newer ideas come along that obsolete older\nframeworks, don\u2019t hesitate to delete such kruft.\nConclusion\nIf we had to list the three most important things that we\u2019ve learned from the Open MPI project, I\nthink they would be as follows:\n\u2022 One size does not \ufb01t all (users). The run-time plugin and companion MCA parameter system\nallow users \ufb02exibility that is necessary in the world of portable software. Complex software\nsystems cannot (always) magically adapt to a given system; providing user-level controls allows\na human to \ufb01gure out\u2014and override\u2014 when the software behaves sub-optimally.\n\u2022 Di\ufb00erences are good. Developer disagreements are good. Embrace challenges to the status\nquo; do not get complacent. A plucky grad student saying \u201cHey, check this out...\u201d can lead to\nthe basis of a whole new feature or a major evolution of the product.\n\u2022 Although outside the scope of this book, people and community matter. A lot.\n237\n", "page": 251, "type": "text", "section": "Page 251"}
{"text": "238\nOpen MPI\n", "page": 252, "type": "text", "section": "Page 252"}
{"text": "[chapter16]\nOSCAR\nJennifer Ruttan\nSince their initial adoption, EMR (electronic medical record) systems have attempted to bridge the\ngap between the physical and digital worlds of patient care. Governments in countries around the\nworld have attempted to come up with a solution that enables better care for patients at a lower cost\nwhile reducing the paper trail that medicine typically generates. Many governments have been very\nsuccessful in their attempts to create such a system\u2014some, like that of the Canadian province of\nOntario, have not (some may remember the so-called \u201ceHealth Scandal\u201d in Ontario that, according\nto the Auditor General, cost taxpayers $1 billion CAD).\nAn EMR permits the digitization of a patient chart, and when used properly should make it\neasier for a physician to deliver care. A good system should provide a physician a bird\u2019s eye view\nof a patient\u2019s current and ongoing conditions, their prescription history, their recent lab results,\nhistory of their previous visits, and so on. OSCAR (Open Source Clinical Application Resource), an\napproximately ten-year-old project of McMaster University in Hamilton, Ontario, Canada, is the\nopen source community\u2019s attempt to provide such a system to physicians at low or no cost.\nOSCAR has many subsystems that provide functionality on a component-by-component basis.\nFor example, oscarEncounter provides an interface for interacting with a patient\u2019s chart directly; Rx3\nis a prescription module that checks for allergies and drug interactions automatically and allows a\nphysician to directly fax a prescription to a pharmacy from the UI; the Integrator is a component to\nenable data sharing between multiple compatible EMRs. All of these separate components come\ntogether to build the typical OSCAR user experience.\nOSCAR won\u2019t be for every physician; for example, a specialist may not \ufb01nd all the features of\nthe system useful, and it is not easily customizable. However, it o\ufb00ers a complete set of features for\na general physician interacting with patients on a day-to-day basis.\nIn addition, OSCAR is CMS 3.0 certi\ufb01ed (and has applied for CMS 4.0 certi\ufb01cation)\u2014which\nallows physicians to receive funding for installing it in their clinic1. Receiving CMS certi\ufb01cation\ninvolves passing a set of requirements from the Government of Ontario and paying a fee.\nThis chapter will discuss the architecture of OSCAR in fairly general terms, describing the\nhierarchy, major components, and most importantly the impact that past decisions have made on the\nproject. As a conclusion and to wrap up, there will be a discussion on how OSCAR might have been\ndesigned today if there was an opportunity to do so.\n1See https://www.emradvisor.ca/ for details.\n", "page": 253, "type": "text", "section": "Page 253"}
{"text": "16.1\nSystem Hierarchy\nAs a Tomcat web application, OSCAR generally follows the typical model-view-controller design\npattern. This means that the model code (Data Access Objects, or DAOs) is separate from the\ncontroller code (servlets) and those are separated from the views (Java Server Pages, or JSPs). The\nmost signi\ufb01cant di\ufb00erence between the two is that servlets are classes and JSPs are HTML pages\nmarked up with Java code. Data gets placed into memory when a servlet executes and the JSP reads\nthat same data, usually done via reads and writes to the attributes of the request object. Just about\nevery JSP page in OSCAR has this kind of design.\n16.2\nPast Decision Making\nI mentioned that OSCAR is a fairly old project. This has implications for how e\ufb00ectively the MVC\npattern has been applied. In short, there are sections of the code that completely disregard the\npattern as they were written before tighter enforcement of the MVC pattern began. Some of the most\ncommon features are written this way; for example, performing many actions related to demographics\n(patient records) are done via the demographiccontrol.jsp \ufb01le\u2014this includes creating patients\nand updating their data.\nOSCAR\u2019s age is a hurdle for tackling many of the problems that are facing the source tree today.\nIndeed, there has been signi\ufb01cant e\ufb00ort made to improve the situation, including enforcing design\nrules via a code review process. This is an approach that the community at present has decided will\nallow better collaboration in the future, and will prevent poor code from becoming part of the code\nbase, which has been a problem in the past.\nThis is by no means a restriction on how we could design parts of the system now; it does,\nhowever, make it more complicated when deciding to \ufb01x bugs in a dated part of OSCAR. Do you, as\nsomebody tasked to \ufb01x a bug in the Demographic Creation function, \ufb01x the bug with code in the\nsame style as it currently exists? Or do you re-write the module completely so that it closely follows\nthe MVC design pattern?\nAs developers we must carefully weigh our options in situations like those. There is no guarantee\nthat if you re-architect a part of the system you will not create new bugs, and when patient data is on\nthe line, we must make the decision carefully.\n16.3\nVersion Control\nA CVS repository was used for much of OSCAR\u2019s life. Commits weren\u2019t often checked for consistency\nand it was possible to commit code that could break the build. It was tough for developers to keep up\nwith changes\u2014especially new developers joining the project late in its lifecycle. A new developer\ncould see something that they would want to change, make the change, and get it into the source\nbranch several weeks before anybody would notice that something signi\ufb01cant had been modi\ufb01ed\n(this was especially prevalent during long holidays, such as Christmas break, when not many people\nwere watching the source tree).\nThings have changed; OSCAR\u2019s source tree is now controlled by git. Any commits to the main\nbranch have to pass code-style checking and unit testing, successfully compile, and be code reviewed\nby the developers (much of this is handled by the combination of Hudson2 and Gerrit3). The project\n2A continuous integration server: http://hudson-ci.org/\n3A code review tool: http://code.google.com/p/gerrit/\n240\nOSCAR\n", "page": 254, "type": "text", "section": "Page 254"}
{"text": "has become much more tightly controlled. Many or all of the issues caused by poor handling of the\nsource tree have been solved.\n16.4\nData Models/DAOs\nWhen looking through the OSCAR source, you may notice that there are many di\ufb00erent ways to\naccess the database: you can use a direct connection to the database via a class called DBHandler,\nuse a legacy Hibernate model, or use a generic JPA model. As new and easier database access models\nbecame available, they were integrated into OSCAR. The result is that there is now a slightly noisy\npicture of how OSCAR interacts with data in MySQL, and the di\ufb00erences between the three types of\ndata access methods are best described with examples.\nEForms (DBHandler)\nThe EForm system allows users to create their own forms to attach to patient records\u2014this feature is\nusually used to replace a paper-based form with a digital version. On each creation of a form of a\nparticular type, the form\u2019s template \ufb01le is loaded; then the data in the form is stored in the database\nfor each instance. Each instance is attached to a patient record.\nEForms allow you to pull in certain types of data from a patient chart or other area of the system\nvia free-form SQL queries (which are de\ufb01ned in a \ufb01le called apconfig.xml). This can be extremely\nuseful, as a form can load and then immediately be populated with demographic or other relevant\ninformation without intervention from the user; for example, you wouldn\u2019t have to type in a patient\u2019s\nname, age, date of birth, hometown, phone number, or the last note that was recorded for that patient.\nA design decision was made, when originally developing the EForm module, to use raw database\nqueries to populate a POJO (plain-old Java object) called EForm in the controller that is then passed\nto the view layer to display data on the screen, sort of like a JavaBean. Using a POJO in this case is\nactually closer in design to the Hibernate or JPA architecture, as I\u2019ll discuss in the next sections.\nAll of the functionality regarding saving EForm instances and templates is done via raw SQL\nqueries run through the DBHandler class. Ultimately, DBHandler is a wrapper for a simple JDBC\nobject and does not scrutinize a query before sending it to the SQL server. It should be added here\nthat DBHandler is a potential security \ufb02aw as it allows unchecked SQL to be sent to the server. Any\nclass that uses DBHandler must implement its own checking to make sure that SQL injection doesn\u2019t\noccur.\nDepending on the type of application you\u2019re writing, direct access of a database is sometimes\n\ufb01ne. In certain cases, it can even speed development up. Using this method to access the database\ndoesn\u2019t conform to the model-view-controller design pattern, though: if you\u2019re going to change your\ndatabase structure (the model), you have to change the SQL query elsewhere (in the controller).\nSometimes, adding certain columns or changing their type in OSCAR\u2019s database tables requires this\nkind of invasive procedure just to implement small features.\nIt may not surprise you to \ufb01nd out that the DBHandler object is one of the oldest pieces of code\nstill intact in the source. I personally don\u2019t know where it originated from but I consider it to be the\nmost \u201cprimitive\u201d of database access types that exist in the OSCAR source. No new code is permitted\nto use this class, and if code is committed that uses it, the commit will be rejected automatically.\nJennifer Ruttan\n241\n", "page": 255, "type": "text", "section": "Page 255"}
{"text": "Demographic Records (Hibernate)\nA demographic record contains general metadata about a patient; for example, their name, age,\naddress, language, and sex; consider it to be the result of an intake form that a patient \ufb01lls out during\ntheir \ufb01rst visit to a doctor. All of this data is retrieved and displayed as part of OSCAR\u2019s Master\nRecord for a speci\ufb01c demographic.\nUsing Hibernate to access the database is far safer than using DBHandler. For one, you have\nto explicitly de\ufb01ne which columns match to which \ufb01elds in your model object (in this case, the\nDemographic class). If you want to perform complex joins, they have to be done as prepared\nstatements. Finally, you will only ever receive an object of the type you ask for when performing a\nquery, which is very convenient.\nThe process of working with a Hibernate-style DAO and Model pair is quite simple. In the case\nof the Demographic object, there\u2019s a \ufb01le called Demographic.hbm.xml that describes the mapping\nbetween object \ufb01eld and database column. The \ufb01le describes which table to look at and what type of\nobject to return. When OSCAR starts, this \ufb01le will be read and a sanity check occurs to make sure\nthat this kind of mapping can actually be made (server startup fails if it can\u2019t). Once running, you\ngrab an instance of the DemographicDao object and run queries against it.\nThe best part about using Hibernate over DBHandler is that all of the queries to the server are\nprepared statements. This restricts you from running free-form SQL during runtime, but it also\nprevents any type of SQL injection attack. Hibernate will often build large queries to grab the data,\nand it doesn\u2019t always perform in an extremely e\ufb03cient way.\nIn the previous section I mentioned an example of the EForm module using DBHandler to\npopulate a POJO. This is the next logical step to preventing that kind of code from being written. If\nthe model has to change, only the .hbm.xml \ufb01le and the model class have to change (a new \ufb01eld and\ngetter/setter for the new column), and doing so won\u2019t impact the rest of the application.\nWhile newer than DBHandler, the Hibernate method is also starting to show its age. It\u2019s not\nalways convenient to use and requires a big con\ufb01guration \ufb01le for each table you want to access.\nSetting up a new object pair takes time and if you do it incorrectly OSCAR won\u2019t even start. For this\nreason, nobody should be writing new code that uses pure Hibernate, either. Instead, generic JPA is\nbeing embraced in new development.\nIntegrator Consent (JPA)\nThe newest form of database access is done via generic JPA. If the OSCAR project decided to switch\nfrom Hibernate to another database access API, conforming to the JPA standard for DAOs and Model\nobjects would make it very easy to migrate. Unfortunately, because this is so \u201cnew\u201d to the OSCAR\nproject, there are almost no areas of the system that actually use this method to get data.\nIn any case, let me explain how it works. Instead of a .hbm.xml \ufb01le, you add annotations to your\nModel and DAO objects. These annotations describe the table to look in, column mappings for \ufb01elds,\nand join queries. Everything is contained inside the two \ufb01les and nothing else is necessary for their\noperation. Hibernate still runs behind the scenes, though, in actually retrieving the data from the\ndatabase.\nAll of the Integrator\u2019s models are written using JPA\u2014and they are pretty good examples of both\nthe new style of database access as well as demonstrating that as a new technology to be implemented\ninto OSCAR, it hasn\u2019t been used in very many places yet. The Integrator is a relatively new addition\nto the source. It makes quite a lot of sense to use this new data access model as opposed to Hibernate.\n242\nOSCAR\n", "page": 256, "type": "text", "section": "Page 256"}
{"text": "Touching on a now-common theme in this section of the chapter, the annotated POJOs that JPA\nuses allow for a far more streamlined experience. For example, during the Integrator\u2019s build process,\nan SQL \ufb01le is created that sets up all of the tables for you\u2014an enormously useful thing to have. With\nthat ability, it\u2019s impossible to create mismatching tables and model objects (as you can do with any\nother type of database access method) and you never have to worry about naming of columns and\ntables. There are no direct SQL queries, so it\u2019s not possible to create SQL injection attacks. In short,\nit \u201cjust works\u201d.\nThe way that JPA works can be considered to be fairly similar to the way that ActiveRecord works\nin Ruby on Rails. The model class de\ufb01nes the data type and the database stores it; what happens in\nbetween that\u2014getting data in and out\u2014is not up to the user.\nIssues with Hibernate and JPA\nBoth Hibernate and JPA o\ufb00er some signi\ufb01cant bene\ufb01ts in typical use cases. For simple retrieval and\nstorage, they really cut time out of development and debugging.\nHowever, that doesn\u2019t mean that their implementation into OSCAR has been without issue.\nBecause the user doesn\u2019t de\ufb01ne the SQL between the database and the POJO referencing a speci\ufb01c\nrow, Hibernate gets to choose the best way to do it. The \u201cbest way\u201d can manifest itself in a couple of\nways: Hibernate can choose to just retrieve the simple data from the row, or it can perform a join and\nretrieve a lot of information at once. Sometimes these joins get out of hand.\nHere\u2019s another example: The casemgmt_note table stores all patient notes. Each note object\nstores lots of metadata about the note\u2014but it also stores a list of all of the issues that the note deals\nwith (issues can be things like, \u201csmoking cessation\u201d or \u201cdiabetes\u201d, which describe the contents of\nthe note). The list of issues is represented in the note object as a List<CaseManagementIssue>.\nIn order to get that list, the casemgmt_note table is joined with the casemgmt_issue_notes table\n(which acts as a mapping table) and \ufb01nally the casemgmt_issue table.\nWhen you want to write a custom query in Hibernate, which this situation requires, you don\u2019t\nwrite standard SQL\u2014you write HQL (Hibernate Query Language) that is then translated to SQL (by\ninserting internal column names for all the \ufb01elds to be selected) before parameters are inserted and\nthe query is sent to the database server. In this speci\ufb01c case, the query was written with basic joins\nwith no join columns\u2014meaning that when the query was eventually translated to SQL, it was so\nlarge that it wasn\u2019t immediately obvious what the query was gathering. Additionally, in almost all\ncases, this never created a large enough temporary table for it to matter. For most users, this query\nactually runs quickly enough that it\u2019s not noticeable. However, this query is unbelievably ine\ufb03cient.\nLet\u2019s step back for a second. When you perform a join on two tables, the server has to create a\ntemporary table in memory. In the most generic type of joins, the number of rows is equal to the\nnumber of rows in the \ufb01rst table multiplied by the number of rows in the second table. So if your\ntable has 500,000 rows, and you join it with a table that has 10,000,000 rows, you\u2019ve just created\na 5\u00d71012 row temporary table in memory, which the select statement is then run against and that\ntemporary table is discarded.\nIn one extreme case that we ran into, the join across three tables caused a temporary table to be\ncreated that was around 7\u00d71012 rows in length, of which about 1000 rows were eventually selected.\nThis operation took about 5 minutes and locked the casemgmt_note table while it was running.\nThe problem was solved, eventually, through the use of a prepared statement that restricted\nthe scope of the \ufb01rst table before joining with the other two. The newer, far more e\ufb03cient query\nbrought the number of rows to select down to a very manageable 300,000 and enormously improved\nJennifer Ruttan\n243\n", "page": 257, "type": "text", "section": "Page 257"}
{"text": "performance of the notes retrieval operation (down to about 0.1 seconds to perform the same select\nstatement).\nThe moral of the story is simply that while Hibernate does a fairly good job, unless the join is\nvery explicitly de\ufb01ned and controlled (either in the .hbm.xml \ufb01le or a join annotation in the object\nclass for a JPA model), it can very quickly get out of control. Dealing with objects instead of SQL\nqueries requires you to leave the actual implementation of the query up to the database access library\nand only really allows you to control de\ufb01nition. Unless you\u2019re careful with how you de\ufb01ne things, it\ncan all fall apart under extreme conditions. Furthermore, if you\u2019re a database programmer with lots\nof SQL knowledge, it won\u2019t really help much when designing a JPA-enabled class, and it removes\nsome of the control that you would have if you were writing an SQL statement manually. Ultimately,\na good knowledge of both SQL and JPA annotations and how they a\ufb00ect queries is required.\n16.5\nPermissions\nCAISI (Client Access to Integrated Services and Information) was originally a standalone product\u2014a\nfork of OSCAR\u2014to help manage homeless shelters in Toronto. A decision was eventually made to\nmerge the code from CAISI into the main source branch. The original CAISI project may no longer\nexist, but what it gave to OSCAR is very important: its permission model.\nThe permissions model in OSCAR is extremely powerful and can be used to create just about\nas many roles and permission sets as possible. Providers belong to programs (as sta\ufb00) where\nthey have a speci\ufb01c role. Each program takes place at a facility. Each role has a description (for\nexample, \u201cdoctor\u201d, \u201cnurse\u201d, \u201csocial worker\u201d, and so on) and a set of attached global permissions.\nThe permissions are written in a format that makes them very easy to understand: \u201cread nurse notes\u201d\nmay be a permission that a doctor role may have, but the nurse role may not have the \u201cread doctor\nnotes\u201d permission.\nThis format may be easy to understand, but under the hood it requires quite a bit of heavy lifting\nto actually check for these types of permissions. The name of the role that the current provider has is\nchecked against its list of permissions for a match with the action that they are trying to perform.\nFor example, a provider attempting to read a doctor\u2019s notes would cause \u201cread doctor notes\u201d to be\nchecked for each and every note written by a doctor.\nAnother problem is the reliance on English for permission de\ufb01nition. Anybody using OSCAR in\na language other than English would still need to write their permissions in a format such as \u201cread\n[role] notes\u201d, using the English words \u201cread\u201d, \u201cwrite\u201d, \u201cnotes\u201d, and so on.\nCAISI\u2019s permission model is a signi\ufb01cant part of OSCAR, but it\u2019s not the only model in place.\nBefore CAISI was implemented, another role-based (but not program-based) system was developed\nand is still in use in many parts of the system today.\nFor this second system, providers are assigned one or many roles (for example, \u201cdoctor\u201d, \u201cnurse\u201d,\n\u201cadmin\u201d, and so on). They can be assigned as many roles as necessary\u2014the roles\u2019 permissions\nstack on top of each other. These permissions are generally used for restricting access to parts of\nthe system, as opposed to CAISI\u2019s permissions which restrict access to certain pieces of data on\na patient\u2019s chart. For example, a user has to have the \u201c_admin\u201d \u201cread\u201d permission on a role that\nthey have assigned to them to be able to access the Admin panel. Having the \u201cread\u201d permission will\nexempt them from being able to perform administrative tasks, however. They\u2019ll need the \u201cwrite\u201d\npermission as well for that.\nBoth of these systems accomplish roughly the same goal; it\u2019s due to CAISI\u2019s merge later in the\nproject lifecycle that they both exist. They don\u2019t always exist happily together, so in reality it can be\n244\nOSCAR\n", "page": 258, "type": "text", "section": "Page 258"}
{"text": "a lot easier to just focus on using one for day-to-day operations of OSCAR. You can generally date\ncode in OSCAR by knowing which permissions model preceded which other permissions model:\nProvider Type then Provider Roles then CAISI Programs/Roles\nThe oldest type of permissions model, \u201cProvider Type\u201d, is so dated that it\u2019s actually not used in\nmost parts of the system and is in fact defaulted to \u201cdoctor\u201d during new provider creation\u2014having it\nas any other value (such as \u201creceptionist\u201d) causes signi\ufb01cant issues throughout the system. It\u2019s easier\nand more \ufb01ne-grained to control permissions via Provider Roles instead.\n16.6\nIntegrator\nOSCAR\u2019s Integrator component is a separate web application that independent OSCAR instances\nuse to exchange patient, program and provider information over a secure link. It can be optionally\ninstalled as a component for an installation in an environment such as a LHN (Local Health Network)\nor a hospital. The easiest way to describe the Integrator is as a temporary storage facility.\nConsider the following use case and argument for use of the Integrator: in Hospital X, there is\nan ENT (ear, nose, and throat) clinic as well as an endocrinology clinic. If an ENT doctor refers\ntheir patient to an endocrinologist upstairs, they may be required to send along patient history and\nrecords. This is inconvenient and generates more paper than is necessary\u2014perhaps the patient is\nonly seeing the endocrinologist once. By using the Integrator, the patient\u2019s data can be accessed on\nthe endocrinologist\u2019s EMR, and access to the contents of the patient\u2019s chart can be revoked after the\nvisit.\nA more extreme example: if an unconscious man shows up in an ER with nothing but his health\ncard, because the home clinic and the hospital\u2019s system are connected via the Integrator, the man\u2019s\nrecord can be pulled and it can be very quickly realized that he has been prescribed the blood thinner\nwarfarin. Ultimately, information retrieval like this is what an EMR like OSCAR paired with the\nIntegrator can achieve.\nTechnical Details\nThe Integrator is available in source code form only, which requires the user to retrieve and build it\nmanually. Like OSCAR, it runs on a standard installation of Tomcat with MySQL.\nWhen the URL where the Integrator lives is accessed, it doesn\u2019t appear to display anything useful.\nThis component is almost purely a web service; OSCAR communicates via POST and GET requests\nto the Integrator URL.\nAs an independently developed project (initially as part of the CAISI project), the Integrator is\nfairly strict in adhering to the MVC design pattern. The original developers have done an excellent\njob of setting it up with very clearly de\ufb01ned lines between the models, views, and controllers. The\nmost recently implemented type of database access layer that I mentioned earlier\u2014generic JPA\u2014is\nthe only such layer in the project. (As an interesting side note: because the entire project is properly\nset up with JPA annotations on all the model classes, an SQL script is created at build time that\ncan be used to initialize the structure of the database; the Integrator, therefore, doesn\u2019t ship with a\nstand-alone SQL script.)\nCommunication is handled via web service calls described in WSDL XML \ufb01les that are available\non the server. A client could query the Integrator to \ufb01nd out what kind of functions are available and\nadapt to it. This really means that the Integrator is compatible with any kind of EMR that somebody\nJennifer Ruttan\n245\n", "page": 259, "type": "text", "section": "Page 259"}
{"text": "decides to write a client for; the data format is generic enough that it could easily be mapped to local\ntypes.\nFor OSCAR, though, a client library is built and included in the main source tree, for simplicity\u2019s\nsake. That library only ever needs to be updated if new functions become available on the Integrator.\nA bug \ufb01x on the Integrator doesn\u2019t require an update of that \ufb01le.\nDesign\nData for the Integrator comes in from all of the connected EMRs at scheduled times and, once\nthere, another EMR can request that data. None of the data on the Integrator is stored permanently,\nthough\u2014its database could be erased and it could be rebuilt from the client data.\nThe dataset sent is con\ufb01gured individually at each OSCAR instance which is connected to a\nparticular Integrator, and except in situations where the entire patient database has to be sent to the\nIntegrator server, only patient records that have been viewed since the previous push to the server are\nsent. The process isn\u2019t exactly like delta patching, but it\u2019s close.\nFigure 16.1: Data exchange between OSCARs and Integrator\nLet me go into a little more detail about how the Integrator works with an example: a remote\nclinic seeing another clinic\u2019s patient. When that clinic wants to access the patient\u2019s record, the clinics\n\ufb01rst have to have been connected to the same Integrator server. The receptionist can search the\nIntegrator for the remote patient (by name and optionally date of birth or sex) and \ufb01nd their record\nstored on the server. They initiate the copy of a limited set of the patient\u2019s demographic information\nand then double-check with the patient to make sure that they consent to the retrieval of their record by\ncompleting a consent form. Once completed, the Integrator server will deliver whatever information\nthe Integrator knows about that patient\u2014notes, prescriptions, allergies, vaccinations, documents,\nand so on. This data is cached locally so that the local OSCAR doesn\u2019t have to send a request to the\nIntegrator every time it wants to see this data, but the local cache expires every hour.\nAfter the initial setup of a remote patient by copying their demographic data to the local OSCAR,\nthat patient is set up as any other on the system. All of the remote data that is retrieved from\nthe Integrator is marked as such (and the clinic from which it came from is noted), but it\u2019s only\ntemporarily cached on the local OSCAR. Any local data that is recorded is recorded just like any\nother patient data\u2014to the patient record, and sent to the Integrator\u2014but not permanently stored on\nany remote machine.\nThis has a very important implication, especially for patient consent and how that factors into\nthe design of the Integrator. Let\u2019s say that a patient sees a remote physician and is \ufb01ne with them\nhaving access to their record, but only temporarily. After their visit, they can revoke the consent for\nthat clinic to be able to view that patient\u2019s record and the next time that clinic opens the patient\u2019s\n246\nOSCAR\n", "page": 260, "type": "text", "section": "Page 260"}
{"text": "Figure 16.2: The Demographic information and associated data is sent to the Integrator during a data push from\nthe home clinic. The record on the Integrator may not be a representation of the complete record from the home\nclinic as the OSCAR can choose not to send all patient data.\nFigure 16.3: A remote OSCAR requests data from the Integrator by asking for a speci\ufb01c patient record. The\nIntegrator server sends only the demographic information, which is stored permanently on the remote OSCAR.\nchart there won\u2019t be any data there (with the exception of any data that was locally recorded). This\nultimately gives control over how and when a record is viewed directly to the patient and is similar\nto walking into a clinic carrying a copy of your paper chart. They can see the chart while they\u2019re\ninteracting with you, but you take it home with you when you leave.\nFigure 16.4: A remote clinic can see the contents of a patient chart by asking for the data; if the appropriate\nconsent is present, the data is sent. The data is never stored permanently on the remote OSCAR.\nAnother very important ability is for physicians to decide what kinds of data they want to share\nwith the other connected clinics via their Integrator server. A clinic can choose to share all of a\ndemographic record or only parts of it, such as notes but not documents, allergies but not prescriptions,\nand so on. Ultimately it\u2019s up to the group of physicians who set up the Integrator server to decide\nwhat kinds of data they\u2019re comfortable with sharing with each other.\nAs I mentioned before, the Integrator is only a temporary storage warehouse and no data is ever\nstored permanently there. This is another very important decision that was made during development;\nit allows clinics to back out of sharing any and all data via the Integrator very easily\u2014and in fact if\nnecessary the entire Integrator database can be wiped. If the database is wiped, no user of a client\nwill ever notice because the data will be accurately reconstructed from the original data on all of the\nJennifer Ruttan\n247\n", "page": 261, "type": "text", "section": "Page 261"}
{"text": "various connected clients. An implication is that the OSCAR provider needs to trust the Integrator\nprovider to have wiped the database when they say so\u2014it is therefore best to deploy an Integrator to\na group of physicians already in a legal organization such as a Family Health Organization or Family\nHealth Team; the Integrator server would be housed at one of these physician\u2019s clinics.\nData Format\nThe Integrator\u2019s client libraries are built via wsdl2java, which creates a set of classes representing\nthe appropriate data types the web service communicates in. There are classes for each data type as\nwell as classes representing keys for each of these data types.\nIt\u2019s outside the scope of this chapter to describe how to build the Integrator\u2019s client library. What\u2019s\nimportant to know is that once the library is built, it must be included with the rest of the JARs in\nOSCAR. This JAR contains everything necessary to set up the Integrator connection and access\nall of the data types that the Integrator server will return to OSCAR, such as CachedDemographic,\nCachedDemographicNote, and CachedProvider, among many others. In addition to the data types\nthat are returned, there are \u201cWS\u201d classes that are used for the retrieval of such lists of data in the \ufb01rst\nplace\u2014the most frequently used being DemographicWs.\nDealing with the Integrator data can sometimes be a little tricky. OSCAR doesn\u2019t have anything\ntruly built-in to handle this kind of data, so what usually happens is when retrieving a certain kind of\npatient data (for example, notes for a patient\u2019s chart) the Integrator client is asked to retrieve data\nfrom the server. That data is then manually transformed into a local class representing that data (in\nthe case of notes, it\u2019s a CaseManagementNote). A Boolean \ufb02ag is set inside the class to indicate\nthat it\u2019s a piece of remote content and that is used to change how the data is displayed to the user on\nthe screen. On the opposite end, CaisiIntegratorUpdateTask handles taking local OSCAR data,\nconverting it into the Integrator\u2019s data format, and then sending that data to the Integrator server.\nThis design may not be as e\ufb03cient or as clean as possible, but it does enable older parts of\nthe system to become \u201ccompatible\u201d with Integrator-delivered data without much modi\ufb01cation. In\naddition, keeping the view as simple as possible by referring to only one type of class improves the\nreadability of the JSP \ufb01le and makes it easier to debug in the event of an error.\n16.7\nLessons Learned\nAs you can probably imagine, OSCAR has its share of issues when it comes to overall design. It does,\nhowever, provide a complete feature set that most users will \ufb01nd no issues with. That\u2019s ultimately\nthe goal of the project: provide a good solution that works in most situations.\nI can\u2019t speak for the entire OSCAR community, so this section will be highly subjective and from\nmy point of view. I feel that there are some important takeaways from an architectural discussion\nabout the project.\nFirst, it\u2019s clear that poor source control in the past has caused the architecture of the system to\nbecome highly chaotic in parts, especially in areas where the controllers and the views blend together.\nThe way that the project was run in the past didn\u2019t prevent this from happening, but the process has\nchanged since and hopefully we won\u2019t have to deal with such a problem again.\nNext, because the project is so old, it\u2019s di\ufb03cult to upgrade (or even change) libraries without\ncausing signi\ufb01cant disruption throughout the code base. That\u2019s exactly what has happened, though.\nI often \ufb01nd it di\ufb03cult to \ufb01gure out what\u2019s necessary and what isn\u2019t when I\u2019m looking in the library\nfolder. In addition to that, sometimes when libraries undergo major upgrades they break backwards\n248\nOSCAR\n", "page": 262, "type": "text", "section": "Page 262"}
{"text": "compatibility (changing package names is a common o\ufb00ense). There are often several libraries\nincluded with OSCAR that all accomplish the same task\u2014this goes back to poor source control, but\nalso the fact that that there has been no list or documentation describing which library is required by\nwhich component.\nAdditionally, OSCAR is a little in\ufb02exible when it comes to adding new features to existing\nsubsystems. For example, if you want to add a new box to the E-Chart, you\u2019ll have to create a new\nJSP page and a new servlet, modify the layout of the E-Chart (in a few places), and modify the\ncon\ufb01guration \ufb01le of the application so that your servlet can load.\nNext, due to the lack of documentation, sometimes it is nearly impossible to \ufb01gure out how a part\nof the system works\u2014the original contributor may not even be part of the project anymore\u2014and\noften the only tool you have to \ufb01gure it out is a debugger. As a project of this age, this is costing\nthe community the potential for new contributors to get involved. However, it\u2019s something that, as a\ncollaborative e\ufb00ort, the community is working on.\nFinally, OSCAR is a repository for medical information and its security is compromised by the\ninclusion of the DBHandler class (discussed in a previous section). I personally feel that freeform\ndatabase queries that accept parameters should never be acceptable in an EMR because it\u2019s so easy\nto perform SQL injection attacks. While it\u2019s good that no new code is permitted that uses this class,\nit should be a priority of the development team to remove all instances of its use.\nAll of that may sound like some harsh criticism of the project. In the past, all of these problems\nhave been signi\ufb01cant and, like I said, prevent the community from growing as the barrier to entry\nis so high. This is something that is changing, so in the future, these issues won\u2019t be so much of a\nhindrance.\nIn looking back over the project\u2019s history (and especially over the past few versions) we can come\nup with a better design for how the application would be built. The system still has to provide a base\nlevel of functionality (mandated by the Ontario government for certi\ufb01cation as an EMR), so that\nall has to be baked in by default. But if OSCAR were to be redesigned today, it should be designed\nin a truly modular fashion that would allow modules to be treated as plugins; if you didn\u2019t like the\ndefault E-Form module, you could write your own (or even another module entirely). It should be\nable to speak to more systems (or more systems should be able to speak to it), including the medical\nhardware that you see in increasing use throughout the industry, such as devices for measuring visual\nacuity. This also means that it would be easy to adapt OSCAR to the requirements of local and\nfederal governments around the world for storing medical data. Since every region has a di\ufb00erent set\nof laws and requirements, this kind of design would be crucial for making sure that OSCAR develops\na worldwide userbase.\nI also believe that security should be the most important feature of all. An EMR is only as secure\nas its least secure component, so there should be focus on abstracting away as much data access\nas possible from the application so that it stores and retrieves data in a sandbox-style environment\nthrough a main data access layer API that has been audited by a third-party and found to be adequate\nfor storing medical information. Other EMRs can hide behind obscurity and proprietary code as\na security measure (which isn\u2019t really a security measure at all), but being open source, OSCAR\nshould lead the charge with better data protection.\nI stand \ufb01rmly as a believer in the OSCAR project. We have hundreds of users that we know about\n(and the many hundreds that we don\u2019t), and we receive valuable feedback from the physicians who\nare interacting with our project on a daily basis. Through the development of new processes and\nnew features, we hope to grow the installed base and to support users from other regions. It is our\nintention to make sure that what we deliver is something that improves the lives of the physicians who\nuse OSCAR as well as the lives of their patients, by creating better tools to help manage healthcare.\n249\n", "page": 263, "type": "text", "section": "Page 263"}
{"text": "250\nOSCAR\n", "page": 264, "type": "text", "section": "Page 264"}
{"text": "[chapter17]\nProcessing.js\nMike Kamermans\nOriginally developed by Ben Fry and Casey Reas, the Processing programming language started\nas an open source programming language (based on Java) to help the electronic arts and visual\ndesign communities learn the basics of computer programming in a visual context. O\ufb00ering a highly\nsimpli\ufb01ed model for 2D and 3D graphics compared to most programming languages, it quickly\nbecame well-suited for a wide range of activities, from teaching programming through writing small\nvisualisations to creating multi-wall art installations, and became able to perform a wide variety of\ntasks, from simply reading in a sequence of strings to acting as the de facto IDE for programming\nand operating the popular \u201cArduino\u201d open source hardware prototyping boards. Continuing to gain\npopularity, Processing has \ufb01rmly taken its place as an easy to learn, widely used programming\nlanguage for all things visual, and so much more.\nThe basic Processing program, called a \u201csketch\u201d, consists of two functions: setup and draw.\nThe \ufb01rst is the main program entry point, and can contain any amount of initialization instructions.\nAfter \ufb01nishing setup, Processing programs can do one of two things: 1) call draw, and schedule\nanother call to draw at a \ufb01xed interval upon completion; or 2) call draw, and wait for input events\nfrom the user. By default, Processing does the former; calling noLoop results in the latter. This\nallows for two modes to present sketches, namely a \ufb01xed framerate graphical environment, and an\ninteractive, event-based updating graphical environment. In both cases, user events are monitored\nand can be handled either in their own event handlers, or for certain events that set persistent global\nvalues, directly in the draw function.\nProcessing.js is a sister project of Processing, designed to bring it to the web without the need\nfor Java or plugins. It started as an attempt by John Resig to see if the Processing language could be\nported to the web, by using the\u2014at the time brand new\u2014HTML5 <canvas> element as a graphical\ncontext, with a proof of concept library released to the public in 2008. Written with the idea in\nmind that \u201cyour code should just work\u201d, Processing.js has been re\ufb01ned over the years to make data\nvisualisations, digital art, interactive animations, educational graphs, video games, etc. work using\nweb standards and without any plugins. You write code using the Processing language, either in\nthe Processing IDE or your favourite editor of choice, include it on a web page using a <canvas>\nelement, and Processing.js does the rest, rendering everything in the <canvas> element and letting\nusers interact with the graphics in the same way they would with a normal standalone Processing\nprogram.\n", "page": 265, "type": "text", "section": "Page 265"}
{"text": "17.1\nHow Does It Work?\nProcessing.js is a bit unusual as an open source project, in that the code base is a single \ufb01le called\nprocessing.js which contains the code for Processing, the single object that makes up the entire\nlibrary. In terms of how the code is structured, we constantly shu\ufb04e things around inside this object\nas we try to clean it up a little bit with every release. Its design is relatively straightforward, and its\nfunction can be described in a single sentence: it rewrites Processing source code into pure JavaScript\nsource code, and every Processing API function call is mapped to a corresponding function in the\nJavaScript Processing object, which e\ufb00ects the same thing on a <canvas> element as the Processing\ncall would e\ufb00ect on a Java Applet canvas.\nFor speed, we have two separate code paths for 2D and 3D functions, and when a sketch is loaded,\neither one or the other is used for resolving function wrappers so that we don\u2019t add bloat to running\ninstances. However, in terms of data structures and code \ufb02ow, knowing JavaScript means you can\nread processing.js, with the possible exception of the syntax parser.\nUnifying Java and JavaScript\nRewriting Processing source code into JavaScript source code means that you can simply tell the\nbrowser to execute the rewritten source, and if you rewrote it correctly, things just work. But, making\nsure the rewrite is correct has taken, and still occasionally takes, quite a bit of e\ufb00ort. Processing\nsyntax is based on Java, which means that Processing.js has to essentially transform Java source code\ninto JavaScript source code. Initially, this was achieved by treating the Java source code as a string,\nand iteratively replacing substrings of Java with their JavaScript equivalents1. For a small syntax set,\nthis is \ufb01ne, but as time went on and complexity added to complexity, this approach started to break\ndown. Consequently, the parser was completely rewritten to build an Abstract Syntax Tree (AST)\ninstead, \ufb01rst breaking down the Java source code into functional blocks, and then mapping each of\nthose blocks to their corresponding JavaScript syntax. The result is that, at the cost of readability2,\nProcessing.js now e\ufb00ectively contains an on-the-\ufb02y Java-to-JavaScript transcompiler.\nHere is the code for a Processing sketch:\nvoid setup() {\nsize(200,200);\nnoCursor();\nnoStroke();\nsmooth(); }\nvoid draw() {\nfill(255,10);\nrect(-1,-1,width+1,height+1);\nfloat f = frameCount*PI/frameRate;\nfloat d = 10+abs(60*sin(f));\nfill(0,100,0,50);\nellipse(mouseX, mouseY, d,d); }\n1For those interested in an early incarnation of the parser, it can be found at https://github.com/jeresig/processing-\njs/blob/51d280c516c0530cd9e63531076dfa147406e6b2/processing.js, running from line 37 to line 266.\n2Readers are welcome to peruse https://github.com/jeresig/processing-js/blob/v1.3.0/processing.js#L17649,\nup to line 19217.\n252\nProcessing.js\n", "page": 266, "type": "text", "section": "Page 266"}
{"text": "And here is its Processing.js conversion:\nfunction($p) {\nfunction setup() {\n$p.size(200, 200);\n$p.noCursor();\n$p.noStroke();\n$p.smooth(); }\n$p.setup = setup;\nfunction draw() {\n$p.fill(255, 10);\n$p.rect(-1, -1, $p.width + 1, $p.height + 1);\nvar f = $p.frameCount * $p.PI / $p.__frameRate;\nvar d = 10 + $p.abs(60 * $p.sin(f));\n$p.fill(0, 100, 0, 50);\n$p.ellipse($p.mouseX, $p.mouseY, d, d); }\n$p.draw = draw; }\nThis sounds like a great thing, but there are a few problems when converting Java syntax to\nJavaScript syntax:\n1. Java programs are isolated entities. JavaScript programs share the world with a web page.\n2. Java is strongly typed. JavaScript is not.\n3. Java is a class/instance based object-oriented language. JavaScript is not.\n4. Java has distinct variables and methods. JavaScript does not.\n5. Java allows method overloading. JavaScript does not.\n6. Java allows importing compiled code. JavaScript has no idea what that even means.\nDealing with these problems has been a tradeo\ufb00between what users need, and what we can do\ngiven web technologies. The following sections will discuss each of these issues in greater detail.\n17.2\nSigni\ufb01cant Di\ufb00erences\nJava programs have their own threads; JavaScript can lock up your browser.\nJava programs are isolated entities, running in their own thread in the greater pool of applications\non your system. JavaScript programs, on the other hand, live inside a browser, and compete with\neach other in a way that desktop applications don\u2019t. When a Java program loads a \ufb01le, the program\nwaits until the resource is done loading, and operation resumes as intended. In a setting where the\nprogram is an isolated entity on its own, this is \ufb01ne. The operating system stays responsive because\nit\u2019s responsible for thread scheduling, and even if the program takes an hour to load all its data, you\ncan still use your computer. On a web page, this is not how things work. If you have a JavaScript\n\u201cprogram\u201d waiting for a resource to be done loading, it will lock its process until that resource is\navailable. If you\u2019re using a browser that uses one process per tab, it will lock up your tab, and the rest\nof the browser is still usable. If you\u2019re using a browser that doesn\u2019t, your entire browser will seem\nfrozen. So, regardless of what the process represents, the page the script runs on won\u2019t be usable\nuntil the resource is done loading, and it\u2019s entirely possible that your JavaScript will lock up the\nentire browser.\nMike Kamermans\n253\n", "page": 267, "type": "text", "section": "Page 267"}
{"text": "This is unacceptable on the modern web, where resources are transferred asynchronously, and\nthe page is expected to function normally while resources are loaded in the background. While\nthis is great for traditional web pages, for web applications this is a real brain twister: how do you\nmake JavaScript idle, waiting for a resource to load, when there is no explicit mechanism to make\nJavaScript idle? While there is no explicit threading in JavaScript, there is an event model, and there\nis an XMLHTTPRequest object for requesting arbitrary (not just XML or HTML) data from arbitrary\nURLS. This object comes with several di\ufb00erent status events, and we can use it to asynchronously\nget data while the browser stays responsive. Which is great in programs in which you control the\nsource code: you make it simply stop after scheduling the data request, and make it pick up execution\nwhen the data is available. However, this is near impossible for code that was written based on the\nidea of synchronous resource loading. Injecting \u201cidling\u201d in programs that are supposed to run at a\n\ufb01xed framerate is not an option, so we have to come up with alternative approaches.\nFor some things, we decided to force synchronous waiting anyway. Loading a \ufb01le with strings,\nfor instance, uses a synchronous XMLHTTPRequest, and will halt execution of the page until the data\nis available. For other things, we had to get creative. Loading images, for instance, uses the browser\u2019s\nbuilt-in mechanism for loading images; we build a new Image in JavaScript, set its src attribute to\nthe image URL, and the browser does the rest, notifying us that the image is ready through the onload\nevent. This doesn\u2019t even rely on an XMLHTTPRequest, it simply exploits the browser\u2019s capabilities.\nTo make matters easier when you already know which images you are loading, we added preload\ndirectives so that the sketch does not start execution until preloading is complete. A user can indicate\nany number of images to preload via a comment block at the start of the sketch; Processing.js then\ntracks outstanding image loading. The onload event for an image tells us that it is done transferring\nand is considered ready to be rendered (rather than simply having been downloaded but not decoded\nto a pixel array in memory yet), after which we can populate the corresponding Processing PImage\nobject with the correct values (width, height, pixel data, etc.) and clear the image from the list.\nOnce the list is empty, the sketch gets executed, and images used during its lifetime will not require\nwaiting.\nHere is an example of preload directives:\n/* @pjs preload=\"./worldmap.jpg\"; */\nPImage img;\nvoid setup() {\nsize(640,480);\nnoLoop();\nimg = loadImage(\"worldmap.jpg\"); }\nvoid draw() {\nimage(img,0,0); }\nFor other things, we\u2019ve had to build more complicated \u201cwait for me\u201d systems. Fonts, unlike\nimages, do not have built-in browser loading (or at least not a system as functional as image loading).\nWhile it is possible to load a font using a CSS @font-face rule and rely on the browser to make it all\nhappen, there are no JavaScript events that can be used to determine that a font \ufb01nished loading. We\nare slowly seeing events getting added to browsers to generate JavaScript events for font download\ncompletion, but these events come \u201ctoo early\u201d, as the browser may need anywhere from a few to a\nfew hundred more milliseconds to actually parse the font for use on the page after download. Thus,\nacting on these events will still lead to either no font being applied, or the wrong font being applied\n254\nProcessing.js\n", "page": 268, "type": "text", "section": "Page 268"}
{"text": "if there is a known fallback font. Rather than relying on these events, we embed a tiny TrueType\nfont that only contains the letter \u201cA\u201d with impossibly small metrics, and instruct the browser to load\nthis font via an @font-face rule with a data URI that contains the font\u2019s bytecode as a BASE64\nstring. This font is so small that we can rely on it being immediately available. For any other font\nload instruction we compare text metrics between the desired font and this tiny font. A hidden <div>\nis set up with text styled using the desired font, with our tiny font as fallback. As long as the text in\nthat <div> is impossibly small, we know the desired font is not available yet, and we simply poll at\nset intervals until the text has sensible metrics.\nJava is strongly typed; JavaScript is not.\nIn Java, the number 2 and the number 2.0 are di\ufb00erent values, and they will do di\ufb00erent things\nduring mathematical operations. For instance, the code i = 1/2 will result in i being 0, because the\nnumbers are treated as integers, whereas i = 1/2.0, i = 1.0/2, and even i = 1./2. will all result\nin i being 0.5, because the numbers are considered decimal fractions with a non-zero integer part,\nand a zero fractional part. Even if the intended data type is a \ufb02oating point number, if the arithmetic\nuses only integers, the result will be an integer. This lets you write fairly creative math statements in\nJava, and consequently in Processing, but these will generate potentially wildly di\ufb00erent results when\nported to Processing.js, as JavaScript only knows \u201cnumbers\u201d. As far as JavaScript is concerned, 2\nand 2.0 are the same number, and this can give rise to very interesting bugs when running a sketch\nusing Processing.js.\nThis might sound like a big issue, and at \ufb01rst we were convinced it would be, but you can\u2019t argue\nwith real world feedback: it turns out this is almost never an issue for people who put their sketches\nonline using Processing.js. Rather than solving this in some cool and creative way, the resolution of\nthis problem was actually remarkably straightforward; we didn\u2019t solve it, and as a design choice, we\ndon\u2019t intend to ever revisit that decision. Short of adding a symbol table with strong typing so that\nwe can fake types in JavaScript and switch functionality based on type, this incompatibility cannot\nproperly be solved without leaving much harder to \ufb01nd edge case bugs, and so rather than adding\nbulk to the code and slowdown to execution, we left this quirk in. It is a well-documented quirk, and\n\u201cgood code\u201d won\u2019t try to take advantage of Java\u2019s implicit number type casting. That said, sometimes\nyou will forget, and the result can be quite interesting.\nJava is a class/instance-based object-oriented language, with separate variable and method\nspaces; JavaScript is not.\nJavaScript uses prototype objects, and the inheritance model that comes with it. This means all\nobjects are essentially key/value pairs where each key is a string, and values are either primitives,\narrays, objects, or functions. On the inheritance side, prototypes can extend other prototypes,\nbut there is no real concept of \u201csuperclass\u201d and \u201csubclass\u201d. In order to make \u201cproper\u201d Java-style\nobject-oriented code work, we had to implement classical inheritance for JavaScript in Processing.js,\nwithout making it super slow (we think we succeeded in that respect). We also had to come up\nwith a way to prevent variable names and function names from stepping on each other. Because of\nthe key/value nature of JavaScript objects, de\ufb01ning a variable called line, followed by a function\nlike line(x1,y1,x2,y2) will leave you with an object that uses whatever was declared last for a\nkey. JavaScript \ufb01rst sets object.line = \"some value\" for you, and then sets object.line =\nfunction(x1,y1,x2,y2){...}, overriding what you thought your variable line was.\nMike Kamermans\n255\n", "page": 269, "type": "text", "section": "Page 269"}
{"text": "It would have slowed down the library a lot to create separate administration for variables and\nmethods/functions, so again the documentation explains that it\u2019s a bad idea to use variables and\nfunctions with the same name. If everyone wrote \u201cproper\u201d code, this wouldn\u2019t be much of a problem,\nas you want to name variables and functions based on what they\u2019re for, or what they do, but the\nreal world does things di\ufb00erently. Sometimes your code won\u2019t work, and it\u2019s because we decided\nthat having your code break due to a naming con\ufb02ict is preferable to your code always working, but\nalways being slow. A second reason for not implementing variable and function separation was that\nthis could break JavaScript code used inside Processing sketches. Closures and the scope chain for\nJavaScript rely on the key/value nature of objects, so driving a wedge in that by writing our own\nadministration would have also severely impacted performance in terms of Just-In-Time compilation\nand compression based on functional closures.\nJava allows method overloading; JavaScript does not.\nOne of Java\u2019s more powerful features is that you can de\ufb01ne a function, let\u2019s say add(int,int),\nand then de\ufb01ne another function with the same name, but a di\ufb00erent number of arguments, e.g.\nadd(int,int,int), or with di\ufb00erent argument types, e.g. add(ComplexNumber,ComplexNumber).\nCalling add with two or three integer arguments will automatically call the appropriate function,\nand calling add with \ufb02oats or Car objects will generate an error. JavaScript, on the other hand, does\nnot support this. In JavaScript, a function is a property, and you can dereference it (in which case\nJavaScript will give you a value based on type coercion, which in this case returns true when the\nproperty points to a function de\ufb01nition, or false when it doesn\u2019t), or you can call it as a function using\nthe execution operators (which you will know as parentheses with zero or more arguments between\nthem). If you de\ufb01ne a function as add(x,y) and then call it as add(1,2,3,4,5,6), JavaScript is\nokay with that. It will set x to 1 and y to 2 and simply ignore the rest of the arguments. In order\nto make overloading work, we rewrite functions with the same name but di\ufb00erent argument count\nto a numbered function, so that function(a,b,c) in the source becomes function$3(a,b,c) in\nthe rewritten code, and function(a,b,c,d) becomes function$4(a,b,c,d), ensuring the correct\ncode paths.\nWe also mostly solved overloading of functions with the same number but di\ufb00erently typed\narguments, as long as the argument types can be seen as di\ufb00erent by JavaScript. JavaScript can\ntell the functional type of properties using the typeof operator, which will return either number,\nstring, object or function depending on what a property represents. Declaring var x = 3 followed\nby x = \u20196\u2019 will cause typeof x to report number after the initial declaration, and string after\nreassignment. As long as functions with the same argument count di\ufb00er in argument type, we rename\nthem and switch based on the result of the typeof operation. This does not work when the functions\ntake arguments of type object, so for these functions we have an additional check involving the\ninstanceof operator (which returns the name of the function that was used to create the object) to\nmake function overloading work. In fact, the only place where we cannot successfully transcompile\noverloaded functions is where the argument count is the same between functions, and the argument\ntypes are di\ufb00erent numerical types. As JavaScript only has one numerical type, declaring functions\nsuch as add(int x, int y), add(float x, float y) and add(double x, double y) will clash.\nEverything else, however, will work just \ufb01ne.\n256\nProcessing.js\n", "page": 270, "type": "text", "section": "Page 270"}
{"text": "Java allows importing compiled code.\nSometimes, plain Processing is not enough, and additional functionality is introduced in the form\nof a Processing library. These take the form of a .jarchive with compiled Java code, and o\ufb00er\nthings like networking, audio, video, hardware interfacing and other exotic functions not covered by\nProcessing itself.\nThis is a problem, because compiled Java code is Java byte code. This has given us many\nheadaches: how do we support library imports without writing a Java byte code decompiler? After\nabout a year of discussions, we settled on what may seem the simplest solution. Rather than trying\nto also cover Processing libraries, we decided to support the import keyword in sketches, and\ncreate a Processing.js Library API, so that library developers can write a JavaScript version of\ntheir library (where feasible, given the web\u2019s nature), so that if they write a package that is used\nvia import processing.video, native Processing will pick the .jarchive, and Processing.js will\ninstead pick processing.video.js, thus ensuring that things \u201cjust work\u201d. This functionality is slated for\nProcessing.js 1.4, and library imports is the last major feature that is still missing from Processing.js\n(we currently support the import keyword only in the sense that it is removed from the source code\nbefore conversion), and will be the last major step towards parity.\nWhy Pick JavaScript if It Can\u2019t Do Java?\nThis is not an unreasonable question, and it has multiple answers. The most obvious one is that\nJavaScript comes with the browser. You don\u2019t \u201cinstall\u201d JavaScript yourself, there\u2019s no plugin to\ndownload \ufb01rst; it\u2019s just there. If you want to port something to the web, you\u2019re stuck with JavaScript.\nAlthough, given the \ufb02exibility of JavaScript, \u201cstuck with\u201d is really not doing justice to how powerful\nthe language is. So, one reason to pick JavaScript is \u201cbecause it\u2019s already there\u201d. Pretty much every\ndevice that is of interest comes with a JavaScript-capable browser these days. The same cannot be\nsaid for Java, which is being o\ufb00ered less and less as a preinstalled technology, if it is available at all.\nHowever, the proper answer is that it\u2019s not really true that JavaScript \u201ccan\u2019t do\u201d the things that\nJava does; it can, it would just be slower. Even though out of the box JavaScript can\u2019t do some of the\nthings Java does, it\u2019s still a Turing-complete programming language and it can be made to emulate any\nother programming language, at the cost of speed. We could, technically, write a full Java interpreter,\nwith a String heap, separate variable and method models, class/instance object-orientation with\nrigid class hierarchies, and everything else under the Sun (or, these days, Oracle), but that\u2019s not what\nwe\u2019re in it for: Processing.js is about o\ufb00ering a Processing-to-the-web conversion, in as little code as\nis necessary for that. This means that even though we decided not to make it do certain Java things,\nour library has one huge bene\ufb01t: it can cope with embedded JavaScript really, really well.\nIn fact, during a meeting between the Processing.js and Processing people at Bocoup in Boston,\nin 2010, Ben Fry asked John Resig why he used regular expression replacement and only partial\nconversion instead of doing a proper parser and compiler. John\u2019s response was that it was important\nto him that people be able to mix Processing syntax (Java) and JavaScript without having to choose\nbetween them. That initial choice has been crucial in shaping the philosophy of Processing.js ever\nsince. We\u2019ve worked hard to keep it true in our code, and we can see a clear payo\ufb00when we look\nat all the \u201cpurely web\u201d users of Processing.js, who never used Processing, and will happily mix\nProcessing and JavaScript syntax without a problem.\nMike Kamermans\n257\n", "page": 271, "type": "text", "section": "Page 271"}
{"text": "The following example shows how JavaScript and Processing work together.\n// JavaScript (would throw an error in native Processing)\nvar cs = { x: 50,\ny: 0,\nlabel: \"my label\",\nrotate: function(theta) {\nvar nx = this.x*cos(theta) - this.y*sin(theta);\nvar ny = this.x*sin(theta) + this.y*cos(theta);\nthis.x = nx; this.y = ny; }};\n// Processing\nfloat angle = 0;\nvoid setup() {\nsize(200,200);\nstrokeWeight(15); }\nvoid draw() {\ntranslate(width/2,height/2);\nangle += PI/frameRate;\nwhile(angle>2*PI) { angle-=2*PI; }\njQuery(\u2019#log\u2019).text(angle); // JavaScript (error in native Processing)\ncs.rotate(angle);\n// legal JavaScript as well as Processing\nstroke(random(255));\npoint(cs.x, cs.y); }\nA lot of things in Java are promises: strong typing is a content promise to the compiler, visibility\nis a promise on who will call methods and reference variables, interfaces are promises that instances\ncontain the methods the interface describes, etc. Break those promises and the compiler complains.\nBut, if you don\u2019t\u2014and this is a one of the most important thoughts for Processing.js\u2014then you don\u2019t\nneed the additional code for those promises in order for a program to work. If you stick a number in\na variable, and your code treats that variable as if it has a number in it, then at the end of the day var\nvarname is just as good as int varname. Do you need typing? In Java, you do; in JavaScript, you\ndon\u2019t, so why force it in? The same goes for other code promises. If the Processing compiler doesn\u2019t\ncomplain about your code, then we can strip all the explicit syntax for your promises and it\u2019ll still\nwork the same.\nThis has made Processing.js a ridiculously useful library for data visualisation, media presentation\nand even entertainment. Sketches in native Processing work, but sketches that mix Java and JavaScript\nalso work just \ufb01ne, as do sketches that use pure JavaScript by treating Processing.js as a glori\ufb01ed\ncanvas drawing framework. In an e\ufb00ort to reach parity with native Processing, without forcing\nJava-only syntax, the project has been taken in by an audience as wide as the web itself. We\u2019ve seen\nactivity all over the web using Processing.js. Everyone from IBM to Google has built visualisations,\npresentations and even games with Processing.js\u2014Processing.js is making a di\ufb00erence.\nAnother great thing about converting Java syntax to JavaScript while leaving JavaScript untouched\nis that we\u2019ve enabled something we hadn\u2019t even thought about ourselves: Processing.js will work with\nanything that will work with JavaScript. One of the really interesting things that we\u2019re now seeing,\nfor instance, is that people are using Co\ufb00eeScript (a wonderfully simple, Ruby-like programming\nlanguage that transcompiles to JavaScript) in combination with Processing.js, with really cool results.\nEven though we set out to build \u201cProcessing for the web\u201d based on parsing Processing syntax, people\n258\nProcessing.js\n", "page": 272, "type": "text", "section": "Page 272"}
{"text": "took what we did and used it with brand new syntaxes. They could never have done that if we had\nmade Processing.js simply be a Java interpreter. By sticking with code conversion rather than writing\na code interpreter, Processing.js has given Processing a reach on the web far beyond what it would\nhave had if it had stayed Java-only, or even if it had kept a Java-only syntax, with execution on the\nweb taken care of by JavaScript. The uptake of our code not just by end users, but also by people\nwho try to integrate it with their own technologies, has been both amazing and inspiring. Clearly\nwe\u2019re doing something right, and the web seems happy with what we\u2019re doing.\nThe Result\nAs we are coming up to Processing.js 1.4.0, our work has resulted in a library that will run any\nsketch you give it, provided it does not rely on compiled Java library imports. If you can write it in\nProcessing, and it runs, you can put it on a webpage and it will just run. Due to the di\ufb00erences in\nhardware access and low level implementations of di\ufb00erent parts of the rendering pipeline there will\nbe timing di\ufb00erences, but in general a sketch that runs at 60 frames per seconds in the Processing\nIDE will run at 60 frames per second on a modern computer, with a modern browser. We have\nreached a point where bug reports have started to die down, and most work is no longer about adding\nfeature support, but more about bug \ufb01xing and code optimization.\nThanks to the e\ufb00orts of many developers working to resolve over 1800 bug reports, Processing\nsketches run using Processing.js \u201cjust work\u201d. Even sketches that rely on library imports can be made\nto work, provided that the library code is at hand. Under favourable circumstances, the library is\nwritten in a way that lets you rewrite it to pure Processing code with a few search-replace operations.\nIn this case the code can be made to work online virtually immediately. When the library does things\nthat cannot be implemented in pure Processing, but can be implemented using plain JavaScript, more\nwork is required to e\ufb00ectively emulate the library using JavaScript code, but porting is still possible.\nThe only instances of Processing code that cannot be ported are those that rely on functionality that\nis inherently unavailable to browsers, such as interfacing directly with hardware devices (such as\nwebcams or Arduino boards) or performing unattended disk writes, though even this is changing.\nBrowsers are constantly adding functionality to allow for more elaborate applications, and limiting\nfactors today may disappear a year from now, so that hopefully in the not too distant future, even\nsketches that are currently impossible to run online will become portable.\n17.3\nThe Code Components\nProcessing.js is presented and developed as a large, single \ufb01le, but architecturally it represents three\ndi\ufb00erent components: 1) the launcher, responsible for converting Processing source to Processing.js\n\ufb02avoured JavaScript and executing it, 2) static functionality that can be used by all sketches, and 3)\nsketch functionality that has to be tied to individual instances.\nThe Launcher\nThe launcher component takes care of three things: code preprocessing, code conversion, and sketch\nexecution.\nMike Kamermans\n259\n", "page": 273, "type": "text", "section": "Page 273"}
{"text": "Preprocessing\nIn the preprocessing step, Processing.js directives are split o\ufb00from the code, and acted upon. These\ndirectives come in two \ufb02avours: settings and load instructions. There is a small number of directives,\nkeeping with the \u201cit should just work\u201d philosophy, and the only settings that sketch authors can change\nare related to page interaction. By default a sketch will keep running if the page is not in focus, but the\npauseOnBlur = true directive sets up a sketch in such a way that it will halt execution when the page\nthe sketch is running on is not in focus, resuming execution when the page is in focus again. Also by\ndefault, keyboard input is only routed to a sketch when it is focussed. This is especially important\nwhen people run multiple sketches on the same page, as keyboard input intended for one sketch\nshould not be processed by another. However, this functionality can be disabled, routing keyboard\nevents to every sketch that is running on a page, using the globalKeyEvents = true directive.\nLoad instructions take the form of the aforementioned image preloading and font preloading.\nBecause images and fonts can be used by multiple sketches, they are loaded and tracked globally, so\nthat di\ufb00erent sketches don\u2019t attempt multiple loads for the same resource.\nCode Conversion\nThe code conversion component decomposes the source code into AST nodes, such as statements\nand expressions, methods, variables, classes, etc. This AST then expanded to JavaScript source code\nthat builds a sketch-equivalent program when executed. This converted source code makes heavy use\nof the Processing.js instance framework for setting up class relations, where classes in the Processing\nsource code become JavaScript prototypes with special functions for determining superclasses and\nbindings for superclass functions and variables.\nSketch Execution\nThe \ufb01nal step in the launch process is sketch execution, which consists of determining whether or\nnot all preloading has \ufb01nished, and if it has, adding the sketch to the list of running instances and\ntriggering its JavaScript onLoad event so that any sketch listeners can take the appropriate action.\nAfter this the Processing chain is run through: setup, then draw, and if the sketch is a looping sketch,\nsetting up an interval call to draw with an interval length that gets closest to the desired framerate for\nthe sketch.\nStatic Library\nMuch of Processing.js falls under the \u201cstatic library\u201d heading, representing constants, universal\nfunctions, and universal data types. A lot of these actually do double duty, being de\ufb01ned as global\nproperties, but also getting aliased by instances for quicker code paths. Global constants such as key\ncodes and color mappings are housed in the Processing object itself, set up once, and then referenced\nwhen instances are built via the Processing constructor. The same applies to self-contained helper\nfunctions, which lets us keep the code as close to \u201cwrite once, run anywhere\u201d as we can without\nsacri\ufb01cing performance.\nProcessing.js has to support a large number of complex data types, not just in order to support\nthe data types used in Processing, but also for its internal workings. These, too, are de\ufb01ned in the\nProcessing constructor:\nChar, an internal object used to overcome some of the behavioural quirks of Java\u2019s char datatype.\nPShape, which represents shape objects.\n260\nProcessing.js\n", "page": 274, "type": "text", "section": "Page 274"}
{"text": "PShapeSVG, an extension for PShape objects, which is built from and represents SVG XML.\nFor PShapeSVG, we implemented our own SVG-to-<canvas>-instructions code. Since Pro-\ncessing does not implement full SVG support, the code we saved by not relying on an external\nSVG library means that we can account for every line of code relating to SVG imports. It\nonly parses what it has to, and doesn\u2019t waste space with code that follows the spec, but is\nunused because native Processing does not support it.\nXMLElement, an XML document object.\nFor XMLElement, too, we implemented our own code, relying on the browser to \ufb01rst load the\nXML element into a Node-based structure, then traveling the node structure to build a leaner\nobject. Again, this means we don\u2019t have any dead code sitting in Processing.js, taking up\nspace and potentially causing bugs because a patch accidentally makes use of a function that\nshouldn\u2019t be there.\nPMatrix2D and PMatrix3D, which perform matrix operations in 2D and 3D mode.\nPImage, which represents an image resource.\nThis is e\ufb00ectively a wrapper of the Image object, with some additional functions and properties\nso that its API matches the Processing API.\nPFont, which represents a font resource.\nThere is no Font object de\ufb01ned for JavaScript (at least for now), so rather than actually storing\nthe font as an object, our PFont implementation loads a font via the browser, computes its\nmetrics based on how the browser renders text with it, and then caches the resultant PFont\nobject. For speed, PFonts have a reference to the canvas that was used to determine the font\nproperties, in case textWidth must be calculated, but because we track PFont objects based\non name/size pair, if a sketch uses a lot of distinct text sizes, or fonts in general, this will\nconsume too much memory. As such, PFonts will clear their cached canvas and instead call\na generic textWidth computation function when the cache grows too large. As a secondary\nmemory preservation strategy, if the font cache continues to grow after clearing the cached\ncanvas for each PFont, font caching is disabled entirely, and font changes in the sketch simply\nbuild new throwaway PFont objects for every change in font name, text size or text leading.\nDrawingShared, Drawing2D, and Drawing3D, which house all the graphics functions.\nThe DrawingShared object is actually the biggest speed trap in Processing.js. It determines\nif a sketch is launching in 2D or 3D mode, and then rebinds all graphics functions to either\nthe Drawing2D or Drawing3D object. This ensures short code path for graphics instructions,\nas 2D Processing sketches cannot used 3D functions, and vice versa. By only binding one of\nthe two sets of graphics functions, we gain speed from not having to switch on the graphics\nmode in every function to determine the code path, and we save space by not binding the\ngraphics functions that are guaranteed not to be used.\nArrayList, a container that emulates Java\u2019s ArrayList.\nHashMap, a container that emulates Java\u2019s HashMap.\nArrayList, and HashMap in particular, are special data structures because of how Java\nimplements them. These containers rely on the Java concepts of equality and hashing, and all\nobjects in Java have an equals and a hashCode method that allow them to be stored in lists\nand maps.\nFor non-hashing containers, objects are resolved based on equality rather than identity.\nThus, list.remove(myobject) iterates through the list looking for an element for which\nelement.equals(myobject), rather than element == myobject, is true. Because all objects\nmust have an equals method, we implemented a \u201cvirtual equals\u201d function on the JavaScript\nMike Kamermans\n261\n", "page": 275, "type": "text", "section": "Page 275"}
{"text": "side of things. This function takes two objects as arguments, checks whether either of them\nimplements their own equals function, and if so, falls through to that function. If they don\u2019t,\nand the passed objects are primitives, primitive equality is checked. If they\u2019re not, then there\nis no equality.\nFor hashing containers, things are even more interesting, as hashing containers act as shortcut\ntrees. The container actually wraps a variable number of lists, each tied to a speci\ufb01c hash\ncode. Objects are found based on \ufb01rst \ufb01nding the container that matches their hash code,\nin which the object is then searched for based on equality evaluation. As all objects in Java\nhave a hashCode method, we also wrote a \u201cvirtual hashcode\u201d function, which takes a single\nobject as an argument. The function checks whether the object implements its own hashCode\nfunction, and if so falls through to that function. If it doesn\u2019t, the hash code is computed\nbased on the same hashing algorithm that is used in Java.\nAdministration\nThe \ufb01nal piece of functionality in the static code library is the instance list of all sketches that are\ncurrently running on the page. This instance list stores sketches based on the canvas they have been\nloaded in, so that users can call Processing.getInstanceById(\u2019canvasid\u2019) and get a reference\nto their sketch for page interaction purposes.\nInstance Code\nInstance code takes the form of p.functor = function(arg, ...) de\ufb01nitions for the Processing\nAPI, and p.constant = ... for sketch state variables (where p is our reference to the sketch being\nset up). Neither of these are located in dedicated code blocks. Rather, the code is organized based on\nfunction, so that instance code relating to PShape operations is de\ufb01ned near the PShape object, and\ninstance code for graphics functions are de\ufb01ned near, or in, the Drawing2D and Drawing3D objects.\nIn order to keep things fast, a lot of code that could be written as static code with an instance wrap-\nper is actually implemented as purely instance code. For instance, the lerpColor(c1, c2, ratio)\nfunction, which determines the color corresponding to the linear interpolation of two colors, is\nde\ufb01ned as an instance function. Rather than having p.lerpColor(c1, c2, ratio) acting as a wrap-\nper for some static function Processing.lerpColor(c1, c2, ratio), the fact that nothing else\nin Processing.js relies on lerpColor means that code execution is faster if we write it as a pure\ninstance function. While this does \u201cbloat\u201d the instance object, most functions for which we insist on\nan instance function rather than a wrapper to the static library are small. Thus, at the expense of\nmemory we create really fast code paths. While the full Processing object will take up a one-time\nmemory slice worth around 5 MB when initially set up, the prerequisite code for individual sketches\nonly takes up about 500 KB.\n17.4\nDeveloping Processing.js\nProcessing.js is worked on intensively, which we can only do because our development approach\nsticks to a few basic rules. As these rules in\ufb02uence the architecture of Processing.js, it\u2019s worth having\na brief look at them before closing this chapter.\n262\nProcessing.js\n", "page": 276, "type": "text", "section": "Page 276"}
{"text": "Make It Work\nWriting code that works sounds like a tautological premise; you write code, and by the time you\u2019re\ndone your code either works, because that\u2019s what you set out to do, or it doesn\u2019t, and you\u2019re not done\nyet. However, \u201cmake it work\u201d comes with a corollary: Make it work, and when you\u2019re done, prove it.\nIf there is one thing above all other things that has allowed Processing.js to grow at the pace it\nhas, it is the presence of tests. Any ticket that requires touching the code, be it either by writing new\ncode or rewriting old code, cannot be marked as resolved until there is a unit or reference test that\nallows others to verify not only that the code works the way it should, but also that it breaks when it\nshould. For most code, this typically involves a unit test\u2014a short bit of code that calls a function and\nsimply tests whether the function returns the correct values, for both legal and illegal function calls.\nNot only does this allow us to test code contributions, it also lets us perform regression tests.\nBefore any code is accepted and merged into our stable development branch, the modi\ufb01ed Pro-\ncessing.js library is validated against an ever-growing battery of unit tests. Big \ufb01xes and performance\ntests in particular are prone to passing their own unit tests, but breaking parts that worked \ufb01ne before\nthe rewrite. Having tests for every function in the API, as well as internal functions, means that\nas Processing.js grows, we don\u2019t accidentally break compatibility with previous versions. Barring\ndestructive API changes, if none of the tests failed before a code contribution or modi\ufb01cation, none\nof the tests are allowed to fail with the new code in.\nThe following is an example of a unit test verifying inline object creation.\ninterface I {\nint getX();\nvoid test(); }\nI i = new I() {\nint x = 5;\npublic int getX() {\nreturn x; }\npublic void test() {\nx++; }};\ni.test();\n_checkEqual(i.getX(), 6);\n_checkEqual(i instanceof I, true);\n_checkEqual(i instanceof Object, true);\nIn addition to regular code unit tests, we also have visual reference (or \u201cref\u201d) tests. As Processing.js\nis a port of a visual programming language, some tests cannot be performed using just unit tests.\nTesting to see whether an ellipse gets drawn on the correct pixels, or whether a single-pixel-wide\nvertical line is drawn crisp or smoothed cannot be determined without a visual reference. Because all\nmainstream browsers implement the <canvas> element and Canvas2D API with subtle di\ufb00erences,\nthese things can only be tested by running code in a browser and verifying that the resulting sketch\nlooks the same as what native Processing generates. To make life easier for developers, we use an\nautomated test suite for this, where new test cases are run through Processing, generating \u201cwhat it\nshould look like\u201d data to be used for pixel comparison. This data is then stored as a comment inside\nthe sketch that generated it, forming a test, and these tests are then run by Processing.js on a visual\nreference test page which executes each test and performs pixel comparisons between \u201cwhat it should\nMike Kamermans\n263\n", "page": 277, "type": "text", "section": "Page 277"}
{"text": "look like\u201d and \u201cwhat it looks like\u201d. If the pixels are o\ufb00, the test fails, and the developer is presented\nwith three images: what it should look like, how Processing.js rendered it, and the di\ufb00erence between\nthe two, marking problem areas as red pixels, and correct areas as white. Much like unit tests, these\ntests must pass before any code contribution can be accepted.\nMake It Fast\nIn an open source project, making things work is only the \ufb01rst step in the life of a function. Once\nthings work, you want to make sure things work fast. Based on the \u201cif you can\u2019t measure it, you can\u2019t\nimprove it\u201d principle, most functions in Processing.js don\u2019t just come with unit or ref tests, but also\nwith performance (or \u201cperf\u201d) tests. Small bits of code that simply call a function, without testing the\ncorrectness of the function, are run several hundred times in a row, and their run time is recorded on\na special performance test web page. This lets us quantify how well (or not!) Processing.js performs\nin browsers that support HTML5\u2019s <canvas> element. Every time an optimization patch passes\nunit and ref testing, it is run through our performance test page. JavaScript is a curious beast, and\nbeautiful code can, in fact, run several orders of magnitude slower than code that contains the same\nlines several times over, with inline code rather than function calls. This makes performance testing\ncrucial. We have been able to speed up certain parts of the library by three orders of magnitude\nsimply by discovering hot loops during perf testing, reducing the number of function calls by inlining\ncode, and by making functions return the moment they know what their return value should be, rather\nthan having only a single return at the very end of the function.\nAnother way in which we try to make Processing.js fast is by looking at what runs it. As\nProcessing.js is highly dependent on the e\ufb03ciency of JavaScript engines, it makes sense to also look\nat which features various engines o\ufb00er to speed things up. Especially now that browsers are starting to\nsupport hardware accelerated graphics, instant speed boosts are possible when engines o\ufb00er new and\nmore e\ufb03cient data types and functions to perform the low level operations that Processing.js depends\non. For instance, JavaScript technically has no static typing, but graphics hardware programming\nenvironments do. By exposing the data structures used to talk to the hardware directly to JavaScript,\nit is possible to signi\ufb01cantly speed up sections of code if we know that they will only use speci\ufb01c\nvalues.\nMake It Small\nThere are two ways to make code small. First, write compact code. If you\u2019re manipulating a variable\nmultiple times, compact it to a single manipulation (if possible). If you access an object variable\nmultiple times, cache it. If you call a function multiple times, cache the result. Return once you\nhave all the information you need, and generally apply all the tricks a code optimiser would apply\nyourself. JavaScript is a particularly nice language for this, since it comes with an incredible amount\nof \ufb02exibility. For example, rather than using:\nif ((result = functionresult)!==null) {\nvar = result;\n} else {\nvar = default;\n}\nin JavaScript this becomes:\nvar = functionresult || default\n264\nProcessing.js\n", "page": 278, "type": "text", "section": "Page 278"}
{"text": "There is also another form of small code, and that\u2019s in terms of runtime code. Because JavaScript\nlets you change function bindings on the \ufb02y, running code becomes much smaller if you can say\n\u201cbind the function for line2D to the function call for line\u201d once you know that a program runs in 2D\nrather than 3D mode, so that you don\u2019t have to perform:\nif(mode==2D) { line2D() } else { line3D() }\nfor every function call that might be either in 2D or 3D mode.\nFinally, there is the process of mini\ufb01cation. There are a number of good systems that let you\ncompress your JavaScript code by renaming variables, stripping whitespace, and applying certain\ncode optimisations that are hard to do by hand while still keeping the code readable. Examples of\nthese are the YUI mini\ufb01er and Google\u2019s closure compiler. We use these technologies in Processing.js\nto o\ufb00er end users bandwidth convenience\u2014mini\ufb01cation after stripping comments can shrink the\nlibrary by as much as 50%, and taking advantage of modern browser/server interaction for gzipped\ncontent, we can o\ufb00er the entire Processing.js library in gzipped form in 65 KB.\nIf All Else Fails, Tell People\nNot everything that can currently be done in Processing can be done in the browser. Security models\nprevent certain things like saving \ufb01les to the hard disk and performing USB or serial port I/O, and\na lack of typing in JavaScript can have unexpected consequences (such as all math being \ufb02oating\npoint math). Sometimes we\u2019re faced with the choice between adding an incredible amount of code\nto enable an edge case, or mark the ticket as a \u201cwont\ufb01x\u201d issue. In such cases, a new ticket gets \ufb01led,\ntypically titled \u201cAdd documentation that explains why...\u201d.\nIn order to make sure these things aren\u2019t lost, we have documentation for people who start\nusing Processing.js with a Processing background, and for people who start using Processing.js\nwith a JavaScript background, covering the di\ufb00erences between what is expected, and what actually\nhappens. Certain things just deserve special mention, because no matter how much work we put\ninto Processing.js, there are certain things we cannot add without sacri\ufb01cing usability. A good\narchitecture doesn\u2019t just cover the way things are, it also covers why; without that, you\u2019ll just end up\nhaving the same discussions about what the code looks like and whether it should be di\ufb00erent every\ntime the team changes.\n17.5\nLessons Learned\nThe most important lesson we learned while writing Processing.js is that when porting a language,\nwhat matters is that the result is correct, not whether or not the code used in your port is similar to\nthe original. Even though Java and JavaScript syntax are fairly similar, and modifying Java code\nto legal JavaScript code is fairly easy, it often pays to look at what JavaScript can natively do and\nexploit that to get the same functional result. Taking advantage of the lack of typing by recycling\nvariables, using certain built-in functions that are fast in JavaScript but slow in Java, or avoiding\npatterns that are fast in Java but slow in JavaScript means your code may look radically di\ufb00erent, but\nhas the exact same e\ufb00ect. You often hear people say not to reinvent the wheel, but that only applies\nto working with a single programming language. When you\u2019re porting, reinvent as many wheels as\nyou need to obtain the performance you require.\nAnother important lesson is to return early, return often, and branch as little as possible. An if/then\nstatement followed by a return can be made (sometimes drastically) faster by using an if-return/return\nMike Kamermans\n265\n", "page": 279, "type": "text", "section": "Page 279"}
{"text": "construction instead, using the return statement as a conditional shortcut. While it\u2019s conceptually\npretty to aggregate your entire function state before calling the ultimate return statement for that\nfunction, it also means your code path may traverse code that is entirely unrelated to what you will\nbe returning. Don\u2019t waste cycles; return when you have all the information you need.\nA third lesson concerns testing your code. In Processing.js we had the bene\ufb01t of starting with\nvery good documentation outlining how Processing was \u201csupposed\u201d to work, and a large set of test\ncases, most of which started out as \u201cknown fail\u201d. This allowed us to do two things: 1) write code\nagainst tests, and 2) create tests before writing code. The usual process, in which code is written and\nthen test cases are written for that code, actually creates biased tests. Rather than testing whether or\nnot your code does what it should do, according to the speci\ufb01cation, you are only testing whether\nyour code is bug-free. In Processing.js, we instead start by creating test cases based on what the\nfunctional requirements for some function or set of functions is, based on the documentation for it.\nWith these unbiased tests, we can then write code that is functionally complete, rather than simply\nbug-free but possibly de\ufb01cient.\nThe last lesson is also the most general one: apply the rules of agile development to individual\n\ufb01xes as well. No one bene\ufb01ts from you retreating into dev mode and not being heard from for three\ndays straight while you write the perfect solution. Rather, get your solutions to the point where they\nwork, and not even necessarily for all test cases, then ask for feedback. Working alone, with a test\nsuite for catching errors, is no guarantee of good or complete code. No amount of automated testing\nis going to point out that you forgot to write tests for certain edge cases, or that there is a better\nalgorithm than the one you picked, or that you could have reordered your statements to make the\ncode better suited for JIT compilation. Treat \ufb01xes like releases: present \ufb01xes early, update often, and\nwork feedback into your improvements.\n266\nProcessing.js\n", "page": 280, "type": "text", "section": "Page 280"}
{"text": "[chapter18]\nPuppet\nLuke Kanies\n18.1\nIntroduction\nPuppet is an open source IT management tool written in Ruby, used for datacenter automation\nand server management at Google, Twitter, the New York Stock Exchange, and many others. It is\nprimarily maintained by Puppet Labs, which also founded the project. Puppet can manage as few as\n2 machines and as many as 50,000, on teams with one system administrator or hundreds.\nPuppet is a tool for con\ufb01guring and maintaining your computers; in its simple con\ufb01guration\nlanguage, you explain to Puppet how you want your machines con\ufb01gured, and it changes them as\nneeded to match your speci\ufb01cation. As you change that speci\ufb01cation over time\u2014such as with package\nupdates, new users, or con\ufb01guration updates\u2014Puppet will automatically update your machines to\nmatch. If they are already con\ufb01gured as desired, then Puppet does nothing.\nIn general, Puppet does everything it can to use existing system features to do its work; e.g., on\nRed Hat it will use yum for packages and init.d for services, but on OS X it will use dmg for packages\nand launchd for services. One of the guiding goals in Puppet is to have the work it does make sense\nwhether you are looking at Puppet code or the system itself, so following system standards is critical.\nPuppet comes from multiple traditions of other tools. In the open source world, it is most\nin\ufb02uenced by CFEngine, which was the \ufb01rst open source general-purpose con\ufb01guration tool, and\nISconf, whose use of make for all work inspired the focus on explicit dependencies throughout the\nsystem. In the commercial world, Puppet is a response to BladeLogic and Opsware (both since\nacquired by larger companies), each of which was successful in the market when Puppet was begun,\nbut each of which was focused on selling to executives at large companies rather than building great\ntools directly for system administrators. Puppet is meant to solve similar problems to these tools, but\nit is focused on a very di\ufb00erent user.\nFor a simple example of how to use Puppet, here is a snippet of code that will make sure the\nsecure shell service (SSH) is installed and con\ufb01gured properly:\nclass ssh {\npackage { ssh: ensure => installed }\nfile { \"/etc/ssh/sshd_config\":\nsource => \u2019puppet:///modules/ssh/sshd_config\u2019,\nensure => present,\nrequire => Package[ssh]\n}\nservice { sshd:\n", "page": 281, "type": "text", "section": "Page 281"}
{"text": "ensure => running,\nrequire => [File[\"/etc/ssh/sshd_config\"], Package[ssh]]\n}\n}\nThis makes sure the package is installed, the \ufb01le is in place, and the service is running. Note that\nwe\u2019ve speci\ufb01ed dependencies between the resources, so that we always perform any work in the right\norder. This class could then be associated with any host to apply this con\ufb01guration to it. Notice that\nthe building blocks of a Puppet con\ufb01guration are structured objects, in this case package, file, and\nservice. We call these objects resources in Puppet, and everything in a Puppet con\ufb01guration comes\ndown to these resources and the dependencies between them.\nA normal Puppet site will have tens or even hundreds of these code snippets, which we call\nclasses; we store these classes on disk in \ufb01les called manifests, and collect them in related groups\ncalled modules. For instance, you might have an ssh module with this ssh class plus any other\nrelated classes, along with modules for mysql, apache, and sudo.\nMost Puppet interactions are via the command line or long-running HTTP services, but there\nare graphical interfaces for some things such as report processing. Puppet Labs also produces\ncommercial products around Puppet, which tend more toward graphical web-based interfaces.\nPuppet\u2019s \ufb01rst prototype was written in the summer of 2004, and it was turned into a full-time\nfocus in February of 2005. It was initially designed and written by Luke Kanies, a sysadmin who\nhad a lot of experience writing small tools, but none writing tools greater than 10,000 lines of code.\nIn essence, Luke learned to be a programmer while writing Puppet, and that shows in its architecture\nin both positive and negative ways.\nPuppet was \ufb01rst and foremost built to be a tool for sysadmins, to make their lives easier and\nallow them to work faster, more e\ufb03ciently, and with fewer errors. The \ufb01rst key innovation meant to\ndeliver on this was the resources mentioned above, which are Puppet\u2019s primitives; they would both\nbe portable across most operating systems and also abstract away implementation detail, allowing the\nuser to focus on outcomes rather than how to achieve them. This set of primitives was implemented\nin Puppet\u2019s Resource Abstraction Layer.\nPuppet resources must be unique on a given host. You can only have one package named \u201cssh\u201d,\none service named \u201csshd\u201d, and one \ufb01le named \u201c/etc/ssh/sshd_con\ufb01g\u201d. This prevents di\ufb00erent parts of\nyour con\ufb01gurations from con\ufb02icting with each other, and you \ufb01nd out about those con\ufb02icts very early\nin the con\ufb01guration process. We refer to these resources by their type and title; e.g., Package[ssh]\nand Service[sshd]. You can have a package and a service with the same name because they are\ndi\ufb00erent types, but not two packages or services with the same name.\nThe second key innovation in Puppet provides the ability to directly specify dependencies between\nresources. Previous tools focused on the individual work to be done, rather than how the various bits\nof work were related; Puppet was the \ufb01rst tool to explicitly say that dependencies are a \ufb01rst-class\npart of your con\ufb01gurations and must be modeled that way. It builds a graph of resources and their\ndependencies as one of the core data types, and essentially everything in Puppet hangs o\ufb00of this\ngraph (called a Catalog) and its vertices and edges.\nThe last major component in Puppet is its con\ufb01guration language. This language is declarative,\nand is meant to be more con\ufb01guration data than full programming\u2014it most resembles Nagios\u2019s\ncon\ufb01guration format, but is also heavily in\ufb02uenced by CFEngine and Ruby.\nBeyond the functional components, Puppet has had two guiding principles throughout its develop-\nment: it should be as simple as possible, always preferring usability even at the expense of capability;\nand it should be built as a framework \ufb01rst and application second, so that others could build their\nown applications on Puppet\u2019s internals as desired. It was understood that Puppet\u2019s framework needed\n268\nPuppet\n", "page": 282, "type": "text", "section": "Page 282"}
{"text": "a killer application to be adopted widely, but the framework was always the focus, not the application.\nMost people think of Puppet as being that application, rather than the framework behind it.\nWhen Puppet\u2019s prototype was \ufb01rst built, Luke was essentially a decent Perl programmer with a\nlot of shell experience and some C experience, mostly working in CFEngine. The odd thing is he\nhad experience building parsers for simple languages, having built two as part of smaller tools and\nalso having rewritten CFEngine\u2019s parser from scratch in an e\ufb00ort to make it more maintainable (this\ncode was never submitted to the project, because of small incompatibilities).\nA dynamic language was easily decided on for Puppet\u2019s implementation, based on much higher\ndeveloper productivity and time to market, but choosing the language proved di\ufb03cult. Initial\nprototypes in Perl went nowhere, so other languages were sought for experimentation. Python was\ntried, but Luke found the language quite at odds with how he thought about the world. Based on\nwhat amounted to a rumor of utility heard from a friend, Luke tried Ruby, and in four hours had built\na usable prototype. When Puppet became a full-time e\ufb00ort in 2005 Ruby was a complete unknown,\nso the decision to stick with it was a big risk, but again programmer productivity was deemed the\nprimary driver in language choice. The major distinguishing feature in Ruby, at least as opposed to\nPerl, was how easy it was to build non-hierarchical class relationships, but it also mapped very well\nto Luke\u2019s brain, which turned out to be critical.\n18.2\nArchitectural Overview\nThis chapter is primarily about the architecture of Puppet\u2019s implementation (that is, the code that\nwe\u2019ve used to make Puppet do the things it\u2019s supposed to do) but it\u2019s worth brie\ufb02y discussing its\napplication architecture (that is, how the parts communicate), so that the implementation makes\nsome sense.\nPuppet has been built with two modes in mind: A client/server mode with a central server\nand agents running on separate hosts, or a serverless mode where a single process does all of the\nwork. To ensure consistency between these modes, Puppet has always had network transparency\ninternally, so that the two modes used the same code paths whether they went over the network or\nnot. Each executable can con\ufb01gure local or remote service access as appropriate, but otherwise they\nbehave identically. Note also that you can use the serverless mode in what amounts to a client/server\ncon\ufb01guration, by pulling all con\ufb01guration \ufb01les to each client and having it parse them directly.\nThis section will focus on the client/server mode, because it\u2019s more easily understood as separate\ncomponents, but keep in mind that this is all true of the serverless mode, too.\nOne of the de\ufb01ning choices in Puppet\u2019s application architecture is that clients should not get\naccess to raw Puppet modules; instead, they get a con\ufb01guration compiled just for them. This provides\nmultiple bene\ufb01ts: First, you follow the principle of least privilege, in that each host only knows exactly\nwhat it needs to know (how it should be con\ufb01gured), but it does not know how any other servers are\ncon\ufb01gured. Second, you can completely separate the rights needed to compile a con\ufb01guration (which\nmight include access to central data stores) from the need to apply that con\ufb01guration. Third, you can\nrun hosts in a disconnected mode where they repeatedly apply a con\ufb01guration with no contact to\na central server, which means you remain in compliance even if the server is down or the client is\ndisconnected (such as would be the case in a mobile installation, or when the clients are in a DMZ).\nGiven this choice, the work\ufb02ow becomes relatively straightforward:\n1. The Puppet agent process collects information about the host it is running on, which it passes\nto the server.\nLuke Kanies\n269\n", "page": 283, "type": "text", "section": "Page 283"}
{"text": "2. The parser uses that system information and Puppet modules on local disk to compile a\ncon\ufb01guration for that particular host and returns it to the agent.\n3. The agent applies that con\ufb01guration locally, thus a\ufb00ecting the local state of the host, and \ufb01les\nthe resulting report with the server.\nFigure 18.1: Puppet data\ufb02ow\nThus, the agent has access to its own system information, its con\ufb01guration, and each report it\ngenerates. The server has copies of all of this data, plus access to all of the Puppet modules, and any\nback-end databases and services that might be needed to compile the con\ufb01guration.\nBeyond the components that go into this work\ufb02ow, which we\u2019ll address next, there are many data\ntypes that Puppet uses for internal communication. These data types are critical, because they\u2019re how\nall communication is done and they\u2019re public types which any other tools can consume or produce.\nThe most important data types are:\nFacts: System data collected on each machine and used to compile con\ufb01gurations.\nManifest: Files containing Puppet code, generally organized into collections called \u201cmodules\u201d.\nCatalog: A graph of a given host\u2019s resources to be managed and the dependencies between them.\nReport: The collection of all events generated during application of a given Catalog.\nBeyond Facts, Manifests, Catalogs, and Reports, Puppet supports data types for \ufb01les, certi\ufb01cates\n(which it uses for authentication), and others.\n270\nPuppet\n", "page": 284, "type": "text", "section": "Page 284"}
{"text": "Agent\npuppet master\nConnect\nPlugins\nFacter\n(facts)\nENC?\npuppet console\n(or other data source)\nFinish\nReport?\n\ufb01le?\nDatabase\nclasses:\n- ssh:\n  ssh_key: AAAA\u2026\nparameters:\nconf: LISA11\n./enc.script $certname\nnode $certname {\n  $conf = LISA11\n  class { 'ssh':\n    ssh_key => 'AAA.',\n  }\n}\nCommon\nProd\nDev\nCompile Catalog\n(functions)\nApply Catalog\npuppet \ufb01leserver\npuppet report\npuppet:///server/\nReport Procesor\nFigure 18.2: Orchestration of data \ufb02ow between Puppet processes and components\n18.3\nComponent Analysis\nAgent\nThe \ufb01rst component encountered in a Puppet run is the agent process. This was traditionally a\nseparate executable called puppetd, but in version 2.6 we reduced down to one executable so now it\nis invoked with puppet agent, akin to how Git works. The agent has little functionality of its own;\nit is primarily con\ufb01guration and code that implements the client-side aspects of the above-described\nwork\ufb02ow.\nFacter\nThe next component after the agent is an external tool called Facter, which is a very simple tool used\nto discover information about the host it is running on. This is data like the operating system, IP\naddress, and host name, but Facter is easily extensible so many organizations add their own plugins\nto discover custom data. The agent sends the data discovered by Facter to the server, at which point\nit takes over the work\ufb02ow.\nExternal Node Classi\ufb01er\nOn the server, the \ufb01rst component encountered is what we call the External Node Classi\ufb01er, or\nENC. The ENC accepts the host name and returns a simple data structure containing the high-level\ncon\ufb01guration for that host. The ENC is generally a separate service or application: either another\nopen source project, such as Puppet Dashboard or Foreman, or integration with existing data stores,\nsuch as LDAP. The purpose of the ENC is to specify what functional classes a given host belongs to,\nLuke Kanies\n271\n", "page": 285, "type": "text", "section": "Page 285"}
{"text": "and what parameters should be used to con\ufb01gure those classes. For example, a given host might be\nin the debian and webserver classes, and have the parameter datacenter set to atlanta.\nNote that as of Puppet 2.7, the ENC is not a required component; users can instead directly\nspecify node con\ufb01gurations in Puppet code. Support for an ENC was added about 2 years after\nPuppet was launched because we realized that classifying hosts is fundamentally di\ufb00erent than\ncon\ufb01guring them, and it made more sense to split these problems into separate tools than to extend\nthe language to support both facilities. The ENC is always recommended, and at some point soon\nwill become a required component (at which point Puppet will ship with a su\ufb03ciently useful one\nthat that requirement will not be a burden).\nOnce the server receives classi\ufb01cation information from the ENC and system information from\nFacter (via the agent), it bundles all of the information into a Node object and passes it on to the\nCompiler.\nCompiler\nAs mentioned above, Puppet has a custom language built for specifying system con\ufb01gurations. Its\ncompiler is really three chunks: A Yacc-style parser generator and a custom lexer; a group of classes\nused to create our Abstract Syntax Tree (AST); and the Compiler class that handles the interactions\nof all of these classes and also functions as the API to this part of the system.\nThe most complicated thing about the compiler is the fact that most Puppet con\ufb01guration code\nis lazily loaded on \ufb01rst reference (to reduce both load times and irrelevant logging about missing-\nbut-unneeded dependencies), which means there aren\u2019t really explicit calls to load and parse the\ncode.\nPuppet\u2019s parser uses a normal Yacc1-style parser generator (built using the open source Racc2\ntool). Unfortunately, there were no open source lexer generators when Puppet was begun, so it uses a\ncustom lexer.\nBecause we use an AST in Puppet, every statement in the Puppet grammar evaluates to an instance\nof a Puppet AST class (e.g., Puppet::Parser::AST::Statement), rather than taking action directly,\nand these AST instances are collected into a tree as the grammar tree is reduced. This AST provides\na performance bene\ufb01t when a single server is compiling con\ufb01gurations for many di\ufb00erent nodes,\nbecause we can parse once but compile many times. It also gives us the opportunity to perform some\nintrospection of the AST, which provides us information and capability we wouldn\u2019t have if parsing\noperated directly.\nVery few approachable AST examples were available when Puppet was begun, so there has been\na lot of evolution in it, and we\u2019ve arrived at what seems a relatively unique formulation. Rather than\ncreating a single AST for the entire con\ufb01guration, we create many small ASTs, keyed o\ufb00their name.\nFor instance, this code:\nclass ssh {\npackage { ssh: ensure => present }\n}\ncreates a new AST containing a single Puppet::Parser::AST::Resource instance, and stores that\nAST by the name \u201cssh\u201d in the hash of all classes for this particular environment. (I\u2019ve left out details\nabout other constructs akin to classes, but they are unnecessary for this discussion.)\n1http://dinosaur.compilertools.net/\n2https://github.com/tenderlove/racc\n272\nPuppet\n", "page": 286, "type": "text", "section": "Page 286"}
{"text": "Given the AST and a Node object (from the ENC), the compiler takes the classes speci\ufb01ed in the\nnode object (if there are any), looks them up and evaluates them. In the course of this evaluation, the\ncompiler is building up a tree of variable scopes; every class gets its own scope which is attached to\nthe creating scope. This amounts to dynamic scoping in Puppet: if one class includes another class,\nthen the included class can look up variables directly in the including class. This has always been a\nnightmare, and we have been on the path to getting rid of this capability.\nThe Scope tree is temporary and is discarded once compiling is done, but the artifact of compiling\nis also built up gradually over the course of the compilation. We call this artifact a Catalog, but it\nis just a graph of resources and their relationships. Nothing of the variables, control structures, or\nfunction calls survive into the catalog; it\u2019s plain data, and can be trivially converted to JSON, YAML,\nor just about anything else.\nDuring compilation, we create containment relationships; a class \u201ccontains\u201d all of the resources\nthat come with that class (e.g., the ssh package above is contained by the ssh class). A class might\ncontain a de\ufb01nition, which itself contains either yet more de\ufb01nitions, or individual resources. A\ncatalog tends to be a very horizontal, disconnected graph: many classes, each no more than a couple\nof levels deep.\nOne of the awkward aspects of this graph is that it also contains \u201cdependency\u201d relationships, such\nas a service requiring a package (maybe because the package installation actually creates the service),\nbut these dependency relationships are actually speci\ufb01ed as parameter values on the resources, rather\nthan as edges in the structure of the graph. Our graph class (called SimpleGraph, for historical\nreasons) does not support having both containment and dependency edges in the same graph, so we\nhave to convert between them for various purposes.\nTransaction\nOnce the catalog is entirely constructed (assuming there is no failure), it is passed on to the Transaction.\nIn a system with a separate client and server, the Transaction runs on the client, which pulls the\nCatalog down via HTTP as in Figure 18.2.\nPuppet\u2019s transaction class provides the framework for actually a\ufb00ecting the system, whereas\neverything else we\u2019ve discussed just builds up and passes around objects. Unlike transactions in\nmore common systems such as databases, Puppet transactions do not have behaviors like atomicity.\nThe transaction performs a relatively straightforward task: walk the graph in the order speci\ufb01ed\nby the various relationships, and make sure each resource is in sync. As mentioned above, it\nhas to convert the graph from containment edges (e.g., Class[ssh] contains Package[ssh] and\nService[sshd]) to dependency edges (e.g., Service[sshd] depends on Package[ssh]), and then\nit does a standard topological sort of the graph, selecting each resource in turn.\nFor a given resource, we perform a simple three-step process: retrieve the current state of that\nresource, compare it to the desired state, and make any changes necessary to \ufb01x discrepancies. For\ninstance, given this code:\nfile { \"/etc/motd\":\nensure => file,\ncontent => \"Welcome to the machine\",\nmode => 644\n}\nthe transaction checks the content and mode of /etc/motd, and if they don\u2019t match the speci\ufb01ed state,\nit will \ufb01x either or both of them. If /etc/motd is somehow a directory, then it will back up all of the\n\ufb01les in that directory, remove it, and replace it with a \ufb01le that has the appropriate content and mode.\nLuke Kanies\n273\n", "page": 287, "type": "text", "section": "Page 287"}
{"text": "This process of making changes is actually handled by a simple ResourceHarness class that de\ufb01nes\nthe entire interface between Transaction and Resource. This reduces the number of connections\nbetween the classes, and makes it easier to make changes to either independently.\nResource Abstraction Layer\nThe Transaction class is the heart of getting work done with Puppet, but all of the work is actually\ndone by the Resource Abstraction Layer (RAL), which also happens to be the most interesting\ncomponent in Puppet, architecturally speaking.\nThe RAL was the \ufb01rst component created in Puppet and, other than the language, it most clearly\nde\ufb01nes what the user can do. The job of the RAL is to de\ufb01ne what it means to be a resource and how\nresources can get work done on the system, and Puppet\u2019s language is speci\ufb01cally built to specify\nresources as modeled by the RAL. Because of this, it\u2019s also the most important component in the\nsystem, and the hardest to change. There are plenty of things we would like to \ufb01x in the RAL, and\nwe\u2019ve made a lot of critical improvements to it over the years (the most crucial being the addition of\nProviders), but there is still a lot of work to do to the RAL in the long term.\nIn the Compiler subsystem, we model resources and resource types with separate classes (named,\nconveniently, Puppet::Resource and Puppet::Resource::Type). Our goal is to have these classes\nalso form the heart of the RAL, but for now these two behaviors (resource and type) are modeled\nwithin a single class, Puppet::Type. (The class is named poorly because it signi\ufb01cantly predates\nour use of the term Resource, and at the time we were directly serializing memory structures when\ncommunicating between hosts, so it was actually quite complicated to change class names.)\nWhen Puppet::Type was \ufb01rst created, it seemed reasonable to put resource and resource type\nbehaviors in the same class; after all, resources are just instances of resource types. Over time,\nhowever, it became clear that the relationship between a resource and its resource type aren\u2019t modeled\nwell in a traditional inheritance structure. Resource types de\ufb01ne what parameters a resource can\nhave, but not whether it accepts parameters (they all do), for instance. Thus, our base class of\nPuppet::Type has class-level behaviors that determine how resource types behave, and instance-\nlevel behaviors that determine how resource instances behave. It additionally has the responsibility\nof managing registration and retrieval of resource types; if you want the \u201cuser\u201d type, you call\nPuppet::Type.type(:user).\nThis mix of behaviors makes Puppet::Type quite di\ufb03cult to maintain. The whole class is less\nthan 2,000 lines of code, but working at three levels\u2014resource, resource type, and resource type\nmanager\u2014makes it convoluted. This is obviously why it\u2019s a major target for being refactored, but\nit\u2019s more plumbing than user-facing, so it\u2019s always been hard to justify e\ufb00ort here rather than directly\nin features.\nBeyond Puppet::Type, there are two major kinds of classes in the RAL, the most interesting of\nwhich are what we call Providers. When the RAL was \ufb01rst developed, each resource type mixed\nthe de\ufb01nition of a parameter with code that knew how to manage it. For instance, we would de\ufb01ne\nthe \u201ccontent\u201d parameter, and then provide a method that could read the content of a \ufb01le, and another\nmethod that could change the content:\nPuppet::Type.newtype(:file) do\n...\nnewproperty(:content) do\ndef retrieve\nFile.read(@resource[:name])\nend\n274\nPuppet\n", "page": 288, "type": "text", "section": "Page 288"}
{"text": "def sync\nFile.open(@resource[:name], \"w\") { |f| f.print @resource[:content] }\nend\nend\nend\nThis example is simpli\ufb01ed considerably (e.g., we use checksums internally, rather than the full\ncontent strings), but you get the idea.\nThis became impossible to manage as we needed to support multiple varieties of a given resource\ntype. Puppet now supports more than 30 kinds of package management, and it would have been\nimpossible to support all of those within a single Package resource type. Instead, we provide a clean\ninterface between the de\ufb01nition of the resource type\u2014essentially, what the name of the resource\ntype is and what properties it supports\u2014from how you manage that type of resource. Providers\nde\ufb01ne getter and setter methods for all of a resource type\u2019s properties, named in obvious ways. For\nexample, this is how a provider of the above property would look:\nPuppet::Type.newtype(:file) do\nnewproperty(:content)\nend\nPuppet::Type.type(:file).provide(:posix) do\ndef content\nFile.read(@resource[:name])\nend\ndef content=(str)\nFile.open(@resource[:name], \"w\") { |f| f.print(str) }\nend\nend\nThis is a touch more code in the simplest cases, but is much easier to understand and maintain,\nespecially as either the number of properties or number of providers increases.\nI said at the beginning of this section that the Transaction doesn\u2019t actually a\ufb00ect the system\ndirectly, and it instead relies on the RAL for that. Now it\u2019s clear that it\u2019s the providers that do the\nactual work. In fact, in general the providers are the only part of Puppet that actually touch the\nsystem. The transaction asks for a \ufb01le\u2019s content, and the provider collects it; the transaction speci\ufb01es\nthat a \ufb01le\u2019s content should be changed, and the provider changes it. Note, however, that the provider\nnever decides to a\ufb00ect the system\u2014the Transaction owns the decisions, and the provider does the\nwork. This gives the Transaction complete control without requiring that it understand anything\nabout \ufb01les, users, or packages, and this separation is what enables Puppet to have a full simulation\nmode where we can largely guarantee the system won\u2019t be a\ufb00ected.\nThe second major class type in the RAL is responsible for the parameters themselves. We actually\nsupport three kinds of parameters: metaparameters, which a\ufb00ect all resource types (e.g., whether\nyou should run in simulation mode); parameters, which are values that aren\u2019t re\ufb02ected on disk (e.g.,\nwhether you should follow links in \ufb01les); and properties, which model aspects of the resource that\nyou can change on disk (e.g., a \ufb01le\u2019s content, or whether a service is running). The di\ufb00erence between\nproperties and parameters is especially confusing to people, but if you just think of properties as\nhaving getter and setter methods in the providers, it\u2019s relatively straightforward.\nLuke Kanies\n275\n", "page": 289, "type": "text", "section": "Page 289"}
{"text": "Reporting\nAs the transaction walks the graph and uses the RAL to change the system\u2019s con\ufb01guration, it\nprogressively builds a report. This report largely consists of the events generated by changes to the\nsystem. These events, in turn, are comprehensive re\ufb02ections of what work was done: they retain\na timestamp the resource changed, the previous value, the new value, any message generated, and\nwhether the change succeeded or failed (or was in simulation mode).\nThe events are wrapped in a ResourceStatus object that maps to each resource. Thus, for a given\nTransaction, you know all of the resources that are run, and you know any changes that happen, along\nwith all of the metadata you might need about those changes.\nOnce the transaction is complete, some basic metrics are calculated and stored in the report, and\nthen it is sent o\ufb00to the server (if con\ufb01gured). With the report sent, the con\ufb01guration process is\ncomplete, and the agent goes back to sleep or the process just ends.\n18.4\nInfrastructure\nNow that we have a thorough understanding of what Puppet does and how, it\u2019s worth spending a\nlittle time on the pieces that don\u2019t show up as capabilities but are still critical to getting the job done.\nPlugins\nOne of the great things about Puppet is that it is very extensible. There are at least 12 di\ufb00erent\nkinds of extensibility in Puppet, and most of these are meant to be usable by just about anyone. For\nexample, you can create custom plugins for these areas:\n\u2022 resource types and custom providers\n\u2022 report handlers, such as for storing reports in a custom database\n\u2022 Indirector plugins for interacting with existing data stores\n\u2022 facts for discovering extra information about your hosts\nHowever, Puppet\u2019s distributed nature means that agents need a way to retrieve and load new\nplugins. Thus, at the start of every Puppet run, the \ufb01rst thing we do is download all plugins that the\nserver has available. These might include new resource types or providers, new facts, or even new\nreport processors.\nThis makes it possible to heavily upgrade Puppet agents without ever changing the core Puppet\npackages. This is especially useful for highly customized Puppet installations.\nIndirector\nYou\u2019ve probably detected by now that we have a tradition of bad class names in Puppet, and according\nto most people, this one takes the cake. The Indirector is a relatively standard Inversion of Control\nframework with signi\ufb01cant extensibility. Inversion of Control systems allow you to separate develop-\nment of functionality from how you control which functionality you use. In Puppet\u2019s case, this allows\nus to have many plugins that provide very di\ufb00erent functionality, such as reaching the compiler via\nHTTP or loading it in-process, and switch between them with a small con\ufb01guration change rather\nthan a code change. In other words, Puppet\u2019s Indirector is basically an implementation of a service\nlocator, as described on the Wikipedia page for \u201cInversion of Control\u201d. All of the hand-o\ufb00s from\none class to another go through the Indirector, via a standard REST-like interface (e.g., we support\n276\nPuppet\n", "page": 290, "type": "text", "section": "Page 290"}
{"text": "\ufb01nd, search, save, and destroy as methods), and switching Puppet from serverless to client/server is\nlargely a question of con\ufb01guring the agent to use an HTTP endpoint for retrieving catalogs, rather\nthan using a compiler endpoint.\nBecause it is an Inversion of Control framework where con\ufb01guration is stringently separated\nfrom the code paths, this class can also be di\ufb03cult to understand, especially when you\u2019re debugging\nwhy a given code path was used.\nNetworking\nPuppet\u2019s prototype was written in the summer of 2004, when the big networking question was\nwhether to use XMLRPC or SOAP. We chose XMLRPC, and it worked \ufb01ne but had most of the\nproblems everyone else had: it didn\u2019t encourage standard interfaces between components, and it\ntended to get overcomplicated very quickly as a result. We also had signi\ufb01cant memory problems,\nbecause the encoding needed for XMLRPC resulted in every object appearing at least twice in\nmemory, which quickly gets expensive for large \ufb01les.\nFor our 0.25 release (begun in 2008), we began the process of switching all networking to a\nREST-like model, but we chose a much more complicated route than just changing out the networking.\nWe developed the Indirector as the standard framework for inter-component communication, and\nbuilt REST endpoints as just one option. It took two releases to fully support REST, and we have\nnot quite \ufb01nished converting to using JSON (instead of YAML) for all serialization. We undertook\nswitching to JSON for two major reasons: \ufb01rst, YAML processing Ruby is painfully slow, and pure\nRuby processing of JSON is a lot faster; second, most of the web seems to be moving to JSON, and\nit tends to be implemented more portably than YAML. Certainly in the case of Puppet, the \ufb01rst use\nof YAML was not portable across languages, and was often not portable across di\ufb00erent versions of\nPuppet, because it was essentially serialization of internal Ruby objects.\nOur next major release of Puppet will \ufb01nally remove all of the XMLRPC support.\n18.5\nLessons Learned\nIn terms of implementation, we\u2019re proudest of the various kinds of separation that exist in Puppet:\nthe language is completely separate from the RAL, the Transaction cannot directly touch the system,\nand the RAL can\u2019t decide to do work on its own. This gives the application developer a lot of control\nover application work\ufb02ow, along with a lot of access to information about what is happening and\nwhy.\nPuppet\u2019s extensibility and con\ufb01gurability are also major assets, because anyone can build on top\nof Puppet quite easily without having to hack the core. We\u2019ve always built our own capabilities on\nthe same interfaces we recommend our users use.\nPuppet\u2019s simplicity and ease of use have always been its major draw. It\u2019s still too di\ufb03cult to get\nrunning, but it\u2019s miles easier than any of the other tools on the market. This simplicity comes with a\nlot of engineering costs, especially in the form of maintenance and extra design work, but it\u2019s worth\nit to allow users to focus on their problems instead of the tool.\nPuppet\u2019s con\ufb01gurability is a real feature, but we took it a bit too far. There are too many ways\nyou can wire Puppet together, and it\u2019s too easy to build a work\ufb02ow on top of Puppet that will make\nyou miserable. One of our major near-term goals is to dramatically reduce the knobs you can turn in\na Puppet con\ufb01guration, so the user cannot so easily con\ufb01gure it poorly, and so we can more easily\nupgrade it over time without worrying about obscure edge cases.\nLuke Kanies\n277\n", "page": 291, "type": "text", "section": "Page 291"}
{"text": "We also just generally changed too slowly. There are major refactors we\u2019ve been wanting to do\nfor years but have never quite tackled. This has meant a more stable system for our users in the short\nterm, but also a more di\ufb03cult-to-maintain system, and one that\u2019s much harder to contribute to.\nLastly, it took us too long to realize that our goals of simplicity were best expressed in the\nlanguage of design. Once we began speaking about design rather than just simplicity, we acquired a\nmuch better framework for making decisions about adding or removing features, with a better means\nof communicating the reasoning behind those decisions.\n18.6\nConclusion\nPuppet is both a simple system and a complex one. It has many moving parts, but they\u2019re wired\ntogether quite loosely, and each of them has changed pretty dramatically since its founding in 2005.\nIt is a framework that can be used for all manner of con\ufb01guration problems, but as an application it\nis simple and approachable.\nOur future success rests on that framework becoming more solid and more simple, and that\napplication staying approachable while it gains capability.\n278\nPuppet\n", "page": 292, "type": "text", "section": "Page 292"}
{"text": "[chapter19]\nPyPy\nBenjamin Peterson\nPyPy is a Python implementation and a dynamic language implementation framework.\nThis chapter assumes familiarity with some basic interpreter and compiler concepts like bytecode\nand constant folding.\n19.1\nA Little History\nPython is a high-level, dynamic programming language. It was invented by the Dutch programmer\nGuido van Rossum in the late 1980s. Guido\u2019s original implementation is a traditional bytecode\ninterpreter written in C, and consequently known as CPython. There are now many other Python\nimplementations. Among the most notable are Jython, which is written in Java and allows for\ninterfacing with Java code, IronPython, which is written in C# and interfaces with Microsoft\u2019s\n.NET framework, and PyPy, the subject of this chapter. CPython is still the most widely used\nimplementation and currently the only one to support Python 3, the next generation of the Python\nlanguage. This chapter will explain the design decisions in PyPy that make it di\ufb00erent from other\nPython implementations and indeed from any other dynamic language implementation.\n19.2\nOverview of PyPy\nPyPy, except for a negligible number of C stubs, is written completely in Python. The PyPy source\ntree contains two major components: the Python interpreter and the RPython translation toolchain.\nThe Python interpreter is the programmer-facing runtime that people using PyPy as a Python\nimplementation invoke. It is actually written in a subset of Python called Restricted Python (usually\nabbreviated RPython). The purpose of writing the Python interpreter in RPython is so the interpreter\ncan be fed to the second major part of PyPy, the RPython translation toolchain. The RPython\ntranslator takes RPython code and converts it to a chosen lower-level language, most commonly\nC. This allows PyPy to be a self-hosting implementation, meaning it is written in the language it\nimplements. As we shall see throughout this chapter, the RPython translator also makes PyPy a\ngeneral dynamic language implementation framework.\nPyPy\u2019s powerful abstractions make it the most \ufb02exible Python implementation. It has nearly\n200 con\ufb01guration options, which vary from selecting di\ufb00erent garbage collector implementations to\naltering parameters of various translation optimizations.\n", "page": 293, "type": "text", "section": "Page 293"}
{"text": "19.3\nThe Python Interpreter\nSince RPython is a strict subset of Python, the PyPy Python interpreter can be run on top of another\nPython implementation untranslated. This is, of course, extremely slow but it makes it possible to\nquickly test changes in the interpreter. It also enables normal Python debugging tools to be used to\ndebug the interpreter. Most of PyPy\u2019s interpreter tests can be run both on the untranslated interpreter\nand the translated interpreter. This allows quick testing during development as well as assurance that\nthe translated interpreter behaves the same as the untranslated one.\nFor the most part, the details of the PyPy Python interpreter are quite similiar to that of CPython;\nPyPy and CPython use nearly identical bytecode and data structures during interpretation. The\nprimary di\ufb00erence between the two is PyPy has a clever abstraction called object spaces (or objspaces\nfor short). An objspace encapsulates all the knowledge needed to represent and manipulate Python\ndata types. For example, performing a binary operation on two Python objects or fetching an attribute\nof an object is handled completely by the objspace. This frees the interpreter from having to know\nanything about the implementation details of Python objects. The bytecode interpreter treats Python\nobjects as black boxes and calls objspace methods whenever it needs to manipulate them. For example,\nhere is a rough implementation of the BINARY_ADD opcode, which is called when two objects are\ncombined with the + operator. Notice how the operands are not inspected by the interpreter; all\nhandling is delegated immediately to the objspace.\ndef BINARY_ADD(space, frame):\nobject1 = frame.pop() # pop left operand off stack\nobject2 = frame.pop() # pop right operand off stack\nresult = space.add(object1, object2) # perform operation\nframe.push(result) # record result on stack\nThe objspace abstraction has numerous advantages. It allows new data type implementations to\nbe swapped in and out without modifying the interpreter. Also, since the sole way to manipulate\nobjects is through the objspace, the objspace can intercept, proxy, or record operations on objects.\nUsing the powerful abstraction of objspaces, PyPy has experimented with thunking, where results\ncan be lazily but completely transparently computed on demand, and tainting, where any operation\non an object will raise an exception (useful for passing sensitive data through untrusted code). The\nmost important application of objspaces, however, will be discussed in Section 19.4.\nThe objspace used in a vanilla PyPy interpreter is called the standard objspace (std objspace for\nshort). In addition to the abstraction provided by the objspace system, the standard objspace provides\nanother level of indirection; a single data type may have multiple implementations. Operations on data\ntypes are then dispatched using multimethods. This allows picking the most e\ufb03cient representation\nfor a given piece of data. For example, the Python long type (ostensibly a bigint data type) can\nbe represented as a standard machine-word-sized integer when it is small enough. The memory\nand computationally more expensive arbitrary-precision long implementation need only be used\nwhen necessary. There\u2019s even an implementation of Python integers available using tagged pointers.\nContainer types can also be specialized to certain data types. For example, PyPy has a dictionary\n(Python\u2019s hash table data type) implementation specialized for string keys. The fact that the same data\ntype can be represented by di\ufb00erent implementations is completely transparent to application-level\ncode; a dictionary specialized to strings is identical to a generic dictionary and will degenerate\ngracefully if non-string keys are put into it.\nPyPy distinguishes between interpreter-level (interp-level) and application-level (app-level) code.\nInterp-level code, which most of the interpreter is written in, must be in RPython and is translated.\n280\nPyPy\n", "page": 294, "type": "text", "section": "Page 294"}
{"text": "It directly works with the objspace and wrapped Python objects. App-level code is always run by\nthe PyPy bytecode interpreter. As simple as interp-level RPython code is, compared to C or Java,\nPyPy developers have found it easiest to use pure app-level code for some parts of the interpreter.\nConsequently, PyPy has support for embedding app-level code in the interpreter. For example, the\nfunctionality of the Python print statement, which writes objects to standard output, is implemented\nin app-level Python. Builtin modules can also be written partially in interp-level code and partially\nin app-level code.\n19.4\nThe RPython Translator\nThe RPython translator is a toolchain of several lowering phases that rewrite RPython to a target\nlanguage, typically C. The higher-level phases of translation are shown in Figure 19.1. The translator\nis itself written in (unrestricted) Python and intimately linked to the PyPy Python interpreter for\nreasons that will be illuminated shortly.\nFigure 19.1: Translation steps\nThe \ufb01rst thing the translator does is load the RPython program into its process. (This is done\nwith the normal Python module loading support.) RPython imposes a set of restrictions on normal,\ndynamic Python. For example, functions cannot be created at runtime, and a single variable cannot\nhave the possibility of holding incompatible types, such as an integer and a object instance. When\nthe program is initially loaded by the translator, though, it is running on a normal Python interpreter\nand can use all of Python\u2019s dynamic features. PyPy\u2019s Python interpreter, a huge RPython program,\nmakes heavy use of this feature for metaprogramming. For example, it generates code for standard\nobjspace multimethod dispatch. The only requirement is that the program is valid RPython by the\ntime the translator starts the next phase of translation.\nThe translator builds \ufb02ow graphs of the RPython program through a process called abstract\ninterpretation. Abstract interpretation reuses the PyPy Python interpreter to interpret RPython\nprograms with a special objspace called the \ufb02ow objspace. Recall that the Python interpreter treats\nobjects in a program like black boxes, calling out to the objspace to perform any operation. The \ufb02ow\nobjspace, instead of the standard set of Python objects, has only two objects: variables and constants.\nVariables represent values not known during translation, and constants, not surprisingly, represent\nimmutable values that are known. The \ufb02ow objspace has a basic facility for constant folding; if it is\nasked to do an operation where all the arguments are constants, it will statically evaluate it. What\nBenjamin Peterson\n281\n", "page": 295, "type": "text", "section": "Page 295"}
{"text": "is immutable and must be constant in RPython is broader than in standard Python. For example,\nmodules, which are emphatically mutable in Python, are constants in the \ufb02ow objspace because\nthey don\u2019t exist in RPython and must be constant-folded out by the \ufb02ow objspace. As the Python\ninterpreter interprets the bytecode of RPython functions, the \ufb02ow objspace records the operations it\nis asked to perform. It takes care to record all branches of conditional control \ufb02ow constructs. The\nend result of abstract interpretation for a function is a \ufb02ow-graph consisting of linked blocks, where\neach block has one or more operations.\nAn example of the \ufb02ow-graph generating process is in order. Consider a simple factorial function:\ndef factorial(n):\nif n == 1:\nreturn 1\nreturn n * factorial(n - 1)\nThe \ufb02ow-graph for the function looks like Figure 19.2.\nFigure 19.2: Flow-graph of factorial\nThe factorial function has been divided into blocks containing the operations the \ufb02owspace\nrecorded. Each block has input arguments and a list of operations on the variables and constants.\nThe \ufb01rst block has an exit switch at the end, which determines which block control-\ufb02ow will pass to\nafter the \ufb01rst block is run. The exit switch can be based on the value of some variable or whether\nan exception occurred in the last operation of the block. Control-\ufb02ow follows the lines between the\nblocks.\nThe \ufb02ow-graph generated in the \ufb02ow objspace is in static single assignment form, or SSA, an\nintermediate representation commonly used in compilers. The key feature of SSA is that every\n282\nPyPy\n", "page": 296, "type": "text", "section": "Page 296"}
{"text": "variable is only assigned once. This property simpli\ufb01es the implementation of many compiler\ntransformations and optimizations.\nAfter a function graph is generated, the annotation phase begins. The annotator assigns a type\nto the results and arguments of each operation. For example, the factorial function above will be\nannotated to accept and return an integer.\nThe next phase is called RTyping. RTyping uses type information from the annotator to expand\neach high-level \ufb02ow-graph operation into low-level ones. It is the \ufb01rst part of translation where the\ntarget backend matters. The backend chooses a type system for the RTyper to specialize the program\nto. The RTyper currently has two type systems: A low-level typesystem for backends like C and one\nfor higher-level typesystems with classes. High-level Python operations and types are transformed\ninto the level of the type system. For example, an add operation with operands annotated as integers\nwill generate a int_add operation with the low-level type system. More complicated operations like\nhash table lookups generate function calls.\nAfter RTyping, some optimizations on the low-level \ufb02ow-graph are performed. They are mostly\nof the traditional compiler variety like constant folding, store sinking, and dead code removal.\nPython code typically has frequent dynamic memory allocations. RPython, being a Python\nderivative, inherits this allocation intensive pattern. In many cases, though, allocations are temporary\nand local to a function. Malloc removal is an optimization that addresses these cases. Malloc removal\nremoves these allocations by \u201c\ufb02attening\u201d the previously dynamically allocated object into component\nscalars when possible.\nTo see how malloc removals works, consider the following function that computes the Euclidean\ndistance between two points on the plane in a roundabout fashion:\ndef distance(x1, y1, x2, y2):\np1 = (x1, y1)\np2 = (x2, y2)\nreturn math.hypot(p1[0] - p2[0], p1[1] - p2[1])\nWhen initially RTyped, the body of the function has the following operations:\nv60 = malloc((GcStruct tuple2))\nv61 = setfield(v60, (\u2019item0\u2019), x1_1)\nv62 = setfield(v60, (\u2019item1\u2019), y1_1)\nv63 = malloc((GcStruct tuple2))\nv64 = setfield(v63, (\u2019item0\u2019), x2_1)\nv65 = setfield(v63, (\u2019item1\u2019), y2_1)\nv66 = getfield(v60, (\u2019item0\u2019))\nv67 = getfield(v63, (\u2019item0\u2019))\nv68 = int_sub(v66, v67)\nv69 = getfield(v60, (\u2019item1\u2019))\nv70 = getfield(v63, (\u2019item1\u2019))\nv71 = int_sub(v69, v70)\nv72 = cast_int_to_float(v68)\nv73 = cast_int_to_float(v71)\nv74 = direct_call(math_hypot, v72, v73)\nThis code is suboptimal in several ways. Two tuples that never escape the function are allocated.\nAdditionally, there is unnecessary indirection accessing the tuple \ufb01elds.\nRunning malloc removal produces the following concise code:\nBenjamin Peterson\n283\n", "page": 297, "type": "text", "section": "Page 297"}
{"text": "v53 = int_sub(x1_0, x2_0)\nv56 = int_sub(y1_0, y2_0)\nv57 = cast_int_to_float(v53)\nv58 = cast_int_to_float(v56)\nv59 = direct_call(math_hypot, v57, v58)\nThe tuple allocations have been completely removed and the indirections \ufb02attened out. Later, we\nwill see how a technique similar to malloc removal is used on application-level Python in the PyPy\nJIT (Section 19.5).\nPyPy also does function inlining. As in lower-level languages, inlining improves performance\nin RPython. Somewhat surprisingly, it also reduces the size of the \ufb01nal binary. This is because it\nallows more constant folding and malloc removal to take place, which reduces overall code size.\nThe program, now in optimized, low-level \ufb02ow-graphs, is passed to the backend to generate\nsources. Before it can generate C code, the C backend must perform some additional transformations.\nOne of these is exception transformation, where exception handling is rewritten to use manual stack\nunwinding. Another is the insertion of stack depth checks. These raise an exception at runtime if the\nrecursion is too deep. Places where stack depth checks are needed are found by computing cycles in\nthe call graph of the program.\nAnother one of the transformations performed by the C backend is adding garbage collection\n(GC). RPython, like Python, is a garbage-collected language, but C is not, so a garbage collector has\nto be added. To do this, a garbage collection transformer converts the \ufb02ow-graphs of the program into\na garbage-collected program. PyPy\u2019s GC transformers provide an excellent demonstration of how\ntranslation abstracts away mundane details. In CPython, which uses reference counting, the C code\nof the interpreter must carefully keep track of references to Python objects it is manipulating. This\nnot only hardcodes the garbage collection scheme in the entire codebase but is prone to subtle human\nerrors. PyPy\u2019s GC transformer solves both problems; it allows di\ufb00erent garbage collection schemes\nto be swapped in and out seamlessly. It is trivial to evaluate a garbage collector implementation (of\nwhich PyPy has many), simply by tweaking a con\ufb01guration option at translation. Modulo transformer\nbugs, the GC transformer also never makes reference mistakes or forgets to inform the GC when\nan object is no longer in use. The power of the GC abstraction allows GC implementations that\nwould be practically impossible to hardcode in an interpreter. For example, several of PyPy\u2019s GC\nimplementations require a write barrier. A write barrier is a check which must be performed every\ntime a GC-managed object is placed in another GC-managed array or structure. The process of\ninserting write barriers would be laborious and fraught with mistakes if done manually, but is trivial\nwhen done automatically by the GC transformer.\nThe C backend can \ufb01nally emit C source code. The generated C code, being generated from\nlow-level \ufb02ow-graphs, is an ugly mess of gotos and obscurely named variables. An advantage of\nwriting C is that the C compiler can do most of the complicated static transformation work required\nto make a \ufb01nal binary-like loop optimizations and register allocation.\n19.5\nThe PyPy JIT\nPython, like most dynamic languages, has traditionally traded e\ufb03ciency for \ufb02exibility. The archi-\ntecture of PyPy, being especially rich in \ufb02exibility and abstraction, makes very fast interpretation\ndi\ufb03cult. The powerful objspace and multimethod abstractions in the std objspace do not come with-\nout a cost. Consequently, the vanilla PyPy interpreter performs up to 4 times slower than CPython.\nTo remedy not only this but Python\u2019s reputation as a sluggish language, PyPy has a just-in-time\n284\nPyPy\n", "page": 298, "type": "text", "section": "Page 298"}
{"text": "compiler (commonly written JIT). The JIT compiles frequently used codepaths into assembly during\nthe runtime of the program.\nThe PyPy JIT takes advantage of PyPy\u2019s unique translation architecture described in Section 19.4.\nPyPy actually has no Python-speci\ufb01c JIT; it has a JIT generator. JIT generation is implemented as\nsimply another optional pass during translation. A interpreter desiring JIT generation need only\nmake two special function calls called jit hints.\nPyPy\u2019s JIT is a tracing JIT. This means it detects \u201chot\u201d (meaning frequently run) loops to\noptimize by compiling to assembly. When the JIT has decided it is going to compile a loop, it records\noperations in one iteration of the loop, a process called tracing. These operations are subsequently\ncompiled to machine code.\nAs mentioned above, the JIT generator requires only two hints in the interpreter to generate a JIT:\nmerge_point and can_enter_jit. can_enter_jit tells the JIT where in the interpreter a loop\nstarts. In the Python interpreter, this is the end of the JUMP_ABSOLUTE bytecode. (JUMP_ABSOLUTE\nmakes the interpreter jump to the head of the app-level loop.) merge_point tells the JIT where it is\nsafe to return to the interpreter from the JIT. This is the beginning of the bytecode dispatch loop in\nthe Python interpreter.\nThe JIT generator is invoked after the RTyping phase of translation. Recall that at this point,\nthe program\u2019s \ufb02ow-graphs consist of low-level operations nearly ready for target code generation.\nThe JIT generator locates the hints mentioned above in the interpreter and replaces them with calls\nto invoke the JIT during runtime. The JIT generator then writes a serialized representation of the\n\ufb02ow-graphs of every function that the interpreter wants jitted. These serialized \ufb02ow-graphs are called\njitcodes. The entire interpreter is now described in terms of low-level RPython operations. The\njitcodes are saved in the \ufb01nal binary for use at runtime.\nAt runtime, the JIT maintains a counter for every loop that is executed in the program. When a\nloop\u2019s counter exceeds a con\ufb01gurable threshold, the JIT is invoked and tracing begins. The key object\nin tracing is the meta-interpreter. The meta-interpreter executes the jitcodes created in translation.\nIt is thus interpreting the main interpreter, hence the name. As it traces the loop, it creates a list\nof the operations it is executing and records them in JIT intermediate representation (IR), another\noperation format. This list is called the trace of the loop. When the meta-interpreter encounters a\ncall to a jitted function (one for which jitcode exists), the meta-interpreter enters it and records its\noperations to original trace. Thus, the tracing has the e\ufb00ect of \ufb02attening out the call stack; the only\ncalls in the trace are to interpreter functions that are outside the knowledge of jit.\nThe meta-interpreter is forced to specialize the trace to properties of the loop iteration it is\ntracing. For example, when the meta-interpreter encounters a conditional in the jitcode, it naturally\nmust choose one path based on the state of the program. When it makes a choice based on runtime\ninformation, the meta-interpreter records an IR operation called a guard. In the case of a conditional,\nthis will be a guard_true or guard_false operation on the condition variable. Most arithmetic\noperations also have guards, which ensure the operation did not over\ufb02ow. Essentially, guards codify\nassumptions the meta-interpreter is making as it traces. When assembly is generated, the guards\nwill protect assembly from being run in a context it is not specialized for. Tracing ends when the\nmeta-interpreter reaches the same can_enter_jit operation with which it started tracing. The loop\nIR can now be passed to the optimizer.\nThe JIT optimizer features a few classical compiler optimizations and many optimizations special-\nized for dynamic languages. Among the most important of the latter are virtuals and virtualizables.\nVirtuals are objects which are known not to escape the trace, meaning they are not passed as\narguments to external, non-jitted function calls. Structures and constant length arrays can be virtuals.\nVirtuals do not have to be allocated, and their data can be stored directly in registers and on the\nBenjamin Peterson\n285\n", "page": 299, "type": "text", "section": "Page 299"}
{"text": "stack. (This is much like the static malloc removal phase described in the section about translation\nbackend optimizations.) The virtuals optimization strips away the indirection and memory allocation\nine\ufb03ciencies in the Python interpreter. For example, by becoming virtual, boxed Python integer\nobjects are unboxed into simple word-sized integers and can be stored directly in machine registers.\nA virtualizable acts much like a virtual but may escape the trace (that is, be passed to non-\njitted functions). In the Python interpreter the frame object, which holds variable values and the\ninstruction pointer, is marked virtualizable. This allows stack manipulations and other operations on\nthe frame to be optimized out. Although virtuals and virtualizables are similar, they share nothing in\nimplementation. Virtualizables are handled during tracing by the meta-interpreter. This is unlike\nvirtuals, which are handled during trace optimization. The reason for this is virtualizables require\nspecial treatment, since they may escape the trace. Speci\ufb01cally, the meta-interpreter has to ensure\nthat non-jitted functions that may use the virtualizable don\u2019t actually try to fetch its \ufb01elds. This is\nbecause in jitted code, the \ufb01elds of virtualizable are stored in the stack and registers, so the actual\nvirtualizable may be out of date with respect to its current values in the jitted code. During JIT\ngeneration, code which accesses a virtualizable is rewritten to check if jitted assembly is running. If\nit is, the JIT is asked to update the \ufb01elds from data in assembly. Additionally when the external call\nreturns to jitted code, execution bails back to the interpreter.\nAfter optimization, the trace is ready to be assembled. Since the JIT IR is already quite low-level,\nassembly generation is not too di\ufb03cult. Most IR operations correspond to only a few x86 assembly\noperations. The register allocator is a simple linear algorithm. At the moment, the increased time that\nwould be spent in the backend with a more sophisticated register allocation algorithm in exchange for\ngenerating slightly better code has not been justi\ufb01ed. The trickiest portions of assembly generation\nare garbage collector integration and guard recovery. The GC has to be made aware of stack roots in\nthe generated JIT code. This is accomplished by special support in the GC for dynamic root maps.\nFigure 19.3: Bailing back to the interpreter on guard failure\nWhen a guard fails, the compiled assembly is no longer valid and control must return to the\nbytecode interpreter. This bailing out is one of the most di\ufb03cult parts of JIT implementation, since\nthe interpreter state has to be reconstructed from the register and stack state at the point the guard\nfailed. For each guard, the assembler writes a compact description of where all the values needed to\nreconstruct the interpreter state are. At guard failure, execution jumps to a function which decodes\nthis description and passes the recovery values to a higher level be reconstructed. The failing guard\nmay be in the middle of the execution of a complicated opcode, so the interpreter can not just start\nwith the next opcode. To solve this, PyPy uses a blackhole interpreter. The blackhole interpreter\nexecutes jitcodes starting from the point of guard failure until the next merge point is reached.\nThere, the real interpreter can resume. The blackhole interpreter is so named because unlike the\nmeta-interpreter, it doesn\u2019t record any of the operations it executes. The process of guard failure is\ndepicted in Figure 19.3.\n286\nPyPy\n", "page": 300, "type": "text", "section": "Page 300"}
{"text": "As described up to this point, the JIT would be essentially useless on any loop with a frequently\nchanging condition, because a guard failure would prevent assembly from running very many\niterations. Every guard has a failure counter. After the failure count has passed a certain threshold,\nthe JIT starts tracing from the point of guard failure instead of bailing back to the interpreter. This\nnew sub-trace is called a bridge. When the tracing reaches the end of the loop, the bridge is optimized\nand compiled and the original loop is patched at the guard to jump to the new bridge instead of the\nfailure code. This way, loops with dynamic conditions can be jitted.\nHow successful have the techniques used in the PyPy JIT proven? At the time of this writing, PyPy\nis a geometric average of \ufb01ve times faster than CPython on a comprehensive suite of benchmarks.\nWith the JIT, app-level Python has the possibility of being faster than interp-level code. PyPy\ndevelopers have recently had the excellent problem of having to write interp-level loops in app-level\nPython for performance.\nMost importantly, the fact that the JIT is not speci\ufb01c to Python means it can be applied to any\ninterpreter written within the PyPy framework. This need not necessarily be a language interpreter.\nFor example, the JIT is used for Python\u2019s regular expression engine. NumPy is a powerful array\nmodule for Python used in numerical computing and scienti\ufb01c research. PyPy has an experimental\nreimplementation of NumPy. It harnesses the power of the PyPy JIT to speed up operations on\narrays. While the NumPy implementation is still in its early stages, initial performance results look\npromising.\n19.6\nDesign Drawbacks\nWhile it beats C any day, writing in RPython can be a frustrating experience. Its implicit typing\nis di\ufb03cult to get used to at \ufb01rst. Not all Python language features are supported and others are\narbitrarily restricted. RPython is not speci\ufb01ed formally anywhere and what the translator accepts\ncan vary from day to day as RPython is adapted to PyPy\u2019s needs. The author of this chapter often\nmanages to create programs that churn in the translator for half an hour, only to fail with an obscure\nerror.\nThe fact that the RPython translator is a whole-program analyzer creates some practical problems.\nThe smallest change anywhere in translated code requires retranslating the entire interpreter. That\ncurrently takes about 40 minutes on a fast, modern system. The delay is especially annoying for\ntesting how changes a\ufb00ect the JIT, since measuring performance requires a translated interpreter. The\nrequirement that the whole program be present at translation means modules containing RPython\ncannot be built and loaded separately from the core interpreter.\nThe levels of abstraction in PyPy are not always as clear cut as in theory. While technically the\nJIT generator should be able to produce an excellent JIT for a language given only the two hints\nmentioned above, the reality is that it behaves better on some code than others. The Python interpreter\nhas seen a lot of work towards making it more \u201cjit-friendly\u201d, including many more JIT hints and\neven new data structures optimized for the JIT.\nThe many layers of PyPy can make tracking down bugs a laborious process. A Python interpreter\nbug could be directly in the interpreter source or buried somewhere in the semantics of RPython and\nthe translation toolchain. Especially when a bug cannot be reproduced on the untranslated interpreter,\ndebugging is di\ufb03cult. It typically involves running GDB on the nearly unreadable generated C\nsources.\nTranslating even a restricted subset of Python to a much lower-level language like C is not an\neasy task. The lowering passes described in Section 19.4 are not really independent. Functions\nBenjamin Peterson\n287\n", "page": 301, "type": "text", "section": "Page 301"}
{"text": "are being annotated and rtyped throughout translation, and the annotator has some knowledge of\nlow-level types. The RPython translator is thus a tangled web of cross-dependencies. The translator\ncould do with cleaning up in several places, but doing it is neither easy nor much fun.\n19.7\nA Note on Process\nIn part to combat its own complexity (see Section 19.6), PyPy has adopted several so-called \u201cagile\u201d\ndevelopment methodologies. By far the most important of these is test-driven development. All\nnew features and bug \ufb01xes are required to have tests to verify their correctness. The PyPy Python\ninterpreter is also run against CPython\u2019s regression test suite. PyPy\u2019s test driver, py.test, was spun o\ufb00\nand is now used in many other projects. PyPy also has a continuous integration system that runs\nthe test suite and translates the interpreter on a variety of platforms. Binaries for all platforms are\nproduced daily and the benchmark suite is run. All these tests ensure that the various components\nare behaving, no matter what change is made in the complicated architecture.\nThere is a strong culture of experimentation in the PyPy project. Developers are encouraged\nto make branches in the Mercurial repository. There, ideas in development can be re\ufb01ned without\ndestabilizing the main branch. Branches are not always successful, and some are abandoned. If\nanything though, PyPy developers are tenacious. Most famously, the current PyPy JIT is the \ufb01fth\nattempt to add a JIT to PyPy!\nFigure 19.4: The jitviewer showing Python bytecode and associated JIT IR operations\n288\nPyPy\n", "page": 302, "type": "text", "section": "Page 302"}
{"text": "The PyPy project also prides itself on its visualization tools. The \ufb02ow-graph charts in Section 19.4\nare one example. PyPy also has tools to show invocation of the garbage collector over time and view\nthe parse trees of regular expressions. Of special interest is jitviewer, a program that allows one to\nvisually peel back the layers of a jitted function, from Python bytecode to JIT IR to assembly. (The\njitviewer is shown in Figure 19.4.) Visualization tools help developers understand how PyPy\u2019s many\nlayers interact with each other.\n19.8\nSummary\nThe Python interpreter treats Python objects as black boxes and leaves all behavior to be de\ufb01ned by\nthe objspace. Individual objspaces can provide special extended behavior to Python objects. The\nobjspace approach also enables the abstract interpretation technique used in translation.\nThe RPython translator allows details like garbage collection and exception handling to be\nabstracted from the language interpreter. It also opens up the possibly of running PyPy on many\ndi\ufb00erent runtime platforms by using di\ufb00erent backends.\nOne of the most important uses of the translation architecture is the JIT generator. The generality\nof the JIT generator allows JITs for new languages and sub-languages like regular expressions to be\nadded. PyPy is the fastest Python implementation today because of its JIT generator.\nWhile most of PyPy\u2019s development e\ufb00ort has gone into the Python interpreter, PyPy can be used\nfor the implementation of any dynamic language. Over the years, partial interpreters for JavaScript,\nProlog, Scheme, and IO have been written with PyPy.\n19.9\nLessons Learned\nFinally, some of lessons to take away from the PyPy project:\nRepeated refactoring is often a necessary process. For example, it was originally envisioned that\nthe C backend for the translator would be able to work o\ufb00the high-level \ufb02ow graphs! It took several\niterations for the current multi-phase translation process to be born.\nThe most important lesson of PyPy is the power of abstraction. In PyPy, abstractions separate\nimplementation concerns. For example, RPython\u2019s automatic garbage collection allows a developer\nworking the interpreter to not worry about memory management. At the same time, abstractions have\na mental cost. Working on the translation chain involves juggling the various phases of translation\nat once in one\u2019s head. What layer a bug resides in can also be clouded by abstractions; abstraction\nleakage, where swapping low-level components that should be interchangeable breaks higher-level\ncode, is perennial problem. It is important that tests are used to verify that all parts of the system are\nworking, so a change in one system does not break a di\ufb00erent one. More concretely, abstractions can\nslow a program down by creating too much indirection.\nThe \ufb02exibility of (R)Python as an implementation language makes experimenting with new\nPython language features (or even new languages) easy. Because of its unique architecture, PyPy\nwill play a large role in the future of Python and dynamic language implementation.\n289\n", "page": 303, "type": "text", "section": "Page 303"}
{"text": "290\nPyPy\n", "page": 304, "type": "text", "section": "Page 304"}
{"text": "[chapter20]\nSQLAlchemy\nMichael Bayer\nSQLAlchemy is a database toolkit and object-relational mapping (ORM) system for the Python\nprogramming language, \ufb01rst introduced in 2005. From the beginning, it has sought to provide an\nend-to-end system for working with relational databases in Python, using the Python Database API\n(DBAPI) for database interactivity. Even in its earliest releases, SQLAlchemy\u2019s capabilities attracted\na lot of attention. Key features include a great deal of \ufb02uency in dealing with complex SQL queries\nand object mappings, as well as an implementation of the \u201cunit of work\u201d pattern, which provides for\na highly automated system of persisting data to a database.\nStarting from a small, roughly implemented concept, SQLAlchemy quickly progressed through\na series of transformations and reworkings, turning over new iterations of its internal architectures as\nwell as its public API as the userbase continued to grow. By the time version 0.5 was introduced in\nJanuary of 2009, SQLAlchemy had begun to assume a stable form that was already proving itself\nin a wide variety of production deployments. Throughout 0.6 (April, 2010) and 0.7 (May, 2011),\narchitectural and API enhancements continued the process of producing the most e\ufb03cient and stable\nlibrary possible. As of this writing, SQLAlchemy is used by a large number of organizations in a\nvariety of \ufb01elds, and is considered by many to be the de facto standard for working with relational\ndatabases in Python.\n20.1\nThe Challenge of Database Abstraction\nThe term \u201cdatabase abstraction\u201d is often assumed to mean a system of database communication\nwhich conceals the majority of details of how data is stored and queried. The term is sometimes\ntaken to the extreme, in that such a system should not only conceal the speci\ufb01cs of the relational\ndatabase in use, but also the details of the relational structures themselves and even whether or not\nthe underlying storage is relational.\nThe most common critiques of ORMs center on the assumption that this is the primary purpose\nof such a tool\u2014to \u201chide\u201d the usage of a relational database, taking over the task of constructing an\ninteraction with the database and reducing it to an implementation detail. Central to this approach\nof concealment is that the ability to design and query relational structures is taken away from the\ndeveloper and instead handled by an opaque library.\nThose who work heavily with relational databases know that this approach is entirely impractical.\nRelational structures and SQL queries are vastly functional, and comprise the core of an application\u2019s\ndesign. How these structures should be designed, organized, and manipulated in queries varies not\n", "page": 305, "type": "text", "section": "Page 305"}
{"text": "just on what data is desired, but also on the structure of information. If this utility is concealed,\nthere\u2019s little point in using a relational database in the \ufb01rst place.\nThe issue of reconciling applications that seek concealment of an underlying relational database\nwith the fact that relational databases require great speci\ufb01city is often referred to as the \u201cobject-\nrelational impedance mismatch\u201d problem. SQLAlchemy takes a somewhat novel approach to this\nproblem.\nSQLAlchemy\u2019s Approach to Database Abstraction\nSQLAlchemy takes the position that the developer must be willing to consider the relational form of\nhis or her data. A system which pre-determines and conceals schema and query design decisions\nmarginalizes the usefulness of using a relational database, leading to all of the classic problems of\nimpedance mismatch.\nAt the same time, the implementation of these decisions can and should be executed through\nhigh-level patterns as much as possible. Relating an object model to a schema and persisting it via\nSQL queries is a highly repetitive task. Allowing tools to automate these tasks allows the development\nof an application that\u2019s more succinct, capable, and e\ufb03cient, and can be created in a fraction of the\ntime it would take to develop these operations manually.\nTo this end, SQLAlchemy refers to itself as a toolkit, to emphasize the role of the developer\nas the designer/builder of all relational structures and linkages between those structures and the\napplication, not as a passive consumer of decisions made by a library. By exposing relational\nconcepts, SQLAlchemy embraces the idea of \u201cleaky abstraction\u201d, encouraging the developer to tailor\na custom, yet fully automated, interaction layer between the application and the relational database.\nSQLAlchemy\u2019s innovation is the extent to which it allows a high degree of automation with little to\nno sacri\ufb01ce in control over the relational database.\n20.2\nThe Core/ORM Dichotomy\nCentral to SQLAlchemy\u2019s goal of providing a toolkit approach is that it exposes every layer of\ndatabase interaction as a rich API, dividing the task into two main categories known as Core and\nORM. The Core includes Python Database API (DBAPI) interaction, rendering of textual SQL\nstatements understood by the database, and schema management. These features are all presented\nas public APIs. The ORM, or object-relational mapper, is then a speci\ufb01c library built on top of\nthe Core. The ORM provided with SQLAlchemy is only one of any number of possible object\nabstraction layers that could be built upon the Core, and many developers and organizations build\ntheir applications on top of the Core directly.\nThe Core/ORM separation has always been SQLAlchemy\u2019s most de\ufb01ning feature, and it has both\npros and cons. The explicit Core present in SQLAlchemy leads the ORM to relate database-mapped\nclass attributes to a structure known as a Table, rather than directly to their string column names as\nexpressed in the database; to produce a SELECT query using a structure called select, rather than\npiecing together object attributes directly into a string statement; and to receive result rows through a\nfacade called ResultProxy, which transparently maps the select to each result row, rather than\ntransferring data directly from a database cursor to a user-de\ufb01ned object.\nCore elements may not be visible in a very simple ORM-centric application. However, as the\nCore is carefully integrated into the ORM to allow \ufb02uid transition between ORM and Core constructs,\na more complex ORM-centric application can \u201cmove down\u201d a level or two in order to deal with the\n292\nSQLAlchemy\n", "page": 306, "type": "text", "section": "Page 306"}
{"text": "Third party libraries / Python core\nSQLAlchemy Core\nSQLAlchemy ORM\nSQL Expression \nLanguage\nDialect\nConnection \nPooling\nDBAPI\nSchema / Types\nEngine\nObject Relational Mapper (ORM)\nDatabase\nFigure 20.1: SQLAlchemy layer diagram\ndatabase in a more speci\ufb01c and \ufb01nely tuned manner, as the situation requires. As SQLAlchemy has\nmatured, the Core API has become less explicit in regular use as the ORM continues to provide\nmore sophisticated and comprehensive patterns. However, the availability of the Core was also a\ncontributor to SQLAlchemy\u2019s early success, as it allowed early users to accomplish much more than\nwould have been possible when the ORM was still being developed.\nThe downside to the ORM/Core approach is that instructions must travel through more steps.\nPython\u2019s traditional C implementation has a signi\ufb01cant overhead penalty for individual function\ncalls, which are the primary cause of slowness in the runtime. Traditional methods of ameliorating\nthis include shortening call chains through rearrangement and inlining, and replacing performance-\ncritical areas with C code. SQLAlchemy has spent many years using both of these methods to\nimprove performance. However, the growing acceptance of the PyPy interpreter for Python may\npromise to squash the remaining performance problems without the need to replace the majority of\nSQLAlchemy\u2019s internals with C code, as PyPy vastly reduces the impact of long call chains through\njust-in-time inlining and compilation.\n20.3\nTaming the DBAPI\nAt the base of SQLAlchemy is a system for interacting with the database via the DBAPI. The\nDBAPI itself is not an actual library, only a speci\ufb01cation. Therefore, implementations of the DBAPI\nare available for a particular target database, such as MySQL or PostgreSQL, or alternatively for\nparticular non-DBAPI database adapters, such as ODBC and JDBC.\nMichael Bayer\n293\n", "page": 307, "type": "text", "section": "Page 307"}
{"text": "The DBAPI presents two challenges. The \ufb01rst is to provide an easy-to-use yet full-featured facade\naround the DBAPI\u2019s rudimentary usage patterns. The second is to handle the extremely variable\nnature of speci\ufb01c DBAPI implementations as well as the underlying database engines.\nThe Dialect System\nThe interface described by the DBAPI is extremely simple. Its core components are the DBAPI\nmodule itself, the connection object, and the cursor object\u2014a \u201ccursor\u201d in database parlance represents\nthe context of a particular statement and its associated results. A simple interaction with these objects\nto connect and retrieve data from a database is as follows:\nconnection = dbapi.connect(user=\"user\", pw=\"pw\", host=\"host\")\ncursor = connection.cursor()\ncursor.execute(\"select * from user_table where name=?\", (\"jack\",))\nprint \"Columns in result:\", [desc[0] for desc in cursor.description]\nfor row in cursor.fetchall():\nprint \"Row:\", row\ncursor.close()\nconnection.close()\nSQLAlchemy creates a facade around the classical DBAPI conversation. The point of entry\nto this facade is the create_engine call, from which connection and con\ufb01guration information is\nassembled. An instance of Engine is produced as the result. This object then represents the gateway\nto the DBAPI, which itself is never exposed directly.\nFor simple statement executions, Engine o\ufb00ers what\u2019s known as an implicit execution interface.\nThe work of acquiring and closing both a DBAPI connection and cursor are handled behind the\nscenes:\nengine = create_engine(\"postgresql://user:pw&#64;host/dbname\")\nresult = engine.execute(\"select * from table\")\nprint result.fetchall()\nWhen SQLAlchemy 0.2 was introduced the Connection object was added, providing the ability\nto explicitly maintain the scope of the DBAPI connection:\nconn = engine.connect()\nresult = conn.execute(\"select * from table\")\nprint result.fetchall()\nconn.close()\nThe result returned by the execute method of Engine or Connection is called a ResultProxy,\nwhich o\ufb00ers an interface similar to the DBAPI cursor but with richer behavior. The Engine,\nConnection, and ResultProxy correspond to the DBAPI module, an instance of a speci\ufb01c DBAPI\nconnection, and an instance of a speci\ufb01c DBAPI cursor, respectively.\nBehind the scenes, the Engine references an object called a Dialect. The Dialect is an\nabstract class for which many implementations exist, each one targeted at a speci\ufb01c DBAPI/database\ncombination. A Connection created on behalf of the Engine will refer to this Dialect for all\ndecisions, which may have varied behaviors depending on the target DBAPI and database in use.\nThe Connection, when created, will procure and maintain an actual DBAPI connection from\na repository known as a Pool that\u2019s also associated with the Engine. The Pool is responsible for\n294\nSQLAlchemy\n", "page": 308, "type": "text", "section": "Page 308"}
{"text": "creating new DBAPI connections and, usually, maintaining them in an in-memory pool for frequent\nre-use.\nDuring a statement execution, an additional object called an ExecutionContext is created\nby the Connection. The object lasts from the point of execution throughout the lifespan of the\nResultProxy. It may also be available as a speci\ufb01c subclass for some DBAPI/database combinations.\nFigure 20.2 illustrates all of these objects and their relationships to each other as well as to the\nDBAPI components.\n<<uses>>\nEngine\nDialect\npsycopg2\nDBAPI\n<<uses>>\nExecutionContext\nDBAPI cursor\n<<uses>>\nsqlalchemy.engine\npsycopg2\n<<produces>>\nConnection\n<<creates>>\n<<creates>>\nResultProxy\n<<creates>>\n<<uses>>\nDBAPI \nconnection\n<<produces>>\n<<uses>>\nPool\nsqlalchemy.pool\n<<uses>>\n<<maintains>>\n<<uses>>\nFigure 20.2: Engine, Connection, ResultProxy API\nDealing with DBAPI Variability\nFor the task of managing variability in DBAPI behavior, \ufb01rst we\u2019ll consider the scope of the problem.\nThe DBAPI speci\ufb01cation, currently at version two, is written as a series of API de\ufb01nitions which\nallow for a wide degree of variability in behavior, and leave a good number of areas unde\ufb01ned.\nAs a result, real-life DBAPIs exhibit a great degree of variability in several areas, including when\nPython unicode strings are acceptable and when they are not; how the \u201clast inserted id\u201d\u2014that is, an\nautogenerated primary key\u2014may be acquired after an INSERT statement; and how bound parameter\nvalues may be speci\ufb01ed and interpreted. They also have a large number of idiosyncratic type-oriented\nbehaviors, including the handling of binary, precision numeric, date, Boolean, and unicode data.\nSQLAlchemy approaches this by allowing variability in both Dialect and ExecutionContext\nvia multi-level subclassing.\nFigure 20.3 illustrates the relationship between Dialect and\nMichael Bayer\n295\n", "page": 309, "type": "text", "section": "Page 309"}
{"text": "ExecutionContext when used with the psycopg2 dialect. The PGDialect class provides behaviors\nthat are speci\ufb01c to the usage of the PostgreSQL database, such as the ARRAY datatype and schema\ncatalogs; the PGDialect_psycopg2 class then provides behaviors speci\ufb01c to the psycopg2 DBAPI,\nincluding unicode data handlers and server-side cursor behavior.\nDialect\nDefaultDialect\nPGDialect_psycopg2\nPGDialect\nExecutionContext\nDefaultExecutionContext\nPGExecutionContext\nPGExecutionContext_psycopg2\nsqlalchemy.dialects.postgresql\nsqlalchemy.engine\n<<uses>>\nFigure 20.3: Simple Dialect/ExecutionContext hierarchy\nA variant on the above pattern presents itself when dealing with a DBAPI that supports multiple\ndatabases. Examples of this include pyodbc, which deals with any number of database backends via\nODBC, and zxjdbc, a Jython-only driver which deals with JDBC. The above relationship is augmented\nby the use of a mixin class from the sqlalchemy.connectors package which provides DBAPI\nbehavior that is common to multiple backends. Figure 20.4 illustrates the common functionality of\nsqlalchemy.connectors.pyodbc shared among pyodbc-speci\ufb01c dialects for MySQL and Microsoft\nSQL Server.\nThe Dialect and ExecutionContext objects provide a means to de\ufb01ne every interaction with\nthe database and DBAPI, including how connection arguments are formatted and how special quirks\nduring statement execution are handled. The Dialect is also a factory for SQL compilation constructs\nthat render SQL correctly for the target database, and type objects which de\ufb01ne how Python data\nshould be marshaled to and from the target DBAPI and database.\n20.4\nSchema De\ufb01nition\nWith database connectivity and interactivity established, the next task is to provide for the creation\nand manipulation of backend-agnostic SQL statements. To achieve this, we need to de\ufb01ne \ufb01rst\nhow we will refer to the tables and columns present in a database\u2014the so-called \u201cschema\u201d. Tables\nand columns represent how data is organized, and most SQL statements consist of expressions and\ncommands referring to these structures.\n296\nSQLAlchemy\n", "page": 310, "type": "text", "section": "Page 310"}
{"text": "Dialect\nDefaultDialect\nMSDialect_pyodbc\nMSDialect\nsqlalchemy.dialects.mssql\nsqlalchemy.engine\nPyODBCConnector\nsqlalchemy.connectors\nMySQLDialect\nMySQLDialect_pyodbc\nsqlalchemy.dialects.mysql\nFigure 20.4: Common DBAPI behavior shared among dialect hierarchies\nAn ORM or data access layer needs to provide programmatic access to the SQL language; at the\nbase is a programmatic system of describing tables and columns. This is where SQLAlchemy o\ufb00ers\nthe \ufb01rst strong division of Core and ORM, by o\ufb00ering the Table and Column constructs that describe\nthe structure of the database independently of a user\u2019s model class de\ufb01nition. The rationale behind\nthe division of schema de\ufb01nition from object relational mapping is that the relational schema can be\ndesigned unambiguously in terms of the relational database, including platform-speci\ufb01c details if\nnecessary, without being muddled by object-relational concepts\u2014these remain a separate concern.\nBeing independent of the ORM component also means the schema description system is just as\nuseful for any other kind of object-relational system which may be built on the Core.\nThe Table and Column model falls under the scope of what\u2019s referred to as metadata, o\ufb00ering\na collection object called MetaData to represent a collection of Table objects. The structure is\nderived mostly from Martin Fowler\u2019s description of \u201cMetadata Mapping\u201d in Patterns of Enterprise\nApplication Architecture. Figure 20.5 illustrates some key elements of the sqlalchemy.schema\npackage.\nTable represents the name and other attributes of an actual table present in a target schema.\nIts collection of Column objects represents naming and typing information about individual table\ncolumns. A full array of objects describing constraints, indexes, and sequences is provided to \ufb01ll in\nmany more details, some of which impact the behavior of the engine and SQL construction system.\nIn particular, ForeignKeyConstraint is central to determining how two tables should be joined.\nTable and Column in the schema package are unique versus the rest of the package in that they are\ndual-inheriting, both from the sqlalchemy.schema package and the sqlalchemy.sql.expression\npackage, serving not just as schema-level constructs, but also as core syntactical units in the SQL\nexpression language. This relationship is illustrated in Figure 20.6.\nIn Figure 20.6 we can see that Table and Column inherit from the SQL world as speci\ufb01c forms of\n\u201cthings you can select from\u201d, known as a FromClause, and \u201cthings you can use in a SQL expression\u201d,\nknown as a ColumnElement.\nMichael Bayer\n297\n", "page": 311, "type": "text", "section": "Page 311"}
{"text": "Column\nMetaData\nConstraint\nUniqueConstraint\nCheckConstraint\n<<contains>>\n<<contains>>\n<<contains>>\nIndex\nsqlalchemy.schema\nSequence\nTable\nForeignKeyConstraint\nSchemaElement\nFigure 20.5: Basic sqlalchemy.schema objects\nColumn\nSchemaElement\n<<contains>>\nsqlalchemy.schema\nTable\nFromClause\nColumnElement\nClauseElement\nsqlalchemy.sql\nTableClause\nColumnClause\nFigure 20.6: The dual lives of Table and Column\n20.5\nSQL Expressions\nDuring SQLAlchemy\u2019s creation, the approach to SQL generation wasn\u2019t clear. A textual language\nmight have been a likely candidate; this is a common approach which is at the core of well-known\nobject-relational tools like Hibernate\u2019s HQL. For Python, however, a more intriguing choice was\navailable: using Python objects and expressions to generatively construct expression tree structures,\neven re-purposing Python operators so that operators could be given SQL statement behavior.\nWhile it may not have been the \ufb01rst tool to do so, full credit goes to the SQLBuilder library\nincluded in Ian Bicking\u2019s SQLObject as the inspiration for the system of Python objects and operators\nused by SQLAlchemy\u2019s expression language. In this approach, Python objects represent lexical\nportions of a SQL expression. Methods on those objects, as well as overloaded operators, generate\nnew lexical constructs derived from them. The most common object is the \u201cColumn\u201d object\u2014\n298\nSQLAlchemy\n", "page": 312, "type": "text", "section": "Page 312"}
{"text": "SQLObject would represent these on an ORM-mapped class using a namespace accessed via the .q\nattribute; SQLAlchemy named the attribute .c. The .c attribute remains today on Core selectable\nelements, such as those representing tables and select statements.\nExpression Trees\nA SQLAlchemy SQL expression construct is very much the kind of structure you\u2019d create if you were\nparsing a SQL statement\u2014it\u2019s a parse tree, except the developer creates the parse tree directly, rather\nthan deriving it from a string. The core type of node in this parse tree is called ClauseElement, and\nFigure 20.7 illustrates the relationship of ClauseElement to some key classes.\nClauseElement\nFromClause\nColumnClause\nColumnElement\nSelect\nCompoundSelect\nSelectBase\nJoin\nAlias\nTableClause\n_Label\n_BinaryExpression\nClauseList\n_BindParam\n_UnaryExpression\nUpdateBase\nValuesBase\nInsert\nUpdate\nDelete\nFigure 20.7: Basic expression hierarchy\nThrough the use of constructor functions, methods, and overloaded Python operator functions, a\nstructure for a statement like:\nSELECT id FROM user WHERE name = ?\nmight be constructed in Python like:\nfrom sqlalchemy.sql import table, column, select\nuser = table(\u2019user\u2019, column(\u2019id\u2019), column(\u2019name\u2019))\nstmt = select([user.c.id]).where(user.c.name==\u2019ed\u2019)\nThe structure of the above select construct is shown in Figure 20.8. Note the representation of\nthe literal value \u2019ed\u2019 is contained within the _BindParam construct, thus causing it to be rendered\nas a bound parameter marker in the SQL string using a question mark.\nMichael Bayer\n299\n", "page": 313, "type": "text", "section": "Page 313"}
{"text": "TableClause\nSelect\nname='user'\nColumnClause\nname='id'\nColumnClause\nname='name'\n_BindParam\nvalue='ed'\n_BinaryExpression\nleft\nright\noperator=eq\n_whereclause\n_raw_columns\ncolumns\n_froms\nFigure 20.8: Example expression tree\nFrom the tree diagram, one can see that a simple descending traversal through the nodes can\nquickly create a rendered SQL statement, as we\u2019ll see in greater detail in the section on statement\ncompilation.\nPython Operator Approach\nIn SQLAlchemy, an expression like this:\ncolumn(\u2019a\u2019) == 2\nproduces neither True nor False, but instead a SQL expression construct. The key to this is to\noverload operators using the Python special operator functions: e.g., methods like __eq__, __ne__,\n__le__, __lt__, __add__, __mul__. Column-oriented expression nodes provide overloaded Python\noperator behavior through the usage of a mixin called ColumnOperators. Using operator overloading,\nan expression column(\u2019a\u2019) == 2 is equivalent to:\nfrom sqlalchemy.sql.expression import _BinaryExpression\nfrom sqlalchemy.sql import column, bindparam\nfrom sqlalchemy.operators import eq\n_BinaryExpression(\nleft=column(\u2019a\u2019),\nright=bindparam(\u2019a\u2019, value=2, unique=True),\noperator=eq\n)\nThe eq construct is actually a function originating from the Python operator built-in. Representing\noperators as an object (i.e., operator.eq) rather than a string (i.e., =) allows the string representation\nto be de\ufb01ned at statement compilation time, when database dialect information is known.\n300\nSQLAlchemy\n", "page": 314, "type": "text", "section": "Page 314"}
{"text": "Compilation\nThe central class responsible for rendering SQL expression trees into textual SQL is the Compiled\nclass. This class has two primary subclasses, SQLCompiler and DDLCompiler. SQLCompiler handles\nSQL rendering operations for SELECT, INSERT, UPDATE, and DELETE statements, collectively\nclassi\ufb01ed as DQL (data query language) and DML (data manipulation language), while DDLCompiler\nhandles various CREATE and DROP statements, classi\ufb01ed as DDL (data de\ufb01nition language).\nThere is an additional class hierarchy focused around string representations of types, starting at\nTypeCompiler. Individual dialects then provide their own subclasses of all three compiler types to\nde\ufb01ne SQL language aspects speci\ufb01c to the target database. Figure 20.9 provides an overview of this\nclass hierarchy with respect to the PostgreSQL dialect.\nCompiled\nDDLCompiler\nSQLCompiler\nPGCompiler\nGenericType\nCompiler\nPGDDLCompiler\nPGTypeCompiler\nTypeCompiler\nsqlalchemy.engine\nsqlalchemy.sql.compiler\nsqlalchemy.dialects.postgresql\nFigure 20.9: Compiler hierarchy, including PostgreSQL-speci\ufb01c implementation\nThe Compiled subclasses de\ufb01ne a series of visit methods, each one referred to by a particular\nsubclass of ClauseElement. A hierarchy of ClauseElement nodes is walked and a statement is\nconstructed by recursively concatenating the string output of each visit function. As this proceeds,\nthe Compiled object maintains state regarding anonymous identi\ufb01er names, bound parameter names,\nand nesting of subqueries, among other things, all of which aim for the production of a string SQL\nstatement as well as a \ufb01nal collection of bound parameters with default values. Figure 20.10 illustrates\nthe process of visit methods resulting in textual units.\nlabel_select_columns()\nSELECT   id   FROM  user  WHERE       name      =            ?\nvisit_table()\nvisit_select()\nvisit_binary()\nvisit_column()\nvisit_bind_param()\nFigure 20.10: Call hierarchy of a statement compilation\nMichael Bayer\n301\n", "page": 315, "type": "text", "section": "Page 315"}
{"text": "A completed Compiled structure contains the full SQL string and collection of bound values.\nThese are coerced by an ExecutionContext into the format expected by the DBAPI\u2019s execute\nmethod, which includes such considerations as the treatment of a unicode statement object, the type\nof collection used to store bound values, as well as speci\ufb01cs on how the bound values themselves\nshould be coerced into representations appropriate to the DBAPI and target database.\n20.6\nClass Mapping with the ORM\nWe now shift our attention to the ORM. The \ufb01rst goal is to use the system of table metadata we\u2019ve\nde\ufb01ned to allow mapping of a user-de\ufb01ned class to a collection of columns in a database table.\nThe second goal is to allow the de\ufb01nition of relationships between user-de\ufb01ned classes, based on\nrelationships between tables in a database.\nSQLAlchemy refers to this as \u201cmapping\u201d, following the well known Data Mapper pattern de-\nscribed in Fowler\u2019s Patterns of Enterprise Architecture. Overall, the SQLAlchemy ORM draws\nheavily from the practices detailed by Fowler. It\u2019s also heavily in\ufb02uenced by the famous Java relational\nmapper Hibernate and Ian Bicking\u2019s SQLObject product for Python.\nClassical vs. Declarative\nWe use the term classical mapping to refer to SQLAlchemy\u2019s system of applying an object-relational\ndata mapping to an existing user class. This form considers the Table object and the user-de\ufb01ned class\nto be two individually de\ufb01ned entities which are joined together via a function called mapper. Once\nmapper has been applied to a user-de\ufb01ned class, the class takes on new attributes that correspond to\ncolumns in the table:\nclass User(object):\npass\nmapper(User, user_table)\n# now User has an \".id\" attribute\nUser.id\nmapper can also a\ufb03x other kinds of attributes to the class, including attributes which correspond to\nreferences to other kinds of objects, as well as arbitrary SQL expressions. The process of a\ufb03xing\narbitrary attributes to a class is known in the Python world as \u201cmonkeypatching\u201d; however, since we\nare doing it in a data-driven and non-arbitrary way, the spirit of the operation is better expressed\nwith the term class instrumentation.\nModern usage of SQLAlchemy centers around the Declarative extension, which is a con\ufb01gura-\ntional system that resembles the common active-record-like class declaration system used by many\nother object-relational tools. In this system, the end user explicitly de\ufb01nes attributes inline with the\nclass de\ufb01nition, each representing an attribute on the class that is to be mapped. The Table object,\nin most cases, is not mentioned explicitly, nor is the mapper function; only the class, the Column\nobjects, and other ORM-related attributes are named:\nclass User(Base):\n__tablename__ = \u2019user\u2019\nid = Column(Integer, primary_key=True)\n302\nSQLAlchemy\n", "page": 316, "type": "text", "section": "Page 316"}
{"text": "It may appear, above, that the class instrumentation is being achieved directly by our placement of\nid = Column(), but this is not the case. The Declarative extension uses a Python metaclass, which\nis a handy way to run a series of operations each time a new class is \ufb01rst declared, to generate a new\nTable object from what\u2019s been declared, and to pass it to the mapper function along with the class.\nThe mapper function then does its job in exactly the same way, patching its own attributes onto the\nclass, in this case towards the id attribute, and replacing what was there previously. By the time the\nmetaclass initialization is complete (that is, when the \ufb02ow of execution leaves the block delineated\nby User), the Column object marked by id has been moved into a new Table, and User.id has been\nreplaced by a new attribute speci\ufb01c to the mapping.\nIt was always intended that SQLAlchemy would have a shorthand, declarative form of con\ufb01gura-\ntion. However, the creation of Declarative was delayed in favor of continued work solidifying the\nmechanics of classical mapping. An interim extension called ActiveMapper, which later became the\nElixir project, existed early on. It rede\ufb01nes mapping constructs in a higher-level declaration system.\nDeclarative\u2019s goal was to reverse the direction of Elixir\u2019s heavily abstracted approach by establishing\na system that preserved SQLAlchemy classical mapping concepts almost exactly, only reorganizing\nhow they are used to be less verbose and more amenable to class-level extensions than a classical\nmapping would be.\nWhether classical or declarative mapping is used, a mapped class takes on new behaviors that\nallow it to express SQL constructs in terms of its attributes. SQLAlchemy originally followed\nSQLObject\u2019s behavior of using a special attribute as the source of SQL column expressions, referred\nto by SQLAlchemy as .c, as in this example:\nresult = session.query(User).filter(User.c.username == \u2019ed\u2019).all()\nIn version 0.4, however, SQLAlchemy moved the functionality into the mapped attributes them-\nselves:\nresult = session.query(User).filter(User.username == \u2019ed\u2019).all()\nThis change in attribute access proved to be a great improvement, as it allowed the column-\nlike objects present on the class to gain additional class-speci\ufb01c capabilities not present on those\noriginating directly from the underlying Table object. It also allowed usage integration between\ndi\ufb00erent kinds of class attributes, such as attributes which refer to table columns directly, attributes\nthat refer to SQL expressions derived from those columns, and attributes that refer to a related class.\nFinally, it provided a symmetry between a mapped class, and an instance of that mapped class, in\nthat the same attribute could take on di\ufb00erent behavior depending on the type of parent. Class-bound\nattributes return SQL expressions while instance-bound attributes return actual data.\nAnatomy of a Mapping\nThe id attribute that\u2019s been attached to our User class is a type of object known in Python as a\ndescriptor, an object that has __get__, __set__, and __del__ methods, which the Python runtime\ndefers to for all class and instance operations involving this attribute. SQLAlchemy\u2019s implementation\nis known as an InstrumentedAttribute, and we\u2019ll illustrate the world behind this facade with\nanother example. Starting with a Table and a user de\ufb01ned class, we set up a mapping that has just\none mapped column, as well as a relationship, which de\ufb01nes a reference to a related class:\nuser_table = Table(\"user\", metadata,\nColumn(\u2019id\u2019, Integer, primary_key=True),\nMichael Bayer\n303\n", "page": 317, "type": "text", "section": "Page 317"}
{"text": ")\nclass User(object):\npass\nmapper(User, user_table, properties={\n\u2019related\u2019:relationship(Address)\n})\nWhen the mapping is complete, the structure of objects related to the class is detailed in Fig-\nure 20.11.\nMapper\nInstrumented\nAttribute\nScalar\nAttributeImpl\nColumnProperty\nColumnLoader\nTable\nColumn\n__get__()\n__set__()\n__del__()\nRelationship\nProperty\nLazyLoader\nOneToManyDP\nInstrumented\nAttribute\nCollection\nAttributeImpl\n_sa_class_state/\nclass_\n__get__()\n__set__()\n__del__()\nmanager/\nmapper\nmapped_table\n_props\ncolumns\nproperty\nproperty\ncolumns\ntarget\nrelated mapper\nid\nrelated\n(dict)\nsqlalchemy.orm.instrumentation\nsqlalchemy.orm.attributes\nsqlalchemy.orm.mapper\nsqlalchemy.orm.properties\nsqlalchemy.schema\nUser\n(user-de\ufb01ned \nclass)\nClassManager\nsqlalchemy.orm.strategies\nsqlalchemy.orm.dependency\nFigure 20.11: Anatomy of a mapping\nThe \ufb01gure illustrates a SQLAlchemy mapping de\ufb01ned as two separate layers of interaction\nbetween the user-de\ufb01ned class and the table metadata to which it is mapped. Class instrumentation\nis pictured towards the left, while SQL and database functionality is pictured towards the right.\nThe general pattern at play is that object composition is used to isolate behavioral roles, and object\ninheritance is used to distinguish amongst behavioral variances within a particular role.\nWithin the realm of class instrumentation, the ClassManager is linked to the mapped class,\nwhile its collection of InstrumentedAttribute objects are linked to each attribute mapped on the\nclass. InstrumentedAttribute is also the public-facing Python descriptor mentioned previously,\nand produces SQL expressions when used in a class-based expression (e.g., User.id==5). When\ndealing with an instance of User, InstrumentedAttribute delegates the behavior of the attribute\nto an AttributeImpl object, which is one of several varieties tailored towards the type of data being\nrepresented.\n304\nSQLAlchemy\n", "page": 318, "type": "text", "section": "Page 318"}
{"text": "Towards the mapping side, the Mapper represents the linkage of a user-de\ufb01ned class and a\nselectable unit, most typically Table. Mapper maintains a collection of per-attribute objects known\nas MapperProperty, which deals with the SQL representation of a particular attribute. The most\ncommon variants of MapperProperty are ColumnProperty, representing a mapped column or SQL\nexpression, and RelationshipProperty, representing a linkage to another mapper.\nMapperProperty delegates attribute loading behavior\u2014including how the attribute renders in a\nSQL statement and how it is populated from a result row\u2014to a LoaderStrategy object, of which\nthere are several varieties. Di\ufb00erent LoaderStrategies determine if the loading behavior of an\nattribute is deferred, eager, or immediate. A default version is chosen at mapper con\ufb01guration\ntime, with the option to use an alternate strategy at query time. RelationshipProperty also\nreferences a DependencyProcessor, which handles how inter-mapper dependencies and attribute\nsynchronization should proceed at \ufb02ush time. The choice of DependencyProcessor is based on the\nrelational geometry of the parent and target selectables linked to the relationship.\nThe Mapper/RelationshipProperty structure forms a graph, where Mapper objects are nodes\nand RelationshipProperty objects are directed edges. Once the full set of mappers have been\ndeclared by an application, a deferred \u201cinitialization\u201d step known as the con\ufb01guration proceeds. It is\nused mainly by each RelationshipProperty to solidify the details between its parent and target\nmappers, including choice of AttributeImpl as well as DependencyProcessor. This graph is a\nkey data structure used throughout the operation of the ORM. It participates in operations such as\nthe so-called \u201ccascade\u201d behavior that de\ufb01nes how operations should propagate along object paths, in\nquery operations where related objects and collections are \u201ceagerly\u201d loaded at once, as well as on\nthe object \ufb02ushing side where a dependency graph of all objects is established before \ufb01ring o\ufb00a\nseries of persistence steps.\n20.7\nQuery and Loading Behavior\nSQLAlchemy initiates all object loading behavior via an object called Query. The basic state Query\nstarts with includes the entities, which is the list of mapped classes and/or individual SQL expressions\nto be queried. It also has a reference to the Session, which represents connectivity to one or more\ndatabases, as well as a cache of data that\u2019s been accumulated with respect to transactions on those\nconnections. Below is a rudimentary usage example:\nfrom sqlalchemy.orm import Session\nsession = Session(engine)\nquery = session.query(User)\nWe create a Query that will yield instances of User, relative to a new Session we\u2019ve created.\nQuery provides a generative builder pattern in the same way as the select construct discussed\npreviously, where additional criteria and modi\ufb01ers are associated with a statement construct one\nmethod call at a time. When an iterative operation is called on the Query, it constructs a SQL\nexpression construct representing a SELECT, emits it to the database, and then interprets the result\nset rows as ORM-oriented results corresponding to the initial set of entities being requested.\nQuery makes a hard distinction between the SQL rendering and the data loading portions of the\noperation. The former refers to the construction of a SELECT statement, the latter to the interpretation\nof SQL result rows into ORM-mapped constructs. Data loading can, in fact, proceed without a SQL\nrendering step, as the Query may be asked to interpret results from a textual query hand-composed\nby the user.\nMichael Bayer\n305\n", "page": 319, "type": "text", "section": "Page 319"}
{"text": "Both SQL rendering and data loading utilize a recursive descent through the graph formed by the\nseries of lead Mapper objects, considering each column- or SQL-expression-holding ColumnProperty\nas a leaf node and each RelationshipProperty which is to be included in the query via a so-called\n\u201ceager-load\u201d as an edge leading to another Mapper node. The traversal and action to take at each\nnode is ultimately the job of each LoaderStrategy associated with every MapperProperty, adding\ncolumns and joins to the SELECT statement being built in the SQL rendering phase, and producing\nPython functions that process result rows in the data loading phase.\nThe Python functions produced in the data loading phase each receive a database row as they\nare fetched, and produce a possible change in the state of a mapped attribute in memory as a result.\nThey are produced for a particular attribute conditionally, based on examination of the \ufb01rst incoming\nrow in the result set, as well as on loading options. If a load of the attribute is not to proceed, no\ncallable function is produced.\nFigure 20.12 illustrates the traversal of several LoaderStrategy objects in a joined eager load-\ning scenario, illustrating their connection to a rendered SQL statement which occurs during the\n_compile_context method of Query. It also shows generation of row population functions which\nreceive result rows and populate individual object attributes, a process which occurs within the\ninstances method of Query.\nLOAD TIME\nQUERY TIME\nColumnLoader\nSELECT\n \n    user.id \n             as user_id, \n    address.id \n            as address_id, \n    address.user_id \n            as address_user_id,\n \n    address.street \n            as address_street\nFROM user \n    LEFT OUTER JOIN address \n    ON user.id=\n       address.user_id\nWHERE user.id =5 \nname=\"id\"\nJoinedLoader\nname=\"related\"\nColumnLoader\nname=\"id\"\nColumnLoader\nname=\"user_id\"\nColumnLoader\nname=\"street\"\nQuery._compile_context()\nQuery.instances()\nMapper._instance_processor()(row, context)\nUser\nAddress\nid\nstreet\nrelated\nuser_id\nid\n renders column\nrenders column\nrenders column\nrenders column\nfn(row, context)\nfn(row, context)\nfn(row, context)\nfn(row, context)\nfn(row, context)\ncollection\nRESULTS\nMapperProperty.setup(context)\nMapper._instance_processor()(row, context)\nMapperProperty.setup(context)\nFigure 20.12: Traversal of loader strategies including a joined eager load\n306\nSQLAlchemy\n", "page": 320, "type": "text", "section": "Page 320"}
{"text": "SQLAlchemy\u2019s early approach to populating results used a traditional traversal of \ufb01xed object\nmethods associated with each strategy to receive each row and act accordingly. The loader callable\nsystem, \ufb01rst introduced in version 0.5, represented a dramatic leap in performance, as many decisions\nregarding row handling could be made just once up front instead of for each row, and a signi\ufb01cant\nnumber of function calls with no net e\ufb00ect could be eliminated.\n20.8\nSession/Identity Map\nIn SQLAlchemy, the Session object presents the public interface for the actual usage of the ORM\u2014\nthat is, loading and persisting data. It provides the starting point for queries and persistence operations\nfor a given database connection.\nThe Session, in addition to serving as the gateway for database connectivity, maintains an active\nreference to the set of all mapped entities which are present in memory relative to that Session. It\u2019s\nin this way that the Session implements a facade for the identity map and unit of work patterns,\nboth identi\ufb01ed by Fowler. The identity map maintains a database-identity-unique mapping of all\nobjects for a particular Session, eliminating the problems introduced by duplicate identities. The\nunit of work builds on the identity map to provide a system of automating the process of persisting\nall changes in state to the database in the most e\ufb00ective manner possible. The actual persistence step\nis known as a \u201c\ufb02ush\u201d, and in modern SQLAlchemy this step is usually automatic.\nDevelopment History\nThe Session started out as a mostly concealed system responsible for the single task of emitting a\n\ufb02ush. The \ufb02ush process involves emitting SQL statements to the database, corresponding to changes\nin the state of objects tracked by the unit of work system and thereby synchronizing the current\nstate of the database with what\u2019s in memory. The \ufb02ush has always been one of the most complex\noperations performed by SQLAlchemy.\nThe invocation of \ufb02ush started out in very early versions behind a method called commit, and\nit was a method present on an implicit, thread-local object called objectstore. When one used\nSQLAlchemy 0.1, there was no need to call Session.add, nor was there any concept of an explicit\nSession at all. The only user-facing steps were to create mappers, create new objects, modify\nexisting objects loaded through queries (where the queries themselves were invoked directly from\neach Mapper object), and then persist all changes via the objectstore.commit command. The pool\nof objects for a set of operations was unconditionally module-global and unconditionally thread-local.\nThe objectstore.commit model was an immediate hit with the \ufb01rst group of users, but the\nrigidity of this model quickly ran into a wall. Users new to modern SQLAlchemy sometimes lament\nthe need to de\ufb01ne a factory, and possibly a registry, for Session objects, as well as the need to keep\ntheir objects organized into just one Session at a time, but this is far preferable to the early days\nwhen the entire system was completely implicit. The convenience of the 0.1 usage pattern is still\nlargely present in modern SQLAlchemy, which features a session registry normally con\ufb01gured to\nuse thread local scoping.\nThe Session itself was only introduced in version 0.2 of SQLAlchemy, modeled loosely after\nthe Session object present in Hibernate. This version featured integrated transactional control,\nwhere the Session could be placed into a transaction via the begin method, and completed via\nthe commit method. The objectstore.commit method was renamed to objectstore.flush, and\nnew Session objects could be created at any time. The Session itself was broken o\ufb00from another\nMichael Bayer\n307\n", "page": 321, "type": "text", "section": "Page 321"}
{"text": "object called UnitOfWork, which remains as a private object responsible for executing the actual\n\ufb02ush operation.\nWhile the \ufb02ush process started as a method explicitly invoked by the user, the 0.4 series of\nSQLAlchemy introduced the concept of auto\ufb02ush, which meant that a \ufb02ush was emitted immediately\nbefore each query. The advantage of auto\ufb02ush is that the SQL statement emitted by a query always\nhas access on the relational side to the exact state that is present in memory, as all changes have been\nsent over. Early versions of SQLAlchemy couldn\u2019t include this feature, because the most common\npattern of usage was that the \ufb02ush statement would also commit the changes permanently. But when\nauto\ufb02ush was introduced, it was accompanied by another feature called the transactional Session,\nwhich provided a Session that would start out automatically in a transaction that remained until\nthe user called commit explicitly. With the introduction of this feature, the flush method no longer\ncommitted the data that it \ufb02ushed, and could safely be called on an automated basis. The Session\ncould now provide a step-by-step synchronization between in-memory state and SQL query state by\n\ufb02ushing as needed, with nothing permanently persisted until the explicit commit step. This behavior\nis, in fact, exactly the same in Hibernate for Java. However, SQLAlchemy embraced this style of\nusage based on the same behavior in the Storm ORM for Python, introduced when SQLAlchemy\nwas in version 0.3.\nVersion 0.5 brought more transaction integration when post-transaction expiration was intro-\nduced; after each commit or rollback, by default all states within the Session are expired (erased),\nto be populated again when subsequent SQL statements re-select the data, or when the attributes on\nthe remaining set of expired objects are accessed in the context of the new transaction. Originally,\nSQLAlchemy was constructed around the assumption that SELECT statements should be emitted as\nlittle as possible, unconditionally. The expire-on-commit behavior was slow in coming for this reason;\nhowever, it entirely solved the issue of the Session which contained stale data post-transaction with\nno simple way to load newer data without rebuilding the full set of objects already loaded. Early on,\nit seemed that this problem couldn\u2019t be reasonably solved, as it wasn\u2019t apparent when the Session\nshould consider the current state to be stale, and thus produce an expensive new set of SELECT\nstatements on the next access. However, once the Session moved to an always-in-a-transaction\nmodel, the point of transaction end became apparent as the natural point of data expiration, as\nthe nature of a transaction with a high degree of isolation is that it cannot see new data until it\u2019s\ncommitted or rolled back anyway. Di\ufb00erent databases and con\ufb01gurations, of course, have varied\ndegrees of transaction isolation, including no transactions at all. These modes of usage are entirely\nacceptable with SQLAlchemy\u2019s expiration model; the developer only needs to be aware that a lower\nisolation level may expose un-isolated changes within a Session if multiple Sessions share the same\nrows. This is not at all di\ufb00erent from what can occur when using two database connections directly.\nSession Overview\nFigure 20.13 illustrates a Session and the primary structures it deals with.\nThe public-facing portions above are the Session itself and the collection of user objects, each\nof which is an instance of a mapped class. Here we see that mapped objects keep a reference to a\nSQLAlchemy construct called InstanceState, which tracks ORM state for an individual instance\nincluding pending attribute changes and attribute expiration status. InstanceState is the instance-\nlevel side of the attribute instrumentation discussed in the preceding section, Anatomy of a Mapping,\ncorresponding to the ClassManager at the class level, and maintaining the state of the mapped\nobject\u2019s dictionary (i.e., the Python __dict__ attribute) on behalf of the AttributeImpl objects\nassociated with the class.\n308\nSQLAlchemy\n", "page": 322, "type": "text", "section": "Page 322"}
{"text": "Session\nIdentityMap\n(dict of key->state)\nSessionTransaction\nConnection\ndatabase\nvia\nDBAPI\nInstanceState\n(persistent)\nuser object\nInstanceState\n(persistent+ \ndeleted)\nInstanceState\n(pending)\nInstanceState\n(transient)\nuser object\nuser object\nuser object\nnew\ndeleted\nidentity_map\ntransaction\n1..n\nweakref\nweakref\nweakref\nweakref\nmodi\ufb01ed\nFigure 20.13: Session overview\nState Tracking\nThe IdentityMap is a mapping of database identities to InstanceState objects, for those objects\nwhich have a database identity, which are referred to as persistent. The default implementation\nof IdentityMap works with InstanceState to self-manage its size by removing user-mapped\ninstances once all strong references to them have been removed\u2014in this way it works in the same\nway as Python\u2019s WeakValueDictionary. The Session protects the set of all objects marked as\ndirty or deleted, as well as pending objects marked new, from garbage collection, by creating strong\nreferences to those objects with pending changes. All strong references are then discarded after the\n\ufb02ush.\nInstanceState also performs the critical task of maintaining \u201cwhat\u2019s changed\u201d for the attributes\nof a particular object, using a move-on-change system that stores the \u201cprevious\u201d value of a particular\nattribute in a dictionary called committed_state before assigning the incoming value to the object\u2019s\ncurrent dictionary. At \ufb02ush time, the contents of committed_state and the __dict__ associated\nwith the object are compared to produce the set of net changes on each object.\nIn the case of collections,\na separate collections package coordinates with the\nInstrumentedAttribute/InstanceState system to maintain a collection of net changes to a par-\nticular mapped collection of objects. Common Python classes such as set, list and dict are\nsubclassed before use and augmented with history-tracking mutator methods. The collection system\nwas reworked in 0.4 to be open ended and usable for any collection-like object.\nTransactional Control\nSession, in its default state of usage, maintains an open transaction for all operations which is\ncompleted when commit or rollback is called. The SessionTransaction maintains a set of\nzero or more Connection objects, each representing an open transaction on a particular database.\nSessionTransaction is a lazy-initializing object that begins with no database state present. As a\nparticular backend is required to participate in a statement execution, a Connection corresponding\nto that database is added to SessionTransaction\u2019s list of connections. While a single connection\nat a time is common, the multiple connection scenario is supported where the speci\ufb01c connection\nused for a particular operation is determined based on con\ufb01gurations associated with the Table,\nMichael Bayer\n309\n", "page": 323, "type": "text", "section": "Page 323"}
{"text": "Mapper, or SQL construct itself involved in the operation. Multiple connections can also coordinate\nthe transaction using two-phase behavior, for those DBAPIs which provide it.\n20.9\nUnit of Work\nThe flush method provided by Session turns over its work to a separate module called unitofwork.\nAs mentioned earlier, the \ufb02ush process is probably the most complex function of SQLAlchemy.\nThe job of the unit of work is to move all of the pending state present in a particular Session out\nto the database, emptying out the new, dirty, and deleted collections maintained by the Session.\nOnce completed, the in-memory state of the Session and what\u2019s present in the current transaction\nmatch. The primary challenge is to determine the correct series of persistence steps, and then to\nperform them in the correct order. This includes determining the list of INSERT, UPDATE, and\nDELETE statements, including those resulting from the cascade of a related row being deleted or\notherwise moved; ensuring that UPDATE statements contain only those columns which were actually\nmodi\ufb01ed; establishing \u201csynchronization\u201d operations that will copy the state of primary key columns\nover to referencing foreign key columns, at the point at which newly generated primary key identi\ufb01ers\nare available; ensuring that INSERTs occur in the order in which objects were added to the Session\nand as e\ufb03ciently as possible; and ensuring that UPDATE and DELETE statements occur within a\ndeterministic ordering so as to reduce the chance of deadlocks.\nHistory\nThe unit of work implementation began as a tangled system of structures that was written in an ad hoc\nway; its development can be compared to \ufb01nding the way out of a forest without a map. Early bugs\nand missing behaviors were solved with bolted-on \ufb01xes, and while several refactorings improved\nmatters through version 0.5, it was not until version 0.6 that the unit of work\u2014by that time stable,\nwell-understood, and covered by hundreds of tests\u2014could be rewritten entirely from scratch. After\nmany weeks of considering a new approach that would be driven by consistent data structures, the\nprocess of rewriting it to use this new model took only a few days, as the idea was by this time well\nunderstood. It was also greatly helped by the fact that the new implementation\u2019s behavior could be\ncarefully cross-checked against the existing version. This process shows how the \ufb01rst iteration of\nsomething, however awful, is still valuable as long as it provides a working model. It further shows\nhow total rewrites of a subsystem is often not only appropriate, but an integral part of development\nfor hard-to-develop systems.\nTopological Sort\nThe key paradigm behind the unit of work is that of assembling the full list of actions to be taken into\na data structure, with each node representing a single step; this is known in design patterns parlance\nas the command pattern. The series of \u201ccommands\u201d within this structure is then organized into a\nspeci\ufb01c ordering using a topological sort. A topological sort is a process that sorts items based on\na partial ordering, that is, only certain elements must precede others. Figure 20.14 illustrates the\nbehavior of the topological sort.\nThe unit of work constructs a partial ordering based on those persistence commands which must\nprecede others. The commands are then topologically sorted and invoked in order. The determination\nof which commands precede which is derived primarily from the presence of a relationship that\n310\nSQLAlchemy\n", "page": 324, "type": "text", "section": "Page 324"}
{"text": ",     ,     , \nA\nD\nC\n(      ,      )\n(      ,      )\nB\nC\n(      ,      )\nA\nD\nB\nC\nA\nD\nA\nC\nB\nPartial Ordering\nTopologically Sorted Sets\n\"A\" comes\nbefore \"C\"\n\"B\" comes\nbefore \"C\"\n\"A\" comes\nbefore \"D\"\nC\nA\nD\nB\nC\nB\nD\nA\n,     ,     , \n,     ,     , \n,     ,     , \n,     ,     , \nC\nD\nB\nA\n.  .  . etc\nFigure 20.14: Topological sort\nbridges two Mapper objects\u2014generally, one Mapper is considered to be dependent on the other, as\nthe relationship implies that one Mapper has a foreign key dependency on the other. Similar rules\nexist for many-to-many association tables, but here we focus on the case of one-to-many/many-to-one\nrelationships. Foreign key dependencies are resolved in order to prevent constraint violations from\noccurring, with no reliance on needing to mark constraints as \u201cdeferred\u201d. But just as importantly,\nthe ordering allows primary key identi\ufb01ers, which on many platforms are only generated when an\nINSERT actually occurs, to be populated from a just-executed INSERT statement\u2019s result into the\nparameter list of a dependent row that\u2019s about to be inserted. For deletes, the same ordering is used\nin reverse\u2014dependent rows are deleted before those on which they depend, as these rows cannot be\npresent without the referent of their foreign key being present.\nThe unit of work features a system where the topological sort is performed at two di\ufb00erent levels,\nbased on the structure of dependencies present. The \ufb01rst level organizes persistence steps into buckets\nbased on the dependencies between mappers, that is, full \u201cbuckets\u201d of objects corresponding to a\nparticular class. The second level breaks up zero or more of these \u201cbuckets\u201d into smaller batches,\nto handle the case of reference cycles or self-referring tables. Figure 20.15 illustrates the \u201cbuckets\u201d\ngenerated to insert a set of User objects, then a set of Address objects, where an intermediary\nstep copies newly generated User primary key values into the user_id foreign key column of each\nAddress object.\nIn the per-mapper sorting situation, any number of User and Address objects can be \ufb02ushed\nwith no impact on the complexity of steps or how many \u201cdependencies\u201d must be considered.\nThe second level of sorting organizes persistence steps based on direct dependencies between\nindividual objects within the scope of a single mapper. The simplest example of when this occurs\nis a table which contains a foreign key constraint to itself; a particular row in the table needs to be\ninserted before another row in the same table which refers to it. Another is when a series of tables\nhave a reference cycle: table A references table B, which references table C, that then references\ntable A. Some A objects must be inserted before others so as to allow the B and C objects to also be\nMichael Bayer\n311\n", "page": 325, "type": "text", "section": "Page 325"}
{"text": "SaveUpdateAll\n(User)\nProcessAll\n(User->Address)\nSaveUpdateAll\n(Address)\nINSERT INTO user\nINSERT INTO user\nINSERT INTO address\nINSERT INTO address\ncopy user.id to \naddress.user_id\ncopy user.id to \naddress.user_id\nDependency:\n(user, address)\nTopological Sort\nDONE\nFigure 20.15: Organizing objects by mapper\ninserted. The table that refers to itself is a special case of reference cycle.\nTo determine which operations can remain in their aggregated, per-Mapper buckets, and which\nwill be broken into a larger set of per-object commands, a cycle detection algorithm is applied to\nthe set of dependencies that exist between mappers, using a modi\ufb01ed version of a cycle detection\nalgorithm found on Guido Van Rossum\u2019s blog1. Those buckets involved in cycles are are then broken\nup into per-object operations and mixed into the collection of per-mapper buckets through the addition\nof new dependency rules from the per-object buckets back to the per-mapper buckets. Figure 20.16\nillustrates the bucket of User objects being broken up into individual per-object commands, resulting\nfrom the addition of a new relationship from User to itself called contact.\nThe rationale behind the bucket structure is that it allows batching of common statements as\nmuch as possible, both reducing the number of steps required in Python and making possible more\ne\ufb03cient interactions with the DBAPI, which can sometimes execute thousands of statements within\na single Python method call. Only when a reference cycle exists between mappers does the more\nexpensive per-object-dependency pattern kick in, and even then it only occurs for those portions of\nthe object graph which require it.\n1http://neopythonic.blogspot.com/2009/01/detecting-cycles-in-directed-graph.html\n312\nSQLAlchemy\n", "page": 326, "type": "text", "section": "Page 326"}
{"text": "Dependency:\n(user, address)\nTopological Sort\nDependency:\n(user, user)\nDONE\nSaveUpdateState\nINSERT INTO user\nSaveUpdateState\nINSERT INTO user\nProcessState\n(User->User)\ncopy user.id to \nuser.contact_id\nProcessAll\n(User->Address)\ncopy user.id to \naddress.user_id\ncopy user.id to \naddress.user_id\nSaveUpdateAll\n(Address)\nINSERT INTO address\nINSERT INTO address\nFigure 20.16: Organizing reference cycles into individual steps\n20.10\nConclusion\nSQLAlchemy has aimed very high since its inception, with the goal of being the most feature-rich and\nversatile database product possible. It has done so while maintaining its focus on relational databases,\nrecognizing that supporting the usefulness of relational databases in a deep and comprehensive way\nis a major undertaking; and even now, the scope of the undertaking continues to reveal itself as larger\nthan previously perceived.\nThe component-based approach is intended to extract the most value possible from each area of\nfunctionality, providing many di\ufb00erent units that applications can use alone or in combination. This\nsystem has been challenging to create, maintain, and deliver.\nThe development course was intended to be slow, based on the theory that a methodical, broad-\nbased construction of solid functionality is ultimately more valuable than fast delivery of features\nwithout foundation. It has taken a long time for SQLAlchemy to construct a consistent and well-\ndocumented user story, but throughout the process, the underlying architecture was always a step\nahead, leading in some cases to the \u201ctime machine\u201d e\ufb00ect where features can be added almost before\nusers request them.\nMichael Bayer\n313\n", "page": 327, "type": "text", "section": "Page 327"}
{"text": "The Python language has been a reliable host (if a little \ufb01nicky, particularly in the area of\nperformance). The language\u2019s consistency and tremendously open run-time model has allowed\nSQLAlchemy to provide a nicer experience than that o\ufb00ered by similar products written in other\nlanguages.\nIt is the hope of the SQLAlchemy project that Python gain ever-deeper acceptance into as wide a\nvariety of \ufb01elds and industries as possible, and that the use of relational databases remains vibrant\nand progressive. The goal of SQLAlchemy is to demonstrate that relational databases, Python, and\nwell-considered object models are all very much worthwhile development tools.\n314\nSQLAlchemy\n", "page": 328, "type": "text", "section": "Page 328"}
{"text": "[chapter21]\nTwisted\nJessica McKellar\nTwisted is an event-driven networking engine in Python. It was born in the early 2000s, when the\nwriters of networked games had few scalable and no cross-platform libraries, in any language, at\ntheir disposal. The authors of Twisted tried to develop games in the existing networking landscape,\nstruggled, saw a clear need for a scalable, event-driven, cross-platform networking framework and\ndecided to make one happen, learning from the mistakes and hardships of past game and networked\napplication writers.\nTwisted supports many common transport and application layer protocols, including TCP, UDP,\nSSL/TLS, HTTP, IMAP, SSH, IRC, and FTP. Like the language in which it is written, it is \u201cbatteries-\nincluded\u201d; Twisted comes with client and server implementations for all of its protocols, as well as\nutilities that make it easy to con\ufb01gure and deploy production-grade Twisted applications from the\ncommand line.\n21.1\nWhy Twisted?\nIn 2000, glyph, the creator of Twisted, was working on a text-based multiplayer game called Twisted\nReality. It was a big mess of threads, 3 per connection, in Java. There was a thread for input that\nwould block on reads, a thread for output that would block on some kind of write, and a \u201clogic\u201d thread\nthat would sleep while waiting for timers to expire or events to queue. As players moved through the\nvirtual landscape and interacted, threads were deadlocking, caches were getting corrupted, and the\nlocking logic was never quite right\u2014the use of threads had made the software complicated, buggy,\nand hard to scale.\nSeeking alternatives, he discovered Python, and in particular Python\u2019s select module for multi-\nplexing I/O from stream objects like sockets and pipes1; at the time, Java didn\u2019t expose the operating\nsystem\u2019s select interface or any other asynchronous I/O API2. A quick prototype of the game in\nPython using select immediately proved less complex and more reliable than the threaded version.\nAn instant convert to Python, select, and event-driven programming, glyph wrote a client and\nserver for the game in Python using the select API. But then he wanted to do more. Fundamentally,\nhe wanted to be able to turn network activity into method calls on objects in the game. What if you\ncould receive email in the game, like the Nethack mailer daemon? What if every player in the game\n1The Single UNIX Speci\ufb01cation, Version 3 (SUSv3) describes the select API.\n2The java.nio package for non-blocking I/O was added in J2SE 1.4, released in 2002.\n", "page": 329, "type": "text", "section": "Page 329"}
{"text": "had a home page? Glyph found himself needing good Python IMAP and HTTP clients and servers\nthat used select.\nHe \ufb01rst turned to Medusa, a platform developed in the mid-\u201990s for writing networking servers\nin Python based on the asyncore module3. asyncore is an asynchronous socket handler that builds\na dispatcher and callback interface on top of the operating system\u2019s select API.\nThis was an inspiring \ufb01nd for glyph, but Medusa had two drawbacks:\n1. It was on its way towards being unmaintained by 2001 when glyph started working on Twisted\nReality.\n2. asyncore is such a thin wrapper around sockets that application programmers are still re-\nquired to manipulate sockets directly. This means portability is still the responsibility of the\nprogrammer. Additionally, at the time, asyncore\u2019s Windows support was buggy, and glyph\nknew that he wanted to run a GUI client on Windows.\nGlyph was facing the prospect of implementing a networking platform himself and realized that\nTwisted Reality had opened the door to a problem that was just as interesting as his game.\nOver time, Twisted Reality the game became Twisted the networking platform, which would do\nwhat existing networking platforms in Python didn\u2019t:\n\u2022 Use event-driven programming instead of multi-threaded programming.\n\u2022 Be cross-platform: provide a uniform interface to the event noti\ufb01cation systems exposed by\nmajor operating systems.\n\u2022 Be \u201cbatteries-included\u201d: provide implementations of popular application-layer protocols out\nof the box, so that Twisted is immediately useful to developers.\n\u2022 Conform to RFCs, and prove conformance with a robust test suite.\n\u2022 Make it easy to use multiple networking protocols together.\n\u2022 Be extensible.\n21.2\nThe Architecture of Twisted\nTwisted is an event-driven networking engine. Event-driven programming is so integral to Twisted\u2019s\ndesign philosophy that it is worth taking a moment to review what exactly event-driven programming\nmeans.\nEvent-driven programming is a programming paradigm in which program \ufb02ow is determined by\nexternal events. It is characterized by an event loop and the use of callbacks to trigger actions when\nevents happen. Two other common programming paradigms are (single-threaded) synchronous and\nmulti-threaded programming.\nLet\u2019s compare and contrast single-threaded, multi-threaded, and event-driven programming\nmodels with an example. Figure 21.1 shows the work done by a program over time under these three\nmodels. The program has three tasks to complete, each of which blocks while waiting for I/O to\n\ufb01nish. Time spent blocking on I/O is greyed out.\nIn the single-threaded synchronous version of the program, tasks are performed serially. If\none task blocks for a while on I/O, all of the other tasks have to wait until it \ufb01nishes and they are\nexecuted in turn. This de\ufb01nite order and serial processing are easy to reason about, but the program\nis unnecessarily slow if the tasks don\u2019t depend on each other, yet still have to wait for each other.\nIn the threaded version of the program, the three tasks that block while doing work are performed\nin separate threads of control. These threads are managed by the operating system and may run\n3http://www.nightmare.com/medusa/\n316\nTwisted\n", "page": 330, "type": "text", "section": "Page 330"}
{"text": "Figure 21.1: Threading models\nconcurrently on multiple processors or interleaved on a single processor. This allows progress to\nbe made by some threads while others are blocking on resources. This is often more time-e\ufb03cient\nthan the analogous synchronous program, but one has to write code to protect shared resources that\ncould be accessed concurrently from multiple threads. Multi-threaded programs can be harder to\nreason about because one now has to worry about thread safety via process serialization (locking),\nreentrancy, thread-local storage, or other mechanisms, which when implemented improperly can\nlead to subtle and painful bugs.\nThe event-driven version of the program interleaves the execution of the three tasks, but in a\nsingle thread of control. When performing I/O or other expensive operations, a callback is registered\nwith an event loop, and then execution continues while the I/O completes. The callback describes\nhow to handle an event once it has completed. The event loop polls for events and dispatches them\nas they arrive, to the callbacks that are waiting for them. This allows the program to make progress\nwhen it can without the use of additional threads. Event-driven programs can be easier to reason\nabout than multi-threaded programs because the programmer doesn\u2019t have to worry about thread\nsafety.\nJessica McKellar\n317\n", "page": 331, "type": "text", "section": "Page 331"}
{"text": "The event-driven model is often a good choice when there are:\n1. many tasks, that are...\n2. largely independent (so they don\u2019t have to communicate with or wait on each other), and...\n3. some of these tasks block while waiting on events.\nIt is also a good choice when an application has to share mutable data between tasks, because no\nsynchronization has to be performed.\nNetworking applications often have exactly these properties, which is what makes them such a\ngood \ufb01t for the event-driven programming model.\nReusing Existing Applications\nMany popular clients and servers for various networking protocols already existed when Twisted was\ncreated. Why did glyph not just use Apache, IRCd, BIND, OpenSSH, or any of the other pre-existing\napplications whose clients and servers would have to get re-implemented from scratch for Twisted?\nThe problem is that all of these server implementations have networking code written from scratch,\ntypically in C, with application code coupled directly to the networking layer. This makes them very\ndi\ufb03cult to use as libraries. They have to be treated as black boxes when used together, giving a\ndeveloper no chance to reuse code if he or she wanted to expose the same data over multiple protocols.\nAdditionally, the server and client implementations are often separate applications that don\u2019t share\ncode. Extending these applications and maintaining cross-platform client-server compatibility is\nharder than it needs to be.\nWith Twisted, the clients and servers are written in Python using a consistent interface. This\nmakes it is easy to write new clients and servers, to share code between clients and servers, to share\napplication logic between protocols, and to test one\u2019s code.\nThe Reactor\nTwisted implements the reactor design pattern, which describes demultiplexing and dispatching\nevents from multiple sources to their handlers in a single-threaded environment.\nThe core of Twisted is the reactor event loop. The reactor knows about network, \ufb01le system, and\ntimer events. It waits on and then handles these events, abstracting away platform-speci\ufb01c behavior\nand presenting interfaces to make responding to events anywhere in the network stack easy.\nThe reactor essentially accomplishes:\nwhile True:\ntimeout = time_until_next_timed_event()\nevents = wait_for_events(timeout)\nevents += timed_events_until(now())\nfor event in events:\nevent.process()\nA reactor based on the poll API4 is the current default on all platforms. Twisted additionally\nsupports a number of platform-speci\ufb01c high-volume multiplexing APIs. Platform-speci\ufb01c reactors\ninclude the KQueue reactor based on FreeBSD\u2019s kqueue mechanism, an epoll-based reactor for\nsystems supporting the epoll interface (currently Linux 2.6), and an IOCP reactor based on Windows\nInput/Output Completion Ports.\nExamples of polling implementation-dependent details that Twisted takes care of include:\n4The Single UNIX Speci\ufb01cation, Version 3 (SUSv3) describes the poll API.\n318\nTwisted\n", "page": 332, "type": "text", "section": "Page 332"}
{"text": "\u2022 Network and \ufb01lesystem limits.\n\u2022 Bu\ufb00ering behavior.\n\u2022 How to detect a dropped connection.\n\u2022 The values returned in error cases.\nTwisted\u2019s reactor implementation also takes care of using the underlying non-blocking APIs\ncorrectly and handling obscure edge cases correctly. Python doesn\u2019t expose the IOCP API at all, so\nTwisted maintains its own implementation.\nManaging Callback Chains\nCallbacks are a fundamental part of event-driven programming and are the way that the reactor\nindicates to an application that events have completed. As event-driven programs grow, handling\nboth the success and error cases for the events in one\u2019s application becomes increasingly complex.\nFailing to register an appropriate callback can leave a program blocking on event processing that\nwill never happen, and errors might have to propagate up a chain of callbacks from the networking\nstack through the layers of an application.\nLet\u2019s examine some of the pitfalls of event-driven programs by comparing synchronous and\nasynchronous versions of a toy URL fetching utility in Python-like pseudo-code:\nSynchronous URL fetcher:\nimport getPage\ndef processPage(page):\nprint page\ndef logError(error):\nprint error\ndef finishProcessing(value):\nprint \"Shutting down...\"\nexit(0)\nurl = \"http://google.com\"\ntry:\npage = getPage(url)\nprocessPage(page)\nexcept Error, e:\nlogError(error)\nfinally:\nfinishProcessing()\nAsynchronous URL fetcher:\nfrom twisted.internet import reactor\nimport getPage\ndef processPage(page):\nprint page\nfinishProcessing()\ndef logError(error):\nprint error\nfinishProcessing()\ndef finishProcessing(value):\nprint \"Shutting down...\"\nreactor.stop()\nurl = \"http://google.com\"\n# getPage takes: url,\n#\nsuccess callback, error callback\ngetPage(url, processPage, logError)\nreactor.run()\nIn the asynchronous URL fetcher, reactor.run() starts the reactor event loop. In both the syn-\nchronous and asynchronous versions, a hypothetical getPage function does the work of page retrieval.\nprocessPage is invoked if the retrieval is successful, and logError is invoked if an Exception is\nraised while attempting to retrieve the page. In either case, finishProcessing is called afterwards.\nThe callback to logError in the asynchronous version mirrors the except part of the try/except\nblock in the synchronous version. The callback to processPage mirrors else, and the unconditional\ncallback to finishProcessing mirrors finally.\nJessica McKellar\n319\n", "page": 333, "type": "text", "section": "Page 333"}
{"text": "In the synchronous version, by virtue of the structure of a try/except block exactly one of\nlogError and processPage is called, and finishProcessing is always called once; in the asyn-\nchronous version it is the programmer\u2019s responsibility to invoke the correct chain of success and\nerror callbacks. If, through programming error, the call to finishProcessing were left out of\nprocessPage or logError along their respective callback chains, the reactor would never get\nstopped and the program would run forever.\nThis toy example hints at the complexity frustrating programmers during the \ufb01rst few years\nof Twisted\u2019s development. Twisted responded to this complexity by growing an object called a\nDeferred.\nDeferreds\nThe Deferred object is an abstraction of the idea of a result that doesn\u2019t exist yet. It also helps\nmanage the callback chains for this result. When returned by a function, a Deferred is a promise\nthat the function will have a result at some point. That single returned Deferred contains references\nto all of the callbacks registered for an event, so only this one object needs to be passed between\nfunctions, which is much simpler to keep track of than managing callbacks individually.\nDeferreds have a pair of callback chains, one for success (callbacks) and one for errors (errbacks).\nDeferreds start out with two empty chains. One adds pairs of callbacks and errbacks to handle\nsuccesses and failures at each point in the event processing. When an asynchronous result arrives,\nthe Deferred is \u201c\ufb01red\u201d and the appropriate callbacks or errbacks are invoked in the order in which\nthey were added.\nHere is a version of the asynchronous URL fetcher pseudo-code which uses Deferreds:\nfrom twisted.internet import reactor\nimport getPage\ndef processPage(page):\nprint page\ndef logError(error):\nprint error\ndef finishProcessing(value):\nprint \"Shutting down...\"\nreactor.stop()\nurl = \"http://google.com\"\ndeferred = getPage(url) # getPage returns a Deferred\ndeferred.addCallbacks(processPage, logError)\ndeferred.addBoth(finishProcessing)\nreactor.run()\nIn this version, the same event handlers are invoked, but they are all registered with a single\nDeferred object instead of spread out in the code and passed as arguments to getPage.\nThe Deferred is created with two stages of callbacks. First, addCallbacks adds the processPage\ncallback and logError errback to the \ufb01rst stage of their respective chains. Then addBoth adds\nfinishProcessing to the second stage of both chains. Diagrammatically, the callback chains look\nlike Figure 21.2.\n320\nTwisted\n", "page": 334, "type": "text", "section": "Page 334"}
{"text": "Figure 21.2: Callback chains\nDeferreds can only be \ufb01red once; attempting to re-\ufb01re them will raise an Exception. This gives\nDeferreds semantics closer to those of the try/except blocks of their synchronous cousins, which\nmakes processing the asynchronous events easier to reason about and avoids subtle bugs caused by\ncallbacks being invoked more or less than once for a single event.\nUnderstanding Deferreds is an important part of understanding the \ufb02ow of Twisted programs.\nHowever, when using the high-level abstractions Twisted provides for networking protocols, one\noften doesn\u2019t have to use Deferreds directly at all.\nThe Deferred abstraction is powerful and has been borrowed by many other event-driven\nplatforms, including jQuery, Dojo, and Mochikit.\nTransports\nTransports represent the connection between two endpoints communicating over a network. Trans-\nports are responsible for describing connection details, like being stream- or datagram-oriented, \ufb02ow\ncontrol, and reliability. TCP, UDP, and Unix sockets are examples of transports. They are designed\nto be \u201cminimally functional units that are maximally reusable\u201d and are decoupled from protocol\nimplementations, allowing for many protocols to utilize the same type of transport. Transports\nimplement the ITransport interface, which has the following methods:\nJessica McKellar\n321\n", "page": 335, "type": "text", "section": "Page 335"}
{"text": "write\nWrite some data to the physical connection, in sequence, in a\nnon-blocking fashion.\nwriteSequence\nWrite a list of strings to the physical connection.\nloseConnection\nWrite all pending data and then close the connection.\ngetPeer\nGet the remote address of this connection.\ngetHost\nGet the address of this side of the connection.\nDecoupling transports from procotols also makes testing the two layers easier. A mock transport\ncan simply write data to a string for inspection.\nProtocols\nProcotols describe how to process network events asynchronously. HTTP, DNS, and IMAP are\nexamples of application protocols. Protocols implement the IProtocol interface, which has the\nfollowing methods:\nmakeConnection\nMake a connection to a transport and a server.\nconnectionMade\nCalled when a connection is made.\ndataReceived\nCalled whenever data is received.\nconnectionLost\nCalled when the connection is shut down.\nThe relationship between the reactor, protocols, and transports is best illustrated with an example.\nHere are complete implementations of an echo server and client, \ufb01rst the server:\nfrom twisted.internet import protocol, reactor\nclass Echo(protocol.Protocol):\ndef dataReceived(self, data):\n# As soon as any data is received, write it back\nself.transport.write(data)\nclass EchoFactory(protocol.Factory):\ndef buildProtocol(self, addr):\nreturn Echo()\nreactor.listenTCP(8000, EchoFactory())\nreactor.run()\nAnd the client:\nfrom twisted.internet import reactor, protocol\nclass EchoClient(protocol.Protocol):\ndef connectionMade(self):\nself.transport.write(\"hello, world!\")\ndef dataReceived(self, data):\nprint \"Server said:\", data\nself.transport.loseConnection()\ndef connectionLost(self, reason):\n322\nTwisted\n", "page": 336, "type": "text", "section": "Page 336"}
{"text": "print \"connection lost\"\nclass EchoFactory(protocol.ClientFactory):\ndef buildProtocol(self, addr):\nreturn EchoClient()\ndef clientConnectionFailed(self, connector, reason):\nprint \"Connection failed - goodbye!\"\nreactor.stop()\ndef clientConnectionLost(self, connector, reason):\nprint \"Connection lost - goodbye!\"\nreactor.stop()\nreactor.connectTCP(\"localhost\", 8000, EchoFactory())\nreactor.run()\nRunning the server script starts a TCP server listening for connections on port 8000. The server\nuses the Echo protocol, and data is written out over a TCP transport. Running the client makes a TCP\nconnection to the server, echoes the server response, and then terminates the connection and stops\nthe reactor. Factories are used to produce instances of protocols for both sides of the connection.\nThe communication is asynchronous on both sides; connectTCP takes care of registering callbacks\nwith the reactor to get noti\ufb01ed when data is available to read from a socket.\nApplications\nTwisted is an engine for producing scalable, cross-platform network servers and clients. Making it\neasy to deploy these applications in a standardized fashion in production environments is an important\npart of a platform like this getting wide-scale adoption.\nTo that end, Twisted developed the Twisted application infrastructure, a re-usable and con\ufb01gurable\nway to deploy a Twisted application. It allows a programmer to avoid boilerplate code by hooking an\napplication into existing tools for customizing the way it is run, including daemonization, logging,\nusing a custom reactor, pro\ufb01ling code, and more.\nThe application infrastructure has four main parts: Services, Applications, con\ufb01guration man-\nagement (via TAC \ufb01les and plugins), and the twistd command-line utility. To illustrate this infras-\ntructure, we\u2019ll turn the echo server from the previous section into an Application.\nService\nA Service is anything that can be started and stopped and which adheres to the IService interface.\nTwisted comes with service implementations for TCP, FTP, HTTP, SSH, DNS, and many other\nprotocols. Many Services can register with a single application.\nThe core of the IService interface is:\nstartService\nStart the service. This might include loading con\ufb01guration data,\nsetting up database connections, or listening on a port.\nstopService\nShut down the service. This might include saving state to disk,\nclosing database connections, or stopping listening on a port.\nJessica McKellar\n323\n", "page": 337, "type": "text", "section": "Page 337"}
{"text": "Our echo service uses TCP, so we can use Twisted\u2019s default TCPServer implementation of this\nIService interface.\nApplication\nAn Application is the top-level service that represents the entire Twisted application. Services register\nthemselves with an Application, and the twistd deployment utility described below searches for\nand runs Applications.\nWe\u2019ll create an echo Application with which the echo Service can register.\nTAC Files\nWhen managing Twisted applications in a regular Python \ufb01le, the developer is responsible for\nwriting code to start and stop the reactor and to con\ufb01gure the application. Under the Twisted\napplication infrastructure, protocol implementations live in a module, Services using those protocols\nare registered in a Twisted Application Con\ufb01guration (TAC) \ufb01le, and the reactor and con\ufb01guration\nare managed by an external utility.\nTo turn our echo server into an echo application, we can follow a simple algorithm:\n1. Move the Protocol parts of the echo server into their own module.\n2. Inside a TAC \ufb01le:\n(a) Create an echo Application.\n(b) Create an instance of the TCPServer Service which will use our EchoFactory, and\nregister it with the Application.\nThe code for managing the reactor will be taken care of by twistd, discussed below. The\napplication code ends up looking like this:\nThe echo.py \ufb01le:\nfrom twisted.internet import protocol, reactor\nclass Echo(protocol.Protocol):\ndef dataReceived(self, data):\nself.transport.write(data)\nclass EchoFactory(protocol.Factory):\ndef buildProtocol(self, addr):\nreturn Echo()\nThe echo_server.tac \ufb01le:\nfrom twisted.application import internet, service\nfrom echo import EchoFactory\napplication = service.Application(\"echo\")\nechoService = internet.TCPServer(8000, EchoFactory())\nechoService.setServiceParent(application)\n324\nTwisted\n", "page": 338, "type": "text", "section": "Page 338"}
{"text": "twistd\ntwistd (pronounced \u201ctwist-dee\u201d) is a cross-platform utility for deploying Twisted applications. It\nruns TAC \ufb01les and handles starting and stopping an application. As part of Twisted\u2019s batteries-\nincluded approach to network programming, twistd comes with a number of useful con\ufb01guration\n\ufb02ags, including daemonizing the application, the location of log \ufb01les, dropping privileges, running\nin a chroot, running under a non-default reactor, or even running the application under a pro\ufb01ler.\nWe can run our echo server Application with:\n$ twistd -y echo_server.tac\nIn this simplest case, twistd starts a daemonized instance of the application, logging to\ntwistd.log. After starting and stopping the application, the log looks like this:\n2011-11-19 22:23:07-0500 [-] Log opened.\n2011-11-19 22:23:07-0500 [-] twistd 11.0.0 (/usr/bin/python 2.7.1) starting up.\n2011-11-19 22:23:07-0500 [-] reactor class: twisted.internet.selectreactor.SelectReactor.\n2011-11-19 22:23:07-0500 [-] echo.EchoFactory starting on 8000\n2011-11-19 22:23:07-0500 [-] Starting factory <echo.EchoFactory instance at 0x12d8670>\n2011-11-19 22:23:20-0500 [-] Received SIGTERM, shutting down.\n2011-11-19 22:23:20-0500 [-] (TCP Port 8000 Closed)\n2011-11-19 22:23:20-0500 [-] Stopping factory <echo.EchoFactory instance at 0x12d8670>\n2011-11-19 22:23:20-0500 [-] Main loop terminated.\n2011-11-19 22:23:20-0500 [-] Server Shut Down.\nRunning a service using the Twisted application infrastructure allows developers to skip writ-\ning boilerplate code for common service functionalities like logging and daemonization. It also\nestablishes a standard command line interface for deploying applications.\nPlugins\nAn alternative to the TAC-based system for running Twisted applications is the plugin system.\nWhile the TAC system makes it easy to register simple hierarchies of pre-de\ufb01ned services within\nan application con\ufb01guration \ufb01le, the plugin system makes it easy to register custom services as\nsubcommands of the twistd utility, and to extend the command-line interface to an application.\nUsing this system:\n1. Only the plugin API is required to remain stable, which makes it easy for third-party developers\nto extend the software.\n2. Plugin discoverability is codi\ufb01ed. Plugins can be loaded and saved when a program is \ufb01rst run,\nre-discovered each time the program starts up, or polled for repeatedly at runtime, allowing\nthe discovery of new plugins installed after the program has started.\nTo extend a program using the Twisted plugin system, all one has to do is create objects which\nimplement the IPlugin interface and put them in a particular location where the plugin system\nknows to look for them.\nHaving already converted our echo server to a Twisted application, transformation into a Twisted\nplugin is straightforward. Alongside the echo module from before, which contains the Echo protocol\nand EchoFactory de\ufb01nitions, we add a directory called twisted, containing a subdirectory called\nplugins, containing our echo plugin de\ufb01nition. This plugin will allow us to start an echo server and\nspecify the port to use as arguments to the twistd utility:\nJessica McKellar\n325\n", "page": 339, "type": "text", "section": "Page 339"}
{"text": "from zope.interface import implements\nfrom twisted.python import usage\nfrom twisted.plugin import IPlugin\nfrom twisted.application.service import IServiceMaker\nfrom twisted.application import internet\nfrom echo import EchoFactory\nclass Options(usage.Options):\noptParameters = [[\"port\", \"p\", 8000, \"The port number to listen on.\"]]\nclass EchoServiceMaker(object):\nimplements(IServiceMaker, IPlugin)\ntapname = \"echo\"\ndescription = \"A TCP-based echo server.\"\noptions = Options\ndef makeService(self, options):\n\"\"\"\nConstruct a TCPServer from a factory defined in myproject.\n\"\"\"\nreturn internet.TCPServer(int(options[\"port\"]), EchoFactory())\nserviceMaker = EchoServiceMaker()\nOur echo server will now show up as a server option in the output of twistd --help, and running\ntwistd echo --port=1235 will start an echo server on port 1235.\nTwisted comes with a pluggable authentication system for servers called twisted.cred, and a\ncommon use of the plugin system is to add an authentication pattern to an application. One can use\ntwisted.cred AuthOptionMixin to add command-line support for various kinds of authentication\no\ufb00the shelf, or to add a new kind. For example, one could add authentication via a local Unix\npassword database or an LDAP server using the plugin system.\ntwistd comes with plugins for many of Twisted\u2019s supported protocols, which turns the work of\nspinning up a server into a single command. Here are some examples of twistd servers that ship\nwith Twisted:\ntwistd web --port 8080 --path .\nRun an HTTP server on port 8080, serving both static and dynamic content out of the current\nworking directory.\ntwistd dns -p 5553 --hosts-file=hosts\nRun a DNS server on port 5553, resolving domains out of a \ufb01le called hosts in the format of\n/etc/hosts.\nsudo twistd conch -p tcp:2222\nRun an ssh server on port 2222. ssh keys must be set up independently.\ntwistd mail -E -H localhost -d localhost=emails\nRun an ESMTP POP3 server, accepting email for localhost and saving it to the emails\ndirectory.\ntwistd makes it easy to spin up a server for testing clients, but it is also pluggable, production-\ngrade code.\n326\nTwisted\n", "page": 340, "type": "text", "section": "Page 340"}
{"text": "In that respect, Twisted\u2019s application deployment mechanisms via TAC \ufb01les, plugins, and twistd\nhave been a success. However, anecdotally, most large Twisted deployments end up having to rewrite\nsome of these management and monitoring facilities; the architecture does not quite expose what\nsystem administrators need. This is a re\ufb02ection of the fact that Twisted has not historically had\nmuch architectural input from system administrators\u2014the people who are experts at deploying and\nmaintaining applications.\nTwisted would be well-served to more aggressively solicit feedback from expert end users when\nmaking future architectural decisions in this space.\n21.3\nRetrospective and Lessons Learned\nTwisted recently celebrated its 10th anniversary. Since its inception, inspired by the networked game\nlandscape of the early 2000s, it has largely achieved its goal of being an extensible, cross-platform,\nevent-driven networking engine. Twisted is used in production environments at companies from\nGoogle and Lucas\ufb01lm to Justin.TV and the Launchpad software collaboration platform. Server\nimplementations in Twisted are the core of numerous other open source applications, including\nBuildBot, BitTorrent, and Tahoe-LAFS.\nTwisted has had few major architectural changes since its initial development. The one crucial\naddition was Deferred, as discussed above, for managing pending results and their callback chains.\nThere was one important removal, which has almost no footprint in the current implementation:\nTwisted Application Persistence.\nTwisted Application Persistence\nTwisted Application Persistence (TAP) was a way of keeping an application\u2019s con\ufb01guration and state\nin a pickle. Running an application using this scheme was a two-step process:\n1. Create the pickle that represents an Application, using the now defunct mktap utility.\n2. Use twistd to unpickle and run the Application.\nThis process was inspired by Smalltalk images, an aversion to the proliferation of seemingly ad\nhoc con\ufb01guration languages that were hard to script, and a desire to express con\ufb01guration details in\nPython.\nTAP \ufb01les immediately introduced unwanted complexity. Classes would change in Twisted without\ninstances of those classes getting changed in the pickle. Trying to use class methods or attributes\nfrom a newer version of Twisted on the pickled object would crash the application. The notion of\n\u201cupgraders\u201d that would upgrade pickles to new API versions was introduced, but then a matrix of\nupgraders, pickle versions, and unit tests had to be maintained to cover all possible upgrade paths,\nand comprehensively accounting for all interface changes was still hard and error-prone.\nTAPs and their associated utilities were abandoned and then eventually removed from Twisted\nand replaced with TAC \ufb01les and plugins. TAP was backronymed to Twisted Application Plugin, and\nfew traces of the failed pickling system exist in Twisted today.\nThe lesson learned from the TAP \ufb01asco was that to have reasonable maintainability, persistent\ndata needs an explicit schema. More generally, it was a lesson about adding complexity to a project:\nwhen considering introducing a novel system for solving a problem, make sure the complexity of that\nsolution is well understood and tested and that the bene\ufb01ts are clearly worth the added complexity\nbefore committing the project to it.\nJessica McKellar\n327\n", "page": 341, "type": "text", "section": "Page 341"}
{"text": "web2: a lesson on rewrites\nWhile not primarily an architectural decision, a project management decision about rewriting the\nTwisted Web implementation has had long-term rami\ufb01cations for Twisted\u2019s image and the maintainers\u2019\nability to make architectural improvements to other parts of the code base, and it deserves a short\ndiscussion.\nIn the mid-2000s, the Twisted developers decided to do a full rewrite of the twisted.web APIs as\na separate project in the Twisted code base called web2. web2 would contain numerous improvements\nover twisted.web, including full HTTP 1.1 support and a streaming data API.\nweb2 was labelled as experimental, but ended up getting used by major projects anyway and\nwas even accidentally released and packaged by Debian. Development on web and web2 continued\nconcurrently for years, and new users were perennially frustrated by the side-by-side existence of\nboth projects and a lack of clear messaging about which project to use. The switchover to web2 never\nhappened, and in 2011 web2 was \ufb01nally removed from the code base and the website. Some of the\nimprovements from web2 are slowly getting ported back to web.\nPartially because of web2, Twisted developed a reputation for being hard to navigate and struc-\nturally confusing to newcomers. Years later, the Twisted community still works hard to combat this\nimage.\nThe lesson learned from web2 was that rewriting a project from scratch is often a bad idea, but if\nit has to happen make sure that the developer community understands the long-term plan, and that\nthe user community has one clear choice of implementation to use during the rewrite.\nIf Twisted could go back and do web2 again, the developers would have done a series of backwards-\ncompatible changes and deprecations to twisted.web instead of a rewrite.\nKeeping Up with the Internet\nThe way that we use the Internet continues to evolve. The decision to implement many protocols\nas part of the core software burdens Twisted with maintaining code for all of those protocols.\nImplementations have to evolve with changing standards and the adoption of new protocols while\nmaintaining a strict backwards-compatibility policy.\nTwisted is primarily a volunteer-driven project, and the limiting factor for development is not\ncommunity enthusiasm, but rather volunteer time. For example, RFC 2616 de\ufb01ning HTTP 1.1 was\nreleased in 1999, work began on adding HTTP 1.1 support to Twisted\u2019s HTTP protocol implementa-\ntions in 2005, and the work was completed in 2009. Support for IPv6, de\ufb01ned in RFC 2460 in 1998,\nis in progress but unmerged as of 2011.\nImplementations also have to evolve as the interfaces exposed by supported operating systems\nchange. For example, the epoll event noti\ufb01cation facility was added to Linux 2.5.44 in 2002, and\nTwisted grew an epoll-based reactor to take advantage of this new API. In 2007, Apple released\nOS 10.5 Leopard with a poll implementation that didn\u2019t support devices, which was buggy enough\nbehavior for Apple to not expose select.poll in its build of Python5. Twisted has had to work\naround this issue and document it for users ever since.\nSometimes, Twisted development doesn\u2019t keep up with the changing networking landscape,\nand enhancements are moved to libraries outside of the core software. For example, the Wokkel\nproject6, a collection of enhancements to Twisted\u2019s Jabber/XMPP support, has lived as a to-be-\nmerged independent project for years without a champion to oversee the merge. An attempt was\n5http://twistedmatrix.com/trac/ticket/4173\n6http://wokkel.ik.nu/\n328\nTwisted\n", "page": 342, "type": "text", "section": "Page 342"}
{"text": "made to add WebSockets to Twisted as browsers began to adopt support for the new protocol in 2009,\nbut development moved to external projects after a decision not to include the protocol until it moved\nfrom an IETF draft to a standard.\nAll of this being said, the proliferation of libraries and add-ons is a testament to Twisted\u2019s \ufb02exibility\nand extensibility. A strict test-driven development policy and accompanying documentation and\ncoding standards help the project avoid regressions and preserve backwards compatibility while\nmaintaining a large matrix of supported protocols and platforms. It is a mature, stable project that\ncontinues to have very active development and adoption.\nTwisted looks forward to being the engine of your Internet for another ten years.\n329\n", "page": 343, "type": "text", "section": "Page 343"}
{"text": "330\nTwisted\n", "page": 344, "type": "text", "section": "Page 344"}
{"text": "[chapter22]\nYesod\nMichael Snoyman\nYesod is a web framework written in the Haskell programming language. While many popular web\nframeworks exploit the dynamic nature of their host languages, Yesod exploits the static nature of\nHaskell to produce safer, faster code.\nDevelopment began about two years ago and has been going strong ever since. Yesod cut its\nteeth on real life projects, with all of its initial features born out of an actual, real-life need. At \ufb01rst,\ndevelopment was almost entirely a one-man show. After about a year of development the community\ne\ufb00orts kicked in, and Yesod has since blossomed into a thriving open source project.\nDuring the embryonic phase, when Yesod was incredibly ephemeral and ill-de\ufb01ned, it would\nhave been counter-productive to try and get a team to work on it. By the time it stabilized enough to\nbe useful to others, it was the right time to \ufb01nd out the downsides to some of the decisions that had\nbeen made. Since then, we have made major changes to the user-facing API to make it more useful,\nand are quickly solidifying a 1.0 release.\nThe question you may ask is: Why another web framework? Let\u2019s instead redirect to a di\ufb00erent\nquestion: Why use Haskell? It seems that most of the world is happy with one of two styles of\nlanguage:\n\u2022 Statically typed languages, like Java, C# and C++. These languages provide speed and type\nsafety, but are more cumbersome to program with.\n\u2022 Dynamically typed languages, like Ruby and Python. These languages greatly increase produc-\ntivity (at least in the short run), but run slowly and have very little support from the compiler\nto ensure correctness. (The solution to this last point is unit testing. We\u2019ll get to that later.)\nThis is a false dichotomy. There\u2019s no reason why statically typed languages need to be so clumsy.\nHaskell is able to capture a huge amount of the expressivity of Ruby and Python, while remaining a\nstrongly typed language. In fact, Haskell\u2019s type system catches many more bugs than Java and its ilk.\nNull pointer exceptions are completely eliminated; immutable data structures simplify reasoning\nabout your code and simplify parallel and concurrent programming.\nSo why Haskell? It is an e\ufb03cient, developer-friendly language which provides many compile-time\nchecks of program correctness.\nThe goal of Yesod is to extend Haskell\u2019s strengths into web development. Yesod strives to\nmake your code as concise as possible. As much as possible, every line of your code is checked for\ncorrectness at compile time. Instead of requiring large libraries of unit tests to test basic properties,\nthe compiler does it all for you. Under the surface, Yesod uses as many advanced performance\ntechniques as we can muster to make your high-level code \ufb02y.\n", "page": 345, "type": "text", "section": "Page 345"}
{"text": "22.1\nCompared to Other Frameworks\nIn general terms, Yesod is more similar to than di\ufb00erent than the leading frameworks such as Rails\nand Django. It generally follows the Model-View-Controller (MVC) paradigm, has a templating\nsystem that separates view from logic, provides an Object-Relational Mapping (ORM) system, and\nhas a front controller approach to routing.\nThe devil is in the details. Yesod strives to push as much error catching to the compile phase\ninstead of runtime, and to automatically catch both bugs and security \ufb02aws through the type system.\nWhile Yesod tries to maintain a user-friendly, high-level API, it uses a number of newer techniques\nfrom the functional programming world to achieve high performance, and is not afraid to expose\nthese internals to developers.\nThe main architectural challenge in Yesod is balancing these two seemingly con\ufb02icting goals. For\nexample, there is nothing revolutionary about Yesod\u2019s approach to routing (called type-safe URLs1.\nHistorically, implementing such a solution was a tedious, error-prone process. Yesod\u2019s innovation is\nto use Template Haskell (a form of code generation) to automate the boilerplate required to bootstrap\nthe process. Similarly, type-safe HTML has been around for a long while; Yesod tries to keep the\ndeveloper-friendly aspect of common template languages while keeping the power of type safety.\n22.2\nWeb Application Interface\nA web application needs some way to communicate with a server. One possible approach is to bake\nthe server directly into the framework, but doing so necessarily limits your options for deployment\nand leads to poor interfaces. Many languages have created standard interfaces to address this issue:\nPython has WSGI and Ruby has Rack. In Haskell, we have WAI: Web Application Interface.\nWAI is not intended to be a high-level interface. It has two speci\ufb01c goals: generality and\nperformance. By staying general, WAI has been able to support backends for everything from\nstandalone servers to old school CGI and even works directly with Webkit to produce faux desktop\napplications. The performance side will introduce us to a number of the cool features of Haskell.\nDatatypes\nOne of the biggest advantages of Haskell\u2014and one of the things we make the most use of in Yesod\u2014\nis strong static typing. Before we begin to write the code for how to solve something, we need to\nthink about what the data will look like. WAI is a perfect example of this paradigm. The core concept\nwe want to express is that of an application. An application\u2019s most basic expression is a function that\ntakes a request and returns a response. In Haskell lingo:\ntype Application = Request -> Response\nThis just raises the question: what do Request and Response look like? A request has a number\nof pieces of information, but the most basic are the requested path, query string, request headers,\nand request body. And a response has just three components: a status code, response headers and\nresponse body.\nHow do we represent something like a query string? Haskell keeps a strict separation between\nbinary and textual data. The former is represented by ByteString, the latter by Text. Both are\n1http://www.yesodweb.com/blog/2012/01/aosa-chapter#file1414-routes\n332\nYesod\n", "page": 346, "type": "text", "section": "Page 346"}
{"text": "Figure 22.1: Overall structure of a Yesod application\nhighly optimized datatypes that provide a high-level, safe API. In the case of a query string we store\nthe raw bytes transferred over the wire as a ByteString and the parsed, decoded values as Text.\nStreaming\nA ByteString represents a single memory bu\ufb00er. If we were to naively use a plain ByteString for\nholding the entire request or response bodies, our applications could never scale to large requests\nor responses. Instead, we use a technique called enumerators, very similar in concept to generators\nin Python. Our Application becomes a consumer of a stream of ByteStrings representing the\nincoming request body, and a producer of a separate stream for the response.\nWe now need to slightly revise our de\ufb01nition of an Application. An Application will take a\nRequest value, containing headers, query string, etc., and will consume a stream of ByteStrings,\nproducing a Response. So the revised de\ufb01nition of an Application is:\ntype Application = Request -> Iteratee ByteString IO Response\nThe IO simply explains what types of side e\ufb00ects an application can perform. In the case of IO, it\ncan perform any kind of interaction with the outside world, an obvious necessity for the vast majority\nof web applications.\nBuilder\nThe trick in our arsenal is how we produce our response bu\ufb00ers. We have two competing desires here:\nminimizing system calls, and minimizing bu\ufb00er copies. On the one hand, we want to minimize system\ncalls for sending data over the socket. To do this we need to store outgoing data in a bu\ufb00er. However,\nif we make this bu\ufb00er too large, we will exhaust our memory and slow down the application\u2019s\nresponse time. On the other hand, we want to minimize the number of times data is copied between\nbu\ufb00ers, preferably copying just once from the source to destination bu\ufb00er.\nMichael Snoyman\n333\n", "page": 347, "type": "text", "section": "Page 347"}
{"text": "Haskell\u2019s solution is the builder. A builder is an instruction for how to \ufb01ll a memory bu\ufb00er, such\nas: place the \ufb01ve bytes \"hello\" in the next open position. Instead of passing a stream of memory\nbu\ufb00ers to the server, a WAI application passes a stream of these instructions. The server takes the\nstream and uses it to \ufb01ll up optimally sized memory bu\ufb00ers. As each bu\ufb00er is \ufb01lled, the server makes\na system call to send the data over over the wire and then starts \ufb01lling up the next bu\ufb00er.\n(The optimal size for a bu\ufb00er will depend on many factors such as cache size. The underlying\nblaze-builder library underwent signi\ufb01cant performance testing to determine the best trade-o\ufb00.)\nIn theory, this kind of optimization could be performed in the application itself. However, by\nencoding this approach in the interface, we are able to simply prepend the response headers to the\nresponse body. The result is that, for small to medium-sized responses, the entire response can be\nsent with a single system call and memory is copied only once.\nHandlers\nNow that we have an application, we need some way to run it. In WAI parlance, this is a handler. WAI\nhas some basic, standard handlers, such as the standalone server Warp (discussed below), FastCGI,\nSCGI and CGI. This spectrum allows WAI applications to be run on anything from dedicated servers\nto shared hosting. But in addition to these, WAI has some more interesting backends:\nWebkit: This backend embeds a Warp server and calls out to QtWebkit. By launching a server, then\nlaunching a new standalone browser window, we have faux desktop applications.\nLaunch: This is a slight variant on Webkit. Having to deploy the Qt and Webkit libraries can be a\nbit burdensome, so instead we just launch the user\u2019s default browser.\nTest: Even testing counts as a handler. After all, testing is simply the act of running an application\nand inspecting the responses.\nMost developers will likely use Warp. It is lightweight enough to be used for testing. It requires\nno con\ufb01g \ufb01les, no folder hierarchy and no long-running, administrator-owned process. It\u2019s a simple\nlibrary that gets compiled into your application or run via the Haskell interpreter. Warp is an\nincredibly fast server, with protection from all kinds of attack vectors, such as Slowloris and in\ufb01nite\nheaders. Warp can be the only web server you need, though it is also quite happy to sit behind a\nreverse HTTP proxy.\nThe PONG benchmark measures the requests per second of various servers for the 4-byte response\nbody \"PONG\". In the graph shown in Figure 22.2, Yesod is measured as a framework on top of\nWarp. As can be seen, the Haskell servers (Warp, Happstack and Snap) lead the pack.\nMost of the reasons for Warp\u2019s speed have already been spelled out in the overall description of\nWAI: enumerators, builders and packed datatypes. The last piece in the puzzle is from the Glasgow\nHaskell Compiler\u2019s (GHC\u2019s) multithreaded runtime. GHC, Haskell\u2019s \ufb02agship compiler, has light-\nweight green threads. Unlike system threads, it is possible to spin up thousands of these without\nserious performance hits. Therefore, in Warp each connection is handled by its own green thread.\nThe next trick is asynchronous I/O. Any web server hoping to scale to tens of thousands of\nrequests per second will need some type of asynchronous communication. In most languages, this\ninvolves complicated programming involving callbacks. GHC lets us cheat: we program as if we\u2019re\nusing a synchronous API, and GHC automatically switches between di\ufb00erent green threads waiting\nfor activity.\nUnder the surface, GHC uses whatever system is provided by the host operating system, such as\nkqueue, epoll and select. This gives us all the performance of an event-based I/O system, without\nworrying about cross-platform issues or writing in a callback-oriented way.\n334\nYesod\n", "page": 348, "type": "text", "section": "Page 348"}
{"text": "Figure 22.2: Warp PONG benchmark\nMiddleware\nIn between handlers and applications, we have middleware. Technically, middleware is an applica-\ntion transformer: it takes one Application, and returns a new one. This is de\ufb01ned as:\ntype Middleware = Application -> Application\nThe best way to understand the purpose of middleware is to look at some common examples:\n\u2022 gzip automatically compresses the response from an application.\n\u2022 jsonp automatically converts JSON responses to JSON-P responses when the client provided\na callback parameter.\n\u2022 autohead will generate appropriate HEAD responses based on the GET response of an\napplication.\n\u2022 debug will print debug information to the console or a log on each request.\nThe idea here is to factor out common code from applications and let it be shared easily. Note\nthat, based on the de\ufb01nition of middleware, we can easily stack these things up. The general work\ufb02ow\nof middleware is:\n1. Take the request value and apply some modi\ufb01cations.\n2. Pass the modi\ufb01ed request to the application and receive a response.\n3. Modify the response and return it to the handler.\nIn the case of stacked middleware, instead of passing to the application or handler, the in-between\nmiddleware will actually be passing to the inner and outer middleware, respectively.\nWai-test\nNo amount of static typing will obviate the need for testing. We all know that automated testing is a\nnecessity for any serious applications. wai-test is the recommended approach to testing a WAI\napplication. Since requests and responses are simple datatypes, it is easy to mock up a fake request,\nMichael Snoyman\n335\n", "page": 349, "type": "text", "section": "Page 349"}
{"text": "pass it to an application, and test properties about the response. wai-test simply provides some\nconvenience functions for testing common properties like the presence of a header or a status code.\n22.3\nTemplates\nIn the typical Model-View-Controller (MVC) paradigm, one of the goals is to separate logic from the\nview. Part of this separation is achieved through the use of a template language. However, there are\nmany di\ufb00erent ways to approach this issue. At one end of the spectrum, for example, PHP/ASP/JSP\nwill allow you to embed any arbitrary code within your template. At the other end, you have systems\nlike StringTemplate and QuickSilver, which are passed some arguments and have no other way of\ninteracting with the rest of the program.\nEach system has its pros and cons. Having a more powerful template system can be a huge\nconvenience. Need to show the contents of a database table? No problem, pull it in with the template.\nHowever, such an approach can quickly lead to convoluted code, interspersing database cursor\nupdates with HTML generation. This can be commonly seen in a poorly written ASP project.\nWhile weak template systems make for simple code, they also tend towards a lot of redundant work.\nYou will often need to not only keep your original values in datatypes, but also create dictionaries of\nvalues to pass to the template. Maintaining such code is not easy, and usually there is no way for a\ncompiler to help you out.\nYesod\u2019s family of template languages, the Shakespearean languages, strive for a middle ground.\nBy leveraging Haskell\u2019s standard referential transparency, we can be assured that our templates\nproduce no side e\ufb00ects. However, they still have full access to all the variables and functions available\nin your Haskell code. Also, since they are fully checked for both well-formedness, variable resolution\nand type safety at compile time, typos are much less likely to have you searching through your code\ntrying to pin down a bug.\nWhy the Name Shakespeare?\nThe HTML language, Hamlet, was the \ufb01rst language written, and originally based its syntax on\nHaml. Since it was at the time a \"reduced\" Haml, Hamlet seemed appropriate. As we added\nCSS and Javascript options, we decided to keep the naming theme with Cassius and Julius. At\nthis point, Hamlet looks nothing like Haml, but the name stuck anyway.\nTypes\nOne of the overarching themes in Yesod is proper use of types to make developers\u2019 lives easier. In\nYesod templates, we have two main examples:\n1. All content embedded into a Hamlet template must have a type of Html. As we\u2019ll see later,\nthis forces us to properly escape dangerous HTML when necessary, while avoiding accidental\ndouble-escaping as well.\n2. Instead of concatenating URLs directly in our template, we have datatypes\u2014known as type-\nsafe URLs\u2014which represent the routes in our application.\nAs a real-life example, suppose that a user submits his/her name to an application via a form.\nThis data would be represented with the Text datatype. Now we would like to display this variable,\n336\nYesod\n", "page": 350, "type": "text", "section": "Page 350"}
{"text": "called name, in a page. The type system\u2014at compile time\u2014prevents it from being simply stuck into\na Hamlet template, since it\u2019s not of type Html. Instead we must convert it somehow. For this, there\nare two conversion functions:\n1. toHtml will automatically escape any entities. So if a user submits the string <script\nsrc=\"http://example.com/evil.js\"></script>, the less-than signs will automatically\nbe converted to &<.\n2. preEscapedText, on the other hand, will leave the content precisely as it is now.\nSo in the case of untrusted input from a possibly nefarious user, toHtml would be our recom-\nmended approach. On the other hand, let us say we have some static HTML stored on our server that\nwe would like to insert into some pages verbatim. In that case, we could load it into a Text value\nand then apply preEscapedText, thereby avoiding any double-escaping.\nBy default, Hamlet will use the toHtml function on any content you try to interpolate. Therefore,\nyou only need to explicitly perform a conversion if you want to avoid escaping. This follows the\ndictum of erring on the side of caution.\nname <- runInputPost $ ireq textField \"name\"\nsnippet <- readFile \"mysnippet.html\"\nreturn [hamlet|\n<p>Welcome #{name}, you are on my site!\n<div .copyright>#{preEscapedText snippet}\n|]\nThe \ufb01rst step in type-safe URLs is creating a datatype that represents all the routes in your site.\nLet us say you have a site for displaying Fibonacci numbers. The site will have a separate page for\neach number in the sequence, plus the homepage. This could be modeled with the Haskell datatype:\ndata FibRoute = Home | Fib Int\nWe could then create a page like so:\n<p>You are currently viewing number #{show index} in the sequence. Its value is #{fib index}.\n<p>\n<a href=@{Fib (index + 1)}>Next number\n<p>\n<a href=@{Home}>Homepage\nThen all we need is some function to convert a type-safe URL into a string representation. In our\ncase, that could look something like this:\nrender :: FibRoute -> Text\nrender Home = \"/home\"\nrender (Fib i) = \"/fib/\" ++ show i\nFortunately, all of the boilerplate of de\ufb01ning and rendering type-safe URL datatypes is handled\nfor the developer automatically by Yesod. We will cover that in more depth later.\nMichael Snoyman\n337\n", "page": 351, "type": "text", "section": "Page 351"}
{"text": "The Other Languages\nIn addition to Hamlet, there are three other languages: Julius, Cassius and Lucius. Julius is used for\nJavascript; however, it\u2019s a simple pass-through language, just allowing for interpolation. In other\nwords, barring accidental use of the interpolation syntax, any piece of Javascript could be dropped\ninto Julius and be valid. For example, to test the performance of Julius, jQuery was run through the\nlanguage without an issue.\nThe other two languages are alternate CSS syntaxes. Those familiar with the di\ufb00erence between\nSass and Less will recognize this immediately: Cassius is whitespace delimited, while Lucius uses\nbraces. Lucius is in fact a superset of CSS, meaning all valid CSS \ufb01les are valid Lucius \ufb01les. In\naddition to allowing text interpolation, there are some helper datatypes provided to model unit sizes\nand colors. Also, type-safe URLs work in these languages, making it convenient for specifying\nbackground images.\nAside from the type safety and compile-time checks mentioned above, having specialized lan-\nguages for CSS and Javascript give us a few other advantages:\n\u2022 For production, all the CSS and Javascript is compiled into the \ufb01nal executable, increasing\nperformance (by avoiding \ufb01le I/O) and simplifying deployment.\n\u2022 By being based around the e\ufb03cient builder construct described earlier, the templates can be\nrendered very quickly.\n\u2022 There is built-in support for automatically including these in \ufb01nal webpages. We will get into\nthis in more detail when describing widgets below.\n22.4\nPersistent\nMost web applications will want to store information in a database. Traditionally, this has meant\nsome kind of SQL database. In that regard, Yesod continues a long tradition, with PostgreSQL as\nour most commonly used backend. But as we have been seeing in recent years, SQL isn\u2019t always\nthe answer to the persistence question. Therefore, Yesod was designed to work well with NoSQL\ndatabases as well, and ships with a MongoDB backend as a \ufb01rst-class citizen.\nThe result of this design decision is Persistent, Yesod\u2019s preferred storage option. There are really\ntwo guiding lights for Persistent: make it as back-end-agnostic as possible, and let user code be\ncompletely type-checked.\nAt the same time, we fully recognize that it is impossible to completely shield the user from all\ndetails of the backend. Therefore, we provide two types of escape routes:\n\u2022 Back-end-speci\ufb01c functionality as necessary. For example, Persistent provides features for\nSQL joins and MongoDB lists and hashes. Proper portability warnings will apply, but if you\nwant this functionality, it\u2019s there.\n\u2022 Easy access to performing raw queries. We don\u2019t believe it\u2019s possible for any abstraction to\ncover every use case of the underlying library. If you just have to write a 5-table, correlated\nsubquery in SQL, go right ahead.\nTerminology\nThe most primitive datatype in Persistent is the PersistValue. This represents any raw data that can\nappear within the database, such as a number, a date, or a string. Of course, sometimes you\u2019ll have\n338\nYesod\n", "page": 352, "type": "text", "section": "Page 352"}
{"text": "some more user-friendly datatypes you want to store, like HTML. For that, we have the PersistField\nclass. Internally, a PersistField expresses itself to the database in terms of a PersistValue.\nAll of this is very nice, but we will want to combine di\ufb00erent \ufb01elds together into a larger picture.\nFor this, we have a PersistEntity, which is basically a collection of PersistFields. And \ufb01nally,\nwe have a PersistBackend that describes how to create, read, update and delete these entities.\nAs a practical example, consider storing a person in a database. We want to store the person\u2019s\nname, birthday, and a pro\ufb01le image (a PNG \ufb01le). We create a new entity Person with three \ufb01elds:\na Text, a Day and a PNG. Each of those gets stored in the database using a di\ufb00erent PersistValue\nconstructor: PersistText, PersistDay and PersistByteString, respectively.\nThere is nothing surprising about the \ufb01rst two mappings, but the last one is interesting. There is\nno speci\ufb01c constructor for storing PNG content in a database, so instead we use a more generic type\n(a ByteString, which is just a sequence of bytes). We could use the same mechanism to store other\ntypes of arbitrary data.\n(The commonly held best practice for storing images is to keep the data on the \ufb01lesystem and\njust keep a path to the image in the database. We do not advocate against using that approach, but\nare rather using database-stored images as an illustrative example.)\nHow is all this represented in the database? Consider SQL as an example: the Person entity\nbecomes a table with three columns (name, birthday, and picture). Each \ufb01eld is stored as a di\ufb00erent\nSQL type: Text becomes a VARCHAR, Day becomes a Date and PNG becomes a BLOB (or BYTEA).\nThe story for MongoDB is very similar. Person becomes its own document, and its three \ufb01elds\neach become a MongoDB \ufb01eld. There is no need for datatypes or creation of a schema in MongoDB.\nPersistent\nSQL\nMongoDB\nPersistEntity\nTable\nDocument\nPersistField\nColumn\nField\nPersistValue\nColumn type\nN/A\nType Safety\nPersistent handles all of the data marshaling concerns behind the scenes. As a user of Persistent, you\nget to completely ignore the fact that a Text becomes a VARCHAR. You are able to simply declare\nyour datatypes and use them.\nEvery interaction with Persistent is strongly typed. This prevents you from accidentally putting\na number in the date \ufb01elds; the compiler will not accept it. Entire classes of subtle bugs simply\ndisappear at this point.\nNowhere is the power of strong typing more pronounced than in refactoring. Let\u2019s say you have\nbeen storing users\u2019 ages in the database, and you realize that you really wanted to store birthdays\ninstead. You are able to make a single line change to your entities declaration \ufb01le, hit compile, and\nautomatically \ufb01nd every single line of code that needs to be updated.\nIn most dynamically-typed languages, and their web frameworks, the recommended approach to\nsolving this issue is writing unit tests. If you have full test coverage, then running your tests will\nimmediately reveal what code needs to be updated. This is all well and good, but it is a weaker\nsolution than true types:\n\u2022 It is all predicated on having full test coverage. This takes extra time, and worse, is boilerplate\ncode that the compiler should be able to do for you.\n\u2022 You might be a perfect developer who never forgets to write a test, but can you say the same\nfor every person who will touch your codebase?\nMichael Snoyman\n339\n", "page": 353, "type": "text", "section": "Page 353"}
{"text": "\u2022 Even 100% test coverage doesn\u2019t guarantee that you really have tested every case. All it\u2019s done\nis proven you\u2019ve tested every line of code.\nCross-Database Syntax\nCreating an SQL schema that works for multiple SQL engines can be tricky enough. How do you\ncreate a schema that will also work with a non-SQL database like MongoDB?\nPersistent allows you to de\ufb01ne your entities in a high-level syntax, and will automatically create\nthe SQL schema for you. In the case of MongoDB, we currently use a schema-less approach. This\nalso allows Persistent to ensure that your Haskell datatypes match perfectly with the database\u2019s\nde\ufb01nitions.\nAdditionally, having all this information gives Persistent the ability to perform more advanced\nfunctions, such as migrations, for you automatically.\nMigrations\nPersistent not only creates schema \ufb01les as necessary, but will also automatically apply database\nmigrations if possible. Database modi\ufb01cation is one of the less-developed pieces of the SQL standard,\nand thus each engine has a di\ufb00erent take on the process. As such, each Persistent backend de\ufb01nes its\nown set of migration rules. In PostgreSQL, which has a rich set of ALTER TABLE rules, we use those\nextensively. Since SQLite lacks much of that functionality, we are reduced to creating temporary\ntables and copying rows. MongoDB\u2019s schema-less approach means no migration support is required.\nThis feature is purposely limited to prevent any kind of data loss. It will not remove any columns\nautomatically; instead, it will give you an error message, telling you the unsafe operations that are\nnecessary in order to continue. You will then have the option of either manually running the SQL it\nprovides you, or changing your data model to avoid the dangerous behavior.\nRelations\nPersistent is non-relational in nature, meaning it has no requirement for backends to support relations.\nHowever, in many use cases, we may want to use relations. In those cases, developers will have full\naccess to them.\nAssume we want to now store a list of skills with each user. If we were writing a MongoDB-\nspeci\ufb01c app, we could go ahead and just store that list as a new \ufb01eld in the original Person entity.\nBut that approach would not work in SQL. In SQL, we call this kind of relationship a one-to-many\nrelationship.\nThe idea is to store a reference to the \"one\" entity (person) with each \"many\" entity (skill). Then\nif we want to \ufb01nd all the skills a person has, we simply \ufb01nd all skills that reference that person. For\nthis reference, every entity has an ID. And as you might expect by now, these IDs are completely\ntype-safe. The datatype for a Person ID is PersonId. So to add our new skill, we would just add the\nfollowing to our entity de\ufb01nition:\nSkill\nperson PersonId\nname Text\ndescription Text\nUniqueSkill person name\n340\nYesod\n", "page": 354, "type": "text", "section": "Page 354"}
{"text": "This ID datatype concept comes up throughout Persistent and Yesod. You can dispatch based on\nan ID. In such a case, Yesod will automatically marshal the textual representation of the ID to the\ninternal one, catching any parse errors along the way. These IDs are used for lookup and deletion\nwith the get and delete functions, and are returned by the insertion and query functions insert\nand selectList.\n22.5\nYesod\nIf we are looking at the typical Model-View-Controller (MVC) paradigm, Persistent is the model and\nShakespeare is the view. This would leave Yesod as the controller.\nThe most basic feature of Yesod is routing. It features a declarative syntax and type-safe dispatch.\nLayered on top of this, Yesod provides many other features: streaming content generation, widgets,\ni18n, static \ufb01les, forms and authentication. But the core feature added by Yesod is really routing.\nThis layered approach makes it simpler for users to swap di\ufb00erent components of the system.\nSome people are not interested in using Persistent. For them, nothing in the core system even mentions\nPersistent. Likewise, while they are commonly used features, not everyone needs authentication or\nstatic \ufb01le serving.\nOn the other hand, many users will want to integrate all of these features. And doing so, while\nenabling all the optimizations available in Yesod, is not always straightforward. To simplify the\nprocess, Yesod also provides a sca\ufb00olding tool that sets up a basic site with the most commonly used\nfeatures.\nRoutes\nGiven that routing is really the main function of Yesod, let\u2019s start there. The routing syntax is very\nsimple: a resource pattern, a name, and request methods. For example, a simple blog site might look\nlike:\n/ HomepageR GET\n/add-entry AddEntryR GET POST\n/entry/#EntryId EntryR GET\nThe \ufb01rst line de\ufb01nes the homepage. This says \"I respond to the root path of the domain, I\u2019m\ncalled HomepageR, and I answer GET requests.\" (The trailing \"R\" on the resource names is simply a\nconvention, it doesn\u2019t hold any special meaning besides giving a cue to the developer that something\nis a route.)\nThe second line de\ufb01nes the add-entry page. This time, we answer both GET and POST requests.\nYou might be wondering why Yesod, as opposed to most frameworks, requires you to explicitly state\nyour request methods. The reason is that Yesod tries to adhere to RESTful principles as much as\npossible, and GET and POST requests really have very di\ufb00erent meanings. Not only do you state\nthese two methods separately, but later you will de\ufb01ne their handler functions separately. (This is\nactually an optional feature in Yesod. If you want, you can leave o\ufb00the list of methods and your\nhandler function will deal with all methods.)\nThe third line is a bit more interesting. After the second slash we have #EntryId. This de\ufb01nes\na parameter of type EntryId. We already alluded to this feature in the Persistent section: Yesod\nwill now automatically marshal the path component into the relevant ID value. Assuming an SQL\nbackend (Mongo is addressed later), if a user requests /entry/5, the handler function will get called\nMichael Snoyman\n341\n", "page": 355, "type": "text", "section": "Page 355"}
{"text": "with an argument EntryId 5. But if the user requests /entry/some-blog-post, Yesod will return\na 404.\nThis is obviously possible in most other web frameworks as well. The approach taken by Django,\nfor instance, would use a regular expression for matching the routes, e.g. r\"/entry/(+\n.)\". The\nYesod approach, however, provides some advantages:\n\u2022 Typing \"EntryId\" is much more semantic/developer-friendly than a regular expression.\n\u2022 Regular expressions cannot express everything (or at least, can\u2019t do so succinctly). We can use\n/calendar/#Day in Yesod; do you want to type a regex to match dates in your routes?\n\u2022 Yesod also automatically marshals the data for us. In our calendar case, our handler function\nwould receive a Day value. In the Django equivalent, the function would receive a piece of\ntext which it would then have to marshal itself. This is tedious, repetitive and ine\ufb03cient.\n\u2022 So far we\u2019ve assumed that a database ID is just a string of digits. But what if it\u2019s more\ncomplicated? MongoDB uses GUIDs, for example. In Yesod, your #EntryId will still work,\nand the type system will instruct Yesod how to parse the route. In a regex system, you would\nhave to go through all of your routes and change the \\d+ to whatever monstrosity of regex is\nneeded to match GUIDs.\nType-Safe URLs\nThis approach to routing gives birth to one of Yesod\u2019s most powerful features: type-safe URLs.\nInstead of just splicing together pieces of text to refer to a route, every route in your application can\nbe represented by a Haskell value. This immediately eliminates a large number of 404 Not Found\nerrors: it is simply not possible to produce an invalid URL. (It is still possible to produce a URL that\nwould lead to a 404 error, such as by referring to a blog post that does not exist. However, all URLs\nwill be formed correctly.)\nSo how does this magic work? Each site has a route datatype, and each resource pattern gets its\nown constructor. In our previous example, we would get something that looks like:\ndata MySiteRoute = HomepageR\n| AddEntryR\n| EntryR EntryId\nIf you want to link to the homepage, you use HomepageR. To link to a speci\ufb01c entry, you would\nuse the EntryR constructor with an EntryId parameter. For example, to create a new entry and\nredirect to it, you could write:\nentryId <- insert (Entry \"My Entry\" \"Some content\")\nredirect RedirectTemporary (EntryR entryId)\nHamlet, Lucius and Julius all include built-in support for these type-safe URLs. Inside a Hamlet\ntemplate you can easily create a link to the add-entry page:\n<a href=@{AddEntryR}>Create a new entry.\nThe best part? Just like Persistent entities, the compiler will keep you honest. If you change any\nof your routes (e.g., you want to include the year and month in your entry routes), Yesod will force\nyou to update every single reference throughout your codebase.\n342\nYesod\n", "page": 356, "type": "text", "section": "Page 356"}
{"text": "Handlers\nOnce you de\ufb01ne your routes, you need to tell Yesod how you want to respond to requests. This is\nwhere handler functions come into play. The setup is simple: for each resource (e.g., HomepageR)\nand request method, create a function named methodResourceR. For our previous example, we\nwould need four functions: getHomepageR, getAddEntryR, postAddEntryR, and getEntryR.\nAll of the parameters collected from the route are passed in as arguments to the handler function.\ngetEntryR will take a \ufb01rst argument of type EntryId, while all the other functions will take no\narguments.\nThe handler functions live in a Handler monad, which provides a great deal of functionality,\nsuch as redirecting, accessing sessions, and running database queries. For the last one, a typical way\nto start o\ufb00the getEntryR function would be:\ngetEntryR entryId = do\nentry <- runDB $ get404 entryId\nThis will run a database action that will get the entry associated with the given ID from the\ndatabase. If there is no such entry, it will return a 404 response.\nEach handler function will return some value, which must be an instance of HasReps. This is\nanother RESTful feature at play: instead of just returning some HTML or some JSON, you can return\na value that will return either one, depending on the HTTP Accept request header. In other words, in\nYesod, a resource is a speci\ufb01c piece of data, and it can be returned in one of many representations.\nWidgets\nAssume you want to include a navbar on a few di\ufb00erent pages of your site. This navbar will load up\nthe \ufb01ve most recent blog posts (stored in your database), generate some HTML, and then need some\nCSS and Javascript to style and enhance.\nWithout a higher-level interface to tie these components together, this could be a pain to implement.\nYou could add the CSS to the site-wide CSS \ufb01le, but that\u2019s adding extra declarations you don\u2019t always\nneed. Likewise with the Javascript, though a bit worse: having that extra Javascript might cause\nproblems on a page it was not intended to live on. You will also be breaking modularity by having to\ngenerate the database results from multiple handler functions.\nIn Yesod, we have a very simple solution: widgets. A widget is a piece of code that ties together\nHTML, CSS and Javascript, allowing you to add content to both the head and body, and can run any\narbitrary code that belongs in a handler. For example, to implement our navbar:\n-- Get last five blog posts. The \"lift\" says to run this code like we\u2019re in the handler.\nentries <- lift $ runDB $ selectList [] [LimitTo 5, Desc EntryPosted]\ntoWidget [hamlet|\n<ul .navbar>\n$forall entry <- entries\n<li>#{entryTitle entry}\n|]\ntoWidget [lucius| .navbar { color: red } |]\ntoWidget [julius|alert(\"Some special Javascript to play with my navbar\");|]\nBut there is even more power at work here. When you produce a page in Yesod, the standard\napproach is to combine a number of widgets together into a single widget containing all your page\nMichael Snoyman\n343\n", "page": 357, "type": "text", "section": "Page 357"}
{"text": "content, and then apply defaultLayout. This function is de\ufb01ned per site, and applies the standard\nsite layout.\nThere are two out-of-the-box approaches to handling where the CSS and Javascript go:\n1. Concatenate them and place them into style and script tags, respectively, within your\nHTML.\n2. Place them in external \ufb01les and refer to them with link and script tags, respectively.\nIn addition, your Javascript can be automatically mini\ufb01ed. Option 2 is the preferred approach,\nsince it allows a few extra optimizations:\n1. The \ufb01les are created with names based on a hash of the contents. This means you can place\ncached values far in the future without worries of users receiving stale content.\n2. Your Javascript can be asynchronously loaded.\nThe second point requires a bit of elaboration. Widgets not only contain raw Javascript, they also\ncontain a list of Javascript dependencies. For example, many sites will refer to the jQuery library\nand then add some Javascript that uses it. Yesod is able to automatically turn all of that into an\nasynchronous load via yepnope.js.\nIn other words, widgets allow you to create modular, composable code that will result in incredibly\ne\ufb03cient serving of your static resources.\nSubsites\nMany websites share common areas of functionality. Perhaps the two most common examples of\nthis are serving static \ufb01les and authentication. In Yesod, you can easily drop in this code using a\nsubsite. All you need to do is add an extra line to your routes. For example, to add the static subsite,\nyou would write:\n/static StaticR Static getStatic\nThe \ufb01rst argument tells where in the site the subsite starts. The static subsite is usually used\nat /static, but you could use whatever you want. StaticR is the name of the route; this is also\nentirely up to you, but convention is to use StaticR. Static is the name of the static subsite; this is\none you do not have control over. getStatic is a function that returns the settings for the static site,\nsuch as where the static \ufb01les are located.\nLike all of your handlers, the subsite handlers also have access to the defaultLayout function.\nThis means that a well-designed subsite will automatically use your site skin without any extra\nintervention on your part.\n22.6\nLessons Learned\nYesod has been a very rewarding project to work on. It has given me an opportunity to work on a\nlarge system with a diverse group of developers. One of the things that has truly shocked me is how\ndi\ufb00erent the end product has become from what I had originally intended. I started o\ufb00Yesod by\ncreating a list of goals. Very few of the main features we currently tout in Yesod are in that list, and a\ngood portion of that list is no longer something I plan to implement. The \ufb01rst lesson is:\nYou will have a better idea of the system you need after you start working on it. Do not tie yourself\ndown to your initial ideas.\n344\nYesod\n", "page": 358, "type": "text", "section": "Page 358"}
{"text": "As this was my \ufb01rst major piece of Haskell code, I learned a lot about the language during\nYesod\u2019s development. I\u2019m sure others can relate to the feeling of \"How did I ever write code like\nthis?\" Even though that initial code was not of the same caliber as the code we have in Yesod at this\npoint, it was solid enough to kick-start the project. The second lesson is:\nDon\u2019t be deterred by supposed lack of mastery of the tools at hand. Write the best code you can,\nand keep improving it.\nOne of the most di\ufb03cult steps in Yesod\u2019s development was moving from a single-person team\u2014\nme\u2014to collaborating with others. It started o\ufb00simply, with merging pull requests on GitHub,\nand eventually moved to having a number of core maintainers. I had established some of my own\ndevelopment patterns, which were nowhere explained or documented. As a result, contributors found\nit di\ufb03cult to pull my latest unreleased changes and play around with them. This hindered others both\nwhen contributing and testing.\nWhen Greg Weber came aboard as another lead on Yesod, he put in place a lot of the coding\nstandards that were sorely lacking. To compound the problems, there were some inherent di\ufb03culties\nplaying with the Haskell development toolchain; speci\ufb01cally in dealing with Yesod\u2019s large number\nof packages. One of the goals of the entire Yesod team has since been to create standard scripts and\ntools to automate building. Many of these tools are making their way back into the general Haskell\ncommunity. The \ufb01nal lesson is:\nConsider early on how to make your project approachable for others.\n345\n", "page": 359, "type": "text", "section": "Page 359"}
{"text": "346\nYesod\n", "page": 360, "type": "text", "section": "Page 360"}
{"text": "[chapter23]\nYocto\nElizabeth Flanagan\nThe Yocto Project is an open source project that provides a common starting point for developers of\nembedded Linux systems to create customized distributions for embedded products in a hardware-\nagnostic setting. Sponsored by the Linux Foundation, Yocto is more than a build system. It provides\ntools, processes, templates and methods so developers can rapidly create and deploy products for the\nembedded market. One of the core components of Yocto is the Poky Build system. As Poky is a\nlarge and complex system, we will be focusing on one of its core components, BitBake. BitBake is a\nGentoo-Portage-inspired build tool, used by both the Yocto Project and OpenEmbedded communities\nto utilize metadata to create Linux images from source.\nIn 2001, Sharp Corporation introduced the SL-5000 PDA, named Zaurus, which ran an embedded\nLinux distribution, Lineo. Not long after the Zaurus\u2019s introduction, Chris Larson founded the\nOpenZaurus Project, a replacement Linux distribution for the SharpROM, based on a build system\ncalled buildroot. With the founding of the project, people began contributing many more software\npackages, as well as targets for other devices, and it wasn\u2019t long before the build system for OpenZaurus\nbegan to show fragility. In January 2003, the community began discussing a new build system to\nincorporate the community usage model of a generic build system for embedded Linux distributions.\nThis would eventually become OpenEmbedded. Chris Larson, Michael Lauer, and Holger Schurig\nbegan work on OpenEmbedded by porting hundreds of OpenZaurus packages over to the new build\nsystem.\nThe Yocto Project springs from this work. At the project\u2019s core is the Poky build system, created\nby Richard Purdie. It began as a stabilized branch of OpenEmbedded using a core subset of the\nthousands of OpenEmbedded recipes, across a limited set of architectures. Over time, it slowly\ncoalesced into more than just an embedded build system, but into a complete software development\nplatform, with an Eclipse plugin, a fakeroot replacement and QEMU based images. Around November\n2010, the Linux Foundation announced that this work would all continue under the heading of the\nYocto Project as a Linux Foundation-sponsored project. It was then established that Yocto and\nOpenEmbedded would coordinate on a core set of package metadata called OE-Core, combining the\nbest of both Poky and OpenEmbedded with an increased use of layering for additional components.\n23.1\nIntroduction to the Poky Build System\nThe Poky build system is the core of the Yocto Project. In Poky\u2019s default con\ufb01guration, it can provide\na starting image footprint that ranges from a shell-accessible minimal image all the way up to a Linux\nStandard Base-compliant image with a GNOME Mobile and Embedded (GMAE) based reference\n", "page": 361, "type": "text", "section": "Page 361"}
{"text": "user interface called Sato. From these base image types, metadata layers can be added to extend\nfunctionality; layers can provide an additional software stack for an image type, add a board support\npackage (BSP) for additional hardware or even represent a new image type. Using the 1.1 release\nof Poky, named \u201cedison\u201d, we will show how BitBake uses these recipes and con\ufb01guration \ufb01les to\ngenerate an embedded image.\nFrom a very high level, the build process starts out by setting up the shell environment for the\nbuild run. This is done by sourcing a \ufb01le, oe-init-build-env, that exists in the root of the Poky\nsource tree. This sets up the shell environment, creates an initial customizable set of con\ufb01guration\n\ufb01les and wraps the BitBake runtime with a shell script that Poky uses to determine if the minimal\nsystem requirements have been met.\nFor example, one of the things it will look for is the existence of Pseudo, a fakeroot replacement\ncontributed to the Yocto Project by Wind River Systems. At this point, bitbake core-image-minimal,\nfor example, should be able to create a fully functional cross-compilation environment and then\ncreate a Linux image based on the image de\ufb01nition for core-image-minimal from source as de\ufb01ned\nin the Yocto metadata layer.\nFigure 23.1: High-level overview of Poky task execution\nDuring the creation of our image, BitBake will parse its con\ufb01guration, include any additional\nlayers, classes, tasks or recipes de\ufb01ned, and begin by creating a weighted dependency chain. This\nprocess provides an ordered and weighted task priority map. BitBake then uses this map to determine\nwhat packages must be built in which order so as to most e\ufb03ciently ful\ufb01ll compilation dependencies.\nTasks needed by the most other tasks are weighted higher, and thus run earlier during the build process.\nThe task execution queue for our build is created. BitBake also stores the parsed metadata summaries\nand if, on subsequent runs, it determines that the metadata has changed, it can re-parse only what\n348\nYocto\n", "page": 362, "type": "text", "section": "Page 362"}
{"text": "BitBake recipe for grep\nDESCRIPTION = \"GNU grep utility\"\nHOMEPAGE = \"http://savannah.gnu.org/projects/grep/\"\nBUGTRACKER = \"http://savannah.gnu.org/bugs/?group=grep\"\nSECTION = \"console/utils\"\nLICENSE = \"GPLv3\"\nLIC_FILES_CHKSUM = \"file://COPYING;md5=8006d9c814277c1bfc4ca22af94b59ee\"\nPR = \"r0\"\nSRC_URI = \"${GNU_MIRROR}/grep/grep-${PV}.tar.gz\"\nSRC_URI[md5sum] = \"03e3451a38b0d615cb113cbeaf252dc0\"\nSRC_URI[sha256sum]=\"e9118eac72ecc71191725a7566361ab7643edfd3364869a47b78dc934a357970\"\ninherit autotools gettext\nEXTRA_OECONF = \"--disable-perl-regexp\"\ndo_configure_prepend\n() {\nrm\n-f ${S}/m4/init.m4\n}\ndo_install () {\nautotools_do_install\ninstall -d ${D}${base_bindir}\nmv ${D}${bindir}/grep ${D}${base_bindir}/grep.${PN}\nmv ${D}${bindir}/egrep ${D}${base_bindir}/egrep.${PN}\nmv ${D}${bindir}/fgrep ${D}${base_bindir}/fgrep.${PN}\n}\npkg_postinst_${PN}() {\nupdate-alternatives --install ${base_bindir}/grep grep grep.${PN} 100\nupdate-alternatives --install ${base_bindir}/egrep egrep egrep.${PN} 100\nupdate-alternatives --install ${base_bindir}/fgrep fgrep fgrep.${PN} 100\n}\npkg_prerm_${PN}() {\nupdate-alternatives --remove grep grep.${PN}\nupdate-alternatives --remove egrep egrep.${PN}\nupdate-alternatives --remove fgrep fgrep.${PN}\n}\nElizabeth Flanagan\n349\n", "page": 363, "type": "text", "section": "Page 363"}
{"text": "has changed. The BitBake scheduler and parser are some of the more interesting architectural\ndesigns of BitBake and some of the decisions surrounding them and their implementation by BitBake\ncontributors will be discussed later.\nBitBake then runs through its weighted task queue, spawning threads (up to the number de\ufb01ned\nby BB_NUMBER_THREADS in conf/local.conf) that begin executing those tasks in the predetermined\norder. The tasks executed during a package\u2019s build may be modi\ufb01ed, prepended- or appended-to\nthrough its recipe. The basic, default package task order of execution starts by fetching and unpacking\npackage source and then con\ufb01guring and cross-compiling the unpacked source. The compiled source\nis then split up into packages and various calculations are made on the compilation result such as\nthe creation of debug package information. The split packages are then packaged into a supported\npackage format; RPM, ipk and deb are supported. BitBake will then use these packages to build the\nroot \ufb01le system.\nPoky Build System Concepts\nOne of the most powerful properties of the Poky build system is that every aspect of a build is\ncontrolled by metadata. Metadata can be loosely grouped into con\ufb01guration \ufb01les or package recipes.\nA recipe is a collection of non-executable metadata used by BitBake to set variables or de\ufb01ne\nadditional build-time tasks. A recipe contains \ufb01elds such as the recipe description, the recipe version,\nthe license of the package and the upstream source repository. It may also indicate that the build\nprocess uses autotools, make, distutils or any other build process, in which case the basic\nfunctionality can be de\ufb01ned by classes it inherits from the OE-Core layer\u2019s class de\ufb01nitions in\n./meta/classes. Additional tasks can also be de\ufb01ned, as well as task prerequisites. BitBake also\nsupports both _prepend and _append as a method of extending task functionality by injecting code\nindicated by using prepend or append su\ufb03x into the beginning or end of a task.\nCon\ufb01guration \ufb01les can be broken down into two types. There are those that con\ufb01gure BitBake\nand the overall build run, and those that con\ufb01gure the various layers Poky uses to create di\ufb00erent\ncon\ufb01gurations of a target image. A layer is any grouping of metadata that provides some sort of\nadditional functionality. These can be BSP for new devices, additional image types or additional\nsoftware outside of the core layers. In fact, the core Yocto metadata, meta-yocto, is itself a layer\napplied on top of the OE-Core metadata layer, meta which adds additional software and image types\nto the OE-Core layer.\nAn example of how one would use layering is by creating a NAS device for the Intel n660\n(Crownbay), using x32, the new 32-bit native ABI for x86-64, with a custom software layer that adds\na user interface.\nGiven the task at hand, we could split this functionality out into layers. At the lowest level we\nwould utilize a BSP layer for Crownbay that would enable Crownbay-speci\ufb01c hardware functionality,\nsuch as video drivers. As we want x32, we would use the experimental meta-x32 layer. The NAS\nfunctionality would be layered on top of this by adding the Yocto Project\u2019s example NAS layer,\nmeta-baryon. And lastly, we\u2019ll use an imaginary layer called meta-myproject, to provide the\nsoftware and con\ufb01guration to create a graphical user interface for con\ufb01guration of the NAS.\nDuring the setup of the BitBake environment, some initial con\ufb01guration \ufb01les are generated by\nsourcing oe-build-init-env. These con\ufb01guration \ufb01les allow us quite a bit of control over how\nand what Poky generates. The \ufb01rst of these con\ufb01guration \ufb01les is bblayers.conf. This \ufb01le is what\nwe will use to add additional layers in order to build our example project.\n350\nYocto\n", "page": 364, "type": "text", "section": "Page 364"}
{"text": "Here\u2019s an example of a bblayers.conf \ufb01le:\n# LAYER_CONF_VERSION is increased each time build/conf/bblayers.conf\n# changes incompatibly\nLCONF_VERSION = \"4\"\nBBFILES ?= \"\"\nBBLAYERS = \" \\\n/home/eflanagan/poky/meta \\\n/home/eflanagan/poky/meta-yocto \\\n/home/eflanagan/poky/meta-intel/crownbay \\\n/home/eflanagan/poky/meta-x32 \\\n/home/eflanagan/poky/meta-baryon\\\n/home/eflanagan/poky/meta-myproject \\\n\"\nThe BitBake layers \ufb01le, bblayers, de\ufb01nes a variable BBLAYERS that BitBake uses to look for\nBitBake layers. In order to fully understand this, we should also look at how our layers are actually\nconstructed. Using meta-baryon 1 as our example layer, we want to examine the layer con\ufb01guration\n\ufb01le. This \ufb01le, conf/layer.conf, is what BitBake parses after its initial parsing of bblayers.conf.\nFrom here it adds additional recipes, classes and con\ufb01guration to the build.\nFigure 23.2: Example of BitBake layering\nHere\u2019s meta-baryon\u2019s layer.conf:\n# Layer configuration for meta-baryon layer\n# Copyright 2011 Intel Corporation\n# We have a conf directory, prepend to BBPATH to prefer our versions\nBBPATH := \"${LAYERDIR}:${BBPATH}\"\n# We have recipes-* directories, add to BBFILES\nBBFILES := \"${BBFILES} ${LAYERDIR}/recipes-*/*/*.bb ${LAYERDIR}/recipes-*/*/*.bbappend\"\nBBFILE_COLLECTIONS += \"meta-baryon\"\nBBFILE_PATTERN_meta-baryon := \"^${LAYERDIR}/\"\nBBFILE_PRIORITY_meta-baryon = \"7\"\n1git://git.yoctoproject.org/meta-baryon\nElizabeth Flanagan\n351\n", "page": 365, "type": "text", "section": "Page 365"}
{"text": "All of the BitBake con\ufb01guration \ufb01les help generate BitBake\u2019s datastore which is used during\nthe creation of the task execution queue. During the beginning of a build, BitBake\u2019s BBCooker\nclass is started. The cooker manages the build task execution by baking the recipes. One of the\n\ufb01rst things the cooker does is attempt to load and parse con\ufb01guration data. Remember, though,\nthat BitBake is looking for two types of con\ufb01guration data. In order to tell the build system where\nit should \ufb01nd this con\ufb01guration data (and in turn where to \ufb01nd recipe metadata), the cooker\u2019s\nparseConfigurationFiles method is called. With few exceptions, the \ufb01rst con\ufb01guration \ufb01le that\nthe cooker looks for is bblayers.conf. After this \ufb01le is parsed, BitBake then parses each layer\u2019s\nlayer.conf \ufb01le.\nOnce layer con\ufb01guration \ufb01les are parsed, parseConfigurationFiles then parses bitbake.conf\nwhose main purpose is to set up global build time variables, such as directory structure naming\nfor various rootfs directories and the initial LDFLAGS to be used during compile time. Most end\nusers will never touch this \ufb01le as most anything needed to be changed here would be within a recipe\ncontext, as opposed to build wide or could be overridden in a con\ufb01guration \ufb01le such as local.conf.\nAs this \ufb01le is parsed, BitBake also includes con\ufb01guration \ufb01les that are relative to each layer in\nBBLAYERS and adds the variables found in those \ufb01les to its data store.\nHere is a portion of a bitbake.conf showing included con\ufb01guration \ufb01les:\ninclude conf/site.conf\ninclude conf/auto.conf\ninclude conf/local.conf\ninclude conf/build/${BUILD_SYS}.conf\ninclude conf/target/${TARGET_SYS}.conf\ninclude conf/machine/${MACHINE}.conf\n23.2\nBitBake Architecture\nBefore we delve into some of BitBake\u2019s current architectural design, it would help to understand\nhow BitBake once worked. In order to fully appreciate how far BitBake has come, we will consider\nthe initial version, BitBake 1.0. In that \ufb01rst release of BitBake, a build\u2019s dependency chain was\ndetermined based on recipe dependencies. If something failed during the build of an image, BitBake\nwould move on to the next task and try to build it again later. What this means, obviously, is that\nbuilds took a very long time. One of the things BitBake also did is keep each and every variable that\na recipe used in one very large dictionary. Given the number of recipes and the number of variables\nand tasks needed to accomplish a build, BitBake 1.0 was a memory hog. At a time when memory\nwas expensive and systems had much less, builds could be painful a\ufb00airs. It was not unheard of for\na system to run out of memory (writing to swap!) as it slugged through a long running build. In\nits \ufb01rst incarnation, while it did the job (sometimes), it did it slowly while consuming an enormous\namount of resources. Worse, as BitBake 1.0 had no concept of a data persistence cache or shared\nstate, it also had no ability to do incremental builds. If a build failed, one would have to restart it\nfrom scratch.\nA quick di\ufb00between the current BitBake version used in Poky \u201cedison\u201d 1.13.3 and 1.0 shows\nthe implementation of BitBake\u2019s client-server architecture, the data persistence cache, its datastore, a\ncopy-on-write improvement for the datastore, shared state implementation and drastic improvements\nin how it determines task and package dependency chains. This evolution has made it more reliable,\nmore e\ufb03cient and more dynamic. Much of this functionality came out of necessity for quicker, more\nreliable builds that used fewer resources. Three improvements to BitBake that we will examine are\n352\nYocto\n", "page": 366, "type": "text", "section": "Page 366"}
{"text": "the implementation of a client-server architecture, optimizations around BitBake\u2019s data storage and\nwork done on how BitBake determines its build and task dependency chain.\nBitBake IPC\nSince we now know a good deal about how the Poky build system uses con\ufb01gurations, recipes and\nlayers to create embedded images, we\u2019re prepared to begin to look under the hood of BitBake and\nexamine how this is all combined. Starting with the core BitBake executable, bitbake/bin/bake,\nwe can begin to see the process BitBake follows as it begins to set up the infrastructure needed to\nbegin a build. The \ufb01rst item of interest is BitBake\u2019s Interprocess Communications (IPC). Initially,\nBitBake had no concept of a client-server. This functionality was factored into the BitBake design\nover a period of time in order to allow BitBake to run multiple processes during a build, as it was\ninitially single-threaded, and to allow di\ufb00erent user experiences.\nFigure 23.3: Overview of BitBake IPC\nAll Poky builds are begun by starting a user interface instance. The user interface provides a\nmechanism for logging of build output, build status and build progress, as well as for receiving\nevents from build tasks through the BitBake event module. The default user interface used is knotty,\nBitBake\u2019s command line interface. Called knotty, or \u201c(no) tty\u201d, since it handles both ttys and non-ttys,\nit is one of a few interfaces that are supported. One of these additional user interfaces is Hob. Hob\nis the graphical interface to BitBake, a kind of \u201cBitBake commander\u201d. In addition to the typical\nfunctions you would see in the knotty user interface, Hob (written by Joshua Lock) brings the ability\nto modify con\ufb01guration \ufb01les, add additional layers and packages, and fully customize a build.\nBitBake user interfaces have the ability to send commands to the next module brought up by\nthe BitBake executable, the BitBake server. Like the user interface, BitBake also supports multiple\nElizabeth Flanagan\n353\n", "page": 367, "type": "text", "section": "Page 367"}
{"text": "di\ufb00erent server types, such as XMLRPC. The default server that most users use when executing\nBitBake from the knotty user interface is BitBake\u2019s process server. After bringing up the server, the\nBitBake executable brings up the cooker.\nThe cooker is a core portion of BitBake and is where most of the particularly interesting things\nthat occur during a Poky build are called from. The cooker is what manages the parsing of metadata,\ninitiates the generation of the dependency and task trees, and manages the build. One of the functions\nof BitBake\u2019s server architecture is allowing multiple ways of exposing the command API, indirectly,\nto the user interface. The command module is the worker of BitBake, running build commands and\ntriggering events that get passed up to the user interface through BitBake\u2019s event handler. Once the\ncooker is brought up from the BitBake executable, it initializes the BitBake datastore and then begins\nto parse all of Poky\u2019s con\ufb01guration \ufb01les. It then creates the runqueue object, and triggers the build.\nBitBake DataSmart Copy-on-Write Data Storage\nIn BitBake 1.0, all BitBake variables were parsed and stored in one very large dictionary during\nthe initialization of that version\u2019s data class. As previously mentioned, this was problematic in\nthat very large Python dictionaries are slow on writes and member access, and if the build host\nruns out of physical memory during the build, we end up using swap. While this is less likely in\nmost systems in late 2011, when OpenEmbedded and BitBake were \ufb01rst starting up, the average\ncomputer\u2019s speci\ufb01cation usually had less than one or two gigabytes of memory.\nThis was one of the major pain points in early BitBake. Two major issues needed to be worked\nout in order to help increase performance: one was precomputation of the build dependency chain;\nthe other was to reduce the size of data being stored in memory. Much of the data being stored\nfor a recipe doesn\u2019t change from recipe to recipe; for example, with TMPDIR, BB_NUMBER_THREADS\nand other global BitBake variables, having a copy of the entire data environment per recipe stored\nin memory was ine\ufb03cient. The solution was Tom Ansell\u2019s copy-on-write dictionary that \u201cabuses\nclasses to be nice and fast\u201d. BitBake\u2019s COW module is both an especially fearless and clever hack.\nRunning python BitBake/lib/bb/COW.py and examining the module will give you an idea of how\nthis copy-on-write implementation works and how BitBake uses it to store data e\ufb03ciently\nThe DataSmart module, which uses the COW dictionary, stores the data from the initial Poky\ncon\ufb01guration, data from .conf \ufb01les and .bbclass \ufb01les, in a dict as a data object. Each of these\nobjects can contain another data object of just the di\ufb00of the data. So, if a recipe changes something\nfrom the initial data con\ufb01guration, instead of copying the entire con\ufb01guration in order to localize it,\na di\ufb00of the parent data object is stored at the next layer down in the COW stack. When an attempt is\nmade to access a variable, the data module will use DataSmart to look into the top level of the stack.\nIf the variable is not found it will defer to a lower level of the stack until it does \ufb01nd the variable or\nthrows an error.\nOne of the other interesting things about the DataSmart module centers around variable expansion.\nAs BitBake variables can contain executable Python code, one of the things that needs to be done\nis run the variable through BitBake\u2019s bb.codeparser to ensure that it\u2019s valid Python and that it\ncontains no circular references. An example of a variable containing Python code is this example\ntaken from ./meta/conf/distro/include/tclibc-eglibc.inc:\nLIBCEXTENSION = \"${@[\u2019\u2019, \u2019-gnu\u2019][(d.getVar(\u2019ABIEXTENSION\u2019, True) or \u2019\u2019) != \u2019\u2019]}\"\nThis\nvariable\nis\nincluded\nfrom\none\nof\nthe\nOE-Core\ncon\ufb01guration\n\ufb01les,\n./meta/conf/distro/include/defaultsetup.conf, and is used to provide a set of default op-\ntions across di\ufb00erent distro con\ufb01gurations that one would want to lay on top of Poky or OpenEmbed-\n354\nYocto\n", "page": 368, "type": "text", "section": "Page 368"}
{"text": "ded. This \ufb01le imports some eglibc-speci\ufb01c variables that are set dependent on the value of another\nBitBake variable ABIEXTENSION. During the creation of the datastore, the Python code within this\nvariable needs to be parsed and validated to ensure tasks that use this variable will not fail.\nBitBake Scheduler\nOnce BitBake has parsed the con\ufb01guration and created its datastore, it needs to parse the recipes\nrequired for the image and produce a build chain. This is one of the more substantial improvements\nto BitBake. Originally, BitBake took its build priorities from a recipe. If a recipe had a DEPENDS, it\nwould try to \ufb01gure out what to build in order to satisfy that dependency. If a task failed because it\nlacked a prerequisite needed for its buildout, it was simply put to the side and attempted later. This\nhad obvious drawbacks, both in e\ufb03ciency and reliability.\nAs no precomputed dependency chain was established, task execution order was \ufb01gured out\nduring the build run. This limited BitBake to being single-threaded. To give an idea of how painful\nsingle-threaded BitBake builds can be, the smallest image \u201ccore-image-minimal\u201d on a standard\ndeveloper machine in 2011 (Intel Core i7, 16 gigabytes of DDR3 memory) takes about three or four\nhours to build a complete cross-compilation toolchain and use it to produce packages that are then\nused to create an image. For reference, a build on the same machine with BB_NUMBER_THREADS at\n14 and PARALLEL_MAKE set to \u201c-j 12\u201d takes about 30 to 40 minutes. As one could imagine, running\nsingle-threaded with no precomputed order of task execution on slower hardware that had less\nmemory with a large portion wasted by duplicate copies of the entire datastore took much longer.\nDependencies\nWhen we talk of build dependencies, we need to make a distinction between the various types. A\nbuild dependency, or DEPENDS, is something we require as a prerequisite so that Poky can build the\nrequired package, whereas a runtime dependency, RDEPENDS, requires that the image the package is\nto be installed on also contains the package listed as an RDEPENDS. Take, for example, the package\ntask-core-boot. If we look at the recipe for it in\nmeta/recipes-core/tasks/task-core-boot.bb\nwe will see two BitBake variables set: RDEPENDS and DEPENDS. BitBake uses these two \ufb01elds\nduring the creation of its dependency chain.\nHere is a portion of task-core-boot.bb showing DEPENDS and RDEPENDS:\nDEPENDS = \"virtual/kernel\"\n...\nRDEPENDS_task-core-boot = \"\\\nbase-files \\\nbase-passwd \\\nbusybox \\\ninitscripts \\\n...\nPackages aren\u2019t the only thing in BitBake with dependencies. Tasks also have their own depen-\ndencies. Within the scope of BitBake\u2019s runqueue, we recognize four types: internally dependent,\nDEPENDS dependent, RDEPENDS dependent and inter-task dependent.\nElizabeth Flanagan\n355\n", "page": 369, "type": "text", "section": "Page 369"}
{"text": "Internally dependent tasks are set within a recipe and add a task before and/or after another\ntask. For example, in a recipe, we could add a task called do_deploy by adding the line addtask\ndeploy before do_build after do_compile. This would add a dependency for running the\ndo_deploy task prior to do_build being started, but after do_compile is completed. DEPENDS and\nRDEPENDS dependent tasks are tasks that run after a denoted task. For example, if we wanted to run\ndo_deploy of a package after the do_install of its DEPENDS or RDEPENDS, our recipe would include\ndo_deploy[deptask] = \u2019do_install\u2019 or do_deploy[rdeptask] = \u2019do_install\u2019. For inter-\ntask dependencies, if we wanted a task to be dependent on a di\ufb00erent package\u2019s task we would add, us-\ning the above example of do_deploy, do_deploy[depends] = \u201c<target\u2019s name>:do_install\u201d.\nRunQueue\nAs an image build can have hundreds of recipes, each with multiple packages and task, each with its\nown dependency, BitBake is now tasked with trying to sort this out into something it can use as an\norder of execution. After the cooker has gotten the entire list of packages needed to be built from the\ninitialization of the bb.data object, it will begin to create a weighted task map from this data in order\nto produce an ordered list of tasks it needs to run, called the runqueue. Once the runqueue is created,\nBitBake can begin executing it in order of priority, tasking out each portion to a di\ufb00erent thread.\nWithin the provider module, BitBake will \ufb01rst look to see if there is a PREFERRED_PROVIDER\nfor a given package or image. As more than one recipe can provide a given package and as tasks are\nde\ufb01ned in recipes, BitBake needs to decide which provider of a package it will use. It will sort all\nthe providers of the package, weighting each provider by various criteria. For example, preferred\nversions of software will get a higher priority than others. However, BitBake also takes into account\npackage version as well as the dependencies of other packages. Once it has selected the recipe from\nwhich it will derive its package, BitBake will iterate over the DEPENDS and RDEPENDS of that\nrecipe and proceed to compute the providers for those packages. This chain reaction will produce a\nlist of packages needed for image generation as well as providers for those packages.\nRunqueue now has a full list of all packages that need to be built and a dependency chain. In\norder to begin execution of the build, the runqueue module now needs to create the TaskData object\nso it can begin to sort out a weighted task map. It begins by taking each buildable package it has\nfound, splitting the tasks needed to generate that package and weighing each of those tasks based\non the number of packages that require it. Tasks with a higher weight have more dependents, and\ntherefore are generally run earlier in the build. Once this is complete, the runqueue module then\nprepares to convert the TaskData object into a runqueue.\nThe creation of the runqueue is somewhat complex. BitBake \ufb01rst iterates through the list of task\nnames within the TaskData object in order to determine task dependencies. As it iterates through\nTaskData, it begins to build a weighted task map. When it is complete, if it has found no circular\ndependencies, unbuildable tasks or any such problems, it will then order the task map by weight and\nreturn a complete runqueue object to the cooker. The cooker will begin to attempt to execute the\nrunqueue, task by task. Depending upon image size and computing resources, Poky may take from\na half-hour to hours to generate a cross-compilation toolchain, a package feed and the embedded\nLinux image speci\ufb01ed. It is worth noting that from the time of executing bitbake <image_name>\nfrom the command line, the entire process up to right before the execution of the task execution\nqueue has taken less than a few seconds.\n356\nYocto\n", "page": 370, "type": "text", "section": "Page 370"}
{"text": "23.3\nConclusion\nIn my discussions with community members and my own personal observations, I\u2019ve identi\ufb01ed a few\nareas where things should, perhaps, have been done di\ufb00erently, as well as a few valuable lessons. It\nis important to note that \u201carm chair quarterbacking\u201d a decade-long development e\ufb00ort is not meant\nas a criticism of those who\u2019ve poured their time and e\ufb00ort into a wholly remarkable collection of\nsoftware. As developers, the most di\ufb03cult part of our job is predicting what we will need years down\nthe road and how we can set up a framework to enable that work now. Few can achieve that without\nsome road bumps.\nThe \ufb01rst lesson is to be sure to develop a written, agreed-upon standards document that is well\nunderstood by the community. It should be designed for maximum \ufb02exibility and growth.\nOne place where I\u2019ve personally run into this issue is with my work in OE-Core\u2019s license manifest\ncreation class, especially with my experiences working with the LICENSE variable. As no clearly\ndocumented standard existed for what LICENSE should contain, a review of the many recipes\navailable showed many variations. The various LICENSE strings contained everything from Python\nabstract syntax tree-parsable values to values that one would have little hope of gaining meaningful\ndata from. There was a convention that was commonly used within the community; however, the\nconvention had many variations, some less correct than others. This wasn\u2019t the problem of the\ndeveloper who wrote the recipe; it was a community failure to de\ufb01ne a standard.\nAs little prior work was actually done with the LICENSE variable outside of checking for its\nexistence, there was no particular concern about a standard for that variable. Much trouble could\nhave been avoided had a project-wide agreed-upon standard been developed early on.\nThe next lesson is a bit more general and speaks to an issue seen not only within the Yocto Project\nbut in other large scale projects that are systems-design speci\ufb01c. It is the one of the most important\nthings developers can do to limit the amount of e\ufb00ort duplication, refactoring and churn their project\nencounters: spend time\u2014lots of time\u2014on front-end planning and architectural design.\nIf you think you\u2019ve spent enough time on architectural design, you probably haven\u2019t. If you think\nyou haven\u2019t spent enough time on architectural design, you de\ufb01nitely haven\u2019t. Spending more time on\nfront end planning won\u2019t stop you from later having to rip apart code or even do major architectural\nchanges, but it will certainly reduce the amount of duplicated e\ufb00ort in the long run.\nDesigning your software to be as modular as possible, knowing that you will end up revisiting\nareas for anything from minor tweaks to major rewrites, will make it so that when you do run into\nthese issues, code rewrites are less hair-raising.\nOne obvious place where this would have helped in the Yocto Project is identifying the needs of\nend users with low memory systems. Had more thought been put into BitBake\u2019s datastore earlier,\nperhaps we could have predicted the problems associated with the datastore taking up too much\nmemory and dealt with it earlier.\nThe lesson here is that while it is nearly impossible to identify every pain point your project will\nrun into during its lifetime, taking the time to do serious front-end planning will help reduce the\ne\ufb00ort needed later. BitBake, OE-Core and Yocto are all fortunate in this regard as there was a fair\namount of architectural planning done early. This enabled us to be able to make major changes to the\narchitecture without too much pain and su\ufb00ering.\nElizabeth Flanagan\n357\n", "page": 371, "type": "text", "section": "Page 371"}
{"text": "23.4\nAcknowledgements\nFirst, thank you to Chris Larson, Michael Lauer, and Holger Schurig and the many, many people\nwho have contributed to BitBake, OpenEmbedded, OE-Core and Yocto over the years. Thank you\nalso goes to Richard Purdie for his letting me pick his brain, both on historical and technical aspects\nof OE, and for his constant encouragement and guidance, especially with some of the dark magic of\nBitBake.\n358\nYocto\n", "page": 372, "type": "text", "section": "Page 372"}
{"text": "[chapter24]\nZeroMQ\nMartin S\u00fastrik\n\u00d8MQ is a messaging system, or \u201cmessage-oriented middleware\u201d, if you will. It\u2019s used in environments\nas diverse as \ufb01nancial services, game development, embedded systems, academic research and\naerospace.\nMessaging systems work basically as instant messaging for applications. An application decides\nto communicate an event to another application (or multiple applications), it assembles the data to\nbe sent, hits the \u201csend\u201d button and there we go\u2014the messaging system takes care of the rest.\nUnlike instant messaging, though, messaging systems have no GUI and assume no human beings\nat the endpoints capable of intelligent intervention when something goes wrong. Messaging systems\nthus have to be both fault-tolerant and much faster than common instant messaging.\n\u00d8MQ was originally conceived as an ultra-fast messaging system for stock trading and so the\nfocus was on extreme optimization. The \ufb01rst year of the project was spent devising benchmarking\nmethodology and trying to de\ufb01ne an architecture that was as e\ufb03cient as possible.\nLater on, approximately in the second year of development, the focus shifted to providing a\ngeneric system for building distributed applications and supporting arbitrary messaging patterns,\nvarious transport mechanisms, arbitrary language bindings, etc.\nDuring the third year the focus was mainly on improving usability and \ufb02attening the learning\ncurve. We\u2019ve adopted the BSD Sockets API, tried to clean up the semantics of individual messaging\npatterns, and so on.\nHopefully, this chapter will give an insight into how the three goals above translated into the\ninternal architecture of \u00d8MQ, and provide some tips for those who are struggling with the same\nproblems.\nSince its third year \u00d8MQ has outgrown its codebase; there is an initiative to standardise the\nwire protocols it uses, and an experimental implementation of a \u00d8MQ-like messaging system inside\nthe Linux kernel, etc. These topics are not covered in this book. However, you can check online\nresources123 for further details.\n24.1\nApplication vs. Library\n\u00d8MQ is a library, not a messaging server. It took us several years working on AMQP protocol,\na \ufb01nancial industry attempt to standardise the wire protocol for business messaging\u2014writing a\n1http://www.250bpm.com/concepts\n2http://groups.google.com/group/sp-discuss-group\n3http://www.250bpm.com/hits\n", "page": 373, "type": "text", "section": "Page 373"}
{"text": "reference implementation for it and participating in several large-scale projects heavily based on\nmessaging technology\u2014to realise that there\u2019s something wrong with the classic client/server model\nof smart messaging server (broker) and dumb messaging clients.\nOur primary concern at the time was with the performance: If there\u2019s a server in the middle,\neach message has to pass the network twice (from the sender to the broker and from the broker to the\nreceiver) inducing a penalty in terms of both latency and throughput. Moreover, if all the messages\nare passed through the broker, at some point it\u2019s bound to become the bottleneck.\nA secondary concern was related to large-scale deployments: when the deployment crosses\norganisational boundaries the concept of a central authority managing the whole message \ufb02ow\ndoesn\u2019t apply any more. No company is willing to cede control to a server in di\ufb00erent company; there\nare trade secrets and there\u2019s legal liability. The result in practice is that there\u2019s one messaging server\nper company, with hand-written bridges to connect it to messaging systems in other companies. The\nwhole ecosystem is thus heavily fragmented, and maintaining a large number of bridges for every\ncompany involved doesn\u2019t make the situation better. To solve this problem, we need a fully distributed\narchitecture, an architecture where every component can be possibly governed by a di\ufb00erent business\nentity. Given that the unit of management in server-based architecture is the server, we can solve the\nproblem by installing a separate server for each component. In such a case we can further optimize\nthe design by making the server and the component share the same processes. What we end up with\nis a messaging library.\n\u00d8MQ was started when we got an idea about how to make messaging work without a central\nserver. It required turning the whole concept of messaging upside down and replacing the model of\nan autonomous centralised store of messages in the center of the network with a \u201csmart endpoint,\ndumb network\u201d architecture based on the end-to-end principle. The technical consequence of that\ndecision was that \u00d8MQ, from the very beginning, was a library, not an application.\nIn the meantime we\u2019ve been able to prove that this architecture is both more e\ufb03cient (lower\nlatency, higher throughput) and more \ufb02exible (it\u2019s easy to build arbitrary complex topologies instead\nof being tied to classic hub-and-spoke model).\nFigure 24.1: \u00d8MQ being used by di\ufb00erent libraries\nOne of the unintended consequences, however, was that opting for the library model improved\nthe usability of the product. Over and over again users express their happiness about the fact that\nthey don\u2019t have to install and manage a stand-alone messaging server. It turns out that not having a\nserver is a preferred option as it cuts operational cost (no need to have a messaging server admin)\nand improves time-to-market (no need to negotiate the need to run the server with the client, the\nmanagement or the operations team).\nThe lesson learned is that when starting a new project, you should opt for the library design if\nat all possible. It\u2019s pretty easy to create an application from a library by invoking it from a trivial\n360\nZeroMQ\n", "page": 374, "type": "text", "section": "Page 374"}
{"text": "program; however, it\u2019s almost impossible to create a library from an existing executable. A library\no\ufb00ers much more \ufb02exibility to the users, at the same time sparing them non-trivial administrative\ne\ufb00ort.\n24.2\nGlobal State\nGlobal variables don\u2019t play well with libraries. A library may be loaded several times in the process\nbut even then there\u2019s only a single set of global variables. Figure 24.1 shows a \u00d8MQ library being\nused from two di\ufb00erent and independent libraries. The application then uses both of those libraries.\nWhen such a situation occurs, both instances of \u00d8MQ access the same variables, resulting in\nrace conditions, strange failures and unde\ufb01ned behaviour.\nTo prevent this problem, the \u00d8MQ library has no global variables. Instead, a user of the library\nis responsible for creating the global state explicitly. The object containing the global state is called\ncontext. While from the user\u2019s perspective context looks more or less like a pool of worker threads,\nfrom \u00d8MQ\u2019s perspective it\u2019s just an object to store any global state that we happen to need. In the\npicture above, libA would have its own context and libB would have its own as well. There would\nbe no way for one of them to break or subvert the other one.\nThe lesson here is pretty obvious: Don\u2019t use global state in libraries. If you do, the library is\nlikely to break when it happens to be instantiated twice in the same process.\n24.3\nPerformance\nWhen \u00d8MQ was started, its primary goal was to optimize performance. Performance of messaging\nsystems is expressed using two metrics: throughput\u2014how many messages can be passed during a\ngiven amount of time; and latency\u2014how long it takes for a message to get from one endpoint to the\nother.\nWhich metric should we focus on? What\u2019s the relationship between the two? Isn\u2019t it obvious?\nRun the test, divide the overall time of the test by number of messages passed and what you get is\nlatency. Divide the number of messages by time and what you get is throughput. In other words,\nlatency is the inverse value of throughput. Trivial, right?\nInstead of starting coding straight away we spent some weeks investigating the performance\nmetrics in detail and we found out that the relationship between throughput and latency is much\nmore subtle than that, and often the metrics are quite counter-intuitive.\nImagine A sending messages to B. (See Figure 24.2.) The overall time of the test is 6 seconds.\nThere are 5 messages passed. Therefore the throughput is 0.83 msgs/sec ( 5\n6) and the latency is 1.2\nsec ( 6\n5), right?\nHave a look at the diagram again. It takes a di\ufb00erent time for each message to get from A to\nB: 2 sec, 2.5 sec, 3 sec, 3.5 sec, 4 sec. The average is 3 seconds, which is pretty far away from our\noriginal calculation of 1.2 second. This example shows the misconceptions people are intuitively\ninclined to make about performance metrics.\nNow have a look at the throughput. The overall time of the test is 6 seconds. However, at A it\ntakes just 2 seconds to send all the messages. From A\u2019s perspective the throughput is 2.5 msgs/sec\n( 5\n2). At B it takes 4 seconds to receive all messages. So from B\u2019s perspective the throughput is 1.25\nmsgs/sec ( 5\n4). Neither of these numbers matches our original calculation of 1.2 msgs/sec.\nTo make a long story short, latency and throughput are two di\ufb00erent metrics; that much is obvious.\nThe important thing is to understand the di\ufb00erence between the two and their mutual relationship.\nMartin S\u00fastrik\n361\n", "page": 375, "type": "text", "section": "Page 375"}
{"text": "Figure 24.2: Sending messages from A to B\nLatency can be measured only between two di\ufb00erent points in the system; There\u2019s no such thing\nas latency at point A. Each message has its own latency. You can average the latencies of multiple\nmessages; however, there\u2019s no such thing as latency of a stream of messages.\nThroughput, on the other hand, can be measured only at a single point of the system. There\u2019s a\nthroughput at the sender, there\u2019s a throughput at the receiver, there\u2019s a throughput at any intermediate\npoint between the two, but there\u2019s no such thing as overall throughput of the whole system. And\nthroughput make sense only for a set of messages; there\u2019s no such thing as throughput of a single\nmessage.\nAs for the relationship between throughput and latency, it turns out there really is a relationship;\nhowever, the formula involves integrals and we won\u2019t discuss it here. For more information, read the\nliterature on queueing theory.\nThere are many more pitfalls in benchmarking the messaging systems that we won\u2019t go further\ninto. The stress should rather be placed on the lesson learned: Make sure you understand the problem\nyou are solving. Even a problem as simple as \u201cmake it fast\u201d can take lot of work to understand properly.\nWhat\u2019s more, if you don\u2019t understand the problem, you are likely to build implicit assumptions and\npopular myths into your code, making the solution either \ufb02awed or at least much more complex or\nmuch less useful than it could possibly be.\n24.4\nCritical Path\nWe discovered during the optimization process that three factors have a crucial impact on performance:\n\u2022 Number of memory allocations\n\u2022 Number of system calls\n\u2022 Concurrency model\nHowever, not every memory allocation or every system call has the same e\ufb00ect on performance.\nThe performance we are interested in in messaging systems is the number of messages we can transfer\nbetween two endpoints during a given amount of time. Alternatively, we may be interested in how\nlong it takes for a message to get from one endpoint to another.\n362\nZeroMQ\n", "page": 376, "type": "text", "section": "Page 376"}
{"text": "However, given that \u00d8MQ is designed for scenarios with long-lived connections, the time it\ntakes to establish a connection or the time needed to handle a connection error is basically irrelevant.\nThese events happen very rarely and so their impact on overall performance is negligible.\nThe part of a codebase that gets used very frequently, over and over again, is called the critical\npath; optimization should focus on the critical path.\nLet\u2019s have a look at an example: \u00d8MQ is not extremely optimized with respect to memory alloca-\ntions. For example, when manipulating strings, it often allocates a new string for each intermediate\nphase of the transformation. However, if we look strictly at the critical path\u2014the actual message\npassing\u2014we\u2019ll \ufb01nd out that it uses almost no memory allocations. If messages are small, it\u2019s just one\nmemory allocation per 256 messages (these messages are held in a single large allocated memory\nchunk). If, in addition, the stream of messages is steady, without huge tra\ufb03c peaks, the number of\nmemory allocations on the critical path drops to zero (the allocated memory chunks are not returned\nto the system, but re-used over and over again).\nLesson learned: optimize where it makes di\ufb00erence. Optimizing pieces of code that are not on\nthe critical path is wasted e\ufb00ort.\n24.5\nAllocating Memory\nAssuming that all the infrastructure was initialised and a connection between two endpoints has been\nestablished, there\u2019s only one thing to allocate when sending a message: the message itself. Thus, to\noptimize the critical path we had to look into how messages are allocated and passed up and down\nthe stack.\nIt\u2019s common knowledge in the high-performance networking \ufb01eld that the best performance is\nachieved by carefully balancing the cost of message allocation and the cost of message copying4. For\nsmall messages, copying is much cheaper than allocating memory. It makes sense to allocate no new\nmemory chunks at all and instead to copy the message to preallocated memory whenever needed.\nFor large messages, on the other hand, copying is much more expensive than memory allocation.\nIt makes sense to allocate the message once and pass a pointer to the allocated block, instead of\ncopying the data. This approach is called \u201czero-copy\u201d.\nFigure 24.3: Message copying (or not)\n\u00d8MQ handles both cases in a transparent manner. A \u00d8MQ message is represented by an opaque\nhandle. The content of very small messages is encoded directly in the handle. So making a copy of\n4For example, http://hal.inria.fr/docs/00/29/28/31/PDF/Open-MX-IOAT.pdf. See di\ufb00erent handling of \u201csmall\u201d,\n\u201cmedium\u201d and \u201clarge\u201d messages.\nMartin S\u00fastrik\n363\n", "page": 377, "type": "text", "section": "Page 377"}
{"text": "the handle actually copies the message data. When the message is larger, it\u2019s allocated in a separate\nbu\ufb00er and the handle contains just a pointer to the bu\ufb00er. Making a copy of the handle doesn\u2019t result\nin copying the message data, which makes sense when the message is megabytes long (Figure 24.3).\nIt should be noted that in the latter case the bu\ufb00er is reference-counted so that it can be referenced by\nmultiple handles without the need to copy the data.\nLesson learned: When thinking about performance, don\u2019t assume there\u2019s a single best solution. It\nmay happen that there are several subclasses of the problem (e.g., small messages vs. large messages),\neach having its own optimal algorithm.\n24.6\nBatching\nIt has already been mentioned that the sheer number of system calls in a messaging system can\nresult in a performance bottleneck. Actually, the problem is much more generic than that. There\u2019s a\nnon-trivial performance penalty associated with traversing the call stack and thus, when creating\nhigh-performance applications, it\u2019s wise to avoid as much stack traversing as possible.\nFigure 24.4: Sending four messages\nConsider Figure 24.4. To send four messages, you have to traverse the entire network stack\nfour times (i.e., \u00d8MQ, glibc, user/kernel space boundary, TCP implementation, IP implementation,\nEthernet layer, the NIC itself and back up the stack again).\nHowever, if you decide to join those messages into a single batch, there would be only one\ntraversal of the stack (Figure 24.5). The impact on message throughput can be overwhelming: up to\ntwo orders of magnitude, especially if the messages are small and hundreds of them can be packed\ninto a single batch.\nFigure 24.5: Batching messages\nOn the other hand, batching can have negative impact on latency. Let\u2019s take, for example, the\nwell-known Nagle\u2019s algorithm, as implemented in TCP. It delays the outbound messages for a certain\n364\nZeroMQ\n", "page": 378, "type": "text", "section": "Page 378"}
{"text": "amount of time and merges all the accumulated data into a single packet. Obviously, the end-to-end\nlatency of the \ufb01rst message in the packet is much worse than the latency of the last one. Thus, it\u2019s\ncommon for applications that need consistently low latency to switch Nagle\u2019s algorithm o\ufb00. It\u2019s even\ncommon to switch o\ufb00batching on all levels of the stack (e.g., NIC\u2019s interrupt coalescing feature).\nBut again, no batching means extensive traversing of the stack and results in low message\nthroughput. We seem to be caught in a throughput versus latency dilemma.\n\u00d8MQ tries to deliver consistently low latencies combined with high throughput using the following\nstrategy: when message \ufb02ow is sparse and doesn\u2019t exceed the network stack\u2019s bandwidth, \u00d8MQ turns\nall the batching o\ufb00to improve latency. The trade-o\ufb00here is somewhat higher CPU usage\u2014we still\nhave to traverse the stack frequently. However, that isn\u2019t considered to be a problem in most cases.\nWhen the message rate exceeds the bandwidth of the network stack, the messages have to be\nqueued\u2014stored in memory till the stack is ready to accept them. Queueing means the latency is\ngoing to grow. If the message spends one second in the queue, end-to-end latency will be at least\none second. What\u2019s even worse, as the size of the queue grows, latencies will increase gradually. If\nthe size of the queue is not bound, the latency can exceed any limit.\nIt has been observed that even though the network stack is tuned for lowest possible latency\n(Nagle\u2019s algorithm switched o\ufb00, NIC interrupt coalescing turned o\ufb00, etc.) latencies can still be\ndismal because of the queueing e\ufb00ect, as described above.\nIn such situations it makes sense to start batching aggressively. There\u2019s nothing to lose as the\nlatencies are already high anyway. On the other hand, aggressive batching improves throughput and\ncan empty the queue of pending messages\u2014which in turn means the latency will gradually drop as\nthe queueing delay decreases. Once there are no outstanding messages in the queue, the batching\ncan be turned o\ufb00to improve the latency even further.\nOne additional observation is that the batching should only be done on the topmost level. If the\nmessages are batched there, the lower layers have nothing to batch anyway, and so all the batching\nalgorithms underneath do nothing except introduce additional latency.\nLesson learned: To get optimal throughput combined with optimal response time in an asyn-\nchronous system, turn o\ufb00all the batching algorithms on the low layers of the stack and batch on the\ntopmost level. Batch only when new data are arriving faster than they can be processed.\n24.7\nArchitecture Overview\nUp to this point we have focused on generic principles that make \u00d8MQ fast. From now on we\u2019ll have\na look at the actual architecture of the system (Figure 24.6).\nThe user interacts with \u00d8MQ using so-called \u201csockets\u201d. They are pretty similar to TCP sockets,\nthe main di\ufb00erence being that each socket can handle communication with multiple peers, a bit like\nunbound UDP sockets do.\nThe socket object lives in the user\u2019s thread (see the discussion of threading models in the next\nsection). Aside from that, \u00d8MQ is running multiple worker threads that handle the asynchronous part\nof the communication: reading data from the network, enqueueing messages, accepting incoming\nconnections, etc.\nThere are various objects living in the worker threads. Each of these objects is owned by exactly\none parent object (ownership is denoted by a simple full line in the diagram). The parent can live\nin a di\ufb00erent thread than the child. Most objects are owned directly by sockets; however, there are\ncouple of cases where an object is owned by an object which is owned by the socket. What we get is\na tree of objects, with one such tree per socket. The tree is used during shut down; no object can\nMartin S\u00fastrik\n365\n", "page": 379, "type": "text", "section": "Page 379"}
{"text": "Figure 24.6: \u00d8MQ architecture\nshut itself down until it closes all its children. This way we can ensure that the shut down process\nworks as expected; for example, that pending outbound messages are pushed to the network prior to\nterminating the sending process.\nRoughly speaking, there are two kinds of asynchronous objects; there are objects that are not\ninvolved in message passing and there are objects that are. The former have to do mainly with\nconnection management. For example, a TCP listener object listens for incoming TCP connections\nand creates an engine/session object for each new connection. Similarly, a TCP connector object\ntries to connect to the TCP peer and when it succeeds it creates an engine/session object to manage\nthe connection. When such connection fails, the connector object tries to re-establish it.\nThe latter are objects that are handling data transfer itself. These objects are composed of two\nparts: the session object is responsible for interacting with the \u00d8MQ socket, and the engine object\nis responsible for communication with the network. There\u2019s only one kind of the session object,\nbut there\u2019s a di\ufb00erent engine type for each underlying protocol \u00d8MQ supports. Thus, we have TCP\nengines, IPC (inter-process communication) engines, PGM5 engines, etc. The set of engines is\nextensible\u2014in the future we may choose to implement, say, a WebSocket engine or an SCTP engine.\nThe sessions are exchanging messages with the sockets. There are two directions to pass messages\nin and each direction is handled by a pipe object. Each pipe is basically a lock-free queue optimized\nfor fast passing of messages between threads.\n5Reliable multicast protocol, see RFC 3208.\n366\nZeroMQ\n", "page": 380, "type": "text", "section": "Page 380"}
{"text": "Finally, there\u2019s a context object (discussed in the previous sections but not shown on the diagram)\nthat holds the global state and is accessible by all the sockets and all the asynchronous objects.\n24.8\nConcurrency Model\nOne of the requirements for \u00d8MQ was to take advantage of multi-core boxes; in other words, to scale\nthe throughput linearly with the number of available CPU cores.\nOur previous experience with messaging systems showed that using multiple threads in a classic\nway (critical sections, semaphores, etc.) doesn\u2019t yield much performance improvement. In fact,\na multi-threaded version of a messaging system can be slower than a single-threaded one, even if\nmeasured on a multi-core box. Individual threads are simply spending too much time waiting for\neach other while, at the same time, eliciting a lot of context switching that slows the system down.\nGiven these problems, we\u2019ve decided to go for a di\ufb00erent model. The goal was to avoid locking\nentirely and let each thread run at full speed. The communication between threads was to be provided\nvia asynchronous messages (events) passed between the threads. This, as insiders know, is the classic\nactor model.\nThe idea was to launch one worker thread per CPU core\u2014having two threads sharing the same\ncore would only mean a lot of context switching for no particular advantage. Each internal \u00d8MQ\nobject, such as say, a TCP engine, would be tightly bound to a particular worker thread. That, in turn,\nmeans that there\u2019s no need for critical sections, mutexes, semaphores and the like. Additionally, these\n\u00d8MQ objects won\u2019t be migrated between CPU cores so would thus avoid the negative performance\nimpact of cache pollution (Figure 24.7).\nFigure 24.7: Multiple worker threads\nThis design makes a lot of traditional multi-threading problems disappear. Nevertheless, there\u2019s\na need to share the worker thread among many objects, which in turn means there has to be some\nkind of cooperative multitasking. This means we need a scheduler; objects need to be event-driven\nrather than being in control of the entire event loop; we have to take care of arbitrary sequences of\nevents, even very rare ones; we have to make sure that no object holds the CPU for too long; etc.\nIn short, the whole system has to become fully asynchronous. No object can a\ufb00ord to do a\nblocking operation, because it would not only block itself but also all the other objects sharing the\nsame worker thread. All objects have to become, whether explicitly or implicitly, state machines.\nMartin S\u00fastrik\n367\n", "page": 381, "type": "text", "section": "Page 381"}
{"text": "With hundreds or thousands of state machines running in parallel you have to take care of all the\npossible interactions between them and\u2014most importantly\u2014of the shutdown process.\nIt turns out that shutting down a fully asynchronous system in a clean way is a dauntingly complex\ntask. Trying to shut down a thousand moving parts, some of them working, some idle, some in the\nprocess of being initiated, some of them already shutting down by themselves, is prone to all kinds of\nrace conditions, resource leaks and similar. The shutdown subsystem is de\ufb01nitely the most complex\npart of \u00d8MQ. A quick check of the bug tracker indicates that some 30\u201350% of reported bugs are\nrelated to shutdown in one way or another.\nLesson learned: When striving for extreme performance and scalability, consider the actor model;\nit\u2019s almost the only game in town in such cases. However, if you are not using a specialised system\nlike Erlang or \u00d8MQ itself, you\u2019ll have to write and debug a lot of infrastructure by hand. Additionally,\nthink, from the very beginning, about the procedure to shut down the system. It\u2019s going to be the\nmost complex part of the codebase and if you have no clear idea how to implement it, you should\nprobably reconsider using the actor model in the \ufb01rst place.\n24.9\nLock-Free Algorithms\nLock-free algorithms have been in vogue lately. They are simple mechanisms for inter-thread\ncommunication that don\u2019t rely on the kernel-provided synchronisation primitives, such as mutexes\nor semaphores; rather, they do the synchronisation using atomic CPU operations, such as atomic\ncompare-and-swap (CAS). It should be understood that they are not literally lock-free\u2014instead,\nlocking is done behind the scenes on the hardware level.\n\u00d8MQ uses a lock-free queue in pipe objects to pass messages between the user\u2019s threads and\n\u00d8MQ\u2019s worker threads. There are two interesting aspects to how \u00d8MQ uses the lock-free queue.\nFirst, each queue has exactly one writer thread and exactly one reader thread. If there\u2019s a need\nfor 1-to-N communication, multiple queues are created (Figure 24.8). Given that this way the queue\ndoesn\u2019t have to take care of synchronising the writers (there\u2019s only one writer) or readers (there\u2019s\nonly one reader) it can be implemented in an extra-e\ufb03cient way.\nFigure 24.8: Queues\nSecond, we realised that while lock-free algorithms were more e\ufb03cient than classic mutex-based\nalgorithms, atomic CPU operations are still rather expensive (especially when there\u2019s contention\nbetween CPU cores) and doing an atomic operation for each message written and/or each message\nread was slower than we were willing to accept.\nThe way to speed it up\u2014once again\u2014was batching. Imagine you had 10 messages to be written\nto the queue. It can happen, for example, when you received a network packet containing 10 small\nmessages. Receiving a packet is an atomic event; you cannot get half of it. This atomic event results\nin the need to write 10 messages to the lock-free queue. There\u2019s not much point in doing an atomic\n368\nZeroMQ\n", "page": 382, "type": "text", "section": "Page 382"}
{"text": "operation for each message. Instead, you can accumulate the messages in a \u201cpre-write\u201d portion of the\nqueue that\u2019s accessed solely by the writer thread, and then \ufb02ush it using a single atomic operation.\nThe same applies to reading from the queue. Imagine the 10 messages above were already\n\ufb02ushed to the queue. The reader thread can extract each message from the queue using an atomic\noperation. However, it\u2019s overkill; instead, it can move all the pending messages to a \u201cpre-read\u201d\nportion of the queue using a single atomic operation. Afterwards, it can retrieve the messages from\nthe \u201cpre-read\u201d bu\ufb00er one by one. \u201cPre-read\u201d is owned and accessed solely by the reader thread and\nthus no synchronisation whatsoever is needed in that phase.\nFigure 24.9: Lock-free queue\nThe arrow on the left of Figure 24.9 shows how the pre-write bu\ufb00er can be \ufb02ushed to the queue\nsimply by modifying a single pointer. The arrow on the right shows how the whole content of the\nqueue can be shifted to the pre-read by doing nothing but modifying another pointer.\nLesson learned: Lock-free algorithms are hard to invent, troublesome to implement and almost\nimpossible to debug. If at all possible, use an existing proven algorithm rather than inventing your\nown. When extreme performance is required, don\u2019t rely solely on lock-free algorithms. While they\nare fast, the performance can be signi\ufb01cantly improved by doing smart batching on top of them.\n24.10\nAPI\nThe user interface is the most important part of any product. It\u2019s the only part of your program visible\nto the outside world and if you get it wrong the world will hate you. In end-user products it\u2019s either\nthe GUI or the command line interface. In libraries it\u2019s the API.\nIn early versions of \u00d8MQ the API was based on AMQP\u2019s model of exchanges and queues6. I\nspent the end of 2009 rewriting it almost from scratch to use the BSD Socket API instead. That was\nthe turning point; \u00d8MQ adoption soared from that point on. While before it was a niche product\nused by a bunch of messaging experts, afterwards it became a handy commonplace tool for anybody.\nIn a year or so the size of the community increased tenfold, some 20 bindings to di\ufb00erent languages\nwere implemented, etc.\nThe user interface de\ufb01nes the perception of a product. With basically no change to the functionality\u2014\njust by changing the API\u2014\u00d8MQ changed from an \u201centerprise messaging\u201d product to a \u201cnetworking\u201d\nproduct. In other words, the perception changed from \u201ca complex piece of infrastructure for big\nbanks\u201d to \u201chey, this helps me to send my 10-byte-long message from application A to application B\u201d.\n6See the AMQP speci\ufb01cation at http://www.amqp.org/specification/1.0/amqp-org-download. From a historical\nperspective it\u2019s interesting to have a look at the white paper from 2007 that tries to reconcile AMQP with a brokerless model\nof messaging. The white paper is at http://www.zeromq.org/whitepapers:messaging-enabled-network.\nMartin S\u00fastrik\n369\n", "page": 383, "type": "text", "section": "Page 383"}
{"text": "Lesson learned: Understand what you want your project to be and design the user interface\naccordingly. Having a user interface that doesn\u2019t align with the vision of the project is a 100%\nguaranteed way to fail.\nOne of the important aspects of the move to the BSD Sockets API was that it wasn\u2019t a revolutionary\nfreshly invented API, but an existing and well-known one. Actually, the BSD Sockets API is one of\nthe oldest APIs still in active use today; it dates back to 1983 and 4.2BSD Unix. It\u2019s been widely\nused and stable for literally decades.\nThe above fact brings a lot of advantages. Firstly, it\u2019s an API that everybody knows, so the\nlearning curve is ludicrously \ufb02at. Even if you\u2019ve never heard of \u00d8MQ, you can build your \ufb01rst\napplication in couple of minutes thanks to the fact that you are able to reuse your BSD Sockets\nknowledge.\nSecondly, using a widely implemented API enables integration of \u00d8MQ with existing technologies.\nFor example, exposing \u00d8MQ objects as \u201csockets\u201d or \u201c\ufb01le descriptors\u201d allows for processing TCP,\nUDP, pipe, \ufb01le and \u00d8MQ events in the same event loop. Another example: the experimental project\nto bring \u00d8MQ-like functionality to the Linux kernel7 turned out to be pretty simple to implement.\nBy sharing the same conceptual framework it can re-use a lot of infrastructure already in place.\nThirdly and probably most importantly, the fact that the BSD Sockets API survived almost three\ndecades despite numerous attempts to replace it means that there is something inherently right in\nthe design. BSD Sockets API designers have\u2014whether deliberately or by chance\u2014made the right\ndesign decisions. By adopting the API we can automatically share those design decisions without\neven knowing what they were and what problem they were solving.\nLesson learned: While code reuse has been promoted from time immemorial and pattern reuse\njoined in later on, it\u2019s important to think of reuse in an even more generic way. When designing a\nproduct, have a look at similar products. Check which have failed and which have succeeded; learn\nfrom the successful projects. Don\u2019t succumb to Not Invented Here syndrome. Reuse the ideas, the\nAPIs, the conceptual frameworks, whatever you \ufb01nd appropriate. By doing so you are allowing users\nto reuse their existing knowledge. At the same time you may be avoiding technical pitfalls you are\nnot even aware of at the moment.\n24.11\nMessaging Patterns\nIn any messaging system, the most important design problem is that of how to provide a way for the\nuser to specify which messages are routed to which destinations. There are two main approaches,\nand I believe this dichotomy is quite generic and applicable to basically any problem encountered in\nthe domain of software.\nOne approach is to adopt the Unix philosophy of \u201cdo one thing and do it well\u201d. What this means\nis that the problem domain should be arti\ufb01cially restricted to a small and well-understood area. The\nprogram should then solve this restricted problem in a correct and exhaustive way. An example of\nsuch approach in the messaging area is MQTT8. It\u2019s a protocol for distributing messages to a set of\nconsumers. It can\u2019t be used for anything else (say for RPC) but it is easy to use and does message\ndistribution well.\nThe other approach is to focus on generality and provide a powerful and highly con\ufb01gurable\nsystem. AMQP is an example of such a system. Its model of queues and exchanges provides the\n7https://github.com/250bpm/linux-2.6\n8http://mqtt.org/\n370\nZeroMQ\n", "page": 384, "type": "text", "section": "Page 384"}
{"text": "user with the means to programmatically de\ufb01ne almost any routing algorithm they can think of. The\ntrade-o\ufb00, of course, is a lot of options to take care of.\n\u00d8MQ opts for the former model because it allows the resulting product to be used by basically\nanyone, while the generic model requires messaging experts to use it. To demonstrate the point, let\u2019s\nhave a look how the model a\ufb00ects the complexity of the API. What follows is implementation of\nRPC client on top of a generic system (AMQP):\nconnect (\"192.168.0.111\")\nexchange.declare (exchange=\"requests\", type=\"direct\", passive=false,\ndurable=true, no-wait=true, arguments={})\nexchange.declare (exchange=\"replies\", type=\"direct\", passive=false,\ndurable=true, no-wait=true, arguments={})\nreply-queue = queue.declare (queue=\"\", passive=false, durable=false,\nexclusive=true, auto-delete=true, no-wait=false, arguments={})\nqueue.bind (queue=reply-queue, exchange=\"replies\",\nrouting-key=reply-queue)\nqueue.consume (queue=reply-queue, consumer-tag=\"\", no-local=false,\nno-ack=false, exclusive=true, no-wait=true, arguments={})\nrequest = new-message (\"Hello World!\")\nrequest.reply-to = reply-queue\nrequest.correlation-id = generate-unique-id ()\nbasic.publish (exchange=\"requests\", routing-key=\"my-service\",\nmandatory=true, immediate=false)\nreply = get-message ()\nOn the other hand, \u00d8MQ splits the messaging landscape into so-called \u201cmessaging patterns\u201d.\nExamples of the patterns are \u201cpublish/subscribe\u201d, \u201crequest/reply\u201d or \u201cparallelised pipeline\u201d. Each\nmessaging pattern is completely orthogonal to other patterns and can be thought of as a separate tool.\nWhat follows is the re-implementation of the above application using \u00d8MQ\u2019s request/reply pattern.\nNote how all the option tweaking is reduced to the single step of choosing the right messaging pattern\n(\u201cREQ\u201d):\ns = socket (REQ)\ns.connect (\"tcp://192.168.0.111:5555\")\ns.send (\"Hello World!\")\nreply = s.recv ()\nUp to this point we\u2019ve argued that speci\ufb01c solutions are better than generic solutions. We want\nour solution to be as speci\ufb01c as possible. However, at the same time we want to provide our customers\nwith as wide a range of functionality as possible. How can we solve this apparent contradiction?\nThe answer consists of two steps:\n1. De\ufb01ne a layer of the stack to deal with a particular problem area (e.g. transport, routing,\npresentation, etc.).\n2. Provide multiple implementations of the layer. There should be a separate non-intersecting\nimplementation for each use case.\nLet\u2019s have a look at the example of the transport layer in the Internet stack. It\u2019s meant to provide\nservices such as transferring data streams, applying \ufb02ow control, providing reliability, etc., on the\ntop of the network layer (IP). It does so by de\ufb01ning multiple non-intersecting solutions: TCP for\nconnection-oriented reliable stream transfer, UDP for connectionless unreliable packet transfer, SCTP\nfor transfer of multiple streams, DCCP for unreliable connections and so on.\nMartin S\u00fastrik\n371\n", "page": 385, "type": "text", "section": "Page 385"}
{"text": "Note that each implementation is completely orthogonal: a UDP endpoint cannot speak to\na TCP endpoint. Neither can a SCTP endpoint speak to a DCCP endpoint. It means that new\nimplementations can be added to the stack at any moment without a\ufb00ecting the existing portions of\nthe stack. Conversely, failed implementations can be forgotten and discarded without compromising\nthe viability of the transport layer as a whole.\nThe same principle applies to messaging patterns as de\ufb01ned by \u00d8MQ. Messaging patterns\nform a layer (the so-called \u201cscalability layer\u201d) on top of the transport layer (TCP and friends).\nIndividual messaging patterns are implementations of this layer. They are strictly orthogonal\u2014the\npublish/subscribe endpoint can\u2019t speak to the request/reply endpoint, etc. Strict separation between\nthe patterns in turn means that new patterns can be added as needed and that failed experiments with\nnew patterns won\u2019t hurt the existing patterns.\nLesson learned: When solving a complex and multi-faceted problem it may turn out that a\nmonolithic general-purpose solution may not be the best way to go. Instead, we can think of the\nproblem area as an abstract layer and provide multiple implementations of this layer, each focused on\na speci\ufb01c well-de\ufb01ned use case. When doing so, delineate the use case carefully. Be sure about what\nis in the scope and what is not. By restricting the use case too aggressively the application of your\nsoftware may be limited. If you de\ufb01ne the problem too broadly, however, the product may become\ntoo complex, blurry and confusing for the users.\n24.12\nConclusion\nAs our world becomes populated with lots of small computers connected via the Internet\u2014mobile\nphones, RFID readers, tablets and laptops, GPS devices, etc.\u2014the problem of distributed computing\nceases to be the domain of academic science and becomes a common everyday problem for every\ndeveloper to tackle. The solutions, unfortunately, are mostly domain-speci\ufb01c hacks. This article\nsummarises our experience with building a large-scale distributed system in a systematic manner. It\nfocuses on problems that are interesting from a software architecture point of view, and we hope that\ndesigners and programmers in the open source community will \ufb01nd it useful.\n372\nZeroMQ\n", "page": 386, "type": "text", "section": "Page 386"}
{"text": "Bibliography\n[aut12]\nAutoconf, 2012.\n[CKP05]\nManuel M. T. Chakravarty, Gabriele Keller, and Simon Peyton Jones. Associated type\nsynonyms. In Proceedings of the Tenth ACM SIGPLAN International Conference on\nFunctional Programming, ICFP \u201905, pages 241\u2013253. ACM, 2005.\n[ecl12]\nEclipse home page, 2012.\n[Ent11a]\nEnthought. Kiva source code, 2011.\n[Ent11b]\nEnthought. PDF reference and Adobe extensions to the PDF speci\ufb01cation, 2011.\n[For09]\nMessage Passing Interface Forum. MPI: A Message-Passing Interface Standard,\nVersion 2.2, September 2009.\n[GFB+04]\nEdgar Gabriel, Graham E. Fagg, George Bosilca, Thara Angskun, Jack J. Dongarra,\nJe\ufb00rey M. Squyres, Vishal Sahay, Prabhanjan Kambadur, Brian Barrett, Andrew Lums-\ndaine, Ralph H. Castain, David J. Daniel, Richard L. Graham, and Timothy S. Woodall.\nOpen MPI: Goals, concept, and design of a next generation MPI implementation. In\nProc. 11th European PVM/MPI Users\u2019 Group Meeting, pages 97\u2013104, September\n2004.\n[GGHL+96] Al Geist, William Gropp, Steve Huss-Lederman, Andrew Lumsdaine, Ewin Lusk,\nWilliam Saphir, Tony Skjellum, and Mark Snir. MPI-2: Extending the Message-\nPassing Interface. In Euro-Par \u201996 Parallel Processing, pages 128\u2013135. Springer\nVerlag, 1996.\n[GLP93]\nA. Gill, J. Launchbury, and S. L. Peyton Jones. A Short Cut to Deforestation. In\nACM Conference on Functional Programming and Computer Architecture (FPCA\u201993).\nACM, 1993.\n[HHPW07]\nPaul Hudak, John Hughes, Simon Peyton Jones, and Philip Wadler. A History of\nHaskell: being lazy with class. In Proceedings of the Third ACM SIGPLAN Conference\non the History of Programming Languages, pages 12\u20131\u201312\u201355. ACM, 2007.\n[Knu86]\nDonald E. Knuth. Computers & Typesetting B: TeX: The Program. Addison Wesley,\n1986.\n[Mar10]\nThe Haskell 2010 Report, 2010.\nhttp://www.haskell.org/haskellwiki/\nLanguage_and_library_specification#The_Haskell_2010_report.\n", "page": 387, "type": "text", "section": "Page 387"}
{"text": "[McG07]\nPaul McGuire. Getting Started with Pyparsing. O\u2019Reilly Media, 2007.\n[Mes93]\nMessage Passing Interface Forum. MPI: A Message Passing Interface. In Proc.\nSupercomputing \u201993, pages 878\u2013883. IEEE Computer Society Press, November 1993.\n[MHJP08]\nSimon Marlow, Tim Harris, Roshan P. James, and Simon Peyton Jones. Parallel\ngenerational-copying garbage collection with a block-structured heap. In Proceedings\nof the 7th International Symposium on Memory Management, ISMM \u201908, pages 11\u201320.\nACM, 2008.\n[MPT04]\nSimon Marlow, Simon Peyton Jones, and Wolfgang Thaller. Extending the Haskell\nForeign Function Interface with Concurrency. In Proceedings of the ACM SIGPLAN\nWorkshop on Haskell, pages 57\u201368, September 2004.\n[PM02]\nSimon Peyton Jones and Simon Marlow. Secrets of the Glasgow Haskell Compiler\ninliner. Journal of Functional Programming, 12:393\u2013434, July 2002.\n[PTH01]\nSimon L. Peyton Jones, Andrew Tolmach, and Tony Hoare. Playing by the rules:\nrewriting as a practical optimisation technique in GHC. In Ralf Hinze, editor, 2001\nHaskell Workshop. ACM SIGPLAN, September 2001.\n[PVWW06] Simon Peyton Jones, Dimitrios Vytiniotis, Stephanie Weirich, and Geo\ufb00rey Washburn.\nSimple uni\ufb01cation-based type inference for GADTs. In Proceedings of the Eleventh\nACM SIGPLAN International Conference on Functional Programming, pages 50\u201361.\nACM, 2006.\n[Ray]\nWhy GPSes suck, and what to do about it. http://esr.ibiblio.org/?p=801.\n[SCPD07]\nMartin Sulzmann, Manuel Chakravarty, Simon Peyton Jones, and Kevin Donnelly.\nSystem F with type equality coercions. In ACM SIGPLAN International Workshop on\nTypes in Language Design and Implementation (TLDI\u201907). ACM, 2007.\n[She06]\nMaxim Shemanarev. Anti-Grain Geometry: A high quality rendering engine for C++,\n2002-2006.\n[SPS+00]\nRichard M. Stallman, Roland Pesch, Stan Shebs, et al. Debugging with GDB: The\nGNU Source-Level Debugger. Free Software Foundation, 2000.\n374\nBIBLIOGRAPHY\n", "page": 388, "type": "text", "section": "Page 388"}
{"text": "Colophon\nThe image on the cover is of New York\u2019s Equitable Building, photographed and modi\ufb01ed by James\nHowe (http://www.jameshowephotography.com).\nThe cover font is Junction by Caroline Hadilaksono. The text font is T\nEXGyre Termes and the heading\nfont is T\nEXGyre Heros, both by Bogus\u0142aw Jackowski and Janusz M. Nowacki. The code font is\nInconsolata by Raph Levien.\n", "page": 389, "type": "text", "section": "Page 389"}
