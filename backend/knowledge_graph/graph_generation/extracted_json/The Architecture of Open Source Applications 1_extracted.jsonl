{"text": "The Architecture of Open Source\nApplications\nAmy Brown and Greg Wilson (eds.)\nLulu.com, 2011, 978-1-257-63801-7 \nLicense / Buy / Contribute\nArchitects look at thousands of buildings during their training, and study critiques of those\nbuildings written by masters. In contrast, most software developers only ever get to know a\nhandful of large programs well\u2014usually programs they wrote themselves\u2014and never study the\ngreat programs of history. As a result, they repeat one another's mistakes rather than building on\none another's successes.\nThis book's goal is to change that. In it, the authors of twenty-five open source applications\nexplain how their software is structured, and why. What are each program's major components?\nHow do they interact? And what did their builders learn during their development? In answering\nthese questions, the contributors to this book provide unique insights into how they think.\nIf you are a junior developer, and want to learn how your more experienced colleagues think, this\nbook is the place to start. If you are an intermediate or senior developer, and want to see how\nyour peers have solved hard design problems, this book can help you too.\nContents\nIntroduction\nAmy Brown and Greg Wilson\nix\n1. Asterisk\nRussell Bryant\n1\n2. Audacity\nJames Crook\n15\n3. The Bourne-Again Shell\nChet Ramey\n29\n4. Berkeley DB\nMargo Seltzer and Keith Bostic\n45\n5. CMake\nBill Hoffman and Kenneth Martin\n67\n6. Eclipse\nKim Moir\n77\n7. Graphite\nChris Davis\n101\n8. The Hadoop Distributed\nFile System\nRobert Chansler, Hairong Kuang, Sanjay Radia,\nKonstantin Shvachko, and Suresh Srinivas\n111\n9. Continuous Integration\nC. Titus Brown and Rosangela Canino-Koning\n125\n10. Jitsi\nEmil Ivov\n139\n", "page": 1, "type": "text", "section": "Page 1"}
{"text": "11. LLVM\nChris Lattner\n155\n12. Mercurial\nDirkjan Ochtman\n171\n13. The NoSQL Ecosystem\nAdam Marcus\n185\n14. Python Packaging\nTarek Ziad\u00e9\n205\n15. Riak and Erlang/OTP\nFrancesco Cesarini, Andy Gross, and Justin Sheehy\n229\n16. Selenium WebDriver\nSimon Stewart\n245\n17. Sendmail\nEric Allman\n271\n18. SnowFlock\nRoy Bryant and Andr\u00e9s Lagar-Cavilla\n291\n19. SocialCalc\nAudrey Tang\n303\n20. Telepathy\nDanielle Madeley\n325\n21. Thousand Parsec\nAlan Laudicina and Aaron Mavrinac\n345\n22. Violet\nCay Horstmann\n361\n23. VisTrails\nJuliana Freire, David Koop, Emanuele Santos,\nCarlos Scheidegger, Claudio Silva, and Huy T. Vo\n377\n24. VTK\nBerk Geveci and Will Schroeder\n395\n25. Battle For Wesnoth\nRichard Shimooka and David White\n411\nBibliography\nMaking Software\nThis work is made available under the Creative Commons Attribution 3.0 Unported license. \nAll royalties from sales of this book will be donated to Amnesty International.\nFollow us at http://third-bit.com or search for #aosa on Twitter.\nPurchasing\nCopies of this book may be purchased from Lulu.com and other online booksellers. All royalties\nfrom these sales will be donated to Amnesty International. If you do buy a copy, please buy\ndirectly from Lulu:\nLulu Amazon\nYou pay:\n$35.00\n$35.00\nLulu gets:\n$3.74\n$0.94\nAmazon gets:\n$17.50\nAmnesty gets: $14.98\n$3.78\nContributing\nDozens of volunteers worked hard to create this book, but there is still lots to do. You can help by\nreporting errors, by helping to translate the content into other languages, or by describing the\narchitecture of other open source projects. Please contact us at aosa@aosabook.org if you would\nlike to get involved.\n", "page": 2, "type": "text", "section": "Page 2"}
{"text": "The Architecture of\nOpen Source Applications\nAmy Brown and Greg Wilson (eds.)\nISBN 978-1-257-63801-7 \nLicense / Buy / Contribute\nIntroduction\nAmy Brown and Greg Wilson\nCarpentry is an exacting craft, and people can spend their entire lives learning how to do it well.\nBut carpentry is not architecture: if we step back from pitch boards and miter joints, buildings as a\nwhole must be designed, and doing that is as much an art as it is a craft or science.\nProgramming is also an exacting craft, and people can spend their entire lives learning how to do it\nwell. But programming is not software architecture. Many programmers spend years thinking\nabout (or wrestling with) larger design issues: Should this application be extensible? If so, should\nthat be done by providing a scripting interface, through some sort of plugin mechanism, or in\nsome other way entirely? What should be done by the client, what should be left to the server,\nand is \"client-server\" even a useful way to think about this application? These are not\nprogramming questions, any more than where to put the stairs is a question of carpentry.\nBuilding architecture and software architecture have a lot in common, but there is one crucial\ndifference. While architects study thousands of buildings in their training and during their careers,\nmost software developers only ever get to know a handful of large programs well. And more often\nthan not, those are programs they wrote themselves. They never get to see the great programs\nof history, or read critiques of those programs' design written by experienced practitioners. As a\nresult, they repeat one another's mistakes rather than building on one another's successes.\nThis book is our attempt to change that. Each chapter describes the architecture of an open\nsource application: how it is structured, how its parts interact, why it's built that way, and what\nlessons have been learned that can be applied to other big design problems. The descriptions are\nwritten by the people who know the software best, people with years or decades of experience\ndesigning and re-designing complex applications. The applications themselves range in scale from\nsimple drawing programs and web-based spreadsheets to compiler toolkits and multi-million line\nvisualization packages. Some are only a few years old, while others are approaching their thirtieth\nanniversary. What they have in common is that their creators have thought long and hard about\ntheir design, and are willing to share those thoughts with you. We hope you enjoy what they have\nwritten.\nContributors\nEric P. Allman (Sendmail): Eric Allman is the original author of sendmail, syslog, and trek, and the\nco-founder of Sendmail, Inc. He has been writing open source software since before it had a\nname, much less became a \"movement\". He is a member of the ACM Queue Editorial Review\nBoard and the Cal Performances Board of Trustees. His personal web site is\nhttp://www.neophilic.com/~eric.\nKeith Bostic (Berkeley DB): Keith was a member of the University of California Berkeley Computer\nSystems Research Group, where he was the architect of the 2.10BSD release and a principal\ndeveloper of 4.4BSD and related releases. He received the USENIX Lifetime Achievement Award\n(\"The Flame\"), which recognizes singular contributions to the Unix community, as well as a\nDistinguished Achievement Award from the University of California, Berkeley, for making the 4BSD\nrelease Open Source. Keith was the architect and one of the original developers of Berkeley DB,\nthe Open Source embedded database system.\nAmy Brown (editorial): Amy has a bachelor's degree in Mathematics from the University of\n", "page": 3, "type": "text", "section": "Page 3"}
{"text": "Waterloo, and worked in the software industry for ten years. She now writes and edits books,\nsometimes about software. She lives in Toronto and has two children and a very old cat.\nC. Titus Brown (Continuous Integration): Titus has worked in evolutionary modeling, physical\nmeteorology, developmental biology, genomics, and bioinformatics. He is now an Assistant\nProfessor at Michigan State University, where he has expanded his interests into several new\nareas, including reproducibility and maintainability of scientific software. He is also a member of the\nPython Software Foundation, and blogs at http://ivory.idyll.org.\nRoy Bryant (Snowflock): In 20 years as a software architect and CTO, Roy designed systems\nincluding Electronics Workbench (now National Instruments' Multisim) and the Linkwalker Data\nPipeline, which won Microsoft's worldwide Winning Customer Award for High-Performance\nComputing in 2006. After selling his latest startup, he returned to the University of Toronto to do\ngraduate studies in Computer Science with a research focus on virtualization and cloud\ncomputing. Most recently, he published his Kaleidoscope extensions to Snowflock at ACM's\nEurosys Conference in 2011. His personal web site is http://www.roybryant.net/.\nRussell Bryant (Asterisk): Russell is the Engineering Manager for the Open Source Software team\nat Digium, Inc. He has been a core member of the Asterisk development team since the Fall of\n2004. He has since contributed to almost all areas of Asterisk development, from project\nmanagement to core architectural design and development. He blogs at\nhttp://www.russellbryant.net.\nRosangela Canino-Koning (Continuous Integration): After 13 years of slogging in the software\nindustry trenches, Rosangela returned to university to pursue a Ph.D. in Computer Science and\nEvolutionary Biology at Michigan State University. In her copious spare time, she likes to read, hike,\ntravel, and hack on open source bioinformatics software. She blogs at http://www.voidptr.net.\nFrancesco Cesarini (Riak): Francesco Cesarini has used Erlang on a daily basis since 1995, having\nworked in various turnkey projects at Ericsson, including the OTP R1 release. He is the founder of\nErlang Solutions and co-author of O'Reilly's Erlang Programming. He currently works as Technical\nDirector at Erlang Solutions, but still finds the time to teach graduates and undergraduates alike at\nOxford University in the UK and the IT University of Gotheburg in Sweden.\nRobert Chansler (HDFS): Robert is a Senior Manager for Software Development at Yahoo!. After\ngraduate studies in distributed systems at Carnegie-Mellon University, he worked on compilers\n(Tartan Labs), printing and imaging systems (Adobe Systems), electronic commerce (Adobe\nSystems, Impresse), and storage area network management (SanNavigator, McDATA). Returning\nto distributed systems and HDFS, Rob found many familiar problems, but all of the numbers had\ntwo or three more zeros.\nJames Crook (Audacity): James is a contract software developer based in Dublin, Ireland. Currently\nhe is working on tools for electronics design, though in a previous life he developed bioinformatics\nsoftware. He has many audacious plans for Audacity, and he hopes some, at least, will see the\nlight of day.\nChris Davis (Graphite): Chris is a software consultant and Google engineer who has been designing\nand building scalable monitoring and automation tools for over 12 years. Chris originally wrote\nGraphite in 2006 and has lead the open source project ever since. When he's not writing code he\nenjoys cooking, making music, and doing research. His research interests include knowledge\nmodeling, group theory, information theory, chaos theory, and complex systems.\nJuliana Freire (VisTrails): Juliana is an Associate Professor of Computer Science, at the University of\nUtah. Before, she was member of technical staff at the Database Systems Research Department\nat Bell Laboratories (Lucent Technologies) and an Assistant Professor at OGI/OHSU. Her research\ninterests include provenance, scientific data management, information integration, and Web\nmining. She is a recipient of an NSF CAREER and an IBM Faculty award. Her research has been\nfunded by the National Science Foundation, Department of Energy, National Institutes of Health,\nIBM, Microsoft and Yahoo!.\nBerk Geveci (VTK): Berk is the Director of Scientific Computing at Kitware. He is responsible for\n", "page": 4, "type": "text", "section": "Page 4"}
{"text": "leading the development effort of ParaView, and award winning visualization application based on\nVTK. His research interests include large scale parallel computing, computational dynamics, finite\nelements and visualization algorithms.\nAndy Gross (Riak): Andy Gross is Principal Architect at Basho Technologies, managing the design\nand development of Basho's Open Source and Enterprise data storage systems. Andy started at\nBasho in December of 2007 with 10 years of software and distributed systems engineering\nexperience. Prior to Basho, Andy held senior distributed systems engineering positions at Mochi\nMedia, Apple, Inc., and Akamai Technologies.\nBill Hoffman (CMake): Bill is CTO and co-Founder of Kitware, Inc. He is a key developer of the\nCMake project, and has been working with large C++ systems for over 20 years.\nCay Horstmann (Violet): Cay is a professor of computer science at San Jose State University, but\never so often, he takes a leave of absence to work in industry or teach in a foreign country. He is\nthe author of many books on programming languages and software design, and the original\nauthor of the Violet and GridWorld open-source programs.\nEmil Ivov (Jitsi): Emil is the founder and project lead of the Jitsi project (previously SIP\nCommunicator). He is also involved with other initiatives like the ice4j.org, and JAIN SIP projects.\nEmil obtained his Ph.D. from the University of Strasbourg in early 2008, and has been focusing\nprimarily on Jitsi related activities ever since.\nDavid Koop (VisTrails): David is a Ph.D. candidate in computer science at the University of Utah\n(finishing in the summer of 2011). His research interests include visualization, provenance, and\nscientific data management. He is a lead developer of the VisTrails system, and a senior software\narchitect at VisTrails, Inc.\nHairong Kuang (HDFS) is a long time contributor and committer to the Hadoop project, which she\nhas passionately worked on currently at Facebook and previously at Yahoo!. Prior to industry, she\nwas an Assistant Professor at California State Polytechnic University, Pomona. She received Ph.D.\nin Computer Science from the University of California at Irvine. Her interests include cloud\ncomputing, mobile agents, parallel computing, and distributed systems.\nH. Andr\u00e9s Lagar-Cavilla (Snowflock): Andr\u00e9s is a software systems researcher who does\nexperimental work on virtualization, operating systems, security, cluster computing, and mobile\ncomputing. He has a B.A.Sc. from Argentina, and an M.Sc. and Ph.D. in Computer Science from\nUniversity of Toronto, and can be found online at http://lagarcavilla.org.\nChris Lattner (LLVM): Chris is a software developer with a diverse range of interests and\nexperiences, particularly in the area of compiler tool chains, operating systems, graphics and\nimage rendering. He is the designer and lead architect of the Open Source LLVM Project. See\nhttp://nondot.org/~sabre/ for more about Chris and his projects.\nAlan Laudicina (Thousand Parsec): Alan is an M.Sc. student in computer science at Wayne State\nUniversity, where he studies distributed computing. In his spare time he codes, learns\nprogramming languages, and plays poker. You can find more about him at http://alanp.ca/.\nDanielle Madeley (Telepathy): Danielle is an Australian software engineer working on Telepathy and\nother magic for Collabora Ltd. She has bachelor's degrees in electronic engineering and computer\nscience. She also has an extensive collection of plush penguins. She blogs at\nhttp://blogs.gnome.org/danni/.\nAdam Marcus (NoSQL): Adam is a Ph.D. student focused on the intersection of database systems\nand social computing at MIT's Computer Science and Artificial Intelligence Lab. His recent work ties\ntraditional database systems to social streams such as Twitter and human computation platforms\nsuch as Mechanical Turk. He likes to build usable open source systems from his research\nprototypes, and prefers tracking open source storage systems to long walks on the beach. He\nblogs at http://blog.marcua.net.\nKenneth Martin (CMake): Ken is currently Chairman and CFO of Kitware, Inc., a research and\n", "page": 5, "type": "text", "section": "Page 5"}
{"text": "development company based in the US. He co-founded Kitware in 1998 and since then has helped\ngrow the company to its current position as a leading R&D provider with clients across many\ngovernment and commercial sectors.\nAaron Mavrinac (Thousand Parsec): Aaron is a Ph.D. candidate in electrical and computer\nengineering at the University of Windsor, researching camera networks, computer vision, and\nrobotics. When there is free time, he fills some of it working on Thousand Parsec and other free\nsoftware, coding in Python and C, and doing too many other things to get good at any of them.\nHis web site is http://www.mavrinac.com.\nKim Moir (Eclipse): Kim works at the IBM Rational Software lab in Ottawa as the Release\nEngineering lead for the Eclipse and Runtime Equinox projects and is a member of the Eclipse\nArchitecture Council. Her interests lie in build optimization, Equinox and building component based\nsoftware. Outside of work she can be found hitting the pavement with her running mates,\npreparing for the next road race. She blogs at http://relengofthenerds.blogspot.com/.\nDirkjan Ochtman (Mercurial): Dirkjan graduated as a Master in CS in 2010, and has been working\nat a financial startup for 3 years. When not procrastinating in his free time, he hacks on Mercurial,\nPython, Gentoo Linux and a Python CouchDB library. He lives in the beautiful city of Amsterdam.\nHis personal web site is http://dirkjan.ochtman.nl/.\nSanjay Radia (HDFS): Sanjay is the architect of the Hadoop project at Yahoo!, and a Hadoop\ncommitter and Project Management Committee member at the Apache Software Foundation.\nPreviously he held senior engineering positions at Cassatt, Sun Microsystems and INRIA where he\ndeveloped software for distributed systems and grid/utility computing infrastructures. Sanjay has\nPh.D. in Computer Science from University of Waterloo, Canada.\nChet Ramey (Bash): Chet has been involved with bash for more than twenty years, the past\nseventeen as primary developer. He is a longtime employee of Case Western Reserve University in\nCleveland, Ohio, from which he received his B.Sc. and M.Sc. degrees. He lives near Cleveland with\nhis family and pets, and can be found online at http://tiswww.cwru.edu/~chet.\nEmanuele Santos (VisTrails): Emanuele is a research scientist at the University of Utah. Her\nresearch interests include scientific data management, visualization, and provenance. She received\nher Ph.D. in Computing from the University of Utah in 2010. She is also a lead developer of the\nVisTrails system.\nCarlos Scheidegger (VisTrails): Carlos has a Ph.D. in Computing from the University of Utah, and is\nnow a researcher at AT&T Labs\u2013Research. Carlos has won best paper awards at IEEE Visualization\nin 2007, and Shape Modeling International in 2008. His research interests include data visualization\nand analysis, geometry processing and computer graphics.\nWill Schroeder (VTK): Will is President and co-Founder of Kitware, Inc. He is a computational\nscientist by training and has been of the key developers of VTK. He enjoys writing beautiful code,\nespecially when it involves computational geometry or graphics.\nMargo Seltzer (Berkeley DB): Margo is the Herchel Smith Professor of Computer Science at\nHarvard's School of Engineering and Applied Sciences and an Architect at Oracle Corporation. She\nwas one of the principal designers of Berkeley DB and a co-founder of Sleepycat Software. Her\nresearch interests are in filesystems, database systems, transactional systems, and medical data\nmining. Her professional life is online at http://www.eecs.harvard.edu/~margo, and she blogs at\nhttp://mis-misinformation.blogspot.com/.\nJustin Sheehy (Riak): Justin is the CTO of Basho Technologies, the company behind the creation of\nWebmachine and Riak. Most recently before Basho, he was a principal scientist at the MITRE\nCorporation and a senior architect for systems infrastructure at Akamai. At both of those\ncompanies he focused on multiple aspects of robust distributed systems, including scheduling\nalgorithms, language-based formal models, and resilience.\nRichard Shimooka (Battle for Wesnoth): Richard is a Research Associate at Queen's University's\nDefence Management Studies Program in Kingston, Ontario. He is also a Deputy Administrator and\n", "page": 6, "type": "text", "section": "Page 6"}
{"text": "Secretary for the Battle For Wesnoth. Richard has written several works examining the\norganizational cultures of social groups, ranging from governments to open source projects.\nKonstantin V. Shvachko (HDFS), a veteran HDFS developer, is a principal Hadoop architect at\neBay. Konstantin specializes in efficient data structures and algorithms for large-scale distributed\nstorage systems. He discovered a new type of balanced trees, S-trees, for optimal indexing of\nunstructured data, and was a primary developer of an S-tree-based Linux filesystem, treeFS, a\nprototype of reiserFS. Konstantin holds a Ph.D. in computer science from Moscow State\nUniversity, Russia. He is also a member of the Project Management Committee for Apache\nHadoop.\nClaudio Silva (VisTrails): Claudio is a full professor of computer science at the University of Utah.\nHis research interests are in visualization, geometric computing, computer graphics, and scientific\ndata management. He received his Ph.D. in computer science from the State University of New\nYork at Stony Brook in 1996. Later in 2011, he will be joining the Polytechnic Institute of New York\nUniversity as a full professor of computer science and engineering.\nSuresh Srinivas (HDFS): Suresh works on HDFS as software architect at Yahoo!. He is a Hadoop\ncommitter and PMC member at Apache Software Foundation. Prior to Yahoo!, he worked at\nSylantro Systems, developing scalable infrastructure for hosted communication services. Suresh\nhas a bachelor's degree in Electronics and Communication from National Institute of Technology\nKarnataka, India.\nSimon Stewart (Selenium): Simon lives in London and works as a Software Engineer in Test at\nGoogle. He is a core contributor to the Selenium project, was the creator of WebDriver and is\nenthusiastic about Open Source. Simon enjoys beer and writing better software, sometimes at the\nsame time. His personal home page is http://www.pubbitch.org/.\nAudrey Tang (SocialCalc): Audrey is a self-educated programmer and translator based in Taiwan.\nShe curently works at Socialtext, where her job title is \"Untitled Page\", as well as at Apple as\ncontractor for localization and release engineering. She previously designed and led the Pugs\nproject, the first working Perl 6 implementation; she has also served in language design\ncommittees for Haskell, Perl 5, and Perl 6, and has made numerous contributions to CPAN and\nHackage. She blogs at http://pugs.blogs.com/audreyt/.\nHuy T. Vo (VisTrails): Huy is receiving his Ph.D. from the University of Utah in May, 2011. His\nresearch interests include visualization, dataflow architecture and scientific data management. He\nis a senior developer at VisTrails, Inc. He also holds a Research Assistant Professor appointment\nwith the Polytechnic Institute of New York University.\nDavid White (Battle for Wesnoth): David is the founder and lead developer of Battle for Wesnoth.\nDavid has been involved with several Open Source video game projects, including Frogatto which\nhe also co-founded. David is a performance engineer at Sabre Holdings, a leader in travel\ntechnology.\nGreg Wilson (editorial): Greg has worked over the past 25 years in high-performance scientific\ncomputing, data visualization, and computer security, and is the author or editor of several\ncomputing books (including the 2008 Jolt Award winner Beautiful Code) and two books for\nchildren. Greg received a Ph.D. in Computer Science from the University of Edinburgh in 1993. He\nblogs at http://third-bit.com and http://software-carpentry.org.\nTarek Ziad\u00e9 (Python Packaging): Tarek lives in Burgundy, France. He's a Senior Software Engineer\nat Mozilla, building servers in Python. In his spare time, he leads the packaging effort in Python.\nAcknowledgments\nWe would like to thank our reviewers:\nEric Aderhold\nMuhammad Ali\nLillian Angel\nRobert Beghian\nTaavi Burns\nLuis Pedro Coelho\n", "page": 7, "type": "text", "section": "Page 7"}
{"text": "David Cooper\nMauricio de Simone\nJonathan Deber\nPatrick Dubroy\nIgor Foox\nAlecia Fowler\nMarcus Hanwell\nJohan Harjono\nVivek Lakshmanan\nGreg Lapouchnian\nLaurie MacDougall Sookraj\nJosh McCarthy\nJason Montojo\nColin Morris\nChristian Muise\nVictor Ng\nNikita Pchelin\nAndrew Petersen\nAndrey Petrov\nTom Plaskon\nPascal Rapicault\nTodd Ritchie\nSamar Sabie\nMisa Sakamoto\nDavid Scannell\nClara Severino\nTim Smith\nKyle Spaans\nSana Tapal\nTony Targonski\nMiles Thibault\nDavid Wright\nTina Yee\nWe would also like to thank Jackie Carter, who helped with the early stages of editing.\nThe cover image is a photograph by Peter Dutton of the 48 Free Street Mural by Chris Denison in\nPortland, Maine. The photograph is licensed under the Creative Commons Attribution-\nNonCommercial-ShareAlike 2.0 Generic license.\nLicense, Credits, and Disclaimers\nThis work is licensed under the Creative Commons Attribution 3.0 Unported license (CC BY 3.0).\nYou are free:\nto Share\u2014to copy, distribute and transmit the work\nto Remix\u2014to adapt the work\nunder the following conditions:\nAttribution\u2014you must attribute the work in the manner specified by the author or licensor\n(but not in any way that suggests that they endorse you or your use of the work).\nwith the understanding that:\nWaiver\u2014Any of the above conditions can be waived if you get permission from the copyright\nholder.\nPublic Domain\u2014Where the work or any of its elements is in the public domain under\napplicable law, that status is in no way affected by the license.\nOther Rights\u2014In no way are any of the following rights affected by the license:\nYour fair dealing or fair use rights, or other applicable copyright exceptions and\nlimitations;\nThe author's moral rights;\nRights other persons may have either in the work itself or in how the work is used, such\nas publicity or privacy rights.\nNotice\u2014For any reuse or distribution, you must make clear to others the license terms of\nthis work. The best way to do this is with a link to\nhttp://creativecommons.org/licenses/by/3.0/.\nTo view a copy of this license, visit http://creativecommons.org/licenses/by/3.0/ or send a letter to\nCreative Commons, 444 Castro Street, Suite 900, Mountain View, California, 94041, USA.\nProduct and company names mentioned herein may be the trademarks of their respective\nowners.\nWhile every precaution has been taken in the preparation of this book, the editors and authors\nassume no responsibility for errors or omissions, or for damages resulting from the use of the\ninformation contained herein.\n", "page": 8, "type": "text", "section": "Page 8"}
{"text": "Dedication\nDedicated to Brian Kernighan,\nwho has taught us all so much;\nand to prisoners of conscience everywhere.\n", "page": 9, "type": "text", "section": "Page 9"}
{"text": "The Architecture of\nOpen Source Applications\nAmy Brown and Greg Wilson (eds.)\nISBN 978-1-257-63801-7 \nLicense / Buy / Contribute\nChapter 1. Asterisk\nRussell Bryant\nAsterisk1 is an open source telephony applications platform distributed under the GPLv2. In short,\nit is a server application for making, receiving, and performing custom processing of phone calls.\nThe project was started by Mark Spencer in 1999. Mark had a company called Linux Support\nServices and he needed a phone system to help operate his business. He did not have a lot of\nmoney to spend on buying one, so he just made his own. As the popularity of Asterisk grew,\nLinux Support Services shifted focus to Asterisk and changed its name to Digium, Inc.\nThe name Asterisk comes from the Unix wildcard character, *. The goal for the Asterisk project is\nto do everything telephony. Through pursuing this goal, Asterisk now supports a long list of\ntechnologies for making and receiving phone calls. This includes many VoIP (Voice over IP)\nprotocols, as well as both analog and digital connectivity to the traditional telephone network, or\nthe PSTN (Public Switched Telephone Network). This ability to get many different types of phone\ncalls into and out of the system is one of Asterisk's main strengths.\nOnce phone calls are made to and from an Asterisk system, there are many additional features\nthat can be used to customize the processing of the phone call. Some features are larger pre-built\ncommon applications, such as voicemail. There are other smaller features that can be combined\ntogether to create custom voice applications, such as playing back a sound file, reading digits, or\nspeech recognition.\n1.1. Critical Architectural Concepts\nThis section discusses some architectural concepts that are critical to all parts of Asterisk. These\nideas are at the foundation of the Asterisk architecture.\n1.1.1. Channels\nA channel in Asterisk represents a connection between the Asterisk system and some telephony\nendpoint (Figure 1.1). The most common example is when a phone makes a call into an Asterisk\nsystem. This connection is represented by a single channel. In the Asterisk code, a channel exists\nas an instance of the ast_channel data structure. This call scenario could be a caller interacting\nwith voicemail, for example.\nFigure 1.1: A Single Call Leg, Represented by a Single Channel\n1.1.2. Channel Bridging\n", "page": 10, "type": "text", "section": "Page 10"}
{"text": "Perhaps a more familiar call scenario would be a connection between two phones, where a person\nusing phone A has called a person on phone B. In this call scenario, there are two telephony\nendpoints connected to the Asterisk system, so two channels exist for this call (Figure 1.2).\nFigure 1.2: Two Call Legs Represented by Two Channels\nWhen Asterisk channels are connected like this, it is referred to as a channel bridge. Channel\nbridging is the act of connecting channels together for the purpose of passing media between\nthem. The media stream is most commonly an audio stream. However, there may also be a video\nor a text stream in the call. Even in the case where there is more than one media stream (such as\nboth audio and video), it is still handled by a single channel for each end of the call in Asterisk. In\nFigure 1.2, where there are two channels for phones A and B, the bridge is responsible for\npassing the media coming from phone A to phone B, and similarly, for passing the media coming\nfrom phone B to phone A. All media streams are negotiated through Asterisk. Anything that\nAsterisk does not understand and have full control over is not allowed. This means that Asterisk\ncan do recording, audio manipulation, and translation between different technologies.\nWhen two channels are bridged together, there are two methods that may be used to accomplish\nthis: generic bridging and native bridging. A generic bridge is one that works regardless of what\nchannel technologies are in use. It passes all audio and signalling through the Asterisk abstract\nchannel interfaces. While this is the most flexible bridging method, it is also the least efficient due\nto the levels of abstraction necessary to accomplish the task. Figure 1.2 illustrates a generic\nbridge.\nA native bridge is a technology specific method of connecting channels together. If two channels\nare connected to Asterisk using the same media transport technology, there may be a way to\nconnect them that is more efficient than going through the abstraction layers in Asterisk that exist\nfor connecting different technologies together. For example, if specialized hardware is being used\nfor connecting to the telephone network, it may be possible to bridge the channels on the\nhardware so that the media does not have to flow up through the application at all. In the case of\nsome VoIP protocols, it is possible to have endpoints send their media streams to each other\ndirectly, such that only the call signalling information continues to flow through the server.\nThe decision between generic bridging and native bridging is done by comparing the two channels\nwhen it is time to bridge them. If both channels indicate that they support the same native\nbridging method, then that will be used. Otherwise, the generic bridging method will be used. To\ndetermine whether or not two channels support the same native bridging method, a simple C\nfunction pointer comparison is used. It's certainly not the most elegant method, but we have not\nyet hit any cases where this was not sufficient for our needs. Providing a native bridge function\nfor a channel is discussed in more detail in Section 1.2. Figure 1.3 illustrates an example of a native\nbridge.\n", "page": 11, "type": "text", "section": "Page 11"}
{"text": "Figure 1.3: Example of a Native Bridge\n1.1.3. Frames\nCommunication within the Asterisk code during a call is done by using frames, which are instances\nof the ast_frame data structure. Frames can either be media frames or signalling frames. During\na basic phone call, a stream of media frames containing audio would be passing through the\nsystem. Signalling frames are used to send messages about call signalling events, such as a digit\nbeing pressed, a call being put on hold, or a call being hung up.\nThe list of available frame types is statically defined. Frames are marked with a numerically\nencoded type and subtype. A full list can be found in the source code in\ninclude/asterisk/frame.h; some examples are:\nVOICE: These frames carry a portion of an audio stream.\nVIDEO: These frames carry a portion of a video stream.\nMODEM: The encoding used for the data in this frame, such as T.38 for sending a FAX over IP.\nThe primary usage of this frame type is for handling a FAX. It is important that frames of\ndata be left completely undisturbed so that the signal can be successfully decoded at the\nother end. This is different than AUDIO frames, because in that case, it is acceptable to\ntranscode into other audio codecs to save bandwidth at the cost of audio quality.\nCONTROL: The call signalling message that this frame indicates. These frames are used to\nindicate call signalling events. These events include a phone being answered, hung up, put on\nhold, etc.\nDTMF_BEGIN: Which digit just started. This frame is sent when a caller presses a DTMF key2\non their phone.\nDTMF_END: Which digit just ended. This frame is sent when a caller stops pressing a DTMF\nkey on their phone.\n1.2. Asterisk Component Abstractions\nAsterisk is a highly modularized application. There is a core application that is built from the source\nin the main/ directory of the source tree. However, it is not very useful by itself. The core\napplication acts primarily as a module registry. It also has code that knows how to connect all of\nthe abstract interfaces together to make phone calls work. The concrete implementations of these\ninterfaces are registered by loadable modules at runtime.\nBy default, all modules found in a predefined Asterisk modules directory on the filesystem will be\nloaded when the main application is started. This approach was chosen for its simplicity. However,\nthere is a configuration file that can be updated to specify exactly which modules to load and in\nwhat order to load them. This makes the configuration a bit more complex, but provides the ability\nto specify that modules that are not needed should not be loaded. The primary benefit is reducing\nthe memory footprint of the application. However, there are some security benefits, as well. It is\nbest not to load a module that accepts connections over a network if it is not actually needed.\nWhen the module loads, it registers all of its implementations of component abstractions with the\n", "page": 12, "type": "text", "section": "Page 12"}
{"text": "Asterisk core application. There are many types of interfaces that modules can implement and\nregister with the Asterisk core. A module is allowed to register as many of these different\ninterfaces as it would like. Generally, related functionality is grouped into a single module.\n1.2.1. Channel Drivers\nThe Asterisk channel driver interface is the most complex and most important interface available.\nThe Asterisk channel API provides the telephony protocol abstraction which allows all other\nAsterisk features to work independently of the telephony protocol in use. This component is\nresponsible for translating between the Asterisk channel abstraction and the details of the\ntelephony technology that it implements.\nThe definition of the Asterisk channel driver interface is called the ast_channel_tech interface. It\ndefines a set of methods that must be implemented by a channel driver. The first method that a\nchannel driver must implement is an ast_channel factory method, which is the requester\nmethod in ast_channel_tech. When an Asterisk channel is created, either for an incoming or\noutgoing phone call, the implementation of ast_channel_tech associated with the type of\nchannel needed is responsible for instantiation and initialization of the ast_channel for that call.\nOnce an ast_channel has been created, it has a reference to the ast_channel_tech that\ncreated it. There are many other operations that must be handled in a technology-specific way.\nWhen those operations must be performed on an ast_channel, the handling of the operation is\ndeferred to the appropriate method from ast_channel_tech. Figure 1.2 shows two channels in\nAsterisk. Figure 1.4 expands on this to show two bridged channels and how the channel\ntechnology implementations fit into the picture.\nFigure 1.4: Channel Technology and Abstract Channel Layers\nThe most important methods in ast_channel_tech are:\nrequester: This callback is used to request a channel driver to instantiate an ast_channel\nobject and initialize it as appropriate for this channel type.\ncall: This callback is used to initiate an outbound call to the endpoint represented by an\nast_channel.\nanswer: This is called when Asterisk decides that it should answer the inbound call\nassociated with this ast_channel.\nhangup: This is called when the system has determined that the call should be hung up. The\nchannel driver will then communicate to the endpoint that the call is over in a protocol specific\nmanner.\nindicate: Once a call is up, there are a number of other events that may occur that need to\nbe signalled to an endpoint. For example, if the device is put on hold, this callback is called to\nindicate that condition. There may be a protocol specific method of indicating that the call has\nbeen on hold, or the channel driver may simply initiate the playback of music on hold to the\ndevice.\nsend_digit_begin: This function is called to indicate the beginning of a digit (DTMF) being\nsent to this device.\nsend_digit_end: This function is called to indicate the end of a digit (DTMF) being sent to\nthis device.\n", "page": 13, "type": "text", "section": "Page 13"}
{"text": "read: This function is called by the Asterisk core to read back an ast_frame from this\nendpoint. An ast_frame is an abstraction in Asterisk that is used to encapsulate media\n(such as audio or video), as well as to signal events.\nwrite: This function is used to send an ast_frame to this device. The channel driver will\ntake the data and packetize it as appropriate for the telephony protocol that it implements\nand pass it along to the endpoint.\nbridge: This is the native bridge callback for this channel type. As discussed before, native\nbridging is when a channel driver is able to implement a more efficient bridging method for\ntwo channels of the same type instead of having all signalling and media flow through\nadditional unnecessary abstraction layers. This is incredibly important for performance\nreasons.\nOnce a call is over, the abstract channel handling code that lives in the Asterisk core will invoke the\nast_channel_tech hangup callback and then destroy the ast_channel object.\n1.2.2. Dialplan Applications\nAsterisk administrators set up call routing using the Asterisk dialplan, which resides in the\n/etc/asterisk/extensions.conf file. The dialplan is made up of a series of call rules called\nextensions. When a phone call comes in to the system, the dialed number is used to find the\nextension in the dialplan that should be used for processing the call. The extension includes a list\nof dialplan applications which will be executed on the channel. The applications available for\nexecution in the dialplan are maintained in an application registry. This registry is populated at\nruntime as modules are loaded.\nAsterisk has nearly two hundred included applications. The definition of an application is very\nloose. Applications can use any of the Asterisk internal APIs to interact with the channel. Some\napplications do a single task, such as Playback, which plays back a sound file to the caller. Other\napplications are much more involved and perform a large number of operations, such as the\nVoicemail application.\nUsing the Asterisk dialplan, multiple applications can be used together to customize call handling. If\nmore extensive customization is needed beyond what is possible in the provided dialplan language,\nthere are scripting interfaces available that allow call handling to be customized using any\nprogramming language. Even when using these scripting interfaces with another programming\nlanguage, dialplan applications are still invoked to interact with the channel.\nBefore we get into an example, let's have a look at the syntax of an Asterisk dialplan that handles\ncalls to the number 1234. Note that the choice of 1234 here is arbitrary. It invokes three dialplan\napplications. First, it answers the call. Next, it plays back a sound file. Finally, it hangs up the call.\n; Define the rules for what happens when someone dials 1234.\n;\nexten => 1234,1,Answer()\n    same => n,Playback(demo-congrats)\n    same => n,Hangup()\nThe exten keyword is used to define the extension. On the right side of the exten line, the 1234\nmeans that we are defining the rules for when someone calls 1234. The next 1 means this is the\nfirst step that is taken when that number is dialed. Finally, Answer instructs the system to answer\nthe call. The next two lines that begin with the same keyword are rules for the last extension that\nwas specified, which in this case is 1234. The n is short for saying that this is the next step to\ntake. The last item on those lines specifies what action to take.\nHere is another example of using the Asterisk dialplan. In this case, an incoming call is answered.\nThe caller is played a beep, and then up to 4 digits are read from the caller and stored into the\nDIGITS variable. Then, the digits are read back to the caller. Finally, the call is ended.\nexten => 5678,1,Answer()\n    same => n,Read(DIGITS,beep,4)\n", "page": 14, "type": "text", "section": "Page 14"}
{"text": "    same => n,SayDigits(${DIGITS})\n    same => n,Hangup()\nAs previously mentioned, the definition of an application is very loose\u2014the function prototype\nregistered is very simple:\nint (*execute)(struct ast_channel *chan, const char *args);\nHowever, the application implementations use virtually all of the APIs found in\ninclude/asterisk/.\n1.2.3. Dialplan Functions\nMost dialplan applications take a string of arguments. While some values may be hard coded,\nvariables are used in places where behavior needs to be more dynamic. The following example\nshows a dialplan snippet that sets a variable and then prints out its value to the Asterisk command\nline interface using the Verbose application.\nexten => 1234,1,Set(MY_VARIABLE=foo)\n    same => n,Verbose(MY_VARIABLE is ${MY_VARIABLE})\nDialplan functions are invoked by using the same syntax as the previous example. Asterisk\nmodules are able to register dialplan functions that can retrieve some information and return it to\nthe dialplan. Alternatively, these dialplan functions can receive data from the dialplan and act on it.\nAs a general rule, while dialplan functions may set or retrieve channel meta data, they do not do\nany signalling or media processing. That is left as the job of dialplan applications.\nThe following example demonstrates usage of a dialplan function. First, it prints out the CallerID of\nthe current channel to the Asterisk command line interface. Then, it changes the CallerID by using\nthe Set application. In this example, Verbose and Set are applications, and CALLERID is a\nfunction.\nexten => 1234,1,Verbose(The current CallerID is ${CALLERID(num)})\n    same => n,Set(CALLERID(num)=<256>555-1212)\nA dialplan function is needed here instead of just a simple variable since the CallerID information is\nstored in data structures on the instance of ast_channel. The dialplan function code knows how\nto set and retrieve the values from these data structures.\nAnother example of using a dialplan function is for adding custom information into the call logs,\nwhich are referred to as CDRs (Call Detail Records). The CDR function allows the retrieval of call\ndetail record information, as well as adding custom information.\nexten => 555,1,Verbose(Time this call started: ${CDR(start)})\n    same => n,Set(CDR(mycustomfield)=snickerdoodle)\n1.2.4. Codec Translators\nIn the world of VOIP, many different codecs are used for encoding media to be sent across\nnetworks. The variety of choices offers tradeoffs in media quality, CPU consumption, and\nbandwidth requirements. Asterisk supports many different codecs and knows how to translate\nbetween them when necessary.\nWhen a call is set up, Asterisk will attempt to get two endpoints to use a common media codec so\nthat transcoding is not required. However, that is not always possible. Even if a common codec is\nbeing used, transcoding may still be required. For example, if Asterisk is configured to do some\nsignal processing on the audio as it passes through the system (such as to increase or decrease\nthe volume level), Asterisk will need to transcode the audio back to an uncompressed form before\nit can perform the signal processing. Asterisk can also be configured to do call recording. If the\nconfigured format for the recording is different than that of the call, transcoding will be required.\n", "page": 15, "type": "text", "section": "Page 15"}
{"text": "Codec Negotiation\nThe method used to negotiate which codec will be used for a media stream is specific to\nthe technology used to connect the call to Asterisk. In some cases, such as a call on the\ntraditional telephone network (the PSTN), there may not be any negotiation to do.\nHowever, in other cases, especially using IP protocols, there is a negotiation mechanism\nused where capabilities and preferences are expressed and a common codec is agreed\nupon.\nFor example, in the case of SIP (the most commonly used VOIP protocol) this is a high\nlevel view of how codec negotiation is performed when a call is sent to Asterisk.\n1. An endpoint sends a new call request to Asterisk which includes the list of codecs it\nis willing to use.\n2. Asterisk consults its configuration provided by the administrator which includes a list\nof allowed codecs in preferred order. Asterisk will respond by choosing the most\npreferred codec (based on its own configured preferences) that is listed as allowed\nin the Asterisk configuration and was also listed as supported in the incoming\nrequest.\nOne area that Asterisk does not handle very well is that of more complex codecs,\nespecially video. Codec negotiation demands have gotten more complicated over the last\nten years. We have more work to do to be able to better deal with the newest audio\ncodecs and to be able to support video much better than we do today. This is one of the\ntop priorities for new development for the next major release of Asterisk.\nCodec translator modules provide one or more implementations of the ast_translator\ninterface. A translator has source and destination format attributes. It also provides a callback\nthat will be used to convert a chunk of media from the source to the destination format. It knows\nnothing about the concept of a phone call. It only knows how to convert media from one format\nto another.\nFor more detailed information about the translator API, see include/asterisk/translate.h\nand main/translate.c. Implementations of the translator abstraction can be found in the\ncodecs directory.\n1.3. Threads\nAsterisk is a very heavily multithreaded application. It uses the POSIX threads API to manage\nthreads and related services such as locking. All of the Asterisk code that interacts with threads\ndoes so by going through a set of wrappers used for debugging purposes. Most threads in\nAsterisk can be classified as either a Network Monitor Thread, or a Channel Thread (sometimes\nalso referred to as a PBX thread, because its primary purpose is to run the PBX for a channel).\n1.3.1. Network Monitor Threads\nNetwork monitor threads exist in every major channel driver in Asterisk. They are responsible for\nmonitoring whatever network they are connected to (whether that is an IP network, the PSTN,\netc.) and monitor for incoming calls or other types of incoming requests. They handle the initial\nconnection setup steps such as authentication and dialed number validation. Once the call setup\nhas been completed, the monitor threads will create an instance of an Asterisk channel\n(ast_channel), and start a channel thread to handle the call for the rest of its lifetime.\n1.3.2. Channel Threads\nAs discussed earlier, a channel is a fundamental concept in Asterisk. Channels are either inbound\nor outbound. An inbound channel is created when a call comes in to the Asterisk system. These\nchannels are the ones that execute the Asterisk dialplan. A thread is created for every inbound\n", "page": 16, "type": "text", "section": "Page 16"}
{"text": "channel that executes the dialplan. These threads are referred to as channel threads.\nDialplan applications always execute in the context of a channel thread. Dialplan functions almost\nalways do, as well. It is possible to read and write dialplan functions from an asynchronous\ninterface such as the Asterisk CLI. However, it is still always the channel thread that is the owner\nof the ast_channel data structure and controls the object lifetime.\n1.4. Call Scenarios\nThe previous two sections introduced important interfaces for Asterisk components, as well as\nthe thread execution model. In this section, some common call scenarios are broken down to\ndemonstrate how Asterisk components operate together to process phone calls.\n1.4.1. Checking Voicemail\nOne example call scenario is when someone calls into the phone system to check their Voicemail.\nThe first major component involved in this scenario is the channel driver. The channel driver will be\nresponsible for handling the incoming call request from the phone, which will occur in the channel\ndriver's monitor thread. Depending on the telephony technology being used to deliver the call to\nthe system, there may be some sort of negotiation required to set up the call. Another step of\nsetting up the call is determining the intended destination for the call. This is usually specified by\nthe number that was dialed by the caller. However, in some cases there is no specific number\navailable since the technology used to deliver the call does not support specifying the dialed\nnumber. An example of this would be an incoming call on an analog phone line.\nIf the channel driver verifies that the Asterisk configuration has extensions defined in the dialplan\n(the call routing configuration) for the dialed number, it will then allocate an Asterisk channel object\n(ast_channel) and create a channel thread. The channel thread has the primary responsibility for\nhandling the rest of the call (Figure 1.5).\nFigure 1.5: Call Setup Sequence Diagram\n", "page": 17, "type": "text", "section": "Page 17"}
{"text": "The main loop of the channel thread handles dialplan execution. It goes to the rules defined for the\ndialed extension and executes the steps that have been defined. The following is an example\nextension expressed in the extensions.conf dialplan syntax. This extension answers the call\nand executes the VoicemailMain application when someone dials *123. This application is what a\nuser would call to be able to check messages left in their mailbox.\nexten => *123,1,Answer()\n    same => n,VoicemailMain()\nWhen the channel thread executes the Answer application, Asterisk will answer the incoming call.\nAnswering a call requires technology specific processing, so in addition to some generic answer\nhandling, the answer callback in the associated ast_channel_tech structure is called to handle\nanswering the call. This may involve sending a special packet over an IP network, taking an analog\nline off hook, etc.\nThe next step is for the channel thread to execute VoicemailMain (Figure 1.6). This application is\nprovided by the app_voicemail module. One important thing to note is that while the Voicemail\ncode handles a lot of call interaction, it knows nothing about the technology that is being used to\ndeliver the call into the Asterisk system. The Asterisk channel abstraction hides these details from\nthe Voicemail implementation.\nThere are many features involved in providing a caller access to their Voicemail. However, all of\nthem are primarily implemented as reading and writing sound files in response to input from the\ncaller, primarily in the form of digit presses. DTMF digits can be delivered to Asterisk in many\ndifferent ways. Again, these details are handled by the channel drivers. Once a key press has\narrived in Asterisk, it is converted into a generic key press event and passed along to the\nVoicemail code.\nOne of the important interfaces in Asterisk that has been discussed is that of a codec translator.\nThese codec implementations are very important to this call scenario. When the Voicemail code\nwould like to play back a sound file to the caller, the format of the audio in the sound file may not\nbe the same format as the audio being used in the communication between the Asterisk system\nand the caller. If it must transcode the audio, it will build a translation path of one or more codec\ntranslators to get from the source to the destination format.\nFigure 1.6: A Call to VoicemailMain\nAt some point, the caller will be done interacting with the Voicemail system and hang up. The\nchannel driver will detect that this has occurred and convert this into a generic Asterisk channel\nsignalling event. The Voicemail code will receive this signalling event and will exit, since there is\nnothing left to do once the caller hangs up. Control will return back to the main loop in the channel\nthread to continue dialplan execution. Since in this example there is no further dialplan processing\nto be done, the channel driver will be given an opportunity to handle technology specific hangup\nprocessing and then the ast_channel object will be destroyed.\n1.4.2. Bridged Call\n", "page": 18, "type": "text", "section": "Page 18"}
{"text": "Another very common call scenario in Asterisk is a bridged call between two channels. This is the\nscenario when one phone calls another through the system. The initial call setup process is\nidentical to the previous example. The difference in handling begins when the call has been set up\nand the channel thread begins executing the dialplan.\nThe following dialplan is a simple example that results in a bridged call. Using this extension, when\na phone dials 1234, the dialplan will execute the Dial application, which is the main application\nused to initiate an outbound call.\nexten => 1234,1,Dial(SIP/bob)\nThe argument specified to the Dial application says that the system should make an outbound\ncall to the device referred to as SIP/bob. The SIP portion of this argument specifies that the SIP\nprotocol should be used to deliver the call. bob will be interpreted by the channel driver that\nimplements the SIP protocol, chan_sip. Assuming the channel driver has been properly\nconfigured with an account called bob, it will know how to reach Bob's phone.\nThe Dial application will ask the Asterisk core to allocate a new Asterisk channel using the\nSIP/bob identifier. The core will request that the SIP channel driver perform technology specific\ninitialization. The channel driver will also initiate the process of making a call out to the phone. As\nthe request proceeds, it will pass events back into the Asterisk core, which will be received by the\nDial application. These events may include a response that the call has been answered, the\ndestination is busy, the network is congested, the call was rejected for some reason, or a number\nof other possible responses. In the ideal case, the call will be answered. The fact that the call has\nbeen answered is propagated back to the inbound channel. Asterisk will not answer the part of the\ncall that came into the system until the outbound call was answered. Once both channels are\nanswered, the bridging of the channels begins (Figure 1.7).\nFigure 1.7: Block Diagram of a Bridged Call in a Generic Bridge\nDuring a channel bridge, audio and signalling events from one channel are passed to the other\nuntil some event occurs that causes the bridge to end, such as one side of the call hanging up.\nThe sequence diagram in Figure 1.8 demonstrates the key operations that are performed for an\naudio frame during a bridged call.\n", "page": 19, "type": "text", "section": "Page 19"}
{"text": "Figure 1.8: Sequence Diagram for Audio Frame Processing During a Bridge\nOnce the call is done, the hangup process is very similar to the previous example. The major\ndifference here is that there are two channels involved. The channel technology specific hangup\nprocessing will be executed for both channels before the channel thread stops running.\n1.5. Final Comments\nThe architecture of Asterisk is now more than ten years old. However, the fundamental concepts\nof channels and flexible call handling using the Asterisk dialplan still support the development of\ncomplex telephony systems in an industry that is continuously evolving. One area that the\narchitecture of Asterisk does not address very well is scaling a system across multiple servers.\nThe Asterisk development community is currently developing a companion project called Asterisk\nSCF (Scalable Communications Framework) which is intended to address these scalability\nconcerns. In the next few years, we expect to see Asterisk, along with Asterisk SCF, continue to\ntake over significant portions of the telephony market, including much larger installations.\nFootnotes\n1. http://www.asterisk.org/\n2. DTMF stands for Dual-Tone Multi-Frequency. This is the tone that is sent in the audio of a\nphone call when someone presses a key on their telephone.\n", "page": 20, "type": "text", "section": "Page 20"}
{"text": "The Architecture of\nOpen Source Applications\nAmy Brown and Greg Wilson (eds.)\nISBN 978-1-257-63801-7 \nLicense / Buy / Contribute\nChapter 2. Audacity\nJames Crook\nAudacity is a popular sound recorder and audio editor. It is a capable program while still being\neasy to use. The majority of users are on Windows but the same Audacity source code compiles\nto run on Linux and Mac too.\nDominic Mazzoni wrote the original version of Audacity in 1999 while he was a research student at\nCarnegie Mellon University. Dominic wanted to create a platform on which to develop and debug\naudio processing algorithms. The software grew to become useful in its own right in many other\nways. Once Audacity was released as open source software, it attracted other developers. A\nsmall, gradually-changing team of enthusiasts have modified, maintained, tested, updated, written\ndocumentation for, helped users with, and translated Audacity's interface into other languages\nover the years.\nOne goal is that its user interface should be discoverable: people should be able to sit down\nwithout a manual and start using it right away, gradually discovering its features. This principle has\nbeen crucial in giving Audacity greater consistency to the user interface than there otherwise\nwould be. For a project in which many people have a hand this kind of unifying principle is more\nimportant than it might seem at first.\nIt would be good if the architecture of Audacity had a similar guiding principle, a similar kind of\ndiscoverability. The closest we have to that is \"try and be consistent\". When adding new code,\ndevelopers try to follow the style and conventions of code nearby. In practice, though, the\nAudacity code base is a mix of well-structured and less well-structured code. Rather than an\noverall architecture the analogy of a small city is better: there are some impressive buildings but\nyou will also find run-down neighborhoods that are more like a shanty town.\n2.1. Structure in Audacity\nAudacity is layered upon several libraries. While most new programming in Audacity code doesn't\nrequire a detailed knowledge of exactly what is going on in these libraries, familiarity with their APIs\nand what they do is important. The two most important libraries are PortAudio which provides a\nlow-level audio interface in a cross-platform way, and wxWidgets which provides GUI components\nin a cross-platform way.\nWhen reading Audacity's code, it helps to realize that only a fraction of the code is essential.\nLibraries contribute a lot of optional features\u2014though people who use those features might not\nconsider them optional. For example, as well as having its own built-in audio effects, Audacity\nsupports LADSPA (Linux Audio Developer's Simple Plugin API) for dynamically loadable plugin audio\neffects. The VAMP API in Audacity does the same thing for plugins that analyze audio. Without\nthese APIs, Audacity would be less feature-rich, but it does not absolutely depend on these\nfeatures.\nOther optional libraries used by Audacity are libFLAC, libogg, and libvorbis. These provide various\naudio compression formats. MP3 format is catered for by dynamically loading the LAME or\nFFmpeg library. Licensing restrictions prevent these very popular compression libraries from being\nbuilt-in.\nLicensing is behind some other decisions about Audacity libraries and structure. For example,\nsupport for VST plugins is not built in because of licensing restrictions. We would also like to use\nthe very efficient FFTW fast Fourier transform code in some of our code. However, we only\nprovide that as an option for people who compile Audacity themselves, and instead fall back to a\nslightly slower version in our normal builds. As long as Audacity accepts plugins, it can be and has\n", "page": 21, "type": "text", "section": "Page 21"}
{"text": "been argued that Audacity cannot use FFTW. FFTW's authors do not want their code to be\navailable as a general service to arbitrary other code. So, the architectural decision to support\nplugins leads to a trade-off in what we can offer. It makes LADSPA plugins possible but bars us\nfrom using FFTW in our pre-built executables.\nArchitecture is also shaped by considerations of how best to use our scarce developer time. With\na small team of developers, we do not have the resources to do, for example, the in-depth\nanalysis of security loopholes that teams working on Firefox and Thunderbird do. However, we do\nnot want Audacity to provide a route to bypass a firewall, so we have a rule not to have TCP/IP\nconnections to or from Audacity at all. Avoiding TCP/IP cuts out many security concerns. The\nawareness of our limited resources leads us to better design. It helps us cut features that would\ncost us too much in developer time and focus on what is essential.\nA similar concern for developers' time applies to scripting languages. We want scripting, but the\ncode implementing the languages does not need to be in Audacity. It does not make sense to\ncompile copies of each scripting language into Audacity to give users all the choices they could\nwant.1 We have instead implemented scripting with a single plugin module and a pipe, which we will\ncover later.\nFigure 2.1: Layers in Audacity\nFigure 2.1 shows some layers and modules in Audacity. The diagram highlights three important\nclasses within wxWidgets, each of which has a reflection in Audacity. We're building higher-level\nabstractions from related lower-level ones. For example, the BlockFile system is a reflection of and\nis built on wxWidgets' wxFiles. It might, at some stage, make sense to split out BlockFiles,\nShuttleGUI, and command handling into an intermediate library in their own right. This would\nencourage us to make them more general.\nLower down in the diagram is a narrow strip for \"Platform Specific Implementation Layers.\" Both\nwxWidgets and PortAudio are OS abstraction layers. Both contain conditional code that chooses\nbetween different implementations depending on the target platform.\nThe \"Other Supporting Libraries\" category includes a wide collection of libraries. Interestingly quite\na few of these rely on dynamically loaded modules. Those dynamic modules know nothing of\nwxWidgets.\nOn the Windows platform we used to compile Audacity as a single monolithic executable with\nwxWidgets and Audacity application code in the same executable. In 2008 we changed over to\nusing a modular structure with wxWidgets as a separate DLL. This is to allow additional optional\nDLLs to be loaded at run time where those DLLs directly use features of wxWidgets. Plugins that\nplug in above the dotted line in the diagram can use wxWidgets.\nThe decision to use DLLs for wxWidgets has its downsides. The distribution is now larger, partly\nbecause many unused functions are provided in the DLLs that would previously have been\noptimized away. Audacity also takes longer to start up because each DLL is loaded separately. The\nadvantages are considerable. We expect modules to have similar advantages for us as they do for\nApache. As we see it, modules allow the core of Apache to be very stable while facilitating\n", "page": 22, "type": "text", "section": "Page 22"}
{"text": "experimentation, special features and new ideas in the modules. Modules go a very long way to\ncounteracting the temptation to fork a project to take it in a new direction. We think it's been a\nvery important architectural change for us. We're expecting these advantages but have not seen\nthem yet. Exposing the wxWidgets functions is only a first step and we have more to do to have a\nflexible modular system.\nThe structure of a program like Audacity clearly is not designed up front. It is something that\ndevelops over time. By and large the architecture we now have works well for us. We find\nourselves fighting the architecture when we try to add features that affect many of the source\nfiles. For example, Audacity currently handles stereo and mono tracks in a special cased way. If\nyou wanted to modify Audacity to handle surround sound you'd need to make changes in many\nclasses in Audacity.\nGoing Beyond Stereo: The GetLink Story}\nAudacity has never had an abstraction for number of channels. Instead the abstraction it\nuses is to link audio channels. There is a function GetLinkthat returns the other audio\nchannel in a pair if there are two and that returns NULL if the track is mono. Code that\nuses GetLink typically looks exactly as if it were originally written for mono and later a\ntest of (GetLink() != NULL) used to extend that code to handle stereo. I'm not sure it\nwas actually written that way, but I suspect it. There's no looping using GetLink to iterate\nthrough all channels in a linked list. Drawing, mixing, reading and writing all contain a test\nfor the stereo case rather than general code that can work for n channels where n is\nmost likely to be one or two. To go for the more general code you'd need to make\nchanges at around 100 of these calls to the GetLink function modifying at least 26 files.\nIt's easy to search the code to find GetLink calls and the changes needed are not that\ncomplex, so it is not as big a deal to fix this \"problem\" as it might sound at first. The\nGetLink story is not about a structural defect that is hard to fix. Rather it's illustrative of\nhow a relatively small defect can travel into a lot of code, if allowed to.\nWith hindsight it would have been good to make the GetLink function private and instead\nprovide an iterator to iterate through all channels in a track. This would have avoided\nmuch special case code for stereo, and at the same time made code that uses the list of\naudio channels agnostic with respect to the list implementation.\nThe more modular design is likely to drive us towards better hiding of internal structure. As we\ndefine and extend an external API we'll need to look more closely at the functions we're providing.\nThis will draw our attention to abstractions that we don't want to lock in to an external API.\n2.2. wxWidgets GUI Library\nThe most significant single library for Audacity user interface programmers is the wxWidgets GUI\nlibrary, which provides such things as buttons, sliders, check boxes, windows and dialogs. It\nprovides the most visible cross-platform behavior. The wxWidgets library has its own string class\nwxString, it has cross-platform abstractions for threads, filesystems, and fonts, and a\nmechanism for localization to other languages, all of which we use. We advise people new to\nAudacity development to first download wxWidgets and compile and experiment with some of the\nsamples that come with that library. wxWidgets is a relatively thin layer on the underlying GUI\nobjects provided by the operating system.\nTo build up complex dialogs wxWidgets provides not only individual widget elements but also\nsizers that control the elements' sizes and positions. This is a lot nicer than giving absolute fixed\npositions to graphical elements. If the widgets are resized either directly by the user or, say, by\nusing a different font size, the positioning of the elements in the dialogs updates in a very natural\nway. Sizers are important for a cross-platform application. Without them we might have to have\ncustom layouts of dialogs for each platform.\nOften the design for these dialogs is in a resource file that is read by the program. However in\nAudacity we exclusively compile dialog designs into the program as a series of calls to wxWidgets\nfunctions. This provides maximum flexibility: that is, dialogs whose exact contents and behavior\nwill be determined by application level code.\n", "page": 23, "type": "text", "section": "Page 23"}
{"text": "You could at one time find places in Audacity where the initial code for creating a GUI had clearly\nbeen code-generated using a graphical dialog building tool. Those tools helped us get a basic\ndesign. Over time the basic code was hacked around to add new features, resulting in many\nplaces where new dialogs were created by copying and modifying existing, already hacked-around\ndialog code.\nAfter a number of years of such development we found that large sections of the Audacity source\ncode, particularly the dialogs for configuring user preferences, consisted of tangled repetitive\ncode. That code, though simple in what it did, was surprisingly hard to follow. Part of the problem\nwas that the sequence in which dialogs were built up was quite arbitrary: smaller elements were\ncombined into larger ones and eventually into complete dialogs, but the order in which elements\nwere created by the code did not (and did not need to) resemble the order elements were laid out\non screen. The code was also verbose and repetitive. There was GUI-related code to transfer data\nfrom preferences stored on disk to intermediate variables, code to transfer from intermediate\nvariables to the displayed GUI, code to transfer from the displayed GUI to intermediate variables,\nand code to transfer from intermediate variables to the stored preferences. There were\ncomments in the code along the lines of //this is a mess, but it was quite some time before\nanything was done about it.\n2.3. ShuttleGui Layer\nThe solution to untangling all this code was a new class, ShuttleGui, that much reduced the\nnumber of lines of code needed to specify a dialog, making the code more readable. ShuttleGui is\nan extra layer between the wxWidgets library and Audacity. Its job is to transfer information\nbetween the two. Here's an example which results in the GUI elements pictured in Figure 2.2.\nShuttleGui S;\n// GUI Structure\nS.StartStatic(\"Some Title\",\u2026);\n{\n    S.AddButton(\"Some Button\",\u2026);\n    S.TieCheckbox(\"Some Checkbox\",\u2026);\n}\nS.EndStatic();\nFigure 2.2: Example Dialog\nThis code defines a static box in a dialog and that box contains a button and a checkbox. The\ncorrespondence between the code and the dialog should be clear. The StartStatic and\nEndStatic are paired calls. Other similar StartSomething/EndSomething pairs, which must\nmatch, are used for controlling other aspects of layout of the dialog. The curly brackets and the\nindenting that goes with them aren't needed for this to be correct code. We adopted the\nconvention of adding them in to make the structure and particularly the matching of the paired\ncalls obvious. It really helps readability in larger examples.\nThe source code shown does not just create the dialog. The code after the comment \"//GUI\nStructure\" can also be used to shuttle data from the dialog out to where the user preferences\nare stored, and to shuttle data back in. Previously a lot of the repetitive code came from the need\nto do this. Nowadays that code is only written once and is buried within the ShuttleGui class.\nThere are other extensions to the basic wxWidgets in Audacity. Audacity has its own class for\nmanaging toolbars. Why doesn't it use wxWidget's built in toolbar class? The reason is historic:\nAudacity's toolbars were written before wxWidgets provided a toolbar class.\n", "page": 24, "type": "text", "section": "Page 24"}
{"text": "2.4. The TrackPanel\nThe main panel in Audacity which displays audio waveforms is the TrackPanel. This is a custom\ncontrol drawn by Audacity. It's made up of components such as smaller panels with track\ninformation, a ruler for the timebase, rulers for amplitude, and tracks which may show\nwaveforms, spectra or textual labels. The tracks can be resized and moved around by dragging.\nThe tracks containing textual labels make use of our own re-implementation of an editable text\nbox rather than using the built-in text box. You might think these panels tracks and rulers should\neach be a wxWidgets component, but they are not.\nFigure 2.3: Audacity Interface with Track Panel Elements Labelled\nThe screenshot shown in Figure 2.3 shows the Audacity user interface. All the components that\nhave been labelled are custom for Audacity. As far as wxWidgets is concerned there is one\nwxWidget component for the TrackPanel. Audacity code, not wxWidgets, takes care of the\npositioning and repainting within that.\nThe way all these components fit together to make the TrackPanel is truly horrible. (It's the code\nthat's horrible; the end result the user sees looks just fine.) The GUI and application-specific code\nis all mixed together, not separated cleanly. In a good design only our application-specific code\nshould know about left and right audio channels, decibels, muting and soloing. GUI elements\nshould be application agnostic elements that are reusable in a non-audio application. Even the\npurely GUI parts of TrackPanel are a patchwork of special case code with absolute positions and\nsizes and not enough abstraction. It would be so much nicer, cleaner and more consistent if these\nspecial components were self-contained GUI elements and if they used sizers with the same kinds\nof interface as wxWidgets uses.\nTo get to such a TrackPanel we'd need a new sizer for wxWidgets that can move and resize tracks\nor, indeed, any other widget. wxWidgets sizers aren't yet that flexible. As a spin off benefit we\ncould use that sizer elsewhere. We could use it in the toolbars that hold the buttons, making it\neasy to customize the order of buttons within a toolbar by dragging.\n", "page": 25, "type": "text", "section": "Page 25"}
{"text": "Some exploratory work has been done in creating and using such sizers, but not enough. Some\nexperiments with making the GUI components fully fledged wxWidgets ran into a problem: doing\nso reduces our control over repainting of the widgets, resulting in flicker when resizing and\nmoving components. We would need to extensively modify wxWidgets to achieve flicker-free\nrepainting, and better separate the resizing steps from the repainting steps.\nA second reason to be wary of this approach for the TrackPanel is that we already know\nwxWidgets start running very slowly when there are large numbers of widgets. This is mostly\noutside of wxWidget's control. Each wxWidget, button, and text entry box uses a resource from\nthe windowing system. Each has a handle to access it. Processing large numbers of these takes\ntime. Processing is slow even when the majority of widgets are hidden or off screen. We want to\nbe able to use many small widgets on our tracks.\nThe best solution is to use a flyweight pattern, lightweight widgets that we draw ourselves, which\ndo not have corresponding objects that consume windowing system resources or handles. We\nwould use a structure like wxWidgets's sizers and component widgets, and give the components\na similar API but not actually derive from wxWidgets classes. We'd be refactoring our existing\nTrackPanel code so that its structure became a lot clearer. If this were an easy solution it would\nalready have been done, but diverging opinions about exactly what we want to end up with\nderailed an earlier attempt. Generalizing our current ad hoc approach would take significant design\nwork and coding. There is a great temptation to leave complex code that already works well\nenough alone.\n2.5. PortAudio Library: Recording and Playback\nPortAudio is the audio library that gives Audacity the ability to play and record audio in a cross-\nplatform way. Without it Audacity would not be able to use the sound card of the device it's\nrunning on. PortAudio provides the ring buffers, sample rate conversion when playing/recording\nand, crucially, provides an API that hides the differences between audio on Mac, Linux and\nWindows. Within PortAudio there are alternative implementation files to support this API for each\nplatform.\nI've never needed to dig into PortAudio to follow what happens inside. It is, however, useful to\nknow how we interface with PortAudio. Audacity accepts data packets from PortAudio (recording)\nand sends packets to PortAudio (playback). It's worth looking at exactly how the sending and\nreceiving happens, and how it fits in with reading and writing to disk and updates to the screen.\nSeveral different processes are going on at the same time. Some happen frequently, transfer\nsmall amounts of data, and must be responded to quickly. Others happen less frequently, transfer\nlarger quantities of data, and the exact timing of when they happen is less critical. This is an\nimpedance mismatch between the processes, and buffers are used to accommodate it. A second\npart of the picture is that we are dealing with audio devices, hard drives, and the screen. We don't\ngo down to the wire and so have to work with the APIs we're given. Whilst we would like each of\nour processes to look similar, for example to have each running from a wxThread, we don't have\nthat luxury (Figure 2.4).\nFigure 2.4: Threads and Buffers in Playback and Recording\n", "page": 26, "type": "text", "section": "Page 26"}
{"text": "One audio thread is started by PortAudio code and interacts directly with the audio device. This is\nwhat drives recording or playback. This thread has to be responsive or packets will get lost. The\nthread, under the control of PortAudio code, calls audacityAudioCallback which, when\nrecording, adds newly arrived small packets to a larger (five second) capture buffer. When playing\nback it takes small chunks off a five second playback buffer. The PortAudio library knows nothing\nabout wxWidgets and so this thread created by PortAudio is a pthread.\nA second thread is started by code in Audacity's class AudioIO. When recording, AudioIO takes\nthe data from the capture buffer and appends it to Audacity's tracks so that it will eventually get\ndisplayed. Additionally, when enough data has been added, AudioIO writes the data to disk. This\nsame thread also does the disk reads for audio playback. The function AudioIO::FillBuffers\nis the key function here and depending on the settings of some Boolean variables, handles both\nrecording and playback in the one function. It's important that the one function handle both\ndirections. Both the recording and playback parts are used at the same time when doing\n\"software play through,\" where you overdub what was previously recorded. In the AudioIO thread\nwe are totally at the mercy of the operating system's disk IO. We may stall for an unknown length\nof time reading or writing to a disk. We could not do those reads or writes in\naudacityAudioCallback because of the need to be responsive there.\nCommunication between these two threads happens via shared variables. Because we control\nwhich threads are writing to these variables and when, we avoid the need for more expensive\nmutexes.\nIn both playback and recording, there is an additional requirement: Audacity also needs to update\nthe GUI. This is the least time critical operation. The update happens in the main GUI thread and is\ndue to a periodic timer that ticks twenty times a second. This timer's tick causes\nTrackPanel::OnTimer to be called, and if updates to the GUI are found to be needed, they are\napplied. This main GUI thread is created within wxWidgets rather than by our own code. It is\nspecial in that other threads cannot directly update the GUI. Using a timer to get the GUI thread to\ncheck if it needs to update the screen allows us to reduce the number of repaints to a level that is\nacceptable for a responsive display, and not make too heavy demands on processor time for\ndisplaying.\nIs it good design to have an audio device thread, a buffer/disk thread and a GUI thread with\nperiodic timer to handle these audio data transfers? It is somewhat ad hoc to have these three\ndifferent threads that are not based on a single abstract base class. However, the ad-hockery is\nlargely dictated by the libraries we use. PortAudio expects to create a thread itself. The wxWidgets\nframework automatically has a GUI thread. Our need for a buffer filling thread is dictated by our\nneed to fix the impedance mismatch between the frequent small packets of the audio device\nthread and the less frequent larger packets of the disk drive. There is very clear benefit in using\nthese libraries. The cost in using the libraries is that we end up using the abstractions they\nprovide. As a result we copy data in memory from one place to another more than is strictly\nnecessary. In fast data switches I've worked on, I've seen extremely efficient code for handling\nthese kinds of impedance mismatches that is interrupt driven and does not use threads at all.\nPointers to buffers are passed around rather than copying data. You can only do that if the\nlibraries you are using are designed with a richer buffer abstraction. Using the existing interfaces,\nwe're forced to use threads and we're forced to copy data.\n2.6. BlockFiles\nOne of the challenges faced by Audacity is supporting insertions and deletions into audio\nrecordings that may be hours long. Recordings can easily be too long to fit in available RAM. If an\naudio recording is in a single disk file, inserting audio somewhere near the start of that file could\nmean moving a lot of data to make way. Copying that data on disk would be time consuming and\nmean that Audacity could then not respond rapidly to simple edits.\nAudacity's solution to this is to divide audio files into many BlockFiles, each of which could be\naround 1 MB. This is the main reason Audacity has its own audio file format, a master file with the\nextension .aup. It is an XML file which coordinates the various blocks. Changes near the start of a\nlong audio recording might affect just one block and the master .aup file.\nBlockFiles balance two conflicting forces. We can insert and delete audio without excessive\ncopying, and during playback we are guaranteed to get reasonably large chunks of audio with\n", "page": 27, "type": "text", "section": "Page 27"}
{"text": "each request to the disk. The smaller the blocks, the more potential disk requests to fetch the\nsame amount of audio data; the larger the blocks, the more copying on insertions and deletions.\nAudacity's BlockFiles never have internal free space and they never grow beyond the maximum\nblock size. To keep this true when we insert or delete we may end up copying up to one block's\nworth of data. When we don't need a BlockFile anymore we delete it. The BlockFiles are reference\ncounted so if we delete some audio, the relevant BlockFiles will still hang around to support the\nundo mechanism until we save. There is never a need to garbage collect free space within\nAudacity BlockFiles, which we would need to do with an all-in-one-file approach.\nMerging and splitting larger chunks of data is the bread and butter of data management systems,\nfrom B-trees to Google's BigTable tablets to the management of unrolled linked lists. Figure 2.5\nshows what happens in Audacity when removing a span of audio near the start.\nFigure 2.5: Before deletion, .aup file and BlockFiles hold the sequence ABCDEFGHIJKLMNO.\nAfter deletion of FGHI, two BlockFiles are merged.\nBlockFiles aren't just used for the audio itself. There are also BlockFiles that cache summary\ninformation. If Audacity is asked to display a four hour long recording on screen it is not\nacceptable for it to process the entire audio each time it redraws the screen. Instead it uses\nsummary information which gives the maximum and minimum audio amplitude over ranges of\ntime. When zoomed in, Audacity is drawing using actual samples. When zoomed out, Audacity is\ndrawing using summary information.\nA refinement in the BlockFile system is that the blocks needn't be files created by Audacity. They\ncan be references to subsections of audio files such as a timespan from audio stored in the .wav\nformat. A user can create an Audacity project, import audio from a .wav file and mix a number of\ntracks whilst only creating BlockFiles for the summary information. This saves disk space and\nsaves time in copying audio. All told it is, however, a rather bad idea. Far too many of our users\nhave removed the original audio .wav file thinking there will be a complete copy in the Audacity\nproject folder. That's not so and without the original .wav file the audio project can no longer be\nplayed. The default in Audacity nowadays is to always copy imported audio, creating new\nBlockFiles in the process.\nThe BlockFile solution ran into problems on Windows systems where having a large number of\nBlockFiles performed very poorly. This appeared to be because Windows was much slower\nhandling files when there were many in the same directory, a similar problem to the slowdown with\nlarge numbers of widgets. A later addition was made to use a hierarchy of subdirectories, never\nwith more than a hundred files in each subdirectory.\nThe main problem with the BlockFile structure is that it is exposed to end users. We often hear\nfrom users who move the .aup file and don't realize they also need to move the folder containing\nall the BlockFiles too. It would be better if Audacity projects were a single file with Audacity taking\nresponsibility for how the space inside the file is used. If anything this would increase performance\nrather than reduce it. The main additional code needed would be for garbage collection. A simple\napproach to that would be to copy the blocks to a new file when saving if more than a set\npercentage of the file were unused.\n2.7. Scripting\n", "page": 28, "type": "text", "section": "Page 28"}
{"text": "Audacity has an experimental plugin that supports multiple scripting languages. It provides a\nscripting interface over a named pipe. The commands exposed via scripting are in a textual\nformat, as are the responses. As long as the user's scripting language can write text to and read\ntext from a named pipe, the scripting language can drive Audacity. Audio and other high-volume\ndata does not need to travel on the pipe (Figure 2.6).\nFigure 2.6: Scripting Plugin Provides Scripting Over a Named Pipe\nThe plugin itself knows nothing about the content of the text traffic that it carries. It is only\nresponsible for conveying it. The plugin interface (or rudimentary extension point) used by the\nscripting plugin to plug in to Audacity already exposes Audacity commands in textual format. So,\nthe scripting plugin is small, its main content being code for the pipe.\nUnfortunately a pipe introduces similar security risks to having a TCP/IP connection\u2014and we've\nruled out TCP/IP connections for Audacity on security grounds. To reduce that risk the plugin is an\noptional DLL. You have to make a deliberate decision to obtain and use it and it comes with a\nhealth/security warning.\nAfter the scripting feature had already been started, a suggestion surfaced in the feature requests\npage of our wiki that we should consider using KDE's D-Bus standard to provide an inter-process\ncall mechanism using TCP/IP. We'd already started going down a different route but it still might\nmake sense to adapt the interface we've ended up with to support D-Bus.\nOrigins of Scripting Code\nThe scripting feature grew from an enthusiast's adaptation of Audacity for a particular\nneed that was heading in the direction of being a fork. These features, together called\nCleanSpeech, provide for mp3 conversion of sermons. CleanSpeech adds new effects\nsuch as truncate silence\u2014the effect finds and cuts out long silences in audio\u2014and the\nability to apply a fixed sequence of existing noise removal effects, normalization and mp3\nconversion to a whole batch of audio recordings. We wanted some of the excellent\nfunctionality in this, but the way it was written was too special case for Audacity. Bringing\nit into mainstream Audacity led us to code for a flexible sequence rather than a fixed\nsequence. The flexible sequence could use any of the effects via a look-up table for\ncommand names and a Shuttle class to persist the command parameters to a textual\nformat in user preferences. This feature is called batch chains. Very deliberately we\nstopped short of adding conditionals or calculation to avoid inventing an ad hoc scripting\nlanguage.\nIn retrospect the effort to avoid a fork has been well worthwhile. There is still a\nCleanSpeech mode buried in Audacity that can be set by modifying a preference. It also\ncuts down the user interface, removing advanced features. A simplified version of\nAudacity has been requested for other uses, most notably in schools. The problem is that\neach person's view of which are the advanced features and which are the essential ones\nis different. We've subsequently implemented a simple hack that leverages the translation\nmechanism. When the translation of a menu item starts with a \"#\" it is no longer shown in\nthe menus. That way people who want to reduce the menus can make choices\nthemselves without recompiling\u2014more general and less invasive than the mCleanspeech\nflag in Audacity, which in time we may be able to remove entirely.\nThe CleanSpeech work gave us batch chains and the ability to truncate silence. Both have\nattracted additional improvement from outside the core team. Batch chains directly led on\nto the scripting feature. That in turn has begun the process of supporting more general\npurpose plugins to adapt Audacity.\n", "page": 29, "type": "text", "section": "Page 29"}
{"text": "2.8. Real-Time Effects\nAudacity does not have real-time effects, that is, audio effects that are calculated on demand as\nthe audio plays. Instead in Audacity you apply an effect and must wait for it to complete. Real-time\neffects and rendering of audio effects in the background whilst the user interface stays\nresponsive are among the most frequently made feature requests for Audacity.\nA problem we have is that what may be a real-time effect on one machine may not run fast\nenough to be real-time on a much slower machine. Audacity runs on a wide range of machines.\nWe'd like a graceful fallback. On a slower machine we'd still want to be able to request an effect be\napplied to an entire track and to then listen to the processed audio near the middle of the track,\nafter a small wait, with Audacity knowing to process that part first. On a machine too slow to\nrender the effect in real time we'd be able to listen to the audio until playback caught up with the\nrendering. To do this we'd need to remove the restrictions that audio effects hold up the user\ninterface and that the order of processing the audio blocks is strictly left to right.\nA relatively recent addition in Audacity called on demand loading has many of the elements we\nneed for real time effects, though it doesn't involve audio effects at all. When you import an audio\nfile into Audacity, it can now make the summary BlockFiles in a background task. Audacity will\nshow a placeholder of diagonal blue and gray stripes for audio that it has not yet processed and\nrespond to many user commands whilst the audio is still being loaded. The blocks do not have to\nbe processed in left-to-right order. The intention has always been that the same code will in due\ncourse be used for real-time effects.\nOn demand loading gives us an evolutionary approach to adding real time effects. It's a step that\navoids some of the complexities of making the effects themselves real-time. Real-time effects will\nadditionally need overlap between the blocks, otherwise effects like echo will not join up correctly.\nWe'll also need to allow parameters to vary as the audio is playing. By doing on demand loading\nfirst, the code gets used at an earlier stage than it otherwise would. It will get feedback and\nrefinement from actual use.\n2.9. Summary\nThe earlier sections of this chapter illustrate how good structure contribute to a program's\ngrowth, or how the absence of good structure hinders it.\nThird party APIs such as PortAudio and wxWidgets have been of huge benefit. They've given\nus code that works to build on, and abstracted away many platform differences. One price\nwe pay for using them is that we don't get the flexibility to choose the abstractions. We have\nless than pretty code for playback and recording because we have to handle threading in\nthree different ways. The code also does more copying of data than it could do if we\ncontrolled the abstractions.\nThe API given to us by wxWidgets tempted us into writing some verbose, hard to follow\napplication code. Our solution to that was to add a facade in front of wxWidgets to give us\nthe abstractions we wanted and cleaner application code.\nIn the TrackPanel of Audacity we needed to go outside the features that could easily be got\nfrom existing widgets. As a result we rolled our own ad hoc system. There is a cleaner\nsystem with widgets and sizers and logically distinct application level objects struggling to\ncome out of the TrackPanel.\nStructural decisions are wider ranging than deciding how to structure new features. A\ndecision about what not to include in a program can be as important. It can lead to cleaner,\nsafer code. It's a pleasure to get the benefits of scripting languages like Perl without having\nto do the work of maintaining our own copy. Structural decisions also are driven by plans for\nfuture growth. Our embryonic modular system is expected to lead to more experimentation\nby making experiments safer. On demand loading is expected to be an evolutionary step\ntowards on demand processing of real time effects.\nThe more you look, the more obvious it is that Audacity is a community effort. The community is\nlarger than just those contributing directly because it depends on libraries, each of which has its\nown community with its own domain experts. Having read about the mix of structure in Audacity\nit probably comes as no surprise that the community developing it welcomes new developers and\nis well able to handle a wide range of skill levels.\n", "page": 30, "type": "text", "section": "Page 30"}
{"text": "For me there is no question that the nature of the community behind Audacity is reflected in the\nstrengths and weaknesses of the code. A more closed group could write high quality code more\nconsistently than we have, but it would be harder to match the range of capabilities Audacity has\nwith fewer people contributing.\nFootnotes\n1. The one exception to this is the Lisp-based Nyquist language which has been built into\nAudacity from very early days. We would like to make it a separate module, bundled with\nAudacity, but we have not yet had the time to make that change.\n", "page": 31, "type": "text", "section": "Page 31"}
{"text": "The Architecture of\nOpen Source Applications\nAmy Brown and Greg Wilson (eds.)\nISBN 978-1-257-63801-7 \nLicense / Buy / Contribute\nChapter 3. The Bourne-Again Shell\nChet Ramey\n3.1. Introduction\nA Unix shell provides an interface that lets the user interact with the operating system by running\ncommands. But a shell is also a fairly rich programming language: there are constructs for flow\ncontrol, alternation, looping, conditionals, basic mathematical operations, named functions, string\nvariables, and two-way communication between the shell and the commands it invokes.\nShells can be used interactively, from a terminal or terminal emulator such as xterm, and non-\ninteractively, reading commands from a file. Most modern shells, including bash, provide\ncommand-line editing, in which the command line can be manipulated using emacs- or vi-like\ncommands while it's being entered, and various forms of a saved history of commands.\nBash processing is much like a shell pipeline: after being read from the terminal or a script, data is\npassed through a number of stages, transformed at each step, until the shell finally executes a\ncommand and collects its return status.\nThis chapter will explore bash's major components: input processing, parsing, the various word\nexpansions and other command processing, and command execution, from the pipeline\nperspective. These components act as a pipeline for data read from the keyboard or from a file,\nturning it into an executed command.\nFigure 3.1: Bash Component Architecture\n", "page": 32, "type": "text", "section": "Page 32"}
{"text": "3.1.1. Bash\nBash is the shell that appears in the GNU operating system, commonly implemented atop the\nLinux kernel, and several other common operating systems, most notably Mac OS X. It offers\nfunctional improvements over historical versions of sh for both interactive and programming use.\nThe name is an acronym for Bourne-Again SHell, a pun combining the name of Stephen Bourne\n(the author of the direct ancestor of the current Unix shell /bin/sh, which appeared in the Bell\nLabs Seventh Edition Research version of Unix) with the notion of rebirth through\nreimplementation. The original author of bash was Brian Fox, an employee of the Free Software\nFoundation. I am the current developer and maintainer, a volunteer who works at Case Western\nReserve University in Cleveland, Ohio.\nLike other GNU software, bash is quite portable. It currently runs on nearly every version of Unix\nand a few other operating systems\u2014independently-supported ports exist for hosted Windows\nenvironments such as Cygwin and MinGW, and ports to Unix-like systems such as QNX and Minix\nare part of the distribution. It only requires a Posix environment to build and run, such as one\nprovided by Microsoft's Services for Unix (SFU).\n3.2. Syntactic Units and Primitives\n3.2.1. Primitives\nTo bash, there are basically three kinds of tokens: reserved words, words, and operators.\nReserved words are those that have meaning to the shell and its programming language; usually\nthese words introduce flow control constructs, like if and while. Operators are composed of\none or more metacharacters: characters that have special meaning to the shell on their own, such\nas | and >. The rest of the shell's input consists of ordinary words, some of which have special\nmeaning\u2014assignment statements or numbers, for instance\u2014depending on where they appear on\nthe command line.\n3.2.2. Variables and Parameters\nAs in any programming language, shells provide variables: names to refer to stored data and\noperate on it. The shell provides basic user-settable variables and some built-in variables referred\nto as parameters. Shell parameters generally reflect some aspect of the shell's internal state, and\nare set automatically or as a side effect of another operation.\nVariable values are strings. Some values are treated specially depending on context; these will be\nexplained later. Variables are assigned using statements of the form name=value. The value is\noptional; omitting it assigns the empty string to name. If the value is supplied, the shell expands\nthe value and assigns it to name. The shell can perform different operations based on whether or\nnot a variable is set, but assigning a value is the only way to set a variable. Variables that have not\nbeen assigned a value, even if they have been declared and given attributes, are referred to as\nunset.\nA word beginning with a dollar sign introduces a variable or parameter reference. The word,\nincluding the dollar sign, is replaced with the value of the named variable. The shell provides a rich\nset of expansion operators, from simple value replacement to changing or removing portions of a\nvariable's value that match a pattern.\nThere are provisions for local and global variables. By default, all variables are global. Any simple\ncommand (the most familiar type of command\u2014a command name and optional set of arguments\nand redirections) may be prefixed by a set of assignment statements to cause those variables to\nexist only for that command. The shell implements stored procedures, or shell functions, which\ncan have function-local variables.\n", "page": 33, "type": "text", "section": "Page 33"}
{"text": "Variables can be minimally typed: in addition to simple string-valued variables, there are integers\nand arrays. Integer-typed variables are treated as numbers: any string assigned to them is\nexpanded as an arithmetic expression and the result is assigned as the variable's value. Arrays\nmay be indexed or associative; indexed arrays use numbers as subscripts, while associative\narrays use arbitrary strings. Array elements are strings, which can be treated as integers if\ndesired. Array elements may not be other arrays.\nBash uses hash tables to store and retrieve shell variables, and linked lists of these hash tables to\nimplement variable scoping. There are different variable scopes for shell function calls and\ntemporary scopes for variables set by assignment statements preceding a command. When\nthose assignment statements precede a command that is built into the shell, for instance, the shell\nhas to keep track of the correct order in which to resolve variable references, and the linked\nscopes allow bash to do that. There can be a surprising number of scopes to traverse depending\non the execution nesting level.\n3.2.3. The Shell Programming Language\nA simple shell command, one with which most readers are most familiar, consists of a command\nname, such as echo or cd, and a list of zero or more arguments and redirections. Redirections\nallow the shell user to control the input to and output from invoked commands. As noted above,\nusers can define variables local to simple commands.\nReserved words introduce more complex shell commands. There are constructs common to any\nhigh-level programming language, such as if-then-else, while, a for loop that iterates over a\nlist of values, and a C-like arithmetic for loop. These more complex commands allow the shell to\nexecute a command or otherwise test a condition and perform different operations based on the\nresult, or execute commands multiple times.\nOne of the gifts Unix brought the computing world is the pipeline: a linear list of commands, in\nwhich the output of one command in the list becomes the input of the next. Any shell construct\ncan be used in a pipeline, and it's not uncommon to see pipelines in which a command feeds data\nto a loop.\nBash implements a facility that allows the standard input, standard output, and standard error\nstreams for a command to be redirected to another file or process when the command is invoked.\nShell programmers can also use redirection to open and close files in the current shell\nenvironment.\nBash allows shell programs to be stored and used more than once. Shell functions and shell\nscripts are both ways to name a group of commands and execute the group, just like executing\nany other command. Shell functions are declared using a special syntax and stored and executed\nin the same shell's context; shell scripts are created by putting commands into a file and executing\na new instance of the shell to interpret them. Shell functions share most of the execution context\nwith the shell that calls them, but shell scripts, since they are interpreted by a new shell invocation,\nshare only what is passed between processes in the environment.\n3.2.4. A Further Note\nAs you read further, keep in mind that the shell implements its features using only a few data\nstructures: arrays, trees, singly-linked and doubly-linked lists, and hash tables. Nearly all of the\nshell constructs are implemented using these primitives.\nThe basic data structure the shell uses to pass information from one stage to the next, and to\noperate on data units within each processing stage, is the WORD_DESC:\ntypedef struct word_desc {\n  char *word;           /* Zero terminated string. */\n  int flags;            /* Flags associated with this word. */\n} WORD_DESC;\n", "page": 34, "type": "text", "section": "Page 34"}
{"text": "Words are combined into, for example, argument lists, using simple linked lists:\ntypedef struct word_list {\n  struct word_list *next;\n  WORD_DESC *word;\n} WORD_LIST;\nWORD_LISTs are pervasive throughout the shell. A simple command is a word list, the result of\nexpansion is a word list, and the built-in commands each take a word list of arguments.\n3.3. Input Processing\nThe first stage of the bash processing pipeline is input processing: taking characters from the\nterminal or a file, breaking them into lines, and passing the lines to the shell parser to transform\ninto commands. As you would expect, the lines are sequences of characters terminated by\nnewlines.\n3.3.1. Readline and Command Line Editing\nBash reads input from the terminal when interactive, and from the script file specified as an\nargument otherwise. When interactive, bash allows the user to edit command lines as they are\ntyped in, using familiar key sequences and editing commands similar to the Unix emacs and vi\neditors.\nBash uses the readline library to implement command line editing. This provides a set of functions\nallowing users to edit command lines, functions to save command lines as they are entered, to\nrecall previous commands, and to perform csh-like history expansion. Bash is readline's primary\nclient, and they are developed together, but there is no bash-specific code in readline. Many other\nprojects have adopted readline to provide a terminal-based line editing interface.\nReadline also allows users to bind key sequences of unlimited length to any of a large number of\nreadline commands. Readline has commands to move the cursor around the line, insert and\nremove text, retrieve previous lines, and complete partially-typed words. On top of this, users may\ndefine macros, which are strings of characters that are inserted into the line in response to a key\nsequence, using the same syntax as key bindings. Macros afford readline users a simple string\nsubstitution and shorthand facility.\nReadline Structure\nReadline is structured as a basic read/dispatch/execute/redisplay loop. It reads characters from\nthe keyboard using read or equivalent, or obtains input from a macro. Each character is used as\nan index into a keymap, or dispatch table. Though indexed by a single eight-bit character, the\ncontents of each element of the keymap can be several things. The characters can resolve to\nadditional keymaps, which is how multiple-character key sequences are possible. Resolving to a\nreadline command, such as beginning-of-line, causes that command to be executed. A\ncharacter bound to the self-insert command is stored into the editing buffer. It's also possible\nto bind a key sequence to a command while simultaneously binding subsequences to different\ncommands (a relatively recently-added feature); there is a special index into a keymap to indicate\nthat this is done. Binding a key sequence to a macro provides a great deal of flexibility, from\ninserting arbitrary strings into a command line to creating keyboard shortcuts for complex editing\nsequences. Readline stores each character bound to self-insert in the editing buffer, which\nwhen displayed may occupy one or more lines on the screen.\nReadline manages only character buffers and strings using C chars, and builds multibyte\ncharacters out of them if necessary. It does not use wchar_t internally for both speed and\nstorage reasons, and because the editing code existed before multibyte character support\nbecame widespread. When in a locale that supports multibyte characters, readline automatically\nreads an entire multibyte character and inserts it into the editing buffer. It's possible to bind\nmultibyte characters to editing commands, but one has to bind such a character as a key\n", "page": 35, "type": "text", "section": "Page 35"}
{"text": "sequence; this is possible, but difficult and usually not wanted. The existing emacs and vi\ncommand sets do not use multibyte characters, for instance.\nOnce a key sequence finally resolves to an editing command, readline updates the terminal display\nto reflect the results. This happens regardless of whether the command results in characters\nbeing inserted into the buffer, the editing position being moved, or the line being partially or\ncompletely replaced. Some bindable editing commands, such as those that modify the history file,\ndo not cause any change to the contents of the editing buffer.\nUpdating the terminal display, while seemingly simple, is quite involved. Readline has to keep track\nof three things: the current contents of the buffer of characters displayed on the screen, the\nupdated contents of that display buffer, and the actual characters displayed. In the presence of\nmultibyte characters, the characters displayed do not exactly match the buffer, and the redisplay\nengine must take that into account. When redisplaying, readline must compare the current display\nbuffer's contents with the updated buffer, figure out the differences, and decide how to most\nefficiently modify the display to reflect the updated buffer. This problem has been the subject of\nconsiderable research through the years (the string-to-string correction problem). Readline's\napproach is to identify the beginning and end of the portion of the buffer that differs, compute the\ncost of updating just that portion, including moving the cursor backward and forward (e.g., will it\ntake more effort to issue terminal commands to delete characters and then insert new ones than\nto simply overwrite the current screen contents?), perform the lowest-cost update, then clean up\nby removing any characters remaining at the end of the line if necessary and position the cursor\nin the correct spot.\nThe redisplay engine is without question the one piece of readline that has been modified most\nheavily. Most of the changes have been to add functionality\u2014most significantly, the ability to have\nnon-displaying characters in the prompt (to change colors, for instance) and to cope with\ncharacters that take up more than a single byte.\nReadline returns the contents of the editing buffer to the calling application, which is then\nresponsible for saving the possibly-modified results in the history list.\nApplications Extending Readline\nJust as readline offers users a variety of ways to customize and extend readline's default behavior,\nit provides a number of mechanisms for applications to extend its default feature set. First,\nbindable readline functions accept a standard set of arguments and return a specified set of\nresults, making it easy for applications to extend readline with application-specific functions. Bash,\nfor instance, adds more than thirty bindable commands, from bash-specific word completions to\ninterfaces to shell built-in commands.\nThe second way readline allows applications to modify its behavior is through the pervasive use of\npointers to hook functions with well-known names and calling interfaces. Applications can replace\nsome portions of readline's internals, interpose functionality in front of readline, and perform\napplication-specific transformations.\n3.3.2. Non-interactive Input Processing\nWhen the shell is not using readline, it uses either stdio or its own buffered input routines to\nobtain input. The bash buffered input package is preferable to stdio when the shell is not\ninteractive because of the somewhat peculiar restrictions Posix imposes on input consumption:\nthe shell must consume only the input necessary to parse a command and leave the rest for\nexecuted programs. This is particularly important when the shell is reading a script from the\nstandard input. The shell is allowed to buffer input as much as it wants, as long as it is able to roll\nthe file offset back to just after the last character the parser consumes. As a practical matter, this\nmeans that the shell must read scripts a character at a time when reading from non-seekable\ndevices such as pipes, but may buffer as many characters as it likes when reading from files.\nThese idiosyncrasies aside, the output of the non-interactive input portion of shell processing is\nthe same as readline: a buffer of characters terminated by a newline.\n", "page": 36, "type": "text", "section": "Page 36"}
{"text": "3.3.3. Multibyte Characters\nMultibyte character processing was added to the shell a long time after its initial implementation,\nand it was done in a way designed to minimize its impact on the existing code. When in a locale\nthat supports multibyte characters, the shell stores its input in a buffer of bytes (C chars), but\ntreats these bytes as potentially multibyte characters. Readline understands how to display\nmultibyte characters (the key is knowing how many screen positions a multibyte character\noccupies, and how many bytes to consume from a buffer when displaying a character on the\nscreen), how to move forward and backward in the line a character at a time, as opposed to a\nbyte at a time, and so on. Other than that, multibyte characters don't have much effect on shell\ninput processing. Other parts of the shell, described later, need to be aware of multibyte\ncharacters and take them into account when processing their input.\n3.4. Parsing\nThe initial job of the parsing engine is lexical analysis: to separate the stream of characters into\nwords and apply meaning to the result. The word is the basic unit on which the parser operates.\nWords are sequences of characters separated by metacharacters, which include simple\nseparators like spaces and tabs, or characters that are special to the shell language, like\nsemicolons and ampersands.\nOne historical problem with the shell, as Tom Duff said in his paper about rc, the Plan 9 shell, is\nthat nobody really knows what the Bourne shell grammar is. The Posix shell committee deserves\nsignificant credit for finally publishing a definitive grammar for a Unix shell, albeit one that has\nplenty of context dependencies. That grammar isn't without its problems\u2014it disallows some\nconstructs that historical Bourne shell parsers have accepted without error\u2014but it's the best we\nhave.\nThe bash parser is derived from an early version of the Posix grammar, and is, as far as I know,\nthe only Bourne-style shell parser implemented using Yacc or Bison. This has presented its own\nset of difficulties\u2014the shell grammar isn't really well-suited to yacc-style parsing and requires\nsome complicated lexical analysis and a lot of cooperation between the parser and the lexical\nanalyzer.\nIn any event, the lexical analyzer takes lines of input from readline or another source, breaks them\ninto tokens at metacharacters, identifies the tokens based on context, and passes them on to the\nparser to be assembled into statements and commands. There is a lot of context involved\u2014for\ninstance, the word for can be a reserved word, an identifier, part of an assignment statement, or\nother word, and the following is a perfectly valid command:\nfor for in for; do for=for; done; echo $for\nthat displays for.\nAt this point, a short digression about aliasing is in order. Bash allows the first word of a simple\ncommand to be replaced with arbitrary text using aliases. Since they're completely lexical, aliases\ncan even be used (or abused) to change the shell grammar: it's possible to write an alias that\nimplements a compound command that bash doesn't provide. The bash parser implements\naliasing completely in the lexical phase, though the parser has to inform the analyzer when alias\nexpansion is permitted.\nLike many programming languages, the shell allows characters to be escaped to remove their\nspecial meaning, so that metacharacters such as & can appear in commands. There are three\ntypes of quoting, each of which is slightly different and permits slightly different interpretations of\nthe quoted text: the backslash, which escapes the next character; single quotes, which prevent\ninterpretation of all enclosed characters; and double quotes, which prevent some interpretation\nbut allow certain word expansions (and treats backslashes differently). The lexical analyzer\ninterprets quoted characters and strings and prevents them from being recognized by the parser\nas reserved words or metacharacters. There are also two special cases, $'\u2026' and $\"\u2026\", that\n", "page": 37, "type": "text", "section": "Page 37"}
{"text": "expand backslash-escaped characters in the same fashion as ANSI C strings and allow characters\nto be translated using standard internationalization functions, respectively. The former is widely\nused; the latter, perhaps because there are few good examples or use cases, less so.\nThe rest of the interface between the parser and lexical analyzer is straightforward. The parser\nencodes a certain amount of state and shares it with the analyzer to allow the sort of context-\ndependent analysis the grammar requires. For example, the lexical analyzer categorizes words\naccording to the token type: reserved word (in the appropriate context), word, assignment\nstatement, and so on. In order to do this, the parser has to tell it something about how far it has\nprogressed parsing a command, whether it is processing a multiline string (sometimes called a\n\"here-document\"), whether it's in a case statement or a conditional command, or whether it is\nprocessing an extended shell pattern or compound assignment statement.\nMuch of the work to recognize the end of the command substitution during the parsing stage is\nencapsulated into a single function (parse_comsub), which knows an uncomfortable amount of\nshell syntax and duplicates rather more of the token-reading code than is optimal. This function\nhas to know about here documents, shell comments, metacharacters and word boundaries,\nquoting, and when reserved words are acceptable (so it knows when it's in a case statement); it\ntook a while to get that right.\nWhen expanding a command substitution during word expansion, bash uses the parser to find\nthe correct end of the construct. This is similar to turning a string into a command for eval, but\nin this case the command isn't terminated by the end of the string. In order to make this work, the\nparser must recognize a right parenthesis as a valid command terminator, which leads to special\ncases in a number of grammar productions and requires the lexical analyzer to flag a right\nparenthesis (in the appropriate context) as denoting EOF. The parser also has to save and restore\nparser state before recursively invoking yyparse, since a command substitution can be parsed\nand executed as part of expanding a prompt string in the middle of reading a command. Since the\ninput functions implement read-ahead, this function must finally take care of rewinding the bash\ninput pointer to the right spot, whether bash is reading input from a string, a file, or the terminal\nusing readline. This is important not only so that input is not lost, but so the command\nsubstitution expansion functions construct the correct string for execution.\nSimilar problems are posed by programmable word completion, which allows arbitrary commands\nto be executed while parsing another command, and solved by saving and restoring parser state\naround invocations.\nQuoting is also a source of incompatibility and debate. Twenty years after the publication of the\nfirst Posix shell standard, members of the standards working group are still debating the proper\nbehavior of obscure quoting. As before, the Bourne shell is no help other than as a reference\nimplementation to observe behavior.\nThe parser returns a single C structure representing a command (which, in the case of compound\ncommands like loops, may include other commands in turn) and passes it to the next stage of the\nshell's operation: word expansion. The command structure is composed of command objects and\nlists of words. Most of the word lists are subject to various transformations, depending on their\ncontext, as explained in the following sections.\n3.5. Word Expansions\nAfter parsing, but before execution, many of the words produced by the parsing stage are\nsubjected to one or more word expansions, so that (for example) $OSTYPE is replaced with the\nstring \"linux-gnu\".\n3.5.1. Parameter and Variable Expansions\nVariable expansions are the ones users find most familiar. Shell variables are barely typed, and,\nwith few exceptions, are treated as strings. The expansions expand and transform these strings\ninto new words and word lists.\n", "page": 38, "type": "text", "section": "Page 38"}
{"text": "There are expansions that act on the variable's value itself. Programmers can use these to\nproduce substrings of a variable's value, the value's length, remove portions that match a\nspecified pattern from the beginning or end, replace portions of the value matching a specified\npattern with a new string, or modify the case of alphabetic characters in a variable's value.\nIn addition, there are expansions that depend on the state of a variable: different expansions or\nassignments happen based on whether or not the variable is set. For instance, ${parameter:-\nword} will expand to parameter if it's set, and word if it's not set or set to the empty string.\n3.5.2. And Many More\nBash does many other kinds of expansion, each of which has its own quirky rules. The first in\nprocessing order is brace expansion, which turns:\npre{one,two,three}post\ninto:\npreonepost pretwopost prethreepost\nThere is also command substitution, which is a nice marriage of the shell's ability to run commands\nand manipulate variables. The shell runs a command, collects the output, and uses that output as\nthe value of the expansion.\nOne of the problems with command substitution is that it runs the enclosed command\nimmediately and waits for it to complete: there's no easy way for the shell to send input to it. Bash\nuses a feature named process substitution, a sort of combination of command substitution and\nshell pipelines, to compensate for these shortcomings. Like command substitution, bash runs a\ncommand, but lets it run in the background and doesn't wait for it to complete. The key is that\nbash opens a pipe to the command for reading or writing and exposes it as a filename, which\nbecomes the result of the expansion.\nNext is tilde expansion. Originally intended to turn ~alan into a reference to Alan's home\ndirectory, it has grown over the years into a way to refer to a large number of different\ndirectories.\nFinally, there is arithmetic expansion. $((expression)) causes expression to be evaluated\naccording to the same rules as C language expressions. The result of the expression becomes the\nresult of the expansion.\nVariable expansion is where the difference between single and double quotes becomes most\napparent. Single quotes inhibit all expansions\u2014the characters enclosed by the quotes pass\nthrough the expansions unscathed\u2014whereas double quotes permit some expansions and inhibit\nothers. The word expansions and command, arithmetic, and process substitution take place\u2014the\ndouble quotes only affect how the result is handled\u2014but brace and tilde expansion do not.\n3.5.3. Word Splitting\nThe results of the word expansions are split using the characters in the value of the shell variable\nIFS as delimiters. This is how the shell transforms a single word into more than one. Each time\none of the characters in $IFS1 appears in the result, bash splits the word into two. Single and\ndouble quotes both inhibit word splitting.\n3.5.4. Globbing\nAfter the results are split, the shell interprets each word resulting from the previous expansions\nas a potential pattern and tries to match it against an existing filename, including any leading\ndirectory path.\n3.5.5. Implementation\n", "page": 39, "type": "text", "section": "Page 39"}
{"text": "If the basic architecture of the shell parallels a pipeline, the word expansions are a small pipeline\nunto themselves. Each stage of word expansion takes a word and, after possibly transforming it,\npasses it to the next expansion stage. After all the word expansions have been performed, the\ncommand is executed.\nThe bash implementation of word expansions builds on the basic data structures already\ndescribed. The words output by the parser are expanded individually, resulting in one or more\nwords for each input word. The WORD_DESC data structure has proved versatile enough to hold all\nthe information required to encapsulate the expansion of a single word. The flags are used to\nencode information for use within the word expansion stage and to pass information from one\nstage to the next. For instance, the parser uses a flag to tell the expansion and command\nexecution stages that a particular word is a shell assignment statement, and the word expansion\ncode uses flags internally to inhibit word splitting or note the presence of a quoted null string\n(\"$x\", where $x is unset or has a null value). Using a single character string for each word being\nexpanded, with some kind of character encoding to represent additional information, would have\nproved much more difficult.\nAs with the parser, the word expansion code handles characters whose representation requires\nmore than a single byte. For example, the variable length expansion (${#variable}) counts the\nlength in characters, rather than bytes, and the code can correctly identify the end of expansions\nor characters special to expansions in the presence of multibyte characters.\n3.6. Command Execution\nThe command execution stage of the internal bash pipeline is where the real action happens. Most\nof the time, the set of expanded words is decomposed into a command name and set of\narguments, and passed to the operating system as a file to be read and executed with the\nremaining words passed as the rest of the elements of argv.\nThe description thus far has deliberately concentrated on what Posix calls simple commands\u2014\nthose with a command name and a set of arguments. This is the most common type of\ncommand, but bash provides much more.\nThe input to the command execution stage is the command structure built by the parser and a set\nof possibly-expanded words. This is where the real bash programming language comes into play.\nThe programming language uses the variables and expansions discussed previously, and\nimplements the constructs one would expect in a high-level language: looping, conditionals,\nalternation, grouping, selection, conditional execution based on pattern matching, expression\nevaluation, and several higher-level constructs specific to the shell.\n3.6.1. Redirection\nOne reflection of the shell's role as an interface to the operating system is the ability to redirect\ninput and output to and from the commands it invokes. The redirection syntax is one of the things\nthat reveals the sophistication of the shell's early users: until very recently, it required users to\nkeep track of the file descriptors they were using, and explicitly specify by number any other than\nstandard input, output, and error.\nA recent addition to the redirection syntax allows users to direct the shell to choose a suitable file\ndescriptor and assign it to a specified variable, instead of having the user choose one. This\nreduces the programmer's burden of keeping track of file descriptors, but adds extra processing:\nthe shell has to duplicate file descriptors in the right place, and make sure they are assigned to the\nspecified variable. This is another example of how information is passed from the lexical analyzer\nto the parser through to command execution: the analyzer classifies the word as a redirection\ncontaining a variable assignment; the parser, in the appropriate grammar production, creates the\nredirection object with a flag indicating assignment is required; and the redirection code interprets\nthe flag and ensures that the file descriptor number is assigned to the correct variable.\nThe hardest part of implementing redirection is remembering how to undo redirections. The shell\n", "page": 40, "type": "text", "section": "Page 40"}
{"text": "deliberately blurs the distinction between commands executed from the filesystem that cause the\ncreation of a new process and commands the shell executes itself (builtins), but, no matter how\nthe command is implemented, the effects of redirections should not persist beyond the\ncommand's completion2. The shell therefore has to keep track of how to undo the effects of each\nredirection, otherwise redirecting the output of a shell builtin would change the shell's standard\noutput. Bash knows how to undo each type of redirection, either by closing a file descriptor that it\nallocated, or by saving file descriptor being duplicated to and restoring it later using dup2. These\nuse the same redirection objects as those created by the parser and are processed using the\nsame functions.\nSince multiple redirections are implemented as simple lists of objects, the redirections used to\nundo are kept in a separate list. That list is processed when a command completes, but the shell\nhas to take care when it does so, since redirections attached to a shell function or the \".\" builtin\nmust stay in effect until that function or builtin completes. When it doesn't invoke a command, the\nexec builtin causes the undo list to simply be discarded, because redirections associated with\nexec persist in the shell environment.\nThe other complication is one bash brought on itself. Historical versions of the Bourne shell\nallowed the user to manipulate only file descriptors 0-9, reserving descriptors 10 and above for\nthe shell's internal use. Bash relaxed this restriction, allowing a user to manipulate any descriptor\nup to the process's open file limit. This means that bash has to keep track of its own internal file\ndescriptors, including those opened by external libraries and not directly by the shell, and be\nprepared to move them around on demand. This requires a lot of bookkeeping, some heuristics\ninvolving the close-on-exec flag, and yet another list of redirections to be maintained for the\nduration of a command and then either processed or discarded.\n3.6.2. Builtin Commands\nBash makes a number of commands part of the shell itself. These commands are executed by the\nshell, without creating a new process.\nThe most common reason to make a command a builtin is to maintain or modify the shell's internal\nstate. cd is a good example; one of the classic exercises for introduction to Unix classes is to\nexplain why cd can't be implemented as an external command.\nBash builtins use the same internal primitives as the rest of the shell. Each builtin is implemented\nusing a C language function that takes a list of words as arguments. The words are those output\nby the word expansion stage; the builtins treat them as command names and arguments. For the\nmost part, the builtins use the same standard expansion rules as any other command, with a\ncouple of exceptions: the bash builtins that accept assignment statements as arguments (e.g.,\ndeclare and export) use the same expansion rules for the assignment arguments as those the\nshell uses for variable assignments. This is one place where the flags member of the WORD_DESC\nstructure is used to pass information between one stage of the shell's internal pipeline and\nanother.\n3.6.3. Simple Command Execution\nSimple commands are the ones most commonly encountered. The search for and execution of\ncommands read from the filesystem, and collection of their exit status, covers many of the shell's\nremaining features.\nShell variable assignments (i.e., words of the form var=value) are a kind of simple command\nthemselves. Assignment statements can either precede a command name or stand alone on a\ncommand line. If they precede a command, the variables are passed to the executed command in\nits environment (if they precede a built-in command or shell function, they persist, with a few\nexceptions, only as long as the builtin or function executes). If they're not followed by a command\nname, the assignment statements modify the shell's state.\nWhen presented a command name that is not the name of a shell function or builtin, bash\nsearches the filesystem for an executable file with that name. The value of the PATH variable is\n", "page": 41, "type": "text", "section": "Page 41"}
{"text": "used as a colon-separated list of directories in which to search. Command names containing\nslashes (or other directory separators) are not looked up, but are executed directly.\nWhen a command is found using a PATH search, bash saves the command name and the\ncorresponding full pathname in a hash table, which it consults before conducting subsequent\nPATH searches. If the command is not found, bash executes a specially-named function, if it's\ndefined, with the command name and arguments as arguments to the function. Some Linux\ndistributions use this facility to offer to install missing commands.\nIf bash finds a file to execute, it forks and creates a new execution environment, and executes the\nprogram in this new environment. The execution environment is an exact duplicate of the shell\nenvironment, with minor modifications to things like signal disposition and files opened and closed\nby redirections.\n3.6.4. Job Control\nThe shell can execute commands in the foreground, in which it waits for the command to finish\nand collects its exit status, or the background, where the shell immediately reads the next\ncommand. Job control is the ability to move processes (commands being executed) between the\nforeground and background, and to suspend and resume their execution. To implement this, bash\nintroduces the concept of a job, which is essentially a command being executed by one or more\nprocesses. A pipeline, for instance, uses one process for each of its elements. The process group\nis a way to join separate processes together into a single job. The terminal has a process group ID\nassociated with it, so the foreground process group is the one whose process group ID is the\nsame as the terminal's.\nThe shell uses a few simple data structures in its job control implementation. There is a structure\nto represent a child process, including its process ID, its state, and the status it returned when it\nterminated. A pipeline is just a simple linked list of these process structures. A job is quite similar:\nthere is a list of processes, some job state (running, suspended, exited, etc.), and the job's\nprocess group ID. The process list usually consists of a single process; only pipelines result in\nmore than one process being associated with a job. Each job has a unique process group ID, and\nthe process in the job whose process ID is the same as the job's process group ID is called the\nprocess group leader. The current set of jobs is kept in an array, conceptually very similar to how\nit's presented to the user. The job's state and exit status are assembled by aggregating the state\nand exit statuses of the constituent processes.\nLike several other things in the shell, the complex part about implementing job control is\nbookkeeping. The shell must take care to assign processes to the correct process groups, make\nsure that child process creation and process group assignment are synchronized, and that the\nterminal's process group is set appropriately, since the terminal's process group determines the\nforeground job (and, if it's not set back to the shell's process group, the shell itself won't be able\nto read terminal input). Since it's so process-oriented, it's not straightforward to implement\ncompound commands such as while and for loops so an entire loop can be stopped and\nstarted as a unit, and few shells have done so.\n3.6.5. Compound Commands\nCompound commands consist of lists of one or more simple commands and are introduced by a\nkeyword such as if or while. This is where the programming power of the shell is most visible\nand effective.\nThe implementation is fairly unsurprising. The parser constructs objects corresponding to the\nvarious compound commands, and interprets them by traversing the object. Each compound\ncommand is implemented by a corresponding C function that is responsible for performing the\nappropriate expansions, executing commands as specified, and altering the execution flow based\non the command's return status. The function that implements the for command is illustrative. It\nmust first expand the list of words following the in reserved word. The function must then iterate\nthrough the expanded words, assigning each word to the appropriate variable, then executing the\nlist of commands in the for command's body. The for command doesn't have to alter execution\n", "page": 42, "type": "text", "section": "Page 42"}
{"text": "based on the return status of the command, but it does have to pay attention to the effects of the\nbreak and continue builtins. Once all the words in the list have been used, the for command\nreturns. As this shows, for the most part, the implementation follows the description very closely.\n3.7. Lessons Learned\n3.7.1. What I Have Found Is Important\nI have spent over twenty years working on bash, and I'd like to think I have discovered a few\nthings. The most important\u2014one that I can't stress enough\u2014is that it's vital to have detailed\nchange logs. It's good when you can go back to your change logs and remind yourself about why\na particular change was made. It's even better when you can tie that change to a particular bug\nreport, complete with a reproducible test case, or a suggestion.\nIf it's appropriate, extensive regression testing is something I would recommend building into a\nproject from the beginning. Bash has thousands of test cases covering virtually all of its non-\ninteractive features. I have considered building tests for interactive features\u2014Posix has them in its\nconformance test suite\u2014but did not want to have to distribute the framework I judged it would\nneed.\nStandards are important. Bash has benefited from being an implementation of a standard. It's\nimportant to participate in the standardization of the software you're implementing. In addition to\ndiscussions about features and their behavior, having a standard to refer to as the arbiter can\nwork well. Of course, it can also work poorly\u2014it depends on the standard.\nExternal standards are important, but it's good to have internal standards as well. I was lucky\nenough to fall into the GNU Project's set of standards, which provide plenty of good, practical\nadvice about design and implementation.\nGood documentation is another essential. If you expect a program to be used by others, it's\nworth having comprehensive, clear documentation. If software is successful, there will end up\nbeing lots of documentation for it, and it's important that the developer writes the authoritative\nversion.\nThere's a lot of good software out there. Use what you can: for instance, gnulib has a lot of\nconvenient library functions (once you can unravel them from the gnulib framework). So do the\nBSDs and Mac OS X. Picasso said \"Great artists steal\" for a reason.\nEngage the user community, but be prepared for occasional criticism, some that will be head-\nscratching. An active user community can be a tremendous benefit, but one consequence is that\npeople will become very passionate. Don't take it personally.\n3.7.2. What I Would Have Done Differently\nBash has millions of users. I've been educated about the importance of backwards compatibility.\nIn some sense, backwards compatibility means never having to say you're sorry. The world,\nhowever, isn't quite that simple. I've had to make incompatible changes from time to time, nearly\nall of which generated some number of user complaints, though I always had what I considered to\nbe a valid reason, whether that was to correct a bad decision, to fix a design misfeature, or to\ncorrect incompatibilities between parts of the shell. I would have introduced something like formal\nbash compatibility levels earlier.\nBash's development has never been particularly open. I have become comfortable with the idea of\nmilestone releases (e.g., bash-4.2) and individually-released patches. There are reasons for doing\nthis: I accommodate vendors with longer release timelines than the free software and open source\nworlds, and I've had trouble in the past with beta software becoming more widespread than I'd\nlike. If I had to start over again, though, I would have considered more frequent releases, using\nsome kind of public repository.\nNo such list would be complete without an implementation consideration. One thing I've\n", "page": 43, "type": "text", "section": "Page 43"}
{"text": "considered multiple times, but never done, is rewriting the bash parser using straight recursive-\ndescent rather than using bison. I once thought I'd have to do this in order to make command\nsubstitution conform to Posix, but I was able to resolve that issue without changes that extensive.\nWere I starting bash from scratch, I probably would have written a parser by hand. It certainly\nwould have made some things easier.\n3.8. Conclusions\nBash is a good example of a large, complex piece of free software. It has had the benefit of more\nthan twenty years of development, and is mature and powerful. It runs nearly everywhere, and is\nused by millions of people every day, many of whom don't realize it.\nBash has been influenced by many sources, dating back to the original 7th Edition Unix shell,\nwritten by Stephen Bourne. The most significant influence is the Posix standard, which dictates a\nsignificant portion of its behavior. This combination of backwards compatibility and standards\ncompliance has brought its own challenges.\nBash has profited by being part of the GNU Project, which has provided a movement and a\nframework in which bash exists. Without GNU, there would be no bash. Bash has also benefited\nfrom its active, vibrant user community. Their feedback has helped to make bash what it is today\n\u2014a testament to the benefits of free software.\nFootnotes\n1. In most cases, a sequence of one of the characters.\n2. The exec builtin is an exception to this rule.\n", "page": 44, "type": "text", "section": "Page 44"}
{"text": "The Architecture of\nOpen Source Applications\nAmy Brown and Greg Wilson (eds.)\nISBN 978-1-257-63801-7 \nLicense / Buy / Contribute\nChapter 4. Berkeley DB\nMargo Seltzer and Keith Bostic\nConway's Law states that a design reflects the structure of the organization that produced it.\nStretching that a bit, we might anticipate that a software artifact designed and initially produced by\ntwo people might somehow reflect, not merely the structure of the organization, but the internal\nbiases and philosophies each brings to the table. One of us (Seltzer) has spent her career\nbetween the worlds of filesystems and database management systems. If questioned, she'll argue\nthe two are fundamentally the same thing, and furthermore, operating systems and database\nmanagement systems are essentially both resource managers and providers of convenient\nabstractions. The differences are \"merely\" implementation details. The other (Bostic) believes in the\ntool-based approach to software engineering and in the construction of components based on\nsimpler building blocks, because such systems are invariably superior to monolithic architectures\nin the important \"-bilities\": understandability, extensibility, maintainability, testability, and flexibility.\nWhen you combine those two perspectives, it's not surprising to learn that together we spent\nmuch of the last two decades working on Berkeley DB\u2014a software library that provides fast,\nflexible, reliable and scalable data management. Berkeley DB provides much of the same\nfunctionality that people expect from more conventional systems, such as relational databases,\nbut packages it differently. For example, Berkeley DB provides fast data access, both keyed and\nsequential, as well as transaction support and recovery from failure. However, it provides those\nfeatures in a library that links directly with the application that needs those services, rather than\nbeing made available by a standalone server application.\nIn this chapter, we'll take a deeper look at Berkeley DB and see that it is composed of a collection\nof modules, each of which embodies the Unix \"do one thing well\" philosophy. Applications that\nembed Berkeley DB can use those components directly or they can simply use them implicitly via\nthe more familiar operations to get, put, and delete data items. We'll focus on architecture\u2014how\nwe got started, what we were designing, and where we've ended up and why. Designs can (and\ncertainly will!) be forced to adapt and change\u2014what's vital is maintaining principles and a\nconsistent vision over time. We will also briefly consider the code evolution of long-term software\nprojects. Berkeley DB has over two decades of on-going development, and that inevitably takes its\ntoll on good design.\n4.1. In the Beginning\nBerkeley DB dates back to an era when the Unix operating system was proprietary to AT&T and\nthere were hundreds of utilities and libraries whose lineage had strict licensing constraints. Margo\nSeltzer was a graduate student at the University of California, Berkeley, and Keith Bostic was a\nmember of Berkeley's Computer Systems Research Group. At the time, Keith was working on\nremoving AT&T's proprietary software from the Berkeley Software Distribution.\nThe Berkeley DB project began with the modest goal of replacing the in-memory hsearch hash\npackage and the on-disk dbm/ndbm hash packages with a new and improved hash implementation\nable to operate both in-memory and on disk, as well as be freely redistributed without a\nproprietary license. The hash library that Margo Seltzer wrote [SY91] was based on Litwin's\nExtensible Linear Hashing research. It boasted a clever scheme allowing a constant time mapping\nbetween hash values and page addresses, as well as the ability to handle large data\u2014items larger\nthan the underlying hash bucket or filesystem page size, typically four to eight kilobytes.\n", "page": 45, "type": "text", "section": "Page 45"}
{"text": "If hash tables were good, then Btrees and hash tables would be better. Mike Olson, also a\ngraduate student at the University of California, Berkeley, had written a number of Btree\nimplementations, and agreed to write one more. The three of us transformed Margo's hash\nsoftware and Mike's Btree software into an access-method-agnostic API, where applications\nreference hash tables or Btrees via database handles that had handle methods to read and modify\ndata.\nBuilding on these two access methods, Mike Olson and Margo Seltzer wrote a research paper\n([SO92]) describing LIBTP, a programmatic transactional library that ran in an application's\naddress space.\nThe hash and Btree libraries were incorporated into the final 4BSD releases, under the name\nBerkeley DB 1.85. Technically, the Btree access method implements a B+link tree, however, we will\nuse the term Btree for the rest of this chapter, as that is what the access method is called.\nBerkeley DB 1.85's structure and APIs will likely be familiar to anyone who has used any Linux or\nBSD-based system.\nThe Berkeley DB 1.85 library was quiescent for a few years, until 1996 when Netscape contracted\nwith Margo Seltzer and Keith Bostic to build out the full transactional design described in the LIBTP\npaper and create a production-quality version of the software. This effort produced the first\ntransactional version of Berkeley DB, version 2.0.\nThe subsequent history of Berkeley DB is a simpler and more traditional timeline: Berkeley DB 2.0\n(1997) introduced transactions to Berkeley DB; Berkeley DB 3.0 (1999) was a re-designed version,\nadding further levels of abstraction and indirection to accommodate growing functionality.\nBerkeley DB 4.0 (2001) introduced replication and high availability, and Oracle Berkeley DB 5.0\n(2010) added SQL support.\nAt the time of writing, Berkeley DB is the most widely used database toolkit in the world, with\nhundreds of millions of deployed copies running in everything from routers and browsers to\nmailers and operating systems. Although more than twenty years old, the Berkeley DB tool-based\nand object-oriented approach has allowed it to incrementally improve and re-invent itself to match\nthe requirements of the software using it.\nDesign Lesson 1\nIt is vital for any complex software package's testing and maintenance that the software\nbe designed and built as a cooperating set of modules with well-defined API boundaries.\nThe boundaries can (and should!) shift as needs dictate, but they always need to be there.\nThe existence of those boundaries prevents the software from becoming an\nunmaintainable pile of spaghetti. Butler Lampson once said that all problems in computer\nscience can be solved by another level of indirection. More to the point, when asked what\nit meant for something to be object-oriented, Lampson said it meant being able to have\nmultiple implementations behind an API. The Berkeley DB design and implementation\nembody this approach of permitting multiple implementations behind a common interface,\nproviding an object-oriented look and feel, even though the library is written in C.\n4.2. Architectural Overview\nIn this section, we'll review the Berkeley DB library's architecture, beginning with LIBTP, and\nhighlight key aspects of its evolution.\nFigure 4.1, which is taken from Seltzer and Olson's original paper, illustrates the original LIBTP\narchitecture, while Figure 4.2 presents the Berkeley DB 2.0 designed architecture.\n", "page": 46, "type": "text", "section": "Page 46"}
{"text": "Figure 4.1: Architecture of the LIBTP Prototype System\nFigure 4.2: Intended Architecture for Berkeley DB-2.0.\nThe only significant difference between the LIBTP implementation and the Berkeley DB 2.0 design\nwas the removal of the process manager. LIBTP required that each thread of control register itself\nwith the library and then synchronized the individual threads/processes rather than providing\nsubsystem level synchronization. As is discussed in Section 4.4, that original design might have\nserved us better.\nFigure 4.3: Actual Berkeley DB 2.0.6 Architecture.\nThe difference between the design and the actual released db-2.0.6 architecture, shown in\nFigure 4.3, illustrates the reality of implementing a robust recovery manager. The recovery\nsubsystem is shown in gray. Recovery includes both the driver infrastructure, depicted in the\nrecovery box, as well as a set of recovery redo and undo routines that recover the operations\nperformed by the access methods. These are represented by the circle labelled \"access method\nrecovery routines.\" There is a consistent design to how recovery is handled in Berkeley DB 2.0 as\nopposed to hand-coded logging and recovery routines in LIBTP particular to specific access\nmethods. This general purpose design also produces a much richer interface between the various\nmodules.\n", "page": 47, "type": "text", "section": "Page 47"}
{"text": "Figure 4.4 illustrates the Berkeley DB-5.0.21 architecture. The numbers in the diagram reference\nthe APIs listed in the table in Table 4.1. Although the original architecture is still visible, the current\narchitecture shows its age with the addition of new modules, the decomposition of old modules\n(e.g., log has become log and dbreg), and a significant increase in the number of intermodule\nAPIs).\nOver a decade of evolution, dozens of commercial releases, and hundreds of new features later,\nwe see that the architecture is significantly more complex than its ancestors. The key things to\nnote are: First, replication adds an entirely new layer to the system, but it does so cleanly,\ninteracting with the rest of the system via the same APIs as does the historical code. Second, the\nlog module is split into log and dbreg (database registration). This is discussed in more detail in\nSection 4.8. Third, we have placed all inter-module calls into a namespace identified with leading\nunderscores, so that applications won't collide with our function names. We discuss this further in\nDesign Lesson 6.\nFourth, the logging subsystem's API is now cursor based (there is no log_get API; it is replaced\nby the log_cursor API). Historically, Berkeley DB never had more than one thread of control\nreading or writing the log at any instant in time, so the library had a single notion of the current\nseek pointer in the log. This was never a good abstraction, but with replication it became\nunworkable. Just as the application API supports iteration using cursors, the log now supports\niteration using cursors. Fifth, the fileop module inside of the access methods provides support\nfor transactionally protected database create, delete, and rename operations. It took us multiple\nattempts to make the implementation palatable (it is still not as clean as we would like), and after\nreworking it numerous time, we pulled it out into its own module.\nDesign Lesson 2\nA software design is simply one of several ways to force yourself to think through the\nentire problem before attempting to solve it. Skilled programmers use different techniques\nto this end: some write a first version and throw it away, some write extensive manual\npages or design documents, others fill out a code template where every requirement is\nidentified and assigned to a specific function or comment. For example, in Berkeley DB,\nwe created a complete set of Unix-style manual pages for the access methods and\nunderlying components before writing any code. Regardless of the technique used, it's\ndifficult to think clearly about program architecture after code debugging begins, not to\nmention that large architectural changes often waste previous debugging effort. Software\narchitecture requires a different mind set from debugging code, and the architecture you\nhave when you begin debugging is usually the architecture you'll deliver in that release.\n", "page": 48, "type": "text", "section": "Page 48"}
{"text": "Figure 4.4: Berkeley DB-5.0.21 Architecture\nApplication APIs\n1. DBP handle operations\n2. DB_ENV Recovery\n3. Transaction APIs\nopen\nopen(\u2026 DB_RECOVER \u2026)\nDB_ENV->txn_begin\nget\nDB_TXN->abort\nput\nDB_TXN->commit\ndel\nDB_TXN->prepare\ncursor\nAPIs Used by the Access Methods\n4. Into Lock\n5. Into Mpool\n6. Into Log\n7. Into Dbreg\n__lock_downgrade __memp_nameop\n__log_print_record\n__dbreg_setup\n__lock_vec\n__memp_fget\n__dbreg_net_id\n__lock_get\n__memp_fput\n__dbreg_revoke\n__lock_put\n__memp_fset\n__dbreg_teardown\n__memp_fsync\n__dbreg_close_id\n__memp_fopen\n__dbreg_log_id\n__memp_fclose\n__memp_ftruncate\n__memp_extend_freelist\nRecovery APIs\n8. Into Lock\n9. Into Mpool\n10. Into Log\n11. Into Dbreg\n12. Into Txn\n__lock_getlocker\n__memp_fget\n__log_compare\n__dbreg_close_files\n__txn_getckp\n__lock_get_list\n__memp_fput\n__log_open\n__dbreg_mark_restored __txn_checkpoint\n__memp_fset\n__log_earliest\n__dbreg_init_recover\n__txn_reset\n__memp_nameop\n__log_backup\n__txn_recycle_id\n__log_cursor\n__txn_findlastckp\n", "page": 49, "type": "text", "section": "Page 49"}
{"text": "__log_vtruncate\n__txn_ckp_read\nAPIs Used by the Transaction Module\n13. Into Lock\n14. Into Mpool\n15. Into Log\n16. Into Dbreg\n__lock_vec\n__memp_sync\n__log_cursor\n__dbreg_invalidate_files\n__lock_downgrade __memp_nameop\n__log_current_lsn\n__dbreg_close_files\n__dbreg_log_files\nAPI Into the Replication System\n17. From Log\n18. From Txn\n__rep_send_message\n__rep_lease_check\n__rep_bulk_message\n__rep_txn_applied\n__rep_send_message\nAPI From the Replication System\n19. Into Lock\n20. Into Mpool\n21. Into Log\n22. Into Dbreg\n23. Into Txn\n__lock_vec\n__memp_fclose\n__log_get_stable_lsn\n__dbreg_mark_restored __txn_recycle_id\n__lock_get\n__memp_fget\n__log_cursor\n__dbreg_invalidate_files __txn_begin\n__lock_id\n__memp_fput\n__log_newfile\n__dbreg_close_files\n__txn_recover\n__memp_fsync\n__log_flush\n__txn_getckp\n__log_rep_put\n__txn_updateckp\n__log_zero\n__log_vtruncate\nTable 4.1: Berkeley DB 5.0.21 APIs\nWhy architect the transactional library out of components rather than tune it to a single\nanticipated use? There are three answers to this question. First, it forces a more disciplined\ndesign. Second, without strong boundaries in the code, complex software packages inevitably\ndegenerate into unmaintainable piles of glop. Third, you can never anticipate all the ways\ncustomers will use your software; if you empower users by giving them access to software\ncomponents, they will use them in ways you never considered.\nIn subsequent sections we'll consider each component of Berkeley DB, understand what it does\nand how it fits into the larger picture.\n4.3. The Access Methods: Btree, Hash, Recno, Queue\nThe Berkeley DB access methods provide both keyed lookup of, and iteration over, variable and\nfixed-length byte strings. Btree and Hash support variable-length key/value pairs. Recno and\nQueue support record-number/value pairs (where Recno supports variable-length values and\nQueue supports only fixed-length values).\nThe main difference between Btree and Hash access methods is that Btree offers locality of\nreference for keys, while Hash does not. This implies that Btree is the right access method for\nalmost all data sets; however, the Hash access method is appropriate for data sets so large that\nnot even the Btree indexing structures fit into memory. At that point, it's better to use the\nmemory for data than for indexing structures. This trade-off made a lot more sense in 1990 when\nmain memory was typically much smaller than today.\nThe difference between Recno and Queue is that Queue supports record-level locking, at the cost\nof requiring fixed-length values. Recno supports variable-length objects, but like Btree and Hash,\nsupports only page-level locking.\n", "page": 50, "type": "text", "section": "Page 50"}
{"text": "We originally designed Berkeley DB such that the CRUD functionality (create, read, update and\ndelete) was key-based and the primary interface for applications. We subsequently added cursors\nto support iteration. That ordering led to the confusing and wasteful case of largely duplicated\ncode paths inside the library. Over time, this became unmaintainable and we converted all keyed\noperations to cursor operations (keyed operations now allocate a cached cursor, perform the\noperation, and return the cursor to the cursor pool). This is an application of one of the endlessly-\nrepeated rules of software development: don't optimize a code path in any way that detracts from\nclarity and simplicity until you know that it's necessary to do so.\nDesign Lesson 3\nSoftware architecture does not age gracefully. Software architecture degrades in direct\nproportion to the number of changes made to the software: bug fixes corrode the\nlayering and new features stress design. Deciding when the software architecture has\ndegraded sufficiently that you should re-design or re-write a module is a hard decision. On\none hand, as the architecture degrades, maintenance and development become more\ndifficult and at the end of that path is a legacy piece of software maintainable only by\nhaving an army of brute-force testers for every release, because nobody understands\nhow the software works inside. On the other hand, users will bitterly complain over the\ninstability and incompatibilities that result from fundamental changes. As a software\narchitect, your only guarantee is that someone will be angry with you no matter which\npath you choose.\nWe omit detailed discussions of the Berkeley DB access method internals; they implement fairly\nwell-known Btree and hashing algorithms (Recno is a layer on top of the Btree code, and Queue is\na file block lookup function, albeit complicated by the addition of record-level locking).\n4.4. The Library Interface Layer\nOver time, as we added additional functionality, we discovered that both applications and internal\ncode needed the same top-level functionality (for example, a table join operation uses multiple\ncursors to iterate over the rows, just as an application might use a cursor to iterate over those\nsame rows).\nDesign Lesson 4\nIt doesn't matter how you name your variables, methods, functions, or what comments\nor code style you use; that is, there are a large number of formats and styles that are\n\"good enough.\" What does matter, and matters very much, is that naming and style be\nconsistent. Skilled programmers derive a tremendous amount of information from code\nformat and object naming. You should view naming and style inconsistencies as some\nprogrammers investing time and effort to lie to the other programmers, and vice versa.\nFailing to follow house coding conventions is a firing offense.\nFor this reason, we decomposed the access method APIs into precisely defined layers. These\nlayers of interface routines perform all of the necessary generic error checking, function-specific\nerror checking, interface tracking, and other tasks such as automatic transaction management.\nWhen applications call into Berkeley DB, they call the first level of interface routines based on\nmethods in the object handles. (For example, __dbc_put_pp, is the interface call for the Berkeley\nDB cursor \"put\" method, to update a data item. The \"_pp\" is the suffix we use to identify all\nfunctions that an application can call.)\nOne of the Berkeley DB tasks performed in the interface layer is tracking what threads are running\ninside the Berkeley DB library. This is necessary because some internal Berkeley DB operations\nmay be performed only when no threads are running inside the library. Berkeley DB tracks\nthreads in the library by marking that a thread is executing inside the library at the beginning of\n", "page": 51, "type": "text", "section": "Page 51"}
{"text": "every library API and clearing that flag when the API call returns. This entry/exit checking is always\nperformed in the interface layer, as is a similar check to determine if the call is being performed in\na replicated environment.\nThe obvious question is \"why not pass a thread identifier into the library, wouldn't that be easier?\"\nThe answer is yes, it would be a great deal easier, and we surely wish we'd done just that. But,\nthat change would have modified every single Berkeley DB application, most of every application's\ncalls into Berkeley DB, and in many cases would have required application re-structuring.\nDesign Lesson 5\nSoftware architects must choose their upgrade battles carefully: users will accept minor\nchanges to upgrade to new releases (if you guarantee compile-time errors, that is,\nobvious failures until the upgrade is complete; upgrade changes should never fail in subtle\nways). But to make truly fundamental changes, you must admit it's a new code base and\nrequires a port of your user base. Obviously, new code bases and application ports are\nnot cheap in time or resources, but neither is angering your user base by telling them a\nhuge overhaul is really a minor upgrade.\nAnother task performed in the interface layer is transaction generation. The Berkeley DB library\nsupports a mode where every operation takes place in an automatically generated transaction\n(this saves the application having to create and commit its own explicit transactions). Supporting\nthis mode requires that every time an application calls through the API without specifying its own\ntransaction, a transaction is automatically created.\nFinally, all Berkeley DB APIs require argument checking. In Berkeley DB there are two flavors of\nerror checking\u2014generic checks to determine if our database has been corrupted during a\nprevious operation or if we are in the midst of a replication state change (for example, changing\nwhich replica allows writes). There are also checks specific to an API: correct flag usage, correct\nparameter usage, correct option combinations, and any other type of error we can check before\nactually performing the requested operation.\nThis API-specific checking is all encapsulated in functions suffixed with _arg. Thus, the error\nchecking specific to the cursor put method is located in the function __dbc_put_arg, which is\ncalled by the __dbc_put_pp function.\nFinally, when all the argument verification and transaction generation is complete, we call the\nworker method that actually performs the operation (in our example, it would be __dbc_put),\nwhich is the same function we use when calling the cursor put functionality internally.\nThis decomposition evolved during a period of intense activity, when we were determining\nprecisely what actions we needed to take when working in replicated environments. After iterating\nover the code base some non-trivial number of times, we pulled apart all this preamble checking to\nmake it easier to change the next time we identified a problem with it.\n4.5. The Underlying Components\nThere are four components underlying the access methods: a buffer manager, a lock manager, a\nlog manager and a transaction manager. We'll discuss each of them separately, but they all have\nsome common architectural features.\nFirst, all of the subsystems have their own APIs, and initially each subsystem had its own object\nhandle with all methods for that subsystem based on the handle. For example, you could use\nBerkeley DB's lock manager to handle your own locks or to write your own remote lock manager,\nor you could use Berkeley DB's buffer manager to handle your own file pages in shared memory.\nOver time, the subsystem-specific handles were removed from the API in order to simplify\nBerkeley DB applications. Although the subsystems are still individual components that can be\nused independently of the other subsystems, they now share a common object handle, the\n", "page": 52, "type": "text", "section": "Page 52"}
{"text": "DB_ENV \"environment\" handle. This architectural feature enforces layering and generalization. Even\nthough the layer moves from time-to-time, and there are still a few places where one subsystem\nreaches across into another subsystem, it is good discipline for programmers to think about the\nparts of the system as separate software products in their own right.\nSecond, all of the subsystems (in fact, all Berkeley DB functions) return error codes up the call\nstack. As a library, Berkeley DB cannot step on the application's name space by declaring global\nvariables, not to mention that forcing errors to return in a single path through the call stack\nenforces good programmer discipline.\nDesign Lesson 6\nIn library design, respect for the namespace is vital. Programmers who use your library\nshould not need to memorize dozens of reserved names for functions, constants,\nstructures, and global variables to avoid naming collisions between an application and the\nlibrary.\nFinally, all of the subsystems support shared memory. Because Berkeley DB supports sharing\ndatabases between multiple running processes, all interesting data structures have to live in\nshared memory. The most significant implication of this choice is that in-memory data structures\nmust use base address and offset pairs instead of pointers in order for pointer-based data\nstructures to work in the context of multiple processes. In other words, instead of indirecting\nthrough a pointer, the Berkeley DB library must create a pointer from a base address (the address\nat which the shared memory segment is mapped into memory) plus an offset (the offset of a\nparticular data structure in that mapped-in segment). To support this feature, we wrote a version\nof the Berkeley Software Distribution queue package that implemented a wide variety of linked\nlists.\nDesign Lesson 7\nBefore we wrote a shared-memory linked-list package, Berkeley DB engineers hand-coded\na variety of different data structures in shared memory, and these implementations were\nfragile and difficult to debug. The shared-memory list package, modeled after the BSD list\npackage (queue.h), replaced all of those efforts. Once it was debugged, we never had to\ndebug another shared memory linked-list problem. This illustrates three important design\nprinciples: First, if you have functionality that appears more than once, write the shared\nfunctions and use them, because the mere existence of two copies of any specific\nfunctionality in your code guarantees that one of them is incorrectly implemented.\nSecond, when you develop a set of general purpose routines, write a test suite for the set\nof routines, so you can debug them in isolation. Third, the harder code is to write, the\nmore important for it to be separately written and maintained; it's almost impossible to\nkeep surrounding code from infecting and corroding a piece of code.\n4.6. The Buffer Manager: Mpool\nThe Berkeley DB Mpool subsystem is an in-memory buffer pool of file pages, which hides the fact\nthat main memory is a limited resource, requiring the library to move database pages to and from\ndisk when handling databases larger than memory. Caching database pages in memory was what\nenabled the original hash library to significantly out-perform the historic hsearch and ndbm\nimplementations.\nAlthough the Berkeley DB Btree access method is a fairly traditional B+tree implementation,\npointers between tree nodes are represented as page numbers, not actual in-memory pointers,\nbecause the library's implementation uses the on-disk format as its in-memory format as well. The\nadvantage of this representation is that a page can be flushed from the cache without format\nconversion; the disadvantage is that traversing an index structures requires (costlier) repeated\n", "page": 53, "type": "text", "section": "Page 53"}
{"text": "buffer pool lookups rather than (cheaper) memory indirections.\nThere are other performance implications that result from the underlying assumption that the in-\nmemory representation of Berkeley DB indices is really a cache for on-disk persistent data. For\nexample, whenever Berkeley DB accesses a cached page, it first pins the page in memory. This pin\nprevents any other threads or processes from evicting it from the buffer pool. Even if an index\nstructure fits entirely in the cache and need never be flushed to disk, Berkeley DB still acquires\nand releases these pins on every access, because the underlying model provided by Mpool is that\nof a cache, not persistent storage.\n4.6.1. The Mpool File Abstraction\nMpool assumes it sits atop a filesystem, exporting the file abstraction through the API. For\nexample, DB_MPOOLFILE handles represent an on-disk file, providing methods to get/put pages\nto/from the file. While Berkeley DB supports temporary and purely in-memory databases, these\ntoo are referenced by DB_MPOOLFILE handles because of the underlying Mpool abstractions. The\nget and put methods are the primary Mpool APIs: get ensures a page is present in the cache,\nacquires a pin on the page and returns a pointer to the page. When the library is done with the\npage, the put call unpins the page, releasing it for eviction. Early versions of Berkeley DB did not\ndifferentiate between pinning a page for read access versus pinning a page for write access.\nHowever, in order to increase concurrency, we extended the Mpool API to allow callers to indicate\ntheir intention to update a page. This ability to distinguish read access from write access was\nessential to implement multi-version concurrency control. A page pinned for reading that happens\nto be dirty can be written to disk, while a page pinned for writing cannot, since it may be in an\ninconsistent state at any instant.\n4.6.2. Write-ahead Logging\nBerkeley DB uses write-ahead-logging (WAL) as its transaction mechanism to make recovery after\nfailure possible. The term write-ahead-logging defines a policy requiring log records describing any\nchange be propagated to disk before the actual data updates they describe. Berkeley DB's use of\nWAL as its transaction mechanism has important implications for Mpool, and Mpool must balance\nits design point as a generic caching mechanism with its need to support the WAL protocol.\nBerkeley DB writes log sequence numbers (LSNs) on all data pages to document the log record\ncorresponding to the most recent update to a particular page. Enforcing WAL requires that before\nMpool writes any page to disk, it must verify that the log record corresponding to the LSN on the\npage is safely on disk. The design challenge is how to provide this functionality without requiring\nthat all clients of Mpool use a page format identical to that used by Berkeley DB. Mpool addresses\nthis challenge by providing a collection of set (and get) methods to direct its behavior. The\nDB_MPOOLFILE method set_lsn_offset provides a byte offset into a page, indicating where\nMpool should look for an LSN to enforce WAL. If the method is never called, Mpool does not\nenforce the WAL protocol. Similarly, the set_clearlen method tells Mpool how many bytes of a\npage represent metadata that should be explicitly cleared when a page is created in the cache.\nThese APIs allow Mpool to provide the functionality necessary to support Berkeley DB's\ntransactional requirements, without forcing all users of Mpool to do so.\nDesign Lesson 8\nWrite-ahead logging is another example of providing encapsulation and layering, even\nwhen the functionality is never going to be useful to another piece of software: after all,\nhow many programs care about LSNs in the cache? Regardless, the discipline is useful\nand makes the software easier to maintain, test, debug and extend.\n4.7. The Lock Manager: Lock\nLike Mpool, the lock manager was designed as a general-purpose component: a hierarchical lock\n", "page": 54, "type": "text", "section": "Page 54"}
{"text": "manager (see [GLPT76]), designed to support a hierarchy of objects that can be locked (such as\nindividual data items), the page on which a data item lives, the file in which a data item lives, or\neven a collection of files. As we describe the features of the lock manager, we'll also explain how\nBerkeley DB uses them. However, as with Mpool, it's important to remember that other\napplications can use the lock manager in completely different ways, and that's OK\u2014it was\ndesigned to be flexible and support many different uses.\nThe lock manager has three key abstractions: a \"locker\" that identifies on whose behalf a lock is\nbeing acquired, a \"lock_object\" that identifies the item being locked, and a \"conflict matrix\".\nLockers are 32-bit unsigned integers. Berkeley DB divides this 32-bit name space into\ntransactional and non-transactional lockers (although that distinction is transparent to the lock\nmanager). When Berkeley DB uses the lock manager, it assigns locker IDs in the range 0 to\n0x7fffffff to non-transactional lockers and the range 0x80000000 to 0xffffffff to transactions. For\nexample, when an application opens a database, Berkeley DB acquires a long-term read lock on\nthat database to ensure no other thread of control removes or renames it while it is in-use. As this\nis a long-term lock, it does not belong to any transaction and the locker holding this lock is non-\ntransactional.\nAny application using the lock manager needs to assign locker ids, so the lock manager API\nprovides both DB_ENV->lock_id and DB_ENV->lock_id_free calls to allocate and deallocate\nlockers. So applications need not implement their own locker ID allocator, although they certainly\ncan.\n4.7.1. Lock Objects\nLock objects are arbitrarily long opaque byte-strings that represent the objects being locked.\nWhen two different lockers want to lock a particular object, they use the same opaque byte string\nto reference that object. That is, it is the application's responsibility to agree on conventions for\ndescribing objects in terms of opaque byte strings.\nFor example, Berkeley DB uses a DB_LOCK_ILOCK structure to describe its database locks. This\nstructure contains three fields: a file identifier, a page number, and a type.\nIn almost all cases, Berkeley DB needs to describe only the particular file and page it wants to lock.\nBerkeley DB assigns a unique 32-bit number to each database at create time, writes it into the\ndatabase's metadata page, and then uses it as the database's unique identifier in the Mpool,\nlocking, and logging subsystems. This is the fileid to which we refer in the DB_LOCK_ILOCK\nstructure. Not surprisingly, the page number indicates which page of the particular database we\nwish to lock. When we reference page locks, we set the type field of the structure to\nDB_PAGE_LOCK. However, we can also lock other types of objects as necessary. As mentioned\nearlier, we sometimes lock a database handle, which requires a DB_HANDLE_LOCK type. The\nDB_RECORD_LOCK type lets us perform record level locking in the queue access method, and the\nDB_DATABASE_LOCK type lets us lock an entire database.\nDesign Lesson 9\nBerkeley DB's choice to use page-level locking was made for good reasons, but we've\nfound that choice to be problematic at times. Page-level locking limits the concurrency of\nthe application as one thread of control modifying a record on a database page will\nprevent other threads of control from modifying other records on the same page, while\nrecord-level locks permit such concurrency as long as the two threads of control are not\nmodifying the same record. Page-level locking enhances stability as it limits the number of\nrecovery paths that are possible (a page is always in one of a couple of states during\nrecovery, as opposed to the infinite number of possible states a page might be in if\nmultiple records are being added and deleted to a page). As Berkeley DB was intended for\nuse as an embedded system where no database administrator would be available to fix\nthings should there be corruption, we chose stability over increased concurrency.\n", "page": 55, "type": "text", "section": "Page 55"}
{"text": "4.7.2. The Conflict Matrix\nThe last abstraction of the locking subsystem we'll discuss is the conflict matrix. A conflict matrix\ndefines the different types of locks present in the system and how they interact. Let's call the\nentity holding a lock, the holder and the entity requesting a lock the requester, and let's also\nassume that the holder and requester have different locker ids. The conflict matrix is an array\nindexed by [requester][holder], where each entry contains a zero if there is no conflict,\nindicating that the requested lock can be granted, and a one if there is a conflict, indicating that\nthe request cannot be granted.\nThe lock manager contains a default conflict matrix, which happens to be exactly what Berkeley\nDB needs, however, an application is free to design its own lock modes and conflict matrix to suit\nits own purposes. The only requirement on the conflict matrix is that it is square (it has the same\nnumber of rows and columns) and that the application use 0-based sequential integers to describe\nits lock modes (e.g., read, write, etc.). Table 4.2 shows the Berkeley DB conflict matrix.\nHolder\nRequester No-Lock Read Write Wait iWrite iRead iRW uRead wasWrite\nNo-Lock\nRead\n\u2713\n\u2713\n\u2713\n\u2713\nWrite\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nWait\niWrite\n\u2713\n\u2713\n\u2713\n\u2713\niRead\n\u2713\n\u2713\niRW\n\u2713\n\u2713\n\u2713\n\u2713\nuRead\n\u2713\n\u2713\n\u2713\niwasWrite\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nTable 4.2: Read-Writer Conflict Matrix.\n4.7.3. Supporting Hierarchical Locking\nBefore explaining the different lock modes in the Berkeley DB conflict matrix, let's talk about how\nthe locking subsystem supports hierarchical locking. Hierarchical locking is the ability to lock\ndifferent items within a containment hierarchy. For example, files contain pages, while pages\ncontain individual elements. When modifying a single page element in a hierarchical locking system,\nwe want to lock just that element; if we were modifying every element on the page, it would be\nmore efficient to simply lock the page, and if we were modifying every page in a file, it would be\nbest to lock the entire file. Additionally, hierarchical locking must understand the hierarchy of the\ncontainers because locking a page also says something about locking the file: you cannot modify\nthe file that contains a page at the same time that pages in the file are being modified.\nThe question then is how to allow different lockers to lock at different hierarchical levels without\nchaos resulting. The answer lies in a construct called an intention lock. A locker acquires an\nintention lock on a container to indicate the intention to lock things within that container. So,\nobtaining a read-lock on a page implies obtaining an intention-to-read lock on the file. Similarly, to\nwrite a single page element, you must acquire an intention-to-write lock on both the page and the\nfile. In the conflict matrix above, the iRead, iWrite, and iWR locks are all intention locks that\nindicate an intention to read, write or do both, respectively.\nTherefore, when performing hierarchical locking, rather than requesting a single lock on\nsomething, it is necessary to request potentially many locks: the lock on the actual entity as well\nas intention locks on any containing entities. This need leads to the Berkeley DB DB_ENV-\n>lock_vec interface, which takes an array of lock requests and grants them (or rejects them),\natomically.\nAlthough Berkeley DB doesn't use hierarchical locking internally, it takes advantage of the ability to\n", "page": 56, "type": "text", "section": "Page 56"}
{"text": "specify different conflict matrices, and the ability to specify multiple lock requests at once. We use\nthe default conflict matrix when providing transactional support, but a different conflict matrix to\nprovide simple concurrent access without transaction and recovery support. We use DB_ENV-\n>lock_vec to perform lock coupling, a technique that enhances the concurrency of Btree\ntraversals [Com79]. In lock coupling, you hold one lock only long enough to acquire the next lock.\nThat is, you lock an internal Btree page only long enough to read the information that allows you\nto select and lock a page at the next level.\nDesign Lesson 10\nBerkeley DB's general-purpose design was well rewarded when we added concurrent data\nstore functionality. Initially Berkeley DB provided only two modes of operation: either you\nran without any write concurrency or with full transaction support. Transaction support\ncarries a certain degree of complexity for the developer and we found some applications\nwanted improved concurrency without the overhead of full transactional support. To\nprovide this feature, we added support for API-level locking that allows concurrency, while\nguaranteeing no deadlocks. This required a new and different lock mode to work in the\npresence of cursors. Rather than adding special purpose code to the lock manager, we\nwere able to create an alternate lock matrix that supported only the lock modes\nnecessary for the API-level locking. Thus, simply by configuring the lock manager\ndifferently, we were able provide the locking support we needed. (Sadly, it was not as easy\nto change the access methods; there are still significant parts of the access method code\nto handle this special mode of concurrent access.)\n4.8. The Log Manager: Log\nThe log manager provides the abstraction of a structured, append-only file. As with the other\nmodules, we intended to design a general-purpose logging facility, however the logging subsystem\nis probably the module where we were least successful.\nDesign Lesson 11\nWhen you find an architectural problem you don't want to fix \"right now\" and that you're\ninclined to just let go, remember that being nibbled to death by ducks will kill you just as\nsurely as being trampled by elephants. Don't be too hesitant to change entire frameworks\nto improve software structure, and when you make the changes, don't make a partial\nchange with the idea that you'll clean up later\u2014do it all and then move forward. As has\nbeen often repeated, \"If you don't have the time to do it right now, you won't find the time\nto do it later.\" And while you're changing the framework, write the test structure as well.\nA log is conceptually quite simple: it takes opaque byte strings and writes them sequentially to a\nfile, assigning each a unique identifier, called a log sequence number (LSN). Additionally, the log\nmust provide efficient forward and backward traversal and retrieval by LSN. There are two tricky\nparts: first, the log must guarantee it is in a consistent state after any possible failure (where\nconsistent means it contains a contiguous sequence of uncorrupted log records); second,\nbecause log records must be written to stable storage for transactions to commit, the\nperformance of the log is usually what bounds the performance of any transactional application.\nAs the log is an append-only data structure, it can grow without bound. We implement the log as a\ncollection of sequentially numbered files, so log space may be reclaimed by simply removing old\nlog files. Given the multi-file architecture of the log, we form LSNs as pairs specifying a file number\nand offset within the file. Thus, given an LSN, it is trivial for the log manager to locate the record: it\nseeks to the given offset of the given log file and returns the record written at that location. But\nhow does the log manager know how many bytes to return from that location?\n4.8.1. Log Record Formatting\n", "page": 57, "type": "text", "section": "Page 57"}
{"text": "The log must persist per-record metadata so that, given an LSN, the log manager can determine\nthe size of the record to return. At a minimum, it needs to know the length of the record. We\nprepend every log record with a log record header containing the record's length, the offset of the\nprevious record (to facilitate backward traversal), and a checksum for the log record (to identify\nlog corruption and the end of the log file). This metadata is sufficient for the log manager to\nmaintain the sequence of log records, but it is not sufficient to actually implement recovery; that\nfunctionality is encoded in the contents of log records and in how Berkeley DB uses those log\nrecords.\nBerkeley DB uses the log manager to write before- and after-images of data before updating items\nin the database [HR83]. These log records contain enough information to either redo or undo\noperations on the database. Berkeley DB then uses the log both for transaction abort (that is,\nundoing any effects of a transaction when the transaction is discarded) and recovery after\napplication or system failure.\nIn addition to APIs to read and write log records, the log manager provides an API to force log\nrecords to disk (DB_ENV->log_flush). This allows Berkeley DB to implement write-ahead logging\n\u2014before evicting a page from Mpool, Berkeley DB examines the LSN on the page and asks the log\nmanager to guarantee that the specified LSN is on stable storage. Only then does Mpool write the\npage to disk.\nDesign Lesson 12\nMpool and Log use internal handle methods to facilitate write-ahead logging, and in some\ncases, the method declaration is longer than the code it runs, since the code is often\ncomparing two integral values and nothing more. Why bother with such insignificant\nmethods, just to maintain consistent layering? Because if your code is not so object-\noriented as to make your teeth hurt, it is not object-oriented enough. Every piece of code\nshould do a small number of things and there should be a high-level design encouraging\nprogrammers to build functionality out of smaller chunks of functionality, and so on. If\nthere's anything we have learned about software development in the past few decades, it\nis that our ability to build and maintain significant pieces of software is fragile. Building and\nmaintaining significant pieces of software is difficult and error-prone, and as the software\narchitect, you must do everything that you can, as early as you can, as often as you can,\nto maximize the information conveyed in the structure of your software.\nBerkeley DB imposes structure on the log records to facilitate recovery. Most Berkeley DB log\nrecords describe transactional updates. Thus, most log records correspond to page modifications\nto a database, performed on behalf of a transaction. This description provides the basis for\nidentifying what metadata Berkeley DB must attach to each log record: a database, a transaction,\nand a record type. The transaction identifier and record type fields are present in every record at\nthe same location. This allows the recovery system to extract a record type and dispatch the\nrecord to an appropriate handler that can interpret the record and perform appropriate actions.\nThe transaction identifier lets the recovery process identify the transaction to which a log record\nbelongs, so that during the various stages of recovery, it knows whether the record can be\nignored or must be processed.\n4.8.2. Breaking the Abstraction\nThere are also a few \"special\" log records. Checkpoint records are, perhaps, the most familiar of\nthose special records. Checkpointing is the process of making the on-disk state of the database\nconsistent as of some point in time. In other words, Berkeley DB aggressively caches database\npages in Mpool for performance. However, those pages must eventually get written to disk and\nthe sooner we do so, the more quickly we will be able to recover in the case of application or\nsystem failure. This implies a trade-off between the frequency of checkpointing and the length of\nrecovery: the more frequently a system takes checkpoints, the more quickly it will be able to\nrecover. Checkpointing is a transaction function, so we'll describe the details of checkpointing in\n", "page": 58, "type": "text", "section": "Page 58"}
{"text": "the next section. For the purposes of this section, we'll talk about checkpoint records and how the\nlog manager struggles between being a stand-alone module and a special-purpose Berkeley DB\ncomponent.\nIn general, the log manager, itself, has no notion of record types, so in theory, it should not\ndistinguish between checkpoint records and other records\u2014they are simply opaque byte strings\nthat the log manager writes to disk. In practice, the log maintains metadata revealing that it does\nunderstand the contents of some records. For example, during log startup, the log manager\nexamines all the log files it can find to identify the most recently written log file. It assumes that all\nlog files prior to that one are complete and intact, and then sets out to examine the most recent\nlog file and determine how much of it contains valid log records. It reads from the beginning of a\nlog file, stopping if/when it encounters a log record header that does not checksum properly,\nwhich indicates either the end of the log or the beginning of log file corruption. In either case, it\ndetermines the logical end of log.\nDuring this process of reading the log to find the current end, the log manager extracts the\nBerkeley DB record type, looking for checkpoint records. It retains the position of the last\ncheckpoint record it finds in log manager metadata as a \"favor\" to the transaction system. That is,\nthe transaction system needs to find the last checkpoint, but rather than having both the log\nmanager and transaction manager read the entire log file to do so, the transaction manager\ndelegates that task to the log manager. This is a classic example of violating abstraction\nboundaries in exchange for performance.\nWhat are the implications of this tradeoff? Imagine that a system other than Berkeley DB is using\nthe log manager. If it happens to write the value corresponding to the checkpoint record type in\nthe same position that Berkeley DB places its record type, then the log manager will identify that\nrecord as a checkpoint record. However, unless the application asks the log manager for that\ninformation (by directly accessing cached_ckp_lsn field in the log metadata), this information\nnever affects anything. In short, this is either a harmful layering violation or a savvy performance\noptimization.\nFile management is another place where the separation between the log manager and Berkeley DB\nis fuzzy. As mentioned earlier, most Berkeley DB log records have to identify a database. Each log\nrecord could contain the full filename of the database, but that would be expensive in terms of log\nspace, and clumsy, because recovery would have to map that name to some sort of handle it\ncould use to access the database (either a file descriptor or a database handle). Instead, Berkeley\nDB identifies databases in the log by an integer identifier, called a log file id, and implements a set\nof functions, called dbreg (for \"database registration\"), to maintain mappings between filenames\nand log file ids. The persistent version of this mapping (with the record type DBREG_REGISTER) is\nwritten to log records when the database is opened. However, we also need in-memory\nrepresentations of this mapping to facilitate transaction abort and recovery. What subsystem\nshould be responsible for maintaining this mapping?\nIn theory, the file to log-file-id mapping is a high-level Berkeley DB function; it does not belong to\nany of the subsystems, which were intended to be ignorant of the larger picture. In the original\ndesign, this information was left in the logging subsystems data structures because the logging\nsystem seemed like the best choice. However, after repeatedly finding and fixing bugs in the\nimplementation, the mapping support was pulled out of the logging subsystem code and into its\nown small subsystem with its own object-oriented interfaces and private data structures. (In\nretrospect, this information should logically have been placed with the Berkeley DB environment\ninformation itself, outside of any subsystem.)\nDesign Lesson 13\nThere is rarely such thing as an unimportant bug. Sure, there's a typo now and then, but\nusually a bug implies somebody didn't fully understand what they were doing and\nimplemented the wrong thing. When you fix a bug, don't look for the symptom: look for\nthe underlying cause, the misunderstanding, if you will, because that leads to a better\nunderstanding of the program's architecture as well as revealing fundamental underlying\n", "page": 59, "type": "text", "section": "Page 59"}
{"text": "flaws in the design itself.\n4.9. The Transaction Manager: Txn\nOur last module is the transaction manager, which ties together the individual components to\nprovide the transactional ACID properties of atomicity, consistency, isolation, and durability. The\ntransaction manager is responsible for beginning and completing (either committing or aborting)\ntransactions, coordinating the log and buffer managers to take transaction checkpoints, and\norchestrating recovery. We'll visit each of these areas in order.\nJim Gray invented the ACID acronym to describe the key properties that transactions provide\n[Gra81]. Atomicity means that all the operations performed within a transaction appear in the\ndatabase in a single unit\u2014they either are all present in the database or all absent. Consistency\nmeans that a transaction moves the database from one logically consistent state to another. For\nexample, if the application specifies that all employees must be assigned to a department that is\ndescribed in the database, then the consistency property enforces that (with properly written\ntransactions). Isolation means that from the perspective of a transaction, it appears that the\ntransaction is running sequentially without any concurrent transactions running. Finally, durability\nmeans that once a transaction is committed, it stays committed\u2014no failure can cause a\ncommitted transaction to disappear.\nThe transaction subsystem enforces the ACID properties, with the assistance of the other\nsubsystems. It uses traditional transaction begin, commit, and abort operations to delimit the\nbeginning and ending points of a transaction. It also provides a prepare call, which facilitates two\nphase commit, a technique for providing transactional properties across distributed transactions,\nwhich are not discussed in this chapter. Transaction begin allocates a new transaction identifier\nand returns a transaction handle, DB_TXN, to the application. Transaction commit writes a commit\nlog record and then forces the log to disk (unless the application indicates that it is willing to\nforego durability in exchange for faster commit processing), ensuring that even in the presence of\nfailure, the transaction will be committed. Transaction abort reads backwards through the log\nrecords belonging to the designated transaction, undoing each operation that the transaction had\ndone, returning the database to its pre-transaction state.\n4.9.1. Checkpoint Processing\nThe transaction manager is also responsible for taking checkpoints. There are a number of\ndifferent techniques in the literature for taking checkpoints [HR83]. Berkeley DB uses a variant of\nfuzzy checkpointing. Fundamentally, checkpointing involves writing buffers from Mpool to disk.\nThis is a potentially expensive operation, and it's important that the system continues to process\nnew transactions while doing so, to avoid long service disruptions. At the beginning of a\ncheckpoint, Berkeley DB examines the set of currently active transactions to find the lowest LSN\nwritten by any of them. This LSN becomes the checkpoint LSN. The transaction manager then\nasks Mpool to flush its dirty buffers to disk; writing those buffers might trigger log flush\noperations. After all the buffers are safely on disk, the transaction manager then writes a\ncheckpoint record containing the checkpoint LSN. This record states that all the operations\ndescribed by log records before the checkpoint LSN are now safely on disk. Therefore, log\nrecords prior to the checkpoint LSN are no longer necessary for recovery. This has two\nimplications: First, the system can reclaim any log files prior to the checkpoint LSN. Second,\nrecovery need only process records after the checkpoint LSN, because the updates described by\nrecords prior to the checkpoint LSN are reflected in the on-disk state.\nNote that there may be many log records between the checkpoint LSN and the actual checkpoint\nrecord. That's fine, since those records describe operations that logically happened after the\ncheckpoint and that may need to be recovered if the system fails.\n4.9.2. Recovery\nThe last piece of the transactional puzzle is recovery. The goal of recovery is to move the on-disk\n", "page": 60, "type": "text", "section": "Page 60"}
{"text": "database from a potentially inconsistent state to a consistent state. Berkeley DB uses a fairly\nconventional two-pass scheme that corresponds loosely to \"relative to the last checkpoint LSN,\nundo any transactions that never committed and redo any transactions that did commit.\" The\ndetails are a bit more involved.\nBerkeley DB needs to reconstruct its mapping between log file ids and actual databases so that it\ncan redo and undo operations on the databases. The log contains a full history of\nDBREG_REGISTER log records, but since databases stay open for a long time and we do not want\nto require that log files persist for the entire duration a database is open, we'd like a more efficient\nway to access this mapping. Prior to writing a checkpoint record, the transaction manager writes\na collection of DBREG_REGISTER records describing the current mapping from log file ids to\ndatabases. During recovery, Berkeley DB uses these log records to reconstruct the file mapping.\nWhen recovery begins, the transaction manager probes the log manager's cached_ckp_lsn\nvalue to determine the location of the last checkpoint record in the log. This record contains the\ncheckpoint LSN. Berkeley DB needs to recover from that checkpoint LSN, but in order to do so, it\nneeds to reconstruct the log file id mapping that existed at the checkpoint LSN; this information\nappears in the checkpoint prior to the checkpoint LSN. Therefore, Berkeley DB must look for the\nlast checkpoint record that occurs before the checkpoint LSN. Checkpoint records contain, not\nonly the checkpoint LSN, but the LSN of the previous checkpoint to facilitate this process.\nRecovery begins at the most recent checkpoint and using the prev_lsn field in each checkpoint\nrecord, traverses checkpoint records backwards through the log until it finds a checkpoint record\nappearing before the checkpoint LSN. Algorithmically:\nckp_record = read (cached_ckp_lsn)\nckp_lsn = ckp_record.checkpoint_lsn\ncur_lsn = ckp_record.my_lsn\nwhile (cur_lsn > ckp_lsn) {\n    ckp_record = read (ckp_record.prev_ckp)\n    cur_lsn = ckp_record.my_lsn\n}\nStarting with the checkpoint selected by the previous algorithm, recovery reads sequentially until\nthe end of the log to reconstruct the log file id mappings. When it reaches the end of the log, its\nmappings should correspond exactly to the mappings that existed when the system stopped.\nAlso during this pass, recovery keeps track of any transaction commit records encountered,\nrecording their transaction identifiers. Any transaction for which log records appear, but whose\ntransaction identifier does not appear in a transaction commit record, was either aborted or never\ncompleted and should be treated as aborted. When recovery reaches the end of the log, it\nreverses direction and begins reading backwards through the log. For each transactional log\nrecord encountered, it extracts the transaction identifier and consults the list of transactions that\nhave committed, to determine if this record should be undone. If it finds that the transaction\nidentifier does not belong to a committed transaction, it extracts the record type and calls a\nrecovery routine for that log record, directing it to undo the operation described. If the record\nbelongs to a committed transaction, recovery ignores it on the backwards pass. This backward\npass continues all the way back to the checkpoint LSN1. Finally, recovery reads the log one last\ntime in the forward direction, this time redoing any log records belonging to committed\ntransactions. When this final pass completes, recovery takes a checkpoint. At this point, the\ndatabase is fully consistent and ready to begin running the application.\nThus, recovery can be summarized as:\n1. Find the checkpoint prior to the checkpoint LSN in the most recent checkpoint\n2. Read forward to restore log file id mappings and construct a list of committed transactions\n3. Read backward to the checkpoint LSN, undoing all operations for uncommitted transactions\n4. Read forward, redoing all operations for committed transactions\n5. Checkpoint\nIn theory, the final checkpoint is unnecessary. In practice, it bounds the time for future recoveries\nand leaves the database in a consistent state.\n", "page": 61, "type": "text", "section": "Page 61"}
{"text": "Design Lesson 14\nDatabase recovery is a complex topic, difficult to write and harder to debug because\nrecovery simply shouldn't happen all that often. In his Turing Award Lecture, Edsger\nDijkstra argued that programming was inherently difficult and the beginning of wisdom is\nto admit we are unequal to the task. Our goal as architects and programmers is to use\nthe tools at our disposal: design, problem decomposition, review, testing, naming and\nstyle conventions, and other good habits, to constrain programming problems to\nproblems we can solve.\n4.10. Wrapping Up\nBerkeley DB is now over twenty years old. It was arguably the first general-purpose transactional\nkey/value store and is the grandfather of the NoSQL movement. Berkeley DB continues as the\nunderlying storage system for hundreds of commercial products and thousands of Open Source\napplications (including SQL, XML and NoSQL engines) and has millions of deployments across the\nglobe. The lessons we've learned over the course of its development and maintenance are\nencapsulated in the code and summarized in the design tips outlined above. We offer them in the\nhope that other software designers and architects will find them useful.\nFootnotes\n1. Note that we only need to go backwards to the checkpoint LSN, not the checkpoint record\npreceding it.\n", "page": 62, "type": "text", "section": "Page 62"}
{"text": "The Architecture of\nOpen Source Applications\nAmy Brown and Greg Wilson (eds.)\nISBN 978-1-257-63801-7 \nLicense / Buy / Contribute\nChapter 5. CMake\nBill Hoffman and Kenneth Martin\nIn 1999 the National Library of Medicine engaged a small company called Kitware to develop a\nbetter way to configure, build, and deploy complex software across many different platforms. This\nwork was part of the Insight Segmentation and Registration Toolkit, or ITK1. Kitware, the\nengineering lead on the project, was tasked with developing a build system that the ITK\nresearchers and developers could use. The system had to be easy to use, and allow for the most\nproductive use of the researchers' programming time. Out of this directive emerged CMake as a\nreplacement for the aging autoconf/libtool approach to building software. It was designed to\naddress the weaknesses of existing tools while maintaining their strengths.\nIn addition to a build system, over the years CMake has evolved into a family of development tools:\nCMake, CTest, CPack, and CDash. CMake is the build tool responsible for building software. CTest\nis a test driver tool, used to run regression tests. CPack is a packaging tool used to create\nplatform-specific installers for software built with CMake. CDash is a web application for displaying\ntesting results and performing continuous integration testing.\n5.1. CMake History and Requirements\nWhen CMake was being developed, the normal practice for a project was to have a configure\nscript and Makefiles for Unix platforms, and Visual Studio project files for Windows. This duality of\nbuild systems made cross-platform development very tedious for many projects: the simple act of\nadding a new source file to a project was painful. The obvious goal for developers was to have a\nsingle unified build system. The developers of CMake had experience with two approaches of\nsolving the unified build system problem.\nOne approach was the VTK build system of 1999. That system consisted of a configure script for\nUnix and an executable called pcmaker for Windows. pcmaker was a C program that read in Unix\nMakefiles and created NMake files for Windows. The binary executable for pcmaker was checked\ninto the VTK CVS system repository. Several common cases, like adding a new library, required\nchanging that source and checking in a new binary. Although this was a unified system in some\nsense, it had many shortcomings.\nThe other approach the developers had experience with was a gmake based build system for\nTargetJr. TargetJr was a C++ computer vision environment originally developed on Sun\nworkstations. Originally TargetJr used the imake system to create Makefiles. However, at some\npoint, when a Windows port was needed, the gmake system was created. Both Unix compilers\nand Windows compilers could be used with this gmake-based system. The system required\nseveral environment variables to be set prior to running gmake. Failure to have the correct\nenvironment caused the system to fail in ways that were difficult to debug, especially for end\nusers.\nBoth of these systems suffered from a serious flaw: they forced Windows developers to use the\ncommand line. Experienced Windows developers prefer to use integrated development\nenvironments (IDEs). This would encourage Windows developers to create IDE files by hand and\ncontribute them to the project, creating the dual build system again. In addition to the lack of IDE\nsupport, both of the systems described above made it extremely difficult to combine software\nprojects. For example, VTK had very few modules for reading images mostly because the build\n", "page": 63, "type": "text", "section": "Page 63"}
{"text": "system made it very difficult to use libraries like libtiff and libjpeg.\nIt was decided that a new build system would be developed for ITK and C++ in general. The basic\nconstraints of the new build system would be as follows:\nDepend only on a C++ compiler being installed on the system.\nIt must be able to generate Visual Studio IDE input files.\nIt must be easy to create the basic build system targets, including static libraries, shared\nlibraries, executables, and plugins.\nIt must be able to run build time code generators.\nIt must support separate build trees from the source tree.\nIt must be able to perform system introspection, i.e., be able to determine automatically what\nthe target system could and could not do.\nIt must do dependency scanning of C/C++ header files automatically.\nAll features would need to work consistently and equally well on all supported platforms.\nIn order to avoid depending on any additional libraries and parsers, CMake was designed with only\none major dependency, the C++ compiler (which we can safely assume we have if we're building\nC++ code). At the time, building and installing scripting languages like Tcl was difficult on many\npopular UNIX and Windows systems. It can still be an issue today on modern supercomputers\nand secured computers with no Internet connection, so it can still be difficult to build third-party\nlibraries. Since the build system is such a basic requirement for a package, it was decided that no\nadditional dependencies would be introduced into CMake. This did limit CMake to creating its own\nsimple language, which is a choice that still causes some people to dislike CMake. However, at the\ntime the most popular embedded language was Tcl. If CMake had been a Tcl-based build system, it\nis unlikely that it would have gained the popularity that it enjoys today.\nThe ability to generate IDE project files is a strong selling point for CMake, but it also limits CMake\nto providing only the features that the IDE can support natively. However, the benefits of\nproviding native IDE build files outweigh the limitations. Although this decision made the\ndevelopment of CMake more difficult, it made the development of ITK and other projects using\nCMake much easier. Developers are happier and more productive when using the tools they are\nmost familiar with. By allowing developers to use their preferred tools, projects can take best\nadvantage of their most important resource: the developer.\nAll C/C++ programs require one or more of the following fundamental building blocks of software:\nexecutables, static libraries, shared libraries, and plugins. CMake had to provide the ability to\ncreate these products on all supported platforms. Although all platforms support the creation of\nthose products, the compiler flags used to create them vary greatly from compiler to compiler and\nplatform to platform. By hiding the complexity and platform differences behind a simple command\nin CMake, developers are able to create them on Windows, Unix and Mac. This ability allows\ndevelopers to focus on the project rather than on the details of how to build a shared library.\nCode generators provide added complexity to a build system. From the start, VTK provided a\nsystem that automatically wrapped the C++ code into Tcl, Python, and Java by parsing the C++\nheader files, and automatically generating a wrapping layer. This requires a build system that can\nbuild a C/C++ executable (the wrapper generator), then run that executable at build time to create\nmore C/C++ source code (the wrappers for the particular modules). That generated source code\nmust then be compiled into executables or shared libraries. All of this has to happen within the IDE\nenvironments and the generated Makefiles.\nWhen developing flexible cross-platform C/C++ software, it is important to program to the\nfeatures of the system, and not to the specific system. Autotools has a model for doing system\nintrospection which involves compiling small snippets of code, inspecting and storing the results of\nthat compile. Since CMake was meant to be cross-platform it adopted a similar system\nintrospection technique. This allows developers to program to the canonical system instead of to\nspecific systems. This is important to make future portability possible, as compilers and operating\nsystems change over time. For example, code like this:\n#ifdef linux\n", "page": 64, "type": "text", "section": "Page 64"}
{"text": "// do some linux stuff\n#endif\nIs more brittle than code like this:\n#ifdef HAS_FEATURE\n// do something with a feature\n#endif\nAnother early CMake requirement also came from autotools: the ability to create build trees that\nare separate from the source tree. This allows for multiple build types to be performed on the\nsame source tree. It also prevents the source tree from being cluttered with build files, which\noften confuses version control systems.\nOne of the most important features of a build system is the ability to manage dependencies. If a\nsource file is changed, then all products using that source file must be rebuilt. For C/C++ code,\nthe header files included by a .c or .cpp file must also be checked as part of the dependencies.\nTracking down issues where only some of the code that should be compiled actually gets compiled\nas a result of incorrect dependency information can be time consuming.\nAll of the requirements and features of the new build system had to work equally well on all\nsupported platforms. CMake needed to provide a simple API for developers to create complicated\nsoftware systems without having to understand platform details. In effect, software using CMake\nis outsourcing the build complications to the CMake team. Once the vision for the build tool was\ncreated with the basic set of requirements, implementation needed to proceed in an agile way. ITK\nneeded a build system almost from day one. The first versions of CMake did not meet all of the\nrequirements set out in the vision, but they were able to build on Windows and Unix.\n5.2. How CMake Is Implemented\nAs mentioned, CMake's development languages are C and C++. To explain its internals this section\nwill first describe the CMake process from a user's point of view, then examine its structures.\n5.2.1. The CMake Process\nCMake has two main phases. The first is the \"configure\" step, in which CMake processes all the\ninput given to it and creates an internal representation of the build to be performed. Then next\nphase is the \"generate\" step. In this phase the actual build files are created.\nEnvironment Variables (or Not)\nIn many build systems in 1999, and even today, shell level environment variables are used during\nthe build of a project. It is typical that a project has a PROJECT_ROOT environment variable that\npoints to the location of the root of the source tree. Environment variables are also used to point\nto optional or external packages. The trouble with this approach is that for the build to work, all of\nthese external variables need to be set each time a build is performed. To solve this problem\nCMake has a cache file that stores all of the variables required for a build in one place. These are\nnot shell or environment variables, but CMake variables. The first time CMake is run for a particular\nbuild tree, it creates a CMakeCache.txt file which stores all the persistent variables for that build.\nSince the file is part of the build tree, the variables will always be available to CMake during each\nrun.\nThe Configure Step\nDuring the configure step, CMake first reads the CMakeCache.txt if it exists from a prior run. It\nthen reads CMakeLists.txt, found in the root of the source tree given to CMake. During the\nconfigure step, the CMakeLists.txt files are parsed by the CMake language parser. Each of the\nCMake commands found in the file is executed by a command pattern object. Additional\nCMakeLists.txt files can be parsed during this step by the include and add_subdirectory\n", "page": 65, "type": "text", "section": "Page 65"}
{"text": "CMake commands. CMake has a C++ object for each of the commands that can be used in the\nCMake language. Some examples of commands are add_library, if, add_executable,\nadd_subdirectory, and include. In effect, the entire language of CMake is implemented as calls\nto commands. The parser simply converts the CMake input files into command calls and lists of\nstrings that are arguments to commands.\nThe configure step essentially \"runs\" the user-provided CMake code. After all of the code is\nexecuted, and all cache variable values have been computed, CMake has an in-memory\nrepresentation of the project to be built. This will include all of the libraries, executables, custom\ncommands, and all other information required to create the final build files for the selected\ngenerator. At this point, the CMakeCache.txt file is saved to disk for use in future runs of CMake.\nThe in-memory representation of the project is a collection of targets, which are simply things that\nmay be built, such as libraries and executables. CMake also supports custom targets: users can\ndefine their inputs and outputs, and provide custom executables or scripts to be run at build time.\nCMake stores each target in a cmTarget object. These objects are stored in turn in the\ncmMakefile object, which is basically a storage place for all of the targets found in a given\ndirectory of the source tree. The end result is a tree of cmMakefile objects containing maps of\ncmTarget objects.\nThe Generate Step\nOnce the configure step has been completed, the generate step can take place. The generate step\nis when CMake creates the build files for the target build tool selected by the user. At this point the\ninternal representation of targets (libraries, executables, custom targets) is converted to either an\ninput to an IDE build tool like Visual Studio, or a set of Makefiles to be executed by make. CMake's\ninternal representation after the configure step is as generic as possible so that as much code and\ndata structures as possible can be shared between different built tools.\nAn overview of the process can be seen in Figure 5.1.\nFigure 5.1: Overview of the CMake Process\n5.2.2. CMake: The Code\nCMake Objects\nCMake is an object-oriented system using inheritance, design patterns and encapsulation. The\nmajor C++ objects and their relationships can be seen in Figure 5.2.\n", "page": 66, "type": "text", "section": "Page 66"}
{"text": "Figure 5.2: CMake Objects\nThe results of parsing each CMakeLists.txt file are stored in the cmMakefile object. In addition\nto storing the information about a directory, the cmMakefile object controls the parsing of the\nCMakeLists.txt file. The parsing function calls an object that uses a lex/yacc-based parser for\nthe CMake language. Since the CMake language syntax changes very infrequently, and lex and\nyacc are not always available on systems where CMake is being built, the lex and yacc output files\nare processed and stored in the Source directory under version control with all of the other\nhandwritten files.\nAnother important class in CMake is cmCommand. This is the base class for the implementation of\nall commands in the CMake language. Each subclass not only provides the implementation for the\ncommand, but also its documentation. As an example, see the documentation methods on the\ncmUnsetCommand class:\nvirtual const char* GetTerseDocumentation()\n{\n    return \"Unset a variable, cache variable, or environment variable.\";\n}\n/**\n * More documentation.\n */\nvirtual const char* GetFullDocumentation()\n{\n    return\n      \"  unset(<variable> [CACHE])\\n\"\n      \"Removes the specified variable causing it to become undefined.  \"\n      \"If CACHE is present then the variable is removed from the cache \"\n      \"instead of the current scope.\\n\"\n      \"<variable> can be an environment variable such as:\\n\"\n      \"  unset(ENV{LD_LIBRARY_PATH})\\n\"\n      \"in which case the variable will be removed from the current \"\n      \"environment.\";\n}\nDependency Analysis\n", "page": 67, "type": "text", "section": "Page 67"}
{"text": "CMake has powerful built-in dependency analysis capabilities for individual Fortran, C and C++\nsource code files. Since Integrated Development Environments (IDEs) support and maintain file\ndependency information, CMake skips this step for those build systems. For IDE builds, CMake\ncreates a native IDE input file, and lets the IDE handle the file level dependency information. The\ntarget level dependency information is translated to the IDE's format for specifying dependency\ninformation.\nWith Makefile-based builds, native make programs do not know how to automatically compute and\nkeep dependency information up-to-date. For these builds, CMake automatically computes\ndependency information for C, C++ and Fortran files. Both the generation and maintenance of\nthese dependencies are automatically done by CMake. Once a project is initially configured by\nCMake, users only need to run make and CMake does the rest of the work.\nAlthough users do not need to know how CMake does this work, it may be useful to look at the\ndependency information files for a project. This information for each target is stored in four files\ncalled depend.make, flags.make, build.make, and DependInfo.cmake. depend.make stores\nthe dependency information for all the object files in the directory. flags.make contains the\ncompile flags used for the source files of this target. If they change then the files will be\nrecompiled. DependInfo.cmake is used to keep the dependency information up-to-date and\ncontains information about what files are part of the project and what languages they are in.\nFinally, the rules for building the dependencies are stored in build.make. If a dependency for a\ntarget is out of date then the depend information for that target will be recomputed, keeping the\ndependency information current. This is done because a change to a .h file could add a new\ndependency.\nCTest and CPack\nAlong the way, CMake grew from a build system into a family of tools for building, testing, and\npackaging software. In addition to command line cmake, and the CMake GUI programs, CMake\nships with a testing tool CTest, and a packaging tool CPack. CTest and CPack shared the same\ncode base as CMake, but are separate tools not required for a basic build.\nThe ctest executable is used to run regression tests. A project can easily create tests for CTest\nto run with the add_test command. The tests can be run with CTest, which can also be used to\nsend testing results to the CDash application for viewing on the web. CTest and CDash together\nare similar to the Hudson testing tool. They do differ in one major area: CTest is designed to allow\na much more distributed testing environment. Clients can be setup to pull source from version\ncontrol system, run tests, and send the results to CDash. With Hudson, client machines must give\nHudson ssh access to the machine so tests can be run.\nThe cpack executable is used to create installers for projects. CPack works much like the build\npart of CMake: it interfaces with other packaging tools. For example, on Windows the NSIS\npackaging tool is used to create executable installers from a project. CPack runs the install rules of\na project to create the install tree, which is then given to a an installer program like NSIS. CPack\nalso supports creating RPM, Debian .deb files, .tar, .tar.gz and self-extracting tar files.\n5.2.3. Graphical Interfaces\nThe first place many users first see CMake is one of CMake's user interface programs. CMake has\ntwo main user interface programs: a windowed Qt-based application, and a command line curses\ngraphics-based application. These GUIs are graphical editors for the CMakeCache.txt file. They\nare relatively simple interfaces with two buttons, configure and generate, used to trigger the main\nphases of the CMake process. The curses-based GUI is available on Unix TTY-type platforms and\nCygwin. The Qt GUI is available on all platforms. The GUIs can be seen in Figure 5.3 and Figure 5.4.\n", "page": 68, "type": "text", "section": "Page 68"}
{"text": "Figure 5.3: Command Line Interface\nFigure 5.4: Graphics-based Interface\nBoth GUIs have cache variable names on the left, and values on the right. The values on the right\ncan be changed by the user to values that are appropriate for the build. There are two types of\nvariables, normal and advanced. By default the normal variables are shown to the user. A project\ncan determine which variables are advanced inside the CMakeLists.txt files for the project. This\nallows users to be presented with as few choices as necessary for a build.\nSince cache values can be modified as the commands are executed, the process of converging on\na final build can be iterative. For example, turning on an option may reveal additional options. For\nthis reason, the GUI disables the \"generate\" button until the user has had a chance to see all\noptions at least once. Each time the configure button is pressed, new cache variables that have\nnot yet been presented to the user are displayed in red. Once there are no new cache variables\ncreated during a configure run, the generate button is enabled.\n5.2.4. Testing CMake\nAny new CMake developer is first introduced to the testing process used in CMake development.\nThe process makes use of the CMake family of tools (CMake, CTest, CPack, and CDash). As the\ncode is developed and checked into the version control system, continuous integration testing\nmachines automatically build and test the new CMake code using CTest. The results are sent to a\nCDash server which notifies developers via email if there are any build errors, compiler warnings,\nor test failures.\nThe process is a classic continuous integration testing system. As new code is checked into the\nCMake repository, it is automatically tested on the platforms supported by CMake. Given the large\nnumber of compilers and platforms that CMake supports, this type of testing system is essential\nto the development of a stable build system.\nFor example, if a new developer wants to add support for a new platform, the first question he or\nshe is asked is whether they can provide a nightly dashboard client for that system. Without\nconstant testing, it is inevitable that new systems will stop working after some period of time.\n", "page": 69, "type": "text", "section": "Page 69"}
{"text": "5.3. Lessons Learned\nCMake was successfully building ITK from day one, and that was the most important part of the\nproject. If we could redo the development of CMake, not much would change. However, there are\nalways things that could have been done better.\n5.3.1. Backwards Compatibility\nMaintaining backwards compatibility is important to the CMake development team. The main goal\nof the project is to make building software easier. When a project or developer chooses CMake for\na build tool, it is important to honor that choice and try very hard to not break that build with\nfuture releases of CMake. CMake 2.6 implemented a policy system where changes to CMake that\nwould break existing behavior will warn but still perform the old behavior. Each CMakeLists.txt\nfile is required to specify which version of CMake they are expecting to use. Newer versions of\nCMake might warn, but will still build the project as older versions did.\n5.3.2. Language, Language, Language\nThe CMake language is meant to be very simple. However, it is one of the major obstacles to\nadoption when a new project is considering CMake. Given its organic growth, the CMake language\ndoes have a few quirks. The first parser for the language was not even lex/yacc based but rather\njust a simple string parser. Given the chance to do the language over, we would have spent some\ntime looking for a nice embedded language that already existed. Lua is the best fit that might have\nworked. It is very small and clean. Even if an external language like Lua was not used, I would have\ngiven more consideration to the existing language from the start.\n5.3.3. Plugins Did Not Work\nTo provide the ability for extension of the CMake language by projects, CMake has a plugin class.\nThis allows a project to create new CMake commands in C. This sounded like a good idea at the\ntime, and the interface was defined for C so that different compilers could be used. However, with\nthe advent of multiple API systems like 32/64 bit Windows and Linux, the compatibility of plugins\nbecame hard to maintain. While extending CMake with the CMake language is not as powerful, it\navoids CMake crashing or not being able to build a project because a plugin failed to build or load.\n5.3.4. Reduce Exposed APIs\nA big lesson learned during the development of the CMake project is that you don't have to\nmaintain backward compatibility with something that users don't have access to. Several times\nduring the development of CMake, users and customers requested that CMake be made into a\nlibrary so that other languages could be bound to the CMake functionality. Not only would this\nhave fractured the CMake user community with many different ways to use CMake, but it would\nhave been a huge maintenance cost for the CMake project.\nFootnotes\n1. http://www.itk.org/\n", "page": 70, "type": "text", "section": "Page 70"}
{"text": "The Architecture of\nOpen Source Applications\nAmy Brown and Greg Wilson (eds.)\nISBN 978-1-257-63801-7 \nLicense / Buy / Contribute\nChapter 6. Continuous Integration\nC. Titus Brown and Rosangela Canino-Koning\nContinuous Integration (CI) systems are systems that build and test software automatically and\nregularly. Though their primary benefit lies in avoiding long periods between build and test runs, CI\nsystems can also simplify and automate the execution of many otherwise tedious tasks. These\ninclude cross-platform testing, the regular running of slow, data-intensive, or difficult-to-configure\ntests, verification of proper performance on legacy platforms, detection of infrequently failing\ntests, and the regular production of up-to-date release products. And, because build and test\nautomation is necessary for implementing continuous integration, CI is often a first step towards a\ncontinuous deployment framework wherein software updates can be deployed quickly to live\nsystems after testing.\nContinuous integration is a timely subject, not least because of its prominence in the Agile\nsoftware methodology. There has been an explosion of open source CI tools in recent years, in\nand for a variety of languages, implementing a huge range of features in the context of a diverse\nset of architectural models. The purpose of this chapter is to describe common sets of features\nimplemented in continuous integration systems, discuss the architectural options available, and\nexamine which features may or may not be easy to implement given the choice of architecture.\nBelow, we will briefly describe a set of systems that exemplify the extremes of architectural\nchoices available when designing a CI system. The first, Buildbot, is a master/slave system; the\nsecond, CDash is a reporting server model; the third Jenkins, uses a hybrid model; and the fourth,\nPony-Build, is a Python-based decentralized reporting server that we will use as a foil for further\ndiscussion.\n6.1. The Landscape\nThe space of architectures for continuous integration systems seems to be dominated by two\nextremes: master/slave architectures, in which a central server directs and controls remote builds;\nand reporting architectures, in which a central server aggregates build reports contributed by\nclients. All of the continuous integration systems of which we are aware have chosen some\ncombination of features from these two architectures.\nOur example of a centralized architecture, Buildbot, is composed of two parts: the central server,\nor buildmaster, which schedules and coordinates builds between one or more connected clients;\nand the clients, or buildslaves, which execute builds. The buildmaster provides a central location to\nwhich to connect, along with configuration information about which clients should execute which\ncommands in what order. Buildslaves connect to the buildmaster and receive detailed instructions.\nBuildslave configuration consists of installing the software, identifying the master server, and\nproviding connection credentials for the client to connect to the master. Builds are scheduled by\nthe buildmaster, and output is streamed from the buildslaves to the buildmaster and kept on the\nmaster server for presentation via the Web and other reporting and notification systems.\nOn the opposite side of the architecture spectrum lies CDash, which is used for the Visualization\nToolkit (VTK)/Insight Toolkit (ITK) projects by Kitware, Inc. CDash is essentially a reporting server,\ndesigned to store and present information received from client computers running CMake and\nCTest. With CDash, the clients initiate the build and test suite, record build and test results, and\nthen connect to the CDash server to deposit the information for central reporting.\n", "page": 71, "type": "text", "section": "Page 71"}
{"text": "Finally, a third system, Jenkins (known as Hudson before a name change in 2011), provides both\nmodes of operation. With Jenkins, builds can either be executed independently with the results\nsent to the master server; or nodes can be slaved to the Jenkins master server, which then\nschedules and directs the execution of builds.\nBoth the centralized and decentralized models have some features in common, and, as Jenkins\nshows, both models can co-exist in a single implementation. However, Buildbot and CDash exist in\nstark contrast to each other: apart from the commonalities of building software and reporting on\nthe builds, essentially every other aspect of the architecture is different. Why?\nFurther, to what extent does the choice of architecture seem to make certain features easier or\nharder to implement? Do some features emerge naturally from a centralized model? And how\nextensible are the existing implementations\u2014can they easily be modified to provide new reporting\nmechanisms, or scale to many packages, or execute builds and tests in a cloud environment?\n6.1.1. What Does Continuous Integration Software Do?\nThe core functionality of a continuous integration system is simple: build software, run tests, and\nreport the results. The build, test, and reporting can be performed by a script running from a\nscheduled task or cron job: such a script would just check out a new copy of the source code\nfrom the VCS, do a build, and then run the tests. Output would be logged to a file, and either\nstored in a canonical location or sent out via e-mail in case of a build failure. This is simple to\nimplement: in UNIX, for example, this entire process can be implemented for most Python\npackages in a seven line script:\ncd /tmp && \\\nsvn checkout http://some.project.url && \\\ncd project_directory && \\\npython setup.py build && \\\npython setup.py test || \\\necho build failed | sendmail notification@project.domain\ncd /tmp && rm -fr project_directory\nIn Figure 6.1, the unshaded rectangles represent discrete subsystems and functionality within the\nsystem. Arrows show information flow between the various components. The cloud represents\npotential remote execution of build processes. The shaded rectangles represent potential coupling\nbetween the subsystems; for example, build monitoring may include monitoring of the build\nprocess itself and aspects of system health (CPU load, I/O load, memory usage, etc.)\n", "page": 72, "type": "text", "section": "Page 72"}
{"text": "Figure 6.1: Internals of a Continuous Integration System\nBut this simplicity is deceptive. Real-world CI systems usually do much more. In addition to\ninitiating or receiving the results of remote build processes, continuous integration software may\nsupport any of the following additional features:\nCheckout and update: For large projects, checking out a new copy of the source code can be\ncostly in terms of bandwidth and time. Usually, CI systems update an existing working copy\nin place, which communicates only the differences from the previous update. In exchange\nfor this savings, the system must keep track of the working copy and know how to update it,\nwhich usually means at least minimal integration with a VCS.\nAbstract build recipes: A configure/build/test recipe must be written for the software in\nquestion. The underlying commands will often be different for different operating systems,\ne.g. Mac OS X vs. Windows vs. UNIX, which means either specialized recipes need to be\nwritten (introducing potential bugs or disconnection from the actual build environment) or\nsome suitable level of abstraction for recipes must be provided by the CI configuration\nsystem.\nStoring checkout/build/test status: It may be desirable for details of the checkout (files\nupdated, code version), build (warnings or errors) and test (code coverage, performance,\nmemory usage) to be stored and used for later analysis. These results can be used to\nanswer questions across the build architectures (did the latest check-in significantly affect\nperformance on any particular architecture?) or over history (has code coverage changed\ndramatically in the last month?) As with the build recipe, the mechanisms and data types for\nthis kind of introspection are usually specific to the platform or build system.\nRelease packages: Builds may produce binary packages or other products that need to be\nmade externally available. For example, developers who don't have direct access to the build\nmachine may want to test the latest build in a specific architecture; to support this, the CI\nsystem needs to be able to transfer build products to a central repository.\nMultiple architecture builds: Since one goal of continuous integration is to build on multiple\narchitectures to test cross-platform functionality, the CI software may need to track the\narchitecture for each build machine and link builds and build outcomes to each client.\nResource management: If a build step is resource intensive on a single machine, the CI\nsystem may want to run conditionally. For example, builds may wait for the absence of other\n", "page": 73, "type": "text", "section": "Page 73"}
{"text": "builds or users, or delay until a particular CPU or memory load is reached.\nExternal resource coordination: Integration tests may depend on non-local resources such\nas a staging database or a remote web service. The CI system may therefore need to\ncoordinate builds between multiple machines to organize access to these resources.\nProgress reports: For long build processes, regular reporting from the build may also be\nimportant. If a user is primarily interested in the results of the first 30 minutes of a 5 hour\nbuild and test, then it would be a waste of time to make them wait until the end of a run to\nsee any results.\nA high-level view of all of these potential components of a CI system is shown in Figure 6.1. CI\nsoftware usually implements some subset of these components.\n6.1.2. External Interactions\nContinuous integration systems also need to interact with other systems. There are several types\nof potential interactions:\nBuild notification: The outcomes of builds generally need to be communicated to interested\nclients, either via pull (Web, RSS, RPC, etc.) or push notification (e-mail, Twitter,\nPubSubHubbub, etc.) This can include notification of all builds, or only failed builds, or builds\nthat haven't been executed within a certain period.\nBuild information: Build details and products may need to be retrieved, usually via RPC or a\nbulk download system. For example, it may be desirable to have an external analysis system\ndo an in-depth or more targeted analysis, or report on code coverage or performance\nresults. In addition, an external test result repository may be employed to keep track of\nfailing and successful tests separately from the CI system.\nBuild requests: External build requests from users or a code repository may need to be\nhandled. Most VCSs have post-commit hooks that can execute an RPC call to initiate a build,\nfor example. Or, users may request builds manually through a Web interface or other user-\ninitiated RPC.\nRemote control of the CI system: More generally, the entire runtime may be modifiable\nthrough a more-or-less well-defined RPC interface. Either ad hoc extensions or a more\nformally specified interface may need to be able to drive builds on specific platforms, specify\nalternate source branches in order to build with various patches, and execute additional\nbuilds conditionally. This is useful in support of more general workflow systems, e.g. to\npermit commits only after they have passed the full set of CI tests, or to test patches across\na wide variety of systems before final integration. Because of the variety of bug tracker,\npatch systems, and other external systems in use, it may not make sense to include this\nlogic within the CI system itself.\n6.2. Architectures\nBuildbot and CDash have chosen opposite architectures, and implement overlapping but distinct\nsets of features. Below we examine these feature sets and discuss how features are easier or\nharder to implement given the choice of architecture.\n6.2.1. Implementation Model: Buildbot\n", "page": 74, "type": "text", "section": "Page 74"}
{"text": "Figure 6.2: Buildbot Architecture\nBuildbot uses a master/slave architecture, with a single central server and multiple build slaves.\nRemote execution is entirely scripted by the master server in real time: the master configuration\nspecifies the command to be executed on each remote system, and runs them when each\nprevious command is finished. Scheduling and build requests are not only coordinated through\nthe master but directed entirely by the master. No built-in recipe abstraction exists, except for\nbasic version control system integration (\"our code is in this repository\") and a distinction between\ncommands that operate on the build directory vs. within the build directory. OS-specific\ncommands are typically specified directly in the configuration.\nBuildbot maintains a constant connection with each buildslave, and manages and coordinates job\nexecution between them. Managing remote machines through a persistent connection adds\nsignificant practical complexity to the implementation, and has been a long-standing source of\nbugs. Keeping robust long-term network connections running is not simple, and testing\napplications which interact with the local GUI is challenging through a network connection. OS alert\nwindows are particularly difficult to deal with. However, this constant connection makes resource\ncoordination and scheduling straightforward, because slaves are entirely at the disposal of the\nmaster for execution of jobs.\nThe kind of tight control designed into the Buildbot model makes centralized build coordination\nbetween resources very easy. Buildbot implements both master and slave locks on the\nbuildmaster, so that builds can coordinate system-global and machine-local resources. This makes\nBuildbot particularly suitable for large installations that run system integration tests, e.g. tests that\ninteract with databases or other expensive resources.\nThe centralized configuration causes problems for a distributed use model, however. Each new\nbuildslave must be explicitly allowed for in the master configuration, which makes it impossible for\nnew buildslaves to dynamically attach to the central server and offer build services or build results.\nMoreover, because each build slave is entirely driven by the build master, build clients are\nvulnerable to malicious or accidental misconfigurations: the master literally controls the client\nentirely, within the client OS security restrictions.\nOne limiting feature of Buildbot is that there is no simple way to return build products to the\ncentral server. For example, code coverage statistics and binary builds are kept on the remote\nbuildslave, and there is no API to transmit them to the central buildmaster for aggregation and\ndistribution. It is not clear why this feature is absent. It may be a consequence of the limited set of\ncommand abstractions distributed with Buildbot, which are focused on executing remote\ncommands on the build slaves. Or, it may be due to the decision to use the connection between\nthe buildmaster and buildslave as a control system, rather than as an RPC mechanism.\nAnother consequence of the master/slave model and this limited communications channel is that\nbuildslaves do not report system utilization and the master cannot be configured to be aware of\nhigh slave load.\n", "page": 75, "type": "text", "section": "Page 75"}
{"text": "External CPU notification of build results is handled entirely by the buildmaster, and new\nnotification services need to be implemented within the buildmaster itself. Likewise, new build\nrequests must be communicated directly to the buildmaster.\n6.2.2. Implementation Model: CDash\nFigure 6.3: CDash Architecture\nIn contrast to Buildbot, CDash implements a reporting server model. In this model, the CDash\nserver acts as a central repository for information on remotely executed builds, with associated\nreporting on build and test failures, code coverage analysis, and memory usage. Builds run on\nremote clients on their own schedule, and submit build reports in an XML format. Builds can be\nsubmitted both by \"official\" build clients and by non-core developers or users running the\npublished build process on their own machines.\nThis simple model is made possible because of the tight conceptual integration between CDash\nand other elements of the Kitware build infrastructure: CMake, a build configuration system, CTest,\na test runner, and CPack, a packaging system. This software provides a mechanism by which\nbuild, test, and packaging recipes can be implemented at a fairly high level of abstraction in an OS-\nagnostic manner.\nCDash's client-driven process simplifies many aspects of the client-side CI process. The decision to\nrun a build is made by build clients, so client-side conditions (time of day, high load, etc.) can be\ntaken into account by the client before starting a build. Clients can appear and disappear as they\nwish, easily enabling volunteer builds and builds \"in the cloud\". Build products can be sent to the\ncentral server via a straightforward upload mechanism.\nHowever, in exchange for this reporting model, CDash lacks many convenient features of\nBuildbot. There is no centralized coordination of resources, nor can this be implemented simply in\na distributed environment with untrusted or unreliable clients. Progress reports are also not\nimplemented: to do so, the server would have to allow incremental updating of build status. And,\nof course, there is no way to both globally request a build, and guarantee that anonymous clients\nperform the build in response to a check-in\u2014clients must be considered unreliable.\nRecently, CDash added functionality to enable an \"@Home\" cloud build system, in which clients\noffer build services to a CDash server. Clients poll the server for build requests, execute them\nupon request, and return the results to the server. In the current implementation (October 2010),\nbuilds must be manually requested on the server side, and clients must be connected for the\nserver to offer their services. However, it is straightforward to extend this to a more generic\nscheduled-build model in which builds are requested automatically by the server whenever a\nrelevant client is available. The \"@Home\" system is very similar in concept to the Pony-Build system\ndescribed later.\n6.2.3. Implementation Model: Jenkins\nJenkins is a widely used continuous integration system implemented in Java; until early 2011, it was\nknown as Hudson. It is capable of acting either as a standalone CI system with execution on a\nlocal system, or as a coordinator of remote builds, or even as a passive receiver of remote build\ninformation. It takes advantage of the JUnit XML standard for unit test and code coverage\nreporting to integrate reports from a variety of test tools. Jenkins originated with Sun, but is very\n", "page": 76, "type": "text", "section": "Page 76"}
{"text": "widely used and has a robust open-source community associated with it.\nJenkins operates in a hybrid mode, defaulting to master-server build execution but allowing a\nvariety of methods for executing remote builds, including both server- and client-initiated builds.\nLike Buildbot, however, it is primarily designed for central server control, but has been adapted to\nsupport a wide variety of distributed job initiation mechanisms, including virtual machine\nmanagement.\nJenkins can manage multiple remote machines through a connection initiated by the master via an\nSSH connection, or from the client via JNLP (Java Web Start). This connection is two-way, and\nsupports the communication of objects and data via serial transport.\nJenkins has a robust plugin architecture that abstracts the details of this connection, which has\nallowed the development of many third-party plugins to support the return of binary builds and\nmore significant result data.\nFor jobs that are controlled by a central server, Jenkins has a \"locks\" plugin to discourage jobs\nfrom running in parallel, although as of January 2011 it is not yet fully developed.\n6.2.4. Implementation Model: Pony-Build\nFigure 6.4: Pony-Build Architecture\nPony-Build is a proof-of-concept decentralized CI system written in Python. It is composed of\nthree core components, which are illustrated in Figure 6.4. The results server acts as a centralized\ndatabase containing build results received from individual clients. The clients independently contain\nall configuration information and build context, coupled with a lightweight client-side library to help\nwith VCS repository access, build process management, and the communication of results to the\nserver. The reporting server is optional, and contains a simple Web interface, both for reporting\non the results of builds and potentially for requesting new builds. In our implementation, the\nreporting server and results server run in a single multithreaded process but are loosely coupled\nat the API level and could easily be altered to run independently.\nThis basic model is decorated with a variety of webhooks and RPC mechanisms to facilitate build\nand change notification and build introspection. For example, rather than tying VCS change\nnotification from the code repository directly into the build system, remote build requests are\ndirected to the reporting system, which communicates them to the results server. Likewise, rather\nthan building push notification of new builds out to e-mail, instant messaging, and other services\ndirectly into the reporting server, notification is controlled using the PubSubHubbub (PuSH) active\nnotification protocol. This allows a wide variety of consuming applications to receive notification of\n\"interesting\" events (currently limited to new builds and failed builds) via a PuSH webhook.\n", "page": 77, "type": "text", "section": "Page 77"}
{"text": "The advantages of this very decoupled model are substantial:\nEase of communication: The basic architectural components and webhook protocols are\nextremely easy to implement, requiring only a basic knowledge of Web programming.\nEasy modification: The implementation of new notification methods, or a new reporting server\ninterface, is extremely simple.\nMultiple language support: Since the various components call each other via webhooks,\nwhich are supported by most programming languages, different components can be\nimplemented in different languages.\nTestability: Each component can be completely isolated and mocked, so the system is very\ntestable.\nEase of configuration: The client-side requirements are minimal, with only a single library file\nrequired beyond Python itself.\nMinimal server load: Since the central server has virtually no control responsibilities over the\nclients, isolated clients can run in parallel without contacting the server and placing any\ncorresponding load it, other than at reporting time.\nVCS integration: Build configuration is entirely client side, allowing it to be included within the\nVCS.\nEase of results access: Applications that wish to consume build results can be written in any\nlanguage capable of an XML-RPC request. Users of the build system can be granted access\nto the results and reporting servers at the network level, or via a customized interface at the\nreporting server. The build clients only need access to post results to the results server.\nUnfortunately, there are also many serious disadvantages, as with the CDash model:\nDifficulty requesting builds: This difficulty is introduced by having the build clients be entirely\nindependent of the results server. Clients may poll the results server, to see if a build request\nis operative, but this introduces high load and significant latency. Alternatively, command and\ncontrol connections need to be established to allow the server to notify clients of build\nrequests directly. This introduces more complexity into the system and eliminates the\nadvantages of decoupled build clients.\nPoor support for resource locking: It is easy to provide an RPC mechanism for holding and\nreleasing resource locks, but much more difficult to enforce client policies. While CI systems\nlike CDash assume good faith on the client side, clients may fail unintentionally and badly, e.g.\nwithout releasing locks. Implementing a robust distributed locking system is hard and adds\nsignificant undesired complexity. For example, in order to provide master resource locks with\nunreliable clients, the master lock controller must have a policy in place for clients that take a\nlock and never release it, either because they crash or because of a deadlock situation.\nPoor support for real-time monitoring: Real-time monitoring of the build, and control of the\nbuild process itself, is challenging to implement in a system without a constant connection.\nOne significant advantage of Buildbot over the client-driven model is that intermediate\ninspection of long builds is easy, because the results of the build are communicated to the\nmaster interface incrementally. Moreover, because Buildbot retains a control connection, if a\nlong build goes bad in the middle due to misconfiguration or a bad check-in, it can be\ninterrupted and aborted. Adding such a feature into Pony-Build, in which the results server\nhas no guaranteed ability to contact the clients, would require either constant polling by the\nclients, or the addition of a standing connection to the clients.\nTwo other aspects of CIs that were raised by Pony-Build were how best to implement recipes, and\nhow to manage trust. These are intertwined issues, because recipes execute arbitrary code on\nbuild clients.\n6.2.5. Build Recipes\nBuild recipes add a useful level of abstraction, especially for software built in a cross-platform\nlanguage or using a multi-platform build system. For example, CDash relies on a strict kind of\nrecipe; most, or perhaps all, software that uses CDash is built with CMake, CTest, and CPack, and\nthese tools are built to handle multi-platform issues. This is the ideal situation from the viewpoint of\na continuous integration system, because the CI system can simply delegate all issues to the build\n", "page": 78, "type": "text", "section": "Page 78"}
{"text": "tool chain.\nHowever, this is not true for all languages and build environments. In the Python ecosystem, there\nhas been increasing standardization around distutils and distutils2 for building and packaging\nsoftware, but as yet no standard has emerged for discovering and running tests, and collating the\nresults. Moreover, many of the more complex Python packages add specialized build logic into\ntheir system, through a distutils extension mechanism that allows the execution of arbitrary code.\nThis is typical of most build tool chains: while there may be a fairly standard set of commands to\nbe run, there are always exceptions and extensions.\nRecipes for building, testing, and packaging are therefore problematic, because they must solve\ntwo problems: first, they should be specified in a platform independent way, so that a single recipe\ncan be used to build software on multiple systems; and second, they must be customizable to the\nsoftware being built.\n6.2.6. Trust\nThis raises a third problem. Widespread use of recipes by a CI system introduces a second party\nthat must be trusted by the system: not only must the software itself be trustworthy (because\nthe CI clients are executing arbitrary code), but the recipes must also be trustworthy (because\nthey, too, must be able to execute arbitrary code).\nThese trust issues are easy to handle in a tightly controlled environment, e.g. a company where\nthe build clients and CI system are part of an internal process. In other development\nenvironments, however, interested third parties may want to offer build services, for example to\nopen source projects. The ideal solution would be to support the inclusion of standard build\nrecipes in software on a community level, a direction that the Python community is taking with\ndistutils2. An alternative solution would be to allow for the use of digitally signed recipes, so that\ntrusted individuals could write and distribute signed recipes, and CI clients could check to see if\nthey should trust the recipes.\n6.2.7. Choosing a Model\nIn our experience, a loosely coupled RPC or webhook callback-based model for continuous\nintegration is extremely easy to implement, as long as one ignores any requirements for tight\ncoordination that would involve complex coupling. Basic execution of remote checkouts and builds\nhas similar design constraints whether the build is being driven locally or remotely; collection of\ninformation about the build (success/failure, etc.) is primarily driven by client-side requirements;\nand tracking information by architecture and result involves the same basic requirements. Thus a\nbasic CI system can be implemented quite easily using the reporting model.\nWe found the loosely coupled model to be very flexible and expandable, as well. Adding new\nresults reporting, notification mechanisms, and build recipes is easy because the components are\nclearly separated and quite independent. Separated components have clearly delegated tasks to\nperform, and are also easy to test and easy to modify.\nThe only challenging aspect of remote builds in a CDash-like loosely-coupled model is build\ncoordination: starting and stopping builds, reporting on ongoing builds, and coordinating resource\nlocks between different clients is technically demanding compared to the rest of the\nimplementation.\nIt is easy to reach the conclusion that the loosely coupled model is \"better\" all around, but\nobviously this is only true if build coordination is not needed. This decision should be made based\non the needs of projects using the CI system.\n6.3. The Future\nWhile thinking about Pony-Build, we came up with a few features that we would like to see in\nfuture continuous integration systems.\n", "page": 79, "type": "text", "section": "Page 79"}
{"text": "A language-agnostic set of build recipes: Currently, each continuous integration system\nreinvents the wheel by providing its own build configuration language, which is manifestly\nridiculous; there are fewer than a dozen commonly used build systems, and probably only a\nfew dozen test runners. Nonetheless, each CI system has a new and different way of\nspecifying the build and test commands to be run. In fact, this seems to be one of the\nreasons why so many basically identical CI systems exist: each language and community\nimplements their own configuration system, tailored to their own build and test systems, and\nthen layers on the same set of features above that system. Therefore, building a domain-\nspecific language (DSL) capable of representing the options used by the few dozen\ncommonly used build and test tool chains would go a long way toward simplifying the CI\nlandscape.\nCommon formats for build and test reporting: There is little agreement on exactly what\ninformation, in what format, a build and test system needs to provide. If a common format\nor standard could be developed it would make it much easier for continuous integration\nsystems to offer both detailed and summary views across builds. The Test Anywhere\nProtocol, TAP (from the Perl community) and the JUnit XML test output format (from the Java\ncommunity) are two interesting options that are capable of encoding information about\nnumber of tests run, successes and failures, and per-file code coverage details.\nIncreased granularity and introspection in reporting: Alternatively, it would be convenient if\ndifferent build platforms provided a well-documented set of hooks into their configuration,\ncompilation, and test systems. This would provide an API (rather than a common format)\nthat CI systems could use to extract more detailed information about builds.\n6.3.1. Concluding Thoughts\nThe continuous integration systems described above implemented features that fit their\narchitecture, while the hybrid Jenkins system started with a master/slave model but added\nfeatures from the more loosely coupled reporting architecture.\nIt is tempting to conclude that architecture dictates function. This is nonsense, of course. Rather,\nthe choice of architecture seems to canalize or direct development towards a particular set of\nfeatures. For Pony-Build, we were surprised at the extent to which our initial choice of a CDash-\nstyle reporting architecture drove later design and implementation decisions. Some implementation\nchoices, such as the avoidance of a centralized configuration and scheduling system in Pony-Build\nwere driven by our use cases: we needed to allow dynamic attachment of remote build clients,\nwhich is difficult to support with Buildbot. Other features we didn't implement, such as progress\nreports and centralized resource locking in Pony-Build, were desirable but simply too complicated\nto add without a compelling requirement.\nSimilar logic may apply to Buildbot, CDash, and Jenkins. In each case there are useful features that\nare absent, perhaps due to architectural incompatibility. However, from discussions with members\nof the Buildbot and CDash communities, and from reading the Jenkins website, it seems likely that\nthe desired features were chosen first, and the system was then developed using an architecture\nthat permitted those features to be easily implemented. For example, CDash serves a community\nwith a relatively small set of core developers, who develop software using a centralized model.\nTheir primary consideration is to keep the software working on a core set of machines, and\nsecondarily to receive bug reports from tech-savvy users. Meanwhile, Buildbot is increasingly used\nin complex build environments with many clients that require coordination to access shared\nresources. Buildbot's more flexible configuration file format with its many options for scheduling,\nchange notification, and resource locks fits that need better than the other options. Finally, Jenkins\nseems aimed at ease of use and simple continuous integration, with a full GUI for configuring it\nand configuration options for running on the local server.\nThe sociology of open source development is another confounding factor in correlating\narchitecture with features: suppose developers choose open source projects based on how well\nthe project architecture and features fit their use case? If so, then their contributions will generally\nreflect an extension of a use case that already fits the project well. Thus projects may get locked\ninto a certain feature set, since contributors are self-selected and may avoid projects with\narchitectures that don't fit their own desired features. This was certainly true for us in choosing to\n", "page": 80, "type": "text", "section": "Page 80"}
{"text": "implement a new system, Pony-Build, rather than contributing to Buildbot: the Buildbot\narchitecture was simply not appropriate for building hundreds or thousands of packages.\nExisting continuous integration systems are generally built around one of two disparate\narchitectures, and generally implement only a subset of desirable features. As CI systems mature\nand their user populations grow, we would expect them to grow additional features; however,\nimplementation of these features may be constrained by the base choice of architecture. It will be\ninteresting to see how the field evolves.\n6.3.2. Acknowledgments\nWe thank Greg Wilson, Brett Cannon, Eric Holscher, Jesse Noller, and Victoria Laidler for\ninteresting discussions on CI systems in general, and Pony-Build in particular. Several students\ncontributed to Pony-Build development, including Jack Carlson, Fatima Cherkaoui, Max Laite, and\nKhushboo Shakya.\n", "page": 81, "type": "text", "section": "Page 81"}
{"text": "The Architecture of\nOpen Source Applications\nAmy Brown and Greg Wilson (eds.)\nISBN 978-1-257-63801-7 \nLicense / Buy / Contribute\nChapter 7. Eclipse\nKim Moir\nImplementing software modularity is a notoriously difficult task. Interoperability with a large code\nbase written by a diverse community is also difficult to manage. At Eclipse, we have managed to\nsucceed on both counts. In June 2010, the Eclipse Foundation made available its Helios\ncoordinated release, with over 39 projects and 490 committers from over 40 companies working\ntogether to build upon the functionality of the base platform. What was the original architectural\nvision for Eclipse? How did it evolve? How does the architecture of an application serve to\nencourage community engagement and growth? Let's go back to the beginning.\nOn November 7, 2001, an open source project called Eclipse 1.0 was released. At the time, Eclipse\nwas described as \"an integrated development environment (IDE) for anything and nothing in\nparticular.\" This description was purposely generic because the architectural vision was not just\nanother set of tools, but a framework; a framework that was modular and scalable. Eclipse\nprovided a component-based platform that could serve as the foundation for building tools for\ndevelopers. This extensible architecture encouraged the community to build upon a core platform\nand extend it beyond the limits of the original vision. Eclipse started as a platform and the Eclipse\nSDK was the proof-of-concept product. The Eclipse SDK allowed the developers to self-host and\nuse the Eclipse SDK itself to build newer versions of Eclipse.\nThe stereotypical image of an open source developer is that of an altruistic person toiling late into\nnight fixing bugs and implementing fantastic new features to address their own personal interests.\nIn contrast, if you look back at the early history of the Eclipse project, some of the initial code that\nwas donated was based on VisualAge for Java, developed by IBM. The first committers who\nworked on this open source project were employees of an IBM subsidiary called Object\nTechnology International (OTI). These committers were paid to work full time on the open source\nproject, to answer questions on newsgroups, address bugs, and implement new features. A\nconsortium of interested software vendors was formed to expand this open tooling effort. The\ninitial members of the Eclipse consortium were Borland, IBM, Merant, QNX Software Systems,\nRational Software, RedHat, SuSE, and TogetherSoft.\nBy investing in this effort, these companies would have the expertise to ship commercial products\nbased on Eclipse. This is similar to investments that corporations make in contributing to the Linux\nkernel because it is in their self-interest to have employees improving the open source software\nthat underlies their commercial offerings. In early 2004, the Eclipse Foundation was formed to\nmanage and expand the growing Eclipse community. This not-for-profit foundation was funded by\ncorporate membership dues and is governed by a board of directors. Today, the diversity of the\nEclipse community has expanded to include over 170 member companies and almost 1000\ncommitters.\nOriginally, people knew \"Eclipse\" as the SDK only but today it is much more. In July 2010, there\nwere 250 diverse projects under development at eclipse.org. There's tooling to support developing\nwith C/C++, PHP, web services, model driven development, build tooling and many more. Each of\nthese projects is included in a top-level project (TLP) which is managed by a project management\ncommittee (PMC) consisting of senior members of the project nominated for the responsibility of\nsetting technical direction and release goals. In the interests of brevity, the scope of this chapter\nwill be limited to the evolution of the architecture of the Eclipse SDK within Eclipse1 and Runtime\nEquinox2 projects. Since Eclipse has long history, I'll be focusing on early Eclipse, as well as the\n3.0, 3.4 and 4.0 releases.\n7.1. Early Eclipse\n", "page": 82, "type": "text", "section": "Page 82"}
{"text": "At the beginning of the 21st century, there were many tools for software developers, but few of\nthem worked together. Eclipse sought to provide an open source platform for the creation of\ninteroperable tools for application developers. This would allow developers to focus on writing new\ntools, instead of writing to code deal with infrastructure issues like interacting with the filesystem,\nproviding software updates, and connecting to source code repositories. Eclipse is perhaps most\nfamous for the Java Development Tools (JDT). The intent was that these exemplary Java\ndevelopment tools would serve as an example for people interested in providing tooling for other\nlanguages.\nBefore we delve into the architecture of Eclipse, let's look at what the Eclipse SDK looks like to a\ndeveloper. Upon starting Eclipse and selecting the workbench, you'll be presented with the Java\nperspective. A perspective organizes the views and editors that are specific to the tooling that is\ncurrently in use.\nFigure 7.1: Java Perspective\nEarly versions of the Eclipse SDK architecture had three major elements, which corresponded to\nthree major sub-projects: the Platform, the JDT (Java Development Tools) and the PDE (Plug-in\nDevelopment Environment).\n7.1.1. Platform\nThe Eclipse platform is written using Java and a Java VM is required to run it. It is built from small\nunits of functionality called plugins. Plugins are the basis of the Eclipse component model. A plugin\nis essentially a JAR file with a manifest which describes itself, its dependencies, and how it can be\nutilized, or extended. This manifest information was initially stored in a plug-in.xml file which\nresides in the root of the plugin directory. The Java development tools provided plugins for\ndeveloping in Java. The Plug-in Development Environment (PDE) provides tooling for developing\nplugins to extend Eclipse. Eclipse plugins are written in Java but could also contain non-code\ncontributions such as HTML files for online documentation. Each plugin has its own class loader.\nPlugins can express dependencies on other plugins by the use of requires statements in the\nplugin.xml. Looking at the plugin.xml for the org.eclipse.ui plugin you can see its name\nand version specified, as well as the dependencies it needs to import from other plugins.\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n", "page": 83, "type": "text", "section": "Page 83"}
{"text": "<plugin\n   id=\"org.eclipse.ui\"\n   name=\"%Plugin.name\"\n   version=\"2.1.1\"\n   provider-name=\"%Plugin.providerName\"\n   class=\"org.eclipse.ui.internal.UIPlugin\">\n   <runtime>\n      <library name=\"ui.jar\">\n         <export name=\"*\"/>\n         <packages prefixes=\"org.eclipse.ui\"/>\n      </library>\n   </runtime>\n   <requires>\n      <import plugin=\"org.apache.xerces\"/>\n      <import plugin=\"org.eclipse.core.resources\"/>\n      <import plugin=\"org.eclipse.update.core\"/>\n      :       :        :\n      <import plugin=\"org.eclipse.text\" export=\"true\"/>\n      <import plugin=\"org.eclipse.ui.workbench.texteditor\" export=\"true\"/>\n      <import plugin=\"org.eclipse.ui.editors\" export=\"true\"/>\n   </requires>\n</plugin>\nIn order to encourage people to build upon the Eclipse platform, there needs to be a mechanism\nto make a contribution to the platform, and for the platform to accept this contribution. This is\nachieved through the use of extensions and extension points, another element of the Eclipse\ncomponent model. The export identifies the interfaces that you expect others to use when writing\ntheir extensions, which limits the classes that are available outside your plugin to the ones that are\nexported. It also provides additional limitations on the resources that are available outside the\nplugin, as opposed to making all public methods or classes available to consumers. Exported\nplugins are considered public API. All others are considered private implementation details. To write\na plugin that would contribute a menu item to the Eclipse toolbar, you can use the actionSets\nextension point in the org.eclipse.ui plugin.\n<extension-point id=\"actionSets\" name=\"%ExtPoint.actionSets\"\n                 schema=\"schema/actionSets.exsd\"/>\n<extension-point id=\"commands\" name=\"%ExtPoint.commands\"\n                 schema=\"schema/commands.exsd\"/>\n<extension-point id=\"contexts\" name=\"%ExtPoint.contexts\"\n                 schema=\"schema/contexts.exsd\"/>\n<extension-point id=\"decorators\" name=\"%ExtPoint.decorators\"\n                 schema=\"schema/decorators.exsd\"/>\n<extension-point id=\"dropActions\" name=\"%ExtPoint.dropActions\"\n                 schema=\"schema/dropActions.exsd\"/> =\nYour plugin's extension to contribute a menu item to the org.eclipse.ui.actionSet extension\npoint would look like:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<plugin\n   id=\"com.example.helloworld\"\n   name=\"com.example.helloworld\"\n   version=\"1.0.0\">\n   <runtime>\n      <library name=\"helloworld.jar\"/>\n   </runtime>\n   <requires>\n      <import plugin=\"org.eclipse.ui\"/>\n   </requires>\n   <extension\n         point=\"org.eclipse.ui.actionSets\">\n", "page": 84, "type": "text", "section": "Page 84"}
{"text": "      <actionSet\n            label=\"Example Action Set\"\n            visible=\"true\"\n            id=\"org.eclipse.helloworld.actionSet\">\n         <menu\n               label=\"Example &Menu\"\n               id=\"exampleMenu\">\n            <separator\n                  name=\"exampleGroup\">\n            </separator>\n         </menu>\n         <action\n               label=\"&Example Action\"\n               icon=\"icons/example.gif\"\n               tooltip=\"Hello, Eclipse world\"\n               class=\"com.example.helloworld.actions.ExampleAction\"\n               menubarPath=\"exampleMenu/exampleGroup\"\n               toolbarPath=\"exampleGroup\"\n               id=\"org.eclipse.helloworld.actions.ExampleAction\">\n         </action>\n      </actionSet>\n   </extension>\n</plugin>\nWhen Eclipse is started, the runtime platform scans the manifests of the plugins in your install,\nand builds a plugin registry that is stored in memory. Extension points and the corresponding\nextensions are mapped by name. The resulting plugin registry can be referenced from the API\nprovided by the Eclipse platform. The registry is cached to disk so that this information can be\nreloaded the next time Eclipse is restarted. All plugins are discovered upon startup to populate the\nregistry but they are not activated (classes loaded) until the code is actually used. This approach is\ncalled lazy activation. The performance impact of adding additional bundles into your install is\nreduced by not actually loading the classes associated with the plugins until they are needed. For\ninstance, the plugin that contributes to the org.eclipse.ui.actionSet extension point wouldn't be\nactivated until the user selected the new menu item in the toolbar.\nFigure 7.2: Example Menu\nThe code that generates this menu item looks like this:\npackage com.example.helloworld.actions;\nimport org.eclipse.jface.action.IAction;\nimport org.eclipse.jface.viewers.ISelection;\nimport org.eclipse.ui.IWorkbenchWindow;\nimport org.eclipse.ui.IWorkbenchWindowActionDelegate;\nimport org.eclipse.jface.dialogs.MessageDialog;\npublic class ExampleAction implements IWorkbenchWindowActionDelegate {\n    private IWorkbenchWindow window;\n    public ExampleAction() {\n    }\n    public void run(IAction action) {\n        MessageDialog.openInformation(\n            window.getShell(),\n            \"org.eclipse.helloworld\",\n", "page": 85, "type": "text", "section": "Page 85"}
{"text": "            \"Hello, Eclipse architecture world\");\n    }\n    public void selectionChanged(IAction action, ISelection selection) {\n    }\n    public void dispose() {\n    }\n    public void init(IWorkbenchWindow window) {\n        this.window = window;\n    }\n}\nOnce the user selects the new item in the toolbar, the extension registry is queried by the plugin\nimplementing the extension point. The plugin supplying the extension instantiates the contribution,\nand loads the plugin. Once the plugin is activated, the ExampleAction constructor in our example\nis run, and then initializes a Workbench action delegate. Since the selection in the workbench has\nchanged and the delegate has been created, the action can change. The message dialog opens\nwith the message \"Hello, Eclipse architecture world\".\nThis extensible architecture was one of the keys to the successful growth of the Eclipse\necosystem. Companies or individuals could develop new plugins, and either release them as open\nsource or sell them commercially.\nOne of the most important concepts about Eclipse is that everything is a plugin. Whether the\nplugin is included in the Eclipse platform, or you write it yourself, plugins are all first class\ncomponents of the assembled application. Figure 7.3 shows clusters of related functionality\ncontributed by plugins in early versions of Eclipse.\nFigure 7.3: Early Eclipse Architecture\n", "page": 86, "type": "text", "section": "Page 86"}
{"text": "The workbench is the most familiar UI element to users of the Eclipse platform, as it provides the\nstructures that organize how Eclipse appears to the user on the desktop. The workbench consists\nof perspectives, views, and editors. Editors are associated with file types so the correct editor is\nlaunched when a file is opened. An example of a view is the \"problems\" view that indicates errors\nor warnings in your Java code. Together, editors and views form a perspective which presents the\ntooling to the user in an organized fashion.\nThe Eclipse workbench is built on the Standard Widget Toolkit (SWT) and JFace, and SWT deserves\na bit of exploration. Widget toolkits are generally classified as either native or emulated. A native\nwidget toolkit uses operating system calls to build user interface components such as lists and\npush buttons. Interaction with components is handled by the operating system. An emulated\nwidget toolkit implements components outside of the operating system, handling mouse and\nkeyboard, drawing, focus and other widget functionality itself, rather than deferring to the\noperating system. Both designs have different strengths and weaknesses.\nNative widget toolkits are \"pixel perfect.\" Their widgets look and feel like their counterparts in other\napplications on the desktop. Operating system vendors constantly change the look and feel of\ntheir widgets and add new features. Native widget toolkits get these updates for free.\nUnfortunately, native toolkits are difficult to implement because their underlying operating system\nwidget implementations are vastly different, leading to inconsistencies and programs that are not\nportable.\nEmulated widget toolkits either provide their own look and feel, or try to draw and behave like the\noperating system. Their great strength over native toolkits is flexibility (although modern native\nwidget toolkits such as Windows Presentation Framework (WPF) are equally as flexible). Because\nthe code to implement a widget is part of the toolkit rather than embedded in the operating\nsystem, a widget can be made to draw and behave in any manner. Programs that use emulated\nwidget toolkits are highly portable. Early emulated widget toolkits had a bad reputation. They were\noften slow and did a poor job of emulating the operating system, making them look out of place\non the desktop. In particular, Smalltalk-80 programs at the time were easy to recognize due to\ntheir use of emulated widgets. Users were aware that they were running a \"Smalltalk program\"\nand this hurt acceptance of applications written in Smalltalk.\nUnlike other computer languages such as C and C++, the first versions of Java came with a native\nwidget toolkit library called the Abstract Window Toolkit (AWT). AWT was considered to be limited,\nbuggy and inconsistent and was widely decried. At Sun and elsewhere, in part because of\nexperience with AWT, a native widget toolkit that was portable and performant was considered to\nbe unworkable. The solution was Swing, a full-featured emulated widget toolkit.\nAround 1999, OTI was using Java to implement a product called VisualAge Micro Edition. The first\nversion of VisualAge Micro Edition used Swing and OTI's experience with Swing was not positive.\nEarly versions of Swing were buggy, had timing and memory issues and the hardware at the time\nwas not powerful enough to give acceptable performance. OTI had successfully built a native\nwidget toolkit for Smalltalk-80 and other Smalltalk implementations to gain acceptance of Smalltalk.\nThis experience was used to build the first version of SWT. VisualAge Micro Edition and SWT were\na success and SWT was the natural choice when work began on Eclipse. The use of SWT over\nSwing in Eclipse split the Java community. Some saw conspiracies, but Eclipse was a success and\nthe use of SWT differentiated it from other Java programs. Eclipse was performant, pixel perfect\nand the general sentiment was, \"I can't believe it's a Java program.\"\nEarly Eclipse SDKs ran on Linux and Windows. In 2010, there is support for over a dozen\nplatforms. A developer can write an application for one platform, and deploy it to multiple\nplatforms. Developing a new widget toolkit for Java was a contentious issue within the Java\ncommunity at the time, but the Eclipse committers felt that it was worth the effort to provide the\nbest native experience on the desktop. This assertion applies today, and there are millions of lines\nof code that depend on SWT.\nJFace is a layer on top of SWT that provides tools for common UI programming tasks, such as\nframeworks for preferences and wizards. Like SWT, it was designed to work with many windowing\nsystems. However, it is pure Java code and doesn't contain any native platform code.\nThe platform also provided an integrated help system based upon small units of information called\ntopics. A topic consists of a label and a reference to its location. The location can be an HTML\n", "page": 87, "type": "text", "section": "Page 87"}
{"text": "documentation file, or an XML document describing additional links. Topics are grouped together\nin table of contents (TOCs). Consider the topics as the leaves, and TOCs as the branches of\norganization. To add help content to your application, you can contribute to the\norg.eclipse.help.toc extension point, as the org.eclipse.platform.doc.isv\nplugin.xml does below.\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?eclipse version=\"3.0\"?>\n<plugin>\n<!-- ===================================================================== -->\n<!-- Define primary TOC                                                    -->\n<!-- ===================================================================== -->\n   <extension\n         point=\"org.eclipse.help.toc\">\n      <toc\n            file=\"toc.xml\"\n            primary=\"true\">\n      </toc>\n      <index path=\"index\"/>\n   </extension>\n<!-- ===================================================================== -->\n<!-- Define TOCs                                                           -->\n<!-- ===================================================================== -->\n   <extension\n         point=\"org.eclipse.help.toc\">\n      <toc\n            file=\"topics_Guide.xml\">\n      </toc>\n      <toc\n            file=\"topics_Reference.xml\">\n      </toc>\n      <toc\n            file=\"topics_Porting.xml\">\n      </toc>\n      <toc\n            file=\"topics_Questions.xml\">\n      </toc>\n      <toc\n            file=\"topics_Samples.xml\">\n      </toc>\n   </extension>\nApache Lucene is used to index and search the online help content. In early versions of Eclipse,\nonline help was served as a Tomcat web application. Additionally, by providing help within Eclipse\nitself, you can also use the subset of help plugins to provide a standalone help server.3\nEclipse also provides team support to interact with a source code repository, create patches and\nother common tasks. The workspace provided collection of files and metadata that stored your\nwork on the filesystem. There was also a debugger to trace problems in the Java code, as well as a\nframework for building language specific debuggers.\nOne of the goals of the Eclipse project was to encourage open source and commercial consumers\nof this technology to extend the platform to meet their needs, and one way to encourage this\nadoption is to provide a stable API. An API can be thought of as a technical contract specifying the\nbehavior of your application. It also can be thought of as a social contract. On the Eclipse project,\nthe mantra is, \"API is forever\". Thus careful consideration must be given when writing an API given\nthat it is meant to be used indefinitely. A stable API is a contract between the client or API\nconsumer and the provider. This contract ensures that the client can depend on the Eclipse\nplatform to provide the API for the long term without the need for painful refactoring on the part\nof the client. A good API is also flexible enough to allow the implementation to evolve.\n", "page": 88, "type": "text", "section": "Page 88"}
{"text": "7.1.2. Java Development Tools (JDT)\nThe JDT provides Java editors, wizards, refactoring support, debugger, compiler and an\nincremental builder. The compiler is also used for content assist, navigation and other editing\nfeatures. A Java SDK isn't shipped with Eclipse so it's up to the user to choose which SDK to install\non their desktop. Why did the JDT team write a separate compiler to compile your Java code within\nEclipse? They had an initial compiler code contribution from VisualAge Micro Edition. They planned\nto build tooling on top of the compiler, so writing the compiler itself was a logical decision. This\napproach also allowed the JDT committers to provide extension points for extending the compiler.\nThis would be difficult if the compiler was a command line application provided by a third party.\nWriting their own compiler provided a mechanism to provide support for an incremental builder\nwithin the IDE. An incremental builder provides better performance because it only recompiles files\nthat have changed or their dependencies. How does the incremental builder work? When you\ncreate a Java project within Eclipse, you are creating resources in the workspace to store your\nfiles. A builder within Eclipse takes the inputs within your workspace (.java files), and creates an\noutput (.class files). Through the build state, the builder knows about the types (classes or\ninterfaces) in the workspace, and how they reference each other. The build state is provided to\nthe builder by the compiler each time a source file is compiled. When an incremental build is\ninvoked, the builder is supplied with a resource delta, which describes any new, modified or\ndeleted files. Deleted source files have their corresponding class files deleted. New or modified\ntypes are added to a queue. The files in the queue are compiled in sequence and compared with\nthe old class file to determine if there are structural changes. Structural changes are modifications\nto the class that can impact another type that references it. For example, changing a method\nsignature, or adding or removing a method. If there are structural changes, all the types that\nreference it are also added to the queue. If the type has changed at all, the new class file is written\nto the build output folder. The build state is updated with reference information for the compiled\ntype. This process is repeated for all the types in the queue until empty. If there are compilation\nerrors, the Java editor will create problem markers. Over the years, the tooling that JDT provides\nhas expanded tremendously in concert with new versions of the Java runtime itself.\n7.1.3. Plug-in Development Environment (PDE)\nThe Plug-in Development Environment (PDE) provided the tooling to develop, build, deploy and test\nplugins and other artifacts that are used to extend the functionality of Eclipse. Since Eclipse\nplugins were a new type of artifact in the Java world there wasn't a build system that could\ntransform the source into plugins. Thus the PDE team wrote a component called PDE Build which\nexamined the dependencies of the plugins and generated Ant scripts to construct the build\nartifacts.\n7.2. Eclipse 3.0: Runtime, RCP and Robots\n7.2.1. Runtime\nEclipse 3.0 was probably one of the most important Eclipse releases due to the number of\nsignificant changes that occurred during this release cycle. In the pre-3.0 Eclipse architecture, the\nEclipse component model consisted of plugins that could interact with each other in two ways.\nFirst, they could express their dependencies by the use of the requires statement in their\nplugin.xml. If plugin A requires plugin B, plugin A can see all the Java classes and resources\nfrom B, respecting Java class visibility conventions. Each plugin had a version, and they could also\nspecify the versions of their dependencies. Secondly, the component model provided extensions\nand extension points. Historically, Eclipse committers wrote their own runtime for the Eclipse SDK\nto manage classloading, plugin dependencies and extensions and extension points.\nThe Equinox project was created as a new incubator project at Eclipse. The goal of the Equinox\nproject was to replace the Eclipse component model with one that already existed, as well as\nprovide support for dynamic plugins. The solutions under consideration included JMX, Jakarta\nAvalon and OSGi. JMX was not a fully developed component model so it was not deemed\nappropriate. Jakarta Avalon wasn't chosen because it seemed to be losing momentum as a\nproject. In addition to the technical requirements, it was also important to consider the community\nthat supported these technologies. Would they be willing to incorporate Eclipse-specific changes?\n", "page": 89, "type": "text", "section": "Page 89"}
{"text": "Was it actively developed and gaining new adopters? The Equinox team felt that the community\naround their final choice of technology was just as important as the technical considerations.\nAfter researching and evaluating the available alternatives, the committers selected OSGi. Why\nOSGi? It had a semantic versioning scheme for managing dependencies. It provided a framework\nfor modularity that the JDK itself lacked. Packages that were available to other bundles must be\nexplicitly exported, and all others were hidden. OSGi provided its own classloader so the Equinox\nteam didn't have to continue to maintain their own. By standardizing on a component model that\nhad wider adoption outside the Eclipse ecosystem, they felt they could appeal to a broader\ncommunity and further drive the adoption of Eclipse.\nThe Equinox team felt comfortable that since OSGi already had an existing and vibrant community,\nthey could work with that community to help include the functionality that Eclipse required in a\ncomponent model. For instance, at the time, OSGi only supported listing requirements at a\npackage level, not a plugin level as Eclipse required. In addition, OSGi did not yet include the\nconcept of fragments, which were Eclipse's preferred mechanism for supplying platform or\nenvironment specific code to an existing plugin. For example, fragments provide code for working\nwith Linux and Windows filesystems as well as fragments which contribute language translations.\nOnce the decision was made to proceed with OSGi as the new runtime, the committers needed an\nopen source framework implementation. They evaluated Oscar, the precursor to Apache Felix,\nand the Service Management Framework (SMF) developed by IBM. At the time, Oscar was a\nresearch project with limited deployment. SMF was ultimately chosen since it was already used in\nshipping products and thus was deemed enterprise-ready. The Equinox implementation serves as\nthe reference implementation of the OSGi specification.\nA compatibility layer was also provided so that existing plugins would still work in a 3.0 install.\nAsking developers to rewrite their plugins to accommodate changes in the underlying\ninfrastructure of Eclipse 3.0 would have stalled the momentum on Eclipse as a tooling platform.\nThe expectation from Eclipse consumers was that the platform should just continue to work.\nWith the switch to OSGi, Eclipse plugins became known as bundles. A plugin and a bundle are the\nsame thing: They both provide a modular subset of functionality that describes itself with\nmetadata in a manifest. Previously, dependencies, exported packages and the extensions and\nextension points were described in plugin.xml. With the move to OSGi bundles, the extensions\nand extension points continued to be described in plugin.xml since they are Eclipse concepts.\nThe remaining information was described in the META-INF/MANIFEST.MF, OSGi's version of the\nbundle manifest. To support this change, PDE provided a new manifest editor within Eclipse. Each\nbundle has a name and version. The manifest for the org.eclipse.ui bundle looks like this:\nManifest-Version: 1.0\nBundle-ManifestVersion: 2\nBundle-Name: %Plugin.name\nBundle-SymbolicName: org.eclipse.ui; singleton:=true\nBundle-Version: 3.3.0.qualifier\nBundle-ClassPath: .\nBundle-Activator: org.eclipse.ui.internal.UIPlugin\nBundle-Vendor: %Plugin.providerName\nBundle-Localization: plugin\nExport-Package: org.eclipse.ui.internal;x-internal:=true\nRequire-Bundle: org.eclipse.core.runtime;bundle-version=\"[3.2.0,4.0.0)\",\n org.eclipse.swt;bundle-version=\"[3.3.0,4.0.0)\";visibility:=reexport,\n org.eclipse.jface;bundle-version=\"[3.3.0,4.0.0)\";visibility:=reexport,\n org.eclipse.ui.workbench;bundle-version=\"[3.3.0,4.0.0)\";visibility:=reexport,\n org.eclipse.core.expressions;bundle-version=\"[3.3.0,4.0.0)\"\nEclipse-LazyStart: true\nBundle-RequiredExecutionEnvironment: CDC-1.0/Foundation-1.0, J2SE-1.3\nAs of Eclipse 3.1, the manifest can also specify a bundle required execution environment (BREE).\nExecution environments specify the minimum Java environment required for the bundle to run.\nThe Java compiler does not understand bundles and OSGi manifests. PDE provides tooling for\ndeveloping OSGi bundles. Thus, PDE parses the bundle's manifest, and generates the classpath\nfor that bundle. If you specified an execution environment of J2SE-1.4 in your manifest, and then\nwrote some code that included generics, you would be advised of compile errors in your code.\n", "page": 90, "type": "text", "section": "Page 90"}
{"text": "wrote some code that included generics, you would be advised of compile errors in your code.\nThis ensures that your code adheres to the contract you have specified in the manifest.\nOSGi provides a modularity framework for Java. The OSGi framework manages collections of self-\ndescribing bundles and manages their classloading. Each bundle has its own classloader. The\nclasspath available to a bundle is constructed by examining the dependencies of the manifest and\ngenerating a classpath available to the bundle. OSGi applications are collections of bundles. In\norder to fully embrace of modularity, you must be able to express your dependencies in a reliable\nformat for consumers. Thus the manifest describes exported packages that are available to\nclients of this bundle which corresponds to the public API that was available for consumption. The\nbundle that is consuming that API must have a corresponding import of the package they are\nconsuming. The manifest also allows you to express version ranges for your dependencies.\nLooking at the Require-Bundle heading in the above manifest, you will note that the\norg.eclipse.core.runtime bundle that org.eclipse.ui depends on must be at least 3.2.0\nand less than 4.0.0.\nFigure 7.4: OSGi Bundle Lifecycle\nOSGi is a dynamic framework which supports the installation, starting, stopping, or uninstallation\nof bundles. As mentioned before, lazy activation was a core advantage to Eclipse because plugin\nclasses were not loaded until they were needed. The OSGi bundle lifecycle also enables this\napproach. When you start an OSGi application, the bundles are in the installed state. If its\ndependencies are met, the bundle changes to the resolved state. Once resolved, the classes\nwithin that bundle can be loaded and run. The starting state means that the bundle is being\nactivated according to its activation policy. Once activated, the bundle is in the active state, it can\nacquire required resources and interact with other bundles. A bundle is in the stopping state when\nit is executing its activator stop method to clean up any resources that were opened when it was\nactive. Finally, a bundle may be uninstalled, which means that it's not available for use.\nAs the API evolves, there needs to be a way to signal changes to your consumers. One approach\nis to use semantic versioning of your bundles and version ranges in your manifests to specify the\nversion ranges for your dependencies. OSGi uses a four-part versioning naming scheme as\nshown in Figure 7.5.\nFigure 7.5: Versioning Naming Scheme\n", "page": 91, "type": "text", "section": "Page 91"}
{"text": "With the OSGi version numbering scheme, each bundle has a unique identifier consisting of a\nname and a four part version number. An id and version together denote a unique set of bytes to\nthe consumer. By Eclipse convention, if you're making changes to a bundle, each segment of the\nversion signifies to the consumer the type of change being made. Thus, if you want to indicate\nthat you intend to break API, you increment the first (major) segment. If you have just added API,\nyou increment the second (minor) segment. If you fix a small bug that doesn't impact API, the\nthird (service) segment is incremented. Finally, the fourth or qualifier segment is incremented to\nindicate a build id source control repository tag.\nIn addition to expressing the fixed dependencies between bundles, there is also a mechanism\nwithin OSGi called services which provides further decoupling between bundles. Services are\nobjects with a set of properties that are registered with the OSGi service registry. Unlike\nextensions, which are registered in the extension registry when Eclipse scans bundles during\nstartup, services are registered dynamically. A bundle that is consuming a service needs to import\nthe package defining the service contract, and the framework determines the service\nimplementation from the service registry.\nLike a main method in a Java class file, there is a specific application defined to start Eclipse. Eclipse\napplications are defined using extensions. For instance, the application to start the Eclipse IDE\nitself is org.eclipse.ui.ide.workbench which is defined in the\norg.eclipse.ui.ide.application bundle.\n<plugin>\n    <extension\n         id=\"org.eclipse.ui.ide.workbench\"\n         point=\"org.eclipse.core.runtime.applications\">\n      <application>\n         <run\n               class=\"org.eclipse.ui.internal.ide.application.IDEApplication\">\n         </run>\n      </application>\n  </extension>\n</plugin>\nThere are many applications provided by Eclipse such as those to run standalone help servers,\nAnt tasks, and JUnit tests.\n7.2.2. Rich Client Platform (RCP)\nOne of the most interesting things about working in an open source community is that people use\nthe software in totally unexpected ways. The original intent of Eclipse was to provide a platform\nand tooling to create and extend IDEs. However, in the time leading up to the 3.0 release, bug\nreports revealed that the community was taking a subset of the platform bundles and using them\nto build Rich Client Platform (RCP) applications, which many people would recognize as Java\napplications. Since Eclipse was initially constructed with an IDE-centric focus, there had to be\nsome refactoring of the bundles to allow this use case to be more easily adopted by the user\ncommunity. RCP applications didn't require all the functionality in the IDE, so several bundles were\nsplit into smaller ones that could be consumed by the community for building RCP applications.\nExamples of RCP applications in the wild include the use of RCP to monitor the Mars Rover robots\ndeveloped by NASA at the Jet Propulsion Laboratory, Bioclipse for data visualization of\nbioinformatics and Dutch Railway for monitoring train performance. The common thread that ran\nthrough many of these applications was that these teams decided that they could take the utility\nprovided by the RCP platform and concentrate on building their specialized tools on top of it. They\ncould save development time and money by focusing on building their tools on a platform with a\nstable API that guaranteed that their technology choice would have long term support.\n", "page": 92, "type": "text", "section": "Page 92"}
{"text": "Figure 7.6: Eclipse 3.0 Architecture\nLooking at the 3.0 architecture in Figure 7.6, you will note that the Eclipse Runtime still exists to\nprovide the application model and extension registry. Managing the dependencies between\ncomponents, the plugin model is now managed by OSGi. In addition to continuing to be able to\nextend Eclipse for their own IDEs, consumers can also build upon the RCP application framework\nfor more generic applications.\n7.3. Eclipse 3.4\nThe ability to easily update an application to a new version and add new content is taken for\ngranted. In Firefox it happens seamlessly. For Eclipse it hasn't been so easy. Update Manager was\nthe original mechanism that was used to add new content to the Eclipse install or update to a new\nversion.\nTo understand what changes during an update or install operation, it's necessary to understand\nwhat Eclipse means by \"features\". A feature is a PDE artifact that defines a set of bundles that are\npackaged together in a format that can be built or installed. Features can also include other\nfeatures. (See Figure 7.7.)\n", "page": 93, "type": "text", "section": "Page 93"}
{"text": "Figure 7.7: Eclipse 3.3 SDK Feature Hierarchy\nIf you wished to update your Eclipse install to a new build that only incorporated one new bundle,\nthe entire feature had to be updated since this was the coarse grained mechanism that was used\nby update manager. Updating a feature to fix a single bundle is inefficient.\nThere are PDE wizards to create features, and build them in your workspace. The feature.xml\nfile defines the bundles included in the feature, and some simple properties of the bundles. A\nfeature, like a bundle, has a name and a version. Features can include other features, and specify\nversion ranges for the features they include. The bundles that are included in a feature are listed,\nalong with specific properties. For instance, you can see that the\norg.eclipse.launcher.gtk.linux.x86_64 fragment specifies the operating system (os),\nwindowing system (ws) and architecture (arch) where it should be used. Thus upgrading to a new\nrelease, this fragment would only be installed on this platform. These platform filters are included\nin the OSGi manifest of this bundle.\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feature\n      id=\"org.eclipse.rcp\"\n      label=\"%featureName\"\n      version=\"3.7.0.qualifier\"\n      provider-name=\"%providerName\"\n      plugin=\"org.eclipse.rcp\"\n      image=\"eclipse_update_120.jpg\">\n   <description>\n      %description\n   </description>\n   <copyright>\n      %copyright\n   </copyright>\n   <license url=\"%licenseURL\">\n      %license\n   </license>\n   <plugin\n         id=\"org.eclipse.equinox.launcher\"\n", "page": 94, "type": "text", "section": "Page 94"}
{"text": "         download-size=\"0\"\n         install-size=\"0\"\n         version=\"0.0.0\"\n         unpack=\"false\"/>\n   <plugin\n         id=\"org.eclipse.equinox.launcher.gtk.linux.x86_64\"\n         os=\"linux\"\n         ws=\"gtk\"\n         arch=\"x86_64\"\n         download-size=\"0\"\n         install-size=\"0\"\n         version=\"0.0.0\"\n         fragment=\"true\"/>\nAn Eclipse application consists of more than just features and bundles. There are platform specific\nexecutables to start Eclipse itself, license files, and platform specific libraries, as shown in this list\nof files included in the Eclipse application.\ncom.ibm.icu\norg.eclipse.core.commands\norg.eclipse.core.conttenttype\norg.eclipse.core.databinding\norg.eclipse.core.databinding.beans\norg.eclipse.core.expressions\norg.eclipse.core.jobs\norg.eclipse.core.runtime\norg.eclipse.core.runtime.compatibility.auth\norg.eclipse.equinox.common\norg.eclipse.equinox.launcher\norg.eclipse.equinox.launcher.carbon.macosx\norg.eclipse.equinox.launcher.gtk.linux.ppc\norg.eclipse.equinox.launcher.gtk.linux.s390\norg.eclipse.equinox.launcher.gtk.linux.s390x\norg.eclipse.equinox.launcher.gtk.linux.x86\norg.eclipse.equinox.launcher.gtk.linux.x86_64\nThese files couldn't be updated via update manager, because again, it only dealt with features.\nSince many of these files were updated every major release, this meant that users had to\ndownload a new zip each time there was a new release instead of updating their existing install.\nThis wasn't acceptable to the Eclipse community. PDE provided support for product files, which\nspecified all the files needed to build an Eclipse RCP application. However, update manager didn't\nhave a mechanism to provision these files into your install which was very frustrating for users\nand product developers alike. In March 2008, p2 was released into the SDK as the new\nprovisioning solution. In the interest of backward compatibility, Update Manager was still available\nfor use, but p2 was enabled by default.\n7.3.1. p2 Concepts\nEquinox p2 is all about installation units (IU). An IU is a description of the name and id of the\nartifact you are installing. This metadata also describes the capabilities of the artifact (what is\nprovided) and its requirements (its dependencies). Metadata can also express applicability filters if\nan artifact is only applicable to a certain environment. For instance, the\norg.eclipse.swt.gtk.linux.x86 fragment is only applicable if you're installing on a Linux gtk x86\nmachine. Fundamentally, metadata is an expression of the information in the bundle's manifest.\nArtifacts are simply the binary bits being installed. A separation of concerns is achieved by\nseparating the metadata and the artifacts that they describe. A p2 repository consists of both\nmetadata and artifact repositories.\n", "page": 95, "type": "text", "section": "Page 95"}
{"text": "Figure 7.8: P2 Concepts\nA profile is a list of IUs in your install. For instance, your Eclipse SDK has a profile that describes\nyour current install. From within Eclipse, you can request an update to a newer version of the build\nwhich will create a new profile with a different set of IUs. A profile also provides a list of properties\nassociated with the installation, such as the operating system, windowing system, and\narchitecture parameters. Profiles also store the installation directory and the location. Profiles are\nheld by a profile registry, which can store multiple profiles. The director is responsible for invoking\nprovisioning operations. It works with the planner and the engine. The planner examines the\nexisting profile, and determines the operations that must occur to transform the install into its\nnew state. The engine is responsible for carrying out the actual provisioning operations and\ninstalling the new artifacts on disk. Touchpoints are part of the engine that work with the runtime\nimplementation of the system being installed. For instance, for the Eclipse SDK, there is an Eclipse\ntouchpoint which knows how to install bundles. For a Linux system where Eclipse is installed from\nRPM binaries, the engine would deal with an RPM touchpoint. Also, p2 can perform installs in-\nprocess or outside in a separate process, such as a build.\nThere were many benefits to the new p2 provisioning system. Eclipse install artifacts could be\nupdated from release to release. Since previous profiles were stored on disk, there was also a way\nto revert to a previous Eclipse install. Additionally, given a profile and a repository, you could\nrecreate the Eclipse install of a user that was reporting a bug to try to reproduce the problem on\nyour own desktop. Provisioning with p2 provided a way to update and install more than just the\nEclipse SDK, it was a platform that applied to RCP and OSGi use cases as well. The Equinox team\nalso worked with the members of another Eclipse project, the Eclipse Communication Framework\n(ECF) to provide reliable transport for consuming artifacts and metadata in p2 repositories.\nThere were many spirited discussions within the Eclipse community when p2 was released into the\nSDK. Since update manager was a less than optimal solution for provisioning your Eclipse install,\nEclipse consumers had the habit of unzipping bundles into their install and restarting Eclipse. This\napproach resolves your bundles on a best effort basis. It also meant that any conflicts in your\ninstall were being resolved at runtime, not install time. Constraints should be resolved at install\ntime, not run time. However, users were often oblivious to these issues and assumed since the\nbundles existed on disk, they were working. Previously, the update sites that Eclipse provided\nwere a simple directory consisting of JARred bundles and features. A simple site.xml file\nprovided the names of the features that were available to be consumed in the site. With the\nadvent of p2, the metadata that was provided in the p2 repositories was much more complex. To\ncreate metadata, the build process needed to be tweaked to either generate metadata at build\ntime or run a generator task over the existing bundles. Initially, there was a lack of documentation\navailable describing how to make these changes. As well, as is always the case, exposing new\n", "page": 96, "type": "text", "section": "Page 96"}
{"text": "technology to a wider audience exposed unexpected bugs that had to be addressed. However, by\nwriting more documentation and working long hours to address these bugs, the Equinox team\nwas able to address these concerns and now p2 is the underlying provision engine behind many\ncommercial offerings. As well, the Eclipse Foundation ships its coordinated release every year\nusing a p2 aggregate repository of all the contributing projects.\n7.4. Eclipse 4.0\nArchitecture must continually be examined to evaluate if it is still appropriate. Is it able to\nincorporate new technology? Does it encourage growth of the community? Is it easy to attract\nnew contributors? In late 2007, the Eclipse project committers decided that the answers to these\nquestions were no and they embarked on designing a new vision for Eclipse. At the same time,\nthey realized that there were thousands of Eclipse applications that depended on the existing API.\nAn incubator technology project was created in late 2008 with three specific goals: simplify the\nEclipse programming model, attract new committers and enable the platform to take advantage of\nnew web-based technologies while providing an open architecture.\nFigure 7.9: Eclipse 4.0 SDK Early Adopter Release\nEclipse 4.0 was first released in July 2010 for early adopters to provide feedback. It consisted of a\ncombination of SDK bundles that were part of the 3.6 release, and new bundles that graduated\nfrom the technology project. Like 3.0, there was a compatibility layer so that existing bundles\ncould work with the new release. As always, there was the caveat that consumers needed to be\nusing the public API in order to be assured of that compatibility. There was no such guarantee if\nyour bundle used internal code. The 4.0 release provided the Eclipse 4 Application Platform which\nprovided the following features.\n7.4.1. Model Workbench\nIn 4.0, a model workbench is generated using the Eclipse Modeling Framework (EMFgc). There is a\nseparation of concerns between the model and the rendering of the view, since the renderer talks\nto the model and then generates the SWT code. The default is to use the SWT renderers, but\nother solutions are possible. If you create an example 4.x application, an XMI file will be created for\nthe default workbench model. The model can be modified and the workbench will be instantly\nupdated to reflect the changes in the model. Figure 7.10 is an example of a model generated for\nan example 4.x application.\n", "page": 97, "type": "text", "section": "Page 97"}
{"text": "Figure 7.10: Model Generated for Example 4.x Application\n7.4.2. Cascading Style Sheets Styling\nEclipse was released in 2001, before the era of rich Internet applications that could be skinned via\nCSS to provide a different look and feel. Eclipse 4.0 provides the ability to use stylesheets to easily\nchange the look and feel of the Eclipse application. The default CSS stylesheets can be found in the\ncss folder of the org.eclipse.platform bundle.\n7.4.3. Dependency Injection\nBoth the Eclipse extensions registry and OSGi services are examples of service programming\nmodels. By convention, a service programming model contains service producers and consumers.\nThe broker is responsible for managing the relationship between producers and consumers.\nFigure 7.11: Relationship Between Producers and Consumers\nTraditionally, in Eclipse 3.4.x applications, the consumer needed to know the location of the\nimplementation, and to understand inheritance within the framework to consume services. The\nconsumer code was therefore less reusable because people couldn't override which\nimplementation the consumer receives. For example, if you wanted to update the message on the\nstatus line in Eclipse 3.x, the code would look like:\ngetViewSite().getActionBars().getStatusLineManager().setMessage(msg);\nEclipse 3.6 is built from components, but many of these components are too tightly coupled. To\nassemble applications of more loosely coupled components, Eclipse 4.0 uses dependency injection\nto provide services to clients. Dependency injection in Eclipse 4.x is through the use of a custom\nframework that uses the the concept of a context that serves as a generic mechanism to locate\nservices for consumers. The context exists between the application and the framework. Contexts\nare hierarchical. If a context has a request that cannot be satisfied, it will delegate the request to\nthe parent context. The Eclipse context, called IEclipseContext, stores the available services\nand provides OSGi services lookup. Basically, the context is similar to a Java map in that it provides\na mapping of a name or class to an object. The context handles model elements and services.\n", "page": 98, "type": "text", "section": "Page 98"}
{"text": "Every element of the model, will have a context. Services are published in 4.x by means of the\nOSGi service mechanism.\nFigure 7.12: Service Broker Context\nProducers add services and objects to the context which stores them. Services are injected into\nconsumer objects by the context. The consumer declares what it wants, and the context\ndetermines how to satisfy this request. This approach has made consuming dynamic service\neasier. In Eclipse 3.x, a consumer had to attach listeners to be notified when services were\navailable or unavailable. With Eclipse 4.x, once a context has been injected into a consumer object,\nany change is automatically delivered to that object again. In other words, dependency injection\noccurs again. The consumer indicates that it will use the context by the use of Java 5 annotations\nwhich adhere to the JSR 330 standard, such as @inject, as well as some custom Eclipse\nannotations. Constructor, method, and field injection are supported. The 4.x runtime scans the\nobjects for these annotations. The action that is performed depends on the annotation that's\nfound.\nThis separation of concerns between context and application allows for better reuse of\ncomponents, and absolves the consumer from understanding the implementation. In 4.x, the\ncode to update the status line would look like this:\n@Inject\nIStatusLineManager statusLine;\n\u22ee    \u22ee    \u22ee\nstatusLine.setMessage(msg);\n7.4.4. Application Services\nOne of the main goals in Eclipse 4.0 was to simplify the API for consumers so that it was easy to\nimplement common services. The list of simple services came to be known as \"the twenty things\"\nand are known as the Eclipse Application services. The goal is to offer standalone APIs that clients\ncan use without having to have a deep understanding of all the APIs available. They are structured\nas individual services so that they can also be used in other languages other than Java, such as\nJavascript. For example, there is an API to access the application model, to read and modify\npreferences and report errors and warnings.\n", "page": 99, "type": "text", "section": "Page 99"}
{"text": "7.5. Conclusion\nThe component-based architecture of Eclipse has evolved to incorporate new technology while\nmaintaining backward compatibility. This has been costly, but the reward is the growth of the\nEclipse community because of the trust established that consumers can continue to ship\nproducts based on a stable API.\nEclipse has so many consumers with diverse use cases and our expansive API became difficult for\nnew consumers to adopt and understand. In retrospect, we should have kept our API simpler. If\n80% of consumers only use 20% of the API, there is a need for simplification which was one of\nthe reasons that the Eclipse 4.x stream was created.\nThe wisdom of crowds does reveal interesting use cases, such as disaggregating the IDE into\nbundles that could be used to construct RCP applications. Conversely, crowds often generate a lot\nof noise with requests for edge case scenarios that take a significant amount of time to\nimplement.\nIn the early days of the Eclipse project, committers had the luxury of dedicating significant\namounts of time to documentation, examples and answering community questions. Over time,\nthis responsibility has shifted to the Eclipse community as a whole. We could have been better at\nproviding documentation and use cases to help out the community, but this has been difficult\ngiven the large number of items planned for every release. Contrary to the expectation that\nsoftware release dates slip, at Eclipse we consistently deliver our releases on time which allows our\nconsumers to trust that they will be able to do the same.\nBy adopting new technology, and reinventing how Eclipse looks and works, we continue the\nconversation with our consumers and keep them engaged in the community. If you're interested\nin becoming involved with Eclipse, please visit http://www.eclipse.org.\nFootnotes\n1. http://www.eclipse.org\n2. http://www.eclipse.org/equinox\n3. For example: http://help.eclipse.org.\n", "page": 100, "type": "text", "section": "Page 100"}
{"text": "The Architecture of\nOpen Source Applications\nAmy Brown and Greg Wilson (eds.)\nISBN 978-1-257-63801-7 \nLicense / Buy / Contribute\nChapter 8. Graphite\nChris Davis\nGraphite1 performs two pretty simple tasks: storing numbers that change over time and graphing\nthem. There has been a lot of software written over the years to do these same tasks. What\nmakes Graphite unique is that it provides this functionality as a network service that is both easy\nto use and highly scalable. The protocol for feeding data into Graphite is simple enough that you\ncould learn to do it by hand in a few minutes (not that you'd actually want to, but it's a decent\nlitmus test for simplicity). Rendering graphs and retrieving data points are as easy as fetching a\nURL. This makes it very natural to integrate Graphite with other software and enables users to\nbuild powerful applications on top of Graphite. One of the most common uses of Graphite is\nbuilding web-based dashboards for monitoring and analysis. Graphite was born in a high-volume\ne-commerce environment and its design reflects this. Scalability and real-time access to data are\nkey goals.\nThe components that allow Graphite to achieve these goals include a specialized database library\nand its storage format, a caching mechanism for optimizing I/O operations, and a simple yet\neffective method of clustering Graphite servers. Rather than simply describing how Graphite\nworks today, I will explain how Graphite was initially implemented (quite naively), what problems I\nran into, and how I devised solutions to them.\n8.1. The Database Library: Storing Time-Series Data\nGraphite is written entirely in Python and consists of three major components: a database library\nnamed whisper, a back-end daemon named carbon, and a front-end webapp that renders\ngraphs and provides a basic UI. While whisper was written specifically for Graphite, it can also be\nused independently. It is very similar in design to the round-robin-database used by RRDtool, and\nonly stores time-series numeric data. Usually we think of databases as server processes that\nclient applications talk to over sockets. However, whisper, much like RRDtool, is a database\nlibrary used by applications to manipulate and retrieve data stored in specially formatted files. The\nmost basic whisper operations are create to make a new whisper file, update to write new\ndata points into a file, and fetch to retrieve data points.\nFigure 8.1: Basic Anatomy of a whisper File\nAs shown in Figure 8.1, whisper files consist of a header section containing various metadata,\nfollowed by one or more archive sections . Each archive is a sequence of consecutive data points\nwhich are (timestamp, value) pairs. When an update or fetch operation is performed,\nwhisper determines the offset in the file where data should be written to or read from, based on\nthe timestamp and the archive configuration.\n8.2. The Back End: A Simple Storage Service\nGraphite's back end is a daemon process called carbon-cache, usually simply referred to as\n", "page": 101, "type": "text", "section": "Page 101"}
{"text": "carbon. It is built on Twisted, a highly scalable event-driven I/O framework for Python. Twisted\nenables carbon to efficiently talk to a large number of clients and handle a large amount of traffic\nwith low overhead. Figure 8.2 shows the data flow among carbon, whisper and the webapp:\nClient applications collect data and send it to the Graphite back end, carbon, which stores the\ndata using whisper. This data can then be used by the Graphite webapp to generate graphs.\nFigure 8.2: Data Flow\nThe primary function of carbon is to store data points for metrics provided by clients. In Graphite\nterminology, a metric is any measurable quantity that can vary over time (like the CPU utilization of\na server or the number of sales of a product). A data point is simply a (timestamp, value) pair\ncorresponding to the measured value of a particular metric at a point in time. Metrics are uniquely\nidentified by their name, and the name of each metric as well as its data points are provided by\nclient applications. A common type of client application is a monitoring agent that collects system\nor application metrics, and sends its collected values to carbon for easy storage and visualization.\nMetrics in Graphite have simple hierarchical names, similar to filesystem paths except that a dot is\nused to delimit the hierarchy rather than a slash or backslash. carbon will respect any legal name\nand creates a whisper file for each metric to store its data points. The whisper files are stored\nwithin carbon's data directory in a filesystem hierarchy that mirrors the dot-delimited hierarchy in\neach metric's name, so that (for example) servers.www01.cpuUsage maps to\n\u2026/servers/www01/cpuUsage.wsp.\nWhen a client application wishes to send data points to Graphite it must establish a TCP\nconnection to carbon, usually on port 20032. The client does all the talking; carbon does not\nsend anything over the connection. The client sends data points in a simple plain-text format while\nthe connection may be left open and re-used as needed. The format is one line of text per data\npoint where each line contains the dotted metric name, value, and a Unix epoch timestamp\nseparated by spaces. For example, a client might send:\nservers.www01.cpuUsage 42 1286269200\nproducts.snake-oil.salesPerMinute 123 1286269200\n[one minute passes]\nservers.www01.cpuUsageUser 44 1286269260\nproducts.snake-oil.salesPerMinute 119 1286269260\nOn a high level, all carbon does is listen for data in this format and try to store it on disk as quickly\nas possible using whisper. Later on we will discuss the details of some tricks used to ensure\nscalability and get the best performance we can out of a typical hard drive.\n8.3. The Front End: Graphs On-Demand\nThe Graphite webapp allows users to request custom graphs with a simple URL-based API.\nGraphing parameters are specified in the query-string of an HTTP GET request, and a PNG image\nis returned in response. For example, the URL:\nhttp://graphite.example.com/render?target=servers.www01.cpuUsage&\n", "page": 102, "type": "text", "section": "Page 102"}
{"text": "width=500&height=300&from=-24h\nrequests a 500\u00d7300 graph for the metric servers.www01.cpuUsage and the past 24 hours of\ndata. Actually, only the target parameter is required; all the others are optional and use your\ndefault values if omitted.\nGraphite supports a wide variety of display options as well as data manipulation functions that\nfollow a simple functional syntax. For example, we could graph a 10-point moving average of the\nmetric in our previous example like this:\ntarget=movingAverage(servers.www01.cpuUsage,10)\nFunctions can be nested, allowing for complex expressions and calculations.\nHere is another example that gives the running total of sales for the day using per-product\nmetrics of sales-per-minute:\ntarget=integral(sumSeries(products.*.salesPerMinute))&amp;from=midnight\nThe sumSeries function computes a time-series that is the sum of each metric matching the\npattern products.*.salesPerMinute. Then integral computes a running total rather than a\nper-minute count. From here it isn't too hard to imagine how one might build a web UI for viewing\nand manipulating graphs. Graphite comes with its own Composer UI, shown in Figure 8.3, that\ndoes this using Javascript to modify the graph's URL parameters as the user clicks through\nmenus of the available features.\nFigure 8.3: Graphite's Composer Interface\n8.4. Dashboards\nSince its inception Graphite has been used as a tool for creating web-based dashboards. The URL\nAPI makes this a natural use case. Making a dashboard is as simple as making an HTML page full\nof tags like this:\n<img src=\"../http://graphite.example.com/render?parameters-for-my-awesome-graph\">\nHowever, not everyone likes crafting URLs by hand, so Graphite's Composer UI provides a point-\nand-click method to create a graph from which you can simply copy and paste the URL. When\ncoupled with another tool that allows rapid creation of web pages (like a wiki) this becomes easy\nenough that non-technical users can build their own dashboards pretty easily.\n8.5. An Obvious Bottleneck\n", "page": 103, "type": "text", "section": "Page 103"}
{"text": "Once my users started building dashboards, Graphite quickly began to have performance issues. I\ninvestigated the web server logs to see what requests were bogging it down. It was pretty\nobvious that the problem was the sheer number of graphing requests. The webapp was CPU-\nbound, rendering graphs constantly. I noticed that there were a lot of identical requests, and the\ndashboards were to blame.\nImagine you have a dashboard with 10 graphs in it and the page refreshes once a minute. Each\ntime a user opens the dashboard in their browser, Graphite has to handle 10 more requests per\nminute. This quickly becomes expensive.\nA simple solution is to render each graph only once and then serve a copy of it to each user. The\nDjango web framework (which Graphite is built on) provides an excellent caching mechanism that\ncan use various back ends such as memcached. Memcached3 is essentially a hash table provided\nas a network service. Client applications can get and set key-value pairs just like an ordinary hash\ntable. The main benefit of using memcached is that the result of an expensive request (like\nrendering a graph) can be stored very quickly and retrieved later to handle subsequent requests.\nTo avoid returning the same stale graphs forever, memcached can be configured to expire the\ncached graphs after a short period. Even if this is only a few seconds, the burden it takes off\nGraphite is tremendous because duplicate requests are so common.\nAnother common case that creates lots of rendering requests is when a user is tweaking the\ndisplay options and applying functions in the Composer UI. Each time the user changes\nsomething, Graphite must redraw the graph. The same data is involved in each request so it\nmakes sense to put the underlying data in the memcache as well. This keeps the UI responsive to\nthe user because the step of retrieving data is skipped.\n8.6. Optimizing I/O\nImagine that you have 60,000 metrics that you send to your Graphite server, and each of these\nmetrics has one data point per minute. Remember that each metric has its own whisper file on\nthe filesystem. This means carbon must do one write operation to 60,000 different files each\nminute. As long as carbon can write to one file each millisecond, it should be able to keep up. This\nisn't too far fetched, but let's say you have 600,000 metrics updating each minute, or your\nmetrics are updating every second, or perhaps you simply cannot afford fast enough storage.\nWhatever the case, assume the rate of incoming data points exceeds the rate of write operations\nthat your storage can keep up with. How should this situation be handled?\nMost hard drives these days have slow seek time4, that is, the delay between doing I/O operations\nat two different locations, compared to writing a contiguous sequence of data. This means the\nmore contiguous writing we do, the more throughput we get. But if we have thousands of files\nthat need to be written to frequently, and each write is very small (one whisper data point is only\n12 bytes) then our disks are definitely going to spend most of their time seeking.\nWorking under the assumption that the rate of write operations has a relatively low ceiling, the\nonly way to increase our data point throughput beyond that rate is to write multiple data points in\na single write operation. This is feasible because whisper arranges consecutive data points\ncontiguously on disk. So I added an update_many function to whisper, which takes a list of data\npoints for a single metric and compacts contiguous data points into a single write operation. Even\nthough this made each write larger, the difference in time it takes to write ten data points (120\nbytes) versus one data point (12 bytes) is negligible. It takes quite a few more data points before\nthe size of each write starts to noticeably affect the latency.\nNext I implemented a buffering mechanism in carbon. Each incoming data point gets mapped to a\nqueue based on its metric name and is then appended to that queue. Another thread repeatedly\niterates through all of the queues and for each one it pulls all of the data points out and writes\nthem to the appropriate whisper file with update_many. Going back to our example, if we have\n600,000 metrics updating every minute and our storage can only keep up with 1 write per\nmillisecond, then the queues will end up holding about 10 data points each on average. The only\nresource this costs us is memory, which is relatively plentiful since each data point is only a few\nbytes.\nThis strategy dynamically buffers as many datapoints as necessary to sustain a rate of incoming\n", "page": 104, "type": "text", "section": "Page 104"}
{"text": "datapoints that may exceed the rate of I/O operations your storage can keep up with. A nice\nadvantage of this approach is that it adds a degree of resiliency to handle temporary I/O\nslowdowns. If the system needs to do other I/O work outside of Graphite then it is likely that the\nrate of write operations will decrease, in which case carbon's queues will simply grow. The larger\nthe queues, the larger the writes. Since the overall throughput of data points is equal to the rate of\nwrite operations times the average size of each write, carbon is able to keep up as long as there\nis enough memory for the queues. carbon's queueing mechanism is depicted in Figure 8.4.\nFigure 8.4: Carbon's Queueing Mechanism\n8.7. Keeping It Real-Time\nBuffering data points was a nice way to optimize carbon's I/O but it didn't take long for my users\nto notice a rather troubling side effect. Revisiting our example again, we've got 600,000 metrics\nthat update every minute and we're assuming our storage can only keep up with 60,000 write\noperations per minute. This means we will have approximately 10 minutes worth of data sitting in\ncarbon's queues at any given time. To a user this means that the graphs they request from the\nGraphite webapp will be missing the most recent 10 minutes of data: Not good!\nFortunately the solution is pretty straight-forward. I simply added a socket listener to carbon that\nprovides a query interface for accessing the buffered data points and then modifies the Graphite\nwebapp to use this interface each time it needs to retrieve data. The webapp then combines the\ndata points it retrieves from carbon with the data points it retrieved from disk and voila, the\ngraphs are real-time. Granted, in our example the data points are updated to the minute and thus\nnot exactly \"real-time\", but the fact that each data point is instantly accessible in a graph once it is\nreceived by carbon is real-time.\n8.8. Kernels, Caches, and Catastrophic Failures\nAs is probably obvious by now, a key characteristic of system performance that Graphite's own\nperformance depends on is I/O latency. So far we've assumed our system has consistently low I/O\nlatency averaging around 1 millisecond per write, but this is a big assumption that requires a little\ndeeper analysis. Most hard drives simply aren't that fast; even with dozens of disks in a RAID\narray there is very likely to be more than 1 millisecond latency for random access. Yet if you were\nto try and test how quickly even an old laptop could write a whole kilobyte to disk you would find\nthat the write system call returns in far less than 1 millisecond. Why?\nWhenever software has inconsistent or unexpected performance characteristics, usually either\nbuffering or caching is to blame. In this case, we're dealing with both. The write system call doesn't\ntechnically write your data to disk, it simply puts it in a buffer which the kernel then writes to disk\nlater on. This is why the write call usually returns so quickly. Even after the buffer has been written\nto disk, it often remains cached for subsequent reads. Both of these behaviors, buffering and\ncaching, require memory of course.\nKernel developers, being the smart folks that they are, decided it would be a good idea to use\nwhatever user-space memory is currently free instead of allocating memory outright. This turns\nout to be a tremendously useful performance booster and it also explains why no matter how\nmuch memory you add to a system it will usually end up having almost zero \"free\" memory after\ndoing a modest amount of I/O. If your user-space applications aren't using that memory then your\nkernel probably is. The downside of this approach is that this \"free\" memory can be taken away\nfrom the kernel the moment a user-space application decides it needs to allocate more memory\n", "page": 105, "type": "text", "section": "Page 105"}
{"text": "for itself. The kernel has no choice but to relinquish it, losing whatever buffers may have been\nthere.\nSo what does all of this mean for Graphite? We just highlighted carbon's reliance on consistently\nlow I/O latency and we also know that the write system call only returns quickly because the data\nis merely being copied into a buffer. What happens when there is not enough memory for the\nkernel to continue buffering writes? The writes become synchronous and thus terribly slow! This\ncauses a dramatic drop in the rate of carbon's write operations, which causes carbon's queues\nto grow, which eats up even more memory, starving the kernel even further. In the end, this kind\nof situation usually results in carbon running out of memory or being killed by an angry sysadmin.\nTo avoid this kind of catastrophe, I added several features to carbon including configurable limits\non how many data points can be queued and rate-limits on how quickly various whisper\noperations can be performed. These features can protect carbon from spiraling out of control\nand instead impose less harsh effects like dropping some data points or refusing to accept more\ndata points. However, proper values for those settings are system-specific and require a fair\namount of testing to tune. They are useful but they do not fundamentally solve the problem. For\nthat, we'll need more hardware.\n8.9. Clustering\nMaking multiple Graphite servers appear to be a single system from a user perspective isn't\nterribly difficult, at least for a na\u00efve implementation. The webapp's user interaction primarily\nconsists of two operations: finding metrics and fetching data points (usually in the form of a\ngraph). The find and fetch operations of the webapp are tucked away in a library that abstracts\ntheir implementation from the rest of the codebase, and they are also exposed through HTTP\nrequest handlers for easy remote calls.\nThe find operation searches the local filesystem of whisper data for things matching a user-\nspecified pattern, just as a filesystem glob like *.txt matches files with that extension. Being a\ntree structure, the result returned by find is a collection of Node objects, each deriving from\neither the Branch or Leaf sub-classes of Node. Directories correspond to branch nodes and\nwhisper files correspond to leaf nodes. This layer of abstraction makes it easy to support\ndifferent types of underlying storage including RRD files5 and gzipped whisper files.\nThe Leaf interface defines a fetch method whose implementation depends on the type of leaf\nnode. In the case of whisper files it is simply a thin wrapper around the whisper library's own\nfetch function. When clustering support was added, the find function was extended to be able to\nmake remote find calls via HTTP to other Graphite servers specified in the webapp's configuration.\nThe node data contained in the results of these HTTP calls gets wrapped as RemoteNode objects\nwhich conform to the usual Node, Branch, and Leaf interfaces. This makes the clustering\ntransparent to the rest of the webapp's codebase. The fetch method for a remote leaf node is\nimplemented as another HTTP call to retrieve the data points from the node's Graphite server.\nAll of these calls are made between the webapps the same way a client would call them, except\nwith one additional parameter specifying that the operation should only be performed locally and\nnot be redistributed throughout the cluster. When the webapp is asked to render a graph, it\nperforms the find operation to locate the requested metrics and calls fetch on each to retrieve\ntheir data points. This works whether the data is on the local server, remote servers, or both. If a\nserver goes down, the remote calls timeout fairly quickly and the server is marked as being out of\nservice for a short period during which no further calls to it will be made. From a user standpoint,\nwhatever data was on the lost server will be missing from their graphs unless that data is\nduplicated on another server in the cluster.\n8.9.1. A Brief Analysis of Clustering Efficiency\nThe most expensive part of a graphing request is rendering the graph. Each rendering is\nperformed by a single server so adding more servers does effectively increase capacity for\nrendering graphs. However, the fact that many requests end up distributing find calls to every\nother server in the cluster means that our clustering scheme is sharing much of the front-end\nload rather than dispersing it. What we have achieved at this point, however, is an effective way to\ndistribute back-end load, as each carbon instance operates independently. This is a good first\n", "page": 106, "type": "text", "section": "Page 106"}
{"text": "step since most of the time the back end is a bottleneck far before the front end is, but clearly the\nfront end will not scale horizontally with this approach.\nIn order to make the front end scale more effectively, the number of remote find calls made by\nthe webapp must be reduced. Again, the easiest solution is caching. Just as memcached is already\nused to cache data points and rendered graphs, it can also be used to cache the results of find\nrequests. Since the location of metrics is much less likely to change frequently, this should\ntypically be cached for longer. The trade-off of setting the cache timeout for find results too\nlong, though, is that new metrics that have been added to the hierarchy may not appear as\nquickly to the user.\n8.9.2. Distributing Metrics in a Cluster\nThe Graphite webapp is rather homogeneous throughout a cluster, in that it performs the exact\nsame job on each server. carbon's role, however, can vary from server to server depending on\nwhat data you choose to send to each instance. Often there are many different clients sending\ndata to carbon, so it would be quite annoying to couple each client's configuration with your\nGraphite cluster's layout. Application metrics may go to one carbon server, while business metrics\nmay get sent to multiple carbon servers for redundancy.\nTo simplify the management of scenarios like this, Graphite comes with an additional tool called\ncarbon-relay. Its job is quite simple; it receives metric data from clients exactly like the standard\ncarbon daemon (which is actually named carbon-cache) but instead of storing the data, it\napplies a set of rules to the metric names to determine which carbon-cache servers to relay the\ndata to. Each rule consists of a regular expression and a list of destination servers. For each data\npoint received, the rules are evaluated in order and the first rule whose regular expression\nmatches the metric name is used. This way all the clients need to do is send their data to the\ncarbon-relay and it will end up on the right servers.\nIn a sense carbon-relay provides replication functionality, though it would more accurately be\ncalled input duplication since it does not deal with synchronization issues. If a server goes down\ntemporarily, it will be missing the data points for the time period in which it was down but\notherwise function normally. There are administrative scripts that leave control of the re-\nsynchronization process in the hands of the system administrator.\n8.10. Design Reflections\nMy experience in working on Graphite has reaffirmed a belief of mine that scalability has very little\nto do with low-level performance but instead is a product of overall design. I have run into many\nbottlenecks along the way but each time I look for improvements in design rather than speed-ups\nin performance. I have been asked many times why I wrote Graphite in Python rather than Java or\nC++, and my response is always that I have yet to come across a true need for the performance\nthat another language could offer. In [Knu74], Donald Knuth famously said that premature\noptimization is the root of all evil. As long as we assume that our code will continue to evolve in\nnon-trivial ways then all optimization6 is in some sense premature.\nOne of Graphite's greatest strengths and greatest weaknesses is the fact that very little of it was\nactually \"designed\" in the traditional sense. By and large Graphite evolved gradually, hurdle by\nhurdle, as problems arose. Many times the hurdles were foreseeable and various pre-emptive\nsolutions seemed natural. However it can be useful to avoid solving problems you do not actually\nhave yet, even if it seems likely that you soon will. The reason is that you can learn much more\nfrom closely studying actual failures than from theorizing about superior strategies. Problem\nsolving is driven by both the empirical data we have at hand and our own knowledge and intuition.\nI've found that doubting your own wisdom sufficiently can force you to look at your empirical data\nmore thoroughly.\nFor example, when I first wrote whisper I was convinced that it would have to be rewritten in C\nfor speed and that my Python implementation would only serve as a prototype. If I weren't under\na time-crunch I very well may have skipped the Python implementation entirely. It turns out\nhowever that I/O is a bottleneck so much earlier than CPU that the lesser efficiency of Python\nhardly matters at all in practice.\n", "page": 107, "type": "text", "section": "Page 107"}
{"text": "As I said, though, the evolutionary approach is also a great weakness of Graphite. Interfaces, it\nturns out, do not lend themselves well to gradual evolution. A good interface is consistent and\nemploys conventions to maximize predictability. By this measure, Graphite's URL API is currently a\nsub-par interface in my opinion. Options and functions have been tacked on over time, sometimes\nforming small islands of consistency, but overall lacking a global sense of consistency. The only\nway to solve such a problem is through versioning of interfaces, but this too has drawbacks.\nOnce a new interface is designed, the old one is still hard to get rid of, lingering around as\nevolutionary baggage like the human appendix. It may seem harmless enough until one day your\ncode gets appendicitis (i.e. a bug tied to the old interface) and you're forced to operate. If I were\nto change one thing about Graphite early on, it would have been to take much greater care in\ndesigning the external APIs, thinking ahead instead of evolving them bit by bit.\nAnother aspect of Graphite that causes some frustration is the limited flexibility of the hierarchical\nmetric naming model. While it is quite simple and very convenient for most use cases, it makes\nsome sophisticated queries very difficult, even impossible, to express. When I first thought of\ncreating Graphite I knew from the very beginning that I wanted a human-editable URL API for\ncreating graphs7. While I'm still glad that Graphite provides this today, I'm afraid this requirement\nhas burdened the API with excessively simple syntax that makes complex expressions unwieldy. A\nhierarchy makes the problem of determining the \"primary key\" for a metric quite simple because a\npath is essentially a primary key for a node in the tree. The downside is that all of the descriptive\ndata (i.e. column data) must be embedded directly in the path. A potential solution is to maintain\nthe hierarchical model and add a separate metadata database to enable more advanced selection\nof metrics with a special syntax.\n8.11. Becoming Open Source\nLooking back at the evolution of Graphite, I am still surprised both by how far it has come as a\nproject and by how far it has taken me as a programmer. It started as a pet project that was only\na few hundred lines of code. The rendering engine started as an experiment, simply to see if I\ncould write one. whisper was written over the course of a weekend out of desperation to solve a\nshow-stopper problem before a critical launch date. carbon has been rewritten more times than I\ncare to remember. Once I was allowed to release Graphite under an open source license in 2008 I\nnever really expected much response. After a few months it was mentioned in a CNET article that\ngot picked up by Slashdot and the project suddenly took off and has been active ever since. Today\nthere are dozens of large and mid-sized companies using Graphite. The community is quite active\nand continues to grow. Far from being a finished product, there is a lot of cool experimental work\nbeing done, which keeps it fun to work on and full of potential.\nFootnotes\n1. http://launchpad.net/graphite\n2. There is another port over which serialized objects can be sent, which is more efficient than\nthe plain-text format. This is only needed for very high levels of traffic.\n3. http://memcached.org\n4. Solid-state drives generally have extremely fast seek times compared to conventional hard\ndrives.\n5. RRD files are actually branch nodes because they can contain multiple data sources; an RRD\ndata source is a leaf node.\n6. Knuth specifically meant low-level code optimization, not macroscopic optimization such as\ndesign improvements.\n7. This forces the graphs themselves to be open source. Anyone can simply look at a graph's\nURL to understand it or modify it.\n", "page": 108, "type": "text", "section": "Page 108"}
{"text": "The Architecture of\nOpen Source Applications\nAmy Brown and Greg Wilson (eds.)\nISBN 978-1-257-63801-7 \nLicense / Buy / Contribute\nChapter 9. The Hadoop Distributed File\nSystem\nRobert Chansler, Hairong Kuang, Sanjay Radia,\nKonstantin Shvachko, and Suresh Srinivas\nThe Hadoop Distributed File System (HDFS) is designed to store very large data sets reliably, and\nto stream those data sets at high bandwidth to user applications. In a large cluster, thousands of\nservers both host directly attached storage and execute user application tasks. By distributing\nstorage and computation across many servers, the resource can grow with demand while\nremaining economical at every size. We describe the architecture of HDFS and report on\nexperience using HDFS to manage 40 petabytes of enterprise data at Yahoo!\n9.1. Introduction\nHadoop1 provides a distributed filesystem and a framework for the analysis and transformation of\nvery large data sets using the MapReduce [DG04] paradigm. While the interface to HDFS is\npatterned after the Unix filesystem, faithfulness to standards was sacrificed in favor of improved\nperformance for the applications at hand.\nAn important characteristic of Hadoop is the partitioning of data and computation across many\n(thousands) of hosts, and the execution of application computations in parallel close to their data.\nA Hadoop cluster scales computation capacity, storage capacity and I/O bandwidth by simply\nadding commodity servers. Hadoop clusters at Yahoo! span 40,000 servers, and store 40\npetabytes of application data, with the largest cluster being 4000 servers. One hundred other\norganizations worldwide report using Hadoop.\nHDFS stores filesystem metadata and application data separately. As in other distributed\nfilesystems, like PVFS [CIRT00], Lustre2, and GFS [GGL03], HDFS stores metadata on a dedicated\nserver, called the NameNode. Application data are stored on other servers called DataNodes. All\nservers are fully connected and communicate with each other using TCP-based protocols. Unlike\nLustre and PVFS, the DataNodes in HDFS do not rely on data protection mechanisms such as\nRAID to make the data durable. Instead, like GFS, the file content is replicated on multiple\nDataNodes for reliability. While ensuring data durability, this strategy has the added advantage\nthat data transfer bandwidth is multiplied, and there are more opportunities for locating\ncomputation near the needed data.\n9.2. Architecture\n9.2.1. NameNode\nThe HDFS namespace is a hierarchy of files and directories. Files and directories are represented\non the NameNode by inodes. Inodes record attributes like permissions, modification and access\ntimes, namespace and disk space quotas. The file content is split into large blocks (typically 128\nmegabytes, but user selectable file-by-file), and each block of the file is independently replicated at\nmultiple DataNodes (typically three, but user selectable file-by-file). The NameNode maintains the\nnamespace tree and the mapping of blocks to DataNodes. The current design has a single\nNameNode for each cluster. The cluster can have thousands of DataNodes and tens of thousands\n", "page": 109, "type": "text", "section": "Page 109"}
{"text": "of HDFS clients per cluster, as each DataNode may execute multiple application tasks\nconcurrently.\n9.2.2. Image and Journal\nThe inodes and the list of blocks that define the metadata of the name system are called the\nimage. NameNode keeps the entire namespace image in RAM. The persistent record of the image\nstored in the NameNode's local native filesystem is called a checkpoint. The NameNode records\nchanges to HDFS in a write-ahead log called the journal in its local native filesystem. The location of\nblock replicas are not part of the persistent checkpoint.\nEach client-initiated transaction is recorded in the journal, and the journal file is flushed and synced\nbefore the acknowledgment is sent to the client. The checkpoint file is never changed by the\nNameNode; a new file is written when a checkpoint is created during restart, when requested by\nthe administrator, or by the CheckpointNode described in the next section. During startup the\nNameNode initializes the namespace image from the checkpoint, and then replays changes from\nthe journal. A new checkpoint and an empty journal are written back to the storage directories\nbefore the NameNode starts serving clients.\nFor improved durability, redundant copies of the checkpoint and journal are typically stored on\nmultiple independent local volumes and at remote NFS servers. The first choice prevents loss from\na single volume failure, and the second choice protects against failure of the entire node. If the\nNameNode encounters an error writing the journal to one of the storage directories it\nautomatically excludes that directory from the list of storage directories. The NameNode\nautomatically shuts itself down if no storage directory is available.\nThe NameNode is a multithreaded system and processes requests simultaneously from multiple\nclients. Saving a transaction to disk becomes a bottleneck since all other threads need to wait until\nthe synchronous flush-and-sync procedure initiated by one of them is complete. In order to\noptimize this process, the NameNode batches multiple transactions. When one of the\nNameNode's threads initiates a flush-and-sync operation, all the transactions batched at that time\nare committed together. Remaining threads only need to check that their transactions have been\nsaved and do not need to initiate a flush-and-sync operation.\n9.2.3. DataNodes\nEach block replica on a DataNode is represented by two files in the local native filesystem. The first\nfile contains the data itself and the second file records the block's metadata including checksums\nfor the data and the generation stamp. The size of the data file equals the actual length of the\nblock and does not require extra space to round it up to the nominal block size as in traditional\nfilesystems. Thus, if a block is half full it needs only half of the space of the full block on the local\ndrive.\nDuring startup each DataNode connects to the NameNode and performs a handshake. The\npurpose of the handshake is to verify the namespace ID and the software version of the\nDataNode. If either does not match that of the NameNode, the DataNode automatically shuts\ndown.\nThe namespace ID is assigned to the filesystem instance when it is formatted. The namespace ID\nis persistently stored on all nodes of the cluster. Nodes with a different namespace ID will not be\nable to join the cluster, thus protecting the integrity of the filesystem. A DataNode that is newly\ninitialized and without any namespace ID is permitted to join the cluster and receive the cluster's\nnamespace ID.\nAfter the handshake the DataNode registers with the NameNode. DataNodes persistently store\ntheir unique storage IDs. The storage ID is an internal identifier of the DataNode, which makes it\nrecognizable even if it is restarted with a different IP address or port. The storage ID is assigned\nto the DataNode when it registers with the NameNode for the first time and never changes after\nthat.\n", "page": 110, "type": "text", "section": "Page 110"}
{"text": "A DataNode identifies block replicas in its possession to the NameNode by sending a block report.\nA block report contains the block ID, the generation stamp and the length for each block replica\nthe server hosts. The first block report is sent immediately after the DataNode registration.\nSubsequent block reports are sent every hour and provide the NameNode with an up-to-date view\nof where block replicas are located on the cluster.\nDuring normal operation DataNodes send heartbeats to the NameNode to confirm that the\nDataNode is operating and the block replicas it hosts are available. The default heartbeat interval is\nthree seconds. If the NameNode does not receive a heartbeat from a DataNode in ten minutes the\nNameNode considers the DataNode to be out of service and the block replicas hosted by that\nDataNode to be unavailable. The NameNode then schedules creation of new replicas of those\nblocks on other DataNodes.\nHeartbeats from a DataNode also carry information about total storage capacity, fraction of\nstorage in use, and the number of data transfers currently in progress. These statistics are used\nfor the NameNode's block allocation and load balancing decisions.\nThe NameNode does not directly send requests to DataNodes. It uses replies to heartbeats to\nsend instructions to the DataNodes. The instructions include commands to replicate blocks to\nother nodes, remove local block replicas, re-register and send an immediate block report, and\nshut down the node.\nThese commands are important for maintaining the overall system integrity and therefore it is\ncritical to keep heartbeats frequent even on big clusters. The NameNode can process thousands\nof heartbeats per second without affecting other NameNode operations.\n9.2.4. HDFS Client\nUser applications access the filesystem using the HDFS client, a library that exports the HDFS\nfilesystem interface.\nLike most conventional filesystems, HDFS supports operations to read, write and delete files, and\noperations to create and delete directories. The user references files and directories by paths in\nthe namespace. The user application does not need to know that filesystem metadata and storage\nare on different servers, or that blocks have multiple replicas.\nWhen an application reads a file, the HDFS client first asks the NameNode for the list of DataNodes\nthat host replicas of the blocks of the file. The list is sorted by the network topology distance from\nthe client. The client contacts a DataNode directly and requests the transfer of the desired block.\nWhen a client writes, it first asks the NameNode to choose DataNodes to host replicas of the first\nblock of the file. The client organizes a pipeline from node-to-node and sends the data. When the\nfirst block is filled, the client requests new DataNodes to be chosen to host replicas of the next\nblock. A new pipeline is organized, and the client sends the further bytes of the file. Choice of\nDataNodes for each block is likely to be different. The interactions among the client, the\nNameNode and the DataNodes are illustrated in Figure 9.1.\nFigure 9.1: HDFS Client Creates a New File\nUnlike conventional filesystems, HDFS provides an API that exposes the locations of a file blocks.\n", "page": 111, "type": "text", "section": "Page 111"}
{"text": "This allows applications like the MapReduce framework to schedule a task to where the data are\nlocated, thus improving the read performance. It also allows an application to set the replication\nfactor of a file. By default a file's replication factor is three. For critical files or files which are\naccessed very often, having a higher replication factor improves tolerance against faults and\nincreases read bandwidth.\n9.2.5. CheckpointNode\nThe NameNode in HDFS, in addition to its primary role serving client requests, can alternatively\nexecute either of two other roles, either a CheckpointNode or a BackupNode. The role is specified\nat the node startup.\nThe CheckpointNode periodically combines the existing checkpoint and journal to create a new\ncheckpoint and an empty journal. The CheckpointNode usually runs on a different host from the\nNameNode since it has the same memory requirements as the NameNode. It downloads the\ncurrent checkpoint and journal files from the NameNode, merges them locally, and returns the\nnew checkpoint back to the NameNode.\nCreating periodic checkpoints is one way to protect the filesystem metadata. The system can start\nfrom the most recent checkpoint if all other persistent copies of the namespace image or journal\nare unavailable. Creating a checkpoint also lets the NameNode truncate the journal when the new\ncheckpoint is uploaded to the NameNode. HDFS clusters run for prolonged periods of time\nwithout restarts during which the journal constantly grows. If the journal grows very large, the\nprobability of loss or corruption of the journal file increases. Also, a very large journal extends the\ntime required to restart the NameNode. For a large cluster, it takes an hour to process a week-\nlong journal. Good practice is to create a daily checkpoint.\n9.2.6. BackupNode\nA recently introduced feature of HDFS is the BackupNode. Like a CheckpointNode, the\nBackupNode is capable of creating periodic checkpoints, but in addition it maintains an in-memory,\nup-to-date image of the filesystem namespace that is always synchronized with the state of the\nNameNode.\nThe BackupNode accepts the journal stream of namespace transactions from the active\nNameNode, saves them in journal on its own storage directories, and applies these transactions to\nits own namespace image in memory. The NameNode treats the BackupNode as a journal store\nthe same way as it treats journal files in its storage directories. If the NameNode fails, the\nBackupNode's image in memory and the checkpoint on disk is a record of the latest namespace\nstate.\nThe BackupNode can create a checkpoint without downloading checkpoint and journal files from\nthe active NameNode, since it already has an up-to-date namespace image in its memory. This\nmakes the checkpoint process on the BackupNode more efficient as it only needs to save the\nnamespace into its local storage directories.\nThe BackupNode can be viewed as a read-only NameNode. It contains all filesystem metadata\ninformation except for block locations. It can perform all operations of the regular NameNode that\ndo not involve modification of the namespace or knowledge of block locations. Use of a\nBackupNode provides the option of running the NameNode without persistent storage, delegating\nresponsibility of persisting the namespace state to the BackupNode.\n9.2.7. Upgrades and Filesystem Snapshots\nDuring software upgrades the possibility of corrupting the filesystem due to software bugs or\nhuman mistakes increases. The purpose of creating snapshots in HDFS is to minimize potential\ndamage to the data stored in the system during upgrades.\nThe snapshot mechanism lets administrators persistently save the current state of the filesystem,\nso that if the upgrade results in data loss or corruption it is possible to rollback the upgrade and\n", "page": 112, "type": "text", "section": "Page 112"}
{"text": "return HDFS to the namespace and storage state as they were at the time of the snapshot.\nThe snapshot (only one can exist) is created at the cluster administrator's option whenever the\nsystem is started. If a snapshot is requested, the NameNode first reads the checkpoint and\njournal files and merges them in memory. Then it writes the new checkpoint and the empty journal\nto a new location, so that the old checkpoint and journal remain unchanged.\nDuring handshake the NameNode instructs DataNodes whether to create a local snapshot. The\nlocal snapshot on the DataNode cannot be created by replicating the directories containing the\ndata files as this would require doubling the storage capacity of every DataNode on the cluster.\nInstead each DataNode creates a copy of the storage directory and hard links existing block files\ninto it. When the DataNode removes a block it removes only the hard link, and block modifications\nduring appends use the copy-on-write technique. Thus old block replicas remain untouched in\ntheir old directories.\nThe cluster administrator can choose to roll back HDFS to the snapshot state when restarting the\nsystem. The NameNode recovers the checkpoint saved when the snapshot was created.\nDataNodes restore the previously renamed directories and initiate a background process to delete\nblock replicas created after the snapshot was made. Having chosen to roll back, there is no\nprovision to roll forward. The cluster administrator can recover the storage occupied by the\nsnapshot by commanding the system to abandon the snapshot; for snapshots created during\nupgrade, this finalizes the software upgrade.\nSystem evolution may lead to a change in the format of the NameNode's checkpoint and journal\nfiles, or in the data representation of block replica files on DataNodes. The layout version identifies\nthe data representation formats, and is persistently stored in the NameNode's and the DataNodes'\nstorage directories. During startup each node compares the layout version of the current\nsoftware with the version stored in its storage directories and automatically converts data from\nolder formats to the newer ones. The conversion requires the mandatory creation of a snapshot\nwhen the system restarts with the new software layout version.\n9.3. File I/O Operations and Replica Management\nOf course, the whole point of a filesystem is to store data in files. To understand how HDFS does\nthis, we must look at how reading and writing works, and how blocks are managed.\n9.3.1. File Read and Write\nAn application adds data to HDFS by creating a new file and writing the data to it. After the file is\nclosed, the bytes written cannot be altered or removed except that new data can be added to the\nfile by reopening the file for append. HDFS implements a single-writer, multiple-reader model.\nThe HDFS client that opens a file for writing is granted a lease for the file; no other client can write\nto the file. The writing client periodically renews the lease by sending a heartbeat to the\nNameNode. When the file is closed, the lease is revoked. The lease duration is bound by a soft limit\nand a hard limit. Until the soft limit expires, the writer is certain of exclusive access to the file. If\nthe soft limit expires and the client fails to close the file or renew the lease, another client can\npreempt the lease. If after the hard limit expires (one hour) and the client has failed to renew the\nlease, HDFS assumes that the client has quit and will automatically close the file on behalf of the\nwriter, and recover the lease. The writer's lease does not prevent other clients from reading the\nfile; a file may have many concurrent readers.\nAn HDFS file consists of blocks. When there is a need for a new block, the NameNode allocates a\nblock with a unique block ID and determines a list of DataNodes to host replicas of the block. The\nDataNodes form a pipeline, the order of which minimizes the total network distance from the client\nto the last DataNode. Bytes are pushed to the pipeline as a sequence of packets. The bytes that\nan application writes first buffer at the client side. After a packet buffer is filled (typically 64 KB),\nthe data are pushed to the pipeline. The next packet can be pushed to the pipeline before receiving\nthe acknowledgment for the previous packets. The number of outstanding packets is limited by\n", "page": 113, "type": "text", "section": "Page 113"}
{"text": "the outstanding packets window size of the client.\nAfter data are written to an HDFS file, HDFS does not provide any guarantee that data are visible\nto a new reader until the file is closed. If a user application needs the visibility guarantee, it can\nexplicitly call the hflush operation. Then the current packet is immediately pushed to the pipeline,\nand the hflush operation will wait until all DataNodes in the pipeline acknowledge the successful\ntransmission of the packet. All data written before the hflush operation are then certain to be\nvisible to readers.\nFigure 9.2: Data Pipeline While Writing a Block\nIf no error occurs, block construction goes through three stages as shown in Figure 9.2\nillustrating a pipeline of three DataNodes (DN) and a block of five packets. In the picture, bold lines\nrepresent data packets, dashed lines represent acknowledgment messages, and thin lines\nrepresent control messages to setup and close the pipeline. Vertical lines represent activity at the\nclient and the three DataNodes where time proceeds from top to bottom. From t0 to t1 is the\npipeline setup stage. The interval t1 to t2 is the data streaming stage, where t1 is the time when\nthe first data packet gets sent and t2 is the time that the acknowledgment to the last packet gets\nreceived. Here an hflush operation transmits packet 2. The hflush indication travels with the\npacket data and is not a separate operation. The final interval t2 to t3 is the pipeline close stage\nfor this block.\nIn a cluster of thousands of nodes, failures of a node (most commonly storage faults) are daily\noccurrences. A replica stored on a DataNode may become corrupted because of faults in\nmemory, disk, or network. HDFS generates and stores checksums for each data block of an\nHDFS file. Checksums are verified by the HDFS client while reading to help detect any corruption\ncaused either by client, DataNodes, or network. When a client creates an HDFS file, it computes\nthe checksum sequence for each block and sends it to a DataNode along with the data. A\nDataNode stores checksums in a metadata file separate from the block's data file. When HDFS\nreads a file, each block's data and checksums are shipped to the client. The client computes the\nchecksum for the received data and verifies that the newly computed checksums matches the\nchecksums it received. If not, the client notifies the NameNode of the corrupt replica and then\nfetches a different replica of the block from another DataNode.\nWhen a client opens a file to read, it fetches the list of blocks and the locations of each block\nreplica from the NameNode. The locations of each block are ordered by their distance from the\nreader. When reading the content of a block, the client tries the closest replica first. If the read\nattempt fails, the client tries the next replica in sequence. A read may fail if the target DataNode is\nunavailable, the node no longer hosts a replica of the block, or the replica is found to be corrupt\nwhen checksums are tested.\n", "page": 114, "type": "text", "section": "Page 114"}
{"text": "HDFS permits a client to read a file that is open for writing. When reading a file open for writing,\nthe length of the last block still being written is unknown to the NameNode. In this case, the client\nasks one of the replicas for the latest length before starting to read its content.\nThe design of HDFS I/O is particularly optimized for batch processing systems, like MapReduce,\nwhich require high throughput for sequential reads and writes. Ongoing efforts will improve\nread/write response time for applications that require real-time data streaming or random access.\n9.3.2. Block Placement\nFor a large cluster, it may not be practical to connect all nodes in a flat topology. A common\npractice is to spread the nodes across multiple racks. Nodes of a rack share a switch, and rack\nswitches are connected by one or more core switches. Communication between two nodes in\ndifferent racks has to go through multiple switches. In most cases, network bandwidth between\nnodes in the same rack is greater than network bandwidth between nodes in different racks.\nFigure 9.3 describes a cluster with two racks, each of which contains three nodes.\nFigure 9.3: Cluster Topology\nHDFS estimates the network bandwidth between two nodes by their distance. The distance from a\nnode to its parent node is assumed to be one. A distance between two nodes can be calculated by\nsumming the distances to their closest common ancestor. A shorter distance between two nodes\nmeans greater bandwidth they can use to transfer data.\nHDFS allows an administrator to configure a script that returns a node's rack identification given a\nnode's address. The NameNode is the central place that resolves the rack location of each\nDataNode. When a DataNode registers with the NameNode, the NameNode runs the configured\nscript to decide which rack the node belongs to. If no such a script is configured, the NameNode\nassumes that all the nodes belong to a default single rack.\nThe placement of replicas is critical to HDFS data reliability and read/write performance. A good\nreplica placement policy should improve data reliability, availability, and network bandwidth\nutilization. Currently HDFS provides a configurable block placement policy interface so that the\nusers and researchers can experiment and test alternate policies that are optimal for their\napplications.\nThe default HDFS block placement policy provides a tradeoff between minimizing the write cost,\nand maximizing data reliability, availability and aggregate read bandwidth. When a new block is\ncreated, HDFS places the first replica on the node where the writer is located. The second and the\nthird replicas are placed on two different nodes in a different rack. The rest are placed on random\nnodes with restrictions that no more than one replica is placed at any one node and no more than\ntwo replicas are placed in the same rack, if possible. The choice to place the second and third\nreplicas on a different rack better distributes the block replicas for a single file across the cluster.\nIf the first two replicas were placed on the same rack, for any file, two-thirds of its block replicas\nwould be on the same rack.\nAfter all target nodes are selected, nodes are organized as a pipeline in the order of their proximity\nto the first replica. Data are pushed to nodes in this order. For reading, the NameNode first\n", "page": 115, "type": "text", "section": "Page 115"}
{"text": "to the first replica. Data are pushed to nodes in this order. For reading, the NameNode first\nchecks if the client's host is located in the cluster. If yes, block locations are returned to the client\nin the order of its closeness to the reader. The block is read from DataNodes in this preference\norder.\nThis policy reduces the inter-rack and inter-node write traffic and generally improves write\nperformance. Because the chance of a rack failure is far less than that of a node failure, this policy\ndoes not impact data reliability and availability guarantees. In the usual case of three replicas, it can\nreduce the aggregate network bandwidth used when reading data since a block is placed in only\ntwo unique racks rather than three.\n9.3.3. Replication Management\nThe NameNode endeavors to ensure that each block always has the intended number of replicas.\nThe NameNode detects that a block has become under- or over-replicated when a block report\nfrom a DataNode arrives. When a block becomes over replicated, the NameNode chooses a\nreplica to remove. The NameNode will prefer not to reduce the number of racks that host replicas,\nand secondly prefer to remove a replica from the DataNode with the least amount of available disk\nspace. The goal is to balance storage utilization across DataNodes without reducing the block's\navailability.\nWhen a block becomes under-replicated, it is put in the replication priority queue. A block with only\none replica has the highest priority, while a block with a number of replicas that is greater than two\nthirds of its replication factor has the lowest priority. A background thread periodically scans the\nhead of the replication queue to decide where to place new replicas. Block replication follows a\nsimilar policy as that of new block placement. If the number of existing replicas is one, HDFS\nplaces the next replica on a different rack. In case that the block has two existing replicas, if the\ntwo existing replicas are on the same rack, the third replica is placed on a different rack;\notherwise, the third replica is placed on a different node in the same rack as an existing replica.\nHere the goal is to reduce the cost of creating new replicas.\nThe NameNode also makes sure that not all replicas of a block are located on one rack. If the\nNameNode detects that a block's replicas end up at one rack, the NameNode treats the block as\nmis-replicated and replicates the block to a different rack using the same block placement policy\ndescribed above. After the NameNode receives the notification that the replica is created, the\nblock becomes over-replicated. The NameNode then will decides to remove an old replica because\nthe over-replication policy prefers not to reduce the number of racks.\n9.3.4. Balancer\nHDFS block placement strategy does not take into account DataNode disk space utilization. This is\nto avoid placing new\u2014more likely to be referenced\u2014data at a small subset of the DataNodes with\na lot of free storage. Therefore data might not always be placed uniformly across DataNodes.\nImbalance also occurs when new nodes are added to the cluster.\nThe balancer is a tool that balances disk space usage on an HDFS cluster. It takes a threshold\nvalue as an input parameter, which is a fraction between 0 and 1. A cluster is balanced if, for each\nDataNode, the utilization of the node3 differs from the utilization of the whole cluster4 by no more\nthan the threshold value.\nThe tool is deployed as an application program that can be run by the cluster administrator. It\niteratively moves replicas from DataNodes with higher utilization to DataNodes with lower\nutilization. One key requirement for the balancer is to maintain data availability. When choosing a\nreplica to move and deciding its destination, the balancer guarantees that the decision does not\nreduce either the number of replicas or the number of racks.\nThe balancer optimizes the balancing process by minimizing the inter-rack data copying. If the\nbalancer decides that a replica A needs to be moved to a different rack and the destination rack\nhappens to have a replica B of the same block, the data will be copied from replica B instead of\nreplica A.\n", "page": 116, "type": "text", "section": "Page 116"}
{"text": "A configuration parameter limits the bandwidth consumed by rebalancing operations. The higher\nthe allowed bandwidth, the faster a cluster can reach the balanced state, but with greater\ncompetition with application processes.\n9.3.5. Block Scanner\nEach DataNode runs a block scanner that periodically scans its block replicas and verifies that\nstored checksums match the block data. In each scan period, the block scanner adjusts the read\nbandwidth in order to complete the verification in a configurable period. If a client reads a\ncomplete block and checksum verification succeeds, it informs the DataNode. The DataNode\ntreats it as a verification of the replica.\nThe verification time of each block is stored in a human-readable log file. At any time there are up\nto two files in the top-level DataNode directory, the current and previous logs. New verification\ntimes are appended to the current file. Correspondingly, each DataNode has an in-memory\nscanning list ordered by the replica's verification time.\nWhenever a read client or a block scanner detects a corrupt block, it notifies the NameNode. The\nNameNode marks the replica as corrupt, but does not schedule deletion of the replica immediately.\nInstead, it starts to replicate a good copy of the block. Only when the good replica count reaches\nthe replication factor of the block the corrupt replica is scheduled to be removed. This policy aims\nto preserve data as long as possible. So even if all replicas of a block are corrupt, the policy allows\nthe user to retrieve its data from the corrupt replicas.\n9.3.6. Decommissioning\nThe cluster administrator specifies list of nodes to be decommissioned. Once a DataNode is\nmarked for decommissioning, it will not be selected as the target of replica placement, but it will\ncontinue to serve read requests. The NameNode starts to schedule replication of its blocks to\nother DataNodes. Once the NameNode detects that all blocks on the decommissioning DataNode\nare replicated, the node enters the decommissioned state. Then it can be safely removed from the\ncluster without jeopardizing any data availability.\n9.3.7. Inter-Cluster Data Copy\nWhen working with large datasets, copying data into and out of a HDFS cluster is daunting. HDFS\nprovides a tool called DistCp for large inter/intra-cluster parallel copying. It is a MapReduce job;\neach of the map tasks copies a portion of the source data into the destination filesystem. The\nMapReduce framework automatically handles parallel task scheduling, error detection and\nrecovery.\n9.4. Practice at Yahoo!\nLarge HDFS clusters at Yahoo! include about 4000 nodes. A typical cluster node has two quad\ncore Xeon processors running at 2.5 GHz, 4\u201312 directly attached SATA drives (holding two\nterabytes each), 24 Gbyte of RAM, and a 1-gigabit Ethernet connection. Seventy percent of the\ndisk space is allocated to HDFS. The remainder is reserved for the operating system (Red Hat\nLinux), logs, and space to spill the output of map tasks (MapReduce intermediate data are not\nstored in HDFS).\nForty nodes in a single rack share an IP switch. The rack switches are connected to each of eight\ncore switches. The core switches provide connectivity between racks and to out-of-cluster\nresources. For each cluster, the NameNode and the BackupNode hosts are specially provisioned\nwith up to 64 GB RAM; application tasks are never assigned to those hosts. In total, a cluster of\n4000 nodes has 11 PB (petabytes; 1000 terabytes) of storage available as blocks that are\nreplicated three times yielding a net 3.7 PB of storage for user applications. Over the years that\nHDFS has been in use, the hosts selected as cluster nodes have benefited from improved\ntechnologies. New cluster nodes always have faster processors, bigger disks and larger RAM.\nSlower, smaller nodes are retired or relegated to clusters reserved for development and testing of\n", "page": 117, "type": "text", "section": "Page 117"}
{"text": "Hadoop.\nOn an example large cluster (4000 nodes), there are about 65 million files and 80 million blocks. As\neach block typically is replicated three times, every data node hosts 60 000 block replicas. Each\nday, user applications will create two million new files on the cluster. The 40 000 nodes in Hadoop\nclusters at Yahoo! provide 40 PB of on-line data storage.\nBecoming a key component of Yahoo!'s technology suite meant tackling technical problems that\nare the difference between being a research project and being the custodian of many petabytes of\ncorporate data. Foremost are issues of robustness and durability of data. But also important are\neconomical performance, provisions for resource sharing among members of the user\ncommunity, and ease of administration by the system operators.\n9.4.1. Durability of Data\nReplication of data three times is a robust guard against loss of data due to uncorrelated node\nfailures. It is unlikely Yahoo! has ever lost a block in this way; for a large cluster, the probability of\nlosing a block during one year is less than 0.005. The key understanding is that about 0.8 percent\nof nodes fail each month. (Even if the node is eventually recovered, no effort is taken to recover\ndata it may have hosted.) So for the sample large cluster as described above, a node or two is lost\neach day. That same cluster will re-create the 60 000 block replicas hosted on a failed node in\nabout two minutes: re-replication is fast because it is a parallel problem that scales with the size of\nthe cluster. The probability of several nodes failing within two minutes such that all replicas of\nsome block are lost is indeed small.\nCorrelated failure of nodes is a different threat. The most commonly observed fault in this regard\nis the failure of a rack or core switch. HDFS can tolerate losing a rack switch (each block has a\nreplica on some other rack). Some failures of a core switch can effectively disconnect a slice of\nthe cluster from multiple racks, in which case it is probable that some blocks will become\nunavailable. In either case, repairing the switch restores unavailable replicas to the cluster. Another\nkind of correlated failure is the accidental or deliberate loss of electrical power to the cluster. If the\nloss of power spans racks, it is likely that some blocks will become unavailable. But restoring\npower may not be a remedy because one-half to one percent of the nodes will not survive a full\npower-on restart. Statistically, and in practice, a large cluster will lose a handful of blocks during a\npower-on restart.\nIn addition to total failures of nodes, stored data can be corrupted or lost. The block scanner\nscans all blocks in a large cluster each fortnight and finds about 20 bad replicas in the process.\nBad replicas are replaced as they are discovered.\n9.4.2. Features for Sharing HDFS\nAs the use of HDFS has grown, the filesystem itself has had to introduce means to share the\nresource among a large number of diverse users. The first such feature was a permissions\nframework closely modeled on the Unix permissions scheme for file and directories. In this\nframework, files and directories have separate access permissions for the owner, for other\nmembers of the user group associated with the file or directory, and for all other users. The\nprinciple differences between Unix (POSIX) and HDFS are that ordinary files in HDFS have neither\nexecute permissions nor sticky bits.\nIn the earlier version of HDFS, user identity was weak: you were who your host said you are.\nWhen accessing HDFS, the application client simply queries the local operating system for user\nidentity and group membership. In the new framework, the application client must present to the\nname system credentials obtained from a trusted source. Different credential administrations are\npossible; the initial implementation uses Kerberos. The user application can use the same\nframework to confirm that the name system also has a trustworthy identity. And the name\nsystem also can demand credentials from each of the data nodes participating in the cluster.\nThe total space available for data storage is set by the number of data nodes and the storage\nprovisioned for each node. Early experience with HDFS demonstrated a need for some means to\n", "page": 118, "type": "text", "section": "Page 118"}
{"text": "enforce the resource allocation policy across user communities. Not only must fairness of sharing\nbe enforced, but when a user application might involve thousands of hosts writing data,\nprotection against applications inadvertently exhausting resources is also important. For HDFS,\nbecause the system metadata are always in RAM, the size of the namespace (number of files and\ndirectories) is also a finite resource. To manage storage and namespace resources, each directory\nmay be assigned a quota for the total space occupied by files in the sub-tree of the namespace\nbeginning at that directory. A separate quota may also be set for the total number of files and\ndirectories in the sub-tree.\nWhile the architecture of HDFS presumes most applications will stream large data sets as input,\nthe MapReduce programming framework can have a tendency to generate many small output files\n(one from each reduce task) further stressing the namespace resource. As a convenience, a\ndirectory sub-tree can be collapsed into a single Hadoop Archive file. A HAR file is similar to a\nfamiliar tar, JAR, or Zip file, but filesystem operations can address the individual files within the\narchive, and a HAR file can be used transparently as the input to a MapReduce job.\n9.4.3. Scaling and HDFS Federation\nScalability of the NameNode has been a key struggle [Shv10]. Because the NameNode keeps all\nthe namespace and block locations in memory, the size of the NameNode heap limits the number\nof files and also the number of blocks addressable. This also limits the total cluster storage that\ncan be supported by the NameNode. Users are encouraged to create larger files, but this has not\nhappened since it would require changes in application behavior. Furthermore, we are seeing new\nclasses of applications for HDFS that need to store a large number of small files. Quotas were\nadded to manage the usage, and an archive tool has been provided, but these do not\nfundamentally address the scalability problem.\nA new feature allows multiple independent namespaces (and NameNodes) to share the physical\nstorage within a cluster. Namespaces use blocks grouped under a Block Pool. Block pools are\nanalogous to logical units (LUNs) in a SAN storage system and a namespace with its pool of blocks\nis analogous to a filesystem volume.\nThis approach offers a number of advantages besides scalability: it can isolate namespaces of\ndifferent applications improving the overall availability of the cluster. Block pool abstraction allows\nother services to use the block storage with perhaps a different namespace structure. We plan to\nexplore other approaches to scaling such as storing only partial namespace in memory, and truly\ndistributed implementation of the NameNode.\nApplications prefer to continue using a single namespace. Namespaces can be mounted to create\nsuch a unified view. A client-side mount table provide an efficient way to do that, compared to a\nserver-side mount table: it avoids an RPC to the central mount table and is also tolerant of its\nfailure. The simplest approach is to have shared cluster-wide namespace; this can be achieved by\ngiving the same client-side mount table to each client of the cluster. Client-side mount tables also\nallow applications to create a private namespace view. This is analogous to the per-process\nnamespaces that are used to deal with remote execution in distributed systems [Rad94,RP93].\n9.5. Lessons Learned\nA very small team was able to build the Hadoop filesystem and make it stable and robust enough\nto use it in production. A large part of the success was due to the very simple architecture:\nreplicated blocks, periodic block reports and central metadata server. Avoiding the full POSIX\nsemantics also helped. Although keeping the entire metadata in memory limited the scalability of\nthe namespace, it made the NameNode very simple: it avoids the complex locking of typical\nfilesystems. The other reason for Hadoop's success was to quickly use the system for production\nat Yahoo!, as it was rapidly and incrementally improved. The filesystem is very robust and the\nNameNode rarely fails; indeed most of the down time is due to software upgrades. Only recently\nhave failover solutions (albeit manual) emerged\nMany have been surprised by the choice of Java in building a scalable filesystem. While Java posed\n", "page": 119, "type": "text", "section": "Page 119"}
{"text": "challenges for scaling the NameNode due to its object memory overhead and garbage collection,\nJava has been responsible to the robustness of the system; it has avoided corruption due to\npointer or memory management bugs.\n9.6. Acknowledgment\nWe thank Yahoo! for investing in Hadoop and continuing to make it available as open source; 80%\nof the HDFS and MapReduce code was developed at Yahoo! We thank all Hadoop committers and\ncollaborators for their valuable contributions.\nFootnotes\n1. http://hadoop.apache.org\n2. http://www.lustre.org\n3. Defined as the ratio of used space at the node to total capacity of the node.\n4. Defined as the ratio of used space in the cluster to total capacity of the cluster.\n", "page": 120, "type": "text", "section": "Page 120"}
{"text": "The Architecture of\nOpen Source Applications\nAmy Brown and Greg Wilson (eds.)\nISBN 978-1-257-63801-7 \nLicense / Buy / Contribute\nChapter 10. Jitsi\nEmil Ivov\nJitsi is an application that allows people to make video and voice calls, share their desktops, and\nexchange files and messages. More importantly it allows people to do this over a number of\ndifferent protocols, ranging from the standardized XMPP (Extensible Messaging and Presence\nProtocol) and SIP (Session Initiation Protocol) to proprietary ones like Yahoo! and Windows Live\nMessenger (MSN). It runs on Microsoft Windows, Apple Mac OS X, Linux, and FreeBSD. It is\nwritten mostly in Java but it also contains parts written in native code. In this chapter, we'll look at\nJitsi's OSGi-based architecture, see how it implements and manages protocols, and look back on\nwhat we've learned from building it.1\n10.1. Designing Jitsi\nThe three most important constraints that we had to keep in mind when designing Jitsi (at the time\ncalled SIP Communicator) were multi-protocol support, cross-platform operation, and developer-\nfriendliness.\nFrom a developer's perspective, being multi-protocol comes down to having a common interface\nfor all protocols. In other words, when a user sends a message, our graphical user interface\nneeds to always call the same sendMessage method regardless of whether the currently selected\nprotocol actually uses a method called sendXmppMessage or sendSipMsg.\nThe fact that most of our code is written in Java satisfies, to a large degree, our second\nconstraint: cross-platform operation. Still, there are things that the Java Runtime Environment\n(JRE) does not support or does not do the way we'd like it to, such as capturing video from your\nwebcam. Therefore, we need to use DirectShow on Windows, QTKit on Mac OS X, and Video for\nLinux 2 on Linux. Just as with protocols, the parts of the code that control video calls cannot be\nbothered with these details (they are complicated enough as it is).\nFinally, being developer-friendly means that it should be easy for people to add new features.\nThere are millions of people using VoIP today in thousands of different ways; various service\nproviders and server vendors come up with different use cases and ideas about new features. We\nhave to make sure that it is easy for them to use Jitsi the way they want. Someone who needs to\nadd something new should have to read and understand only those parts of the project they are\nmodifying or extending. Similarly, one person's changes should have as little impact as possible on\neveryone else's work.\nTo sum up, we needed an environment where different parts of the code are relatively\nindependent from each other. It had to be possible to easily replace some parts depending on the\noperating system; have others, like protocols, run in parallel and yet act the same; and it had to be\npossible to completely rewrite any one of those parts and have the rest of the code work without\nany changes. Finally, we wanted the ability to easily switch parts on and off, as well as the ability to\ndownload plugins over the Internet to our list.\nWe briefly considered writing our own framework, but soon dropped the idea. We were itching to\nstart writing VoIP and IM code as soon as possible, and spending a couple of months on a plugin\nframework didn't seem that exciting. Someone suggested OSGi, and it seemed to be the perfect\nfit.\n10.2. Jitsi and the OSGi Framework\nPeople have written entire books about OSGi, so we're not going to go over everything the\nframework stands for. Instead we will only explain what it gives us and the way we use it in Jitsi.\n", "page": 121, "type": "text", "section": "Page 121"}
{"text": "Above everything else, OSGi is about modules. Features in OSGi applications are separated into\nbundles. An OSGi bundle is little more than a regular JAR file like the ones used to distribute Java\nlibraries and applications. Jitsi is a collection of such bundles. There is one responsible for\nconnecting to Windows Live Messenger, another one that does XMPP, yet another one that\nhandles the GUI, and so on. All these bundles run together in an environment provided, in our\ncase, by Apache Felix, an open source OSGi implementation.\nAll these modules need to work together. The GUI bundle needs to send messages via the\nprotocol bundles, which in turn need to store them via the bundles handling message history. This\nis what OSGi services are for: they represent the part of a bundle that is visible to everyone else.\nAn OSGi service is most often a group of Java interfaces that allow use of a specific functionality\nlike logging, sending messages over the network, or retrieving the list of recent calls. The classes\nthat actually implement the functionality are known as a service implementation. Most of them\ncarry the name of the service interface they implement, with an \"Impl\" suffix at the end (e.g.,\nConfigurationServiceImpl). The OSGi framework allows developers to hide service\nimplementations and make sure that they are never visible outside the bundle they are in. This\nway, other bundles can only use them through the service interfaces.\nMost bundles also have activators. Activators are simple interfaces that define a start and a\nstop method. Every time Felix loads or removes a bundle in Jitsi, it calls these methods so that the\nbundle can prepare to run or shut down. When calling these methods Felix passes them a\nparameter called BundleContext. The BundleContext gives bundles a way to connect to the OSGi\nenvironment. This way they can discover whatever OSGi service they need to use, or register one\nthemselves (Figure 10.1).\nFigure 10.1: OSGi Bundle Activation\nSo let's see how this actually works. Imagine a service that persistently stores and retrieves\nproperties. In Jitsi this is what we call the ConfigurationService and it looks like this:\npackage net.java.sip.communicator.service.configuration;\npublic interface ConfigurationService\n{\n  public void setProperty(String propertyName, Object property);\n  public Object getProperty(String propertyName);\n}\nA very simple implementation of the ConfigurationService looks like this:\npackage net.java.sip.communicator.impl.configuration;\nimport java.util.*;\nimport net.java.sip.communicator.service.configuration.*;\npublic class ConfigurationServiceImpl implements ConfigurationService\n{\n", "page": 122, "type": "text", "section": "Page 122"}
{"text": "  private final Properties properties = new Properties();\n  public Object getProperty(String name)\n  {\n    return properties.get(name);\n  }\n  public void setProperty(String name, Object value)\n  {\n    properties.setProperty(name, value.toString());\n  }\n}\nNotice how the service is defined in the net.java.sip.communicator.service package, while\nthe implementation is in net.java.sip.communicator.impl. All services and implementations in\nJitsi are separated under these two packages. OSGi allows bundles to only make some packages\nvisible outside their own JAR, so the separation makes it easier for bundles to only export their\nservice packages and keep their implementations hidden.\nThe last thing we need to do so that people can start using our implementation is to register it in\nthe BundleContext and indicate that it provides an implementation of the\nConfigurationService. Here's how this happens:\npackage net.java.sip.communicator.impl.configuration;\nimport org.osgi.framework.*;\nimport net.java.sip.communicator.service.configuration;\npublic class ConfigActivator implements BundleActivator\n{\n  public void start(BundleContext bc) throws Exception\n  {\n    bc.registerService(ConfigurationService.class.getName(), // service name\n         new ConfigurationServiceImpl(), // service implementation\n         null);\n  }\n}\nOnce the ConfigurationServiceImpl class is registered in the BundleContext, other bundles\ncan start using it. Here's an example showing how some random bundle can use our\nconfiguration service:\npackage net.java.sip.communicator.plugin.randombundle;\nimport org.osgi.framework.*;\nimport net.java.sip.communicator.service.configuration.*;\npublic class RandomBundleActivator implements BundleActivator\n{\n  public void start(BundleContext bc) throws Exception\n  {\n    ServiceReference cRef = bc.getServiceReference(\n                              ConfigurationService.class.getName());\n    configService = (ConfigurationService) bc.getService(cRef);\n    // And that's all! We have a reference to the service implementation\n    // and we are ready to start saving properties:\n    configService.setProperty(\"propertyName\", \"propertyValue\");\n  }\n}\nOnce again, notice the package. In net.java.sip.communicator.plugin we keep bundles that\nuse services defined by others but that neither export nor implement any themselves.\nConfiguration forms are a good example of such plugins: They are additions to the Jitsi user\ninterface that allow users to configure certain aspects of the application. When users change\npreferences, configuration forms interact with the ConfigurationService or directly with the\n", "page": 123, "type": "text", "section": "Page 123"}
{"text": "bundles responsible for a feature. However, none of the other bundles ever need to interact with\nthem in any way (Figure 10.2).\nFigure 10.2: Service Structure\n10.3. Building and Running a Bundle\nNow that we've seen how to write the code in a bundle, it's time to talk about packaging. When\nrunning, all bundles need to indicate three different things to the OSGi environment: the Java\npackages they make available to others (i.e. exported packages), the ones that they would like to\nuse from others (i.e. imported packages), and the name of their BundleActivator class. Bundles do\nthis through the manifest of the JAR file that they will be deployed in.\nFor the ConfigurationService that we defined above, the manifest file could look like this:\nBundle-Activator: net.java.sip.communicator.impl.configuration.ConfigActivator\nBundle-Name: Configuration Service Implementation\nBundle-Description: A bundle that offers configuration utilities\nBundle-Vendor: jitsi.org\nBundle-Version: 0.0.1\nSystem-Bundle: yes\nImport-Package: org.osgi.framework,\nExport-Package: net.java.sip.communicator.service.configuration\nAfter creating the JAR manifest, we are ready to create the bundle itself. In Jitsi we use Apache Ant\nto handle all build-related tasks. In order to add a bundle to the Jitsi build process, you need to edit\nthe build.xml file in the root directory of the project. Bundle JARs are created at the bottom of\nthe build.xml file, with bundle-xxx targets. In order to build our configuration service we need\nthe following:\n<target name=\"bundle-configuration\">\n  <jar destfile=\"${bundles.dest}/configuration.jar\" manifest=\n    \"${src}/net/java/sip/communicator/impl/configuration/conf.manifest.mf\" >\n    <zipfileset dir=\"${dest}/net/java/sip/communicator/service/configuration\"\n        prefix=\"net/java/sip/communicator/service/configuration\"/>\n    <zipfileset dir=\"${dest}/net/java/sip/communicator/impl/configuration\"\n        prefix=\"net/java/sip/communicator/impl/configuration\" />\n  </jar>\n</target>\nAs you can see, the Ant target simply creates a JAR file using our configuration manifest, and adds\nto it the configuration packages from the service and impl hierarchies. Now the only thing that\nwe need to do is to make Felix load it.\nWe already mentioned that Jitsi is merely a collection of OSGi bundles. When a user executes the\napplication, they actually start Felix with a list of bundles that it needs to load. You can find that list\nin our lib directory, inside a file called felix.client.run.properties. Felix starts bundles in\n", "page": 124, "type": "text", "section": "Page 124"}
{"text": "the order defined by start levels: All those within a particular level are guaranteed to complete\nbefore bundles in subsequent levels start loading. Although you can't see this in the example code\nabove, our configuration service stores properties in files so it needs to use our FileAccessService,\nshipped within the fileaccess.jar file. We'll therefore make sure that the ConfigurationService\nstarts after the FileAccessService:\n\u22ee    \u22ee    \u22ee\nfelix.auto.start.30= \\\n  reference:file:sc-bundles/fileaccess.jar\nfelix.auto.start.40= \\\n  reference:file:sc-bundles/configuration.jar \\\n  reference:file:sc-bundles/jmdnslib.jar \\\n  reference:file:sc-bundles/provdisc.jar \\\n\u22ee    \u22ee    \u22ee\nIf you look at the felix.client.run.properties file, you'll see a list of packages at the\nbeginning:\norg.osgi.framework.system.packages.extra= \\\n  apple.awt; \\\n  com.apple.cocoa.application; \\\n  com.apple.cocoa.foundation; \\\n  com.apple.eawt; \\\n\u22ee    \u22ee    \u22ee\nThe list tells Felix what packages it needs to make available to bundles from the system classpath.\nThis means that packages that are on this list can be imported by bundles (i.e. added to their\nImport-Package manifest header) without any being exported by any other bundle. The list mostly\ncontains packages that come from OS-specific JRE parts, and Jitsi developers rarely need to add\nnew ones to it; in most cases packages are made available by bundles.\n10.4. Protocol Provider Service\nThe ProtocolProviderService in Jitsi defines the way all protocol implementations behave. It is\nthe interface that other bundles (like the user interface) use when they need to send and receive\nmessages, make calls, and share files through the networks that Jitsi connects to.\nThe protocol service interfaces can all be found under the\nnet.java.sip.communicator.service.protocol package. There are multiple\nimplementations of the service, one per supported protocol, and all are stored in\nnet.java.sip.communicator.impl.protocol.protocol_name.\nLet's start with the service.protocol directory. The most prominent piece is the\nProtocolProviderService interface. Whenever someone needs to perform a protocol-related task,\nthey have to look up an implementation of that service in the BundleContext. The service and its\nimplementations allow Jitsi to connect to any of the supported networks, to retrieve the\nconnection status and details, and most importantly to obtain references to the classes that\nimplement the actual communications tasks like chatting and making calls.\n10.4.1. Operation Sets\nAs we mentioned earlier, the ProtocolProviderService needs to leverage the various\ncommunication protocols and their differences. While this is particularly simple for features that all\nprotocols share, like sending a message, things get trickier for tasks that only some protocols\nsupport. Sometimes these differences come from the service itself: For example, most of the SIP\nservices out there do not support server-stored contact lists, while this is a relatively well-\nsupported feature with all other protocols. MSN and AIM are another good example: at one time\nneither of them offered the ability to send messages to offline users, while everyone else did. (This\nhas since changed.)\nThe bottom line is our ProtocolProviderService needs to have a way of handling these\ndifferences so that other bundles, like the GUI, act accordingly; there's no point in adding a call\nbutton to an AIM contact if there's no way to actually make a call.\n", "page": 125, "type": "text", "section": "Page 125"}
{"text": "OperationSets to the rescue (Figure 10.3). Unsurprisingly, they are sets of operations, and\nprovide the interface that Jitsi bundles use to control the protocol implementations. The methods\nthat you find in an operation set interface are all related to a particular feature.\nOperationSetBasicInstantMessaging, for instance, contains methods for creating and sending\ninstant messages, and registering listeners that allow Jitsi to retrieve messages it receives.\nAnother example, OperationSetPresence, has methods for querying the status of the contacts on\nyour list and setting a status for yourself. So when the GUI updates the status it shows for a\ncontact, or sends a message to a contact, it is first able to ask the corresponding provider\nwhether they support presence and messaging. The methods that ProtocolProviderService\ndefines for that purpose are:\npublic Map<String, OperationSet> getSupportedOperationSets();\npublic <T extends OperationSet> T getOperationSet(Class<T> opsetClass);\nOperationSets have to be designed so that it is unlikely that a new protocol we add has support\nfor only some of the operations defined in an OperationSet. For example, some protocols do not\nsupport server-stored contact lists even though they allow users to query each other's status.\nTherefore, rather than combining the presence management and buddy list retrieval features in\nOperationSetPresence, we also defined an OperationSetPersistentPresence which is only\nused with protocols that can store contacts online. On the other hand, we have yet to come\nacross a protocol that only allows sending messages without receiving any, which is why things\nlike sending and receiving messages can be safely combined.\nFigure 10.3: Operation Sets\n10.4.2. Accounts, Factories and Provider Instances\nAn important characteristic of the ProtocolProviderService is that one instance corresponds\nto one protocol account. Therefore, at any given time you have as many service implementations\nin the BundleContext as you have accounts registered by the user.\nAt this point you may be wondering who creates and registers the protocol providers. There are\ntwo different entities involved. First, there is ProtocolProviderFactory. This is the service that\nallows other bundles to instantiate providers and then registers them as services. There is one\nfactory per protocol and every factory is responsible for creating providers for that particular\nprotocol. Factory implementations are stored with the rest of the protocol internals. For SIP, for\nexample we have\nnet.java.sip.communicator.impl.protocol.sip.ProtocolProviderFactorySipImpl.\nThe second entity involved in account creation is the protocol wizard. Unlike factories, wizards are\nseparated from the rest of the protocol implementation because they involve the graphical user\ninterface. The wizard that allows users to create SIP accounts, for example, can be found in\nnet.java.sip.communicator.plugin.sipaccregwizz.\n", "page": 126, "type": "text", "section": "Page 126"}
{"text": "10.5. Media Service\nWhen working with real-time communication over IP, there is one important thing to understand:\nprotocols like SIP and XMPP, while recognized by many as the most common VoIP protocols, are\nnot the ones that actually move voice and video over the Internet. This task is handled by the Real-\ntime Transport Protocol (RTP). SIP and XMPP are only responsible for preparing everything that\nRTP needs, like determining the address where RTP packets need to be sent and negotiating the\nformat that audio and video need to be encoded in (i.e. codec), etc. They also take care of things\nlike locating users, maintaining their presence, making the phones ring, and many others. This is\nwhy protocols like SIP and XMPP are often referred to as signalling protocols.\nWhat does this mean in the context of Jitsi? Well, first of all it means that you are not going to find\nany code manipulating audio or video flows in either the sip or jabber jitsi packages. This kind of\ncode lives in our MediaService. The MediaService and its implementation are located in\nnet.java.sip.communicator.service.neomedia and\nnet.java.sip.communicator.impl.neomedia.\nWhy \"neomedia\"?\nThe \"neo\" in the neomedia package name indicates that it replaces a similar package that\nwe used originally and that we then had to completely rewrite. This is actually how we\ncame up with one of our rules of thumb: It is hardly ever worth it to spend a lot of time\ndesigning an application to be 100% future-proof. There is simply no way of taking\neverything into account, so you are bound to have to make changes later anyway.\nBesides, it is quite likely that a painstaking design phase will introduce complexities that\nyou will never need because the scenarios you prepared for never happen.\nIn addition to the MediaService itself, there are two other interfaces that are particularly important:\nMediaDevice and MediaStream.\n10.5.1. Capture, Streaming, and Playback\nMediaDevices represent the capture and playback devices that we use during a call (Figure 10.4).\nYour microphone and speakers, your headset and your webcam are all examples of such\nMediaDevices, but they are not the only ones. Desktop streaming and sharing calls in Jitsi capture\nvideo from your desktop, while a conference call uses an AudioMixer device in order to mix the\naudio we receive from the active participants. In all cases, MediaDevices represent only a single\nMediaType. That is, they can only be either audio or video but never both. This means that if, for\nexample, you have a webcam with an integrated microphone, Jitsi sees it as two devices: one that\ncan only capture video, and another one that can only capture sound.\nDevices alone, however, are not enough to make a phone or a video call. In addition to playing and\ncapturing media, one has to also be able to send it over the network. This is where MediaStreams\ncome in. A MediaStream interface is what connects a MediaDevice to your interlocutor. It\nrepresents incoming and outgoing packets that you exchange with them within a call.\nJust as with devices, one stream can be responsible for only one MediaType. This means that in\nthe case of an audio/video call Jitsi has to create two separate media streams and then connect\neach to the corresponding audio or video MediaDevice.\n", "page": 127, "type": "text", "section": "Page 127"}
{"text": "Figure 10.4: Media Streams For Different Devices\n10.5.2. Codecs\nAnother important concept in media streaming is that of MediaFormats, also known as codecs. By\ndefault most operating systems let you capture audio in 48KHz PCM or something similar. This is\nwhat we often refer to as \"raw audio\" and it's the kind of audio you get in WAV files: great quality\nand enormous size. It is quite impractical to try and transport audio over the Internet in the PCM\nformat.\nThis is what codecs are for: they let you present and transport audio or video in a variety of\ndifferent ways. Some audio codecs like iLBC, 8KHz Speex, or G.729, have low bandwidth\nrequirements but sound somewhat muffled. Others like wideband Speex and G.722 give you great\naudio quality but also require more bandwidth. There are codecs that try to deliver good quality\nwhile keeping bandwidth requirements at a reasonable level. H.264, the popular video codec, is a\ngood example of that. The trade-off here is the amount of calculation required during conversion.\nIf you use Jitsi for an H.264 video call you see a good quality image and your bandwidth\nrequirements are quite reasonable, but your CPU runs at maximum.\nAll this is an oversimplification, but the idea is that codec choice is all about compromises. You\neither sacrifice bandwidth, quality, CPU intensity, or some combination of those. People working\nwith VoIP rarely need to know more about codecs.\n10.5.3. Connecting with the Protocol Providers\nProtocols in Jitsi that currently have audio/video support all use our MediaServices exactly the\nsame way. First they ask the MediaService about the devices that are available on the system:\npublic List<MediaDevice> getDevices(MediaType mediaType, MediaUseCase useCase);\nThe MediaType indicates whether we are interested in audio or video devices. The MediaUseCase\nparameter is currently only considered in the case of video devices. It tells the media service\nwhether we'd like to get devices that could be used in a regular call (MediaUseCase.CALL), in which\ncase it returns a list of available webcams, or a desktop sharing session\n(MediaUseCase.DESKTOP), in which case it returns references to the user desktops.\nThe next step is to obtain the list of formats that are available for a specific device. We do this\nthrough the MediaDevice.getSupportedFormats method:\npublic List<MediaFormat> getSupportedFormats();\nOnce it has this list, the protocol implementation sends it to the remote party, which responds\n", "page": 128, "type": "text", "section": "Page 128"}
{"text": "with a subset of them to indicate which ones it supports. This exchange is also known as the\nOffer/Answer Model and it often uses the Session Description Protocol or some form of it.\nAfter exchanging formats and some port numbers and IP addresses, VoIP protocols create,\nconfigure and start the MediaStreams. Roughly speaking, this initialization is along the following\nlines:\n// first create a stream connector telling the media service what sockets\n// to use when transport media with RTP and flow control and statistics\n// messages with RTCP\nStreamConnector connector =  new DefaultStreamConnector(rtpSocket, rtcpSocket);\nMediaStream stream = mediaService.createMediaStream(connector, device, control);\n// A MediaStreamTarget indicates the address and ports where our\n// interlocutor is expecting media. Different VoIP protocols have their\n// own ways of exchanging this information\nstream.setTarget(target);\n// The MediaDirection parameter tells the stream whether it is going to be\n// incoming, outgoing or both\nstream.setDirection(direction);\n// Then we set the stream format. We use the one that came\n// first in the list returned in the session negotiation answer.\nstream.setFormat(format);\n// Finally, we are ready to actually start grabbing media from our\n// media device and streaming it over the Internet\nstream.start();\nNow you can wave at your webcam, grab the mic and say, \"Hello world!\"\n10.6. UI Service\nSo far we have covered parts of Jitsi that deal with protocols, sending and receiving messages and\nmaking calls. Above all, however, Jitsi is an application used by actual people and as such, one of\nits most important aspects is its user interface. Most of the time the user interface uses the\nservices that all the other bundles in Jitsi expose. There are some cases, however, where things\nhappen the other way around.\nPlugins are the first example that comes to mind. Plugins in Jitsi often need to be able to interact\nwith the user. This means they have to open, close, move or add components to existing windows\nand panels in the user interface. This is where our UIService comes into play. It allows for basic\ncontrol over the main window in Jitsi and this is how our icons in the Mac OS X dock and the\nWindows notification area let users control the application.\nIn addition to simply playing with the contact list, plugins can also extend it. The plugin that\nimplements support for chat encryption (OTR) in Jitsi is a good example for this. Our OTR bundle\nneeds to register several GUI components in various parts of the user interface. It adds a padlock\nbutton in the chat window and a sub-section in the right-click menu of all contacts.\nThe good news is that it can do all this with just a few method calls. The OSGi activator for the OTR\nbundle, OtrActivator, contains the following lines:\nHashtable<String, String> filter = new Hashtable<String, String>();\n// Register the right-click menu item.\nfilter(Container.CONTAINER_ID,\n    Container.CONTAINER_CONTACT_RIGHT_BUTTON_MENU.getID());\nbundleContext.registerService(PluginComponent.class.getName(),\n    new OtrMetaContactMenu(Container.CONTAINER_CONTACT_RIGHT_BUTTON_MENU),\n    filter);\n// Register the chat window menu bar item.\n", "page": 129, "type": "text", "section": "Page 129"}
{"text": "filter.put(Container.CONTAINER_ID,\n           Container.CONTAINER_CHAT_MENU_BAR.getID());\nbundleContext.registerService(PluginComponent.class.getName(),\n           new OtrMetaContactMenu(Container.CONTAINER_CHAT_MENU_BAR),\n           filter);\nAs you can see, adding components to our graphical user interface simply comes down to\nregistering OSGi services. On the other side of the fence, our UIService implementation is looking\nfor implementations of its PluginComponent interface. Whenever it detects that a new\nimplementation has been registered, it obtains a reference to it and adds it to the container\nindicated in the OSGi service filter.\nHere's how this happens in the case of the right-click menu item. Within the UI bundle, the class\nthat represents the right click menu, MetaContactRightButtonMenu, contains the following lines:\n// Search for plugin components registered through the OSGI bundle context.\nServiceReference[] serRefs = null;\nString osgiFilter = \"(\"\n    + Container.CONTAINER_ID\n    + \"=\"+Container.CONTAINER_CONTACT_RIGHT_BUTTON_MENU.getID()+\")\";\nserRefs = GuiActivator.bundleContext.getServiceReferences(\n        PluginComponent.class.getName(),\n        osgiFilter);\n// Go through all the plugins we found and add them to the menu.\nfor (int i = 0; i < serRefs.length; i ++)\n{\n    PluginComponent component = (PluginComponent) GuiActivator\n        .bundleContext.getService(serRefs[i]);\n    component.setCurrentContact(metaContact);\n    if (component.getComponent() == null)\n        continue;\n    this.add((Component)component.getComponent());\n}\nAnd that's all there is to it. Most of the windows that you see within Jitsi do exactly the same thing:\nThey look through the bundle context for services implementing the PluginComponent interface\nthat have a filter indicating that they want to be added to the corresponding container. Plugins are\nlike hitch-hikers holding up signs with the names of their destinations, making Jitsi windows the\ndrivers who pick them up.\n10.7. Lessons Learned\nWhen we started work on SIP Communicator, one of the most common criticisms or questions\nwe heard was: \"Why are you using Java? Don't you know it's slow? You'd never be able to get\ndecent quality for audio/video calls!\" The \"Java is slow\" myth has even been repeated by potential\nusers as a reason they stick with Skype instead of trying Jitsi. But the first lesson we've learned\nfrom our work on the project is that efficiency is no more of a concern with Java than it would\nhave been with C++ or other native alternatives.\nWe won't pretend that the decision to choose Java was the result of rigorous analysis of all\npossible options. We simply wanted an easy way to build something that ran on Windows and\nLinux, and Java and the Java Media Framework seemed to offer one relatively easy way of doing\nso.\nThroughout the years we haven't had many reasons to regret this decision. Quite the contrary:\neven though it doesn't make it completely transparent, Java does help portability and 90% of the\ncode in SIP Communicator doesn't change from one OS to the next. This includes all the protocol\nstack implementations (e.g., SIP, XMPP, RTP, etc.) that are complex enough as they are. Not\nhaving to worry about OS specifics in such parts of the code has proven immensely useful.\n", "page": 130, "type": "text", "section": "Page 130"}
{"text": "Furthermore, Java's popularity has turned out to be very important when building our community.\nContributors are a scarce resource as it is. People need to like the nature of the application, they\nneed to find time and motivation\u2014all of this is hard to muster. Not requiring them to learn a new\nlanguage is, therefore, an advantage.\nContrary to most expectations, Java's presumed lack of speed has rarely been a reason to go\nnative. Most of the time decisions to use native languages were driven by OS integration and how\nmuch access Java was giving us to OS-specific utilities. Below we discuss the three most\nimportant areas where Java fell short.\n10.7.1. Java Sound vs. PortAudio\nJava Sound is Java's default API for capturing and playing audio. It is part of the runtime\nenvironment and therefore runs on all the platforms the Java Virtual Machine comes for. During its\nfirst years as SIP Communicator, Jitsi used JavaSound exclusively and this presented us with quite\na few inconveniences.\nFirst of all, the API did not give us the option of choosing which audio device to use. This is a big\nproblem. When using their computer for audio and video calls, users often use advanced USB\nheadsets or other audio devices to get the best possible quality. When multiple devices are\npresent on a computer, JavaSound routes all audio through whichever device the OS considers\ndefault, and this is not good enough in many cases. Many users like to keep all other applications\nrunning on their default sound card so that, for example, they could keep hearing music through\ntheir speakers. What's even more important is that in many cases it is best for SIP Communicator\nto send audio notifications to one device and the actual call audio to another, allowing a user to\nhear an incoming call alert on their speakers even if they are not in front of the computer and\nthen, after picking up the call, to start using a headset.\nNone of this is possible with Java Sound. What's more, the Linux implementation uses OSS which\nis deprecated on most of today's Linux distributions.\nWe decided to use an alternative audio system. We didn't want to compromise our multi-platform\nnature and, if possible, we wanted to avoid having to handle it all by ourselves. This is where\nPortAudio2 came in extremely handy.\nWhen Java doesn't let you do something itself, cross-platform open source projects are the next\nbest thing. Switching to PortAudio has allowed us to implement support for fine-grained\nconfigurable audio rendering and capture just as we described it above. It also runs on Windows,\nLinux, Mac OS X, FreeBSD and others that we haven't had the time to provide packages for.\n10.7.2. Video Capture and Rendering\nVideo is just as important to us as audio. However, this didn't seem to be the case for the creators\nof Java, because there is no default API in the JRE that allows capturing or rendering video. For a\nwhile the Java Media Framework seemed to be destined to become such an API until Sun stopped\nmaintaining it.\nNaturally we started looking for a PortAudio-style video alternative, but this time we weren't so\nlucky. At first we decided to go with the LTI-CIVIL framework from Ken Larson3. This is a\nwonderful project and we used it for quite a while4. However it turned out to be suboptimal when\nused in a real-time communications context.\nSo we came to the conclusion that the only way to provide impeccable video communication for\nJitsi would be for us to implement native grabbers and renderers all by ourselves. This was not an\neasy decision since it implied adding a lot of complexity and a substantial maintenance load to the\nproject but we simply had no choice: we really wanted to have quality video calls. And now we do!\nOur native grabbers and renderers directly use Video4Linux 2, QTKit and DirectShow/Direct3D on\nLinux, Mac OS X, and Windows respectively.\n10.7.3. Video Encoding and Decoding\nSIP Communicator, and hence Jitsi, supported video calls from its first days. That's because the\nJava Media Framework allowed encoding video using the H.263 codec and a 176x144 (CIF) format.\nThose of you who know what H.263 CIF looks like are probably smiling right now; few of us would\n", "page": 131, "type": "text", "section": "Page 131"}
{"text": "use a video chat application today if that's all it had to offer.\nIn order to offer decent quality we've had to use other libraries like FFmpeg. Video encoding is\nactually one of the few places where Java shows its limits performance-wise. So do other\nlanguages, as evidenced by the fact that FFmpeg developers actually use Assembler in a number\nof places in order to handle video in the most efficient way possible.\n10.7.4. Others\nThere are a number of other places where we've decided that we needed to go native for better\nresults. Systray notifications with Growl on Mac OS X and libnotify on Linux are one such example.\nOthers include querying contact databases from Microsoft Outlook and Apple Address Book,\ndetermining source IP address depending on a destination, using existing codec implementations\nfor Speex and G.722, capturing desktop screenshots, and translating chars into key codes.\nThe important thing is that whenever we needed to choose a native solution, we could, and we did.\nThis brings us to our point: Ever since we've started Jitsi we've fixed, added, or even entirely\nrewritten various parts of it because we wanted them to look, feel or perform better. However,\nwe've never ever regretted any of the things we didn't get right the first time. When in doubt, we\nsimply picked one of the available options and went with it. We could have waited until we knew\nbetter what we were doing, but if we had, there would be no Jitsi today.\n10.8. Acknowledgments\nMany thanks to Yana Stamcheva for creating all the diagrams in this chapter.\nFootnotes\n1. To refer directly to the source as you read, download it from http://jitsi.org/source. If\nyou are using Eclipse or NetBeans, you can go to http://jitsi.org/eclipse or\nhttp://jitsi.org/netbeans for instructions on how configure them.\n2. http://portaudio.com/\n3. http://lti-civil.org/\n4. Actually we still have it as a non-default option.\n", "page": 132, "type": "text", "section": "Page 132"}
{"text": "The Architecture of\nOpen Source Applications\nAmy Brown and Greg Wilson (eds.)\nISBN 978-1-257-63801-7 \nLicense / Buy / Contribute\nChapter 11. LLVM\nChris Lattner\nThis chapter discusses some of the design decisions that shaped LLVM1, an umbrella project that\nhosts and develops a set of close-knit low-level toolchain components (e.g., assemblers,\ncompilers, debuggers, etc.), which are designed to be compatible with existing tools typically used\non Unix systems. The name \"LLVM\" was once an acronym, but is now just a brand for the\numbrella project. While LLVM provides some unique capabilities, and is known for some of its great\ntools (e.g., the Clang compiler2, a C/C++/Objective-C compiler which provides a number of\nbenefits over the GCC compiler), the main thing that sets LLVM apart from other compilers is its\ninternal architecture.\nFrom its beginning in December 2000, LLVM was designed as a set of reusable libraries with well-\ndefined interfaces [LA04]. At the time, open source programming language implementations were\ndesigned as special-purpose tools which usually had monolithic executables. For example, it was\nvery difficult to reuse the parser from a static compiler (e.g., GCC) for doing static analysis or\nrefactoring. While scripting languages often provided a way to embed their runtime and\ninterpreter into larger applications, this runtime was a single monolithic lump of code that was\nincluded or excluded. There was no way to reuse pieces, and very little sharing across language\nimplementation projects.\nBeyond the composition of the compiler itself, the communities surrounding popular language\nimplementations were usually strongly polarized: an implementation usually provided either a\ntraditional static compiler like GCC, Free Pascal, and FreeBASIC, or it provided a runtime compiler in\nthe form of an interpreter or Just-In-Time (JIT) compiler. It was very uncommon to see language\nimplementation that supported both, and if they did, there was usually very little sharing of code.\nOver the last ten years, LLVM has substantially altered this landscape. LLVM is now used as a\ncommon infrastructure to implement a broad variety of statically and runtime compiled languages\n(e.g., the family of languages supported by GCC, Java, .NET, Python, Ruby, Scheme, Haskell, D, as\nwell as countless lesser known languages). It has also replaced a broad variety of special purpose\ncompilers, such as the runtime specialization engine in Apple's OpenGL stack and the image\nprocessing library in Adobe's After Effects product. Finally LLVM has also been used to create a\nbroad variety of new products, perhaps the best known of which is the OpenCL GPU\nprogramming language and runtime.\n11.1. A Quick Introduction to Classical Compiler Design\nThe most popular design for a traditional static compiler (like most C compilers) is the three phase\ndesign whose major components are the front end, the optimizer and the back end (Figure 11.1).\nThe front end parses source code, checking it for errors, and builds a language-specific Abstract\nSyntax Tree (AST) to represent the input code. The AST is optionally converted to a new\nrepresentation for optimization, and the optimizer and back end are run on the code.\nFigure 11.1: Three Major Components of a Three-Phase Compiler\n", "page": 133, "type": "text", "section": "Page 133"}
{"text": "The optimizer is responsible for doing a broad variety of transformations to try to improve the\ncode's running time, such as eliminating redundant computations, and is usually more or less\nindependent of language and target. The back end (also known as the code generator) then maps\nthe code onto the target instruction set. In addition to making correct code, it is responsible for\ngenerating good code that takes advantage of unusual features of the supported architecture.\nCommon parts of a compiler back end include instruction selection, register allocation, and\ninstruction scheduling.\nThis model applies equally well to interpreters and JIT compilers. The Java Virtual Machine (JVM) is\nalso an implementation of this model, which uses Java bytecode as the interface between the front\nend and optimizer.\n11.1.1. Implications of this Design\nThe most important win of this classical design comes when a compiler decides to support multiple\nsource languages or target architectures. If the compiler uses a common code representation in\nits optimizer, then a front end can be written for any language that can compile to it, and a back\nend can be written for any target that can compile from it, as shown in Figure 11.2.\nFigure 11.2: Retargetablity\nWith this design, porting the compiler to support a new source language (e.g., Algol or BASIC)\nrequires implementing a new front end, but the existing optimizer and back end can be reused. If\nthese parts weren't separated, implementing a new source language would require starting over\nfrom scratch, so supporting N targets and M source languages would need N*M compilers.\nAnother advantage of the three-phase design (which follows directly from retargetability) is that\nthe compiler serves a broader set of programmers than it would if it only supported one source\nlanguage and one target. For an open source project, this means that there is a larger community\nof potential contributors to draw from, which naturally leads to more enhancements and\nimprovements to the compiler. This is the reason why open source compilers that serve many\ncommunities (like GCC) tend to generate better optimized machine code than narrower compilers\nlike FreePASCAL. This isn't the case for proprietary compilers, whose quality is directly related to\nthe project's budget. For example, the Intel ICC Compiler is widely known for the quality of code it\ngenerates, even though it serves a narrow audience.\nA final major win of the three-phase design is that the skills required to implement a front end are\ndifferent than those required for the optimizer and back end. Separating these makes it easier for\na \"front-end person\" to enhance and maintain their part of the compiler. While this is a social\nissue, not a technical one, it matters a lot in practice, particularly for open source projects that\nwant to reduce the barrier to contributing as much as possible.\n11.2. Existing Language Implementations\nWhile the benefits of a three-phase design are compelling and well-documented in compiler\ntextbooks, in practice it is almost never fully realized. Looking across open source language\nimplementations (back when LLVM was started), you'd find that the implementations of Perl,\nPython, Ruby and Java share no code. Further, projects like the Glasgow Haskell Compiler (GHC)\n", "page": 134, "type": "text", "section": "Page 134"}
{"text": "and FreeBASIC are retargetable to multiple different CPUs, but their implementations are very\nspecific to the one source language they support. There is also a broad variety of special purpose\ncompiler technology deployed to implement JIT compilers for image processing, regular\nexpressions, graphics card drivers, and other subdomains that require CPU intensive work.\nThat said, there are three major success stories for this model, the first of which are the Java and\n.NET virtual machines. These systems provide a JIT compiler, runtime support, and a very well\ndefined bytecode format. This means that any language that can compile to the bytecode format\n(and there are dozens of them3) can take advantage of the effort put into the optimizer and JIT as\nwell as the runtime. The tradeoff is that these implementations provide little flexibility in the choice\nof runtime: they both effectively force JIT compilation, garbage collection, and the use of a very\nparticular object model. This leads to suboptimal performance when compiling languages that\ndon't match this model closely, such as C (e.g., with the LLJVM project).\nA second success story is perhaps the most unfortunate, but also most popular way to reuse\ncompiler technology: translate the input source to C code (or some other language) and send it\nthrough existing C compilers. This allows reuse of the optimizer and code generator, gives good\nflexibility, control over the runtime, and is really easy for front-end implementers to understand,\nimplement, and maintain. Unfortunately, doing this prevents efficient implementation of exception\nhandling, provides a poor debugging experience, slows down compilation, and can be problematic\nfor languages that require guaranteed tail calls (or other features not supported by C).\nA final successful implementation of this model is GCC4. GCC supports many front ends and back\nends, and has an active and broad community of contributors. GCC has a long history of being a\nC compiler that supports multiple targets with hacky support for a few other languages bolted\nonto it. As the years go by, the GCC community is slowly evolving a cleaner design. As of GCC 4.4,\nit has a new representation for the optimizer (known as \"GIMPLE Tuples\") which is closer to being\nseparate from the front-end representation than before. Also, its Fortran and Ada front ends use\na clean AST.\nWhile very successful, these three approaches have strong limitations to what they can be used\nfor, because they are designed as monolithic applications. As one example, it is not realistically\npossible to embed GCC into other applications, to use GCC as a runtime/JIT compiler, or extract\nand reuse pieces of GCC without pulling in most of the compiler. People who have wanted to use\nGCC's C++ front end for documentation generation, code indexing, refactoring, and static\nanalysis tools have had to use GCC as a monolithic application that emits interesting information as\nXML, or write plugins to inject foreign code into the GCC process.\nThere are multiple reasons why pieces of GCC cannot be reused as libraries, including rampant\nuse of global variables, weakly enforced invariants, poorly-designed data structures, sprawling\ncode base, and the use of macros that prevent the codebase from being compiled to support\nmore than one front-end/target pair at a time. The hardest problems to fix, though, are the\ninherent architectural problems that stem from its early design and age. Specifically, GCC suffers\nfrom layering problems and leaky abstractions: the back end walks front-end ASTs to generate\ndebug info, the front ends generate back-end data structures, and the entire compiler depends on\nglobal data structures set up by the command line interface.\n11.3. LLVM's Code Representation: LLVM IR\nWith the historical background and context out of the way, let's dive into LLVM: The most\nimportant aspect of its design is the LLVM Intermediate Representation (IR), which is the form it\nuses to represent code in the compiler. LLVM IR is designed to host mid-level analyses and\ntransformations that you find in the optimizer section of a compiler. It was designed with many\nspecific goals in mind, including supporting lightweight runtime optimizations, cross-\nfunction/interprocedural optimizations, whole program analysis, and aggressive restructuring\ntransformations, etc. The most important aspect of it, though, is that it is itself defined as a first\nclass language with well-defined semantics. To make this concrete, here is a simple example of a\n.ll file:\ndefine i32 @add1(i32 %a, i32 %b) {\nentry:\n", "page": 135, "type": "text", "section": "Page 135"}
{"text": "  %tmp1 = add i32 %a, %b\n  ret i32 %tmp1\n}\ndefine i32 @add2(i32 %a, i32 %b) {\nentry:\n  %tmp1 = icmp eq i32 %a, 0\n  br i1 %tmp1, label %done, label %recurse\nrecurse:\n  %tmp2 = sub i32 %a, 1\n  %tmp3 = add i32 %b, 1\n  %tmp4 = call i32 @add2(i32 %tmp2, i32 %tmp3)\n  ret i32 %tmp4\ndone:\n  ret i32 %b\n}\nThis LLVM IR corresponds to this C code, which provides two different ways to add integers:\nunsigned add1(unsigned a, unsigned b) {\n  return a+b;\n}\n// Perhaps not the most efficient way to add two numbers.\nunsigned add2(unsigned a, unsigned b) {\n  if (a == 0) return b;\n  return add2(a-1, b+1);\n}\nAs you can see from this example, LLVM IR is a low-level RISC-like virtual instruction set. Like a real\nRISC instruction set, it supports linear sequences of simple instructions like add, subtract,\ncompare, and branch. These instructions are in three address form, which means that they take\nsome number of inputs and produce a result in a different register.5 LLVM IR supports labels and\ngenerally looks like a weird form of assembly language.\nUnlike most RISC instruction sets, LLVM is strongly typed with a simple type system (e.g., i32 is a\n32-bit integer, i32** is a pointer to pointer to 32-bit integer) and some details of the machine are\nabstracted away. For example, the calling convention is abstracted through call and ret\ninstructions and explicit arguments. Another significant difference from machine code is that the\nLLVM IR doesn't use a fixed set of named registers, it uses an infinite set of temporaries named\nwith a % character.\nBeyond being implemented as a language, LLVM IR is actually defined in three isomorphic forms:\nthe textual format above, an in-memory data structure inspected and modified by optimizations\nthemselves, and an efficient and dense on-disk binary \"bitcode\" format. The LLVM Project also\nprovides tools to convert the on-disk format from text to binary: llvm-as assembles the textual\n.ll file into a .bc file containing the bitcode goop and llvm-dis turns a .bc file into a .ll file.\nThe intermediate representation of a compiler is interesting because it can be a \"perfect world\" for\nthe compiler optimizer: unlike the front end and back end of the compiler, the optimizer isn't\nconstrained by either a specific source language or a specific target machine. On the other hand,\nit has to serve both well: it has to be designed to be easy for a front end to generate and be\nexpressive enough to allow important optimizations to be performed for real targets.\n11.3.1. Writing an LLVM IR Optimization\nTo give some intuition for how optimizations work, it is useful to walk through some examples.\nThere are lots of different kinds of compiler optimizations, so it is hard to provide a recipe for how\nto solve an arbitrary problem. That said, most optimizations follow a simple three-part structure:\n", "page": 136, "type": "text", "section": "Page 136"}
{"text": "Look for a pattern to be transformed.\nVerify that the transformation is safe/correct for the matched instance.\nDo the transformation, updating the code.\nThe most trivial optimization is pattern matching on arithmetic identities, such as: for any integer\nX, X-X is 0, X-0 is X, (X*2)-X is X. The first question is what these look like in LLVM IR. Some\nexamples are:\n\u22ee    \u22ee    \u22ee\n%example1 = sub i32 %a, %a\n\u22ee    \u22ee    \u22ee\n%example2 = sub i32 %b, 0\n\u22ee    \u22ee    \u22ee\n%tmp = mul i32 %c, 2\n%example3 = sub i32 %tmp, %c\n\u22ee    \u22ee    \u22ee\nFor these sorts of \"peephole\" transformations, LLVM provides an instruction simplification\ninterface that is used as utilities by various other higher level transformations. These particular\ntransformations are in the SimplifySubInst function and look like this:\n// X - 0 -> X\nif (match(Op1, m_Zero()))\n  return Op0;\n// X - X -> 0\nif (Op0 == Op1)\n  return Constant::getNullValue(Op0->getType());\n// (X*2) - X -> X\nif (match(Op0, m_Mul(m_Specific(Op1), m_ConstantInt<2>())))\n  return Op1;\n\u2026\nreturn 0;  // Nothing matched, return null to indicate no transformation.\nIn this code, Op0 and Op1 are bound to the left and right operands of an integer subtract\ninstruction (importantly, these identities don't necessarily hold for IEEE floating point!). LLVM is\nimplemented in C++, which isn't well known for its pattern matching capabilities (compared to\nfunctional languages like Objective Caml), but it does offer a very general template system that\nallows us to implement something similar. The match function and the m_ functions allow us to\nperform declarative pattern matching operations on LLVM IR code. For example, the m_Specific\npredicate only matches if the left hand side of the multiplication is the same as Op1.\nTogether, these three cases are all pattern matched and the function returns the replacement if it\ncan, or a null pointer if no replacement is possible. The caller of this function\n(SimplifyInstruction) is a dispatcher that does a switch on the instruction opcode,\ndispatching to the per-opcode helper functions. It is called from various optimizations. A simple\ndriver looks like this:\nfor (BasicBlock::iterator I = BB->begin(), E = BB->end(); I != E; ++I)\n  if (Value *V = SimplifyInstruction(I))\n    I->replaceAllUsesWith(V);\nThis code simply loops over each instruction in a block, checking to see if any of them simplify. If\nso (because SimplifyInstruction returns non-null), it uses the replaceAllUsesWith method\nto update anything in the code using the simplifiable operation with the simpler form.\n11.4. LLVM's Implementation of Three-Phase Design\n", "page": 137, "type": "text", "section": "Page 137"}
{"text": "In an LLVM-based compiler, a front end is responsible for parsing, validating and diagnosing errors\nin the input code, then translating the parsed code into LLVM IR (usually, but not always, by\nbuilding an AST and then converting the AST to LLVM IR). This IR is optionally fed through a series\nof analysis and optimization passes which improve the code, then is sent into a code generator to\nproduce native machine code, as shown in Figure 11.3. This is a very straightforward\nimplementation of the three-phase design, but this simple description glosses over some of the\npower and flexibility that the LLVM architecture derives from LLVM IR.\nFigure 11.3: LLVM's Implementation of the Three-Phase Design\n11.4.1. LLVM IR is a Complete Code Representation\nIn particular, LLVM IR is both well specified and the only interface to the optimizer. This property\nmeans that all you need to know to write a front end for LLVM is what LLVM IR is, how it works,\nand the invariants it expects. Since LLVM IR has a first-class textual form, it is both possible and\nreasonable to build a front end that outputs LLVM IR as text, then uses Unix pipes to send it\nthrough the optimizer sequence and code generator of your choice.\nIt might be surprising, but this is actually a pretty novel property to LLVM and one of the major\nreasons for its success in a broad range of different applications. Even the widely successful and\nrelatively well-architected GCC compiler does not have this property: its GIMPLE mid-level\nrepresentation is not a self-contained representation. As a simple example, when the GCC code\ngenerator goes to emit DWARF debug information, it reaches back and walks the source level\n\"tree\" form. GIMPLE itself uses a \"tuple\" representation for the operations in the code, but (at least\nas of GCC 4.5) still represents operands as references back to the source level tree form.\nThe implications of this are that front-end authors need to know and produce GCC's tree data\nstructures as well as GIMPLE to write a GCC front end. The GCC back end has similar problems, so\nthey also need to know bits and pieces of how the RTL back end works as well. Finally, GCC\ndoesn't have a way to dump out \"everything representing my code\", or a way to read and write\nGIMPLE (and the related data structures that form the representation of the code) in text form.\nThe result is that it is relatively hard to experiment with GCC, and therefore it has relatively few\nfront ends.\n11.4.2. LLVM is a Collection of Libraries\nAfter the design of LLVM IR, the next most important aspect of LLVM is that it is designed as a set\nof libraries, rather than as a monolithic command line compiler like GCC or an opaque virtual\nmachine like the JVM or .NET virtual machines. LLVM is an infrastructure, a collection of useful\ncompiler technology that can be brought to bear on specific problems (like building a C compiler,\nor an optimizer in a special effects pipeline). While one of its most powerful features, it is also one\nof its least understood design points.\nLet's look at the design of the optimizer as an example: it reads LLVM IR in, chews on it a bit, then\nemits LLVM IR which hopefully will execute faster. In LLVM (as in many other compilers) the\noptimizer is organized as a pipeline of distinct optimization passes each of which is run on the\ninput and has a chance to do something. Common examples of passes are the inliner (which\nsubstitutes the body of a function into call sites), expression reassociation, loop invariant code\nmotion, etc. Depending on the optimization level, different passes are run: for example at -O0 (no\n", "page": 138, "type": "text", "section": "Page 138"}
{"text": "Depending on the optimization level, different passes are run: for example at -O0 (no\noptimization) the Clang compiler runs no passes, at -O3 it runs a series of 67 passes in its\noptimizer (as of LLVM 2.8).\nEach LLVM pass is written as a C++ class that derives (indirectly) from the Pass class. Most\npasses are written in a single .cpp file, and their subclass of the Pass class is defined in an\nanonymous namespace (which makes it completely private to the defining file). In order for the\npass to be useful, code outside the file has to be able to get it, so a single function (to create the\npass) is exported from the file. Here is a slightly simplified example of a pass to make things\nconcrete.6\nnamespace {\n  class Hello : public FunctionPass {\n  public:\n    // Print out the names of functions in the LLVM IR being optimized.\n    virtual bool runOnFunction(Function &F) {\n      cerr << \"Hello: \" << F.getName() << \"\\n\";\n      return false;\n    }\n  };\n}\nFunctionPass *createHelloPass() { return new Hello(); }\nAs mentioned, the LLVM optimizer provides dozens of different passes, each of which are written\nin a similar style. These passes are compiled into one or more .o files, which are then built into a\nseries of archive libraries (.a files on Unix systems). These libraries provide all sorts of analysis\nand transformation capabilities, and the passes are as loosely coupled as possible: they are\nexpected to stand on their own, or explicitly declare their dependencies among other passes if\nthey depend on some other analysis to do their job. When given a series of passes to run, the\nLLVM PassManager uses the explicit dependency information to satisfy these dependencies and\noptimize the execution of passes.\nLibraries and abstract capabilities are great, but they don't actually solve problems. The interesting\nbit comes when someone wants to build a new tool that can benefit from compiler technology,\nperhaps a JIT compiler for an image processing language. The implementer of this JIT compiler has\na set of constraints in mind: for example, perhaps the image processing language is highly\nsensitive to compile-time latency and has some idiomatic language properties that are important to\noptimize away for performance reasons.\nThe library-based design of the LLVM optimizer allows our implementer to pick and choose both\nthe order in which passes execute, and which ones make sense for the image processing domain:\nif everything is defined as a single big function, it doesn't make sense to waste time on inlining. If\nthere are few pointers, alias analysis and memory optimization aren't worth bothering about.\nHowever, despite our best efforts, LLVM doesn't magically solve all optimization problems! Since\nthe pass subsystem is modularized and the PassManager itself doesn't know anything about the\ninternals of the passes, the implementer is free to implement their own language-specific passes\nto cover for deficiencies in the LLVM optimizer or to explicit language-specific optimization\nopportunities. Figure 11.4 shows a simple example for our hypothetical XYZ image processing\nsystem:\n", "page": 139, "type": "text", "section": "Page 139"}
{"text": "Figure 11.4: Hypothetical XYZ System using LLVM\nOnce the set of optimizations is chosen (and similar decisions are made for the code generator)\nthe image processing compiler is built into an executable or dynamic library. Since the only\nreference to the LLVM optimization passes is the simple create function defined in each .o file,\nand since the optimizers live in .a archive libraries, only the optimization passes that are actually\nused are linked into the end application, not the entire LLVM optimizer. In our example above,\nsince there is a reference to PassA and PassB, they will get linked in. Since PassB uses PassD to\ndo some analysis, PassD gets linked in. However, since PassC (and dozens of other optimizations)\naren't used, its code isn't linked into the image processing application.\nThis is where the power of the library-based design of LLVM comes into play. This straightforward\ndesign approach allows LLVM to provide a vast amount of capability, some of which may only be\nuseful to specific audiences, without punishing clients of the libraries that just want to do simple\nthings. In contrast, traditional compiler optimizers are built as a tightly interconnected mass of\ncode, which is much more difficult to subset, reason about, and come up to speed on. With LLVM\nyou can understand individual optimizers without knowing how the whole system fits together.\nThis library-based design is also the reason why so many people misunderstand what LLVM is all\nabout: the LLVM libraries have many capabilities, but they don't actually do anything by\nthemselves. It is up to the designer of the client of the libraries (e.g., the Clang C compiler) to\ndecide how to put the pieces to best use. This careful layering, factoring, and focus on subset-\nability is also why the LLVM optimizer can be used for such a broad range of different applications\nin different contexts. Also, just because LLVM provides JIT compilation capabilities, it doesn't mean\nthat every client uses it.\n11.5. Design of the Retargetable LLVM Code Generator\nThe LLVM code generator is responsible for transforming LLVM IR into target specific machine\ncode. On the one hand, it is the code generator's job to produce the best possible machine code\nfor any given target. Ideally, each code generator should be completely custom code for the\ntarget, but on the other hand, the code generators for each target need to solve very similar\nproblems. For example, each target needs to assign values to registers, and though each target\nhas different register files, the algorithms used should be shared wherever possible.\nSimilar to the approach in the optimizer, LLVM's code generator splits the code generation\nproblem into individual passes\u2014instruction selection, register allocation, scheduling, code layout\noptimization, and assembly emission\u2014and provides many builtin passes that are run by default.\nThe target author is then given the opportunity to choose among the default passes, override the\ndefaults and implement completely custom target-specific passes as required. For example, the\nx86 back end uses a register-pressure-reducing scheduler since it has very few registers, but the\nPowerPC back end uses a latency optimizing scheduler since it has many of them. The x86 back\nend uses a custom pass to handle the x87 floating point stack, and the ARM back end uses a\n", "page": 140, "type": "text", "section": "Page 140"}
{"text": "custom pass to place constant pool islands inside functions where needed. This flexibility allows\ntarget authors to produce great code without having to write an entire code generator from\nscratch for their target.\n11.5.1. LLVM Target Description Files\nThe \"mix and match\" approach allows target authors to choose what makes sense for their\narchitecture and permits a large amount of code reuse across different targets. This brings up\nanother challenge: each shared component needs to be able to reason about target specific\nproperties in a generic way. For example, a shared register allocator needs to know the register\nfile of each target and the constraints that exist between instructions and their register operands.\nLLVM's solution to this is for each target to provide a target description in a declarative domain-\nspecific language (a set of .td files) processed by the tblgen tool. The (simplified) build process\nfor the x86 target is shown in Figure 11.5.\nFigure 11.5: Simplified x86 Target Definition\nThe different subsystems supported by the .td files allow target authors to build up the different\npieces of their target. For example, the x86 back end defines a register class that holds all of its\n32-bit registers named \"GR32\" (in the .td files, target specific definitions are all caps) like this:\ndef GR32 : RegisterClass<[i32], 32,\n  [EAX, ECX, EDX, ESI, EDI, EBX, EBP, ESP,\n   R8D, R9D, R10D, R11D, R14D, R15D, R12D, R13D]> { \u2026 }\nThis definition says that registers in this class can hold 32-bit integer values (\"i32\"), prefer to be\n32-bit aligned, have the specified 16 registers (which are defined elsewhere in the .td files) and\nhave some more information to specify preferred allocation order and other things. Given this\ndefinition, specific instructions can refer to this, using it as an operand. For example, the\n\"complement a 32-bit register\" instruction is defined as:\nlet Constraints = \"$src = $dst\" in\ndef NOT32r : I<0xF7, MRM2r,\n               (outs GR32:$dst), (ins GR32:$src),\n               \"not{l}\\t$dst\",\n               [(set GR32:$dst, (not GR32:$src))]>;\nThis definition says that NOT32r is an instruction (it uses the I tblgen class), specifies encoding\ninformation (0xF7, MRM2r), specifies that it defines an \"output\" 32-bit register $dst and has a\n32-bit register \"input\" named $src (the GR32 register class defined above defines which registers\nare valid for the operand), specifies the assembly syntax for the instruction (using the {} syntax\nto handle both AT&T and Intel syntax), specifies the effect of the instruction and provides the\npattern that it should match on the last line. The \"let\" constraint on the first line tells the register\nallocator that the input and output register must be allocated to the same physical register.\nThis definition is a very dense description of the instruction, and the common LLVM code can do a\n", "page": 141, "type": "text", "section": "Page 141"}
{"text": "lot with information derived from it (by the tblgen tool). This one definition is enough for\ninstruction selection to form this instruction by pattern matching on the input IR code for the\ncompiler. It also tells the register allocator how to process it, is enough to encode and decode the\ninstruction to machine code bytes, and is enough to parse and print the instruction in a textual\nform. These capabilities allow the x86 target to support generating a stand-alone x86 assembler\n(which is a drop-in replacement for the \"gas\" GNU assembler) and disassemblers from the target\ndescription as well as handle encoding the instruction for the JIT.\nIn addition to providing useful functionality, having multiple pieces of information generated from\nthe same \"truth\" is good for other reasons. This approach makes it almost infeasible for the\nassembler and disassembler to disagree with each other in either assembly syntax or in the binary\nencoding. It also makes the target description easily testable: instruction encodings can be unit\ntested without having to involve the entire code generator.\nWhile we aim to get as much target information as possible into the .td files in a nice declarative\nform, we still don't have everything. Instead, we require target authors to write some C++ code\nfor various support routines and to implement any target specific passes they might need (like\nX86FloatingPoint.cpp, which handles the x87 floating point stack). As LLVM continues to grow\nnew targets, it becomes more and more important to increase the amount of the target that can\nbe expressed in the .td file, and we continue to increase the expressiveness of the .td files to\nhandle this. A great benefit is that it gets easier and easier write targets in LLVM as time goes on.\n11.6. Interesting Capabilities Provided by a Modular\nDesign\nBesides being a generally elegant design, modularity provides clients of the LLVM libraries with\nseveral interesting capabilities. These capabilities stem from the fact that LLVM provides\nfunctionality, but lets the client decide most of the policies on how to use it.\n11.6.1. Choosing When and Where Each Phase Runs\nAs mentioned earlier, LLVM IR can be efficiently (de)serialized to/from a binary format known as\nLLVM bitcode. Since LLVM IR is self-contained, and serialization is a lossless process, we can do\npart of compilation, save our progress to disk, then continue work at some point in the future.\nThis feature provides a number of interesting capabilities including support for link-time and install-\ntime optimization, both of which delay code generation from \"compile time\".\nLink-Time Optimization (LTO) addresses the problem where the compiler traditionally only sees one\ntranslation unit (e.g., a .c file with all its headers) at a time and therefore cannot do optimizations\n(like inlining) across file boundaries. LLVM compilers like Clang support this with the -flto or -O4\ncommand line option. This option instructs the compiler to emit LLVM bitcode to the .ofile instead\nof writing out a native object file, and delays code generation to link time, shown in Figure 11.6.\nFigure 11.6: Link-Time Optimization\nDetails differ depending on which operating system you're on, but the important bit is that the\nlinker detects that it has LLVM bitcode in the .o files instead of native object files. When it sees\nthis, it reads all the bitcode files into memory, links them together, then runs the LLVM optimizer\nover the aggregate. Since the optimizer can now see across a much larger portion of the code, it\ncan inline, propagate constants, do more aggressive dead code elimination, and more across file\nboundaries. While many modern compilers support LTO, most of them (e.g., GCC, Open64, the\n", "page": 142, "type": "text", "section": "Page 142"}
{"text": "Intel compiler, etc.) do so by having an expensive and slow serialization process. In LLVM, LTO\nfalls out naturally from the design of the system, and works across different source languages\n(unlike many other compilers) because the IR is truly source language neutral.\nInstall-time optimization is the idea of delaying code generation even later than link time, all the way\nto install time, as shown in Figure 11.7. Install time is a very interesting time (in cases when\nsoftware is shipped in a box, downloaded, uploaded to a mobile device, etc.), because this is when\nyou find out the specifics of the device you're targeting. In the x86 family for example, there are\nbroad variety of chips and characteristics. By delaying instruction choice, scheduling, and other\naspects of code generation, you can pick the best answers for the specific hardware an\napplication ends up running on.\nFigure 11.7: Install-Time Optimization\n11.6.2. Unit Testing the Optimizer\nCompilers are very complicated, and quality is important, therefore testing is critical. For example,\nafter fixing a bug that caused a crash in an optimizer, a regression test should be added to make\nsure it doesn't happen again. The traditional approach to testing this is to write a .c file (for\nexample) that is run through the compiler, and to have a test harness that verifies that the\ncompiler doesn't crash. This is the approach used by the GCC test suite, for example.\nThe problem with this approach is that the compiler consists of many different subsystems and\neven many different passes in the optimizer, all of which have the opportunity to change what the\ninput code looks like by the time it gets to the previously buggy code in question. If something\nchanges in the front end or an earlier optimizer, a test case can easily fail to test what it is\nsupposed to be testing.\nBy using the textual form of LLVM IR with the modular optimizer, the LLVM test suite has highly\nfocused regression tests that can load LLVM IR from disk, run it through exactly one optimization\npass, and verify the expected behavior. Beyond crashing, a more complicated behavioral test\nwants to verify that an optimization is actually performed. Here is a simple test case that checks to\nsee that the constant propagation pass is working with add instructions:\n; RUN: opt < %s -constprop -S | FileCheck %s\ndefine i32 @test() {\n  %A = add i32 4, 5\n  ret i32 %A\n  ; CHECK: @test()\n  ; CHECK: ret i32 9\n}\nThe RUN line specifies the command to execute: in this case, the opt and FileCheck command\nline tools. The opt program is a simple wrapper around the LLVM pass manager, which links in all\nthe standard passes (and can dynamically load plugins containing other passes) and exposes\nthem through to the command line. The FileCheck tool verifies that its standard input matches a\nseries of CHECK directives. In this case, this simple test is verifying that the constprop pass is\nfolding the add of 4 and 5 into 9.\nWhile this might seem like a really trivial example, this is very difficult to test by writing .c files:\nfront ends often do constant folding as they parse, so it is very difficult and fragile to write code\nthat makes its way downstream to a constant folding optimization pass. Because we can load\n", "page": 143, "type": "text", "section": "Page 143"}
{"text": "LLVM IR as text and send it through the specific optimization pass we're interested in, then dump\nout the result as another text file, it is really straightforward to test exactly what we want, both for\nregression and feature tests.\n11.6.3. Automatic Test Case Reduction with BugPoint\nWhen a bug is found in a compiler or other client of the LLVM libraries, the first step to fixing it is\nto get a test case that reproduces the problem. Once you have a test case, it is best to minimize it\nto the smallest example that reproduces the problem, and also narrow it down to the part of LLVM\nwhere the problem happens, such as the optimization pass at fault. While you eventually learn\nhow to do this, the process is tedious, manual, and particularly painful for cases where the\ncompiler generates incorrect code but does not crash.\nThe LLVM BugPoint tool7 uses the IR serialization and modular design of LLVM to automate this\nprocess. For example, given an input .ll or .bc file along with a list of optimization passes that\ncauses an optimizer crash, BugPoint reduces the input to a small test case and determines which\noptimizer is at fault. It then outputs the reduced test case and the opt command used to\nreproduce the failure. It finds this by using techniques similar to \"delta debugging\" to reduce the\ninput and the optimizer pass list. Because it knows the structure of LLVM IR, BugPoint does not\nwaste time generating invalid IR to input to the optimizer, unlike the standard \"delta\" command line\ntool.\nIn the more complex case of a miscompilation, you can specify the input, code generator\ninformation, the command line to pass to the executable, and a reference output. BugPoint will\nfirst determine if the problem is due to an optimizer or a code generator, and will then repeatedly\npartition the test case into two pieces: one that is sent into the \"known good\" component and one\nthat is sent into the \"known buggy\" component. By iteratively moving more and more code out of\nthe partition that is sent into the known buggy code generator, it reduces the test case.\nBugPoint is a very simple tool and has saved countless hours of test case reduction throughout\nthe life of LLVM. No other open source compiler has a similarly powerful tool, because it relies on a\nwell-defined intermediate representation. That said, BugPoint isn't perfect, and would benefit from\na rewrite. It dates back to 2002, and is typically only improved when someone has a really tricky\nbug to track down that the existing tool doesn't handle well. It has grown over time, accreting new\nfeatures (such as JIT debugging) without a consistent design or owner.\n11.7. Retrospective and Future Directions\nLLVM's modularity wasn't originally designed to directly achieve any of the goals described here. It\nwas a self-defense mechanism: it was obvious that we wouldn't get everything right on the first\ntry. The modular pass pipeline, for example, exists to make it easier to isolate passes so that they\ncan be discarded after being replaced by better implementations8.\nAnother major aspect of LLVM remaining nimble (and a controversial topic with clients of the\nlibraries) is our willingness to reconsider previous decisions and make widespread changes to APIs\nwithout worrying about backwards compatibility. Invasive changes to LLVM IR itself, for example,\nrequire updating all of the optimization passes and cause substantial churn to the C++ APIs.\nWe've done this on several occasions, and though it causes pain for clients, it is the right thing to\ndo to maintain rapid forward progress. To make life easier for external clients (and to support\nbindings for other languages), we provide C wrappers for many popular APIs (which are intended\nto be extremely stable) and new versions of LLVM aim to continue reading old .ll and .bc files.\nLooking forward, we would like to continue making LLVM more modular and easier to subset. For\nexample, the code generator is still too monolithic: it isn't currently possible to subset LLVM based\non features. For example, if you'd like to use the JIT, but have no need for inline assembly,\nexception handling, or debug information generation, it should be possible to build the code\ngenerator without linking in support for these features. We are also continuously improving the\nquality of code generated by the optimizer and code generator, adding IR features to better\nsupport new language and target constructs, and adding better support for performing high-level\nlanguage-specific optimizations in LLVM.\n", "page": 144, "type": "text", "section": "Page 144"}
{"text": "The LLVM project continues to grow and improve in numerous ways. It is really exciting to see the\nnumber of different ways that LLVM is being used in other projects and how it keeps turning up in\nsurprising new contexts that its designers never even thought about. The new LLDB debugger is\na great example of this: it uses the C/C++/Objective-C parsers from Clang to parse expressions,\nuses the LLVM JIT to translate these into target code, uses the LLVM disassemblers, and uses\nLLVM targets to handle calling conventions among other things. Being able to reuse this existing\ncode allows people developing debuggers to focus on writing the debugger logic, instead of\nreimplementing yet another (marginally correct) C++ parser.\nDespite its success so far, there is still a lot left to be done, as well as the ever-present risk that\nLLVM will become less nimble and more calcified as it ages. While there is no magic answer to this\nproblem, I hope that the continued exposure to new problem domains, a willingness to reevaluate\nprevious decisions, and to redesign and throw away code will help. After all, the goal isn't to be\nperfect, it is to keep getting better over time.\nFootnotes\n1. http://llvm.org\n2. http://clang.llvm.org\n3. http://en.wikipedia.org/wiki/List_of_JVM_languages\n4. A backronym that now stands for \"GNU Compiler Collection\".\n5. This is in contrast to a two-address instruction set, like X86, which destructively updates an\ninput register, or one-address machines which take one explicit operand and operate on an\naccumulator or the top of the stack on a stack machine.\n6. For all the details, please see Writing an LLVM Pass manual at\nhttp://llvm.org/docs/WritingAnLLVMPass.html.\n7. http://llvm.org/docs/Bugpoint.html\n8. I often say that none of the subsystems in LLVM are really good until they have been\nrewritten at least once.\n", "page": 145, "type": "text", "section": "Page 145"}
{"text": "The Architecture of\nOpen Source Applications\nAmy Brown and Greg Wilson (eds.)\nISBN 978-1-257-63801-7 \nLicense / Buy / Contribute\nChapter 12. Mercurial\nDirkjan Ochtman\nMercurial is a modern distributed version control system (VCS), written mostly in Python with bits\nand pieces in C for performance. In this chapter, I will discuss some of the decisions involved in\ndesigning Mercurial's algorithms and data structures. First, allow me to go into a short history of\nversion control systems, to add necessary context.\n12.1. A Short History of Version Control\nWhile this chapter is primarily about Mercurial's software architecture, many of the concepts are\nshared with other version control systems. In order to fruitfully discuss Mercurial, I'd like to start\noff by naming some of the concepts and actions in different version control systems. To put all of\nthis in perspective, I will also provide a short history of the field.\nVersion control systems were invented to help developers work on software systems\nsimultaneously, without passing around full copies and keeping track of file changes themselves.\nLet's generalize from software source code to any tree of files. One of the primary functions of\nversion control is to pass around changes to the tree. The basic cycle is something like this:\n1. Get the latest tree of files from someone else\n2. Work on a set of changes to this version of the tree\n3. Publish the changes so that others can retrieve them\nThe first action, to get a local tree of files, is called a checkout. The store where we retrieve and\npublish our changes is called a repository, while the result of the checkout is called a working\ndirectory, working tree, or working copy. Updating a working copy with the latest files from the\nrepository is simply called update; sometimes this requires merging, i.e., combining changes from\ndifferent users in a single file. A diff command allows us to review changes between two revisions\nof a tree or file, where the most common mode is to check the local (unpublished) changes in\nyour working copy. Changes are published by issuing a commit command, which will save the\nchanges from the working directory to the repository.\n12.1.1. Centralized Version Control\nThe first version control system was the Source Code Control System, SCCS, first described in\n1975. It was mostly a way of saving deltas to single files that was more efficient than just keeping\naround copies, and didn't help with publishing these changes to others. It was followed in 1982 by\nthe Revision Control System, RCS, which was a more evolved and free alternative to SCCS (and\nwhich is still being maintained by the GNU project).\nAfter RCS came CVS, the Concurrent Versioning System, first released in 1986 as a set of scripts\nto manipulate RCS revision files in groups. The big innovation in CVS is the notion that multiple\nusers can edit simultaneously, with merges being done after the fact (concurrent edits). This also\nrequired the notion of edit conflicts. Developers may only commit a new version of some file if it's\nbased on the latest version available in the repository. If there are changes in the repository and in\nmy working directory, I have to resolve any conflicts resulting from those changes (edits changing\nthe same lines).\nCVS also pioneered the notions of branches, which allow developers to work on different things in\n", "page": 146, "type": "text", "section": "Page 146"}
{"text": "parallel, and tags, which enable naming a consistent snapshot for easy reference. While CVS\ndeltas were initially communicated via the repository on a shared filesystem, at some point CVS\nalso implemented a client-server architecture for use across large networks (such as the\nInternet).\nIn 2000, three developers got together to build a new VCS, christened Subversion, with the\nintention of fixing some of the larger warts in CVS. Most importantly, Subversion works on whole\ntrees at a time, meaning changes in a revisions should be atomic, consistent, isolated, and\ndurable. Subversion working copies also retain a pristine version of the checked out revision in the\nworking directory, so that the common diff operation (comparing the local tree against a checked-\nout changeset) is local and thus fast.\nOne of the interesting concepts in Subversion is that tags and branches are part of a project tree.\nA Subversion project is usually divided into three areas: tags, branches, and trunk. This design\nhas proved very intuitive to users who were unfamiliar with version control systems, although the\nflexibility inherent in this design has caused numerous problems for conversion tools, mostly\nbecause tags and branches have more structural representation in other systems.\nAll of the aforementioned systems are said to be centralized; to the extent that they even know\nhow to exchange changes (starting with CVS), they rely on some other computer to keep track of\nthe history of the repository. Distributed version control systems instead keep a copy of all or\nmost of the repository history on each computer that has a working directory of that repository.\n12.1.2. Distributed Version Control\nWhile Subversion was a clear improvement over CVS, there are still a number of shortcomings.\nFor one thing, in all centralized systems, committing a changeset and publishing it are effectively\nthe same thing, since repository history is centralized in one place. This means that committing\nchanges without network access is impossible. Secondly, repository access in centralized systems\nalways needs one or more network round trips, making it relatively slow compared to the local\naccesses needed with distributed systems. Third, the systems discussed above were not very\ngood at tracking merges (some have since grown better at it). In large groups working\nconcurrently, it's important that the version control system records what changes have been\nincluded in some new revision, so that nothing gets lost and subsequent merges can make use of\nthis information. Fourth, the centralization required by traditional VCSes sometimes seems\nartificial, and promotes a single place for integration. Advocates of distributed VCSes argue that a\nmore distributed system allows for a more organic organization, where developers can push\naround and integrate changes as the project requires at each point in time.\nA number of new tools have been developed to address these needs. From where I sit (the open\nsource world), the most notable three of these in 2011 are Git, Mercurial and Bazaar. Both Git and\nMercurial were started in 2005 when the Linux kernel developers decided to no longer use the\nproprietary BitKeeper system. Both were started by Linux kernel developers (Linus Torvalds and\nMatt Mackall, respectively) to address the need for a version control system that could handle\nhundreds of thousands of changesets in tens of thousands of files (for example, the kernel). Both\nMatt and Linus were also heavily influenced by the Monotone VCS. Bazaar was developed\nseparately but gained widespread usage around the same time, when it was adopted by Canonical\nfor use with all of their projects.\nBuilding a distributed version control system obviously comes with some challenges, many of\nwhich are inherent in any distributed system. For one thing, while the source control server in\ncentralized systems always provided a canonical view of history, there is no such thing in a\ndistributed VCS. Changesets can be committed in parallel, making it impossible to temporally order\nrevisions in any given repository.\nThe solution that has been almost universally adopted is to use a directed acyclic graph (DAG) of\nchangesets instead of a linear ordering (Figure 12.1). That is, a newly committed changeset is the\nchild revision of the revision it was based on, and no revision can depend on itself or its\ndescendant revisions. In this scheme, we have three special types of revisions: root revisions\nwhich have no parents (a repository can have multiple roots), merge revisions which have more\n", "page": 147, "type": "text", "section": "Page 147"}
{"text": "than one parent, and head revisions which have no children. Each repository starts from an empty\nroot revision and proceeds from there along a line of changesets, ending up in one or more\nheads. When two users have committed independently and one of them wants to pull in the\nchanges from the other, he or she will have to explicitly merge the other's changes into a new\nrevision, which he subsequently commits as a merge revision.\nFigure 12.1: Directed Acyclic Graph of Revisions\nNote that the DAG model helps solve some of the problems that are hard to solve in centralized\nversion control systems: merge revisions are used to record information about newly merged\nbranches of the DAG. The resulting graph can also usefully represent a large group of parallel\nbranches, merging into smaller groups, finally merging into one special branch that's considered\ncanonical.\nThis approach requires that the system keep track of the ancestry relations between changesets;\nto facilitate exchange of changeset data, this is usually done by having changesets keep track of\ntheir parents. To do this, changesets obviously also need some kind of identifier. While some\nsystems use a UUID or a similar kind of scheme, both Git and Mercurial have opted to use SHA1\nhashes of the contents of the changesets. This has the additional useful property that the\nchangeset ID can be used to verify the changeset contents. In fact, because the parents are\nincluded in the hashed data, all history leading up to any revision can be verified using its hash.\nAuthor names, commit messages, timestamps and other changeset metadata is hashed just like\nthe actual file contents of a new revision, so that they can also be verified. And since timestamps\nare recorded at commit time, they too do not necessarily progress linearly in any given repository.\nAll of this can be hard for people who have previously only used centralized VCSes to get used to:\nthere is no nice integer to globally name a revision, just a 40-character hexadecimal string.\nMoreover, there's no longer any global ordering, just a local ordering; the only global \"ordering\" is a\nDAG instead of a line. Accidentally starting a new head of development by committing against a\nparent revision that already had another child changeset can be confusing when you're used to a\nwarning from the VCS when this kind of thing happens.\nLuckily, there are tools to help visualize the tree ordering, and Mercurial provides an unambiguous\nshort version of the changeset hash and a local-only linear number to aid identification. The latter\nis a monotonically climbing integer that indicates the order in which changesets have entered the\nclone. Since this order can be different from clone to clone, it cannot be relied on for non-local\noperations.\n12.2. Data Structures\nNow that the concept of a DAG should be somewhat clear, let's try and see how DAGs are stored\nin Mercurial. The DAG model is central to the inner workings of Mercurial, and we actually use\nseveral different DAGs in the repository storage on disk (as well as the in-memory structure of the\ncode). This section explains what they are and how they fit together.\n12.2.1. Challenges\nBefore we dive into actual data structures, I'd like to provide some context about the environment\nin which Mercurial evolved. The first notion of Mercurial can be found in an email Matt Mackall sent\nto the Linux Kernel Mailing List on April 20, 2005. This happened shortly after it was decided that\nBitKeeper could no longer be used for the development of the kernel. Matt started his mail by\noutlining some goals: to be simple, scalable, and efficient.\n", "page": 148, "type": "text", "section": "Page 148"}
{"text": "In [Mac06], Matt claimed that a modern VCS must deal with trees containing millions of files,\nhandle millions of changesets, and scale across many thousands of users creating new revisions\nin parallel over a span of decades. Having set the goals, he reviewed the limiting technology\nfactors:\nspeed: CPU\ncapacity: disk and memory\nbandwidth: memory, LAN, disk, and WAN\ndisk seek rate\nDisk seek rate and WAN bandwidth are the limiting factors today, and should thus be optimized\nfor. The paper goes on to review common scenarios or criteria for evaluating the performance of\nsuch a system at the file level:\nStorage compression: what kind of compression is best suited to save the file history on\ndisk? Effectively, what algorithm makes the most out of the I/O performance while preventing\nCPU time from becoming a bottleneck?\nRetrieving arbitrary file revisions: a number of version control systems will store a given\nrevision in such a way that a large number of older revisions must be read to reconstruct the\nnewer one (using deltas). We want to control this to make sure that retrieving old revisions is\nstill fast.\nAdding file revisions: we regularly add new revisions. We don't want to rewrite old revisions\nevery time we add a new one, because that would become too slow when there are many\nrevisions.\nShowing file history: we want to be able to review a history of all changesets that touched a\ncertain file. This also allows us to do annotations (which used to be called blame in CVS but\nwas renamed to annotate in some later systems to remove the negative connotation):\nreviewing the originating changeset for each line currently in a file.\nThe paper goes on to review similar scenarios at the project level. Basic operations at this level are\nchecking out a revision, committing a new revision, and finding differences in the working\ndirectory. The latter, in particular, can be slow for large trees (like those of the Mozilla or NetBeans\nprojects, both of which use Mercurial for their version control needs).\n12.2.2. Fast Revision Storage: Revlogs\nThe solution Matt came up with for Mercurial is called the revlog (short for revision log). The revlog\nis a way of efficiently storing revisions of file contents (each with some amount of changes\ncompared to the previous version). It needs to be efficient in both access time (thus optimizing for\ndisk seeks) and storage space, guided by the common scenarios outlined in the previous section.\nTo do this, a revlog is really two files on disk: an index and the data file.\n6 bytes\nhunk offset\n2 bytes\nflags\n4 bytes\nhunk length\n4 bytes\nuncompressed length\n4 bytes\nbase revision\n4 bytes\nlink revision\n4 bytes\nparent 1 revision\n4 bytes\nparent 2 revision\n32 bytes hash\nTable 12.1: Mercurial Record Format\nThe index consists of fixed-length records, whose contents are detailed in Table 12.1. Having\nfixed-length records is nice, because it means that having the local revision number allows direct\n(i.e., constant-time) access to the revision: we can simply read to the position (index-\nlength\u00d7revision) in the index file, to locate the data. Separating the index from the data also\n", "page": 149, "type": "text", "section": "Page 149"}
{"text": "means we can quickly read the index data without having to seek the disk through all the file data.\nThe hunk offset and hunk length specify a chunk of the data file to read in order to get the\ncompressed data for that revision. To get the original data we have to start by reading the base\nrevision, and apply deltas through to this revision. The trick here is the decision on when to store a\nnew base revision. This decision is based on the cumulative size of the deltas compared to the\nuncompressed length of the revision (data is compressed using zlib to use even less space on\ndisk). By limiting the length of the delta chain in this way, we make sure that reconstruction of the\ndata in a given revision does not require reading and applying lots of deltas.\nLink revisions are used to have dependent revlogs point back to the highest-level revlog (we'll talk\nmore about this in a little bit), and the parent revisions are stored using the local integer revision\nnumber. Again, this makes it easy to look up their data in the relevant revlog. The hash is used to\nsave the unique identifier for this changeset. We have 32 bytes instead of the 20 bytes required\nfor SHA1 in order to allow future expansion.\n12.2.3. The Three Revlogs\nWith the revlog providing a generic structure for historic data, we can layer the data model for our\nfile tree on top of that. It consists of three types of revlogs: the changelog, manifests, and filelogs.\nThe changelog contains metadata for each revision, with a pointer into the manifest revlog (that is,\na node id for one revision in the manifest revlog). In turn, the manifest is a file that has a list of\nfilenames plus the node id for each file, pointing to a revision in that file's filelog. In the code, we\nhave classes for changelog, manifest, and filelog that are subclasses of the generic revlog class,\nproviding a clean layering of both concepts.\nFigure 12.2: Log Structure\nA changelog revision looks like this:\n0a773e3480fe58d62dcc67bd9f7380d6403e26fa\nDirkjan Ochtman <dirkjan@ochtman.nl>\n1276097267 -7200\nmercurial/discovery.py\ndiscovery: fix description line\nThis is the value you get from the revlog layer; the changelog layer turns it into a simple list of\nvalues. The initial line provides the manifest hash, then we get author name, date and time (in the\nform of a Unix timestamp and a timezone offset), a list of affected files, and the description\nmessage. One thing is hidden here: we allow arbitrary metadata in the changelog, and to stay\nbackwards compatible we added those bits to go after the timestamp.\nNext comes the manifest:\n.hgignore\\x006d2dc16e96ab48b2fcca44f7e9f4b8c3289cb701\n.hgsigs\\x00de81f258b33189c609d299fd605e6c72182d7359\n.hgtags\\x00b174a4a4813ddd89c1d2f88878e05acc58263efa\nCONTRIBUTORS\\x007c8afb9501740a450c549b4b1f002c803c45193a\nCOPYING\\x005ac863e17c7035f1d11828d848fb2ca450d89794\n\u2026\n", "page": 150, "type": "text", "section": "Page 150"}
{"text": "This is the manifest revision that changeset 0a773e points to (Mercurial's UI allows us to shorten\nthe identifier to any unambiguous prefix). It is a simple list of all files in the tree, one per line, where\nthe filename is followed by a NULL byte, followed by the hex-encoded node id that points into the\nfile's filelog. Directories in the tree are not represented separately, but simply inferred from\nincluding slashes in the file paths. Remember that the manifest is diffed in storage just like every\nrevlog, so this structure should make it easy for the revlog layer to store only changed files and\ntheir new hashes in any given revision. The manifest is usually represented as a hashtable-like\nstructure in Mercurial's Python code, with filenames as keys and nodes as values.\nThe third type of revlog is the filelog. Filelogs are stored in Mercurial's internal store directory,\nwhere they're named almost exactly like the file they're tracking. The names are encoded a little bit\nto make sure things work across all major operating systems. For example, we have to deal with\ncasefolding filesystems on Windows and Mac OS X, specific disallowed filenames on Windows, and\ndifferent character encodings as used by the various filesystems. As you can imagine, doing this\nreliably across operating systems can be fairly painful. The contents of a filelog revision, on the\nother hand, aren't nearly as interesting: just the file contents, except with some optional metadata\nprefix (which we use for tracking file copies and renames, among other minor things).\nThis data model gives us complete access to the data store in a Mercurial repository, but it's not\nalways very convenient. While the actual underlying model is vertically oriented (one filelog per file),\nMercurial developers often found themselves wanting to deal with all details from a single revision,\nwhere they start from a changeset from the changelog and want easy access to the manifest and\nfilelogs from that revision. They later invented another set of classes, layered cleanly on top of the\nrevlogs, which do exactly that. These are called contexts.\nOne nice thing about the way the separate revlogs are set up is the ordering. By ordering appends\nso that filelogs get appended to first, then the manifest, and finally the changelog, the repository is\nalways in a consistent state. Any process that starts reading the changelog can be sure all\npointers into the other revlogs are valid, which takes care of a number of issues in this\ndepartment. Nevertheless, Mercurial also has some explicit locks to make sure there are no two\nprocesses appending to the revlogs in parallel.\n12.2.4. The Working Directory\nA final important data structure is what we call the dirstate. The dirstate is a representation of\nwhat's in the working directory at any given point. Most importantly, it keeps track of what revision\nhas been checked out: this is the baseline for all comparisons from the status or diff\ncommands, and also determines the parent(s) for the next changeset to be committed. The\ndirstate will have two parents set whenever the merge command has been issued, trying to merge\none set of changes into the other.\nBecause status and diff are very common operations (they help you check the progress of\nwhat you've currently got against the last changeset), the dirstate also contains a cache of the\nstate of the working directory the last time it was traversed by Mercurial. Keeping track of last\nmodified timestamps and file sizes makes it possible to speed up tree traversal. We also need to\nkeep track of the state of the file: whether it's been added, removed, or merged in the working\ndirectory. This will again help speed up traversing the working directory, and makes it easy to get\nthis information at commit time.\n12.3. Versioning Mechanics\nNow that you are familiar with the underlying data model and the structure of the code at the\nlower levels of Mercurial, let's move up a little bit and consider how Mercurial implements version\ncontrol concepts on top of the foundation described in the previous section.\n12.3.1. Branches\nBranches are commonly used to separate different lines of development that will be integrated\nlater. This might be because someone is experimenting with a new approach, just to be able to\n", "page": 151, "type": "text", "section": "Page 151"}
{"text": "always keep the main line of development in a shippable state (feature branches), or to be able to\nquickly release fixes for an old release (maintenance branches). Both approaches are commonly\nused, and are supported by all modern version control systems. While implicit branches are\ncommon in DAG-based version control named branches (where the branch name is saved in the\nchangeset metadata) are not as common.\nOriginally, Mercurial had no way to explicitly name branches. Branches were instead handled by\nmaking different clones and publishing them separately. This is effective, easy to understand, and\nespecially useful for feature branches, because there is little overhead. However, in large projects,\nclones can still be quite expensive: while the repository store will be hardlinked on most\nfilesystems, creating a separate working tree is slow and may require a lot of disk space.\nBecause of these downsides, Mercurial added a second way to do branches: including a branch\nname in the changeset metadata. A branch command was added that can set the branch name\nfor the current working directory, such that that branch name will be used for the next commit.\nThe normal update command can be used to update to a branch name, and a changeset\ncommitted on a branch will always be related to that branch. This approach is called named\nbranches. However, it took a few more Mercurial releases before Mercurial started including a way\nto close these branches up again (closing a branch will hide the branch from view in a list of\nbranches). Branch closing is implemented by adding an extra field in the changeset metadata,\nstating that this changeset closes the branch. If the branch has more than one head, all of them\nhave to be closed before the branch disappears from the list of branches in the repository.\nOf course, there's more than one way to do it. Git has a different way of naming branches, using\nreferences. References are names pointing to another object in the Git history, usually a\nchangeset. This means that Git's branches are ephemeral: once you remove the reference, there\nis no trace of the branch ever having existed, similar to what you would get when using a separate\nMercurial clone and merging it back into another clone. This makes it very easy and lightweight to\nmanipulate branches locally, and prevents cluttering of the list of branches.\nThis way of branching turned out to be very popular, much more popular than either named\nbranches or branch clones in Mercurial. This has resulted in the bookmarksq extension, which will\nprobably be folded into Mercurial in the future. It uses a simple unversioned file to keep track of\nreferences. The wire protocol used to exchange Mercurial data has been extended to enable\ncommunicating about bookmarks, making it possible to push them around.\n12.3.2. Tags\nAt first sight, the way Mercurial implements tags can be a bit confusing. The first time you add a\ntag (using the tag command), a file called .hgtags gets added to the repository and committed.\nEach line in that file will contain a changeset node id and the tag name for that changeset node.\nThus, the tags file is treated the same way as any other file in the repository.\nThere are three important reasons for this. First, it must be possible to change tags; mistakes do\nhappen, and it should be possible to fix them or delete the mistake. Second, tags should be part\nof changeset history: it's valuable to see when a tag was made, by whom, and for what reason, or\neven if a tag was changed. Third, it should be possible to tag a changeset retroactively. For\nexample, some projects extensively test drive a release artifact exported from the version control\nsystem before releasing it.\nThese properties all fall easily out of the .hgtags design. While some users are confused by the\npresence of the .hgtags file in their working directories, it makes integration of the tagging\nmechanism with other parts of Mercurial (for example, synchronization with other repository\nclones) very simple. If tags existed outside the source tree (as they do in Git, for example),\nseparate mechanisms would have to exist to audit the origin of tags and to deal with conflicts\nfrom (parallel) duplicate tags. Even if the latter is rare, it's nice to have a design where these things\nare not even an issue.\nTo get all of this right, Mercurial only ever appends new lines to the .hgtags file. This also\nfacilitates merging the file if the tags were created in parallel in different clones. The newest node id\n", "page": 152, "type": "text", "section": "Page 152"}
{"text": "for any given tag always takes precedence, and adding the null node id (representing the empty\nroot revision all repositories have in common) will have the effect of deleting the tag. Mercurial will\nalso consider tags from all branches in the repository, using recency calculations to determine\nprecedence among them.\n12.4. General Structure\nMercurial is almost completely written in Python, with only a few bits and pieces in C because they\nare critical to the performance of the whole application. Python was deemed a more suitable\nchoice for most of the code because it is much easier to express high-level concepts in a dynamic\nlanguage like Python. Since much of the code is not really critical to performance, we don't mind\ntaking the hit in exchange for making the coding easier for ourselves in most parts.\nA Python module corresponds to a single file of code. Modules can contain as much code as\nneeded, and are thus an important way to organize code. Modules may use types or call functions\nfrom other modules by explicitly importing the other modules. A directory containing an\n__init__.py module is said to be a package, and will expose all contained modules and packages\nto the Python importer.\nMercurial by default installs two packages into the Python path: mercurial and hgext. The\nmercurial package contains the core code required to run Mercurial, while hgext contains a\nnumber of extensions that were deemed useful enough to be delivered alongside the core.\nHowever, they must still be enabled by hand in a configuration file if desired (which we will discuss\nlater.)\nTo be clear, Mercurial is a command-line application. This means that we have a simple interface:\nthe user calls the hg script with a command. This command (like log, diff or commit) may take a\nnumber of options and arguments; there are also some options that are valid for all commands.\nNext, there are three different things that can happen to the interface.\nhg will often output something the user asked for or show status messages\nhg can ask for further input through command-line prompts\nhg may launch an external program (such as an editor for the commit message or a\nprogram to help merging code conflicts)\n", "page": 153, "type": "text", "section": "Page 153"}
{"text": "Figure 12.3: Import Graph\nThe start of this process can neatly be observed from the import graph in Figure 12.3. All\ncommand-line arguments are passed to a function in the dispatch module. The first thing that\nhappens is that a ui object is instantiated. The ui class will first try to find configuration files in a\nnumber of well-known places (such as your home directory), and save the configuration options\nin the ui object. The configuration files may also contain paths to extensions, which must also be\nloaded at this point. Any global options passed on the command-line are also saved to the ui\nobject at this point.\nAfter this is done, we have to decide whether to create a repository object. While most commands\nrequire a local repository (represented by the localrepo class from the localrepo module),\nsome commands may work on remote repositories (either HTTP, SSH, or some other registered\nform), while some commands can do their work without referring to any repository. The latter\ncategory includes the init command, for example, which is used to initialize a new repository.\nAll core commands are represented by a single function in the commands module; this makes it\nreally easy to find the code for any given command. The commands module also contains a\nhashtable that maps the command name to the function and describes the options that it takes.\nThe way this is done also allows for sharing common sets of options (for example, many\ncommands have options that look like the ones the log command uses). The options description\nallows the dispatch module to check the given options for any command, and to convert any\nvalues passed in to the type expected by the command function. Almost every function also gets\nthe ui object and the repository object to work with.\n12.5. Extensibility\n", "page": 154, "type": "text", "section": "Page 154"}
{"text": "One of the things that makes Mercurial powerful is the ability to write extensions for it. Since\nPython is a relatively easy language to get started with, and Mercurial's API is mostly quite well-\ndesigned (although certainly under-documented in places), a number of people actually first\nlearned Python because they wanted to extend Mercurial.\n12.5.1. Writing Extensions\nExtensions must be enabled by adding a line to one of the configuration files read by Mercurial on\nstartup; a key is provided along with the path to any Python module. There are several ways to\nadd functionality:\nadding new commands;\nwrapping existing commands;\nwrapping the repository used;\nwrap any function in Mercurial; and\nadd new repository types.\nAdding new commands can be done simply by adding a hashtable called cmdtable to the\nextension module. This will get picked up by the extension loader, which will add it to the\ncommands table considered when a command is dispatched. Similarly, extensions can define\nfunctions called uisetup and reposetup which are called by the dispatching code after the UI\nand repository have been instantiated. One common behavior is to use a reposetup function to\nwrap the repository in a repository subclass provided by the extension. This allows the extension\nto modify all kinds of basic behavior. For example, one extension I have written hooks into the\nuisetup and sets the ui.username configuration property based on the SSH authentication details\navailable from the environment.\nMore extreme extensions can be written to add repository types. For example, the\nhgsubversion project (not included as part of Mercurial) registers a repository type for\nSubversion repositories. This makes it possible to clone from a Subversion repository almost as if\nit were a Mercurial repository. It's even possible to push back to a Subversion repository, although\nthere are a number of edge cases because of the impedance mismatch between the two systems.\nThe user interface, on the other hand, is completely transparent.\nFor those who want to fundamentally change Mercurial, there is something commonly called\n\"monkeypatching\" in the world of dynamic languages. Because extension code runs in the same\naddress space as Mercurial, and Python is a fairly flexible language with extensive reflection\ncapabilities, it's possible (and even quite easy) to modify any function or class defined in Mercurial.\nWhile this can result in kind of ugly hacks, it's also a very powerful mechanism. For example, the\nhighlight extension that lives in hgext modifies the built-in webserver to add syntax highlighting\nto pages in the repository browser that allow you to inspect file contents.\nThere's one more way to extend Mercurial, which is much simpler: aliases. Any configuration file\ncan define an alias as a new name for an existing command with a specific group of options\nalready set. This also makes it possible to give shorter names to any commands. Recent versions\nof Mercurial also include the ability to call a shell command as an alias, so that you can design\ncomplicated commands using nothing but shell scripting.\n12.5.2. Hooks\nVersion control systems have long provided hooks as a way for VCS events to interact with the\noutside world. Common usage includes sending off a notification to a continuous integration\nsystem or updating the working directory on a web server so that changes become world-visible.\nOf course, Mercurial also includes a subsystem to invoke hooks like this.\nIn fact, it again contains two variants. One is more like traditional hooks in other version control\nsystems, in that it invokes scripts in the shell. The other is more interesting, because it allows\nusers to invoke Python hooks by specifying a Python module and a function name to call from\nthat module. Not only is this faster because it runs in the same process, but it also hands off repo\nand ui objects, meaning you can easily initiate more complex interactions inside the VCS.\n", "page": 155, "type": "text", "section": "Page 155"}
{"text": "Hooks in Mercurial can be divided in to pre-command, post-command, controlling, and\nmiscellaneous hooks. The first two are trivially defined for any command by specifying a\npre-command or post-command key in the hooks section of a configuration file. For the other two\ntypes, there's a predefined set of events. The difference in controlling hooks is that they are run\nright before something happens, and may not allow that event to progress further. This is\ncommonly used to validate changesets in some way on a central server; because of Mercurial's\ndistributed nature, no such checks can be enforced at commit time. For example, the Python\nproject uses a hook to make sure some aspects of coding style are enforced throughout the code\nbase\u2014if a changeset adds code in a style that is not allowed, it will be rejected by the central\nrepository.\nAnother interesting use of hooks is a pushlog, which is used by Mozilla and a number of corporate\norganizations. A pushlog records each push (since a push may contain any number of\nchangesets) and records who initiated that push and when, providing a kind of audit trail for the\nrepository.\n12.6. Lessons Learned\nOne of the first decisions Matt made when he started to develop Mercurial was to develop it in\nPython. Python has been great for the extensibility (through extensions and hooks) and is very\neasy to code in. It also takes a lot of the work out of being compatible across different platforms,\nmaking it relatively easy for Mercurial to work well across the three major OSes. On the other\nhand, Python is slow compared to many other (compiled) languages; in particular, interpreter\nstartup is relatively slow, which is particularly bad for tools that have many shorter invocations\n(such as a VCS) rather than longer running processes.\nAn early choice was made to make it hard to modify changesets after committing. Because it's\nimpossible to change a revision without modifying its identity hash, \"recalling\" changesets after\nhaving published them on the public Internet is a pain, and Mercurial makes it hard to do so.\nHowever, changing unpublished revisions should usually be fine, and the community has been\ntrying to make this easier since soon after the release. There are extensions that try to solve the\nproblem, but they require learning steps that are not very intuitive to users who have previously\nused basic Mercurial.\nRevlogs are good at reducing disk seeks, and the layered architecture of changelog, manifest and\nfilelogs has worked very well. Committing is fast and relatively little disk space is used for revisions.\nHowever, some cases like file renames aren't very efficient due to the separate storage of\nrevisions for each file; this will eventually be fixed, but it will require a somewhat hacky layering\nviolation. Similarly, the per-file DAG used to help guide filelog storage isn't used a lot in practice,\nsuch that some code used to administrate that data could be considered to be overhead.\nAnother core focus of Mercurial has been to make it easy to learn. We try to provide most of the\nrequired functionality in a small set of core commands, with options consistent across commands.\nThe intention is that Mercurial can mostly be learned progressively, especially for those users who\nhave used another VCS before; this philosophy extends to the idea that extensions can be used to\ncustomize Mercurial even more for a particular use case. For this reason, the developers also tried\nto keep the UI in line with other VCSs, Subversion in particular. Similarly, the team has tried to\nprovide good documentation, available from the application itself, with cross-references to other\nhelp topics and commands. We try hard to provide useful error messages, including hints of what\nto try instead of the operation that failed.\nSome smaller choices made can be surprising to new users. For example, handling tags (as\ndiscussed in a previous section) by putting them in a separate file inside the working directory is\nsomething many users dislike at first, but the mechanism has some very desirable properties\n(though it certainly has its shortcomings as well). Similarly, other VCSs have opted to send only\nthe checked out changeset and any ancestors to a remote host by default, whereas Mercurial\nsends every committed changeset the remote doesn't have. Both approaches make some amount\nof sense, and it depends on the style of development which one is the best for you.\n", "page": 156, "type": "text", "section": "Page 156"}
{"text": "As in any software project, there are a lot of trade-offs to be made. I think Mercurial made good\nchoices, though of course with the benefit of 20/20 hindsight some other choices might have\nbeen more appropriate. Historically, Mercurial seems to be part of a first generation of distributed\nversion control systems mature enough to be ready for general use. I, for one, am looking\nforward to seeing what the next generation will look like.\n", "page": 157, "type": "text", "section": "Page 157"}
{"text": "The Architecture of\nOpen Source Applications\nAmy Brown and Greg Wilson (eds.)\nISBN 978-1-257-63801-7 \nLicense / Buy / Contribute\nChapter 13. The NoSQL Ecosystem\nAdam Marcus\nUnlike most of the other projects in this book, NoSQL is not a tool, but an ecosystem composed\nof several complimentary and competing tools. The tools branded with the NoSQL monicker\nprovide an alternative to SQL-based relational database systems for storing data. To understand\nNoSQL, we have to understand the space of available tools, and see how the design of each one\nexplores the space of data storage possibilities.\nIf you are considering using a NoSQL storage system, you should first understand the wide space\nof options that NoSQL systems span. NoSQL systems do away with many of the traditional\ncomforts of relational database systems, and operations which were typically encapsulated behind\nthe system boundary of a database are now left to application designers. This requires you to take\non the hat of a systems architect, which requires a more in-depth understanding of how such\nsystems are built.\n13.1. What's in a Name?\nIn defining the space of NoSQL, let's first take a stab at defining the name. Taken literally, a NoSQL\nsystem presents a query interface to the user that is not SQL. The NoSQL community generally\ntakes a more inclusive view, suggesting that NoSQL systems provide alternatives to traditional\nrelational databases, and allow developers to design projects which use Not Only a SQL interface.\nIn some cases, you might replace a relational database with a NoSQL alternative, and in others you\nwill employ a mix-and-match approach to different problems you encounter in application\ndevelopment.\nBefore diving into the world of NoSQL, let's explore the cases where SQL and the relational model\nsuit your needs, and others where a NoSQL system might be a better fit.\n13.1.1. SQL and the Relational Model\nSQL is a declarative language for querying data. A declarative language is one in which a\nprogrammer specifies what they want the system to do, rather than procedurally defining how the\nsystem should do it. A few examples include: find the record for employee 39, project out only the\nemployee name and phone number from their entire record, filter employee records to those that\nwork in accounting, count the employees in each department, or join the data from the employees\ntable with the managers table.\nTo a first approximation, SQL allows you to ask these questions without thinking about how the\ndata is laid out on disk, which indices to use to access the data, or what algorithms to use to\nprocess the data. A significant architectural component of most relational databases is a query\noptimizer, which decides which of the many logically equivalent query plans to execute to most\nquickly answer a query. These optimizers are often better than the average database user, but\nsometimes they do not have enough information or have too simple a model of the system in\norder to generate the most efficient execution.\nRelational databases, which are the most common databases used in practice, follow the relational\ndata model. In this model, different real-world entities are stored in different tables. For example,\nall employees might be stored in an Employees table, and all departments might be stored in a\nDepartments table. Each row of a table has various properties stored in columns. For example,\n", "page": 158, "type": "text", "section": "Page 158"}
{"text": "employees might have an employee id, salary, birth date, and first/last names. Each of these\nproperties will be stored in a column of the Employees table.\nThe relational model goes hand-in-hand with SQL. Simple SQL queries, such as filters, retrieve all\nrecords whose field matches some test (e.g., employeeid = 3, or salary > $20000). More complex\nconstructs cause the database to do some extra work, such as joining data from multiple tables\n(e.g., what is the name of the department in which employee 3 works?). Other complex\nconstructs such as aggregates (e.g., what is the average salary of my employees?) can lead to\nfull-table scans.\nThe relational data model defines highly structured entities with strict relationships between them.\nQuerying this model with SQL allows complex data traversals without too much custom\ndevelopment. The complexity of such modeling and querying has its limits, though:\nComplexity leads to unpredictability. SQL's expressiveness makes it challenging to reason\nabout the cost of each query, and thus the cost of a workload. While simpler query\nlanguages might complicate application logic, they make it easier to provision data storage\nsystems, which only respond to simple requests.\nThere are many ways to model a problem. The relational data model is strict: the schema\nassigned to each table specifies the data in each row. If we are storing less structured data,\nor rows with more variance in the columns they store, the relational model may be needlessly\nrestrictive. Similarly, application developers might not find the relational model perfect for\nmodeling every kind of data. For example, a lot of application logic is written in object-oriented\nlanguages and includes high-level concepts such as lists, queues, and sets, and some\nprogrammers would like their persistence layer to model this.\nIf the data grows past the capacity of one server, then the tables in the database will have to\nbe partitioned across computers. To avoid JOINs having to cross the network in order to get\ndata in different tables, we will have to denormalize it. Denormalization stores all of the data\nfrom different tables that one might want to look up at once in a single place. This makes our\ndatabase look like a key-lookup storage system, leaving us wondering what other data\nmodels might better suit the data.\nIt's generally not wise to discard many years of design considerations arbitrarily. When you\nconsider storing your data in a database, consider SQL and the relational model, which are backed\nby decades of research and development, offer rich modeling capabilities, and provide easy-to-\nunderstand guarantees about complex operations. NoSQL is a good option when you have a\nspecific problem, such as large amounts of data, a massive workload, or a difficult data modeling\ndecision for which SQL and relational databases might not have been optimized.\n13.1.2. NoSQL Inspirations\nThe NoSQL movement finds much of its inspiration in papers from the research community. While\nmany papers are at the core of design decisions in NoSQL systems, two stand out in particular.\nGoogle's BigTable [CDG+06] presents an interesting data model, which facilitates sorted storage\nof multi-column historical data. Data is distributed to multiple servers using a hierarchical range-\nbased partitioning scheme, and data is updated with strict consistency (a concept that we will\neventually define in Section 13.5).\nAmazon's Dynamo [DHJ+07] uses a different key-oriented distributed datastore. Dynamo's data\nmodel is simpler, mapping keys to application-specific blobs of data. The partitioning model is more\nresilient to failure, but accomplishes that goal through a looser data consistency approach called\neventual consistency.\nWe will dig into each of these concepts in more detail, but it is important to understand that many\nof them can be mixed and matched. Some NoSQL systems such as HBase1 sticks closely to the\nBigTable design. Another NoSQL system named Voldemort2 replicates many of Dynamo's\nfeatures. Still other NoSQL projects such as Cassandra3 have taken some features from BigTable\n(its data model) and others from Dynamo (its partitioning and consistency schemes).\n", "page": 159, "type": "text", "section": "Page 159"}
{"text": "13.1.3. Characteristics and Considerations\nNoSQL systems part ways with the hefty SQL standard and offer simpler but piecemeal solutions\nfor architecting storage solutions. These systems were built with the belief that in simplifying how\na database operates over data, an architect can better predict the performance of a query. In\nmany NoSQL systems, complex query logic is left to the application, resulting in a data store with\nmore predictable query performance because of the lack of variability in queries\nNoSQL systems part with more than just declarative queries over the relational data. Transactional\nsemantics, consistency, and durability are guarantees that organizations such as banks demand\nof databases. Transactions provide an all-or-nothing guarantee when combining several potentially\ncomplex operations into one, such as deducting money from one account and adding the money\nto another. Consistency ensures that when a value is updated, subsequent queries will see the\nupdated value. Durability guarantees that once a value is updated, it will be written to stable\nstorage (such as a hard drive) and recoverable if the database crashes.\nNoSQL systems relax some of these guarantees, a decision which, for many non-banking\napplications, can provide acceptable and predictable behavior in exchange for improved\nperformance. These relaxations, combined with data model and query language changes, often\nmake it easier to safely partition a database across multiple machines when the data grows\nbeyond a single machine's capability.\nNoSQL systems are still very much in their infancy. The architectural decisions that go into the\nsystems described in this chapter are a testament to the requirements of various users. The\nbiggest challenge in summarizing the architectural features of several open source projects is that\neach one is a moving target. Keep in mind that the details of individual systems will change. When\nyou pick between NoSQL systems, you can use this chapter to guide your thought process, but\nnot your feature-by-feature product selection.\nAs you think about NoSQL systems, here is a roadmap of considerations:\nData and query model: Is your data represented as rows, objects, data structures, or\ndocuments? Can you ask the database to calculate aggregates over multiple records?\nDurability: When you change a value, does it immediately go to stable storage? Does it get\nstored on multiple machines in case one crashes?\nScalability: Does your data fit on a single server? Do the amount of reads and writes require\nmultiple disks to handle the workload?\nPartitioning: For scalability, availability, or durability reasons, does the data need to live on\nmultiple servers? How do you know which record is on which server?\nConsistency: If you've partitioned and replicated your records across multiple servers, how\ndo the servers coordinate when a record changes?\nTransactional semantics: When you run a series of operations, some databases allow you to\nwrap them in a transaction, which provides some subset of ACID (Atomicity, Consistency,\nIsolation, and Durability) guarantees on the transaction and all others currently running. Does\nyour business logic require these guarantees, which often come with performance\ntradeoffs?\nSingle-server performance: If you want to safely store data on disk, what on-disk data\nstructures are best-geared toward read-heavy or write-heavy workloads? Is writing to disk\nyour bottleneck?\nAnalytical workloads: We're going to pay a lot of attention to lookup-heavy workloads of the\nkind you need to run a responsive user-focused web application. In many cases, you will\nwant to build dataset-sized reports, aggregating statistics across multiple users for example.\nDoes your use-case and toolchain require such functionality?\nWhile we will touch on all of these consideration, the last three, while equally important, see the\nleast attention in this chapter.\n13.2. NoSQL Data and Query Models\nThe data model of a database specifies how data is logically organized. Its query model dictates\n", "page": 160, "type": "text", "section": "Page 160"}
{"text": " of a database specifies how data is logically organized. \nquery model\nhow the data can be retrieved and updated. Common data models are the relational model, key-\noriented storage model, or various graph models. Query languages you might have heard of\ninclude SQL, key lookups, and MapReduce. NoSQL systems combine different data and query\nmodels, resulting in different architectural considerations.\n13.2.1. Key-based NoSQL Data Models\nNoSQL systems often part with the relational model and the full expressivity of SQL by restricting\nlookups on a dataset to a single field. For example, even if an employee has many properties, you\nmight only be able to retrieve an employee by her ID. As a result, most queries in NoSQL systems\nare key lookup-based. The programmer selects a key to identify each data item, and can, for the\nmost part, only retrieve items by performing a lookup for their key in the database.\nIn key lookup-based systems, complex join operations or multiple-key retrieval of the same data\nmight require creative uses of key names. A programmer wishing to look up an employee by his\nemployee ID and to look up all employees in a department might create two key types. For\nexample, the key employee:30 would point to an employee record for employee ID 30, and\nemployee_departments:20 might contain a list of all employees in department 20. A join\noperation gets pushed into application logic: to retrieve employees in department 20, an\napplication first retrieves a list of employee IDs from key employee_departments:20, and then\nloops over key lookups for each employee:ID in the employee list.\nThe key lookup model is beneficial because it means that the database has a consistent query\npattern\u2014the entire workload consists of key lookups whose performance is relatively uniform and\npredictable. Profiling to find the slow parts of an application is simpler, since all complex operations\nreside in the application code. On the flip side, the data model logic and business logic are now\nmore closely intertwined, which muddles abstraction.\nLet's quickly touch on the data associated with each key. Various NoSQL systems offer different\nsolutions in this space.\nKey-Value Stores\nThe simplest form of NoSQL store is a key-value store. Each key is mapped to a value containing\narbitrary data. The NoSQL store has no knowledge of the contents of its payload, and simply\ndelivers the data to the application. In our Employee database example, one might map the key\nemployee:30 to a blob containing JSON or a binary format such as Protocol Buffers4, Thrift5, or\nAvro6 in order to encapsulate the information about employee 30.\nIf a developer uses structured formats to store complex data for a key, she must operate against\nthe data in application space: a key-value data store generally offers no mechanisms for querying\nfor keys based on some property of their values. Key-value stores shine in the simplicity of their\nquery model, usually consisting of set, get, and delete primitives, but discard the ability to add\nsimple in-database filtering capabilities due to the opacity of their values. Voldemort, which is\nbased on Amazon's Dynamo, provides a distributed key-value store. BDB7 offers a persistence\nlibrary that has a key-value interface.\nKey-Data Structure Stores\nKey-data structure stores, made popular by Redis8, assign each value a type. In Redis, the\navailable types a value can take on are integer, string, list, set, and sorted set. In addition to\nset/get/delete, type-specific commands, such as increment/decrement for integers, or\npush/pop for lists, add functionality to the query model without drastically affecting performance\ncharacteristics of requests. By providing simple type-specific functionality while avoiding multi-key\noperations such as aggregation or joins, Redis balances functionality and performance.\nKey-Document Stores\n", "page": 161, "type": "text", "section": "Page 161"}
{"text": "Key-document stores, such as CouchDB9, MongoDB10, and Riak11, map a key to some document\nthat contains structured information. These systems store documents in a JSON or JSON-like\nformat. They store lists and dictionaries, which can be embedded recursively inside one-another.\nMongoDB separates the keyspace into collections, so that keys for Employees and Department,\nfor example, do not collide. CouchDB and Riak leave type-tracking to the developer. The freedom\nand complexity of document stores is a double-edged sword: application developers have a lot of\nfreedom in modeling their documents, but application-based query logic can become exceedingly\ncomplex.\nBigTable Column Family Stores\nHBase and Cassandra base their data model on the one used by Google's BigTable. In this model,\na key identifies a row, which contains data stored in one or more Column Families (CFs). Within a\nCF, each row can contain multiple columns. The values within each column are timestamped, so\nthat several versions of a row-column mapping can live within a CF.\nConceptually, one can think of Column Families as storing complex keys of the form (row ID, CF,\ncolumn, timestamp), mapping to values which are sorted by their keys. This design results in data\nmodeling decisions which push a lot of functionality into the keyspace. It is particularly good at\nmodeling historical data with timestamps. The model naturally supports sparse column placement\nsince row IDs that do not have certain columns do not need an explicit NULL value for those\ncolumns. On the flip side, columns which have few or no NULL values must still store the column\nidentifier with each row, which leads to greater space consumption.\nEach project data model differs from the original BigTable model in various ways, but Cassandra's\nchanges are most notable. Cassandra introduces the notion of a supercolumn within each CF to\nallow for another level of mapping, modeling, and indexing. It also does away with a notion of\nlocality groups, which can physically store multiple column families together for performance\nreasons.\n13.2.2. Graph Storage\nOne class of NoSQL stores are graph stores. Not all data is created equal, and the relational and\nkey-oriented data models of storing and querying data are not the best for all data. Graphs are a\nfundamental data structure in computer science, and systems such as HyperGraphDB12 and\nNeo4J13 are two popular NoSQL storage systems for storing graph-structured data. Graph stores\ndiffer from the other stores we have discussed thus far in almost every way: data models, data\ntraversal and querying patterns, physical layout of data on disk, distribution to multiple machines,\nand the transactional semantics of queries. We can not do these stark differences justice given\nspace limitations, but you should be aware that certain classes of data may be better stored and\nqueried as a graph.\n13.2.3. Complex Queries\nThere are notable exceptions to key-only lookups in NoSQL systems. MongoDB allows you to\nindex your data based on any number of properties and has a relatively high-level language for\nspecifying which data you want to retrieve. BigTable-based systems support scanners to iterate\nover a column family and select particular items by a filter on a column. CouchDB allows you to\ncreate different views of the data, and to run MapReduce tasks across your table to facilitate more\ncomplex lookups and updates. Most of the systems have bindings to Hadoop or another\nMapReduce framework to perform dataset-scale analytical queries.\n13.2.4. Transactions\nNoSQL systems generally prioritize performance over transactional semantics. Other SQL-based\nsystems allow any set of statements\u2014from a simple primary key row retrieval, to a complicated\njoin between several tables which is then subsequently averaged across several fields\u2014to be\nplaced in a transaction.\n", "page": 162, "type": "text", "section": "Page 162"}
{"text": "These SQL databases will offer ACID guarantees between transactions. Running multiple\noperations in a transaction is Atomic (the A in ACID), meaning all or none of the operations\nhappen. Consistency (the C) ensures that the transaction leaves the database in a consistent,\nuncorrupted state. Isolation (the I) makes sure that if two transactions touch the same record,\nthey will do without stepping on each other's feet. Durability (the D, covered extensively in the\nnext section), ensures that once a transaction is committed, it's stored in a safe place.\nACID-compliant transactions keep developers sane by making it easy to reason about the state of\ntheir data. Imagine multiple transactions, each of which has multiple steps (e.g., first check the\nvalue of a bank account, then subtract $60, then update the value). ACID-compliant databases\noften are limited in how they can interleave these steps while still providing a correct result across\nall transactions. This push for correctness results in often-unexpected performance\ncharacteristics, where a slow transaction might cause an otherwise quick one to wait in line.\nMost NoSQL systems pick performance over full ACID guarantees, but do provide guarantees at\nthe key level: two operations on the same key will be serialized, avoiding serious corruption to key-\nvalue pairs. For many applications, this decision will not pose noticeable correctness issues, and\nwill allow quick operations to execute with more regularity. It does, however, leave more\nconsiderations for application design and correctness in the hands of the developer.\nRedis is the notable exception to the no-transaction trend. On a single server, it provides a MULTI\ncommand to combine multiple operations atomically and consistently, and a WATCH command to\nallow isolation. Other systems provide lower-level test-and-set functionality which provides some\nisolation guarantees.\n13.2.5. Schema-free Storage\nA cross-cutting property of many NoSQL systems is the lack of schema enforcement in the\ndatabase. Even in document stores and column family-oriented stores, properties across similar\nentities are not required to be the same. This has the benefit of supporting less structured data\nrequirements and requiring less performance expense when modifying schemas on-the-fly. The\ndecision leaves more responsibility to the application developer, who now has to program more\ndefensively. For example, is the lack of a lastname property on an employee record an error to\nbe rectified, or a schema update which is currently propagating through the system? Data and\nschema versioning is common in application-level code after a few iterations of a project which\nrelies on sloppy-schema NoSQL systems.\n13.3. Data Durability\nIdeally, all data modifications on a storage system would immediately be safely persisted and\nreplicated to multiple locations to avoid data loss. However, ensuring data safety is in tension with\nperformance, and different NoSQL systems make different data durability guarantees in order to\nimprove performance. Failure scenarios are varied and numerous, and not all NoSQL systems\nprotect you against these issues.\nA simple and common failure scenario is a server restart or power loss. Data durability in this case\ninvolves having moved the data from memory to a hard disk, which does not require power to\nstore data. Hard disk failure is handled by copying the data to secondary devices, be they other\nhard drives in the same machine (RAID mirroring) or other machines on the network. However, a\ndata center might not survive an event which causes correlated failure (a tornado, for example),\nand some organizations go so far as to copy data to backups in data centers several hurricane\nwidths apart. Writing to hard drives and copying data to multiple servers or data centers is\nexpensive, so different NoSQL systems trade off durability guarantees for performance.\n13.3.1. Single-server Durability\nThe simplest form of durability is a single-server durability, which ensures that any data\nmodification will survive a server restart or power loss. This usually means writing the changed\n", "page": 163, "type": "text", "section": "Page 163"}
{"text": "data to disk, which often bottlenecks your workload. Even if you order your operating system to\nwrite data to an on-disk file, the operating system may buffer the write, avoiding an immediate\nmodification on disk so that it can group several writes together into a single operation. Only when\nthe fsync system call is issued does the operating system make a best-effort attempt to ensure\nthat buffered updates are persisted to disk.\nTypical hard drives can perform 100-200 random accesses (seeks) per second, and are limited to\n30-100 MB/sec of sequential writes. Memory can be orders of magnitudes faster in both\nscenarios. Ensuring efficient single-server durability means limiting the number of random writes\nyour system incurs, and increasing the number of sequential writes per hard drive. Ideally, you\nwant a system to minimize the number of writes between fsync calls, maximizing the number of\nthose writes that are sequential, all the while never telling the user their data has been successfully\nwritten to disk until that write has been fsynced. Let's cover a few techniques for improving\nperformance of single-server durability guarantees.\nControl fsync Frequency\nMemcached14 is an example of a system which offers no on-disk durability in exchange for\nextremely fast in-memory operations. When a server restarts, the data on that server is gone:\nthis makes for a good cache and a poor durable data store.\nRedis offers developers several options for when to call fsync. Developers can force an fsync\ncall after every update, which is the slow and safe choice. For better performance, Redis can\nfsync its writes every N seconds. In a worst-case scenario, the you will lose last N seconds worth\nof operations, which may be acceptable for certain uses. Finally, for use cases where durability is\nnot important (maintaining coarse-grained statistics, or using Redis as a cache), the developer can\nturn off fsync calls entirely: the operating system will eventually flush the data to disk, but without\nguarantees of when this will happen.\nIncrease Sequential Writes by Logging\nSeveral data structures, such as B+Trees, help NoSQL systems quickly retrieve data from disk.\nUpdates to those structures result in updates in random locations in the data structures' files,\nresulting in several random writes per update if you fsync after each update. To reduce random\nwrites, systems such as Cassandra, HBase, Redis, and Riak append update operations to a\nsequentially-written file called a log. While other data structures used by the system are only\nperiodically fsynced, the log is frequently fsynced. By treating the log as the ground-truth state\nof the database after a crash, these storage engines are able to turn random updates into\nsequential ones.\nWhile NoSQL systems such as MongoDB perform writes in-place in their data structures, others\ntake logging even further. Cassandra and HBase use a technique borrowed from BigTable of\ncombining their logs and lookup data structures into one log-structured merge tree. Riak provides\nsimilar functionality with a log-structured hash table. CouchDB has modified the traditional B+Tree\nso that all changes to the data structure are appended to the structure on physical storage. These\ntechniques result in improved write throughput, but require a periodic log compaction to keep the\nlog from growing unbounded.\nIncrease Throughput by Grouping Writes\nCassandra groups multiple concurrent updates within a short window into a single fsync call. This\ndesign, called group commit, results in higher latency per update, as users have to wait on several\nconcurrent updates to have their own update be acknowledged. The latency bump comes at an\nincrease in throughput, as multiple log appends can happen with a single fsync. As of this writing,\nevery HBase update is persisted to the underlying storage provided by the Hadoop Distributed File\nSystem (HDFS)15, which has recently seen patches to allow support of appends that respect\nfsync and group commit.\n13.3.2. Multi-server Durability\n", "page": 164, "type": "text", "section": "Page 164"}
{"text": "13.3.2. Multi-server Durability\nBecause hard drives and machines often irreparably fail, copying important data across machines\nis necessary. Many NoSQL systems offer multi-server durability for data.\nRedis takes a traditional master-slave approach to replicating data. All operations executed against\na master are communicated in a log-like fashion to slave machines, which replicate the operations\non their own hardware. If a master fails, a slave can step in and serve the data from the state of\nthe operation log that it received from the master. This configuration might result in some data\nloss, as the master does not confirm that the slave has persisted an operation in its log before\nacknowledging the operation to the user. CouchDB facilitates a similar form of directional\nreplication, where servers can be configured to replicate changes to documents on other stores.\nMongoDB provides the notion of replica sets, where some number of servers are responsible for\nstoring each document. MongoDB gives developers the option of ensuring that all replicas have\nreceived updates, or to proceed without ensuring that replicas have the most recent data. Many\nof the other distributed NoSQL storage systems support multi-server replication of data. HBase,\nwhich is built on top of HDFS, receives multi-server durability through HDFS. All writes are\nreplicated to two or more HDFS nodes before returning control to the user, ensuring multi-server\ndurability.\nRiak, Cassandra, and Voldemort support more configurable forms of replication. With subtle\ndifferences, all three systems allow the user to specify N, the number of machines which should\nultimately have a copy of the data, and W<N, the number of machines that should confirm the data\nhas been written before returning control to the user.\nTo handle cases where an entire data center goes out of service, multi-server replication across\ndata centers is required. Cassandra, HBase, and Voldemort have rack-aware configurations,\nwhich specify the rack or data center in which various machines are located. In general, blocking\nthe user's request until a remote server has acknowledged an update incurs too much latency.\nUpdates are streamed without confirmation when performed across wide area networks to\nbackup data centers.\n13.4. Scaling for Performance\nHaving just spoken about handling failure, let's imagine a rosier situation: success! If the system\nyou build reaches success, your data store will be one of the components to feel stress under\nload. A cheap and dirty solution to such problems is to scale up your existing machinery: invest in\nmore RAM and disks to handle the workload on one machine. With more success, pouring money\ninto more expensive hardware will become infeasible. At this point, you will have to replicate data\nand spread requests across multiple machines to distribute load. This approach is called scale out,\nand is measured by the horizontal scalability of your system.\nThe ideal horizontal scalability goal is linear scalability, in which doubling the number of machines in\nyour storage system doubles the query capacity of the system. The key to such scalability is in\nhow the data is spread across machines. Sharding is the act of splitting your read and write\nworkload across multiple machines to scale out your storage system. Sharding is fundamental to\nthe design of many systems, namely Cassandra, HBase, Voldemort, and Riak, and more recently\nMongoDB and Redis. Some projects such as CouchDB focus on single-server performance and do\nnot provide an in-system solution to sharding, but secondary projects provide coordinators to\npartition the workload across independent installations on multiple machines.\nLet's cover a few interchangeable terms you might encounter. We will use the terms sharding and\npartitioning interchangeably. The terms machine, server, or node refer to some physical computer\nwhich stores part of the partitioned data. Finally, a cluster or ring refers to the set of machines\nwhich participate in your storage system.\nSharding means that no one machine has to handle the write workload on the entire dataset, but\nno one machine can answer queries about the entire dataset. Most NoSQL systems are key-\noriented in both their data and query models, and few queries touch the entire dataset anyway.\nBecause the primary access method for data in these systems is key-based, sharding is typically\n", "page": 165, "type": "text", "section": "Page 165"}
{"text": "key-based as well: some function of the key determines the machine on which a key-value pair is\nstored. We'll cover two methods of defining the key-machine mapping: hash partitioning and\nrange partitioning.\n13.4.1. Do Not Shard Until You Have To\nSharding adds system complexity, and where possible, you should avoid it. Let's cover two ways\nto scale without sharding: read replicas and caching.\nRead Replicas\nMany storage systems see more read requests than write requests. A simple solution in these\ncases is to make copies of the data on multiple machines. All write requests still go to a master\nnode. Read requests go to machines which replicate the data, and are often slightly stale with\nrespect to the data on the write master.\nIf you are already replicating your data for multi-server durability in a master-slave configuration,\nas is common in Redis, CouchDB, or MongoDB, the read slaves can shed some load from the\nwrite master. Some queries, such as aggregate summaries of your dataset, which might be\nexpensive and often do not require up-to-the-second freshness, can be executed against the\nslave replicas. Generally, the less stringent your demands for freshness of content, the more you\ncan lean on read slaves to improve read-only query performance.\nCaching\nCaching the most popular content in your system often works surprisingly well. Memcached\ndedicates blocks of memory on multiple servers to cache data from your data store. Memcached\nclients take advantage of several horizontal scalability tricks to distribute load across Memcached\ninstallations on different servers. To add memory to the cache pool, just add another Memcached\nhost.\nBecause Memcached is designed for caching, it does not have as much architectural complexity\nas the persistent solutions for scaling workloads. Before considering more complicated solutions,\nthink about whether caching can solve your scalability woes. Caching is not solely a temporary\nband-aid: Facebook has Memcached installations in the range of tens of terabytes of memory!\nRead replicas and caching allow you to scale up your read-heavy workloads. When you start to\nincrease the frequency of writes and updates to your data, however, you will also increase the\nload on the master server that contains all of your up-to-date data. For the rest of this section, we\nwill cover techniques for sharding your write workload across multiple servers.\n13.4.2. Sharding Through Coordinators\nThe CouchDB project focuses on the single-server experience. Two projects, Lounge and\nBigCouch, facilitate sharding CouchDB workloads through an external proxy, which acts as a front\nend to standalone CouchDB instances. In this design, the standalone installations are not aware of\neach other. The coordinator distributes requests to individual CouchDB instances based on the\nkey of the document being requested.\nTwitter has built the notions of sharding and replication into a coordinating framework called\nGizzard16. Gizzard takes standalone data stores of any type\u2014you can build wrappers for SQL or\nNoSQL storage systems\u2014and arranges them in trees of any depth to partition keys by key range.\nFor fault tolerance, Gizzard can be configured to replicate data to multiple physical machines for\nthe same key range.\n13.4.3. Consistent Hash Rings\nGood hash functions distribute a set of keys in a uniform manner. This makes them a powerful\ntool for distributing key-value pairs among multiple servers. The academic literature on a technique\n", "page": 166, "type": "text", "section": "Page 166"}
{"text": "called consistent hashing is extensive, and the first applications of the technique to data stores\nwas in systems called distributed hash tables (DHTs). NoSQL systems built around the principles of\nAmazon's Dynamo adopted this distribution technique, and it appears in Cassandra, Voldemort,\nand Riak.\nHash Rings by Example\nFigure 13.1: A Distributed Hash Table Ring\nConsistent hash rings work as follows. Say we have a hash function H that maps keys to uniformly\ndistributed large integer values. We can form a ring of numbers in the range [1, L] that wraps\naround itself with these values by taking H(key) mod L for some relatively large integer L. This will\nmap each key into the range [1,L]. A consistent hash ring of servers is formed by taking each\nserver's unique identifier (say its IP address), and applying H to it. You can get an intuition for how\nthis works by looking at the hash ring formed by five servers (A-E) in Figure 13.1.\nThere, we picked L = 1000. Let's say that H(A) mod L = 7, H(B) mod L = 234, H(C) mod L\n= 447, H(D) mod L = 660, and H(E) mod L = 875. We can now tell which server a key should\nlive on. To do this, we map all keys to a server by seeing if it falls in the range between that server\nand the next one in the ring. For example, A is responsible for keys whose hash value falls in the\nrange [7,233], and E is responsible for keys in the range [875, 6] (this range wraps around on\nitself at 1000). So if H('employee30') mod L = 899, it will be stored by server E, and if\nH('employee31') mod L = 234, it will be stored on server B.\nReplicating Data\nReplication for multi-server durability is achieved by passing the keys and values in one server's\nassigned range to the servers following it in the ring. For example, with a replication factor of 3,\nkeys mapped to the range [7,233] will be stored on servers A, B, and C. If A were to fail, its\nneighbors B and C would take over its workload. In some designs, E would replicate and take over\nA's workload temporarily, since its range would expand to include A's.\nAchieving Better Distribution\n", "page": 167, "type": "text", "section": "Page 167"}
{"text": "While hashing is statistically effective at uniformly distributing a keyspace, it usually requires many\nservers before it distributes evenly. Unfortunately, we often start with a small number of servers\nthat are not perfectly spaced apart from one-another by the hash function. In our example, A's\nkey range is of length 227, whereas E's range is 132. This leads to uneven load on different\nservers. It also makes it difficult for servers to take over for one-another when they fail, since a\nneighbor suddenly has to take control of the entire range of the failed server.\nTo solve the problem of uneven large key ranges, many DHTs including Riak create several `virtual'\nnodes per physical machine. For example, with 4 virtual nodes, server A will act as server A_1,\nA_2, A_3, and A_4. Each virtual node hashes to a different value, giving it more opportunity to\nmanage keys distributed to different parts of the keyspace. Voldemort takes a similar approach, in\nwhich the number of partitions is manually configured and usually larger than the number of\nservers, resulting in each server receiving a number of smaller partitions.\nCassandra does not assign multiple small partitions to each server, resulting in sometimes uneven\nkey range distributions. For load-balancing, Cassandra has an asynchronous process which\nadjusts the location of servers on the ring depending on their historic load.\n13.4.4. Range Partitioning\nIn the range partitioning approach to sharding, some machines in your system keep metadata\nabout which servers contain which key ranges. This metadata is consulted to route key and range\nlookups to the appropriate servers. Like the consistent hash ring approach, this range partitioning\nsplits the keyspace into ranges, with each key range being managed by one machine and\npotentially replicated to others. Unlike the consistent hashing approach, two keys that are next to\neach other in the key's sort order are likely to appear in the same partition. This reduces the size\nof the routing metadata, as large ranges are compressed to [start, end] markers.\nIn adding active record-keeping of the range-to-server mapping, the range partitioning approach\nallows for more fine-grained control of load-shedding from heavily loaded servers. If a specific key\nrange sees higher traffic than other ranges, a load manager can reduce the size of the range on\nthat server, or reduce the number of shards that this server serves. The added freedom to\nactively manage load comes at the expense of extra architectural components which monitor and\nroute shards.\nThe BigTable Way\nGoogle's BigTable paper describes a range-partitioning hierarchical technique for sharding data\ninto tablets. A tablet stores a range of row keys and values within a column family. It maintains all\nof the necessary logs and data structures to answer queries about the keys in its assigned range.\nTablet servers serve multiple tablets depending on the load each tablet is experiencing.\nEach tablet is kept at a size of 100-200 MB. As tablets change in size, two small tablets with\nadjoining key ranges might be combined, or a large tablet might be split in two. A master server\nanalyzes tablet size, load, and tablet server availability. The master adjusts which tablet server\nserves which tablets at any time.\n", "page": 168, "type": "text", "section": "Page 168"}
{"text": "Figure 13.2: BigTable-based Range Partitioning\nThe master server maintains the tablet assignment in a metadata table. Because this metadata can\nget large, the metadata table is also sharded into tablets that map key ranges to tablets and tablet\nservers responsible for those ranges. This results in a three-layer hierarchy traversal for clients to\nfind a key on its hosting tablet server, as depicted in Figure 13.2.\nLet's look at an example. A client searching for key 900 will query server A, which stores the tablet\nfor metadata level 0. This tablet identifies the metadata level 1 tablet on server 6 containing key\nranges 500-1500. The client sends a request to server B with this key, which responds that the\ntablet containing keys 850-950 is found on a tablet on server C. Finally, the client sends the key\nrequest to server C, and gets the row data back for its query. Metadata tablets at level 0 and 1\nmay be cached by the client, which avoids putting undue load on their tablet servers from repeat\nqueries. The BigTable paper explains that this 3-level hierarchy can accommodate 261 bytes worth\nof storage using 128MB tablets.\nHandling Failures\nThe master is a single point of failure in the BigTable design, but can go down temporarily without\naffecting requests to tablet servers. If a tablet server fails while serving tablet requests, it is up to\nthe master to recognize this and re-assign its tablets while requests temporarily fail.\nIn order to recognize and handle machine failures, the BigTable paper describes the use of\nChubby, a distributed locking system for managing server membership and liveness. ZooKeeper17\nis the open source implementation of Chubby, and several Hadoop-based projects utilize it to\nmanage secondary master servers and tablet server reassignment.\nRange Partitioning-based NoSQL Projects\nHBase employs BigTable's hierarchical approach to range-partitioning. Underlying tablet data is\nstored in Hadoop's distributed filesystem (HDFS). HDFS handles data replication and consistency\namong replicas, leaving tablet servers to handle requests, update storage structures, and initiate\ntablet splits and compactions.\nMongoDB handles range partitioning in a manner similar to that of BigTable. Several configuration\nnodes store and manage the routing tables that specify which storage node is responsible for\nwhich key ranges. These configuration nodes stay in sync through a protocol called two-phase\ncommit, and serve as a hybrid of BigTable's master for specifying ranges and Chubby for highly\n", "page": 169, "type": "text", "section": "Page 169"}
{"text": "available configuration management. Separate routing processes, which are stateless, keep track\nof the most recent routing configuration and route key requests to the appropriate storage\nnodes. Storage nodes are arranged in replica sets to handle replication.\nCassandra provides an order-preserving partitioner if you wish to allow fast range scans over\nyour data. Cassandra nodes are still arranged in a ring using consistent hashing, but rather than\nhashing a key-value pair onto the ring to determine the server to which it should be assigned, the\nkey is simply mapped onto the server which controls the range in which the key naturally fits. For\nexample, keys 20 and 21 would both be mapped to server A in our consistent hash ring in\nFigure 13.1, rather than being hashed and randomly distributed in the ring.\nTwitter's Gizzard framework for managing partitioned and replicated data across many back ends\nuses range partitioning to shard data. Routing servers form hierarchies of any depth, assigning\nranges of keys to servers below them in the hierarchy. These servers either store data for keys in\ntheir assigned range, or route to yet another layer of routing servers. Replication in this model is\nachieved by sending updates to multiple machines for a key range. Gizzard routing nodes manage\nfailed writes in different manner than other NoSQL systems. Gizzard requires that system\ndesigners make all updates idempotent (they can be run twice). When a storage node fails,\nrouting nodes cache and repeatedly send updates to the node until the update is confirmed.\n13.4.5. Which Partitioning Scheme to Use\nGiven the hash- and range-based approaches to sharding, which is preferable? It depends. Range\npartitioning is the obvious choice to use when you will frequently be performing range scans over\nthe keys of your data. As you read values in order by key, you will not jump to random nodes in\nthe network, which would incur heavy network overhead. But if you do not require range scans,\nwhich sharding scheme should you use?\nHash partitioning gives reasonable distribution of data across nodes, and random skew can be\nreduced with virtual nodes. Routing is simple in the hash partitioning scheme: for the most part,\nthe hash function can be executed by clients to find the appropriate server. With more\ncomplicated rebalancing schemes, finding the right node for a key becomes more difficult.\nRange partitioning requires the upfront cost of maintaining routing and configuration nodes, which\ncan see heavy load and become central points of failure in the absence of relatively complex fault\ntolerance schemes. Done well, however, range-partitioned data can be load-balanced in small\nchunks which can be reassigned in high-load situations. If a server goes down, its assigned\nranges can be distributed to many servers, rather than loading the server's immediate neighbors\nduring downtime.\n13.5. Consistency\nHaving spoken about the virtues of replicating data to multiple machines for durability and\nspreading load, it's time to let you in on a secret: keeping replicas of your data on multiple\nmachines consistent with one-another is hard. In practice, replicas will crash and get out of sync,\nreplicas will crash and never come back, networks will partition two sets of replicas, and messages\nbetween machines will get delayed or lost. There are two major approaches to data consistency in\nthe NoSQL ecosystem. The first is strong consistency, where all replicas remain in sync. The\nsecond is eventual consistency, where replicas are allowed to get out of sync, but eventually catch\nup with one-another. Let's first get into why the second option is an appropriate consideration by\nunderstanding a fundamental property of distributed computing. After that, we'll jump into the\ndetails of each approach.\n13.5.1. A Little Bit About CAP\nWhy are we considering anything short of strong consistency guarantees over our data? It all\ncomes down to a property of distributed systems architected for modern networking equipment.\nThe idea was first proposed by Eric Brewer as the CAP Theorem, and later proved by Gilbert and\nLynch [GL02]. The theorem first presents three properties of distributed systems which make up\n", "page": 170, "type": "text", "section": "Page 170"}
{"text": "the acronym CAP:\nConsistency: do all replicas of a piece of data always logically agree on the same version of\nthat data by the time you read it? (This concept of consistency is different than the C in\nACID.)\nAvailability: Do replicas respond to read and write requests regardless of how many replicas\nare inaccessible?\nPartition tolerance: Can the system continue to operate even if some replicas temporarily lose\nthe ability to communicate with each other over the network?\nThe theorem then goes on to say that a storage system which operates on multiple computers\ncan only achieve two of these properties at the expense of a third. Also, we are forced to\nimplement partition-tolerant systems. On current networking hardware using current messaging\nprotocols, packets can be lost, switches can fail, and there is no way to know whether the\nnetwork is down or the server you are trying to send a message to is unavailable. All NoSQL\nsystems should be partition-tolerant. The remaining choice is between consistency and availability.\nNo NoSQL system can provide both at the same time.\nOpting for consistency means that your replicated data will not be out of sync across replicas. An\neasy way to achieve consistency is to require that all replicas acknowledge updates. If a replica\ngoes down and you can not confirm data updates on it, then you degrade availability on its keys.\nThis means that until all replicas recover and respond, the user can not receive successful\nacknowledgment of their update operation. Thus, opting for consistency is opting for a lack of\nround-the-clock availability for each data item.\nOpting for availability means that when a user issues an operation, replicas should act on the data\nthey have, regardless of the state of other replicas. This may lead to diverging consistency of data\nacross replicas, since they weren't required to acknowledge all updates, and some replicas may\nhave not noted all updates.\nThe implications of the CAP theorem lead to the strong consistency and eventual consistency\napproaches to building NoSQL data stores. Other approaches exist, such as the relaxed\nconsistency and relaxed availability approach presented in Yahoo!'s PNUTS [CRS+08] system.\nNone of the open source NoSQL systems we discuss has adopted this technique yet, so we will\nnot discuss it further.\n13.5.2. Strong Consistency\nSystems which promote strong consistency ensure that the replicas of a data item will always be\nable to come to consensus on the value of a key. Some replicas may be out of sync with one-\nanother, but when the user asks for the value of employee30:salary, the machines have a way\nto consistently agree on the value the user sees. How this works is best explained with numbers.\nSay we replicate a key on N machines. Some machine, perhaps one of the N, serves as a\ncoordinator for each user request. The coordinator ensures that a certain number of the N\nmachines has received and acknowledged each request. When a write or update occurs to a key,\nthe coordinator does not confirm with the user that the write occurred until W replicas confirm\nthat they have received the update. When a user wants to read the value for some key, the\ncoordinator responds when at least R have responded with the same value. We say that the\nsystem exemplifies strong consistency if R+W>N.\nPutting some numbers to this idea, let's say that we're replicating each key across N=3 machines\n(call them A, B, and C). Say that the key employee30:salary is initially set to the value $20,000,\nbut we want to give employee30 a raise to $30,000. Let's require that at least W=2 of A, B, or C\nacknowledge each write request for a key. When A and B confirm the write request for\n(employee30:salary, $30,000), the coordinator lets the user know that\nemployee30:salary is safely updated. Let's assume that machine C never received the write\nrequest for employee30:salary, so it still has the value $20,000. When a coordinator gets a\nread request for key employee30:salary, it will send that request to all 3 machines:\n", "page": 171, "type": "text", "section": "Page 171"}
{"text": "If we set R=1, and machine C responds first with $20,000, our employee will not be very\nhappy.\nHowever, if we set R=2, the coordinator will see the value from C, wait for a second response\nfrom A or B, which will conflict with C's outdated value, and finally receive a response from\nthe third machine, which will confirm that $30,000 is the majority opinion.\nSo in order to achieve strong consistency in this case, we need to set R=2} so that R+W3}.\nWhat happens when W replicas do not respond to a write request, or R replicas do not respond to\na read request with a consistent response? The coordinator can timeout eventually and send the\nuser an error, or wait until the situation corrects itself. Either way, the system is considered\nunavailable for that request for at least some time.\nYour choice of R and W affect how many machines can act strangely before your system becomes\nunavailable for different actions on a key. If you force all of your replicas to acknowledge writes,\nfor example, then W=N, and write operations will hang or fail on any replica failure. A common\nchoice is R + W = N + 1, the minimum required for strong consistency while still allowing for\ntemporary disagreement between replicas. Many strong consistency systems opt for W=N and\nR=1, since they then do not have to design for nodes going out of sync.\nHBase bases its replicated storage on HDFS, a distributed storage layer. HDFS provides strong\nconsistency guarantees. In HDFS, a write cannot succeed until it has been replicated to all N\n(usually 2 or 3) replicas, so W = N. A read will be satisfied by a single replica, so R = 1. To avoid\nbogging down write-intensive workloads, data is transferred from the user to the replicas\nasynchronously in parallel. Once all replicas acknowledge that they have received copies of the\ndata, the final step of swapping the new data in to the system is performed atomically and\nconsistently across all replicas.\n13.5.3. Eventual Consistency\nDynamo-based systems, which include Voldemort, Cassandra, and Riak, allow the user to specify\nN, R, and W to their needs, even if R + W <= N. This means that the user can achieve either\nstrong or eventual consistency. When a user picks eventual consistency, and even when the\nprogrammer opts for strong consistency but W is less than N, there are periods in which replicas\nmight not see eye-to-eye. To provide eventual consistency among replicas, these systems employ\nvarious tools to catch stale replicas up to speed. Let's first cover how various systems determine\nthat data has gotten out of sync, then discuss how they synchronize replicas, and finally bring in a\nfew dynamo-inspired methods for speeding up the synchronization process.\nVersioning and Conflicts\nBecause two replicas might see two different versions of a value for some key, data versioning\nand conflict detection is important. The dynamo-based systems use a type of versioning called\nvector clocks. A vector clock is a vector assigned to each key which contains a counter for each\nreplica. For example, if servers A, B, and C are the three replicas of some key, the vector clock will\nhave three entries, (N_A, N_B, N_C), initialized to (0,0,0).\nEach time a replica modifies a key, it increments its counter in the vector. If B modifies a key that\npreviously had version (39, 1, 5), it will change the vector clock to (39, 2, 5). When another\nreplica, say C, receives an update from B about the key's data, it will compare the vector clock\nfrom B to its own. As long as its own vector clock counters are all less than the ones delivered\nfrom B, then it has a stale version and can overwrite its own copy with B's. If B and C have clocks\nin which some counters are greater than others in both clocks, say (39, 2, 5) and (39, 1,\n6), then the servers recognize that they received different, potentially unreconcilable updates over\ntime, and identify a conflict.\nConflict Resolution\nConflict resolution varies across the different systems. The Dynamo paper leaves conflict\nresolution to the application using the storage system. Two versions of a shopping cart can be\n", "page": 172, "type": "text", "section": "Page 172"}
{"text": "resolution to the application using the storage system. Two versions of a shopping cart can be\nmerged into one without significant loss of data, but two versions of a collaboratively edited\ndocument might require human reviewer to resolve conflict. Voldemort follows this model,\nreturning multiple copies of a key to the requesting client application upon conflict.\nCassandra, which stores a timestamp on each key, uses the most recently timestamped version\nof a key when two versions are in conflict. This removes the need for a round-trip to the client and\nsimplifies the API. This design makes it difficult to handle situations where conflicted data can be\nintelligently merged, as in our shopping cart example, or when implementing distributed counters.\nRiak allows both of the approaches offered by Voldemort and Cassandra. CouchDB provides a\nhybrid: it identifies a conflict and allows users to query for conflicted keys for manual repair, but\ndeterministically picks a version to return to users until conflicts are repaired.\nRead Repair\nIf R replicas return non-conflicting data to a coordinator, the coordinator can safely return the\nnon-conflicting data to the application. The coordinator may still notice that some of the replicas\nare out of sync. The Dynamo paper suggests, and Cassandra, Riak, and Voldemort implement, a\ntechnique called read repair for handling such situations. When a coordinator identifies a conflict\non read, even if a consistent value has been returned to the user, the coordinator starts conflict-\nresolution protocols between conflicted replicas. This proactively fixes conflicts with little additional\nwork. Replicas have already sent their version of the data to the coordinator, and faster conflict\nresolution will result in less divergence in the system.\nHinted Handoff\nCassandra, Riak, and Voldemort all employ a technique called hinted handoff to improve write\nperformance for situations where a node temporarily becomes unavailable. If one of the replicas\nfor a key does not respond to a write request, another node is selected to temporarily take over\nits write workload. Writes for the unavailable node are kept separately, and when the backup node\nnotices the previously unavailable node become available, it forwards all of the writes to the newly\navailable replica. The Dynamo paper utilizes a 'sloppy quorum' approach and allows the writes\naccomplished through hinted handoff to count toward the W required write acknowledgments.\nCassandra and Voldemort will not count a hinted handoff against W, and will fail a write which\ndoes not have W confirmations from the originally assigned replicas. Hinted handoff is still useful\nin these systems, as it speeds up recovery when an unavailable node returns.\nAnti-Entropy\nWhen a replica is down for an extended period of time, or the machine storing hinted handoffs for\nan unavailable replica goes down as well, replicas must synchronize from one-another. In this\ncase, Cassandra and Riak implement a Dynamo-inspired process called anti-entropy. In anti-\nentropy, replicas exchange Merkle Trees to identify parts of their replicated key ranges which are\nout of sync. A Merkle tree is a hierarchical hash verification: if the hash over the entire keyspace is\nnot the same between two replicas, they will exchange hashes of smaller and smaller portions of\nthe replicated keyspace until the out-of-sync keys are identified. This approach reduces\nunnecessary data transfer between replicas which contain mostly similar data.\nGossip\nFinally, as distributed systems grow, it is hard to keep track of how each node in the system is\ndoing. The three Dynamo-based systems employ an age-old high school technique known as\ngossip to keep track of other nodes. Periodically (every second or so), a node will pick a random\nnode it once communicated with to exchange knowledge of the health of the other nodes in the\nsystem. In providing this exchange, nodes learn which other nodes are down, and know where to\nroute clients in search of a key.\n13.6. A Final Word\n", "page": 173, "type": "text", "section": "Page 173"}
{"text": "The NoSQL ecosystem is still in its infancy, and many of the systems we've discussed will change\narchitectures, designs, and interfaces. The important takeaways in this chapter are not what each\nNoSQL system currently does, but rather the design decisions that led to a combination of\nfeatures that make up these systems. NoSQL leaves a lot of design work in the hands of the\napplication designer. Understanding the architectural components of these systems will not only\nhelp you build the next great NoSQL amalgamation, but also allow you to use current versions\nresponsibly.\n13.7. Acknowledgments\nI am grateful to Jackie Carter, Mihir Kedia, and the anonymous reviewers for their comments and\nsuggestions to improve the chapter. This chapter would also not be possible without the years of\ndedicated work of the NoSQL community. Keep building!\nFootnotes\n1. http://hbase.apache.org/\n2. http://project-voldemort.com/\n3. http://cassandra.apache.org/\n4. http://code.google.com/p/protobuf/\n5. http://thrift.apache.org/\n6. http://avro.apache.org/\n7. http://www.oracle.com/technetwork/database/berkeleydb/overview/index.html\n8. http://redis.io/\n9. http://couchdb.apache.org/\n10. http://www.mongodb.org/\n11. http://www.basho.com/products_riak_overview.php\n12. http://www.hypergraphdb.org/index\n13. http://neo4j.org/\n14. http://memcached.org/\n15. http://hadoop.apache.org/hdfs/\n16. http://github.com/twitter/gizzard\n17. http://hadoop.apache.org/zookeeper/\n", "page": 174, "type": "text", "section": "Page 174"}
{"text": "The Architecture of\nOpen Source Applications\nAmy Brown and Greg Wilson (eds.)\nISBN 978-1-257-63801-7 \nLicense / Buy / Contribute\nChapter 14. Python Packaging\nTarek Ziad\u00e9\n14.1. Introduction\nThere are two schools of thought when it comes to installing applications. The first, common to\nWindows and Mac OS X, is that applications should be self-contained, and their installation should\nnot depend on anything else. This philosophy simplifies the management of applications: each\napplication is its own standalone \"appliance\", and installing and removing them should not disturb\nthe rest of the OS. If the application needs an uncommon library, that library is included in the\napplication's distribution.\nThe second school, which is the norm for Linux-based systems, treats software as a collection of\nsmall self-contained units called packages. Libraries are bundled into packages, any given library\npackage might depend on other packages. Installing an application might involve finding and\ninstalling particular versions of dozens of other libraries. These dependencies are usually fetched\nfrom a central repository that contains thousands of packages. This philosophy is why Linux\ndistributions use complex package management systems like dpkg and RPM to track\ndependencies and prevent installation of two applications that use incompatible versions of the\nsame library.\nThere are pros and cons to each approach. Having a highly modular system where every piece\ncan be updated or replaced makes management easier, because each library is present in a single\nplace, and all applications that use it benefit when it is updated. For instance, a security fix in a\nparticular library will reach all applications that use it at once, whereas if an application ships with\nits own library, that security fix will be more complex to deploy, especially if different applications\nuse different versions of the library.\nBut that modularity is seen as a drawback by some developers, because they're not in control of\ntheir applications and dependencies. It is easier for them to provide a standalone software\nappliance to be sure that the application environment is stable and not subject to \"dependency\nhell\" during system upgrades.\nSelf-contained applications also make the developer's life easier when she needs to support\nseveral operating systems. Some projects go so far as to release portable applications that\nremove any interaction with the hosting system by working in a self-contained directory, even for\nlog files.\nPython's packaging system was intended to make the second philosophy\u2014multiple dependencies\nfor each install\u2014as developer-, admin-, packager-, and user-friendly as possible. Unfortunately it\nhad (and has) a variety of flaws which caused or allowed all kinds of problems: unintuitive version\nschemes, mishandled data files, difficulty re-packaging, and more. Three years ago I and a group\nof other Pythoneers decided to reinvent it to address these problems. We call ourselves the\nFellowship of the Packaging, and this chapter describes the problems we have been trying to fix,\nand what our solution looks like.\nTerminology\nIn Python a package is a directory containing Python files. Python files are called modules.\nThat definition makes the usage of the word \"package\" a bit vague since it is also used by\nmany systems to refer to a release of a project.\nPython developers themselves are sometimes vague about this. One way to remove this\nambiguity is to use the term \"Python packages\" when we talk about a directory containing\nPython modules. The term \"release\" is used to define one version of a project, and the\nterm \"distribution\" defines a source or a binary distribution of a release as something like a\ntarball or zip file.\n", "page": 175, "type": "text", "section": "Page 175"}
{"text": "14.2. The Burden of the Python Developer\nMost Python programmers want their programs to be usable in any environment. They also\nusually want to use a mix of standard Python libraries and system-dependent libraries. But unless\nyou package your application separately for every existing packaging system, you are doomed to\nprovide Python-specific releases\u2014a Python-specific release is a release aimed to be installed within\na Python installation no matter what the underlying Operating System is\u2014and hope that:\npackagers for every target system will be able to repackage your work,\nthe dependencies you have will themselves be repackaged in every target system, and\nsystem dependencies will be clearly described.\nSometimes, this is simply impossible. For example, Plone (a full-fledged Python-powered CMS)\nuses hundreds of small pure Python libraries that are not always available as packages in every\npackaging system out there. This means that Plone must ship everything that it needs in a\nportable application. To do this, it uses zc.buildout, which collects all its dependencies and\ncreates a portable application that will run on any system within a single directory. It is effectively a\nbinary release, since any piece of C code will be compiled in place.\nThis is a big win for developers: they just have to describe their dependencies using the Python\nstandards described below and use zc.buildout to release their application. But as discussed\nearlier, this type of release sets up a fortress within the system, which most Linux sysadmins will\nhate. Windows admins won't mind, but those managing CentOS or Debian will, because those\nsystems base their management on the assumption that every file in the system is registered,\nclassified, and known to admin tools.\nThose admins will want to repackage your application according to their own standards. The\nquestion we need to answer is, \"Can Python have a packaging system that can be automatically\ntranslated into other packaging systems?\" If so, one application or library can be installed on any\nsystem without requiring extra packaging work. Here, \"automatically\" doesn't necessarily mean\nthat the work should be fully done by a script: RPM or dpkg packagers will tell you that's impossible\n\u2014they always need to add some specifics in the projects they repackage. They'll also tell you that\nthey often have a hard time re-packaging a piece of code because its developers were not aware\nof a few basic packaging rules.\nHere's one example of what you can do to annoy packagers using the existing Python packaging\nsystem: release a library called \"MathUtils\" with the version name \"Fumanchu\". The brilliant\nmathematician who wrote the library have found it amusing to use his cats' names for his project\nversions. But how can a packager know that \"Fumanchu\" is his second cat's name, and that the\nfirst one was called \"Phil\", so that the \"Fumanchu\" version comes after the \"Phil\" one?\nThis may sound extreme, but it can happen with today's tools and standards. The worst thing is\nthat tools like easy_install or pip use their own non-standard registry to keep track of installed\nfiles, and will sort the \"Fumanchu\" and \"Phil\" versions alphanumerically.\nAnother problem is how to handle data files. For example, what if your application uses an SQLite\ndatabase? If you put it inside your package directory, your application might fail because the\nsystem forbids you to write in that part of the tree. Doing this will also compromise the\nassumptions Linux systems make about where application data is for backups (/var).\nIn the real world, system administrators need to be able to place your files where they want\nwithout breaking your application, and you need to tell them what those files are. So let's rephrase\nthe question: is it possible to have a packaging system in Python that can provide all the\ninformation needed to repackage an application with any third-party packaging system out there\nwithout having to read the code, and make everyone happy?\n14.3. The Current Architecture of Packaging\nThe Distutils package that comes with the Python standard library is riddled with the problems\ndescribed above. Since it's the standard, people either live with it and its flaws, or use more\nadvanced tools like Setuptools, which add features on the top of it, or Distribute, a fork of\nSetuptools. There's also Pip, a more advanced installer, that relies on Setuptools.\nHowever, these newer tools are all based on Distutils and inherit its problems. Attempts were\nmade to fix Distutils in place, but the code is so deeply used by other tools that any change to\nit, even its internals, is a potential regression in the whole Python packaging ecosystem.\nWe therefore decided to freeze Distutils and start the development of Distutils2 from the\n", "page": 176, "type": "text", "section": "Page 176"}
{"text": "same code base, without worrying too much about backward compatibility. To understand what\nchanged and why, let's have a closer look at Distutils.\n14.3.1. Distutils Basics and Design Flaws\nDistutils contains commands, each of which is a class with a run method that can be called\nwith some options. Distutils also provides a Distribution class that contains global values\nevery command can look at.\nTo use Distutils, a developer adds a single Python module to a project, conventionally called\nsetup.py. This module contains a call to Distutils' main entry point: the setup function. This\nfunction can take many options, which are held by a Distribution instance and used by\ncommands. Here's an example that defines a few standard options like the name and version of\nthe project, and a list of modules it contains:\nfrom distutils.core import setup\nsetup(name='MyProject', version='1.0', py_modules=['mycode.py'])\nThis module can then be used to run Distutils commands like sdist, which creates a source\ndistribution in an archive and places it in a dist directory:\n$ python setup.py sdist\nUsing the same script, you can install the project using the install command:\n$ python setup.py install\nDistutils provides other commands such as:\nupload to upload a distribution into an online repository.\nregister to register the metadata of a project in an online repository without necessary\nuploading a distribution,\nbdist to creates a binary distribution, and\nbdist_msi to create a .msi file for Windows.\nIt will also let you get information about the project via other command line options.\nSo installing a project or getting information about it is always done by invoking Distutils\nthrough this file. For example, to find out the name of the project:\n$ python setup.py --name\nMyProject\nsetup.py is therefore how everyone interacts with the project, whether to build, package,\npublish, or install it. The developer describes the content of his project through options passed to\na function, and uses that file for all his packaging tasks. The file is also used by installers to install\nthe project on a target system.\n", "page": 177, "type": "text", "section": "Page 177"}
{"text": "Figure 14.1: Setup\nHaving a single Python module used for packaging, releasing, and installing a project is one of\nDistutils' main flaws. For example, if you want to get the name from the lxml project,\nsetup.py will do a lot of things besides returning a simple string as expected:\n$ python setup.py --name\nBuilding lxml version 2.2.\nNOTE: Trying to build without Cython, pre-generated 'src/lxml/lxml.etree.c'\nneeds to be available.\nUsing build configuration of libxslt 1.1.26\nBuilding against libxml2/libxslt in the following directory: /usr/lib/lxml\nIt might even fail to work on some projects, since developers make the assumption that setup.py\nis used only to install, and that other Distutils features are only used by them during\ndevelopment. The multiple roles of the setup.py script can easily cause confusion.\n14.3.2. Metadata and PyPI\nWhen Distutils builds a distribution, it creates a Metadata file that follows the standard\ndescribed in PEP 3141. It contains a static version of all the usual metadata, like the name of the\nproject or the version of the release. The main metadata fields are:\nName: The name of the project.\nVersion: The version of the release.\nSummary: A one-line description.\nDescription: A detailed description.\nHome-Page: The URL of the project.\nAuthor: The author name.\nClassifiers: Classifiers for the project. Python provides a list of classifiers for the license,\nthe maturity of the release (beta, alpha, final), etc.\nRequires, Provides, and Obsoletes: Used to define dependencies with modules.\nThese fields are for the most part easy to map to equivalents in other packaging systems.\nThe Python Package Index (PyPI)2, a central repository of packages like CPAN, is able to register\nprojects and publish releases via Distutils' register and upload commands. register builds\nthe Metadata file and sends it to PyPI, allowing people and tools\u2014like installers\u2014to browse them\nvia web pages or via web services.\n", "page": 178, "type": "text", "section": "Page 178"}
{"text": "Figure 14.2: The PyPI Repository\nYou can browse projects by Classifiers, and get the author name and project URL. Meanwhile,\nRequires can be used to define dependencies on Python modules. The requires option can be\nused to add a Requires metadata element to the project:\nfrom distutils.core import setup\nsetup(name='foo', version='1.0', requires=['ldap'])\nDefining a dependency on the ldap module is purely declarative: no tools or installers ensure that\nsuch a module exists. This would be satisfactory if Python defined requirements at the module\nlevel through a require keyword like Perl does. Then it would just be a matter of the installers\nbrowsing the dependencies at PyPI and installing them; that's basically what CPAN does. But that's\nnot possible in Python since a module named ldap can exist in any Python project. Since\nDistutils allows people to release projects that can contain several packages and modules, this\nmetadata field is not useful at all.\nAnother flaw of Metadata files is that they are created by a Python script, so they are specific to\nthe platform they are executed in. For example, a project that provides features specific to\nWindows could define its setup.py as:\nfrom distutils.core import setup\nsetup(name='foo', version='1.0', requires=['win32com'])\nBut this assumes that the project only works under Windows, even if it provides portable\nfeatures. One way to solve this is to make the requires option specific to Windows:\nfrom distutils.core import setup\nimport sys\nif sys.platform == 'win32':\n    setup(name='foo', version='1.0', requires=['win32com'])\nelse:\n    setup(name='foo', version='1.0')\nThis actually makes the issue worse. Remember, the script is used to build source archives that\nare then released to the world via PyPI. This means that the static Metadata file sent to PyPI is\ndependent on the platform that was used to compile it. In other words, there is no way to indicate\nstatically in the metadata field that it is platform-specific.\n14.3.3. Architecture of PyPI\nFigure 14.3: PyPI Workflow\nAs indicated earlier, PyPI is a central index of Python projects where people can browse existing\nprojects by category or register their own work. Source or binary distributions can be uploaded\n", "page": 179, "type": "text", "section": "Page 179"}
{"text": "and added to an existing project, and then downloaded for installation or study. PyPI also offers\nweb services that can be used by tools like installers.\nRegistering Projects and Uploading Distributions\nRegistering a project to PyPI is done with the Distutils register command. It builds a POST\nrequest containing the metadata of the project, whatever its version is. The request requires an\nAuthorization header, as PyPI uses Basic Authentication to make sure every registered project is\nassociated with a user that has first registered with PyPI. Credentials are kept in the local\nDistutils configuration or typed in the prompt every time a register command is invoked. An\nexample of its use is:\n$ python setup.py register\nrunning register\nRegistering MPTools to http://pypi.python.org/pypi\nServer response (200): OK\nEach registered project gets a web page with an HTML version of the metadata, and packagers\ncan upload distributions to PyPI using upload:\n$ python setup.py sdist upload\nrunning sdist\n\u2026\nrunning upload\nSubmitting dist/mopytools-0.1.tar.gz to http://pypi.python.org/pypi\nServer response (200): OK\nIt's also possible to point users to another location via the Download-URL metadata field rather\nthan uploading files directly to PyPI.\nQuerying PyPI\nBesides the HTML pages PyPI publishes for web users, it provides two services that tools can use\nto browse the content: the Simple Index protocol and the XML-RPC APIs.\nThe Simple Index protocol starts at http://pypi.python.org/simple/, a plain HTML page that\ncontains relative links to every registered project:\n<html><head><title>Simple Index</title></head><body>\n\u22ee    \u22ee    \u22ee\n<a href='MontyLingua/'>MontyLingua</a><br/>\n<a href='mootiro_web/'>mootiro_web</a><br/>\n<a href='Mopidy/'>Mopidy</a><br/>\n<a href='mopowg/'>mopowg</a><br/>\n<a href='MOPPY/'>MOPPY</a><br/>\n<a href='MPTools/'>MPTools</a><br/>\n<a href='morbid/'>morbid</a><br/>\n<a href='Morelia/'>Morelia</a><br/>\n<a href='morse/'>morse</a><br/>\n\u22ee    \u22ee    \u22ee\n</body></html>\nFor example, the MPTools project has a MPTools/ link, which means that the project exists in the\nindex. The site it points at contains a list of all the links related to the project:\nlinks for every distribution stored at PyPI\nlinks for every Home URL defined in the Metadata, for each version of the project registered\nlinks for every Download-URL defined in the Metadata, for each version as well.\nThe page for MPTools contains:\n<html><head><title>Links for MPTools</title></head>\n<body><h1>Links for MPTools</h1>\n<a href=\"../../packages/source/M/MPTools/MPTools-0.1.tar.gz\">MPTools-0.1.tar.gz</a><br/>\n<a href=\"http://bitbucket.org/tarek/mopytools\" rel=\"homepage\">0.1 home_page</a><br/>\n</body></html>\nTools like installers that want to find distributions of a project can look for it in the index page, or\nsimply check if http://pypi.python.org/simple/PROJECT_NAME/ exists.\n", "page": 180, "type": "text", "section": "Page 180"}
{"text": "This protocol has two main limitations. First, PyPI is a single server right now, and while people\nusually have local copies of its content, we have experienced several downtimes in the past two\nyears that have paralyzed developers that are constantly working with installers that browse PyPI\nto get all the dependencies a project requires when it is built. For instance, building a Plone\napplication will generate several hundreds queries at PyPI to get all the required bits, so PyPI may\nact as a single point of failure.\nSecond, when the distributions are not stored at PyPI and a Download-URL link is provided in the\nSimple Index page, installers have to follow that link and hope that the location will be up and will\nreally contain the release. These indirections weakens any Simple Index-based process.\nThe Simple Index protocol's goal is to give to installers a list of links they can use to install a\nproject. The project metadata is not published there; instead, there are XML-RPC methods to get\nextra information about registered projects:\n>>> import xmlrpclib\n>>> import pprint\n>>> client = xmlrpclib.ServerProxy('http://pypi.python.org/pypi')\n>>> client.package_releases('MPTools')\n['0.1']\n>>> pprint.pprint(client.release_urls('MPTools', '0.1'))\n[{'comment_text': &rquot;,\n'downloads': 28,\n'filename': 'MPTools-0.1.tar.gz',\n'has_sig': False,\n'md5_digest': '6b06752d62c4bffe1fb65cd5c9b7111a',\n'packagetype': 'sdist',\n'python_version': 'source',\n'size': 3684,\n'upload_time': <DateTime '20110204T09:37:12' at f4da28>,\n'url': 'http://pypi.python.org/packages/source/M/MPTools/MPTools-0.1.tar.gz'}]\n>>> pprint.pprint(client.release_data('MPTools', '0.1'))\n{'author': 'Tarek Ziade',\n'author_email': 'tarek@mozilla.com',\n'classifiers': [],\n'description': 'UNKNOWN',\n'download_url': 'UNKNOWN',\n'home_page': 'http://bitbucket.org/tarek/mopytools',\n'keywords': None,\n'license': 'UNKNOWN',\n'maintainer': None,\n'maintainer_email': None,\n'name': 'MPTools',\n'package_url': 'http://pypi.python.org/pypi/MPTools',\n'platform': 'UNKNOWN',\n'release_url': 'http://pypi.python.org/pypi/MPTools/0.1',\n'requires_python': None,\n'stable_version': None,\n'summary': 'Set of tools to build Mozilla Services apps',\n'version': '0.1'}\nThe issue with this approach is that some of the data that the XML-RPC APIs are publishing could\nhave been stored as static files and published in the Simple Index page to simplify the work of\nclient tools. That would also avoid the extra work PyPI has to do to handle those queries. It's fine\nto have non-static data like the number of downloads per distribution published in a specialized\nweb service, but it does not make sense to have to use two different services to get all static data\nabout a project.\n14.3.4. Architecture of a Python Installation\nIf you install a Python project using python setup.py install, Distutils\u2014which is included\nin the standard library\u2014will copy the files onto your system.\nPython packages and modules will land in the Python directory that is loaded when the\ninterpreter starts: under the latest Ubuntu they will wind up in\n/usr/local/lib/python2.6/dist-packages/ and under Fedora in\n/usr/local/lib/python2.6/sites-packages/.\nData files defined in a project can land anywhere on the system.\n", "page": 181, "type": "text", "section": "Page 181"}
{"text": "The executable script will land in a bin directory on the system. Depending on the platform,\nthis could be /usr/local/bin or in a bin directory specific to the Python installation.\nEver since Python 2.5, the metadata file is copied alongside the modules and packages as\nproject-version.egg-info. For example, the virtualenv project could have a virtualenv-\n1.4.9.egg-info file. These metadata files can be considered a database of installed projects,\nsince it's possible to iterate over them and build a list of projects with their versions. However, the\nDistutils installer does not record the list of files it installs on the system. In other words, there\nis no way to remove all files that were copied in the system. This is a shame since the install\ncommand has a --record option that can be used to record all installed files in a text file.\nHowever, this option is not used by default and Distutils' documentation barely mentions it.\n14.3.5. Setuptools, Pip and the Like\nAs mentioned in the introduction, some projects tried to fix some of the problems with\nDistutils, with varying degrees of success.\nThe Dependencies Issue\nPyPI allowed developers to publish Python projects that could include several modules organized\ninto Python packages. But at the same time, projects could define module-level dependencies via\nRequire. Both ideas are reasonable, but their combination is not.\nThe right thing to do was to have project-level dependencies, which is exactly what Setuptools\nadded as a feature on the top of Distutils. It also provided a script called easy_install to\nautomatically fetch and install dependencies by looking for them on PyPI. In practice, module-level\ndependency was never really used, and people jumped on Setuptools' extensions. But since\nthese features were added in options specific to Setuptools, and ignored by Distutils or PyPI,\nSetuptools effectively created its own standard and became a hack on a top of a bad design.\neasy_install therefore needs to download the archive of the project and run its setup.py\nscript again to get the metadata it needs, and it has to do this again for every dependency. The\ndependency graph is built bit by bit after each download.\nEven if the new metadata was accepted by PyPI and browsable online, easy_install would still\nneed to download all archives because, as said earlier, metadata published at PyPI is specific to the\nplatform that was used to upload it, which can differ from the target platform. But this ability to\ninstall a project and its dependencies was good enough in 90% of the cases and was a great\nfeature to have. So Setuptools became widely used, although it still suffers from other\nproblems:\nIf a dependency install fails, there is no rollback and the system can end up in a broken state.\nThe dependency graph is built on the fly during installation, so if a dependency conflict is\nencountered the system can end up in a broken state as well.\nThe Uninstall Issue\nSetuptools did not provide an uninstaller, even though its custom metadata could have\ncontained a file listing the installed files. Pip, on the other hand, extended Setuptools' metadata\nto record installed files, and is therefore able to uninstall. But that's yet another custom set of\nmetadata, which means that a single Python installation may contain up to four different flavours\nof metadata for each installed project:\nDistutils' egg-info, which is a single metadata file.\nSetuptools' egg-info, which is a directory containing the metadata and extra Setuptools\nspecific options.\nPip's egg-info, which is an extended version of the previous.\nWhatever the hosting packaging system creates.\n14.3.6. What About Data Files?\nIn Distutils, data files can be installed anywhere on the system. If you define some package\ndata files in setup.py script like this:\nsetup(\u2026,\n  packages=['mypkg'],\n  package_dir={'mypkg': 'src/mypkg'},\n  package_data={'mypkg': ['data/*.dat']},\n", "page": 182, "type": "text", "section": "Page 182"}
{"text": "  )\nthen all files with the .dat extension in the mypkg project will be included in the distribution and\neventually installed along with the Python modules in the Python installation.\nFor data files that need to be installed outside the Python distribution, there's another option that\nstores files in the archive but puts them in defined locations:\nsetup(\u2026,\n    data_files=[('bitmaps', ['bm/b1.gif', 'bm/b2.gif']),\n                ('config', ['cfg/data.cfg']),\n                ('/etc/init.d', ['init-script'])]\n    )\nThis is terrible news for OS packagers for several reasons:\nData files are not part of the metadata, so packagers need to read setup.py and sometimes\ndive into the project's code.\nThe developer should not be the one deciding where data files should land on a target\nsystem.\nThere are no categories for these data files: images, man pages, and everything else are all\ntreated the same way.\nA packager who needs to repackage a project with such a file has no choice but to patch the\nsetup.py file so that it works as expected for her platform. To do that, she must review the code\nand change every line that uses those files, since the developer made an assumption about their\nlocation. Setuptools and Pip did not improve this.\n14.4. Improved Standards\nSo we ended up with with a mixed up and confused packaging environment, where everything is\ndriven by a single Python module, with incomplete metadata and no way to describe everything a\nproject contains. Here's what we're doing to make things better.\n14.4.1. Metadata\nThe first step is to fix our Metadata standard. PEP 345 defines a new version that includes:\na saner way to define versions\nproject-level dependencies\na static way to define platform-specific values\nVersion\nOne goal of the metadata standard is to make sure that all tools that operate on Python projects\nare able to classify them the same way. For versions, it means that every tool should be able to\nknow that \"1.1\" comes after \"1.0\". But if project have custom versioning schemes, this becomes\nmuch harder.\nThe only way to ensure consistent versioning is to publish a standard that projects will have to\nfollow. The scheme we chose is a classical sequence-based scheme. As defined in PEP 386, its\nformat is:\nN.N[.N]+[{a|b|c|rc}N[.N]+][.postN][.devN]\nwhere:\nN is an integer. You can use as many Ns as you want and separate them by dots, as long as\nthere are at least two (MAJOR.MINOR).\na, b, c and rc are alpha, beta and release candidate markers. They are followed by an integer.\nRelease candidates have two markers because we wanted the scheme to be compatible with\nPython, which uses rc. But we find c simpler.\ndev followed by a number is a dev marker.\npost followed by a number is a post-release marker.\nDepending on the project release process, dev or post markers can be used for all intermediate\nversions between two final releases. Most process use dev markers.\nFollowing this scheme, PEP 386 defines a strict ordering:\n", "page": 183, "type": "text", "section": "Page 183"}
{"text": "alpha < beta < rc < final\ndev < non-dev < post, where non-dev can be a alpha, beta, rc or final\nHere's a full ordering example:\n1.0a1 < 1.0a2.dev456 < 1.0a2 < 1.0a2.1.dev456\n  < 1.0a2.1 < 1.0b1.dev456 < 1.0b2 < 1.0b2.post345\n    < 1.0c1.dev456 < 1.0c1 < 1.0.dev456 < 1.0\n      < 1.0.post456.dev34 < 1.0.post456\nThe goal of this scheme is to make it easy for other packaging systems to translate Python\nprojects' versions into their own schemes. PyPI now rejects any projects that upload PEP 345\nmetadata with version numbers that don't follow PEP 386.\nDependencies\nPEP 345 defines three new fields that replace PEP 314 Requires, Provides, and Obsoletes.\nThose fields are Requires-Dist, Provides-Dist, and Obsoletes-Dist, and can be used\nmultiple times in the metadata.\nFor Requires-Dist, each entry contains a string naming some other Distutils project\nrequired by this distribution. The format of a requirement string is identical to that of a Distutils\nproject name (e.g., as found in the Name field) optionally followed by a version declaration within\nparentheses. These Distutils project names should correspond to names as found at PyPI, and\nversion declarations must follow the rules described in PEP 386. Some example are:\nRequires-Dist: pkginfo\nRequires-Dist: PasteDeploy\nRequires-Dist: zope.interface (>3.5.0)\nProvides-Dist is used to define extra names contained in the project. It's useful when a project\nwants to merge with another project. For example the ZODB project can include the\ntransaction project and state:\nProvides-Dist: transaction\nObsoletes-Dist is useful to mark another project as an obsolete version:\nObsoletes-Dist: OldName\nEnvironment Markers\nAn environment marker is a marker that can be added at the end of a field after a semicolon to\nadd a condition about the execution environment. Some examples are:\nRequires-Dist: pywin32 (>1.0); sys.platform == 'win32'\nObsoletes-Dist: pywin31; sys.platform == 'win32'\nRequires-Dist: foo (1,!=1.3); platform.machine == 'i386'\nRequires-Dist: bar; python_version == '2.4' or python_version == '2.5'\nRequires-External: libxslt; 'linux' in sys.platform\nThe micro-language for environment markers is deliberately kept simple enough for non-Python\nprogrammers to understand: it compares strings with the == and in operators (and their\nopposites), and allows the usual Boolean combinations. The fields in PEP 345 that can use this\nmarker are:\nRequires-Python\nRequires-External\nRequires-Dist\nProvides-Dist\nObsoletes-Dist\nClassifier\n14.4.2. What's Installed?\nHaving a single installation format shared among all Python tools is mandatory for interoperability.\nIf we want Installer A to detect that Installer B has previously installed project Foo, they both need\nto share and update the same database of installed projects.\n", "page": 184, "type": "text", "section": "Page 184"}
{"text": "Of course, users should ideally use a single installer in their system, but they may want to switch\nto a newer installer that has specific features. For instance, Mac OS X ships Setuptools, so\nusers automatically have the easy_install script. If they want to switch to a newer tool, they will\nneed it to be backward compatible with the previous one.\nAnother problem when using a Python installer on a platform that has a packaging system like\nRPM is that there is no way to inform the system that a project is being installed. What's worse,\neven if the Python installer could somehow ping the central packaging system, we would need to\nhave a mapping between the Python metadata and the system metadata. The name of the project,\nfor instance, may be different for each. That can occur for several reasons. The most common\none is a conflict name: another project outside the Python land already uses the same name for\nthe RPM. Another cause is that the name used include a python prefix that breaks the convention\nof the platform. For example, if you name your project foo-python, there are high chances that\nthe Fedora RPM will be called python-foo.\nOne way to avoid this problem is to leave the global Python installation alone, managed by the\ncentral packaging system, and work in an isolated environment. Tools like Virtualenv allows this.\nIn any case, we do need to have a single installation format in Python because interoperability is\nalso a concern for other packaging systems when they install themselves Python projects. Once a\nthird-party packaging system has registered a newly installed project in its own database on the\nsystem, it needs to generate the right metadata for the Python installaton itself, so projects\nappear to be installed to Python installers or any APIs that query the Python installation.\nThe metadata mapping issue can be addressed in that case: since an RPM knows which Python\nprojects it wraps, it can generate the proper Python-level metadata. For instance, it knows that\npython26-webob is called WebOb in the PyPI ecosystem.\nBack to our standard: PEP 376 defines a standard for installed packages whose format is quite\nsimilar to those used by Setuptools and Pip. This structure is a directory with a dist-info\nextension that contains:\nMETADATA: the metadata, as described in PEP 345, PEP 314 and PEP 241.\nRECORD: the list of installed files in a csv-like format.\nINSTALLER: the name of the tool used to install the project.\nREQUESTED: the presence of this file indicates that the project installation was explicitly\nrequested (i.e., not installed as a dependency).\nOnce all tools out there understand this format, we'll be able to manage projects in Python without\ndepending on a particular installer and its features. Also, since PEP 376 defines the metadata as a\ndirectory, it will be easy to add new files to extend it. As a matter of fact, a new metadata file called\nRESOURCES, described in the next section, might be added in a near future without modifying PEP\n376. Eventually, if this new file turns out to be useful for all tools, it will be added to the PEP.\n14.4.3. Architecture of Data Files\nAs described earlier, we need to let the packager decide where to put data files during installation\nwithout breaking the developer's code. At the same time, the developer must be able to work with\ndata files without having to worry about their location. Our solution is the usual one: indirection.\nUsing Data Files\nSuppose your MPTools application needs to work with a configuration file. The developer will put\nthat file in a Python package and use __file__ to reach it:\nimport os\nhere = os.path.dirname(__file__)\ncfg = open(os.path.join(here, 'config', 'mopy.cfg'))\nThis implies that configuration files are installed like code, and that the developer must place it\nalongside her code: in this example, in a subdirectory called config.\nThe new architecture of data files we have designed uses the project tree as the root of all files,\nand allows access to any file in the tree, whether it is located in a Python package or a simple\ndirectory. This allowed developers to create a dedicated directory for data files and access them\nusing pkgutil.open:\nimport os\n", "page": 185, "type": "text", "section": "Page 185"}
{"text": "import pkgutil\n# Open the file located in config/mopy.cfg in the MPTools project\ncfg = pkgutil.open('MPTools', 'config/mopy.cfg')\npkgutil.open looks for the project metadata and see if it contains a RESOURCES file. This is a\nsimple map of files to locations that the system may contain:\nconfig/mopy.cfg {confdir}/{distribution.name}\nHere the {confdir} variable points to the system's configuration directory, and\n{distribution.name} contains the name of the Python project as found in the metadata.\nFigure 14.4: Finding a File\nAs long as this RESOURCES metadata file is created at installation time, the API will find the location\nof mopy.cfg for the developer. And since config/mopy.cfg is the path relative to the project\ntree, it means that we can also offer a development mode where the metadata for the project are\ngenerated in-place and added in the lookup paths for pkgutil.\nDeclaring Data Files\nIn practice, a project can define where data files should land by defining a mapper in their\nsetup.cfg file. A mapper is a list of (glob-style pattern, target) tuples. Each pattern\npoints to one of several files in the project tree, while the target is an installation path that may\ncontain variables in brackets. For example, MPTools's setup.cfg could look like this:\n[files]\nresources =\n        config/mopy.cfg {confdir}/{application.name}/\n        images/*.jpg    {datadir}/{application.name}/\nThe sysconfig module will provide and document a specific list of variables that can be used, and\ndefault values for each platform. For example {confdir} is /etc on Linux. Installers can\ntherefore use this mapper in conjunction with sysconfig at installation time to know where the\nfiles should be placed. Eventually, they will generate the RESOURCES file mentioned earlier in the\ninstalled metadata so pkgutil can find back the files.\n", "page": 186, "type": "text", "section": "Page 186"}
{"text": "Figure 14.5: Installer\n14.4.4. PyPI Improvements\nI said earlier that PyPI was effectively a single point of failure. PEP 380 addresses this problem by\ndefining a mirroring protocol so that users can fall back to alternative servers when PyPI is down.\nThe goal is to allow members of the community to run mirrors around the world.\nFigure 14.6: Mirroring\nThe mirror list is provided as a list of host names of the form X.pypi.python.org, where X is in\nthe sequence a,b,c,\u2026,aa,ab,\u2026. a.pypi.python.org is the master server and mirrors start\nwith b. A CNAME record last.pypi.python.org points to the last host name so clients that are\nusing PyPI can get the list of the mirrors by looking at the CNAME.\n", "page": 187, "type": "text", "section": "Page 187"}
{"text": "For example, this call tells use that the last mirror is h.pypi.python.org, meaning that PyPI\ncurrently has 6 mirrors (b through h):\n>>> import socket\n>>> socket.gethostbyname_ex('last.pypi.python.org')[0]\n'h.pypi.python.org'\nPotentially, this protocol allows clients to redirect requests to the nearest mirror by localizing the\nmirrors by their IPs, and also fall back to the next mirror if a mirror or the master server is down.\nThe mirroring protocol itself is more complex than a simple rsync because we wanted to keep\ndownloads statistics accurate and provide minimal security.\nSynchronization\nMirrors must reduce the amount of data transferred between the central server and the mirror.\nTo achieve that, they must use the changelog PyPI XML-RPC call, and only refetch the packages\nthat have been changed since the last time. For each package P, they must copy documents\n/simple/P/ and /serversig/P.\nIf a package is deleted on the central server, they must delete the package and all associated files.\nTo detect modification of package files, they may cache the file's ETag, and may request skipping it\nusing the If-None-Match header. Once the synchronization is over, the mirror changes its\n/last-modified to the current date.\nStatistics Propagation\nWhen you download a release from any of the mirrors, the protocol ensures that the download hit\nis transmitted to the master PyPI server, then to other mirrors. Doing this ensures that people or\ntools browsing PyPI to find out how many times a release was downloaded will get a value\nsummed across all mirrors.\nStatistics are grouped into daily and weekly CSV files in the stats directory at the central PyPI\nitself. Each mirror needs to provide a local-stats directory that contains its own statistics. Each\nfile provides the number of downloads for each archive, grouped by use agents. The central\nserver visits mirrors daily to collect those statistics, and merge them back into the global stats\ndirectory, so each mirror must keep /local-stats up-to-date at least once a day.\nMirror Authenticity\nWith any distributed mirroring system, clients may want to verify that the mirrored copies are\nauthentic. Some of the possible threats include:\nthe central index may be compromised\nthe mirrors might be tampered with\na man-in-the-middle attack between the central index and the end user, or between a mirror\nand the end user\nTo detect the first attack, package authors need to sign their packages using PGP keys, so that\nusers can verify that the package comes from the author they trust. The mirroring protocol itself\nonly addresses the second threat, though some attempt is made to detect man-in-the-middle\nattacks.\nThe central index provides a DSA key at the URL /serverkey, in the PEM format as generated by\nopenssl dsa -pubout3. This URL must not be mirrored, and clients must fetch the official\nserverkey from PyPI directly, or use the copy that came with the PyPI client software. Mirrors\nshould still download the key so that they can detect a key rollover.\nFor each package, a mirrored signature is provided at /serversig/package. This is the DSA\nsignature of the parallel URL /simple/package, in DER form, using SHA-1 with DSA4.\nClients using a mirror need to perform the following steps to verify a package:\n1. Download the /simple page, and compute its SHA-1 hash.\n2. Compute the DSA signature of that hash.\n3. Download the corresponding /serversig, and compare it byte for byte with the value\ncomputed in step 2.\n4. Compute and verify (against the /simple page) the MD5 hashes of all files they download\nfrom the mirror.\n", "page": 188, "type": "text", "section": "Page 188"}
{"text": "Verification is not needed when downloading from central index, and clients should not do it to\nreduce the computation overhead.\nAbout once a year, the key will be replaced with a new one. Mirrors will have to re-fetch all\n/serversig pages. Clients using mirrors need to find a trusted copy of the new server key. One\nway to obtain one is to download it from https://pypi.python.org/serverkey. To detect\nman-in-the-middle attacks, clients need to verify the SSL server certificate, which will be signed by\nthe CACert authority.\n14.5. Implementation Details\nThe implementation of most of the improvements described in the previous section are taking\nplace in Distutils2. The setup.py file is not used anymore, and a project is completely\ndescribed in setup.cfg, a static .ini-like file. By doing this, we make it easier for packagers to\nchange the behavior of a project installation without having to deal with Python code. Here's an\nexample of such a file:\n[metadata]\nname = MPTools\nversion = 0.1\nauthor = Tarek Ziade\nauthor-email = tarek@mozilla.com\nsummary = Set of tools to build Mozilla Services apps\ndescription-file = README\nhome-page = http://bitbucket.org/tarek/pypi2rpm\nproject-url: Repository, http://hg.mozilla.org/services/server-devtools\nclassifier = Development Status :: 3 - Alpha\n    License :: OSI Approved :: Mozilla Public License 1.1 (MPL 1.1)\n[files]\npackages =\n        mopytools\n        mopytools.tests\nextra_files =\n        setup.py\n        README\n        build.py\n        _build.py\nresources =\n    etc/mopytools.cfg {confdir}/mopytools\nDistutils2 use this configuration file to:\ngenerate META-1.2 metadata files that can be used for various actions, like registering at\nPyPI.\nrun any package management command, like sdist.\ninstall a Distutils2-based project.\nDistutils2 also implements VERSION via its version module.\nThe INSTALL-DB implementation will find its way to the standard library in Python 3.3 and will be in\nthe pkgutil module. In the interim, a version of this module exists in Distutils2 for immediate\nuse. The provided APIs will let us browse an installation and know exactly what's installed.\nThese APIs are the basis for some neat Distutils2 features:\ninstaller/uninstaller\ndependency graph view of installed projects\n14.6. Lessons learned\n14.6.1. It's All About PEPs\nChanging an architecture as wide and complex as Python packaging needs to be carefully done by\nchanging standards through a PEP process. And changing or adding a new PEP takes in my\nexperience around a year.\n", "page": 189, "type": "text", "section": "Page 189"}
{"text": "One mistake the community made along the way was to deliver tools that solved some issues by\nextending the Metadata and the way Python applications were installed without trying to change\nthe impacted PEPs.\nIn other words, depending on the tool you used, the standard library Distutils or Setuptools,\napplications where installed differently. The problems were solved for one part of the community\nthat used these new tools, but added more problems for the rest of the world. OS Packagers for\ninstance, had to face several Python standards: the official documented standard and the de-facto\nstandard imposed by Setuptools.\nBut in the meantime, Setuptols had the opportunity to experiment in a realistic scale (the whole\ncommunity) some innovations in a very fast pace, and the feedback was invaluable. We were able\nto write down new PEPs with more confidence in what worked and what did not, and maybe it\nwould have been impossible to do so differently. So it's all about detecting when some third-party\ntools are contributing innovations that are solving problems and that should ignite a PEP change.\n14.6.2. A Package that Enters the Standard Library Has One Foot in\nthe Grave\nI am paraphrasing Guido van Rossum in the section title, but that's one aspect of the batteries-\nincluded philosophy of Python that impacts a lot our efforts.\nDistutils is part of the standard library and Distutils2 will soon be. A package that's in the\nstandard library is very hard to make evolve. There are of course deprecation processes, where\nyou can kill or change an API after 2 minor versions of Python. But once an API is published, it's\ngoing to stay there for years.\nSo any change you make in a package in the standard library that is not a bug fix, is a potential\ndisturbance for the eco-system. So when you're doing important changes, you have to create a\nnew package.\nI've learned it the hard way with Distutils since I had to eventually revert all the changes I had\ndone in it for more that a year and create Distutils2. In the future, if our standards change\nagain in a drastic way, there are high chances that we will start a standalone Distutils3 project\nfirst, unless the standard library is released on its own at some point.\n14.6.3. Backward Compatibility\nChanging the way packaging works in Python is a very long process: the Python ecosystem\ncontains so many projects based on older packaging tools that there is and will be a lot of\nresistance to change. (Reaching consensus on some of the topics discussed in this chapter took\nseveral years, rather than the few months I originally expected.) As with Python 3, it will take years\nbefore all projects switch to the new standard.\nThat's why everything we are doing has to be backward-compatible with all previous tools,\ninstallations and standards, which makes the implementation of Distutils2 a wicked problem.\nFor example, if a project that uses the new standards depends on another project that don't use\nthem yet, we can't stop the installation process by telling the end-user that the dependency is in\nan unkown format !\nFor example, the INSTALL-DB implementation contains compatibility code to browse projects\ninstalled by the original Distutils, Pip, Distribute, or Setuptools. Distutils2 is also able\nto install projects created by the original Distutils by converting their metadata on the fly.\n14.7. References and Contributions\nSome sections in this paper were directly taken from the various PEP documents we wrote for\npackaging. You can find the original documents at http://python.org:\nPEP 241: Metadata for Python Software Packages 1.0: http://python.org/peps/pep-\n0214.html\nPEP 314: Metadata for Python Software Packages 1.1: http://python.org/peps/pep-\n0314.html\nPEP 345: Metadata for Python Software Packages 1.2: http://python.org/peps/pep-\n0345.html\nPEP 376: Database of Installed Python Distributions: http://python.org/peps/pep-\n", "page": 190, "type": "text", "section": "Page 190"}
{"text": "0376.html\nPEP 381: Mirroring infrastructure for PyPI: http://python.org/peps/pep-0381.html\nPEP 386: Changing the version comparison module in Distutils:\nhttp://python.org/peps/pep-0386.html\nI would like to thank all the people that are working on packaging; you will find their name in every\nPEP I've mentioned. I would also like to give a special thank to all members of The Fellowship of the\nPackaging. Also, thanks to Alexis Metaireau, Toshio Kuratomi, Holger Krekel and Stefane Fermigier\nfor their feedback on this chapter.\nThe projects that were discussed in this chapter are:\nDistutils: http://docs.python.org/distutils\nDistutils2: http://packages.python.org/Distutils2\nDistribute: http://packages.python.org/distribute\nSetuptools: http://pypi.python.org/pypi/setuptools\nPip: http://pypi.python.org/pypi/pip\nVirtualenv: http://pypi.python.org/pypi/virtualenv\nFootnotes\n1. The Python Enhancement Proposals, or PEPs, that we refer to are summarized at the end of\nthis chapter\n2. Formerly known as the CheeseShop.\n3. I.e., RFC 3280 SubjectPublicKeyInfo, with the algorithm 1.3.14.3.2.12.\n4. I.e., as a RFC 3279 Dsa-Sig-Value, created by algorithm 1.2.840.10040.4.3.\n", "page": 191, "type": "text", "section": "Page 191"}
{"text": "The Architecture of\nOpen Source Applications\nAmy Brown and Greg Wilson (eds.)\nISBN 978-1-257-63801-7 \nLicense / Buy / Contribute\nChapter 15. Riak and Erlang/OTP\nFrancesco Cesarini, Andy Gross, and Justin Sheehy\nRiak is a distributed, fault tolerant, open source database that illustrates how to build large scale\nsystems using Erlang/OTP. Thanks in large part to Erlang's support for massively scalable\ndistributed systems, Riak offers features that are uncommon in databases, such as high-\navailability and linear scalability of both capacity and throughput.\nErlang/OTP provides an ideal platform for developing systems like Riak because it provides inter-\nnode communication, message queues, failure detectors, and client-server abstractions out of the\nbox. What's more, most frequently-used patterns in Erlang have been implemented in library\nmodules, commonly referred to as OTP behaviors. They contain the generic code framework for\nconcurrency and error handling, simplifying concurrent programming and protecting the\ndeveloper from many common pitfalls. Behaviors are monitored by supervisors, themselves a\nbehavior, and grouped together in supervision trees. A supervision tree is packaged in an\napplication, creating a building block of an Erlang program.\nA complete Erlang system such as Riak is a set of loosely coupled applications that interact with\neach other. Some of these applications have been written by the developer, some are part of the\nstandard Erlang/OTP distribution, and some may be other open source components. They are\nsequentially loaded and started by a boot script generated from a list of applications and versions.\nWhat differs among systems are the applications that are part of the release which is started. In\nthe standard Erlang distribution, the boot files will start the Kernel and StdLib (Standard Library)\napplications. In some installations, the SASL (Systems Architecture Support Library) application is\nalso started. SASL contains release and software upgrade tools together with logging capabilities.\nRiak is no different, other than starting the Riak specific applications as well as their runtime\ndependencies, which include Kernel, StdLib and SASL. A complete and ready-to-run build of Riak\nactually embeds these standard elements of the Erlang/OTP distribution and starts them all in\nunison when riak start is invoked on the command line. Riak consists of many complex\napplications, so this chapter should not be interpreted as a complete guide. It should be seen as\nan introduction to OTP where examples from the Riak source code are used. The figures and\nexamples have been abbreviated and shortened for demonstration purposes.\n15.1. An Abridged Introduction to Erlang\nErlang is a concurrent functional programming language that compiles to byte code and runs in a\nvirtual machine. Programs consist of functions that call each other, often resulting in side effects\nsuch as inter-process message passing, I/O and database operations. Erlang variables are single\nassignment, i.e., once they have been given values, they cannot be updated. The language makes\nextensive use of pattern matching, as shown in the factorial example below:\n-module(factorial).\n-export([fac/1]).\nfac(0) -> 1;\nfac(N) when N>0 ->\n   Prev = fac(N-1),\n   N*Prev.\nHere, the first clause gives the factorial of zero, the second factorials of positive numbers. The\nbody of each clause is a sequence of expressions, and the final expression in the body is the\nresult of that clause. Calling the function with a negative number will result in a run time error, as\nnone of the clauses match. Not handling this case is an example of non-defensive programming, a\npractice encouraged in Erlang.\nWithin the module, functions are called in the usual way; outside, the name of the module is\nprepended, as in factorial:fac(3). It is possible to define functions with the same name but\ndifferent numbers of arguments\u2014this is called their arity. In the export directive in the factorial\n", "page": 192, "type": "text", "section": "Page 192"}
{"text": "module the fac function of arity one is denoted by fac/1.\nErlang supports tuples (also called product types) and lists. Tuples are enclosed in curly brackets,\nas in {ok,37}. In tuples, we access elements by position. Records are another data type; they\nallow us to store a fixed number of elements which are then accessed and manipulated by name.\nWe define a record using the -record(state, {id, msg_list=[]}). To create an instance,\nwe use the expression Var = #state{id=1}, and we examine its contents using\nVar#state.id. For a variable number of elements, we use lists defined in square brackets such\nas in {[}23,34{]}. The notation {[}X|Xs{]} matches a non-empty list with head X and tail Xs.\nIdentifiers beginning with a lower case letter denote atoms, which simply stand for themselves; the\nok in the tuple {ok,37} is an example of an atom. Atoms used in this way are often used to\ndistinguish between different kinds of function result: as well as ok results, there might be results\nof the form {error, \"Error String\"}.\nProcesses in Erlang systems run concurrently in separate memory, and communicate with each\nother by message passing. Processes can be used for a wealth of applications, including gateways\nto databases, as handlers for protocol stacks, and to manage the logging of trace messages from\nother processes. Although these processes handle different requests, there will be similarities in\nhow these requests are handled.\nAs processes exist only within the virtual machine, a single VM can simultaneously run millions of\nprocesses, a feature Riak exploits extensively. For example, each request to the database\u2014reads,\nwrites, and deletes\u2014is modeled as a separate process, an approach that would not be possible\nwith most OS-level threading implementations.\nProcesses are identified by process identifiers, called PIDs, but they can also be registered under\nan alias; this should only be used for long-lived \"static\" processes. Registering a process with its\nalias allows other processes to send it messages without knowing its PID. Processes are created\nusing the spawn(Module, Function, Arguments) built-in function (BIF). BIFs are functions\nintegrated in the VM and used to do what is impossible or slow to execute in pure Erlang. The\nspawn/3 BIF takes a Module, a Function and a list of Arguments as parameters. The call returns\nthe PID of the newly spawned process and as a side effect, creates a new process that starts\nexecuting the function in the module with the arguments mentioned earlier.\nA message Msg is sent to a process with process id Pid using Pid ! Msg. A process can find out\nits PID by calling the BIF self, and this can then be sent to other processes for them to use to\ncommunicate with the original process. Suppose that a process expects to receive messages of\nthe form {ok, N} and {error, Reason}. To process these it uses a receive statement:\nreceive\n   {ok, N} ->\n      N+1;\n   {error, _} ->\n      0\nend\nThe result of this is a number determined by the pattern-matched clause. When the value of a\nvariable is not needed in the pattern match, the underscore wild-card can be used as shown\nabove.\nMessage passing between processes is asynchronous, and the messages received by a process\nare placed in the process's mailbox in the order in which they arrive. Suppose that now the\nreceive expression above is to be executed: if the first element in the mailbox is either {ok, N}\nor {error, Reason} the corresponding result will be returned. If the first message in the\nmailbox is not of this form, it is retained in the mailbox and the second is processed in a similar\nway. If no message matches, the receive will wait for a matching message to be received.\nProcesses terminate for two reasons. If there is no more code to execute, they are said to\nterminate with reason normal. If a process encounters a run-time error, it is said to terminate with\na non-normal reason. A process terminating will not affect other processes unless they are linked\nto it. Processes can link to each other through the link(Pid) BIF or when calling the\nspawn_link(Module, Function, Arguments). If a process terminates, it sends an EXIT signal\nto processes in its link set. If the termination reason is non-normal, the process terminates itself,\npropagating the EXIT signal further. By calling the process_flag(trap_exit, true) BIF,\nprocesses can receive the EXIT signals as Erlang messages in their mailbox instead of terminating.\nRiak uses EXIT signals to monitor the well-being of helper processes performing non-critical work\ninitiated by the request-driving finite state machines. When these helper processes terminate\nabnormally, the EXIT signal allows the parent to either ignore the error or restart the process.\n", "page": 193, "type": "text", "section": "Page 193"}
{"text": "15.2. Process Skeletons\nWe previously introduced the notion that processes follow a common pattern regardless of the\nparticular purpose for which the process was created. To start off, a process has to be spawned\nand then, optionally, have its alias registered. The first action of the newly spawned process is to\ninitialize the process loop data. The loop data is often the result of arguments passed to the spawn\nbuilt-in function at the initialization of the process. Its loop data is stored in a variable we refer to\nas the process state. The state, often stored in a record, is passed to a receive-evaluate function,\nrunning a loop which receives a message, handles it, updates the state, and passes it back as an\nargument to a tail-recursive call. If one of the messages it handles is a `stop' message, the\nreceiving process will clean up after itself and then terminate.\nThis is a recurring theme among processes that will occur regardless of the task the process has\nbeen assigned to perform. With this in mind, let's look at the differences between the processes\nthat conform to this pattern:\nThe arguments passed to the spawn BIF calls will differ from one process to another.\nYou have to decide whether you should register a process under an alias, and if you do, what\nalias should be used.\nIn the function that initializes the process state, the actions taken will differ based on the\ntasks the process will perform.\nThe state of the system is represented by the loop data in every case, but the contents of\nthe loop data will vary among processes.\nWhen in the body of the receive-evaluate loop, processes will receive different messages and\nhandle them in different ways.\nFinally, on termination, the cleanup will vary from process to process.\nSo, even if a skeleton of generic actions exists, these actions are complemented by specific ones\nthat are directly related to the tasks assigned to the process. Using this skeleton as a template,\nprogrammers can create Erlang processes that act as servers, finite state machines, event\nhandlers and supervisors. But instead of re-implementing these patterns every time, they have\nbeen placed in library modules referred to as behaviors. They come as part as the OTP\nmiddleware.\n15.3. OTP Behaviors\nThe core team of developers committing to Riak is spread across nearly a dozen geographical\nlocations. Without very tight coordination and templates to work from, the result would consist of\ndifferent client/server implementations not handling special borderline cases and concurrency-\nrelated errors. There would probably be no uniform way to handle client and server crashes or\nguaranteeing that a response from a request is indeed the response, and not just any message\nthat conforms to the internal message protocol.\nOTP is a set of Erlang libraries and design principles providing ready-made tools with which to\ndevelop robust systems. Many of these patterns and libraries are provided in the form of\n\"behaviors.\"\nOTP behaviors address these issues by providing library modules that implement the most\ncommon concurrent design patterns. Behind the scenes, without the programmer having to be\naware of it, the library modules ensure that errors and special cases are handled in a consistent\nway. As a result, OTP behaviors provide a set of standardized building blocks used in designing\nand building industrial-grade systems.\n15.3.1. Introduction\nOTP behaviors are provided as library modules in the stdlib application which comes as part of\nthe Erlang/OTP distribution. The specific code, written by the programmer, is placed in a separate\nmodule and called through a set of predefined callback functions standardized for each behavior.\nThis callback module will contain all of the specific code required to deliver the desired functionality.\nOTP behaviors include worker processes, which do the actual processing, and supervisors, whose\ntask is to monitor workers and other supervisors. Worker behaviors, often denoted in diagrams\nas circles, include servers, event handlers, and finite state machines. Supervisors, denoted in\nillustrations as squares, monitor their children, both workers and other supervisors, creating what\nis called a supervision tree.\n", "page": 194, "type": "text", "section": "Page 194"}
{"text": "Figure 15.1: OTP Riak Supervision Tree\nSupervision trees are packaged into a behavior called an application. OTP applications are not only\nthe building blocks of Erlang systems, but are also a way to package reusable components.\nIndustrial-grade systems like Riak consist of a set of loosely coupled, possibly distributed\napplications. Some of these applications are part of the standard Erlang distribution and some are\nthe pieces that make up the specific functionality of Riak.\nExamples of OTP applications include the Corba ORB or the Simple Network Management Protocol\n(SNMP) agent. An OTP application is a reusable component that packages library modules together\nwith supervisor and worker processes. From now on, when we refer to an application, we will\nmean an OTP application.\nThe behavior modules contain all of the generic code for each given behavior type. Although it is\npossible to implement your own behavior module, doing so is rare because the ones that come\nwith the Erlang/OTP distribution will cater to most of the design patterns you would use in your\ncode. The generic functionality provided in a behavior module includes operations such as:\nspawning and possibly registering the process;\nsending and receiving client messages as synchronous or asynchronous calls, including\ndefining the internal message protocol;\nstoring the loop data and managing the process loop; and\nstopping the process.\nThe loop data is a variable that will contain the data the behavior needs to store in between calls.\nAfter the call, an updated variant of the loop data is returned. This updated loop data, often\nreferred to as the new loop data, is passed as an argument in the next call. Loop data is also often\nreferred to as the behavior state.\nThe functionality to be included in the callback module for the generic server application to deliver\nthe specific required behavior includes the following:\nInitializing the process loop data, and, if the process is registered, the process name.\nHandling the specific client requests, and, if synchronous, the replies sent back to the client.\nHandling and updating the process loop data in between the process requests.\nCleaning up the process loop data upon termination.\n15.3.2. Generic Servers\nGeneric servers that implement client/server behaviors are defined in the gen_server behavior\nthat comes as part of the standard library application. In explaining generic servers, we will use\nthe riak_core_node_watcher.erl module from the riak_core application. It is a server that\ntracks and reports on which sub-services and nodes in a Riak cluster are available. The module\nheaders and directives are as follows:\n-module(riak_core_node_watcher).\n-behavior(gen_server).\n%% API\n-export([start_link/0,service_up/2,service_down/1,node_up/0,node_down/0,services/0,\n         services/1,nodes/1,avsn/0]).\n%% gen_server callbacks\n-export([init/1,handle_call/3,handle_cast/2,handle_info/2,terminate/2, code_change/3]).\n", "page": 195, "type": "text", "section": "Page 195"}
{"text": "-record(state, {status=up, services=[], peers=[], avsn=0, bcast_tref,\n                bcast_mod={gen_server, abcast}}).\nWe can easily recognize generic servers through the -behavior(gen_server). directive. This\ndirective is used by the compiler to ensure all callback functions are properly exported. The record\nstate is used in the server loop data.\n15.3.3. Starting Your Server\nWith the gen_server behavior, instead of using the spawn and spawn_link BIFs, you will use\nthe gen_server:start and gen_server:start_link functions. The main difference between\nspawn and start is the synchronous nature of the call. Using start instead of spawn makes\nstarting the worker process more deterministic and prevents unforeseen race conditions, as the\ncall will not return the PID of the worker until it has been initialized. You call the functions with\neither of:\ngen_server:start_link(ServerName, CallbackModule, Arguments, Options)\ngen_server:start_link(CallbackModule, Arguments, Options)\nServerName is a tuple of the format {local, Name} or {global, Name}, denoting a local or\nglobal Name for the process alias if it is to be registered. Global names allow servers to be\ntransparently accessed across a cluster of distributed Erlang nodes. If you do not want to register\nthe process and instead reference it using its PID, you omit the argument and use a\nstart_link/3 or start/3 function call instead. CallbackModule is the name of the module in\nwhich the specific callback functions are placed, Arguments is a valid Erlang term that is passed to\nthe init/1 callback function, while Options is a list that allows you to set the memory\nmanagement flags fullsweep_after and heapsize, as well as other tracing and debugging\nflags.\nIn our example, we call start_link/4, registering the process with the same name as the\ncallback module, using the ?MODULE macro call. This macro is expanded to the name of the\nmodule it is defined in by the preprocessor when compiling the code. It is always good practice to\nname your behavior with an alias that is the same as the callback module it is implemented in. We\ndon't pass any arguments, and as a result, just send the empty list. The options list is kept empty:\nstart_link() ->\n    gen_server:start_link({local, ?MODULE}, ?MODULE, [], []).\nThe obvious difference between the start_link and start functions is that start_link links to\nits parent, most often a supervisor, while start doesn't. This needs a special mention as it is an\nOTP behavior's responsibility to link itself to the supervisor. The start functions are often used\nwhen testing behaviors from the shell, as a typing error causing the shell process to crash would\nnot affect the behavior. All variants of the start and start_link functions return {ok, Pid}.\nThe start and start_link functions will spawn a new process that calls the init(Arguments)\ncallback function in the CallbackModule, with the Arguments supplied. The init function must\ninitialize the LoopData of the server and has to return a tuple of the format {ok, LoopData}.\nLoopData contains the first instance of the loop data that will be passed between the callback\nfunctions. If you want to store some of the arguments you passed to the init function, you\nwould do so in the LoopData variable. The LoopData in the Riak node watcher server is the result\nof the schedule_broadcast/1 called with a record of type state where the fields are set to the\ndefault values:\ninit([]) ->\n    %% Watch for node up/down events\n    net_kernel:monitor_nodes(true),\n    %% Setup ETS table to track node status\n    ets:new(?MODULE, [protected, named_table]),\n    {ok, schedule_broadcast(#state{})}.\nAlthough the supervisor process might call the start_link/4 function, a different process calls\nthe init/1 callback: the one that was just spawned. As the purpose of this server is to notice,\nrecord, and broadcast the availability of sub-services within Riak, the initialization asks the Erlang\nruntime to notify it of such events, and sets up a table to store this information in. This needs to\n", "page": 196, "type": "text", "section": "Page 196"}
{"text": "be done during initialization, as any calls to the server would fail if that structure did not yet exist.\nDo only what is necessary and minimize the operations in your init function, as the call to init\nis a synchronous call that prevents all of the other serialized processes from starting until it\nreturns.\n15.3.4. Passing Messages\nIf you want to send a synchronous message to your server, you use the gen_server:call/2\nfunction. Asynchronous calls are made using the gen_server:cast/2 function. Let's start by\ntaking two functions from Riak's service API; we will provide the rest of the code later. They are\ncalled by the client process and result in a synchronous message being sent to the server process\nregistered with the same name as the callback module. Note that validating the data sent to the\nserver should occur on the client side. If the client sends incorrect information, the server should\nterminate.\nservice_up(Id, Pid) ->\n    gen_server:call(?MODULE, {service_up, Id, Pid}).\nservice_down(Id) ->\n    gen_server:call(?MODULE, {service_down, Id}).\nUpon receiving the messages, the gen_server process calls the handle_call/3 callback\nfunction dealing with the messages in the same order in which they were sent:\nhandle_call({service_up, Id, Pid}, _From, State) ->\n    %% Update the set of active services locally\n    Services = ordsets:add_element(Id, State#state.services),\n    S2 = State#state { services = Services },\n    %% Remove any existing mrefs for this service\n    delete_service_mref(Id),\n    %% Setup a monitor for the Pid representing this service\n    Mref = erlang:monitor(process, Pid),\n    erlang:put(Mref, Id),\n    erlang:put(Id, Mref),\n    %% Update our local ETS table and broadcast\n    S3 = local_update(S2),\n    {reply, ok, update_avsn(S3)};\nhandle_call({service_down, Id}, _From, State) ->\n    %% Update the set of active services locally\n    Services = ordsets:del_element(Id, State#state.services),\n    S2 = State#state { services = Services },\n    %% Remove any existing mrefs for this service\n    delete_service_mref(Id),\n    %% Update local ETS table and broadcast\n    S3 = local_update(S2),\n    {reply, ok, update_avsn(S3)};\nNote the return value of the callback function. The tuple contains the control atom reply, telling\nthe gen_server generic code that the second element of the tuple (which in both of these cases\nis the atom ok) is the reply sent back to the client. The third element of the tuple is the new State,\nwhich, in a new iteration of the server, is passed as the third argument to the handle_call/3\nfunction; in both cases here it is updated to reflect the new set of available services. The argument\n_From is a tuple containing a unique message reference and the client process identifier. The tuple\nas a whole is used in library functions that we will not be discussing in this chapter. In the majority\nof cases, you will not need it.\nThe gen_server library module has a number of mechanisms and safeguards built in that operate\nbehind the scenes. If your client sends a synchronous message to your server and you do not\nget a response within five seconds, the process executing the call/2 function is terminated. You\ncan override this by using gen_server:call(Name, Message, Timeout) where Timeout is a\nvalue in milliseconds or the atom infinity.\n", "page": 197, "type": "text", "section": "Page 197"}
{"text": "The timeout mechanism was originally put in place for deadlock prevention purposes, ensuring\nthat servers that accidentally call each other are terminated after the default timeout. The crash\nreport would be logged, and hopefully would result in the error being debugged and fixed. Most\napplications will function appropriately with a timeout of five seconds, but under very heavy loads,\nyou might have to fine-tune the value and possibly even use infinity; this choice is application-\ndependent. All of the critical code in Erlang/OTP uses infinity. Various places in Riak use\ndifferent values for the timeout: infinity is common between coupled pieces of the internals,\nwhile Timeout is set based on a user-passed parameter in cases where the client code talking to\nRiak has specified that an operation should be allowed to time out.\nOther safeguards when using the gen_server:call/2 function include the case of sending a\nmessage to a nonexistent server and the case of a server crashing before sending its reply. In\nboth cases, the calling process will terminate. In raw Erlang, sending a message that is never\npattern-matched in a receive clause is a bug that can cause a memory leak. Two different\nstrategies are used in Riak to mitigate this, both of which involve \"catchall\" matching clauses. In\nplaces where the message might be user-initiated, an unmatched message might be silently\ndiscarded. In places where such a message could only come from Riak's internals, it represents a\nbug and so will be used to trigger an error-alerting internal crash report, restarting the worker\nprocess that received it.\nSending asynchronous messages works in a similar way. Messages are sent asynchronously to\nthe generic server and handled in the handle_cast/2 callback function. The function has to\nreturn a tuple of the format {reply, NewState}. Asynchronous calls are used when we are not\ninterested in the request of the server and are not worried about producing more messages than\nthe server can consume. In cases where we are not interested in a response but want to wait until\nthe message has been handled before sending the next request, we would use a\ngen_server:call/2, returning the atom ok in the reply. Picture a process generating database\nentries at a faster rate than Riak can consume. By using asynchronous calls, we risk filling up the\nprocess mailbox and make the node run out of memory. Riak uses the message-serializing\nproperties of synchronous gen_server calls to regulate load, processing the next request only\nwhen the previous one has been handled. This approach eliminates the need for more complex\nthrottling code: in addition to enabling concurrency, gen_server processes can also be used to\nintroduce serialization points.\n15.3.5. Stopping the Server\nHow do you stop the server? In your handle_call/3 and handle_cast/2 callback functions,\ninstead of returning {reply, Reply, NewState} or {noreply, NewState}, you can return\n{stop, Reason, Reply, NewState} or {stop, Reason, NewState}, respectively.\nSomething has to trigger this return value, often a stop message sent to the server. Upon\nreceiving the stop tuple containing the Reason and State, the generic code executes the\nterminate(Reason, State) callback.\nThe terminate function is the natural place to insert the code needed to clean up the State of\nthe server and any other persistent data used by the system. In our example, we send out one\nlast message to our peers so that they know that this node watcher is no longer up and watching.\nIn this example, the variable State contains a record with the fields status and peers:\nterminate(_Reason, State) ->\n    %% Let our peers know that we are shutting down\n    broadcast(State#state.peers, State#state { status = down }).\nUse of the behavior callbacks as library functions and invoking them from other parts of your\nprogram is an extremely bad practice. For example, you should never call\nriak_core_node_watcher:init(Args) from another module to retrieve the initial loop data.\nSuch retrievals should be done through a synchronous call to the server. Calls to behavior\ncallback functions should originate only from the behavior library modules as a result of an event\noccurring in the system, and never directly by the user.\n15.4. Other Worker Behaviors\nA large number of other worker behaviors can and have been implemented using these same\nideas.\n15.4.1. Finite State Machines\nFinite state machines (FSMs), implemented in the gen_fsm behavior module, are a crucial\n", "page": 198, "type": "text", "section": "Page 198"}
{"text": "component when implementing protocol stacks in telecom systems (the problem domain Erlang\nwas originally invented for). States are defined as callback functions named after the state that\nreturn a tuple containing the next State and the updated loop data. You can send events to these\nstates synchronously and asynchronously. The finite state machine callback module should also\nexport the standard callback functions such as init, terminate, and handle_info.\nOf course, finite state machines are not telecom specific. In Riak, they are used in the request\nhandlers. When a client issues a request such as get, put, or delete, the process listening to\nthat request will spawn a process implementing the corresponding gen_fsm behavior. For\ninstance, the riak_kv_get_fsm is responsible for handling a get request, retrieving data and\nsending it out to the client process. The FSM process will pass through various states as it\ndetermines which nodes to ask for the data, as it sends out messages to those nodes, and as it\nreceives data, errors, or timeouts in response.\n15.4.2. Event Handlers\nEvent handlers and managers are another behavior implemented in the gen_event library module.\nThe idea is to create a centralized point that receives events of a specific kind. Events can be sent\nsynchronously and asynchronously with a predefined set of actions being applied when they are\nreceived. Possible responses to events include logging them to file, sending off an alarm in the\nform of an SMS, or collecting statistics. Each of these actions is defined in a separate callback\nmodule with its own loop data, preserved between calls. Handlers can be added, removed, or\nupdated for every specific event manager. So, in practice, for every event manager there could be\nmany callback modules, and different instances of these callback modules could exist in different\nmanagers. Event handlers include processes receiving alarms, live trace data, equipment related\nevents or simple logs.\nOne of the uses for the gen_event behavior in Riak is for managing subscriptions to \"ring\nevents\", i.e., changes to the membership or partition assignment of a Riak cluster. Processes on a\nRiak node can register a function in an instance of riak_core_ring_events, which implements\nthe gen_event behavior. Whenever the central process managing the ring for that node changes\nthe membership record for the overall cluster, it fires off an event that causes each of those\ncallback modules to call the registered function. In this fashion, it is easy for various parts of Riak\nto respond to changes in one of Riak's most central data structures without having to add\ncomplexity to the central management of that structure.\nMost common concurrency and communication patterns are handled with the three primary\nbehaviors we've just discussed: gen_server, gen_fsm, and gen_event. However, in large\nsystems, some application-specific patterns emerge over time that warrant the creation of new\nbehaviors. Riak includes one such behavior, riak_core_vnode, which formalizes how virtual\nnodes are implemented. Virtual nodes are the primary storage abstraction in Riak, exposing a\nuniform interface for key-value storage to the request-driving FSMs. The interface for callback\nmodules is specified using the behavior_info/1 function, as follows:\nbehavior_info(callbacks) ->\n    [{init,1},\n     {handle_command,3},\n     {handoff_starting,2},\n     {handoff_cancelled,1},\n     {handoff_finished,2},\n     {handle_handoff_command,3},\n     {handle_handoff_data,2},\n     {encode_handoff_item,2},\n     {is_empty,1},\n     {terminate,2},\n     {delete,1}];\nThe above example shows the behavior_info/1 function from riak_core_vnode. The list of\n{CallbackFunction, Arity} tuples defines the contract that callback modules must follow.\nConcrete virtual node implementations must export these functions, or the compiler will emit a\nwarning. Implementing your own OTP behaviors is relatively straightforward. Alongside defining\nyour callback functions, using the proc_lib and sys modules, you need to start them with\nparticular functions, handle system messages and monitor the parent in case it terminates.\n15.5. Supervisors\nThe supervisor behavior's task is to monitor its children and, based on some preconfigured rules,\n", "page": 199, "type": "text", "section": "Page 199"}
{"text": "take action when they terminate. Children consist of both supervisors and worker processes. This\nallows the Riak codebase to focus on the correct case, which enables the supervisor to handle\nsoftware bugs, corrupt data or system errors in a consistent way across the whole system. In the\nErlang world, this non-defensive programming approach is often referred to the \"let it crash\"\nstrategy. The children that make up the supervision tree can include both supervisors and worker\nprocesses. Worker processes are OTP behaviors including the gen_fsm, gen_server, and\ngen_event. The Riak team, not having to handle borderline error cases, get to work with a smaller\ncode base. This code base, because of its use of behaviors, is smaller to start off with, as it only\ndeals with specific code. Riak has a top-level supervisor like most Erlang applications, and also has\nsub-supervisors for groups of processes with related responsibilities. Examples include Riak's\nvirtual nodes, TCP socket listeners, and query-response managers.\n15.5.1. Supervisor Callback Functions\nTo demonstrate how the supervisor behavior is implemented, we will use the\nriak_core_sup.erl module. The Riak core supervisor is the top level supervisor of the Riak core\napplication. It starts a set of static workers and supervisors, together with a dynamic number of\nworkers handling the HTTP and HTTPS bindings of the node's RESTful API defined in application\nspecific configuration files. In a similar way to gen_servers, all supervisor callback modules must\ninclude the -behavior(supervisor). directive. They are started using the start or\nstart_link functions which take the optional ServerName, the CallBackModule, and an\nArgument which is passed to the init/1 callback function.\nLooking at the first few lines of code in the riak_core_sup.erl module, alongside the behavior\ndirective and a macro we will describe later, we notice the start_link/3 function:\n-module(riak_core_sup).\n-behavior(supervisor).\n%% API\n-export([start_link/0]).\n%% Supervisor callbacks\n-export([init/1]).\n-define(CHILD(I, Type), {I, {I, start_link, []}, permanent, 5000, Type, [I]}).\nstart_link() ->\n    supervisor:start_link({local, ?MODULE}, ?MODULE, []).\nStarting a supervisor will result in a new process being spawned, and the init/1 callback function\nbeing called in the callback module riak_core_sup.erl. The ServerName is a tuple of the format\n{local, Name} or {global, Name}, where Name is the supervisor's registered name. In our\nexample, both the registered name and the callback module are the atom riak_core_sup,\noriginating form the ?MODULE macro. We pass the empty list as an argument to init/1, treating\nit as a null value. The init function is the only supervisor callback function. It has to return a tuple\nwith format:\n{ok,  {SupervisorSpecification, ChildSpecificationList}}\nwhere SupervisorSpecification is a 3-tuple {RestartStrategy, AllowedRestarts,\nMaxSeconds} containing information on how to handle process crashes and restarts.\nRestartStrategy is one of three configuration parameters determining how the behavior's\nsiblings are affected upon abnormal termination:\none_for_one: other processes in the supervision tree are not affected.\nrest_for_one: processes started after the terminating process are terminated and\nrestarted.\none_for_all: all processes are terminated and restarted.\nAllowedRestarts states how many times any of the supervisor children may terminate in\nMaxSeconds before the supervisor terminates itself (and its children). When ones terminates, it\nsends an EXIT signal to its supervisor which, based on its restart strategy, handles the termination\naccordingly. The supervisor terminating after reaching the maximum allowed restarts ensures that\ncyclic restarts and other issues that cannot be resolved at this level are escalated. Chances are\nthat the issue is in a process located in a different sub-tree, allowing the supervisor receiving the\nescalation to terminate the affected sub-tree and restart it.\nExamining the last line of the init/1 callback function in the riak_core_sup.erl module, we\nnotice that this particular supervisor has a one-for-one strategy, meaning that the processes are\nindependent of each other. The supervisor will allow a maximum of ten restarts before restarting\nitself.\n", "page": 200, "type": "text", "section": "Page 200"}
{"text": "ChildSpecificationList specifies which children the supervisor has to start and monitor,\ntogether with information on how to terminate and restart them. It consists of a list of tuples of\nthe following format:\n{Id, {Module, Function, Arguments}, Restart, Shutdown, Type, ModuleList}\nId is a unique identifier for that particular supervisor. Module, Function, and Arguments is an\nexported function which results in the behavior start_link function being called, returning the\ntuple of the format {ok, Pid}. The Restart strategy dictates what happens depending on the\ntermination type of the process, which can be:\ntransient processes, which are never restarted;\ntemporary processes, are restarted only if they terminate abnormally; and\npermanent processes, which are always restarted, regardless of the termination being\nnormal or abnormal.\nShutdown is a value in milliseconds referring to the time the behavior is allowed to execute in the\nterminate function when terminating as the result of a restart or shutdown. The atom infinity\ncan also be used, but for behaviors other than supervisors, it is highly discouraged. Type is either\nthe atom worker, referring to the generic servers, event handlers and finite state machines, or\nthe atom supervisor. Together with ModuleList, a list of modules implementing the behavior,\nthey are used to control and suspend processes during the runtime software upgrade\nprocedures. Only existing or user implemented behaviors may be part of the child specification list\nand hence included in a supervision tree.\nWith this knowledge at hand, we should now be able to formulate a restart strategy defining inter-\nprocess dependencies, fault tolerance thresholds and escalation procedures based on a common\narchitecture. We should also be able to understand what is going on in the init/1 example of the\nriak_core_sup.erl module. First of all, study the CHILD macro. It creates the child specification\nfor one child, using the callback module name as Id, making it permanent and giving it a shut\ndown time of 5 seconds. Different child types can be workers or supervisors. Have a look at the\nexample, and see what you can make out of it:\n-define(CHILD(I, Type), {I, {I, start_link, []}, permanent, 5000, Type, [I]}).\ninit([]) ->\n    RiakWebs = case lists:flatten(riak_core_web:bindings(http),\n                                  riak_core_web:bindings(https)) of\n                   [] ->\n                       %% check for old settings, in case app.config\n                       %% was not updated\n                       riak_core_web:old_binding();\n                   Binding ->\n                       Binding\n               end,\n    Children =\n                 [?CHILD(riak_core_vnode_sup, supervisor),\n                  ?CHILD(riak_core_handoff_manager, worker),\n                  ?CHILD(riak_core_handoff_listener, worker),\n                  ?CHILD(riak_core_ring_events, worker),\n                  ?CHILD(riak_core_ring_manager, worker),\n                  ?CHILD(riak_core_node_watcher_events, worker),\n                  ?CHILD(riak_core_node_watcher, worker),\n                  ?CHILD(riak_core_gossip, worker) |\n                  RiakWebs\n                 ],\n    {ok, {{one_for_one, 10, 10}, Children}}.\nMost of the Children started by this supervisor are statically defined workers (or in the case of\nthe vnode_sup, a supervisor). The exception is the RiakWebs portion, which is dynamically\ndefined depending on the HTTP portion of Riak's configuration file.\nWith the exception of library applications, every OTP application, including those in Riak, will have\ntheir own supervision tree. In Riak, various top-level applications are running in the Erlang node,\nsuch as riak_core for distributed systems algorithms, riak_kv for key/value storage\nsemantics, webmachine for HTTP, and more. We have shown the expanded tree under\nriak_core to demonstrate the multi-level supervision going on. One of the many benefits of this\n", "page": 201, "type": "text", "section": "Page 201"}
{"text": "structure is that a given subsystem can be crashed (due to bug, environmental problem, or\nintentional action) and only that subtree will in a first instance be terminated.\nThe supervisor will restart the needed processes and the overall system will not be affected. In\npractice we have seen this work well for Riak. A user might figure out how to crash a virtual node,\nbut it will just be restarted by riak_core_vnode_sup. If they manage to crash that, the\nriak_core supervisor will restart it, propagating the termination to the top-level supervisor. This\nfailure isolation and recovery mechanism allows Riak (and Erlang) developers to straightforwardly\nbuild resilient systems.\nThe value of the supervisory model was shown when one large industrial user created a very\nabusive environment in order to find out where each of several database systems would fall apart.\nThis environment created random huge bursts of both traffic and failure conditions. They were\nconfused when Riak simply wouldn't stop running, even under the worst such arrangement.\nUnder the covers, of course, they were able to make individual processes or subsystems crash in\nmultiple ways\u2014but the supervisors would clean up and restart things to put the whole system\nback into working order every time.\n15.5.2. Applications\nThe application behavior we previously introduced is used to package Erlang modules and\nresources into reusable components. In OTP, there are two kinds of applications. The most\ncommon form, called normal applications, will start a supervision tree and all of the relevant static\nworkers. Library applications such as the Standard Library, which come as part of the Erlang\ndistribution, contain library modules but do not start a supervision tree. This is not to say that the\ncode may not contain processes or supervision trees. It just means they are started as part of a\nsupervision tree belonging to another application.\nAn Erlang system will consist of a set of loosely coupled applications. Some are written by the\ndevelopers, some are available as open source, and others are be part of the Erlang/OTP\ndistribution. The Erlang runtime system and its tools treat all applications equally, regardless of\nwhether they are part of the Erlang distribution or not.\n15.6. Replication and Communication in Riak\nRiak was designed for extreme reliability and availability at a massive scale, and was inspired by\nAmazon's Dynamo storage system [DHJ+07]. Dynamo and Riak's architectures combine aspects\nof both Distributed Hash Tables (DHTs) and traditional databases. Two key techniques that both\nRiak and Dynamo use are consistent hashing for replica placement and a gossip protocol for\nsharing common state.\nConsistent hashing requires that all nodes in the system know about each other, and know what\npartitions each node owns. This assignment data could be maintained in a centrally managed\nconfiguration file, but in large configurations, this becomes extremely difficult. Another alternative\nis to use a central configuration server, but this introduces a single point of failure in the system.\nInstead, Riak uses a gossip protocol to propagate cluster membership and partition ownership\ndata throughout the system.\nGossip protocols, also called epidemic protocols, work exactly as they sound. When a node in the\nsystem wishes to change a piece of shared data, it makes the change to its local copy of the data\nand gossips the updated data to a random peer. Upon receiving an update, a node merges the\nreceived changes with its local state and gossips again to another random peer.\nWhen a Riak cluster is started, all nodes must be configured with the same partition count. The\nconsistent hashing ring is then divided by the partition count and each interval is stored locally as\na {HashRange, Owner} pair. The first node in a cluster simply claims all the partitions. When a\nnew node joins the cluster, it contacts an existing node for its list of {HashRange, Owner} pairs.\nIt then claims (partition count)/(number of nodes) pairs, updating its local state to reflect its new\nownership. The updated ownership information is then gossiped to a peer. This updated state\nthen spread throughout the entire cluster using the above algorithm.\nBy using a gossip protocol, Riak avoids introducing a single point of failure in the form of a\ncentralized configuration server, relieving system operators from having to maintain critical cluster\nconfiguration data. Any node can then use the gossiped partition assignment data in the system\nto route requests. When used together, the gossip protocol and consistent hashing enable Riak to\nfunction as a truly decentralized system, which has important consequences for deploying and\noperating large-scale systems.\n", "page": 202, "type": "text", "section": "Page 202"}
{"text": "15.7. Conclusions and Lessons Learned\nMost programmers believe that smaller and simpler codebases are not only easier to maintain,\nthey often have fewer bugs. By using Erlang's basic distribution primitives for communication in a\ncluster, Riak can start out with a fundamentally sound asynchronous messaging layer and build its\nown protocols without having to worry about that underlying implementation. As Riak grew into a\nmature system, some aspects of its networked communication moved away from use of Erlang's\nbuilt-in distribution (and toward direct manipulation of TCP sockets) while others remained a good\nfit for the included primitives. By starting out with Erlang's native message passing for everything,\nthe Riak team was able to build out the whole system very quickly. These primitives are clean and\nclear enough that it was still easy later to replace the few places where they turned out to not be\nthe best fit in production.\nAlso, due to the nature of Erlang messaging and the lightweight core of the Erlang VM, a user can\njust as easily run 12 nodes on 1 machine or 12 nodes on 12 machines. This makes development\nand testing much easier when compared to more heavyweight messaging and clustering\nmechanisms. This has been especially valuable due to Riak's fundamentally distributed nature.\nHistorically, most distributed systems are very difficult to operate in a \"development mode\" on a\nsingle developer's laptop. As a result, developers often end up testing their code in an\nenvironment that is a subset of their full system, with very different behavior. Since a many-node\nRiak cluster can be trivially run on a single laptop without excessive resource consumption or\ntricky configuration, the development process can more easily produce code that is ready for\nproduction deployment.\nThe use of Erlang/OTP supervisors makes Riak much more resilient in the face of subcomponent\ncrashes. Riak takes this further; inspired by such behaviors, a Riak cluster is also able to easily\nkeep functioning even when whole nodes crash and disappear from the system. This can lead to a\nsometimes-surprising level of resilience. One example of this was when a large enterprise was\nstress-testing various databases and intentionally crashing them to observe their edge conditions.\nWhen they got to Riak, they became confused. Each time they would find a way (through OS-level\nmanipulation, bad IPC, etc) to crash a subsystem of Riak, they would see a very brief dip in\nperformance and then the system returned to normal behavior. This is a direct result of a\nthoughtful \"let it crash\" approach. Riak was cleanly restarting each of these subsystems on\ndemand, and the overall system simply continued to function. That experience shows exactly the\nsort of resilience enabled by Erlang/OTP's approach to building programs.\n15.7.1. Acknowledgments\nThis chapter is based on Francesco Cesarini and Simon Thompson's 2009 lecture notes from the\ncentral European Functional Programming School held in Budapest and Kom\u00e1rno. Major\ncontributions were made by Justin Sheehy of Basho Technologies and Simon Thompson of the\nUniversity of Kent. A special thank you goes to all of the reviewers, who at different stages in the\nwriting of this chapter provided valuable feedback.\n", "page": 203, "type": "text", "section": "Page 203"}
{"text": "The Architecture of\nOpen Source Applications\nAmy Brown and Greg Wilson (eds.)\nISBN 978-1-257-63801-7 \nLicense / Buy / Contribute\nChapter 16. Selenium WebDriver\nSimon Stewart\nSelenium is a browser automation tool, commonly used for writing end-to-end tests of web\napplications. A browser automation tool does exactly what you would expect: automate the\ncontrol of a browser so that repetitive tasks can be automated. It sounds like a simple problem to\nsolve, but as we will see, a lot has to happen behind the scenes to make it work.\nBefore describing the architecture of Selenium it helps to understand how the various related\npieces of the project fit together. At a very high level, Selenium is a suite of three tools. The first of\nthese tools, Selenium IDE, is an extension for Firefox that allows users to record and playback\ntests. The record/playback paradigm can be limiting and isn't suitable for many users, so the\nsecond tool in the suite, Selenium WebDriver, provides APIs in a variety of languages to allow for\nmore control and the application of standard software development practices. The final tool,\nSelenium Grid, makes it possible to use the Selenium APIs to control browser instances distributed\nover a grid of machines, allowing more tests to run in parallel. Within the project, they are referred\nto as \"IDE\", \"WebDriver\" and \"Grid\". This chapter explores the architecture of Selenium WebDriver.\nThis chapter was written during the betas of Selenium 2.0 in late 2010. If you're reading the book\nafter then, then things will have moved forward, and you'll be able to see how the architectural\nchoices described here have unfolded. If you're reading before that date: Congratulations! You\nhave a time machine. Can I have some winning lottery numbers?\n16.1. History\nJason Huggins started the Selenium project in 2004 while working at ThoughtWorks on their in-\nhouse Time and Expenses (T&E) system, which made extensive use of Javascript. Although\nInternet Explorer was the dominant browser at the time, ThoughtWorks used a number of\nalternative browsers (in particular Mozilla variants) and would file bug reports when the T&E app\nwouldn't work on their browser of choice. Open Source testing tools at the time were either\nfocused on a single browser (typically IE) or were simulations of a browser (like HttpUnit). The\ncost of a license for a commercial tool would have exhausted the limited budget for a small in-\nhouse project, so they weren't even considered as viable testing choices.\nWhere automation is difficult, it's common to rely on manual testing. This approach doesn't scale\nwhen the team is very small or when releases are extremely frequent. It's also a waste of\nhumanity to ask people to step through a script that could be automated. More prosaically, people\nare slower and more error prone than a machine for dull repetitive tasks. Manual testing wasn't an\noption.\nFortunately, all the browsers being tested supported Javascript. It made sense to Jason and the\nteam he was working with to write a testing tool in that language which could be used to verify the\nbehavior of the application. Inspired by work being done on FIT1, a table-based syntax was placed\nover the raw Javascript and this allowed tests to be written by people with limited programming\nexperience using a keyword-driven approach in HTML files. This tool, originally called \"Selenium\"\nbut later referred to as \"Selenium Core\", was released under the Apache 2 license in 2004.\nThe table format of Selenium is structured similarly to the ActionFixture from FIT. Each row of the\ntable is split into three columns. The first column gives the name of the command to execute, the\nsecond column typically contains an element identifier and the third column contains an optional\nvalue. For example, this is how to type the string \"Selenium WebDriver\" into an element identified\nwith the name \"q\":\ntype       name=q       Selenium WebDriver\nBecause Selenium was written in pure Javascript, its initial design required developers to host Core\nand their tests on the same server as the application under test (AUT) in order to avoid falling foul\nof the browser\u00c2\u00b4s security policies and the Javascript sandbox. This was not always practical or\npossible. Worse, although a developer's IDE gives them the ability to swiftly manipulate code and\n", "page": 204, "type": "text", "section": "Page 204"}
{"text": "navigate a large codebase, there is no such tool for HTML. It rapidly became clear that maintaining\neven a medium-sized suite of tests was an unwieldy and painful proposition.2\nTo resolve this and other issues, an HTTP proxy was written so that every HTTP request could be\nintercepted by Selenium. Using this proxy made it possible to side-step many of the constraints of\nthe \"same host origin\" policy, where a browser won't allow Javascript to make calls to anything\nother than the server from which the current page has been served, allowing the first weakness\nto be mitigated. The design opened up the possibility of writing Selenium bindings in multiple\nlanguages: they just needed to be able to send HTTP requests to a particular URL. The wire format\nwas closely modeled on the table-based syntax of Selenium Core and it, along with the table-based\nsyntax, became known as \"Selenese\". Because the language bindings were controlling the\nbrowser at a distance, the tool was called \"Selenium Remote Control\", or \"Selenium RC\".\nWhile Selenium was being developed, another browser automation framework was brewing at\nThoughtWorks: WebDriver. The initial code for this was released early in 2007. WebDriver was\nderived from work on projects which wanted to isolate their end-to-end tests from the underlying\ntest tool. Typically, the way that this isolation is done is via the Adapter pattern. WebDriver grew\nout of insight developed by applying this approach consistently over numerous projects, and\ninitially was a wrapper around HtmlUnit. Internet Explorer and Firefox support followed rapidly\nafter release.\nWhen WebDriver was released there were significant differences between it and Selenium RC,\nthough they sat in the same software niche of an API for browser automation. The most obvious\ndifference to a user was that Selenium RC had a dictionary-based API, with all methods exposed\non a single class, whereas WebDriver had a more object-oriented API. In addition, WebDriver only\nsupported Java, whereas Selenium RC offered support for a wide-range of languages. There were\nalso strong technical differences: Selenium Core (on which RC was based) was essentially a\nJavascript application, running inside the browser's security sandbox. WebDriver attempted to\nbind natively to the browser, side-stepping the browser's security model at the cost of significantly\nincreased development effort for the framework itself.\nIn August, 2009, it was announced that the two projects would merge, and Selenium WebDriver is\nthe result of those merged projects. As I write this, WebDriver supports language bindings for\nJava, C#, Python and Ruby. It offers support for Chrome, Firefox, Internet Explorer, Opera, and\nthe Android and iPhone browsers. There are sister projects, not kept in the same source code\nrepository but working closely with the main project, that provide Perl bindings, an implementation\nfor the BlackBerry browser, and for \"headless\" WebKit\u2014useful for those times where tests need\nto run on a continuous integration server without a proper display. The original Selenium RC\nmechanism is still maintained and allows WebDriver to provide support for browsers that would\notherwise be unsupported.\n16.2. A Digression About Jargon\nUnfortunately, the Selenium project uses a lot of jargon. To recap what we've already come\nacross:\nSelenium Core is the heart of the original Selenium implementation, and is a set of Javascript\nscripts that control the browser. This is sometimes referred to as \"Selenium\" and sometimes\nas \"Core\".\nSelenium RC was the name given to the language bindings for Selenium Core, and is\ncommonly, and confusingly, referred to as just \"Selenium\" or \"RC\". It has now been replaced\nby Selenium WebDriver, where RC's API is referred to as the \"Selenium 1.x API\".\nSelenium WebDriver fits in the same niche as RC did, and has subsumed the original 1.x\nbindings. It refers to both the language bindings and the implementations of the individual\nbrowser controlling code. This is commonly referred to as just \"WebDriver\" or sometimes as\nSelenium 2. Doubtless, this will be contracted to \"Selenium\" over time.\nThe astute reader will have noticed that \"Selenium\" is used in a fairly general sense. Fortunately,\ncontext normally makes it clear which particular Selenium people are referring to.\nFinally, there's one more phrase which I'll be using, and there's no graceful way of introducing it:\n\"driver\" is the name given to a particular implementation of the WebDriver API. For example, there\nis a Firefox driver, and an Internet Explorer driver.\n16.3. Architectural Themes\nBefore we start looking at the individual pieces to understand how they're wired together, it's\n", "page": 205, "type": "text", "section": "Page 205"}
{"text": "useful to understand the the overarching themes of the architecture and development of the\nproject. Succinctly put, these are:\nKeep the costs down.\nEmulate the user.\nProve the drivers work\u2026\n\u2026but you shouldn't need to understand how everything works.\nLower the bus factor.\nHave sympathy for a Javascript implementation.\nEvery method call is an RPC call.\nWe are an Open Source project.\n16.3.1. Keep the Costs Down\nSupporting X browsers on Y platforms is inherently an expensive proposition, both in terms of\ninitial development and maintenance. If we can find some way to keep the quality of the product\nhigh without violating too many of the other principles, then that's the route we favor. This is most\nclearly seen in our adoption of Javascript where possible, as you'll read about shortly.\n16.3.2. Emulate the User\nWebDriver is designed to accurately simulate the way that a user will interact with a web\napplication. A common approach for simulating user input is to make use of Javascript to\nsynthesize and fire the series of events that an app would see if a real user were to perform the\nsame interaction. This \"synthesized events\" approach is fraught with difficulties as each browser,\nand sometimes different versions of the same browser, fire slightly different events with slightly\ndifferent values. To complicate matters, most browsers won't allow a user to interact in this way\nwith form elements such as file input elements for security reasons.\nWhere possible WebDriver uses the alternative approach of firing events at the OS level. As these\n\"native events\" aren't generated by the browser this approach circumvents the security\nrestrictions placed on synthesized events and, because they are OS specific, once they are\nworking for one browser on a particular platform reusing the code in another browser is relatively\neasy. Sadly, this approach is only possible where WebDriver can bind closely with the browser and\nwhere the development team have determined how best to send native events without requiring\nthe browser window to be focused (as Selenium tests take a long time to run, and it\u00c2\u00b4s useful to\nbe able to use the machine for other tasks as they run). At the time of writing, this means that\nnative events can be used on Linux and Windows, but not Mac OS X.\nNo matter how WebDriver is emulating user input, we try hard to mimic user behavior as closely\nas possible. This in contrast to RC, which provided APIs that operated at a level far lower than that\nwhich a user works at.\n16.3.3. Prove the Drivers Work\nIt may be an idealistic, \"motherhood and apple pie\" thing, but I believe there's no point in writing\ncode if it doesn't work. The way we prove the drivers work on the Selenium project is to have an\nextensive set of automated test cases. These are typically \"integration tests\", requiring the code to\nbe compiled and making use of a browser interacting with a web server, but where possible we\nwrite \"unit tests\", which, unlike an integration test can be run without a full recompilation. At the\ntime of writing, there are about 500 integration tests and about 250 unit tests that could be run\nacross each and every browser. We add more as we fix issues and write new code, and our focus\nis shifting to writing more unit tests.\nNot every test is run against every browser. Some test specific capabilities that some browsers\ndon't support, or which are handled in different ways on different browsers. Examples would\ninclude the tests for new HTML5 features which aren't supported on all browsers. Despite this,\neach of the major desktop browsers have a significant subset of tests run against them.\nUnderstandably, finding a way to run 500+ tests per browser on multiple platforms is a significant\nchallenge, and it's one that the project continues to wrestle with.\n16.3.4. You Shouldn't Need to Understand How Everything Works\nVery few developers are proficient and comfortable in every language and technology we use.\nConsequently, our architecture needs to allow developers to focus their talents where they can do\nthe most good, without needing them to work on pieces of the codebase where they are\nuncomfortable.\n", "page": 206, "type": "text", "section": "Page 206"}
{"text": "16.3.5. Lower the Bus Factor\nThere's a (not entirely serious) concept in software development called the \"bus factor\". It refers\nto the number of key developers who would need to meet some grisly end\u2014presumably by being\nhit by a bus\u2014to leave the project in a state where it couldn't continue. Something as complex as\nbrowser automation could be especially prone to this, so a lot of our architectural decisions are\nmade to raise this number as high as possible.\n16.3.6. Have Sympathy for a Javascript Implementation\nWebDriver falls back to using pure Javascript to drive the browser if there is no other way of\ncontrolling it. This means that any API we add should be \"sympathetic\" to a Javascript\nimplementation. As a concrete example, HTML5 introduces LocalStorage, an API for storing\nstructured data on the client-side. This is typically implemented in the browser using SQLite. A\nnatural implementation would have been to provide a database connection to the underlying data\nstore, using something like JDBC. Eventually, we settled on an API that closely models the\nunderlying Javascript implementation because something that modeled typical database access\nAPIs wasn't sympathetic to a Javascript implementation.\n16.3.7. Every Call Is an RPC Call\nWebDriver controls browsers that are running in other processes. Although it's easy to overlook\nit, this means that every call that is made through its API is an RPC call and therefore the\nperformance of the framework is at the mercy of network latency. In normal operation, this may\nnot be terribly noticeable\u2014most OSes optimize routing to localhost\u2014but as the network latency\nbetween the browser and the test code increases, what may have seemed efficient becomes less\nso to both API designers and users of that API.\nThis introduces some tension into the design of APIs. A larger API, with coarser functions would\nhelp reduce latency by collapsing multiple calls, but this must be balanced by keeping the API\nexpressive and easy to use. For example, there are several checks that need to be made to\ndetermine whether an element is visible to an end-user. Not only do we need to take into account\nvarious CSS properties, which may need to be inferred by looking at parent elements, but we\nshould probably also check the dimensions of the element. A minimalist API would require each of\nthese checks to be made individually. WebDriver collapses all of them into a single isDisplayed\nmethod.\n16.3.8. Final Thought: This Is Open Source\nAlthough it's not strictly an architectural point, Selenium is an Open Source project. The theme that\nties all the above points together is that we'd like to make it as easy as possible for a new\ndeveloper to contribute. By keeping the depth of knowledge required as shallow as possible, using\nas few languages as necessary and by relying on automated tests to verify that nothing has\nbroken, we hopefully enable this ease of contribution.\nOriginally the project was split into a series of modules, with each module representing a particular\nbrowser with additional modules for common code and for support and utility code. Source trees\nfor each binding were stored under these modules. This approach made a lot of sense for\nlanguages such as Java and C#, but was painful to work with for Rubyists and Pythonistas. This\ntranslated almost directly into relative contributor numbers, with only a handful of people able and\ninterested to work on the Python and Ruby bindings. To address this, in October and November\nof 2010 the source code was reorganized with the Ruby and Python code stored under a single\ntop-level directory per language. This more closely matched the expectations of Open Source\ndevelopers in those languages, and the effect on contributions from the community was\nnoticeable almost immediately.\n16.4. Coping with Complexity\nSoftware is a lumpy construct. The lumps are complexity, and as designers of an API we have a\nchoice as where to push that complexity. At one extreme we could spread the complexity as\nevenly as possible, meaning that every consumer of the API needs to be party to it. The other\nextreme suggests taking as much of the complexity as possible and isolating it in a single place.\nThat single place would be a place of darkness and terror for many if they have to venture there,\nbut the trade-off is that users of the API, who need not delve into the implementation, have that\ncost of complexity paid up-front for them.\nThe WebDriver developers lean more towards finding and isolating the complexity in a few places\n", "page": 207, "type": "text", "section": "Page 207"}
{"text": "rather than spreading it out. One reason for this is our users. They're exceptionally good at finding\nproblems and issues, as a glance at our bug list shows, but because many of them are not\ndevelopers a complex API isn't going to work well. We sought to provide an API that guides people\nin the right direction. As an example, consider the following methods from the original Selenium\nAPI, each of which can be used to set the value of an input element:\ntype\ntypeKeys\ntypeKeysNative\nkeydown\nkeypress\nkeyup\nkeydownNative\nkeypressNative\nkeyupNative\nattachFile\nHere's the equivalent in the WebDriver API:\nsendKeys\nAs discussed earlier, this highlights one of the major philosophical differences between RC and\nWebDriver in that WebDriver is striving to emulate the user, whereas RC offers APIs that deal at a\nlower level that a user would find hard or impossible to reach. The distinction between typeKeys\nand typeKeysNative is that the former always uses synthetic events, whereas the latter\nattempts to use the AWT Robot to type the keys. Disappointingly, the AWT Robot sends the key\npresses to whichever window has focus, which may not necessarily be the browser. WebDriver's\nnative events, by contrast, are sent directly to the window handle, avoiding the requirement that\nthe browser window have focus.\n16.4.1. The WebDriver Design\nThe team refers to WebDriver's API as being \"object-based\". The interfaces are clearly defined and\ntry to adhere to having only a single role or responsibility, but rather than modeling every single\npossible HTML tag as its own class we only have a single WebElement interface. By following this\napproach developers who are using an IDE which supports auto-completion can be led towards\nthe next step to take. The result is that coding sessions may look like this (in Java):\nWebDriver driver = new FirefoxDriver();\ndriver.<user hits space>\nAt this point, a relatively short list of 13 methods to pick from appears. The user selects one:\ndriver.findElement(<user hits space>)\nMost IDEs will now drop a hint about the type of the argument expected, in this case a \"By\". There\nare a number of preconfigured factory methods for \"By\" objects declared as static methods on\nthe By itself. Our user will quickly end up with a line of code that looks like:\ndriver.findElement(By.id(\"some_id\"));\nRole-based Interfaces\nThink of a simplified Shop class. Every day, it needs to be restocked, and it collaborates\nwith a Stockist to deliver this new stock. Every month, it needs to pay staff and taxes.\nFor the sake of argument, let's assume that it does this using an Accountant. One way\nof modeling this looks like:\npublic interface Shop {\n    void addStock(StockItem item, int quantity);\n    Money getSalesTotal(Date startDate, Date endDate);\n}\nWe have two choices about where to draw the boundaries when defining the interface\nbetween the Shop, the Accountant and the Stockist. We could draw a theoretical line as\nshown in Figure 16.1.\nThis would mean that both Accountant and Stockist would accept a Shop as an\nargument to their respective methods. The drawback here, though, is that it's unlikely that\n", "page": 208, "type": "text", "section": "Page 208"}
{"text": "the Accountant really wants to stack shelves, and it's probably not a great idea for the\nStockist to realize the vast mark-up on prices that the Shop is adding. So, a better place\nto draw the line is shown in Figure 16.2.\nWe'll need two interfaces that the Shop needs to implement, but these interfaces clearly\ndefine the role that the Shop fulfills for both the Accountant and the Stockist. They are\nrole-based interfaces:\npublic interface HasBalance {\n    Money getSalesTotal(Date startDate, Date endDate);\n}\npublic interface Stockable {\n    void addStock(StockItem item, int quantity);\n}\npublic interface Shop extends HasBalance, Stockable {\n}\nI find UnsupportedOperationExceptions and their ilk deeply displeasing, but there needs to be\nsomething that allows functionality to be exposed for the subset of users who might need it\nwithout cluttering the rest of the APIs for the majority of users. To this end, WebDriver makes\nextensive use of role-based interfaces. For example, there is a JavascriptExecutor interface\nthat provides the ability to execute arbitrary chunks of Javascript in the context of the current\npage. A successful cast of a WebDriver instance to that interface indicates that you can expect\nthe methods on it to work.\nFigure 16.1: Accountant and Stockist Depend on Shop\nFigure 16.2: Shop Implements HasBalance and Stockable\n16.4.2. Dealing with the Combinatorial Explosion\nOne of the first things that is apparent from a moment's thought about the wide range of\nbrowsers and languages that WebDriver supports is that unless care is taken it would quickly face\nan escalating cost of maintenance. With X browsers and Y languages, it would be very easy to fall\ninto the trap of maintaining X\u00d7Y implementations.\nReducing the number of languages that WebDriver supports would be one way to reduce this\ncost, but we don't want to go down this route for two reasons. Firstly, there's a cognitive load to\nbe paid when switching from one language to another, so it's advantageous to users of the\nframework to be able to write their tests in the same language that they do the majority of their\ndevelopment work in. Secondly, mixing several languages on a single project is something that\nteams may not be comfortable with, and corporate coding standards and requirements often\n", "page": 209, "type": "text", "section": "Page 209"}
{"text": "seem to demand a technology monoculture (although, pleasingly, I think that this second point is\nbecoming less true over time), therefore reducing the number of supported languages isn't an\navailable option.\nReducing the number of supported browsers also isn't an option\u2014there were vociferous\narguments when we phased out support for Firefox 2 in WebDriver, despite the fact that when we\nmade this choice it represented less than 1% of the browser market.\nThe only choice we have left is to try and make all the browsers look identical to the language\nbindings: they should offer a uniform interface that can be addressed easily in a wide variety of\nlanguages. What is more, we want the language bindings themselves to be as easy to write as\npossible, which suggests that we want to keep them as slim as possible. We push as much logic\nas we can into the underlying driver in order to support this: every piece of functionality we fail to\npush into the driver is something that needs to be implemented in every language we support, and\nthis can represent a significant amount of work.\nAs an example, the IE driver has successfully pushed the responsibility for locating and starting IE\ninto the main driver logic. Although this has resulted in a surprising number of lines of code being\nin the driver, the language binding for creating a new instance boils down to a single method call\ninto that driver. For comparison, the Firefox driver has failed to make this change. In the Java\nworld alone, this means that we have three major classes that handle configuring and starting\nFirefox weighing in at around 1300 lines of code. These classes are duplicated in every language\nbinding that wants to support the FirefoxDriver without relying on starting a Java server. That's a\nlot of additional code to maintain.\n16.4.3. Flaws in the WebDriver Design\nThe downside of the decision to expose capabilities in this way is that until someone knows that a\nparticular interface exists they may not realize that WebDriver supports that type of functionality;\nthere's a loss of explorability in the API. Certainly when WebDriver was new we seemed to spend a\nlot of time just pointing people towards particular interfaces. We've now put a lot more effort into\nour documentation and as the API gets more widely used it becomes easier and easier for users\nto find the information they need.\nThere is one place where I think our API is particularly poor. We have an interface called\nRenderedWebElement which has a strange mish-mash of methods to do with querying the\nrendered state of the element (isDisplayed, getSize and getLocation), performing\noperations on it (hover and drag and drop methods), and a handy method for getting the value of\na particular CSS property. It was created because the HtmlUnit driver didn't expose the required\ninformation, but the Firefox and IE drivers did. It originally only had the first set of methods but we\nadded the other methods before I'd done hard thinking about how I wanted the API to evolve. The\ninterface is well known now, and the tough choice is whether we keep this unsightly corner of the\nAPI given that it's widely used, or whether we attempt to delete it. My preference is not to leave a\n\"broken window\" behind, so fixing this before we release Selenium 2.0 is important. As a result, by\nthe time you read this chapter, RenderedWebElement may well be gone.\nFrom an implementor's point of view, binding tightly to a browser is also a design flaw, albeit an\ninescapable one. It takes significant effort to support a new browser, and often several attempts\nneed to be made in order to get it right. As a concrete example, the Chrome driver has gone\nthrough four complete rewrites, and the IE driver has had three major rewrites too. The\nadvantage of binding tightly to a browser is that it offers more control.\n16.5. Layers and Javascript\nA browser automation tool is essentially built of three moving parts:\nA way of interrogating the DOM.\nA mechanism for executing Javascript.\nSome means of emulating user input.\nThis section focuses on the first part: providing a mechanism to interrogate the DOM. The lingua\nfranca of the browser is Javascript, and this seems like the ideal language to use when\ninterrogating the DOM. Although this choice seems obvious, making it leads to some interesting\nchallenges and competing requirements that need balancing when thinking about Javascript.\nLike most large projects, Selenium makes use of a layered set of libraries. The bottom layer is\nGoogle's Closure Library, which supplies primitives and a modularization mechanism allowing\nsource files to be kept focused and as small as possible. Above this, there is a utility library\n", "page": 210, "type": "text", "section": "Page 210"}
{"text": "providing functions that range from simple tasks such as getting the value of an attribute,\nthrough determining whether an element would be visible to an end user, to far more complex\nactions such as simulating a click using synthesized events. Within the project, these are viewed\nas offering the smallest units of browser automation, and so are called Browser Automation\nAtoms or atoms. Finally, there are adapter layers that compose atoms in order to meet the API\ncontracts of both WebDriver and Core.\nFigure 16.3: Layers of Selenium Javascript Library\nThe Closure Library was chosen for several reasons. The main one was that the Closure Compiler\nunderstands the modularization technique the Library uses. The Closure Compiler is a compiler\ntargeting Javascript as the output language. \"Compilation\" can be as simple as ordering input files\nin dependency order, concatenating and pretty printing them, or as complex as doing advanced\nminification and dead code removal. Another undeniable advantage was that several members of\nthe team doing the work on the Javascript code were very familiar with Closure Library.\nThis \"atomic\" library of code is used pervasively throughout the project when there is a\nrequirement to interrogate the DOM. For RC and those drivers largely composed of Javascript, the\nlibrary is used directly, typically compiled as a monolithic script. For drivers written in Java,\nindividual functions from the WebDriver adapter layer are compiled with full optimization enabled,\nand the generated Javascript included as resources in the JARs. For drivers written in C variants,\nsuch as the iPhone and IE drivers, not only are the individual functions compiled with full\noptimization, but the generated output is converted to a constant defined in a header which is\nexecuted via the driver's normal Javascript execution mechanism on demand. Although this seems\nlike a strange thing to do, it allows the Javascript to be pushed into the underlying driver without\nneeding to expose the raw source in multiple places.\nBecause the atoms are used pervasively it's possible to ensure consistent behavior between the\ndifferent browsers, and because the library is written in Javascript and doesn't require elevated\nprivileges to execute the development cycle, is easy and fast. The Closure Library can load\ndependencies dynamically, so the Selenium developer need only write a test and load it in a\nbrowser, modifying code and hitting the refresh button as required. Once the test is passing in\none browser, it's easy to load it in another browser and confirm that it passes there. Because the\nClosure Library does a good job of abstracting away the differences between browsers, this is\noften enough, though it's reassuring to know that there are continuous builds that will run the test\nsuite in every supported browser.\nOriginally Core and WebDriver had many areas of congruent code\u2014code that performed the\nsame function in slightly different ways. When we started work on the atoms, this code was\ncombed through to try and find the \"best of breed\" functionality. After all, both projects had been\nused extensively and their code was very robust so throwing away everything and starting from\nscratch would not only have been wasteful but foolish. As each atom was extracted, the sites at\nwhich it would be used were identified and switched to using the atom. For example, the Firefox\ndriver's getAttribute method shrunk from approximately 50 lines of code to 6 lines long,\nincluding blank lines:\nFirefoxDriver.prototype.getElementAttribute =\n  function(respond, parameters) {\n  var element = Utils.getElementAt(parameters.id,\n                                   respond.session.getDocument());\n  var attributeName = parameters.name;\n  respond.value = webdriver.element.getAttribute(element, attributeName);\n  respond.send();\n", "page": 211, "type": "text", "section": "Page 211"}
{"text": "};\nThat second-to-last line, where respond.value is assigned to, is using the atomic WebDriver\nlibrary.\nThe atoms are a practical demonstration of several of the architectural themes of the project.\nNaturally they enforce the requirement that an implementation of an API be sympathetic to a\nJavascript implementation. What's even better is that the same library is shared throughout the\ncodebase; where once a bug had to be verified and fixed across multiple implementations, it is\nnow enough to fix the bug in one place, which reduces the cost of change while improving stability\nand effectiveness. The atoms also make the bus factor of the project more favorable. Since a\nnormal Javascript unit test can be used to check that a fix works the barrier to joining the Open\nSource project is considerably lower than it was when knowledge of how each driver was\nimplemented was required.\nThere is another benefit to using the atoms. A layer emulating the existing RC implementation but\nbacked by WebDriver is an important tool for teams looking to migrate in a controlled fashion to\nthe newer WebDriver APIs. As Selenium Core is atomized it becomes possible to compile each\nfunction from it individually, making the task of writing this emulating layer both easier to\nimplement and more accurate.\nIt goes without saying that there are downsides to the approach taken. Most importantly,\ncompiling Javascript to a C const is a very strange thing to do, and it always baffles new\ncontributors to the project who want to work on the C code. It is also a rare developer who has\nevery version of every browser and is dedicated enough to run every test in all of those browsers\n\u2014it is possible for someone to inadvertently cause a regression in an unexpected place, and it can\ntake some time to identify the problem, particularly if the continuous builds are being flaky.\nBecause the atoms normalize return values between browsers, there can also be unexpected\nreturn values. For example, consider this HTML:\n<input name=\"example\" checked>\nThe value of the checked attribute will depend on the browser being used. The atoms normalize\nthis, and other Boolean attributes defined in the HTML5 spec, to be \"true\" or \"false\". When this\natom was introduced to the code base, we discovered many places where people were making\nbrowser-dependent assumptions about what the return value should be. While the value was now\nconsistent there was an extended period where we explained to the community what had\nhappened and why.\n16.6. The Remote Driver, and the Firefox Driver in\nParticular\nThe remote WebDriver was originally a glorified RPC mechanism. It has since evolved into one of\nthe key mechanisms we use to reduce the cost of maintaining WebDriver by providing a uniform\ninterface that language bindings can code against. Even though we've pushed as much of the\nlogic as we can out of the language bindings and into the driver, if each driver needed to\ncommunicate via a unique protocol we would still have an enormous amount of code to repeat\nacross all the language bindings.\nThe remote WebDriver protocol is used wherever we need to communicate with a browser\ninstance that's running out of process. Designing this protocol meant taking into consideration a\nnumber of concerns. Most of these were technical, but, this being open source, there was also\nthe social aspect to consider.\nAny RPC mechanism is split into two pieces: the transport and the encoding. We knew that\nhowever we implemented the remote WebDriver protocol, we would need support for both pieces\nin the languages we wanted to use as clients. The first iteration of the design was developed as\npart of the Firefox driver.\nMozilla, and therefore Firefox, was always seen as being a multi-platform application by its\ndevelopers. In order to facilitate the development, Mozilla created a framework inspired by\nMicrosoft's COM that allowed components to be built and bolted together called XPCOM (cross-\nplatform COM). An XPCOM interface is declared using IDL, and there are language bindings for C\nand Javascript as well as other languages. Because XPCOM is used to construct Firefox, and\nbecause XPCOM has Javascript bindings, it's possible to make use of XPCOM objects in Firefox\nextensions.\n", "page": 212, "type": "text", "section": "Page 212"}
{"text": "Normal Win32 COM allows interfaces to be accessed remotely. There were plans to add the same\nability to XPCOM too, and Darin Fisher added an XPCOM ServerSocket implementation to facilitate\nthis. Although the plans for D-XPCOM never came to fruition, like an appendix, the vestigial\ninfrastructure is still there. We took advantage of this to create a very basic server within a\ncustom Firefox extension containing all the logic for controlling Firefox. The protocol used was\noriginally text-based and line-oriented, encoding all strings as UTF-2. Each request or response\nbegan with a number, indicating how many newlines to count before concluding that the request\nor reply had been sent. Crucially, this scheme was easy to implement in Javascript as SeaMonkey\n(Firefox's Javascript engine at the time) stores Javascript strings internally as 16 bit unsigned\nintegers.\nAlthough futzing with custom encoding protocols over raw sockets is a fun way to pass the time,\nit has several drawbacks. There were no widely available libraries for the custom protocol, so it\nneeded to be implemented from the ground up for every language that we wanted to support.\nThis requirement to implement more code would make it less likely that generous Open Source\ncontributors would participate in the development of new language bindings. Also, although a line-\noriented protocol was fine when we were only sending text-based data around, it brought\nproblems when we wanted to send images (such as screenshots) around.\nIt became very obvious, very quickly that this original RPC mechanism wasn't practical.\nFortunately, there was a well-known transport that has widespread adoption and support in\nalmost every language that would allow us to do what we wanted: HTTP.\nOnce we had decided to use HTTP for a transport mechanism, the next choice that needed to be\nmade was whether to use a single end-point (\u00e0 la SOAP) or multiple end points (in the style of\nREST) The original Selenese protocol used a single end-point and had encoded commands and\narguments in the query string. While this approach worked well, it didn't \"feel\" right: we had visions\nof being able to connect to a remote WebDriver instance in a browser to view the state of the\nserver. We ended up choosing an approach we call \"REST-ish\": multiple end-point URLs using the\nverbs of HTTP to help provide meaning, but breaking a number of the constraints required for a\ntruly RESTful system, notably around the location of state and cacheability, largely because there\nis only one location for the application state to meaningfully exist.\nAlthough HTTP makes it easy to support multiple ways of encoding data based on content type\nnegotiation, we decided that we needed a canonical form that all implementations of the remote\nWebDriver protocol could work with. There were a handful of obvious choices: HTML, XML or\nJSON. We quickly ruled out XML: although it's a perfectly reasonable data format and there are\nlibraries that support it for almost every language, my perception of how well-liked it is in the Open\nSource community was that people don't enjoy working with it. In addition, it was entirely possible\nthat although the returned data would share a common \"shape\" it would be easy for additional\nfields to be added3. Although these extensions could be modeled using XML namespaces this\nwould start to introduce Yet More Complexity into the client code: something I was keen to avoid.\nXML was discarded as an option. HTML wasn't really a good choice, as we needed to be able to\ndefine our own data format, and though an embedded micro-format could have been devised and\nused that seems like using a hammer to crack an egg.\nThe final possibility considered was Javascript Object Notation (JSON). Browsers can transform a\nstring into an object using either a straight call to eval or, on more recent browsers, with\nprimitives designed to transform a Javascript object to and from a string securely and without\nside-effects. From a practical perspective, JSON is a popular data format with libraries for handling\nit available for almost every language and all the cool kids like it. An easy choice.\nThe second iteration of the remote WebDriver protocol therefore used HTTP as the transport\nmechanism and UTF-8 encoded JSON as the default encoding scheme. UTF-8 was picked as the\ndefault encoding so that clients could easily be written in languages with limited support for\nUnicode, as UTF-8 is backwardly compatible with ASCII. Commands sent to the server used the\nURL to determine which command was being sent, and encoded the parameters for the command\nin an array.\nFor example a call to WebDriver.get(\"http://www.example.com\") mapped to a POST\nrequest to a URL encoding the session ID and ending with \"/url\", with the array of parameters\nlooking like {[}'http://www.example.com'{]}. The returned result was a little more\nstructured, and had place-holders for a returned value and an error code. It wasn't long until the\nthird iteration of remote protocol, which replaced the request's array of parameters with a\ndictionary of named parameters. This had the benefit of making debugging requests significantly\neasier, and removed the possibility of clients mistakenly mis-ordering parameters, making the\nsystem as a whole more robust. Naturally, it was decided to use normal HTTP error codes to\nindicate certain return values and responses where they were the most appropriate way to do so;\nfor example, if a user attempts to call a URL with nothing mapped to it, or when we want to\n", "page": 213, "type": "text", "section": "Page 213"}
{"text": "indicate the \"empty response\".\nThe remote WebDriver protocol has two levels of error handling, one for invalid requests, and one\nfor failed commands. An example of an invalid request is for a resource that doesn't exist on the\nserver, or perhaps for a verb that the resource doesn't understand (such as sending a DELETE\ncommand to the the resource used for dealing with the URL of the current page) In those cases,\na normal HTTP 4xx response is sent. For a failed command, the responses error code is set to\n500 (\"Internal Server Error\") and the returned data contains a more detailed breakdown of what\nwent wrong.\nWhen a response containing data is sent from the server, it takes the form of a JSON object:\nKey\nDescription\nsessionId An opaque handle used by the server to determine where to route session-specific\ncommands.\nstatus\nA numeric status code summarizing the result of the command. A non-zero value\nindicates that the command failed.\nvalue\nThe response JSON value.\nAn example response would be:\n{\n  sessionId: 'BD204170-1A52-49C2-A6F8-872D127E7AE8',\n  status: 7,\n  value: 'Unable to locate element with id: foo'\n}\nAs can be seen, we encode status codes in the response, with a non-zero value indicating that\nsomething has gone horribly awry. The IE driver was the first to use status codes, and the values\nused in the wire protocol mirror these. Because all error codes are consistent between drivers, it\nis possible to share error handling code between all the drivers written in a particular language,\nmaking the job of the client-side implementors easier.\nThe Remote WebDriver Server is simply a Java servlet that acts as a multiplexer, routing any\ncommands it receives to an appropriate WebDriver instance. It's the sort of thing that a second\nyear graduate student could write. The Firefox driver also implements the remote WebDriver\nprotocol, and its architecture is far more interesting, so let's follow a request through from the call\nin the language bindings to that back-end until it returns to the user.\nAssuming that we're using Java, and that \"element\" is an instance of WebElement, it all starts here:\nelement.getAttribute(&lquot;row&rquot;);\nInternally, the element has an opaque \"id\" that the server-side uses to identify which element we're\ntalking about. For the sake of this discussion, we'll imagine it has the value \"some_opaque_id\". This\nis encoded into a Java Command object with a Map holding the (now named) parameters id for the\nelement ID and name for the name of the attribute being queried.\nA quick look up in a table indicates that the correct URL is:\n/session/:sessionId/element/:id/attribute/:name\nAny section of the URL that begins with a colon is assumed to be a variable that requires\nsubstitution. We've been given the id and name parameters already, and the sessionId is\nanother opaque handle that is used for routing when a server can handle more than one session\nat a time (which the Firefox driver cannot). This URL therefore typically expands to something like:\nhttp://localhost:7055/hub/session/XXX/element/some_opaque_id/attribute/row\nAs an aside, WebDriver's remote wire protocol was originally developed at the same time as URL\nTemplates were proposed as a draft RFC. Both our scheme for specifying URLs and URL\nTemplates allow variables to be expanded (and therefore derived) within a URL. Sadly, although\nURL Templates were proposed at the same time, we only became aware of them relatively late in\nthe day, and therefore they are not used to describe the wire protocol.\nBecause the method we're executing is idempotent4, the correct HTTP method to use is a GET.\nWe delegate down to a Java library that can handle HTTP (the Apache HTTP Client) to call the\nserver.\n", "page": 214, "type": "text", "section": "Page 214"}
{"text": "Figure 16.4: Overview of the Firefox Driver Architecture\nThe Firefox driver is implemented as a Firefox extension, the basic design of which is shown in\nFigure 16.4. Somewhat unusually, it has an embedded HTTP server. Although originally we used\none that we had built ourselves, writing HTTP servers in XPCOM wasn't one of our core\ncompetencies, so when the opportunity arose we replaced it with a basic HTTPD written by Mozilla\nthemselves. Requests are received by the HTTPD and almost straight away passed to a\ndispatcher object.\nThe dispatcher takes the request and iterates over a known list of supported URLs, attempting to\nfind one that matches the request. This matching is done with knowledge of the variable\ninterpolation that went on in the client side. Once an exact match is found, including the verb\nbeing used, a JSON object, representing the command to execute, is constructed. In our case it\nlooks like:\n{\n  'name': 'getElementAttribute',\n  'sessionId': { 'value': 'XXX' },\n  'parameters': {\n    'id': 'some_opaque_key',\n    'name': 'rows'\n  }\n}\nThis is then passed as a JSON string to a custom XPCOM component we've written called the\nCommandProcessor. Here's the code:\nvar jsonResponseString = JSON.stringify(json);\nvar callback = function(jsonResponseString) {\n  var jsonResponse = JSON.parse(jsonResponseString);\n  if (jsonResponse.status != ErrorCode.SUCCESS) {\n    response.setStatus(Response.INTERNAL_ERROR);\n  }\n  response.setContentType('application/json');\n  response.setBody(jsonResponseString);\n  response.commit();\n};\n// Dispatch the command.\nComponents.classes['@googlecode.com/webdriver/command-processor;1'].\n    getService(Components.interfaces.nsICommandProcessor).\n    execute(jsonString, callback);\n", "page": 215, "type": "text", "section": "Page 215"}
{"text": "There's quite a lot of code here, but there are two key points. First, we converted the object above\nto a JSON string. Secondly, we pass a callback to the execute method that causes the HTTP\nresponse to be sent.\nThe execute method of the command processor looks up the \"name\" to determine which function\nto call, which it then does. The first parameter given to this implementing function is a \"respond\"\nobject (so called because it was originally just the function used to send the response back to the\nuser), which encapsulates not only the possible values that might be sent, but also has a method\nthat allows the response to be dispatched back to the user and mechanisms to find out\ninformation about the DOM. The second parameter is the value of the parameters object seen\nabove (in this case, id and name). The advantage of this scheme is that each function has a\nuniform interface that mirrors the structure used on the client side. This means that the mental\nmodels used for thinking about the code on each side are similar. Here's the underlying\nimplementation of getAttribute, which you've seen before in Section 16.5:\nFirefoxDriver.prototype.getElementAttribute = function(respond, parameters) {\n  var element = Utils.getElementAt(parameters.id,\n                                  respond.session.getDocument());\n  var attributeName = parameters.name;\n  respond.value = webdriver.element.getAttribute(element, attributeName);\n  respond.send();\n};\nIn order to make element references consistent, the first line simply looks up the element referred\nto by the opaque ID in a cache. In the Firefox driver, that opaque ID is a UUID and the \"cache\" is\nsimply a map. The getElementAt method also checks to see if the referred to element is both\nknown and attached to the DOM. If either check fails, the ID is removed from the cache (if\nnecessary) and an exception is thrown and returned to the user.\nThe second line from the end makes use of the browser automation atoms discussed earlier, this\ntime compiled as a monolithic script and loaded as part of the extension.\nIn the final line, the send method is called. This does a simple check to ensure that we only send a\nresponse once before it calls the callback given to the execute method. The response is sent back\nto the user in the form of a JSON string, which is decanted into an object that looks like (assuming\nthat getAttribute returned \"7\", meaning the element wasn't found):\n{\n  'value': '7',\n  'status': 0,\n  'sessionId': 'XXX'\n}\nThe Java client then checks the value of the status field. If that value is non-zero, it converts the\nnumeric status code into an exception of the correct type and throws that, using the \"value\" field\nto help set the message sent to the user. If the status is zero the value of the \"value\" field is\nreturned to to the user.\nMost of this makes a certain amount of sense, but there was one piece that an astute reader will\nraise questions about: why did the dispatcher convert the object it had into a string before calling\nthe execute method?\nThe reason for this is that the Firefox Driver also supports running tests written in pure Javascript.\nNormally, this would be an extremely difficult thing to support: the tests are running in the context\nof the browser's Javascript security sandbox, and so may not do a range of things that are useful\nin tests, such as traveling between domains or uploading files. The WebDriver Firefox extension,\nhowever, provides an escape hatch from the sandbox. It announces its presence by adding a\nwebdriver property to the document element. The WebDriver Javascript API uses this as an\nindicator that it can add JSON serialized command objects as the value of a command property on\nthe document element, fire a custom webdriverCommand event and then listen for a\nwebdriverResponse event on the same element to be notified that the response property has\nbeen set.\nThis suggests that browsing the web in a copy of Firefox with the WebDriver extension installed is\na seriously bad idea as it makes it trivially easy for someone to remotely control the browser.\nBehind the scenes, there is a DOM messenger, waiting for the webdriverCommand this reads the\nserialized JSON object and calls the execute method on the command processor. This time, the\n", "page": 216, "type": "text", "section": "Page 216"}
{"text": "callback is one that simply sets the response attribute on the document element and then fires\nthe expected webdriverResponse event.\n16.7. The IE Driver\nInternet Explorer is an interesting browser. It's constructed of a number of COM interfaces\nworking in concert. This extends all the way into the Javascript engine, where the familiar\nJavascript variables actually refer to underlying COM instances. That Javascript window is an\nIHTMLWindow. document is an instance of the COM interface IHTMLDocument. Microsoft have\ndone an excellent job in maintaining existing behavior as they enhanced their browser. This means\nthat if an application worked with the COM classes exposed by IE6 it will still continue to work with\nIE9.\nThe Internet Explorer driver has an architecture that's evolved over time. One of the major forces\nupon its design has been a requirement to avoid an installer. This is a slightly unusual requirement,\nso perhaps needs some explanation. The first reason not to require an installer is that it makes it\nharder for WebDriver to pass the \"5 minute test\", where a developer downloads a package and\ntries it out for a brief period of time. More importantly, it is relatively common for users of\nWebDriver to not be able to install software on their own machines. It also means that no-one\nneeds to remember to log on to the continuous integration servers to run an installer when a\nproject wants to start testing with IE. Finally, running installers just isn't in the culture of some\nlanguages. The common Java idiom is to simply drop JAR files on to the CLASSPATH, and, in my\nexperience, those libraries that require installers tend not to be as well-liked or used.\nSo, no installer. There are consequences to this choice.\nThe natural language to use for programming on Windows would be something that ran on .Net,\nprobably C#. The IE driver integrates tightly with IE by making use of the IE COM Automation\ninterfaces that ship with every version of Windows. In particular, we use COM interfaces from the\nnative MSHTML and ShDocVw DLLs, which form part of IE. Prior to C# 4, CLR/COM interoperability\nwas achieved via the use of separate Primary Interop Assemblies (PIAs) A PIA is essentially a\ngenerated bridge between the managed world of the CLR and that of COM.\nSadly, using C# 4 would mean using a very modern version of the .Net runtime, and many\ncompanies avoid living on the leading edge, preferring the stability and known issues of older\nreleases. By using C# 4 we would automatically exclude a reasonable percentage of our user-\nbase. There are also other disadvantages to using a PIA. Consider licensing restrictions. After\nconsultation with Microsoft, it became clear that the Selenium project would not have the rights to\ndistribute the PIAs of either the MSHTML or ShDocVw libraries. Even if those rights had been\ngranted, each installation of Windows and IE has a unique combination of these libraries, which\nmeans that we would have needed to ship a vast number of these things. Building the PIAs on the\nclient machine on demand is also a non-starter, as they require developer tools that may not exist\non a normal user's machine.\nSo, although C# would have been an attractive language to do the bulk of the coding in, it wasn't\nan option. We needed to use something native, at least for the communication with IE. The next\nnatural choice for this is C++, and this is the language that we chose in the end. Using C++ has\nthe advantage that we don't need to use PIAs, but it does mean that we need to redistribute the\nVisual Studio C++ runtime DLL unless we statically link against them. Since we'd need to run an\ninstaller in order to make that DLL available, we statically link our library for communicating with IE.\nThat's a fairly high cost to pay for a requirement not to use an installer. However, going back to\nthe theme of where complexity should live, it is worth the investment as it makes our users' lives\nconsiderably easier. It is a decision we re-evaluate on an ongoing basis, as the benefit to the user\nis a trade-off with the fact that the pool of people able to contribute to an advanced C++ Open\nSource project seems significantly smaller than those able to contribute to an equivalent C#\nproject.\nThe initial design of the IE driver is shown in Figure 16.5.\nFigure 16.5: Original IE Driver\n", "page": 217, "type": "text", "section": "Page 217"}
{"text": "Starting from the bottom of that stack, you can see that we're using IE's COM Automation\ninterfaces. In order to make these easier to deal with on a conceptual level, we wrapped those raw\ninterfaces with a set of C++ classes that closely mirrored the main WebDriver API. In order to get\nthe Java classes communicating with the C++ we made use of JNI, with the implementations of the\nJNI methods using the C++ abstractions of the COM interfaces.\nThis approach worked reasonably well while Java was the only client language, but it would have\nbeen a source of pain and complexity if each language we supported needed us to alter the\nunderlying library. Thus, although JNI worked, it didn't provide the correct level of abstraction.\nWhat was the correct level of abstraction? Every language that we wanted to support had a\nmechanism for calling down to straight C code. In C#, this takes the form of PInvoke. In Ruby\nthere is FFI, and Python has ctypes. In the Java world, there is an excellent library called JNA (Java\nNative Architecture). We needed to expose our API using this lowest common denominator. This\nwas done by taking our object model and flattening it, using a simple two or three letter prefix to\nindicate the \"home interface\" of the method: \"wd\" for \"WebDriver\" and \"wde\" for WebDriver\nElement. Thus WebDriver.get became wdGet, and WebElement.getText became wdeGetText.\nEach method returns an integer representing a status code, with \"out\" parameters being used to\nallow functions to return more meaningful data. Thus we ended up with method signatures such\nas:\nint wdeGetAttribute(WebDriver*, WebElement*, const wchar_t*, StringWrapper**)\nTo calling code, the WebDriver, WebElement and StringWrapper are opaque types: we\nexpressed the difference in the API to make it clear what value should be used as that parameter,\nthough could just as easily have been \"void *\". You can also see that we were using wide\ncharacters for text, since we wanted to deal with internationalized text properly.\nOn the Java side, we exposed this library of functions via an interface, which we then adapted to\nmake it look like the normal object-oriented interface presented by WebDriver. For example, the\nJava definition of the getAttribute method looks like:\npublic String getAttribute(String name) {\n  PointerByReference wrapper = new PointerByReference();\n  int result = lib.wdeGetAttribute(\n      parent.getDriverPointer(), element, new WString(name), wrapper);\n  errors.verifyErrorCode(result, \"get attribute of\");\n  return wrapper.getValue() == null ? null : new StringWrapper(lib, wrapper).toString();\n}\nThis lead to the design shown in Figure 16.6.\nFigure 16.6: Modified IE Driver\nWhile all the tests were running on the local machine, this worked out well, but once we started\nusing the IE driver in the remote WebDriver we started running into random lock ups. We traced\nthis problem back to a constraint on the IE COM Automation interfaces. They are designed to be\nused in a \"Single Thread Apartment\" model. Essentially, this boils down to a requirement that we\ncall the interface from the same thread every time. While running locally, this happens by default.\nJava app servers, however, spin up multiple threads to handle the expected load. The end result?\nWe had no way of being sure that the same thread would be used to access the IE driver in all\ncases.\n", "page": 218, "type": "text", "section": "Page 218"}
{"text": "One solution to this problem would have been to run the IE driver in a single-threaded executor\nand serialize all access via Futures in the app server, and for a while this was the design we chose.\nHowever, it seemed unfair to push this complexity up to the calling code, and it's all too easy to\nimagine instances where people accidentally make use of the IE driver from multiple threads. We\ndecided to sink the complexity down into the driver itself. We did this by holding the IE instance in\na separate thread and using the PostThreadMessage Win32 API to communicate across the\nthread boundary. Thus, at the time of writing, the design of the IE driver looks like Figure 16.7.\nFigure 16.7: IE Driver as of Selenium 2.0 alpha 7\nThis isn't the sort of design that I would have chosen voluntarily, but it has the advantage of\nworking and surviving the horrors that our users may chose to inflict upon it.\nOne drawback to this design is that it can be hard to determine whether the IE instance has\nlocked itself solid. This may happen if a modal dialog opens while we're interacting with the DOM,\nor it may happen if there's a catastrophic failure on the far side of the thread boundary. We\ntherefore have a timeout associated with every thread message we post, and this is set to what\nwe thought was a relatively generous 2 minutes. From user feedback on the mailing lists, this\nassumption, while generally true, isn't always correct, and later versions of the IE driver may well\nmake the timeout configurable.\nAnother drawback is that debugging the internals can be deeply problematic, requiring a\ncombination of speed (after all, you've got two minutes to trace the code through as far as\npossible), the judicious use of break points and an understanding of the expected code path that\nwill be followed across the thread boundary. Needless to say, in an Open Source project with so\nmany other interesting problems to solve, there is little appetite for this sort of grungy work. This\nsignificantly reduces the bus factor of the system, and as a project maintainer, this worries me.\nTo address this, more and more of the IE driver is being moved to sit upon the same Automation\nAtoms as the Firefox driver and Selenium Core. We do this by compiling each of the atoms we\nplan to use and preparing it as a C++ header file, exposing each function as a constant. At\nruntime, we prepare the Javascript to execute from these constants. This approach means that\nwe can develop and test a reasonable percentage of code for the IE driver without needing a C\ncompiler involved, allowing far more people to contribute to finding and resolving bugs. In the end,\nthe goal is to leave only the interaction APIs in native code, and rely on the atoms as much as\npossible.\nAnother approach we're exploring is to rewrite the IE driver to make use of a lightweight HTTP\nserver, allowing us to treat it as a remote WebDriver. If this occurs, we can remove a lot of the\ncomplexity introduced by the thread boundary, reducing the total amount of code required and\nmaking the flow of control significantly easier to follow.\n16.8. Selenium RC\nIt's not always possible to bind tightly to a particular browser. In those cases, WebDriver falls back\nto the original mechanism used by Selenium. This means using Selenium Core, a pure Javascript\nframework, which introduces a number of drawbacks as it executes firmly in the context of the\nJavascript sandbox. From a user of WebDriver's APIs this means that the list of supported\nbrowsers falls into tiers, with some being tightly integrated with and offering exceptional control,\nand others being driven via Javascript and offering the same level of control as the original\nSelenium RC.\n", "page": 219, "type": "text", "section": "Page 219"}
{"text": "Conceptually, the design used is pretty simple, as you can see in Figure 16.8.\nFigure 16.8: Outline of Selenium RC's Architecture\nAs you can see, there are three moving pieces here: the client code, the intermediate server and\nthe Javascript code of Selenium Core running in the browser. The client side is just an HTTP client\nthat serializes commands to the server-side piece. Unlike the remote WebDriver, there is just a\nsingle end-point, and the HTTP verb used is largely irrelevant. This is partly because the Selenium\nRC protocol is derived from the table-based API offered by Selenium Core, and this means that the\nentire API can be described using three URL query parameters.\nWhen the client starts a new session, the Selenium server looks up the requested \"browser string\"\nto identify a matching browser launcher. The launcher is responsible for configuring and starting\nan instance of the requested browser. In the case of Firefox, this is as simple as expanding a pre-\nbuilt profile with a handful of extensions pre-installed (one for handling a \"quit\" command, and\nanother for modeling \"document.readyState\" which wasn't present on older Firefox releases that\nwe still support). The key piece of configuration that's done is that the server configures itself as a\nproxy for the browser, meaning that at least some requests (those for \"/selenium-server\") are\nrouted through it. Selenium RC can operate in one of three modes: controlling a frame in a single\nwindow (\"singlewindow\" mode), in a separate window controlling the AUT in a second window\n(\"multiwindow\" mode) or by injecting itself into the page via a proxy (\"proxyinjection\" mode).\nDepending on the mode of operation, all requests may be proxied.\nOnce the browser is configured, it is started, with an initial URL pointing to a page hosted on the\nSelenium server\u2014RemoteRunner.html. This page is responsible for bootstrapping the process\nby loading all the required Javascript files for Selenium Core. Once complete, the \"runSeleniumTest\"\nfunction is called. This uses reflection of the Selenium object to initialize the list of available\ncommands that are available before kicking off the main command processing loop.\nThe Javascript executing in the browser opens an XMLHttpRequest to a URL on the waiting server\n(/selenium-server/driver), relying on the fact that the server is proxying all requests to\nensure that the request actually goes somewhere valid. Rather than making a request, the first\nthing that this does is send the response from the previously executed command, or \"OK\" in the\ncase where the browser is just starting up. The server then keeps the request open until a new\ncommand is received from the user's test via the client, which is then sent as the response to the\nwaiting Javascript. This mechanism was originally dubbed \"Response/Request\", but would now be\nmore likely to be called \"Comet with AJAX long polling\".\nWhy does RC work this way? The server needs to be configured as a proxy so that it can\nintercept any requests that are made to it without causing the calling Javascript to fall foul of the\n\"Single Host Origin\" policy, which states that only resources from the same server that the script\nwas served from can be requested via Javascript. This is in place as a security measure, but from\nthe point of view of a browser automation framework developer, it's pretty frustrating and\nrequires a hack such as this.\nThe reason for making an XmlHttpRequest call to the server is two-fold. Firstly, and most\nimportantly, until WebSockets, a part of HTML5, become available in the majority of browsers\n", "page": 220, "type": "text", "section": "Page 220"}
{"text": "there is no way to start up a server process reliably within a browser. That means that the server\nhad to live elsewhere. Secondly, an XMLHttpRequest calls the response callback asynchronously,\nwhich means that while we're waiting for the next command the normal execution of the browser\nis unaffected. The other two ways to wait for the next command would have been to poll the\nserver on a regular basis to see if there was another command to execute, which would have\nintroduced latency to the users tests, or to put the Javascript into a busy loop which would have\npushed CPU usage through the roof and would have prevented other Javascript from executing in\nthe browser (since there is only ever one Javascript thread executing in the context of a single\nwindow).\nInside Selenium Core there are two major moving pieces. These are the main selenium object,\nwhich acts as the host for all available commands and mirrors the API offered to users. The\nsecond piece is the browserbot. This is used by the Selenium object to abstract away the\ndifferences present in each browser and to present an idealized view of commonly used browser\nfunctionality. This means that the functions in selenium are clearer and easier to maintain, whilst\nthe browserbot is tightly focused.\nIncreasingly, Core is being converted to make use of the Automation Atoms. Both selenium and\nbrowserbot will probably need to remain as there is an extensive amount of code that relies on\nusing the APIs it exposes, but it is expected that they will ultimately be shell classes, delegating to\nthe atoms as quickly as possible.\n16.9. Looking Back\nBuilding a browser automation framework is a lot like painting a room; at first glance, it looks like\nsomething that should be pretty easy to do. All it takes is a few coats of paint, and the job's done.\nThe problem is, the closer you get, the more tasks and details emerge, and the longer the task\nbecomes. With a room, it's things like working around light fittings, radiators and the skirting\nboards that start to consume time. For a browser automation framework, it's the quirks and\ndiffering capabilities of browsers that make the situation more complex. The extreme case of this\nwas expressed by Daniel Wagner-Hall as he sat next to me working on the Chrome driver; he\nbanged his hands on the desk and in frustration muttered, \"It's all edge cases!\" It would be nice to\nbe able to go back and tell myself that, and that the project is going to take a lot longer than I\nexpected.\nI also can't help but wonder where the project would be if we'd identified and acted upon the need\nfor a layer like the automation atoms sooner than we did. It would certainly have made some of\nthe challenges the project faced, internal and external, technically and socially, easier to deal with.\nCore and RC were implemented in a focused set of languages\u2014essentially just Javascript and Java.\nJason Huggins used to refer to this as providing Selenium with a level of \"hackability\", which made\nit easy for people to get involved with the project. It's only with the atoms that this level of\nhackability has become widely available in WebDriver. Balanced against this, the reason why the\natoms can be so widely applied is because of the Closure compiler, which we adopted almost as\nsoon as it was released as Open Source.\nIt's also interesting to reflect on the things that we got right. The decision to write the framework\nfrom the viewpoint of the user is something that I still feel is correct. Initially, this paid off as early\nadopters highlighted areas for improvement, allowing the utility of the tool to increase rapidly.\nLater, as WebDriver gets asked to do more and harder things and the number of developers\nusing it increases, it means that new APIs are added with care and attention, keeping the focus of\nthe project tight. Given the scope of what we're trying to do, this focus is vital.\nBinding tightly to the browser is something that is both right and wrong. It's right, as it has\nallowed us to emulate the user with extreme fidelity, and to control the browser extremely well. It's\nwrong because this approach is extremely technically demanding, particularly when finding the\nnecessary hook point into the browser. The constant evolution of the IE driver is a demonstration\nof this in action, and, although it's not covered here, the same is true of the Chrome driver, which\nhas a long and storied history. At some point, we'll need to find a way to deal with this complexity.\n16.10. Looking to the Future\nThere will always be browsers that WebDriver can't integrate tightly to, so there will always be a\nneed for Selenium Core. Migrating this from its current traditional design to a more modular design\nbased on the same Closure Library that the atoms are using is underway. We also expect to\nembed the atoms more deeply within the existing WebDriver implementations.\nOne of the initial goals of WebDriver was to act as a building block for other APIs and tools. Of\ncourse, Selenium doesn't live in a vacuum: there are plenty of other Open Source browser\n", "page": 221, "type": "text", "section": "Page 221"}
{"text": "automation tools. One of these is Watir (Web Application Testing In Ruby), and work has begun,\nas a joint effort by the Selenium and Watir developers, to place the Watir API over the WebDriver\ncore. We're keen to work with other projects too, as successfully driving all the browsers out\nthere is hard work. It would be nice to have a solid kernel that others could build on. Our hope is\nthat the kernel is WebDriver.\nA glimpse of this future is offered by Opera Software, who have independently implemented the\nWebDriver API, using the WebDriver test suites to verify the behavior of their code, and who will\nbe releasing their own OperaDriver. Members of the Selenium team are also working with\nmembers of the Chromium team to add better hooks and support for WebDriver to that browser,\nand by extension to Chrome too. We have a friendly relationship with Mozilla, who have\ncontributed code for the FirefoxDriver, and with the developers of the popular HtmlUnit Java\nbrowser emulator.\nOne view of the future sees this trend continue, with automation hooks being exposed in a\nuniform way across many different browsers. The advantages for people keen to write tests for\nweb applications are clear, and the advantages for browser manufacturers are also obvious. For\nexample, given the relative expense of manual testing, many large projects rely heavily on\nautomated testing. If it's not possible, or even if it's \"only\" extremely taxing, to test with a\nparticular browser, then tests just aren't run for it, with knock-on effects for how well complex\napplications work with that browser. Whether those automation hooks are going to be based on\nWebDriver is an open question, but we can hope!\nThe next few years are going to be very interesting. As we're an open source project, you'd be\nwelcome to join us for the journey at http://selenium.googlecode.com/.\nFootnotes\n1. http://fit.c2.com\n2. This is very similar to FIT, and James Shore, one of that project's coordinators, helps explain\nsome of the drawbacks at http://jamesshore.com/Blog/The-Problems-With-\nAcceptance-Testing.html.\n3. For example, the remote server returns a base64-encoded screen grab with every exception\nas a debugging aid but the Firefox driver doesn't.\n4. I.e., always returns the same result.\n", "page": 222, "type": "text", "section": "Page 222"}
{"text": "The Architecture of\nOpen Source Applications\nAmy Brown and Greg Wilson (eds.)\nISBN 978-1-257-63801-7 \nLicense / Buy / Contribute\nChapter 17. Sendmail\nEric Allman\nMost people think of electronic mail as the program that they interact with\u2014their mail client,\ntechnically known as a Mail User Agent (MUA). But another important part of electronic mail is the\nsoftware that actually transfers the mail from the sender to the recipient\u2014the Mail Transfer Agent\n(MTA). The first MTA on the Internet, and still the most prevalent, was sendmail.\nSendmail was first created before the Internet officially existed. It has been extraordinarily\nsuccessful, having grown from 1981, when it wasn't at all obvious that the Internet was going to\nbe more than an academic experiment with only a few hundred hosts, to today, with over 800\nmillion Internet hosts as of January 20111. Sendmail remains among the most used\nimplementations of SMTP on the Internet.\n17.1. Once Upon a Time\u2026\nThe first versions of the program that would become known as sendmail were written in 1980. It\nstarted as a quick hack to forward messages between different networks. The Internet was being\ndeveloped but was not functional at that time. In fact, many different networks had been\nproposed with no obvious consensus emerging. The Arpanet was in use in the United States and\nthe Internet was being designed as an upgrade, but Europe had thrown its weight behind the OSI\n(Open Systems Interconnect) effort, and for a while it appeared that OSI might triumph. Both of\nthese used leased lines from the phone companies; in the US that speed was 56 Kbps.\nProbably the most successful network of the time, in terms of numbers of computers and people\nconnected, was the UUCP network, which was unusual in that it had absolutely no central\nauthority. It was, in some sense, the original peer-to-peer network, which ran over dialup phone\nlines: 9600 bps was about the fastest available for some time. The fastest network (at 3 Mbps)\nwas based on the Ethernet from Xerox, which ran a protocol called XNS (Xerox Network Systems)\n\u2014but it didn't work outside of a local installation.\nThe environment of the time was rather different than what exists today. Computers were highly\nheterogeneous, to the extent that there wasn't even complete agreement to use 8-bit bytes. For\nexample, other machines included the PDP-10 (36 bit words, 9 bit bytes), the PDP-11 (16 bit\nwords, 8 bit bytes), the CDC 6000 series (60 bit words, 6 bit characters), the IBM 360 (32 bit\nwords, 8 bit bytes), the XDS 940, the ICL 470, and the Sigma 7. One of the up-and-coming\nplatforms was Unix, which at that time came from Bell Laboratories. Most Unix-based machines\nhad 16-bit addresses spaces: at that time the PDP-11 was the major Unix machine, with the Data\nGeneral 8/32 and the VAX-11/780 just appearing. Threads didn't exist\u2014in fact, the concept of\ndynamic processes was still fairly new (Unix had them, but \"serious\" systems such as IBM's\nOS/360 did not). File locking was not supported in the Unix kernel (but tricks were possible using\nfilesystem links).\nTo the extent they existed at all, networks were generally low speed (many based on 9600-baud\nTTY lines; the truly rich might have had Ethernet available, but for local use only). The venerable\nsocket interface wasn't going to be invented for many years. Public key encryption hadn't been\ninvented either, so most network security as we know it today wasn't feasible.\nNetwork email already existed on Unix, but it was created using hacks. The primary user agent at\nthe time was the /bin/mail command (today sometimes referred to as binmail or v7mail),\n", "page": 223, "type": "text", "section": "Page 223"}
{"text": "but some sites had other user agents such as Mail from Berkeley, which actually understood\nhow to treat messages as individual items rather than being a glorified cat program. Every user\nagent read (and usually wrote!) /usr/spool/mail directly; there was no abstraction for how the\nmessages were actually stored.\nThe logic to route a message to the network versus local e-mail was nothing more than seeing if\nthe address contained an exclamation point (UUCP) or a colon (BerkNET). People with Arpanet\naccess had to use a completely separate mail program, which would not interoperate with other\nnetworks, and which even stored local mail in a different place and in a different format.\nTo make things even more interesting, there was virtually no standardization on the format of the\nmessages themselves. There was general agreement that there would be a block of header fields\nat the top of the message, that each header field would be on a new line, and that header field\nnames and values would be separated by a colon. Beyond that, there was very little\nstandardization in either the selection of header field names or the syntaxes of individual fields. For\nexample, some systems used Subj: instead of Subject:, Date: fields were different syntaxes,\nand some systems didn't understand full names in a From: field. On top of all of this, what was\ndocumented was often ambiguous or not quite what was actually in use. In particular, RFC 733\n(which purported to describe the format of Arpanet messages) was different from what was\nactually used in subtle but sometimes important ways, and the method of actually transmitting\nmessages was not officially documented at all (although several RFCs made reference to the\nmechanism, none defined it). The result was that there was somewhat of a priesthood around\nmessaging systems.\nIn 1979, the INGRES Relational Database Management Project (a.k.a. my day job) got a DARPA\ngrant, and with it a 9600bps Arpanet connection to our PDP-11. At the time it was the only\nArpanet connection available in the Computer Science Division, so everyone wanted access to our\nmachine so they could get to the Arpanet. However, that machine was already maxed out, and so\nwe could only make two login ports available for everyone in the department to share. This caused\nsubstantial contention and frequent conflicts. However, I noticed that what people wanted most of\nall was not remote login or file transfer, but e-mail.\nInto this, sendmail (initially called delivermail) emerged as an attempt to unify the chaos into one\nplace. Every MUA (mail user agent, or mail client) would just call delivermail to deliver email rather\nthan figuring out how to do it on an ad hoc (and often incompatible) basis. Delivermail/sendmail\nmade no attempt to dictate how local mail should be stored or delivered; it did absolutely nothing\nexcept shuffle mail between other programs. (This changed when SMTP was added, as we'll see\nshortly.) In some sense it was just glue to hold the various mail systems together rather than\nbeing a mail system in its own right.\nDuring the development of sendmail the Arpanet was transformed into the Internet. The changes\nwere extensive, from the low level packets on the wire up through application protocols, and did\nnot happen instantly. Sendmail was literally developed concurrently with the standards, and in\nsome cases influenced them. It's also notable that sendmail has survived and even thrived as \"the\nnetwork\" (as we think of it today) scaled from a few hundred hosts to hundreds of millions of\nhosts.\nAnother Network\nIt's worth mentioning that another completely separate mail standard was proposed at the\ntime called X.400, which was a part of ISO/OSI (International Standards\nOrganization/Open Systems Interconnect). X.400 was a binary protocol, with the\nmessage encoded using ASN.1 (Abstract Syntax Notation 1), which is still in use in some\nInternet protocols today such as LDAP. LDAP was in turn a simplification of X.500, which\nwas the directory service used by X.400. Sendmail made no attempt whatsoever to be\ndirectly compatible with X.400, although there were some gateway services extant at the\ntime. Although X.400 was initially adopted by many of the commercial vendors at the time,\nInternet mail and SMTP ended up winning in the marketplace.\n", "page": 224, "type": "text", "section": "Page 224"}
{"text": "17.2. Design Principles\nWhile developing sendmail, I adhered to several design principles. All of these in some sense came\ndown to one thing: do as little as possible. This is in sharp contrast to some of the other efforts of\nthe time that had much broader goals and required much larger implementations.\n17.2.1. Accept that One Programmer Is Finite\nI wrote sendmail as a part-time, unpaid project. It was intended to be a quick way of making\nArpanet mail more accessible to people at U.C. Berkeley. The key was to forward mail between\nexisting networks, all of which were implemented as standalone programs that were unaware that\nmore than one network even existed. Modifying more than a tiny amount of the existing software\nwas infeasible with only one part-time programmer. The design had to minimize the amount of\nexisting code that needed to be modified as well as the amount of new code that needed to be\nwritten. This constraint drove most of the rest of the design principles. As it turned out, in most\ncases they would have been the right thing to do even if there had been a larger team available.\n17.2.2. Don't Redesign User Agents\nA Mail User Agent (MUA) is what most end users think of as the \"mail system\"\u2014it's the program\nthat they use to read, write, and answer mail. It is quite distinct from the Mail Transfer Agent\n(MTA), which routes email from the sender to the receiver. At the time sendmail was written, many\nimplementations at least partly combined these two functions, so they were often developed in\ntandem. Trying to work on both at the same time would have been too much, so Sendmail\ncompletely punted on the user interface problem: the only changes to MUAs were to have them\ninvoke sendmail instead of doing their own routing. In particular, there were already several user\nagents, and people were often quite emotional about how they interacted with mail. Trying to work\non both at the same time would have been too much. This separation of the MUA from the MTA is\naccepted wisdom now, but was far from standard practice at the time.\n17.2.3. Don't Redesign the Local Mail Store\nThe local mail store (where messages would be saved until the recipient came along to read them)\nwas not formally standardized. Some sites liked to store them in a centralized place, such as\n/usr/mail, /var/mail, or /var/spool/mail. Other sites liked to store them in the recipient's\nhome directory (e.g., as a file called .mail). Most sites started each message with a line beginning\n\"From\" followed by a space character (an extraordinarily bad decision, but that was the\nconvention at the time), but sites that were Arpanet-focused usually stored messages separated\nby a line containing four control-A characters. Some sites attempted to lock the mailbox to\nprevent collisions, but they used different locking conventions (file locking primitives were not yet\navailable). In short, the only reasonable thing to do was treat local mail storage as a black box.\nOn nearly all sites, the actual mechanism for doing local mailbox storage was embodied in the\n/bin/mail program. This had a (quite primitive) user interface, routing, and storage built into one\nprogram. To incorporate sendmail, the routing portion was pulled out and replaced with a call to\nsendmail. A -d flag was added to force final delivery, i.e., it prevented /bin/mail from calling\nsendmail to do the routing. In later years the code used to deliver a message to a physical mailbox\nwas extracted into another program called mail.local. The /bin/mail program exists today\nonly to include a lowest common denominator for scripts to send mail.\n17.2.4. Make Sendmail Adapt to the World, Not the Other Way Around\nProtocols such as UUCP and BerkNET were already implemented as separate programs that had\ntheir own, sometimes quirky, command line structure. In some cases they were being actively\ndeveloped at the same time as sendmail. It was clear that reimplementing them (for example, to\nconvert them to standard calling conventions) was going to be painful. This led directly to the\nprinciple that sendmail should adapt to the rest of the world rather than trying to make the rest of\nthe world adapt to sendmail.\n", "page": 225, "type": "text", "section": "Page 225"}
{"text": "17.2.5. Change as Little as Possible\nTo the fullest extent possible, during the development of sendmail I didn't touch anything I didn't\nabsolutely have to touch. Besides just not having enough time to do it, there was a culture at\nBerkeley at the time that eschewed most formal code ownership in favor of a policy of \"the last\nperson who touched the code is the go-to person for that program\" (or more simply, \"you touch\nit, you own it\"). Although that sounds chaotic by most modern-day standards, it worked quite well\nin a world where no one at Berkeley was assigned full time to work on Unix; individuals worked on\nparts of the system that they were interested in and committed to and didn't touch the rest of the\ncode base except in dire circumstances.\n17.2.6. Think About Reliability Early\nThe mail system prior to sendmail (including most of the transport systems) wasn't terribly\nconcerned about reliability. For example, versions of Unix prior to 4.2BSD did not have native file\nlocking, although it could be simulated by creating a temporary file and then linking it to a lock file\n(if the lock file already existed the link call would fail). However, sometimes different programs\nwriting the same data file wouldn't agree on how the locking should be done (for example, they\nmight use a different lock file name or even make no attempt to do locking at all), and so it wasn't\nthat uncommon to lose mail. Sendmail took the approach that losing mail wasn't an option\n(possibly a result of my background as a database guy, where losing data is a mortal sin).\n17.2.7. What Was Left Out\nThere were many things that were not done in the early versions. I did not try to re-architect the\nmail system or build a completely general solution: functionality could be added as the need arose.\nVery early versions were not even intended to be completely configurable without access to the\nsource code and a compiler (although this changed fairly early on). In general, the modus\noperandi for sendmail was to get something working quickly and then enhance working code as\nneeded and as the problem was better understood.\n17.3. Development Phases\nLike most long-lived software, sendmail was developed in phases, each with its own basic theme\nand feeling.\n17.3.1. Wave 1: delivermail\nThe first instantiation of sendmail was known as delivermail. It was extremely simple, if not\nsimplistic. Its sole job was to forward mail from one program to another; in particular, it had no\nSMTP support, and so never made any direct network connections. No queuing was necessary\nbecause each network already had its own queue, so the program was really just a crossbar\nswitch. Since delivermail had no direct network protocol support, there was no reason for it to run\nas a daemon\u2014it would be invoked to route each message as it was submitted, pass it to the\nappropriate program that would implement the next hop, and terminate. Also, there was no\nattempt to rewrite headers to match the network to which a message was being delivered. This\ncommonly resulted in messages being forwarded that could not be replied to. The situation was so\nbad that an entire book was written about addressing mail (called, fittingly, !%@:: A Directory of\nElectronic Mail Addressing & Networks [AF94]).\nAll configuration in delivermail was compiled in and was based only on special characters in each\naddress. The characters had precedence. For example, a host configuration might search for an\n\"@\" sign and, if one was found, send the entire address to a designated Arpanet relay host.\nOtherwise, it might search for a colon, and send the message to BerkNET with the designated\nhost and user if it found one, then could check for an exclamation point (\"!\") signalling that the\nmessage should be forwarded to a designated UUCP relay. Otherwise it would attempt local\ndelivery. This configuration might result in the following:\n", "page": 226, "type": "text", "section": "Page 226"}
{"text": "Input\nSent To {net, host, user}\nfoo@bar\n{Arpanet, bar, foo}\nfoo:bar\n{Berknet, foo, bar}\nfoo!bar!baz\n{Uucp, foo, bar!baz}\nfoo!bar@baz {Arpanet, baz, foo!bar}\nNote that address delimiters differed in their associativity, resulting in ambiguities that could only\nbe resolved using heuristics. For example, the last example might reasonably be parsed as {Uucp,\nfoo, bar@baz} at another site.\nThe configuration was compiled in for several reasons: first, with a 16 bit address space and\nlimited memory, parsing a runtime configuration was too expensive. Second, the systems of the\ntime had been so highly customized that recompiling was a good idea, just to make sure you had\nthe local versions of the libraries (shared libraries did not exist with Unix 6th Edition).\nDelivermail was distributed with 4.0 and 4.1 BSD and was more successful than expected;\nBerkeley was far from the only site with hybrid network architectures. It became clear that more\nwork was required.\n17.3.2. Wave 2: sendmail 3, 4, and 5\nVersions 1 and 2 were distributed under the delivermail name. In March 1981 work began on\nversion 3, which would be distributed under the sendmail name. At this point the 16-bit PDP-11\nwas still in common use but the 32-bit VAX-11 was becoming popular, so many of the original\nconstraints associated with small address spaces were starting to be relaxed.\nThe initial goals of sendmail were to convert to runtime configuration, allow message modification\nto provide compatibility across networks for forwarded mail, and have a richer language on which\nto make routing decisions. The technique used was essentially textual rewriting of addresses\n(based on tokens rather than character strings), a mechanism used in some expert systems at\nthe time. There was ad hoc code to extract and save any comment strings (in parentheses) as\nwell as to re-insert them after the programmatic rewriting completed. It was also important to be\nable to add or augment header fields (e.g., adding a Date header field or including the full name of\nthe sender in the From header if it was known).\nSMTP development started in November 1981. The Computer Science Research Group (CSRG) at\nU.C. Berkeley had gotten the DARPA contract to produce a Unix-based platform to support\nDARPA funded research, with the intent of making sharing between projects easier. The initial\nwork on the TCP/IP stack was done by that time, although the details of the socket interface were\nstill changing. Basic application protocols such as Telnet and FTP were done, but SMTP had yet to\nbe implemented. In fact, the SMTP protocol wasn't even finalized at that point; there had been a\nhuge debate about how mail should be sent using a protocol to be creatively named Mail Transfer\nProtocol (MTP). As the debate raged, MTP got more and more complex until in frustration SMTP\n(Simple Mail Transfer Protocol) was drafted more-or-less by fiat (but not officially published until\nAugust 1982). Officially, I was working on the INGRES Relational Database Management System,\nbut since I knew more about the mail system than anyone else around Berkeley at the time, I got\ntalked into implementing SMTP.\nMy initial thought was to create a separate SMTP mailer that would have its own queueing and\ndaemon; that subsystem would attach to sendmail to do the routing. However, several features of\nSMTP made this problematic. For example, the EXPN and VRFY commands required access to the\nparsing, aliasing, and local address verification modules. Also, at the time I thought it was\nimportant that the RCPT command return immediately if the address was unknown, rather than\naccepting the message and then having to send a delivery failure message later. This turns out to\nhave been a prescient decision. Ironically, later MTAs often got this wrong, exacerbating the spam\nbackscatter problem. These issues drove the decision to include SMTP as part of sendmail itself.\nSendmail 3 was distributed with 4.1a and 4.1c BSD (beta versions), sendmail 4 was distributed\nwith 4.2 BSD, and sendmail 5 was distributed with 4.3 BSD.\n", "page": 227, "type": "text", "section": "Page 227"}
{"text": "17.3.3. Wave 3: The Chaos Years\nAfter I left Berkeley and went to a startup company, my time available to work on sendmail rapidly\ndecreased. But the Internet was starting to seriously explode and sendmail was being used in a\nvariety of new (and larger) environments. Most of the Unix system vendors (Sun, DEC, and IBM in\nparticular) created their own versions of sendmail, all of which were mutually incompatible. There\nwere also attempts to build open source versions, notably IDA sendmail and KJS.\nIDA sendmail came from Link\u00f6ping University. IDA included extensions to make it easier to install\nand manage in larger environments and a completely new configuration system. One of the major\nnew features was the inclusion of dbm(3) database maps to support highly dynamic sites. These\nwere available using a new syntax in the configuration file and were used for many functions\nincluding mapping of addresses to and from external syntax (for example, sending out mail as\njohn_doe@example.com instead of johnd@example.com) and routing.\nKing James Sendmail (KJS, produced by Paul Vixie) was an attempt to unify all the various versions\nof sendmail that had sprung up. Unfortunately, it never really got enough traction to have the\ndesired effect. This era was also driven by a plethora of new technologies that were reflected in\nthe mail system. For example, Sun's creation of diskless clusters added the YP (later NIS) directory\nservices and NFS, the Network File System. In particular, YP had to be visible to sendmail, since\naliases were stored in YP rather than in local files.\n17.3.4. Wave 4: sendmail 8\nAfter several years, I returned to Berkeley as a staff member. My job was to manage a group\ninstalling and supporting shared infrastructure for research around the Computer Science\ndepartment. For that to succeed, the largely ad hoc environments of individual research groups\nhad to be unified in some rational way. Much like the early days of the Internet, different research\ngroups were running on radically different platforms, some of which were quite old. In general,\nevery research group ran its own systems, and although some of them were well managed, most\nof them suffered from \"deferred maintenance.\"\nIn most cases email was similarly fractured. Each person's email address was\n\"person@host.berkeley.edu\", where host was the name of the workstation in their office or\nthe shared server they used (the campus didn't even have internal subdomains) with the\nexception of a few special people who had @berkeley.edu addresses. The goal was to switch to\ninternal subdomains (so all individual hosts would be in the cs.berkeley.edu subdomain) and\nhave a unified mail system (so each person would have an @cs.berkeley.edu address). This\ngoal was most easily realized by creating a new version of sendmail that could be used throughout\nthe department.\nI began by studying many of the variants of sendmail that had become popular. My intent was not\nto start from a different code base but rather to understand the functionality that others had\nfound useful. Many of those ideas found their way into sendmail 8, often with modifications to\nmerge related ideas or make them more generic. For example, several versions of sendmail had\nthe ability to access external databases such as dbm(3) or NIS; sendmail 8 merged these into one\n\"map\" mechanism that could handle multiple types of databases (and even arbitrary non-database\ntransformations). Similarly, the \"generics\" database (internal to external name mapping) from IDA\nsendmail was incorporated.\nSendmail 8 also included a new configuration package using the m4(1) macro processor. This was\nintended to be more declarative than the sendmail 5 configuration package, which had been\nlargely procedural. That is, the sendmail 5 configuration package required the administrator to\nessentially lay out the entire configuration file by hand, really only using the \"include\" facility from\nm4 as shorthand. The sendmail 8 configuration file allowed the administrator to just declare what\nfeatures, mailers, and so on were required, and m4 laid out the final configuration file.\nMuch of Section 17.7 discusses the enhancements in sendmail 8.\n", "page": 228, "type": "text", "section": "Page 228"}
{"text": "17.3.5. Wave 5: The Commercial Years\nAs the Internet grew and the number of sendmail sites expanded, support for the ever larger user\nbase became more problematic. For a while I was able to continue support by setting up a group\nof volunteers (informally called the \"Sendmail Consortium\", a.k.a. sendmail.org) who provided free\nsupport via e-mail and newsgroup. But by the late 1990s, the installed base had grown to such an\nextent that it was nearly impossible to support it on a volunteer basis. Together with a more\nbusiness-savvy friend I founded Sendmail, Inc.2, with the expectation of getting new resources to\nbear on the code.\nAlthough the commercial product was originally based largely on configuration and management\ntools, many new features were added to the open-source MTA to support the needs of the\ncommercial world. Notably, the company added support for TLS (connection encryption), SMTP\nAuthentication, site security enhancements such as Denial of Service protection, and most\nimportantly mail filtering plugins (the Milter interface discussed below).\nAt of this writing the commercial product has expanded to include a large suite of e-mail based\napplications, nearly all of which are constructed on the extensions added to sendmail during the\nfirst few years of the company.\n17.3.6. Whatever Happened to sendmail 6 and 7?\nSendmail 6 was essentially the beta for sendmail 8. It was never officially released, but was\ndistributed fairly widely. Sendmail 7 never existed at all; sendmail jumped directly to version 8\nbecause all the other source files for the BSD distribution were bumped to version 8 when 4.4\nBSD was released in June 1993.\n17.4. Design Decisions\nSome design decisions were right. Some started out right and became wrong as the world\nchanged. Some were dubious and haven't become any less so.\n17.4.1. The Syntax of the Configuration File\nThe syntax of the configuration file was driven by a couple of issues. First, the entire application\nhad to fit into a 16-bit address space, so the parser had to be small. Second, early configurations\nwere quite short (under one page), so while the syntax was obscure, the file was still\ncomprehensible. However, as time passed, more operational decisions moved out of the C code\ninto the configuration file, and the file started to grow. The configuration file acquired a reputation\nfor being arcane. One particular frustration for many people was the choice of the tab character\nas an active syntax item. This was a mistake that was copied from other systems of the time,\nnotably make. That particular problem became more acute as window systems (and hence cut-\nand-paste, which usually did not preserve the tabs) became available.\nIn retrospect, as the file got larger and 32-bit machines took over, it would have made sense to\nreconsider the syntax. There was a time when I thought about doing this but decided against it\nbecause I didn't want to break the \"large\" installed base (which at that point was probably a few\nhundred machines). In retrospect this was a mistake; I had simply not appreciated how large the\ninstall base would grow and how many hours it would save me had I changed the syntax early.\nAlso, when the standards stabilized a fair amount of the generality could have been pushed back\ninto the C code base, thus simplifying the configurations.\nOf particular interest was how more functionality got moved into the configuration file. I was\ndeveloping sendmail at the same time as the SMTP standard was evolving. By moving operational\ndecisions into the configuration file I was able to respond rapidly to design changes\u2014usually in\nunder 24 hours. I believe that this improved the SMTP standard, since it was possible to get\noperational experience with a proposed design change quite quickly, but only at the cost of\nmaking the configuration file difficult to understand.\n", "page": 229, "type": "text", "section": "Page 229"}
{"text": "17.4.2. Rewriting Rules\nOne of the difficult decisions when writing sendmail was how to do the necessary rewriting to\nallow forwarding between networks without violating the standards of the receiving network. The\ntransformations required changing metacharacters (for example, BerkNET used colon as a\nseparator, which was not legal in SMTP addresses), rearranging address components, adding or\ndeleting components, etc. For example, the following rewrites would be needed under certain\ncircumstances:\nFrom\nTo\na:foo\na.foo@berkeley.edu\na!b!c\nb!c@a.uucp\n<@a.net,@b.org:user@c.com> <@b.org:user@c.com>\nRegular expressions were not a good choice because they didn't have good support for word\nboundaries, quoting, etc. It quickly became obvious that it would be nearly impossible to write\nregular expressions that were accurate, much less intelligible. In particular, regular expressions\nreserve a number of metacharacters, including \".\", \"*\", \"+\", \"{[}\", and \"{]}\", all of which can\nappear in e-mail addresses. These could have been escaped in configuration files, but I deemed\nthat to be complicated, confusing, and a bit ugly. (This was tried by UPAS from Bell Laboratories,\nthe mailer for Unix Eighth Edition, but it never caught on3.) Instead, a scanning phase was\nnecessary to produce tokens that could then be manipulated much like characters in regular\nexpressions. A single parameter describing \"operator characters\", which were themselves both\ntokens and token separators, was sufficient. Blank spaces separated tokens but were not tokens\nthemselves. The rewriting rules were just pattern match/replace pairs organized into what were\nessentially subroutines.\nInstead of a large number of metacharacters that had to be escaped to lose their \"magic\"\nproperties (as used in regular expressions), I used a single \"escape\" character that combined with\nordinary characters to represent wildcard patterns (to match an arbitrary word, for example). The\ntraditional Unix approach would be to use backslash, but backslash was already used as a quote\ncharacter in some address syntaxes. As it turned out, \"$\" was one of the few characters that had\nnot already been used as a punctuation character in some email syntax.\nOne of the original bad decisions was, ironically, just a matter of how white space was used. A\nspace character was a separator, just as in most scanned input, and so could have been used\nfreely between tokens in patterns. However, the original configuration files distributed did not\ninclude spaces, resulting in patterns that were far harder to understand than necessary. Consider\nthe difference between the following two (semantically identical) patterns:\n$+ + $* @ $+ . $={mydomain}\n$++$*@$+.$={mydomain}\n17.4.3. Using Rewriting for Parsing\nSome have suggested that sendmail should have used conventional grammar-based parsing\ntechniques to parse addresses rather than rewriting rules and leave the rewriting rules for\naddress modification. On the surface this would seem to make sense, given that the standards\ndefine addresses using a grammar. The main reason for reusing rewriting rules is that in some\ncases it was necessary to parse header field addresses (e.g., in order to extract the sender\nenvelope from a header when receiving mail from a network that didn't have a formal envelope).\nSuch addresses aren't easy to parse using (say) an LALR(1) parser such as YACC and a traditional\nscanner because of the amount of lookahead required. For example, parsing the address:\nallman@foo.bar.baz.com <eric@example.com> requires lookahead by either the scanner or\nthe parser; you can't know that the initial \"allman@\u2026\" is not an address until you see the \"<\".\nSince LALR(1) parsers only have one token of lookahead this would have had to be done in the\nscanner, which would have complicated it substantially. Since the rewriting rules already had\narbitrary backtracking (i.e., they could look ahead arbitrarily far), they were sufficient.\n", "page": 230, "type": "text", "section": "Page 230"}
{"text": "A secondary reason was that it was relatively easy to make the patterns recognize and fix broken\ninput. Finally, rewriting was more than powerful enough to do the job, and reusing any code was\nwise.\nOne unusual point about the rewriting rules: when doing the pattern matching, it is useful for both\nthe input and the pattern to be tokenized. Hence, the same scanner is used for both the input\naddresses and the patterns themselves. This requires that the scanner be called with different\ncharacter type tables for differing input.\n17.4.4. Embedding SMTP and Queueing in sendmail\nAn \"obvious\" way to implement outgoing (client) SMTP would have been to build it as an external\nmailer, similarly to UUCP, But this would raise a number of other questions. For example, would\nqueueing be done in sendmail or in the SMTP client module? If it was done in sendmail then either\nseparate copies of messages would have to be sent to each recipient (i.e., no \"piggybacking\",\nwherein a single connection can be opened and then multiple RCPT commands can be sent) or a\nmuch richer communication back-path would be necessary to convey the necessary per-recipient\nstatus than was possible using simple Unix exit codes. If queueing was done in the client module\nthen there was a potential for large amounts of replication; in particular, at the time other\nnetworks such as XNS were still possible contenders. Additionally, including the queue into\nsendmail itself provided a more elegant way of dealing with certain kinds of failures, notably\ntransient problems such as resource exhaustion.\nIncoming (server) SMTP involved a different set of decisions. At the time, I felt it was important to\nimplement the VRFY and EXPN SMTP commands faithfully, which required access to the alias\nmechanism. This would once again require a much richer protocol exchange between the server\nSMTP module and sendmail than was possible using command lines and exit codes\u2014in fact, a\nprotocol akin to SMTP itself.\nI would be much more inclined today to leave queueing in the core sendmail but move both sides\nof the SMTP implementation into other processes. One reason is to gain security: once the server\nside has an open instance of port 25 it no longer needs access to root permissions. Modern\nextensions such as TLS and DKIM signing complicate the client side (since the private keys should\nnot be accessible to unprivileged users), but strictly speaking root access is still not necessary.\nAlthough the security issue is still an issue here, if the client SMTP is running as a non-root user\nwho can read the private keys, that user by definition has special privileges, and hence should not\nbe communicating directly with other sites. All of these issues can be finessed with a bit of work.\n17.4.5. The Implementation of the Queue\nSendmail followed the conventions of the time for storing queue files. In fact, the format used is\nextremely similar to the lpr subsystem of the time. Each job had two files, one with the control\ninformation and one with the data. The control file was a flat text file with the first character of\neach line representing the meaning of that line.\nWhen sendmail wanted to process the queue it had to read all of the control files, storing the\nrelevant information in memory, and then sort the list. That worked fine with a relatively small\nnumber of messages in the queue, but started to break down at around 10,000 queued\nmessages. Specifically, when the directory got large enough to require indirect blocks in the\nfilesystem, there was a serious performance knee that could reduce performance by as much as\nan order of magnitude. It was possible to ameliorate this problem by having sendmail understand\nmultiple queue directories, but that was at best a hack.\nAn alternative implementation might be to store all the control files in one database file. This wasn't\ndone because when sendmail coding began there was no generally available database package,\nand when dbm(3) became available it had several flaws, including the inability to reclaim space, a\nrequirement that all keys that hashed together fit on one (512 byte) page, and a lack of locking.\nRobust database packages didn't appear for many years.\nAnother alternative implementation would have been to have a separate daemon that would keep\n", "page": 231, "type": "text", "section": "Page 231"}
{"text": "the state of the queue in memory, probably writing a log to allow recovery. Given the relatively low\nemail traffic volumes of the time, the lack of memory on most machines, the relatively high cost of\nbackground processes, and the complexity of implementing such a process, this didn't seem like a\ngood tradeoff at the time.\nAnother design decision was to store the message header in the queue control file rather than the\ndata file. The rationale was that most headers needed considerable rewriting that varied from\ndestination to destination (and since messages could have more than one destination, they would\nhave to be customized multiple times), and the cost of parsing the headers seemed high, so\nstoring them in a pre-parsed format seemed like a savings. In retrospect this was not a good\ndecision, as was storing the message body in Unix-standard format (with newline endings) rather\nthan in the format in which it was received (which could use newlines, carriage-return/line-feed,\nbare carriage-return, or line-feed/carriage-return). As the e-mail world evolved and standards were\nadopted, the need for rewriting diminished, and even seemingly innocuous rewriting has the risk\nof error.\n17.4.6. Accepting and Fixing Bogus Input\nSince sendmail was created in a world of multiple protocols and disturbingly few written standards,\nI decided to clean up malformed messages wherever possible. This matches the \"Robustness\nPrinciple\" (a.k.a. Postel's Law) articulated in RFC 7934. Some of these changes were obvious and\neven required: when sending a UUCP message to the Arpanet, the UUCP addresses needed to be\nconverted to Arpanet addresses, if only to allow \"reply\" commands to work correctly, line\nterminations needed to be converted between the conventions used by various platforms, and so\non. Some were less obvious: if a message was received that did not include a From: header field\nrequired in the Internet specifications, should you add a From: header field, pass the message on\nwithout the From: header field, or reject the message? At the time, my prime consideration was\ninteroperability, so sendmail patched the message, e.g., by adding the From: header field.\nHowever, this is claimed to have allowed other broken mail systems to be perpetuated long past\nthe time when they should have been fixed or killed off.\nI believe my decision was correct for the time, but is problematic today. A high degree of\ninteroperability was important to let mail flow unimpeded. Had I rejected malformed messages,\nmost messages at the time would have been rejected. Had I passed them through unfixed,\nrecipients would have received messages that they couldn't reply to and in some cases couldn't\neven determine who sent the message\u2014that or the message would have been rejected by\nanother mailer.\nToday the standards are written, and for the most part those standards are accurate and\ncomplete. It is no longer the case that most messages would be rejected, and yet there is still mail\nsoftware out there that send out mangled messages. This unnecessarily creates numerous\nproblems for other software on the Internet.\n17.4.7. Configuration and the Use of M4\nFor a period I was both making regular changes to the sendmail configuration files and personally\nsupporting many machines. Since a large amount of the configuration file was the same between\ndifferent machines, the use of a tool to build the configuration files was desirable. The m4 macro\nprocessor was included with Unix. It was designed as a front end for programming languages\n(notably ratfor). Most importantly, it had \"include\" capabilities, like \"#include\" in the C language. The\noriginal configuration files used little more than this capability and some minor macro expansions.\nIDA sendmail also used m4, but in a dramatically different way. In retrospect I should have\nprobably studied these prototypes in more detail. They contained many clever ideas, in particular\nthe way they handled quoting.\nStarting with sendmail 6, the m4 configuration files were completely rewritten to be in a more\ndeclarative style and much smaller. This used considerably more of the power of the m4\nprocessor, which was problematic when the introduction of GNU m4 changed some of the\n", "page": 232, "type": "text", "section": "Page 232"}
{"text": "semantics in subtle ways.\nThe original plan was that the m4 configurations would follow the 80/20 rule: they would be simple\n(hence 20% of the work), and would cover 80% of the cases. This broke down fairly quickly, for\ntwo reasons. The minor reason was that it turned out to be relatively easy to handle the vast\nmajority of the cases, at least in the beginning. It became much harder as sendmail and the world\nevolved, notably with the inclusion of features such as TLS encryption and SMTP Authentication,\nbut those didn't come until quite a bit later.\nThe important reason was that it was becoming clear that the raw configuration file was just too\ndifficult for most people to manage. In essence, the .cf (raw) format had become assembly code\n\u2014editable in principle, but in reality quite opaque. The \"source code\" was an m4 script stored in the\n.mc file.\nAnother important distinction is that the raw format configuration file was really a programming\nlanguage. It had procedural code (rulesets), subroutine calls, parameter expansion, and loops (but\nno gotos). The syntax was obscure, but in many ways resembled the sed and awk commands, at\nleast conceptually. The m4 format was declarative: although it was possible to drop into the low-\nlevel raw language, in practice these details were hidden from the user.\nIt isn't clear that this decision was correct or incorrect. I felt at the time (and still feel) that with\ncomplex systems it can be useful to implement what amounts to a Domain Specific Language\n(DSL) for building certain portions of that system. However, exposing that DSL to end users as a\nconfiguration methodology essentially converts all attempts to configure a system into a\nprogramming problem. Great power results from this, but at a non-trivial cost.\n17.5. Other Considerations\nSeveral other architectural and development points deserve to be mentioned.\n17.5.1. A Word About Optimizing Internet Scale Systems\nIn most network-based systems there is a tension between the client and the server. A good\nstrategy for the client may be the wrong thing for the server and vice versa. For example, when\npossible the server would like to minimize its processing costs by pushing as much as possible\nback to the client, and of course the client feels the same way but in the opposite direction. For\nexample, a server might want to keep a connection open while doing spam processing since that\nlowers the cost of rejecting a message (which these days is the common case), but the client\nwants to move on as quickly as possible. Looking at the entire system, that is, the Internet as a\nwhole, the optimum solution may be to balance these two needs.\nThere have been cases of MTAs that have used strategies that explicitly favor either the client or\nthe server. They can do this only because they have a relatively small installed base. When your\nsystem is used on a significant portion of the Internet you have to design it in order to balance the\nload between both sides in an attempt to optimize the Internet as a whole. This is complicated by\nthe fact that there will always be MTAs completely skewed in one direction or the other\u2014for\nexample, mass mailing systems only care about optimizing the outgoing side.\nWhen designing a system that incorporates both sides of the connection, it is important to avoid\nplaying favorites. Note that this is in stark contrast to the usual asymmetry of clients and services\n\u2014for example, web servers and web clients are generally not developed by the same groups.\n17.5.2. Milter\nOne of the most important additions to sendmail was the milter (mail filter) interface. Milter allows\nfor the use of offboard plugins (i.e., they run in a separate process) for mail processing. These\nwere originally designed for anti-spam processing. The milter protocol runs synchronously with\nthe server SMTP protocol. As each new SMTP command is received from the client, sendmail calls\nthe milters with the information from that command. The milter has the opportunity to accept the\n", "page": 233, "type": "text", "section": "Page 233"}
{"text": "command or send a rejection, which rejects the phase of the protocol appropriate for the SMTP\ncommand. Milters are modeled as callbacks, so as an SMTP command comes in, the appropriate\nmilter subroutine is called. Milters are threaded, with a per-connection context pointer handed in to\neach routine to allow passing state.\nIn theory milters could work as loadable modules in the sendmail address space. We declined to\ndo this for three reasons. First, the security issues were too significant: even if sendmail were\nrunning as a unique non-root user id, that user would have access to all of the state of other\nmessages. Similarly, it was inevitable that some milter authors would try to access internal\nsendmail state.\nSecond, we wanted to create a firewall between sendmail and the milters: if a milter crashed, we\nwanted it to be clear who was at fault, and for mail to (potentially) continue to flow. Third, it was\nmuch easier for a milter author to debug a standalone process than sendmail as a whole.\nIt quickly became clear that the milter was useful for more than anti-spam processing. In fact, the\nmilter.org5 web site lists milters for anti-spam, anti-virus, archiving, content monitoring, logging,\ntraffic shaping, and many other categories, produced by commercial companies and open source\nprojects. The postfix mailer6 has added support for milters using the same interface. Milters have\nproven to be one of sendmail's great successes.\n17.5.3. Release Schedules\nThere is a popular debate between \"release early and often\" and \"release stable systems\" schools\nof thought. Sendmail has used both of these at various times. During times of considerable\nchange I was sometimes doing more than one release a day. My general philosophy was to make\na release after each change. This is similar to providing public access to the source management\nsystem tree. I personally prefer doing releases over providing public source trees, at least in part\nbecause I use source management in what is now considered an unapproved way: for large\nchanges, I will check in non-functioning snapshots while I am writing the code. If the tree is shared\nI will use branches for these snapshots, but in any case they are available for the world to see and\ncan create considerable confusion. Also, creating a release means putting a number on it, which\nmakes it easier to track the changes when going through a bug report. Of course, this requires\nthat releases be easy to generate, which is not always true.\nAs sendmail became used in ever more critical production environments this started to become\nproblematic. It wasn't always easy for others to tell the difference between changes that I wanted\nout there for people to test versus changes that were really intended to be used in the wild.\nLabeling releases as \"alpha\" or \"beta\" alleviates but does not fix the problem. The result was that\nas sendmail matured it moved toward less frequent but larger releases. This became especially\nacute when sendmail got folded into a commercial company which had customers who wanted\nboth the latest and greatest but also only stable versions, and wouldn't accept that the two are\nincompatible.\nThis tension between open source developer needs and commercial product needs will never go\naway. There are many advantages to releasing early and often, notably the potentially huge\naudience of brave (and sometimes foolish) testers who stress the system in ways that you could\nalmost never expect to reproduce in a standard development system. But as a project becomes\nsuccessful it tends to turn into a product (even if that product is open source and free), and\nproducts have different needs than projects.\n17.6. Security\nSendmail has had a tumultuous life, security-wise. Some of this is well deserved, but some not, as\nour concept of \"security\" changed beneath us. The Internet started out with a user base of a few\nthousand people, mostly in academic and research settings. It was, in many ways, a kinder,\ngentler Internet than we know today. The network was designed to encourage sharing, not to\nbuild firewalls (another concept that did not exist in the early days). The net is now a dangerous,\nhostile place, filled with spammers and crackers. Increasingly it is being described as a war zone,\n", "page": 234, "type": "text", "section": "Page 234"}
{"text": "and in war zones there are civilian casualties.\nIt's hard to write network servers securely, especially when the protocol is anything beyond the\nmost simple. Nearly all programs have had at least minor problems; even common TCP/IP\nimplementations have been successfully attacked. Higher-level implementation languages have\nproved no panacea, and have even created vulnerabilities of their own. The necessary watch\nphrase is \"distrust all input,\" no matter where it comes from. Distrusting input includes secondary\ninput, for example, from DNS servers and milters. Like most early network software, sendmail was\nfar too trusting in its early versions.\nBut the biggest problem with sendmail was that early versions ran with root permissions. Root\npermission is needed in order to open the SMTP listening socket, to read individual users'\nforwarding information, and to deliver to individual users' mailboxes and home directories.\nHowever, on most systems today the concept of a mailbox name has been divorced from the\nconcept of a system user, which effectively eliminates the need for root access except to open\nthe SMTP listening socket. Today sendmail has the ability to give up root permissions before it\nprocesses a connection, eliminating this concern for environments that can support it. It's worth\nnoting that on those systems that do not deliver directly to users' mailboxes, sendmail can also\nrun in a chrooted environment, allowing further permission isolation.\nUnfortunately, as sendmail gained a reputation for poor security, it started to be blamed for\nproblems that had nothing to do with sendmail. For example, one system administrator made his\n/etc directory world writable and then blamed sendmail when someone replaced the\n/etc/passwd file. It was incidents like this that caused us to tighten security substantially,\nincluding explicitly checking the ownerships and modes on files and directories that sendmail\naccesses. These were so draconian that we were obliged to include the DontBlameSendmail\noption to (selectively) turn off these checks.\nThere are other aspects of security that are not related to protecting the address space of the\nprogram itself. For example, the rise of spam also caused a rise in address harvesting. The VRFY\nand EXPN commands in SMTP were designed specifically to validate individual addresses and\nexpand the contents of mailing lists respectively. These have been so badly abused by spammers\nthat most sites now turn them off entirely. This is unfortunate, at least with VRFY, as this\ncommand was sometimes used by some anti-spam agents to validate the purported sending\naddress.\nSimilarly, anti-virus protection was once seen as a desktop problem, but rose in importance to the\npoint where any commercial-grade MTA had to have anti-virus checking available. Other security-\nrelated requirements in modern settings include mandatory encryption of sensitive data, data loss\nprotection, and enforcement of regulatory requirements, for example, for HIPPA.\nOne of the principles that sendmail took to heart early on was reliability\u2014every message should\neither be delivered or reported back to the sender. But the problem of joe-jobs (attackers forging\nthe return address on a message, viewed by many as a security issue) has caused many sites to\nturn off the creation of bounce messages. If a failure can be determined while the SMTP\nconnection is still open, the server can report the problem by failing the command, but after the\nSMTP connection is closed an incorrectly addressed message will silently disappear. To be fair,\nmost legitimate mail today is single hop, so problems will be reported, but at least in principle the\nworld has decided that security wins over reliability.\n17.7. Evolution of Sendmail\nSoftware doesn't survive in a rapidly changing environment without evolving to fit the changing\nenvironment. New hardware technologies appear, which push changes in the operating system,\nwhich push changes in libraries and frameworks, which push changes in applications. If an\napplication succeeds, it gets used in ever more problematic environments. Change is inevitable; to\nsucceed you have to accept and embrace change. This section describes some of the more\nimportant changes that have occurred as sendmail evolved.\n", "page": 235, "type": "text", "section": "Page 235"}
{"text": "17.7.1. Configuration Became More Verbose\nThe original configuration of sendmail was quite terse. For example, the names of options and\nmacros were all single characters. There were three reasons for this. First, it made parsing very\nsimple (important in a 16-bit environment). Second, there weren't very many options, so it wasn't\nhard to come up with mnemonic names. Third, the single character convention was already\nestablished with command-line flags.\nSimilarly, rewriting rulesets were originally numbered instead of named. This was perhaps tolerable\nwith a small number of rulesets, but as their number grew it became important that they have\nmore mnemonic names.\nAs the environment in which sendmail operated became more complex, and as the 16-bit\nenvironment faded away, the need for a richer configuration language became evident.\nFortunately, it was possible to make these changes in a backward compatible way. These changes\ndramatically improved the understandability of the configuration file.\n17.7.2. More Connections with Other Subsystems: Greater Integration\nWhen sendmail was written the mail system was largely isolated from the rest of the operating\nsystem. There were a few services that required integration, e.g., the /etc/passwd and\n/etc/hosts files. Service switches had not been invented, directory services were nonexistent,\nand configuration was small and hand-maintained.\nThat quickly changed. One of the first additions was DNS. Although the system host lookup\nabstraction (gethostbyname) worked for looking up IP addresses, email had to use other queries\nsuch as MX. Later, IDA sendmail included an external database lookup functionality using dbm(3)\nfiles. Sendmail 8 updated that to a general mapping service that allowed other database types,\nincluding external databases and internal transformations that could not be done using rewriting\n(e.g., dequoting an address).\nToday, the email system relies on many external services that are, in general, not designed\nspecifically for the exclusive use of email. This has moved sendmail toward more abstractions in\nthe code. It has also made maintaining the mail system more difficult as more \"moving parts\" are\nadded.\n17.7.3. Adaptation to a Hostile World\nSendmail was developed in a world that seems completely foreign by today's standards. The user\npopulation on the early network were mostly researchers who were relatively benign, despite the\nsometimes vicious academic politics. Sendmail reflected the world in which it was created, putting\na lot of emphasis on getting the mail through as reliably as possible, even in the face of user\nerrors.\nToday's world is much more hostile. The vast majority of email is malicious. The goal of an MTA has\ntransitioned from getting the mail through to keeping the bad mail out. Filtering is probably the\nfirst priority for any MTA today. This required a number of changes in sendmail.\nFor example, many rulesets have been added to allow checking of parameters on incoming SMTP\ncommands in order to catch problems as early as possible. It is much cheaper to reject a\nmessage when reading the envelope than after you have committed to reading the entire\nmessage, and even more expensive after you have accepted the message for delivery. In the early\ndays filtering was generally done by accepting the message, passing it to a filter program, and\nthen sending it to another instance of sendmail if the message passed (the so-called \"sandwich\"\nconfiguration). This is just far too expensive in today's world.\nSimilarly, sendmail has gone from being a quite vanilla consumer of TCP/IP connections to being\nmuch more sophisticated, doing things like \"peeking\" at network input to see if the sender is\ntransmitting commands before the previous command has been acknowledged. This breaks down\nsome of the previous abstractions that were designed to make sendmail adaptable to multiple\n", "page": 236, "type": "text", "section": "Page 236"}
{"text": "network types. Today, it would involve considerable work to connect sendmail to an XNS or\nDECnet network, for example, since the knowledge of TCP/IP has been built into so much of the\ncode.\nMany configuration features were added to address the hostile world, such as support for access\ntables, Realtime Blackhole Lists, address harvesting mitigation, denial-of-service protection, and\nspam filtering. This has dramatically complicated the task of configuring a mail system, but was\nabsolutely necessary to adapt to today's world.\n17.7.4. Incorporation of New Technologies\nMany new standards have come along over the years that required significant changes to\nsendmail. For example, the addition of TLS (encryption) required significant changes through\nmuch of the code. SMTP pipelining required peering into the low-level TCP/IP stream to avoid\ndeadlocks. The addition of the submission port (587) required the ability to listen to multiple\nincoming ports, including having different behaviors depending on the arrival port.\nOther pressures were forced by circumstances rather than standards. For example, the addition\nof the milter interface was a direct response to spam. Although milter was not a published\nstandard, it was a major new technology.\nIn all cases, these changes enhanced the mail system in some way, be it increased security, better\nperformance, or new functionality. However, they all came with costs, in nearly all cases\ncomplicating both the code base and the configuration file.\n17.8. What If I Did It Today?\nHindsight is 20/20. There are many things I would do differently today. Some were unforeseeable\nat the time (e.g., how spam would change our perception of e-mail, what modern toolsets would\nlook like, etc.), and some were eminently predictable. Some were just that in the process of writing\nsendmail I learned a lot about e-mail, about TCP/IP, and about programming itself\u2014everyone\ngrows as they code.\nBut there are also many things I would do the same, some in contradiction to the standard\nwisdom.\n17.8.1. Things I Would Do Differently\nPerhaps my biggest mistake with sendmail was to not recognize early enough how important it\nwas going to be. I had several opportunities to nudge the world in the correct direction but didn't\ntake them; in fact, in some cases I did damage, e.g., by not making sendmail stricter about bad\ninput when it became appropriate to do so. Similarly, I recognized that the configuration file syntax\nneeded to be improved fairly early on, when there were perhaps a few hundred sendmail\ninstances deployed, but decided not to change things because I didn't want to cause the installed\nuser base undue pain. In retrospect it would have been better to improve things early and cause\ntemporary pain in order to produce a better long-term result.\nVersion 7 Mailbox Syntax\nOne example of this was the way version 7 mailboxes separated messages. They used a line\nbeginning \"From\u2423\" (where \"\u2423\" represents the ASCII space character, 0x20) to separate messages.\nIf a message came in containing the word \"From\u2423\" at the beginning of the line, local mailbox\nsoftware converted it to \">From\u2423\". One refinement on some but not all systems was to require a\npreceding blank line, but this could not be relied upon. To this day, \">From\" appears in extremely\nunexpected places that aren't obviously related to email (but clearly were processed by email at\none time or another). In retrospect I probably could have converted the BSD mail system to use a\nnew syntax. I would have been roundly cursed at the time, but I would have saved the world a\nheap of trouble.\n", "page": 237, "type": "text", "section": "Page 237"}
{"text": "Syntax and Contents of Configuration File\nPerhaps my biggest mistake in the syntax of the configuration file was the use of tab (HT, 0x09) in\nrewriting rules to separate the pattern from the replacement. At the time I was emulating make,\nonly to learn years later that Stuart Feldman, the author of make, thought that was one of his\nbiggest mistakes. Besides being non-obvious when looking at the configuration on a screen, the\ntab character doesn't survive cut-and-paste in most window systems.\nAlthough I believe that rewriting rules were the correct idea (see below), I would change the\ngeneral structure of the configuration file. For example, I did not anticipate the need for\nhierarchies in the configuration (e.g., options that would be set differently for different SMTP\nlistener ports). At the time the configuration file was designed there were no \"standard\" formats.\nToday, I would be inclined to use an Apache-style configuration\u2014it's clean, neat, and has adequate\nexpressive power\u2014or perhaps even embed a language such as Lua.\nWhen sendmail was developed the address spaces were small and the protocols were still in flux.\nPutting as much as possible into the configuration file seemed like a good idea. Today, that looks\nlike a mistake: we have plenty of address space (for an MTA) and the standards are fairly static.\nFurthermore, part of the \"configuration file\" is really code that needs to be updated in new\nreleases. The .mc configuration file fixes that, but having to rebuild your configuration every time\nyou update the software is a pain. A simple solution to this would simply be to have two\nconfiguration files that sendmail would read, one hidden and installed with each new software\nrelease and the other exposed and used for local configuration.\nUse of Tools\nThere are many new tools available today\u2014for example, for configuring and building the software.\nTools can be good leverage if you need them, but they can also be overkill, making it harder than\nnecessary to understand the system. For example, you should never use a yacc(1) grammar\nwhen all you need is strtok(3). But reinventing the wheel isn't a good idea either. In particular,\ndespite some reservations I would almost certainly use autoconf today.\nBackward Compatibility\nWith the benefit of hindsight, and knowing how ubiquitous sendmail became, I would not worry so\nmuch about breaking existing installations in the early days of development. When existing\npractice is seriously broken it should be fixed, not accommodated for. That said, I would still not\ndo strict checking of all message formats; some problems can be easily and safely ignored or\npatched. For example, I would probably still insert a Message-Id: header field into messages that\ndid not have one, but I would be more inclined to reject messages without a From: header field\nrather than try to create one from the information in the envelope.\nInternal Abstractions\nThere are certain internal abstractions that I would not attempt again, and others that I would add.\nFor example, I would not use null-terminated strings, opting instead for a length/value pair, despite\nthe fact that this means that much of the Standard C Library becomes difficult to use. The\nsecurity implications of this alone make it worthwhile. Conversely, I would not attempt to build\nexception handling in C, but I would create a consistent status code system that would be used\nthroughout the code rather than having routines return null, false, or negative numbers to\nrepresent errors.\nI would certainly abstract the concept of mailbox names from Unix user ids. At the time I wrote\nsendmail the model was that you only sent messages to Unix users. Today, that is almost never\nthe case; even on systems that do use that model, there are system accounts that should never\nreceive e-mail.\n17.8.2. Things I Would Do The Same\n", "page": 238, "type": "text", "section": "Page 238"}
{"text": "Of course, some things did work well\u2026\nSyslog\nOne of the successful side projects from sendmail was syslog. At the time sendmail was written,\nprograms that needed to log had a specific file that they would write. These were scattered\naround the filesystem. Syslog was difficult to write at the time (UDP didn't exist yet, so I used\nsomething called mpx files), but well worth it. However, I would make one specific change: I would\npay more attention to making the syntax of logged messages machine parseable\u2014essentially, I\nfailed to predict the existence of log monitoring.\nRewriting Rules\nRewriting rules have been much maligned, but I would use them again (although probably not for\nas many things as they are used for now). Using the tab character was a clear mistake, but given\nthe limitations of ASCII and the syntax of e-mail addresses, some escape character is probably\nrequired7. In general, the concept of using a pattern-replace paradigm worked well and was very\nflexible.\nAvoid Unnecessary Tools\nDespite my comment above that I would use more existing tools, I am reluctant to use many of\nthe run-time libraries available today. In my opinion far too many of them are so bloated as to be\ndangerous. Libraries should be chosen with care, balancing the merits of reuse against the\nproblems of using an overly powerful tool to solve a simple problem. One particular tool I would\navoid is XML, at least as a configuration language. I believe that the syntax is too baroque for\nmuch of what it is used for. XML has its place, but it is overused today.\nCode in C\nSome people have suggested that a more natural implementation language would be Java or C++.\nDespite the well-known problems with C, I would still use it as my implementation language. In part\nthis is personal: I know C much better than I know Java or C++. But I'm also disappointed by the\ncavalier attitude that most object-oriented languages take toward memory allocation. Allocating\nmemory has many performance concerns that can be difficult to characterize. Sendmail uses\nobject-oriented concepts internally where appropriate (for example, the implementation of map\nclasses), but in my opinion going completely object-oriented is wasteful and overly restrictive.\n17.9. Conclusions\nThe sendmail MTA was born into a world of immense upheaval, a sort of \"wild west\" that existed\nwhen e-mail was ad hoc and the current mail standards were not yet formulated. In the\nintervening 31 years the \"e-mail problem\" has changed from just working reliably to working with\nlarge messages and heavy load to protecting sites from spam and viruses and finally today to\nbeing used as a platform for a plethora of e-mail-based applications. Sendmail has evolved into a\nwork-horse that is embraced by even the most risk-averse corporations, even as e-mail has\nevolved from pure text person-to-person communications into a multimedia-based mission-critical\npart of the infrastructure.\nThe reasons for this success are not always obvious. Building a program that survives and even\nthrives in a rapidly changing world with only a handful of part-time developers can't be done using\nconventional software development methodologies. I hope I've provided some insights into how\nsendmail succeeded.\nFootnotes\n", "page": 239, "type": "text", "section": "Page 239"}
{"text": "1. http://ftp.isc.org/www/survey/reports/2011/01/\n2. http://www.sendmail.com\n3. http://doc.cat-v.org/bell_labs/upas_mail_system/upas.pdf\n4. \"Be conservative in what you do, be liberal in what you accept from others\"\n5. http://milter.org\n6. http://postfix.org\n7. Somehow I suspect that using Unicode for configuration would not prove popular.\n", "page": 240, "type": "text", "section": "Page 240"}
{"text": "The Architecture of\nOpen Source Applications\nAmy Brown and Greg Wilson (eds.)\nISBN 978-1-257-63801-7 \nLicense / Buy / Contribute\nChapter 18. SnowFlock\nRoy Bryant and Andr\u00e9s Lagar-Cavilla\nCloud computing provides an attractively affordable computing platform. Instead of buying and\nconfiguring a physical server, with all the associated time, effort and up front costs, users can\nrent \"servers\" in the cloud with a few mouse clicks for less than 10 cents per hour. Cloud\nproviders keep their costs low by providing virtual machines (VMs) instead of physical computers.\nThe key enabler is the virtualization software, called a virtual machine monitor (VMM), that\nemulates a physical machine. Users are safely isolated in their \"guest\" VMs, and are blissfully\nunaware that they typically share the physical machine (\"host\") with many others.\n18.1. Introducing SnowFlock\nClouds are a boon to agile organizations. With physical servers, users are relegated to waiting\nimpatiently while others (slowly) approve the server purchase, place the order, ship the server,\nand install and configure the Operating System (OS) and application stacks. Instead of waiting\nweeks for others to deliver, the cloud user retains control of the process and can create a new,\nstandalone server in minutes.\nUnfortunately, few cloud servers stand alone. Driven by the quick instantiation and pay-per-use\nmodel, cloud servers are typically members of a variable pool of similarly configured servers\nperforming dynamic and scalable tasks related to parallel computing, data mining, or serving web\npages. Because they repeatedly boot new instances from the same, static template, commercial\nclouds fail to fully deliver on the promise of true on-demand computation. After instantiating the\nserver, the cloud user must still manage cluster membership and broker the addition of new\nservers.\nSnowFlock addresses these issues with VM Cloning, our proposed cloud API call. In the same way\nthat application code routinely invokes OS services through a syscall interface, it could now also\ninvoke cloud services through a similar interface. With SnowFlock's VM Cloning, resource\nallocation, cluster management, and application logic can be interwoven programmatically and\ndealt with as a single logical operation.\nThe VM Cloning call instantiates multiple cloud servers that are identical copies of the originating\nparent VM up to the point of cloning. Logically, clones inherit all the state of their parent, including\nOS- and application-level caches. Further, clones are automatically added to an internal private\nnetwork, thus effectively joining a dynamically scalable cluster. New computation resources,\nencapsulated as identical VMs, can be created on-the-fly and can be dynamically leveraged as\nneeded.\nTo be of practical use, VM cloning has to be applicable, efficient, and fast. In this chapter we will\ndescribe how SnowFlock's implementation of VM Cloning can be effectively interwoven in several\ndifferent programming models and frameworks, how it can be implemented to keep application\nruntime and provider overhead to a minimum, and how it can be used to create dozens of new\nVMs in five seconds or less.\nWith an API for the programmatic control of VM Cloning with bindings in C, C++, Python and Java,\nSnowFlock is extremely flexible and versatile. We've successfully used SnowFlock in prototype\nimplementations of several, quite different, systems. In parallel computation scenarios, we've\nachieved excellent results by explicitly cloning worker VMs that cooperatively distribute the load\n", "page": 241, "type": "text", "section": "Page 241"}
{"text": "across many physical hosts. For parallel applications that use the Message Passing Interface (MPI)\nand typically run on a cluster of dedicated servers, we modified the MPI startup manager to\nprovide unmodified applications with good performance and much less overhead by provisioning a\nfresh cluster of clones on demand for each run. Finally, in a quite different use case, we used\nSnowFlock to improve the efficiency and performance of elastic servers. Today's cloud-based\nelastic servers boot new, cold workers as needed to service spikes in demand. By cloning a\nrunning VM instead, SnowFlock brings new workers on line 20 times faster, and because clones\ninherit the warm buffers of their parent, they reach their peak performance sooner.\n18.2. VM Cloning\nAs the name suggests, VM clones are (nearly) identical to their parent VM. There are actually some\nminor but necessary differences to avoid issues such as MAC address collisions, but we'll come\nback to that later. To create a clone, the entire local disk and memory state must be made\navailable, which brings us to the first major design tradeoff: should we copy that state up-front or\non demand?\nThe simplest way to achieve VM cloning is to adapt the standard VM \"migration\" capability.\nTypically, migration is used when a running VM needs to be moved to a different host, such as\nwhen the host becomes overloaded or must be brought down for maintenance. Because the VM\nis purely software, it can be encapsulated in a data file that can then be copied to a new, more\nappropriate host, where it picks up execution after a brief interruption. To accomplish this, off-the-\nshelf VMMs create a file containing a \"checkpoint\" of the VM, including its local filesystem, memory\nimage, virtual CPU (VCPU) registers, etc. In migration, the newly booted copy replaces the original,\nbut the process can be altered to produce a clone while leaving the original running. In this \"eager\"\nprocess, the entire VM state is transferred up front, which provides the best initial performance,\nbecause the entire state of the VM is in place when execution begins. The disadvantage of eager\nreplication is that the laborious process of copying the entire VM must happen before execution\ncan begin, which significantly slows instantiation.\nThe other extreme, adopted by SnowFlock, is \"lazy\" state replication. Instead of copying\neverything the VM might ever need, SnowFlock transfers only the vital bits needed to begin\nexecution, and transfers state later, only when the clone needs it. This has two advantages. First,\nit minimizes the instantiation latency by doing as little work as possible up front. Second, it\nincreases the efficiency by copying only the state that is actually used by the clone. The yield of\nthis benefit, of course, depends on the clone's behavior, but few applications access every page\nof memory and every file in the local filesystem.\nHowever, the benefits of lazy replication aren't free. Because the state transfer is postponed until\nthe last moment, the clone is left waiting for state to arrive before it can continue execution. This\nThis situation parallels swapping of memory to disk in time-shared workstation: applications are\nblocked waiting for state to be fetched from a high latency source. In the case of SnowFlock, the\nblocking somewhat degrades the clone's performance; the severity of the slowdown depends on\nthe application. For high performance computing applications we've found this degradation has\nlittle impact, but a cloned database server may perform poorly at first. It should be noted that this\nis a transient effect: within a few minutes, most of the necessary state has been transferred and\nthe clone's performance matches that of the parent.\nAs an aside, if you're well versed in VMs, you're likely wondering if the optimizations used by \"live\"\nmigration are useful here. Live migration is optimized to shorten the interval between the original\nVM's suspension and the resumption of execution by the new copy. To accomplish this, the Virtual\nMachine Monitor (VMM) pre-copies the VM's state while the original is still running, so that after\nsuspending it, only the recently changed pages need to be transferred. This technique does not\naffect the interval between the migration request and the time the copy begins execution, and so\nwould not reduce the instantiation latency of eager VM cloning.\n18.3. SnowFlock's Approach\nSnowFlock implements VM cloning with a primitive called \"VM Fork\", which is like a standard Unix\n", "page": 242, "type": "text", "section": "Page 242"}
{"text": "fork, but with a few important differences. First, rather than duplicating a single process, VM Fork\nduplicates an entire VM, including all of memory, all processes and virtual devices, and the local\nfilesystem. Second, instead of producing a single copy running on the same physical host, VM\nFork can simultaneously spawn many copies in parallel. Finally, VMs can be forked to distinct\nphysical servers, letting you quickly increase your cloud footprint as needed.\nThe following concepts are key to SnowFlock:\nVirtualization: The VM encapsulates the computation environment, making clouds and\nmachine cloning possible.\nLazy Propagation: The VM state isn't copied until it's needed, so clones come alive in a few\nseconds.\nMulticast: Clone siblings have similar needs in terms of VM state. With multicast, dozens of\nclones start running as quickly as one.\nPage Faults: When a clone tries to use missing memory, it faults and triggers a request to\nthe parent. The clone's execution is blocked until the needed page arrives.\nCopy on Write (CoW): By taking a copy of its memory and disk pages before overwriting\nthem, the parent VM can continue to run while preserving a frozen copy of its state for use\nby the clones.\nWe've implemented SnowFlock using the Xen virtualization system, so it's useful to introduce\nsome Xen-specific terminology for clarity. In a Xen environment, the VMM is called the hypervisor,\nand VMs are called domains. On each physical machine (host), there is a privileged domain, called\n\"domain 0\" (dom0), that has full access to the host and its physical devices, and can be used to\ncontrol additional guest, or \"user\", VMs that are called \"domain U\" (domU).\nIn broad strokes, SnowFlock consists of a set of modifications to the Xen hypervisor that enable it\nto smoothly recover when missing resources are accessed, and a set of supporting processes\nand systems that run in dom0 and cooperatively transfer the missing VM state, and some optional\nmodifications to the OS executing inside clone VMs. There are six main components.\nVM Descriptor: This small object is used to seed the clone, and holds the bare-bones skeleton\nof the VM as needed to begin execution. It lacks the guts and muscle needed to perform any\nuseful work.\nMulticast Distribution System (mcdist): This parent-side system efficiently distributes the VM\nstate information simultaneously to all clones.\nMemory Server Process: This parent-side process maintains a frozen copy of the parent's\nstate, and makes it available to all clones on demand through mcdist.\nMemtap Process: This clone-side process acts on the clone's behalf, and communicates with\nthe memory server to request pages that are needed but missing.\nClone Enlightenment: The guest kernel running inside the clones can alleviate the on-demand\ntransfer of VM state by providing hints to the VMM. This is optional but highly desirable for\nefficiency.\nControl Stack: Daemons run on each physical host to orchestrate the other components and\nmanage the SnowFlock parent and clone VMs.\n", "page": 243, "type": "text", "section": "Page 243"}
{"text": "Figure 18.1: SnowFlock VM Replication Architecture\nPictorially speaking, Figure 18.1 depicts the process of cloning a VM, showing the the four main\nsteps: (1) suspending the parent VM to produce an architectural descriptor; (2) distributing this\ndescriptor to all target hosts; (3) initiating clones that are mostly empty state-wise; and (4)\npropagating state on-demand. The figure also depicts the use of multicast distribution with\nmcdist, and fetch avoidance via guest enlightenment.\nIf you're interested in trying SnowFlock, it's available in two flavors. The documentation and open\nsource code for the original University of Toronto SnowFlock research project are available1 If\nyou'd prefer to take the industrial-strength version for a spin, a free, non-commercial license is\navailable from GridCentric Inc.2 Because SnowFlock includes changes to the hypervisor and\nrequires access to dom0, installing SnowFlock requires privileged access on the host machines.\nFor that reason, you'll need to use your own hardware, and won't be able to try it out as a user in\na commercial cloud environment such as Amazon's EC2.\nThroughout the next few sections we'll describe the different pieces that cooperate to achieve\ninstantaneous and efficient cloning. All the pieces we will describe fit together as shown in\nFigure 18.2.\n", "page": 244, "type": "text", "section": "Page 244"}
{"text": "Figure 18.2: Software Components of SnowFlock\n18.4. Architectural VM Descriptor\nThe key design decision for SnowFlock is to postpone the replication of VM state to a lazy runtime\noperation. In other words, copying the memory of a VM is a late binding operation, allowing for\nmany opportunities for optimization.\nThe first step to carry out this design decision is the generation of an architectural descriptor of\nthe VM state. This is the seed that will be used to create clone VMs. It contains the bare minimum\nnecessary to create a VM and make it schedulable. As the name implies, this bare minimum\nconsists of data structures needed by the underlying architectural specification. In the case of\nSnowFlock, the architecture is a combination of Intel x86 processor requirements and Xen\nrequirements. The architectural descriptor thus contains data structures such as page tables,\nvirtual registers, device metadata, wallclock timestamps, etc. We refer the interested reader to\n[LCWB+11] for an in-depth description of the contents of the architectural descriptor.\nAn architectural descriptor has three important properties: First, it can be created in little time; 200\nmilliseconds is not uncommon. Second, it is small, typically three orders of magnitude smaller than\nthe memory allocation of the originating VM (1 MB for a 1 GB VM). And third, a clone VM can be\ncreated from a descriptor in less than a second (typically 800 milliseconds).\nThe catch, of course, is that the cloned VMs are missing most of their memory state by the time\nthey are created from the descriptor. The following sections explain how we solve this problem\u2014\nand how we take advantage of the opportunities for optimization it presents.\n18.5. Parent-Side Components\nOnce a VM is cloned it becomes a parent for its children or clones. Like all responsible parents, it\nneeds to look out for the well-being of its descendants. It does so by setting up a set of services\nthat provision memory and disk state to cloned VMs on demand.\n18.5.1. Memserver Process\nWhen the architectural descriptor is created, the VM is stopped in its tracks throughout the\nprocess. This is so the VM memory state settles; before actually pausing a VM and descheduling\nfrom execution, internal OS drivers quiesce into a state from which clones can reconnect to the\n", "page": 245, "type": "text", "section": "Page 245"}
{"text": "external world in their new enclosing VMs. We take advantage of this quiescent state to create a\n\"memory server\", or memserver.\nThe memory server will provide all clones with the bits of memory they need from the parent.\nMemory is propagated at the granularity of an x86 memory page (4 kbytes). In its simplest form,\nthe memory server sits waiting for page requests from clones, and serves one page at a time, one\nclone at a time.\nHowever, this is the very same memory the parent VM needs to use to keep executing. If we\nwould allow the parent to just go ahead and modify this memory, we would serve corrupted\nmemory contents to clone VMs: the memory served would be different from that at the point of\ncloning, and clones would be mightily confused. In kernel hacking terms, this is a sure recipe for\nstack traces.\nTo circumvent this problem, a classical OS notion comes to the rescue: Copy-on-Write, or CoW\nmemory. By enlisting the aid of the Xen hypervisor, we can remove writing privileges from all\npages of memory in the parent VM. When the parent actually tries to modify one page, a\nhardware page fault is triggered. Xen knows why this happened, and makes a copy of the page.\nThe parent VM is allowed to write to the original page and continue execution, while the memory\nserver is told to use the copy, which is kept read-only. In this manner, the memory state at the\npoint of cloning remains frozen so that clones are not confused, while the parent is able to\nproceed with execution. The overhead of CoW is minimal: similar mechanisms are used by Linux,\nfor example, when creating new processes.\n18.5.2. Multicasting with Mcdist\nClones are typically afflicted with an existential syndrome known as \"fate determinism.\" We expect\nclones to be created for a single purpose: for example, to align X chains of DNA against a\nsegment Y of a database. Further, we expect a set of clones to be created so that all siblings do\nthe same, perhaps aligning the same X chains against different segments of the database, or\naligning different chains against the same segment Y. Clones will thus clearly exhibit a great\namount of temporal locality in their memory accesses: they will use the same code and large\nportions of common data.\nWe exploit the opportunities for temporal locality through mcdist, our own multicast distribution\nsystem tailored to SnowFlock. Mcdist uses IP multicast to simultaneously distribute the same\npacket to a set of receivers. It takes advantage of network hardware parallelism to decrease the\nload on the memory server. By sending a reply to all clones on the first request for a page, each\nclone's requests act as a prefetch for its siblings, because of their similar memory access\npatterns.\nUnlike other multicast systems, mcdist does not have to be reliable, does not have to deliver\npackets in an ordered manner, and does not have to atomically deliver a reply to all intended\nreceivers. Multicast is strictly an optimization, and delivery need only be ensured to the clone\nexplicitly requesting a page. The design is thus elegantly simple: the server simply multicasts\nresponses, while clients time-out if they have not received a reply for a request, and retry the\nrequest.\nThree optimizations specific to SnowFlock are included in mcdist:\nLockstep Detection: When temporal locality does happen, multiple clones request the same\npage in very close succession. The mcdist server ignores all but the first of such requests.\nFlow Control: Receivers piggyback their receive rate on requests. The server throttles its\nsending rate to a weighted average of the clients' receive rate. Otherwise, receivers will be\ndrowned by too many pages sent by an eager server.\nEnd Game: When the server has sent most pages, it falls back to unicast responses. Most\nrequests at this point are retries, and thus blasting a page through the wire to all clones is\nunnecessary.\n18.5.3. Virtual Disk\n", "page": 246, "type": "text", "section": "Page 246"}
{"text": "SnowFlock clones, due to their short life span and fate determinism, rarely use their disk. The\nvirtual disk for a SnowFlock VM houses the root partition with binaries, libraries and configuration\nfiles. Heavy data processing is done through suitable filesystems such as HDFS or PVFS. Thus,\nwhen SnowFlock clones decide to read from their root disk, they typically have their requests\nsatisfied by the kernel filesystem page cache.\nHaving said that, we still need to provide access to the virtual disk for clones, in the rare instance\nthat such access is needed. We adopted the path of least resistance here, and implemented the\ndisk by closely following the memory replication design. First, the state of the disk is frozen at the\ntime of cloning. The parent VM keeps using its disk in a CoW manner: writes are sent to a separate\nlocation in backing storage, while the view of the disk clones expect remains immutable. Second,\ndisk state is multicast to all clones, using mcdist, with the same 4 KB page granularity, and under\nthe same expectations of temporal locality. Third, replicated disk state for a clone VM is strictly\ntransient: it is stored in a sparse flat file which is deleted once the clone is destroyed.\n18.6. Clone-Side Components\nClones are hollow shells when they are created from an architectural descriptor, so like everybody\nelse, they need a lot of help from their parents to grow up: the children VMs move out and\nimmediately call home whenever they notice something they need is missing, asking their parent to\nsend it over right away.\n18.6.1. Memtap Process\nAttached to each clone after creation, the memtap process is the lifeline of a clone. It maps all of\nthe memory of the clone and fills it on demand as needed. It enlists some crucial bits of help from\nthe Xen hypervisor: access permission to the memory pages of the clones is turned off, and\nhardware faults caused by first access to a page are routed by the hypervisor into the memtap\nprocess.\nIn its simplest incarnation, the memtap process simply asks the memory server for the faulting\npage, but there are more complicated scenarios as well. First, memtap helpers use mcdist. This\nmeans that at any point in time, any page could arrive by virtue of having been requested by\nanother clone\u2014the beauty of asynchronous prefetching. Second, we allow SnowFlock VMs to be\nmulti-processor VMs. There wouldn't be much fun otherwise. This means that multiple faults need\nto be handled in parallel, perhaps even for the same page. Third, in later versions memtap helpers\ncan explicitly prefetch a batch of pages, which can arrive in any order given the lack of guarantees\nfrom the mcdist server. Any of these factors could have led to a concurrency nightmare, and we\nhave all of them.\nThe entire memtap design centers on a page presence bitmap. The bitmap is created and initialized\nwhen the architectural descriptor is processed to create the clone VM. The bitmap is a flat bit\narray sized by the number of pages the VM's memory can hold. Intel processors have handy\natomic bit mutation instructions: setting a bit, or doing a test and set, can happen with the\nguarantee of atomicity with respect to other processors in the same box. This allows us to avoid\nlocks in most cases, and thus to provide access to the bitmap by different entities in different\nprotection domains: the Xen hypervisor, the memtap process, and the cloned guest kernel itself.\nWhen Xen handles a hardware page fault due to a first access to a page, it uses the bitmap to\ndecide whether it needs to alert memtap. It also uses the bitmap to enqueue multiple faulting virtual\nprocessors as dependent on the same absent page. Memtap buffers pages as they arrive. When\nits buffer is full or an explicitly requested page arrives, the VM is paused, and the bitmap is used to\ndiscard any duplicate pages that have arrived but are already present. Any remaining pages that\nare needed are then copied into the VM memory, and the appropriate bitmap bits are set.\n18.6.2. Clever Clones Avoid Unnecessary Fetches\nWe just mentioned that the page presence bitmap is visible to the kernel running inside the clone,\nand that no locks are needed to modify it. This gives clones a powerful \"enlightenment\" tool: they\n", "page": 247, "type": "text", "section": "Page 247"}
{"text": "can prevent the fetching of pages by modifying the bitmap and pretending they are present. This\nis extremely useful performance-wise, and safe to do when pages will be completely overwritten\nbefore they are used.\nThere happens to be a very common situation when this happens and fetches can be avoided. All\nmemory allocations in the kernel (using vmalloc, kzalloc, get_free_page, user-space brk,\nand the like) are ultimately handled by the kernel page allocator. Typically pages are requested by\nintermediate allocators which manage finer-grained chunks: the slab allocator, the glibc malloc\nallocator for a user-space process, etc. However, whether allocation is explicit or implicit, one key\nsemantic implication always holds true: no one cares about what the page contained, because its\ncontents will be arbitrarily overwritten. Why fetch such a page, then? There is no reason to do so,\nand empirical experience shows that avoiding such fetches is tremendously advantageous.\n18.7. VM Cloning Application Interface\nSo far we have focused on the internals of cloning a VM efficiently. As much fun as solipsistic\nsystems can be, we need to turn our attention to those who will use the system: applications.\n18.7.1. API Implementation\nVM Cloning is offered to the application via the simple SnowFlock API, depicted in Figure 18.1.\nCloning is basically a two-stage process. You first request an allocation for the clone instances,\nalthough due to the system policies that are in effect, that allocation may be smaller than\nrequested. Second, you use the allocation to clone your VM. A key assumption is that your VM\nfocuses on a single operation. VM Cloning is appropriate for single-application VMs such as a web\nserver or a render farm component. If you have a hundred-process desktop environment in\nwhich multiple applications concurrently call VM cloning, you're headed for chaos.\nsf_request_ticket(n)\nRequests an allocation for n clones. Returns a ticket describing an allocation for m\u2264n\nclones.\nsf_clone(ticket)\nClones, using the ticket allocation. Returns the clone ID, 0\u2264ID<m.\nsf_checkpoint_parent() Prepares an immutable checkpoint C of the parent VM to be used for creating clones at\nan arbitrarily later time.\nsf_create_clones(C,\nticket)\nSame as sf_clone, uses the checkpoint C. Clones will begin execution at the point at\nwhich the corresponding sf_checkpoint_parent() was invoked.\nsf_exit()\nFor children (1\u2264ID<m), terminates the child.\nsf_join(ticket)\nFor the parent (ID = 0), blocks until all children in the ticket reach their sf_exit\ncall. At that point all children are terminated and the ticket is discarded.\nsf_kill(ticket)\nParent only, discards ticket and immediately kills all associated children.\nTable 18.1: The SnowFlock VM Cloning API\nThe API simply marshals messages and posts them to the XenStore, a shared-memory low-\nthroughput interface used by Xen for control plane transactions. A SnowFlock Local Daemon\n(SFLD) executes on the hypervisor and listens for such requests. Messages are unmarshalled,\nexecuted, and requests posted back.\nPrograms can control VM Cloning directly through the API, which is available for C, C++, Python\nand Java. Shell scripts that harness the execution of a program can use the provided command-\nline scripts instead. Parallel frameworks such as MPI can embed the API: MPI programs can then\nuse SnowFlock without even knowing, and with no modification to their source. Load balancers\nsitting in front of web or application servers can use the API to clone the servers they manage.\nSFLDs orchestrate the execution of VM Cloning requests. They create and transmit architectural\ndescriptors, create cloned VMs, launch disk and memory servers, and launch memtap helper\nprocesses. They are a miniature distributed system in charge of managing the VMs in a physical\ncluster.\n", "page": 248, "type": "text", "section": "Page 248"}
{"text": "SFLDs defer allocation decisions to a central SnowFlock Master Daemon (SFMD). SFMD simply\ninterfaces with appropriate cluster management software. We did not see any need to reinvent\nthe wheel here, and deferred decisions on resource allocation, quotas, policies, etc. to suitable\nsoftware such as Sun Grid Engine or Platform EGO.\n18.7.2. Necessary Mutations\nAfter cloning, most of the cloned VM's processes have no idea that they are no longer the parent,\nand that they are now running in a copy. In most aspects, this just works fine and causes no\nissues. After all, the primary task of the OS is to isolate applications from low-level details, such as\nthe network identity. Nonetheless, making the transition smooth requires a set of mechanisms to\nbe put in place. The meat of the problem is in managing the clone's network identity; to avoid\nconflicts and confusion, we must introduce slight mutations during the cloning process. Also,\nbecause these tweaks may necessitate higher-level accommodations, a hook is inserted to allow\nthe user to configure any necessary tasks, such as (re)mounting network filesystems that rely on\nthe clone's identity.\nClones are born to a world that is mostly not expecting them. The parent VM is part of a network\nmanaged most likely by a DHCP server, or by any other of the myriad ways sysadmins find to do\ntheir job. Rather than assume a necessarily inflexible scenario, we place the parent and all clones in\ntheir own private virtual network. Clones from the same parent are all assigned a unique ID, and\ntheir IP address in this private network is automatically set up upon cloning as a function of the ID.\nThis guarantees that no intervention from a sysadmin is necessary, and that no IP address\ncollisions will ever happen.\nIP reconfiguration is performed directly by a hook we place on the virtual network driver.\nHowever, we also rig the driver to automatically generate synthetic DHCP responses. Thus,\nregardless of your choice of distribution, your virtual network interface will ensure that the proper\nIP coordinates are propagated to the guest OS, even when you are restarting from scratch.\nTo prevent clones from different parents colliding with each others' virtual private networks\u2014and\nto prevent mutual DDoS attacks\u2014clone virtual network are isolated at the Ethernet (or layer 2)\nlevel. We hijack a range of Ethernet MAC OUIs 3 and dedicate them to clones. The OUI will be a\nfunction of the parent VM. Much like the ID of a VM determines its IP address, it also determines\nits non-OUI Ethernet MAC address. The virtual network driver translates the MAC address the VM\nbelieves it has to the one assigned as a function of its ID, and filters out all traffic to and from\nvirtual private network with different OUIs. This isolation is equivalent to that achievable via\nebtables, although much simpler to implement.\nHaving clones talk only to each other may be fun, but not fun enough. Sometimes we will want our\nclones to reply to HTTP requests from the Internet, or mount public data repositories. We equip\nany set of parent and clones with a dedicated router VM. This tiny VM performs firewalling,\nthrottling and NATing of traffic from the clones to the Internet. It also limits inbound connections\nto the parent VM and well-known ports. The router VM is lightweight but represents a single point\nof centralization for network traffic, which can seriously limit scalability. The same network rules\ncould be applied in a distributed fashion to each host upon which a clone VM runs. We have not\nreleased that experimental patch.\nSFLDs assign IDs, and teach the virtual network drivers how they should configure themselves:\ninternal MAC and IP addresses, DHCP directives, router VM coordinates, filtering rules, etc.\n18.8. Conclusion\nBy tweaking the Xen hypervisor and lazily transferring the VM's state, SnowFlock can produce\ndozens of clones of a running VM in a few seconds. Cloning VMs with SnowFlock is thus\ninstantaneous and live\u2014it improves cloud usability by automating cluster management and giving\napplications greater programmatic control over the cloud resources. SnowFlock also improves\ncloud agility by speeding up VM instantiation by a factor of 20, and by improving the performance\nof most newly created VMs by leveraging their parent's warm, in-memory OS and application\n", "page": 249, "type": "text", "section": "Page 249"}
{"text": "caches. The keys to SnowFlock's efficient performance are heuristics that avoid unnecessary page\nfetches, and the multicast system that lets clone siblings cooperatively prefetch their state. All it\ntook was the clever application of a few tried-and-true techniques, some sleight of hand, and a\ngenerous helping of industrial-strength debugging.\nWe learned two important lessons throughout the SnowFlock experience. The first is the often-\nunderestimated value of the KISS theorem. We were expecting to implement complicated\nprefetching techniques to alleviate the spate of requests for memory pages a clone would issue\nupon startup. This was, perhaps surprisingly, not necessary. The system performs very well for\nmany workloads based on one single principle: bring the memory over as needed. Another\nexample of the value of simplicity is the page presence bitmap. A simple data structure with clear\natomic access semantics greatly simplifies what could have been a gruesome concurrency\nproblem, with multiple virtual CPUs competing for page updates with the asynchronous arrival of\npages via multicast.\nThe second lesson is that scale does not lie. In other words, be prepared to have your system\nshattered and new bottlenecks uncovered every time you bump your scale by a power of two.\nThis is intimately tied with the previous lesson: simple and elegant solutions scale well and do not\nhide unwelcome surprises as load increases. A prime example of this principle is our mcdist\nsystem. In large-scale tests, a TCP/IP-based page distribution mechanism fails miserably for\nhundreds of clones. Mcdist succeeds by virtue of its extremely constrained and well-defined roles:\nclients only care about their own pages; the server only cares about maintaining a global flow\ncontrol. By keeping mcdist humble and simple, SnowFlock is able to scale extremely well.\nIf you are interested in knowing more, you can visit the University of Toronto site1 for the\nacademic papers and open-source code licensed under GPLv2, and GridCentric4 for an industrial\nstrength implementation.\nFootnotes\n1. http://sysweb.cs.toronto.edu/projects/1\n2. http://www.gridcentriclabs.com/architecture-of-open-source-applications\n3. OUI, or Organizational Unique ID, is a range of MAC addresses assigned to a vendor.\n4. http://www.gridcentriclabs.com/\n", "page": 250, "type": "text", "section": "Page 250"}
{"text": "The Architecture of\nOpen Source Applications\nAmy Brown and Greg Wilson (eds.)\nISBN 978-1-257-63801-7 \nLicense / Buy / Contribute\nChapter 19. SocialCalc\nAudrey Tang\nThe history of spreadsheets spans more than 30 years. The first spreadsheet program, VisiCalc,\nwas conceived by Dan Bricklin in 1978 and shipped in 1979. The original concept was quite\nstraightforward: a table that spans infinitely in two dimensions, its cells populated with text,\nnumbers, and formulas. Formulas are composed of normal arithmetic operators and various built-\nin functions, and each formula can use the current contents of other cells as values.\nAlthough the metaphor was simple, it had many applications: accounting, inventory, and list\nmanagement are just a few. The possibilities were practically limitless. All these uses made VisiCalc\ninto the first \"killer app\" of the personal computer era.\nIn the decades that followed successors like Lotus 1-2-3 and Excel made incremental\nimprovements, but the core metaphor stayed the same. Most spreadsheets were stored as on-\ndisk files, and loaded into memory when opened for editing. Collaboration was particularly hard\nunder the file-based model:\nEach user needed to install a version of the spreadsheet editor.\nE-mail ping-pong, shared folders, or setting up a dedicated version-control system all added\nbookkeeping overhead.\nChange tracking was limited; for example, Excel does not preserve history for formatting\nchanges and cell comments.\nUpdating formatting or formulas in templates required painstaking changes to existing\nspreadsheet files that used that template.\nFortunately, a new collaboration model emerged to address these issues with elegant simplicity. It\nis the wiki model, invented by Ward Cunningham in 1994, and popularized by Wikipedia in the early\n2000s.\nInstead of files, the wiki model features server-hosted pages, editable in the browser without\nrequiring special software. Those hypertext pages can easily link to each other, and even include\nportions of other pages to form a larger page. All participants view and edit the latest version by\ndefault, with revision history automatically managed by the server.\nInspired by the wiki model, Dan Bricklin started working on WikiCalc in 2005. It aims to combine\nthe authoring ease and multi-person editing of wikis with the familiar visual formatting and\ncalculating metaphor of spreadsheets.\n19.1. WikiCalc\nThe first version of WikiCalc (Figure 19.1) had several features that set it apart from other\nspreadsheets at the time:\nPlain text, HTML, and wiki-style markup rendering for text data.\nWiki-style text that includes commands to insert links, images, and values from cell\nreferences.\nFormula cells may reference values of other WikiCalc pages hosted on other websites.\nAbility to create output to be embedded in other web pages, both static and live data.\nCell formatting with access to CSS style attributes and CSS classes.\nLogging of all edit operations as an audit trail.\nWiki-like retention of each new version of a page with roll-back capability.\n", "page": 251, "type": "text", "section": "Page 251"}
{"text": "Figure 19.1: WikiCalc 1.0 Interface\nFigure 19.2: WikiCalc Components\n", "page": 252, "type": "text", "section": "Page 252"}
{"text": "Figure 19.3: WikiCalc Flow\nWikiCalc 1.0's internal architecture (Figure 19.2) and information flow (Figure 19.3) were\ndeliberately simple, but nevertheless powerful. The ability to compose a master spreadsheet from\nseveral smaller spreadsheets proved particularly handy. For example, imagine a scenario where\neach salesperson keeps numbers in a spreadsheet page. Each sales manager then rolls up their\nreps' numbers into a regional spreadsheet, and the VP of sales then rolls up the regional numbers\ninto a top-level spreadsheet.\nEach time one of the individual spreadsheets is updated, all the roll-up spreadsheets can reflect\nthe update. If someone wants further detail, they simply click through to view the spreadsheet\nbehind the spreadsheet. This roll-up capability eliminates the redundant and error-prone effort of\nupdating numbers in multiple places, and ensures all views of the information stay fresh.\nTo ensure the recalculations are up-to-date, WikiCalc adopted a thin-client design, keeping all the\nstate information on the server side. Each spreadsheet is represented on the browser as a\n<table> element; editing a cell will send an ajaxsetcell call to the server, and the server then\ntells the browser which cells need updating.\nUnsurprisingly, this design depends on a fast connection between the browser and the server.\nWhen the latency is high, users will start to notice the frequent appearance of \"Loading\u2026\"\nmessages between updating a cell and seeing its new contents as shown in Figure 19.4. This is\nespecially a problem for users interactively editing formulas by tweaking the input and expecting to\nsee results in real time.\nFigure 19.4: Loading Message\nMoreover, because the <table> element had the same dimensions as the spreadsheet, a\n100\u00d7100 grid would create 10,000 <td> DOM objects, which strains the memory resource of\nbrowsers, further limiting the size of pages.\nDue to these shortcomings, while WikiCalc was useful as a stand-alone server running on\nlocalhost, it was not very practical to embed as part of web-based content management systems.\nIn 2006, Dan Bricklin teamed up with Socialtext to start developing SocialCalc, a ground-up rewrite\n", "page": 253, "type": "text", "section": "Page 253"}
{"text": "of WikiCalc in Javascript based on some of the original Perl code.\nThis rewrite was aimed at large, distributed collaborations, and sought to deliver a look and feel\nmore like that of a desktop app. Other design goals included:\nCapable of handling hundreds of thousands of cells.\nFast turnaround time for edit operations.\nClient-side audit trail and undo/redo stack.\nBetter use of Javascript and CSS to provide full-fledged layout functionality.\nCross-browser support, despite the more extensive use of responsive Javascript.\nAfter three years of development and various beta releases, Socialtext released SocialCalc 1.0 in\n2009, successfully meeting the design goals. Let's now take a look at the architecture of the\nSocialCalc system.\n19.2. SocialCalc\nFigure 19.5: SocialCalc Interface\nFigure 19.5 and Figure 19.6 show SocialCalc's interface and classes respectively. Compared to\nWikiCalc, the server's role has been greatly reduced. Its only responsibility is responding to HTTP\nGETs by serving entire spreadsheets serialized in the save format; once the browser receives the\ndata, all calculations, change tracking and user interaction are now implemented in Javascript.\n", "page": 254, "type": "text", "section": "Page 254"}
{"text": "Figure 19.6: SocialCalc Class Diagram\nThe Javascript components were designed with a layered MVC (Model/View/Controller) style, with\neach class focusing on a single aspect:\nSheet is the data model, representing an in-memory structure of a spreadsheet. It contains a\ndictionary from coordinates to Cell objects, each representing a single cell. Empty cells need\nno entries, and hence consume no memory at all.\nCell represents a cell's content and formats. Some common properties are shown in\nTable 19.1.\nRenderContext implements the view; it is responsible for rendering a sheet into DOM objects.\nTableControl is the main controller, accepting mouse and keyboard events. As it receives\nview events such as scrolling and resizing, it updates its associated RenderContext object. As\nit receives update events that affects the sheet's content, it schedules new commands to the\nsheet's command queue.\nSpreadSheetControl is the top-level UI with toolbars, status bars, dialog boxes and color\npickers.\nSpreadSheetViewer is an alternate top-level UI that provides a read-only interactive view.\ndatatype\nt\ndatavalue 1Q84\ncolor\nblack\nbgcolor\nwhite\nfont\nitalic bold 12pt Ubuntu\ncomment\nIchi-Kyu-Hachi-Yon\nTable 19.1: Cell Contents and Formats\nWe adopted a minimal class-based object system with simple composition/delegation, and make\n", "page": 255, "type": "text", "section": "Page 255"}
{"text": "no use of inheritance or object prototypes. All symbols are placed under the SocialCalc.*\nnamespace to avoid naming conflicts.\nEach update on the sheet goes through the ScheduleSheetCommands method, which takes a\ncommand string representing the edit. (Some common commands are show in Table 19.2.) The\napplication embedding SocialCalc may define extra commands on their own, by adding named\ncallbacks into the SocialCalc.SheetCommandInfo.CmdExtensionCallbacks object, and use\nthe startcmdextension command to invoke them.\n    set     sheet defaultcolor blue\n    set     A width 100\n    set     A1 value n 42\n    set     A2 text t Hello\n    set     A3 formula A1*2\n    set     A4 empty\n    set     A5 bgcolor green\n    merge   A1:B2\n    unmerge A1\n    erase   A2\n    cut     A3\n    paste   A4\n    copy    A5\n    sort    A1:B9 A up B down\n    name    define Foo A1:A5\n    name    desc   Foo Used in formulas like SUM(Foo)\n    name    delete Foo\n    startcmdextension UserDefined args\nTable 19.2: SocialCalc Commands\n19.3. Command Run-loop\nTo improve responsiveness, SocialCalc performs all recalculation and DOM updates in the\nbackground, so the user can keep making changes to several cells while the engine catches up on\nearlier changes in the command queue.\n", "page": 256, "type": "text", "section": "Page 256"}
{"text": "Figure 19.7: SocialCalc Command Run-loop\nWhen a command is running, the TableEditor object sets its busy flag to true; subsequent\ncommands are then pushed into the deferredCommands queue, ensuring a sequential order of\nexecution. As the event loop diagram in Figure 19.7 shows, the Sheet object keeps sending\nStatusCallback events to notify the user of the current state of command execution, through\neach of the four steps:\nExecuteCommand: Sends cmdstart upon start, and cmdend when the command finishes\nexecution. If the command changed a cell's value indirectly, enter the Recalc step. Otherwise,\nif the command changed the visual appearance of one or more on-screen cells, enter the\nRender step. If neither of the above applies (for example with the copy command), skip to\nthe PositionCalculations step.\nRecalc (asneeded): Sends calcstart upon start, calcorder every 100ms when checking\nthe dependency chain of cells, calccheckdone when the check finishes, and calcfinished\nwhen all affected cells received their re-calculated values. This step is always followed by the\nRender step.\nRender (as needed): Sends schedrender upon start, and renderdone when the <table>\nelement is updated with formatted cells. This step is always followed by PositionCalculations.\n", "page": 257, "type": "text", "section": "Page 257"}
{"text": "PositionCalculations: Sends schedposcalc upon start, and doneposcalc after updating the\nscrollbars, the current editable cell cursor, and other visual components of the\nTableEditor.\nBecause all commands are saved as they are executed, we naturally get an audit log of all\noperations. The Sheet.CreateAuditString method provides a newline-delimited string as the\naudit trail, with each command in a single line.\nExecuteSheetCommand also creates an undo command for each command it executes. For\nexample, if the cell A1 contains \"Foo\" and the user executes set A1 text Bar, then an undo-\ncommand set A1 text Foo is pushed to the undo stack. If the user clicks Undo, then the undo-\ncommand is executed to restore A1 to its original value.\n19.4. Table Editor\nNow let's look at the TableEditor layer. It calculates the on-screen coordinates of its\nRenderContext, and manages horizontal/vertical scroll bars through two TableControl\ninstances.\nFigure 19.8: TableControl Instances Manage Scroll Bars\nThe view layer, handled by the RenderContext class, also differs from WikiCalc's design. Instead\nof mapping each cell to a <td> element, we now simply create a fixed-size <table> that fits the\nbrowser's visible area, and pre-populate it with <td> elements.\nAs the user scrolls the spreadsheet through our custom-drawn scroll bars, we dynamically update\nthe innerHTML of the pre-drawn <td> elements. This means we don't need to create or destroy\nany <tr> or <td> elements in many common cases, which greatly speeds up response time.\nBecause RenderContext only renders the visible region, the size of Sheet object can be arbitrarily\nlarge without affecting its performance.\nTableEditor also contains a CellHandles object, which implements the radial fill/move/slide\nmenu attached to the bottom-right corner to the current editable cell, known as the ECell, shown\nin Figure 19.9.\nFigure 19.9: Current Editable Cell, Known as the ECell\nThe input box is managed by two classes: InputBox and InputEcho. The former manages the\n", "page": 258, "type": "text", "section": "Page 258"}
{"text": "above-the-grid edit row, while the latter shows an updated-as-you-type preview layer, overlaying\nthe ECell's content (Figure 19.10).\nFigure 19.10: The Input Box is Managed by Two Classes\nUsually, the SocialCalc engine only needs to communicate to the server when opening a\nspreadsheet for edit, and when saving it back to server. For this purpose, the\nSheet.ParseSheetSave method parses a save format string into a Sheet object, and the\nSheet.CreateSheetSave method serializes a Sheet object back into the save format.\nFormulas may refer to values from any remote spreadsheet with a URL. The recalc command re-\nfetches the externally referenced spreadsheets, parses them again with\nSheet.ParseSheetSave, and stores them in a cache so the user can refer to other cells in the\nsame remote spreadsheets without re-fetching its content.\n19.5. Save Format\nThe save format is in standard MIME multipart/mixed format, consisting of four text/plain;\ncharset=UTF-8 parts, each part containing newline-delimited text with colon-delimited data fields.\nThe parts are:\nThe meta part lists the types of the other parts.\nThe sheet part lists each cell's format and content, each column's width (if not default), the\nsheet's default format, followed by a list of fonts, colors and borders used in the sheet.\nThe optional edit part saves the TableEditor's edit state, including ECell's last position, as\nwell as the fixed sizes of row/column panes.\nThe optional audit part contains the history of commands executed in the previous editing\nsession.\nFor example, Figure 19.11 shows a spreadsheet with three cells, with 1874 in A1 as the ECell, the\nformula 2^2*43 in A2, and the formula SUM(Foo) in A3 rendered in bold, referring to the named\nrange Foo over A1:A2.\nFigure 19.11: A Spreadsheet with Three Cells\nThe serialized save format for the spreadsheet looks like this:\n    socialcalc:version:1.0\n    MIME-Version: 1.0\n    Content-Type: multipart/mixed; boundary=SocialCalcSpreadsheetControlSave\n    --SocialCalcSpreadsheetControlSave\n    Content-type: text/plain; charset=UTF-8\n    # SocialCalc Spreadsheet Control Save\n    version:1.0\n    part:sheet\n    part:edit\n", "page": 259, "type": "text", "section": "Page 259"}
{"text": "    part:audit\n    --SocialCalcSpreadsheetControlSave\n    Content-type: text/plain; charset=UTF-8\n    version:1.5\n    cell:A1:v:1874\n    cell:A2:vtf:n:172:2^2*43\n    cell:A3:vtf:n:2046:SUM(Foo):f:1\n    sheet:c:1:r:3\n    font:1:normal bold * *\n    name:FOO::A1\\cA2\n    --SocialCalcSpreadsheetControlSave\n    Content-type: text/plain; charset=UTF-8\n    version:1.0\n    rowpane:0:1:14\n    colpane:0:1:16\n    ecell:A1\n    --SocialCalcSpreadsheetControlSave\n    Content-type: text/plain; charset=UTF-8\n    set A1 value n 1874\n    set A2 formula 2^2*43\n    name define Foo A1:A2\n    set A3 formula SUM(Foo)\n    --SocialCalcSpreadsheetControlSave--\nThis format is designed to be human-readable, as well as being relatively easy to generate\nprogrammatically. This makes it possible for Drupal's Sheetnode plugin to use PHP to convert\nbetween this format and other popular spreadsheet formats, such as Excel (.xls) and\nOpenDocument (.ods).\nNow that we have a good idea about how the pieces in SocialCalc fit together, let's look at two real-\nworld examples of extending SocialCalc.\n19.6. Rich-text Editing\nThe first example we'll look at is enhancing SocialCalc's text cells with wiki markup to display its\nrich-text rendering right in the table editor (Figure 19.12).\nFigure 19.12: Rich Text Rendering in the Table Editor\nWe added this feature to SocialCalc right after its 1.0 release, to address the popular request of\ninserting images, links and text markups using a unified syntax. Since Socialtext already has an\nopen-source wiki platform, it was natural to re-use the syntax for SocialCalc as well.\n", "page": 260, "type": "text", "section": "Page 260"}
{"text": "To implement this, we need a custom renderer for the textvalueformat of text-wiki, and to\nchange the default format for text cells to use it.\nWhat is this textvalueformat, you ask? Read on.\n19.6.1. Types and Formats\nIn SocialCalc, each cell has a datatype and a valuetype. Data cells with text or numbers\ncorrespond to text/numeric value types, and formula cells with datatype=\"f\" may generate\neither numeric or text values.\nRecall that on the Render step, the Sheet object generates HTML from each of its cells. It does so\nby inspecting each cell's valuetype: If it begins with t, then the cell's textvalueformat attribute\ndetermines how generation is done. If it begins with n, then the nontextvalueformat attribute is\nused instead.\nHowever, if the cell's textvalueformat or nontextvalueformat attribute is not defined\nexplicitly, then a default format is looked up from its valuetype, as shown in Figure 19.13.\nFigure 19.13: Value Types\nSupport for the text-wiki value format is coded in SocialCalc.format_text_for_display:\nif (SocialCalc.Callbacks.expand_wiki && /^text-wiki/.test(valueformat)) {\n    // do general wiki markup\n    displayvalue = SocialCalc.Callbacks.expand_wiki(\n        displayvalue, sheetobj, linkstyle, valueformat\n    );\n}\nInstead of inlining the wiki-to-HTML expander in format_text_for_display, we will define a new\nhook in SocialCalc.Callbacks. This is the recommended style throughout the SocialCalc\ncodebase; it improves modularity by making it possible to plug in different ways of expanding\nwikitext, as well as keeping compatibility with embedders that do not desire this feature.\n19.6.2. Rendering Wikitext\n", "page": 261, "type": "text", "section": "Page 261"}
{"text": "Next, we'll make use of Wikiwyg1, a Javascript library offering two-way conversions between\nwikitext and HTML.\nWe define the expand_wiki function by taking the cell's text, running it through Wikiwyg's\nwikitext parser and its HTML emitter:\nvar parser = new Document.Parser.Wikitext();\nvar emitter = new Document.Emitter.HTML();\nSocialCalc.Callbacks.expand_wiki = function(val) {\n    // Convert val from Wikitext to HTML\n    return parser.parse(val, emitter);\n}\nThe final step involves scheduling the set sheet defaulttextvalueformat text-wiki\ncommand right after the spreadsheet initializes:\n// We assume there's a <div id=\"tableeditor\"/> in the DOM already\nvar spreadsheet = new SocialCalc.SpreadsheetControl();\nspreadsheet.InitializeSpreadsheetControl(\"tableeditor\", 0, 0, 0);\nspreadsheet.ExecuteCommand('set sheet defaulttextvalueformat text-wiki');\nTaken together, the Render step now works as shown in Figure 19.14.\nFigure 19.14: Render Step\nThat's all! The enhanced SocialCalc now supports a rich set of wiki markup syntax:\n*bold* _italic_ `monospace` {{unformatted}}\n> indented text\n* unordered list\n# ordered list\n\"Hyperlink with label\"<http://softwaregarden.com/>\n{image: http://www.socialtext.com/images/logo.png}\nTry entering *bold* _italic_ `monospace` in A1, and you'll see it rendered as rich text\n(Figure 19.15).\n", "page": 262, "type": "text", "section": "Page 262"}
{"text": "Figure 19.15: Wikywyg Example\n19.7. Real-time Collaboration\nThe next example we'll explore is multi-user, real-time editing on a shared spreadsheet. This may\nseem complicated at first, but thanks to SocialCalc's modular design all it takes is for each on-line\nuser to broadcast their commands to other participants.\nTo distinguish between locally-issued commands and remote commands, we add an isRemote\nparameter to the ScheduleSheetCommands method:\nSocialCalc.ScheduleSheetCommands = function(sheet, cmdstr, saveundo, isRemote) {\n   if (SocialCalc.Callbacks.broadcast && !isRemote) {\n       SocialCalc.Callbacks.broadcast('execute', {\n           cmdstr: cmdstr, saveundo: saveundo\n       });\n   }\n   // \u2026original ScheduleSheetCommands code here\u2026\n}\nNow all we need to do is to define a suitable SocialCalc.Callbacks.broadcast callback\nfunction. Once it's in place, the same commands will be executed on all users connected to the\nsame spreadsheet.\nWhen this feature was first implemented for OLPC (One Laptop Per Child2) by SEETA's Sugar\nLabs3 in 2009, the broadcast function was built with XPCOM calls into D-Bus/Telepathy, the\nstandard transport for OLPC/Sugar networks (see Figure 19.16).\n", "page": 263, "type": "text", "section": "Page 263"}
{"text": "Figure 19.16: OLPC Implementation\nThat worked reasonably well, enabling XO instances in the same Sugar network to collaborate on a\ncommon SocialCalc spreadsheet. However, it is both specific to the Mozilla/XPCOM browser\nplatform, as well as to the D-Bus/Telepathy messaging platform.\n19.7.1. Cross-browser Transport\nTo make this work across browsers and operating systems, we use the Web::Hippie4\nframework, a high-level abstraction of JSON-over-WebSocket with convenient jQuery bindings,\nwith MXHR (Multipart XML HTTP Request5) as the fallback transport mechanism if WebSocket is\nnot available.\nFor browsers with Adobe Flash plugin installed but without native WebSocket support, we use the\nweb_socket.js6 project's Flash emulation of WebSocket, which is often faster and more reliable\nthan MXHR. The operation flow is shown in Figure 19.17.\nFigure 19.17: Cross-Browser Flow\nThe client-side SocialCalc.Callbacks.broadcast function is defined as:\nvar hpipe = new Hippie.Pipe();\nSocialCalc.Callbacks.broadcast = function(type, data) {\n    hpipe.send({ type: type, data: data });\n};\n$(hpipe).bind(\"message.execute\", function (e, d) {\n    var sheet = SocialCalc.CurrentSpreadsheetControlObject.context.sheetobj;\n    sheet.ScheduleSheetCommands(\n        d.data.cmdstr, d.data.saveundo, true // isRemote = true\n    );\n    break;\n});\n", "page": 264, "type": "text", "section": "Page 264"}
{"text": "Although this works quite well, there are still two remaining issues to resolve.\n19.7.2. Conflict Resolution\nThe first one is a race-condition in the order of commands executed: If users A and B\nsimultaneously perform an operation affecting the same cells, then receive and execute\ncommands broadcast from the other user, they will end up in different states, as shown in\nFigure 19.18.\nFigure 19.18: Race Condition Conflict\nWe can resolve this with SocialCalc's built-in undo/redo mechanism, as shown in Figure 19.19.\n", "page": 265, "type": "text", "section": "Page 265"}
{"text": "Figure 19.19: Race Condition Conflict Resolution\nThe process used to resolve the conflict is as follows. When a client broadcasts a command, it\nadds the command to a Pending queue. When a client receives a command, it checks the remote\ncommand against the Pending queue.\nIf the Pending queue is empty, then the command is simply executed as a remote action. If the\nremove command matches a command in the Pending queue, then the local command is removed\nfrom the queue.\nOtherwise, the client checks if there are any queued commands that conflict with the received\ncommand. If there are conflicting commands, the client first Undoes those commands and marks\nthem for later Redo. After undoing the conflicting commands (if any), the remote command is\nexecuted as usual.\nWhen a marked-for-redo command is received from the server, the client will execute it again,\nthen remove it from the queue.\n19.7.3. Remote Cursors\nEven with race conditions resolved, it is still suboptimal to accidentally overwrite the cell another\nuser is currently editing. A simple improvement is for each client to broadcast its cursor position\nto other users, so everyone can see which cells are being worked on.\nTo implement this idea, we add another broadcast handler to the MoveECellCallback event:\neditor.MoveECellCallback.broadcast = function(e) {\n    hpipe.send({\n        type: 'ecell',\n        data: e.ecell.coord\n    });\n};\n$(hpipe).bind(\"message.ecell\", function (e, d) {\n", "page": 266, "type": "text", "section": "Page 266"}
{"text": "    var cr = SocialCalc.coordToCr(d.data);\n    var cell = SocialCalc.GetEditorCellElement(editor, cr.row, cr.col);\n    // \u2026decorate cell with styles specific to the remote user(s) on it\u2026\n});\nTo mark cell focus in spreadsheets, it's common to use colored borders. However, a cell may\nalready define its own border property, and since border is mono-colored, it can only represent\none cursor on the same cell.\nTherefore, on browsers with support for CSS3, we use the box-shadow property to represent\nmultiple peer cursors in the same cell:\n/* Two cursors on the same cell */\nbox-shadow: inset 0 0 0 4px red, inset 0 0 0 2px green;\nFigure 19.20 shows how the screen would look with four people editing on the same spreadsheet.\nFigure 19.20: Four Users Editing One Spreadsheet\n19.8. Lessons Learned\nWe delivered SocialCalc 1.0 on October 19th, 2009, the 30th anniversary of the initial release of\nVisiCalc. The experience of collaborating with my colleagues at Socialtext under Dan Bricklin's\nguidance was very valuable to me, and I'd like to share some lessons I learned during that time.\n19.8.1. Chief Designer with a Clear Vision\nIn [Bro10], Fred Brooks argues that when building complex systems, the conversation is much\nmore direct if we focus on a coherent design concept, rather than derivative representations.\nAccording to Brooks, the formulation of such a coherent design concept is best kept in a single\nperson's mind:\nSince conceptual integrity is the most important attribute of a great design, and since\nthat comes from one or a few minds working uno animo, the wise manager boldly\nentrusts each design task to a gifted chief designer.\nIn the case of SocialCalc, having Tracy Ruggles as our chief user-experience designer was the key\nfor the project to converge toward a shared vision. Since the underlying SocialCalc engine was so\nmalleable, the temptation of feature creep was very real. Tracy's ability to communicate using\ndesign sketches really helped us present features in a way that feels intuitive to users.\n19.8.2. Wikis for Project Continuity\nBefore I joined the SocialCalc project, there was already over two years' worth of ongoing design\nand development, but I was able to catch up and start contributing in less than a week, simply due\nto the fact that everything is in the wiki. From the earliest design notes to the most up-to-date\nbrowser support matrix, the entire process was chronicled in wiki pages and SocialCalc\nspreadsheets.\nReading through the project's workspace brought me quickly to the same page as others, without\nthe usual hand-holding overhead typically associated with orienting a new team member.\nThis would not be possible in traditional open source projects, where most conversation takes\nplace on IRC and mailing lists and the wiki (if present) is only used for documentations and links to\ndevelopment resources. For a newcomer, it's much more difficult to reconstruct context from\nunstructured IRC logs and mail archives.\n", "page": 267, "type": "text", "section": "Page 267"}
{"text": "19.8.3. Embrace Time Zone Differences\nDavid Heinemeier Hansson, creator of Ruby on Rails, once remarked on the benefit of distributed\nteams when he first joined 37signals. \"The seven time zones between Copenhagen and Chicago\nactually meant that we got a lot done with few interruptions.\" With nine time zones between Taipei\nand Palo Alto, that was true for us during SocialCalc's development as well.\nWe often completed an entire Design-Development-QA feedback cycle within a 24-hour day, with\neach aspect taking one person's 8-hour work day in their local daytime. This asynchronous style\nof collaboration compelled us to produce self-descriptive artifacts (design sketch, code and tests),\nwhich in turn greatly improved our trust in each other.\n19.8.4. Optimize for Fun\nIn my 2006 keynote for the CONISLI conference [Tan06], I summarized my experience leading a\ndistributed team implementing the Perl 6 language into a few observations. Among them, Always\nhave a Roadmap, Forgiveness > Permission, Remove deadlocks, Seek ideas, not consensus, and\nSketch ideas with code are particularly relevant for small distributed teams.\nWhen developing SocialCalc, we took great care in distributing knowledge among team members\nwith collaborative code ownership, so nobody would become a critical bottleneck.\nFurthermore, we pre-emptively resolved disputes by actually coding up alternatives to explore the\ndesign space, and were not afraid of replacing fully-working prototypes when a better design\narrived.\nThese cultural traits helped us foster a sense of anticipation and camaraderie despite the absence\nof face-to-face interaction, kept politics to a minimum, and made working on SocialCalc a lot of\nfun.\n19.8.5. Drive Development with Story Tests\nPrior to joining Socialtext, I've advocated the \"interleave tests with the specification\" approach, as\ncan be seen in the Perl 6 specification7, where we annotate the language specification with the\nofficial test suite. However, it was Ken Pier and Matt Heusser, the QA team for SocialCalc, who\nreally opened my eyes to how this can be taken to the next level, bringing tests to the place of\nexecutable specification.\nIn Chapter 16 of [GR09], Matt explained our story-test driven development process as follows:\nThe basic unit of work is a \"story,\" which is an extremely lightweight requirements\ndocument. A story contains a brief description of a feature along with examples of what\nneeds to happen to consider the story completed; we call these examples \"acceptance\ntests\" and describe them in plain English.\nDuring the initial cut of the story, the product owner makes a good-faith first attempt to\ncreate acceptance tests, which are augmented by developers and testers before any\ndeveloper writes a line of code.\nThese story tests are then translated into wikitests, a table-based specification language inspired\nby Ward Cunningham's FIT framework8, which drives automated testing frameworks such as\nTest::WWW::Mechanize9 and Test::WWW::Selenium10.\nIt's hard to overstate the benefit of having story tests as a common language to express and\nvalidate requirements. It was instrumental in reducing misunderstanding, and has all but eliminated\nregressions from our monthly releases.\n19.8.6. Open Source With CPAL\nLast but not least, the open source model we chose for SocialCalc makes an interesting lesson in\nitself.\n", "page": 268, "type": "text", "section": "Page 268"}
{"text": "Socialtext created the Common Public Attribution License11 for SocialCalc. Based on the Mozilla\nPublic License, CPAL is designed to allow the original author to require an attribution to be\ndisplayed on the software's user interface, and has a network-use clause that triggers share-alike\nprovisions when derived work is hosted by a service over the network.\nAfter its approval by both the Open Source Initiative12 and the Free Software Foundation13, we've\nseen prominent sites such as Facebook14 and Reddit15 opting to release their platform's source\ncode under the CPAL, which is very encouraging.\nBecause CPAL is a \"weak copyleft\" license, developers can freely combine it with either free or\nproprietary software, and only need to release modifications to SocialCalc itself. This enabled\nvarious communities to adopt SocialCalc and made it more awesome.\nThere are many interesting possibilities with this open-source spreadsheet engine, and if you can\nfind a way to embed SocialCalc into your favorite project, we'd definitely love to hear about it.\nFootnotes\n1. https://github.com/audreyt/wikiwyg-js\n2. http://one.laptop.org/\n3. http://seeta.in/wiki/index.php?title=Collaboration_in_SocialCalc\n4. http://search.cpan.org/dist/Web-Hippie/\n5. http://about.digg.com/blog/duistream-and-mxhr\n6. https://github.com/gimite/web-socket-js\n7. http://perlcabal.org/syn/S02.html\n8. http://fit.c2.com/\n9. http://search.cpan.org/dist/Test-WWW-Mechanize/\n10. http://search.cpan.org/dist/Test-WWW-Selenium/\n11. https://www.socialtext.net/open/?cpal\n12. http://opensource.org/\n13. http://www.fsf.org\n14. https://github.com/facebook/platform\n15. https://github.com/reddit/reddit\n", "page": 269, "type": "text", "section": "Page 269"}
{"text": "The Architecture of\nOpen Source Applications\nAmy Brown and Greg Wilson (eds.)\nISBN 978-1-257-63801-7 \nLicense / Buy / Contribute\nChapter 20. Telepathy\nDanielle Madeley\nTelepathy1 is a modular framework for real-time communications that handles voice, video, text,\nfile transfer, and so on. What's unique about Telepathy is not that it abstracts the details of\nvarious instant messaging protocols, but that it provides the idea of communications as a service,\nin much the same way that printing is a service, available to many applications at once. To achieve\nthis Telepathy makes extensive use of the D-Bus messaging bus and a modular design.\nCommunications as a service is incredibly useful, because it allows us to break communications\nout of a single application. This enables lots of interesting use cases: being able to see a contact's\npresence in your email application; start communicating with her; launching a file transfer to a\ncontact straight from your file browser; or providing contact-to-contact collaboration within\napplications, known in Telepathy as Tubes.\nTelepathy was created by Robert McQueen in 2005 and since that time has been developed and\nmaintained by several companies and individual contributors including Collabora, the company co-\nfounded by McQueen.\nThe D-Bus Message Bus\nD-Bus is an asynchronous message bus for interprocess communication that forms the\nbackbone of most GNU/Linux systems including the GNOME and KDE desktop\nenvironments. D-Bus is a primarily a shared bus architecture: applications connect to a\nbus (identified by a socket address) and can either transmit a targeted message to\nanother application on the bus, or broadcast a signal to all bus members. Applications on\nthe bus have a bus address, similar to an IP address, and can claim a number of well-\nknown names, like DNS names, for example\norg.freedesktop.Telepathy.AccountManager. All processes communicate via the D-\nBus daemon, which handles message passing, and name registration.\nFrom the user's perspective, there are two buses available on every system. The system\nbus is a bus that allows the user to communicate with system-wide components (printers,\nbluetooth, hardware management, etc.) and is shared by all users on the system. The\nsession bus is unique to that user\u2014i.e., there is a session bus per logged-in user\u2014and is\nused for the user's applications to communicate with each other. When a lot of traffic is\nto be transmitted over the bus, it's also possible for applications to create their own\nprivate bus, or to create a peer-to-peer, unarbitrated bus with no dbus-daemon.\nSeveral libraries implement the D-Bus protocol and can communicate with the D-Bus\ndaemon, including libdbus, GDBus, QtDBus, and python-dbus. These libraries are\nresponsible for sending and receiving D-Bus messages, marshalling types from the\nlanguage's type system into D-Bus' type format and publishing objects on the bus.\nUsually, the libraries also provide convenience APIs for listing connected applications and\nactivatable applications, and requesting well-known names on the bus. At the D-Bus level,\nall of these are done by making method calls on an object published by dbus-daemon\nitself.\nFor more information on D-Bus, see\nhttp://www.freedesktop.org/wiki/Software/dbus.\n", "page": 270, "type": "text", "section": "Page 270"}
{"text": "20.1. Components of the Telepathy Framework\nTelepathy is modular, with each module communicating with the others via a D-Bus messaging\nbus. Most usually via the user's session bus. This communication is detailed in the Telepathy\nspecification2. The components of the Telepathy framework are as shown in Figure 20.1:\nA Connection Manager provides the interface between Telepathy and the individual\ncommunication services. For instance, there is a Connection Manager for XMPP, one for SIP,\none for IRC, and so on. Adding support for a new protocol to Telepathy is simply a matter of\nwriting a new Connection Manager.\nThe Account Manager service is responsible for storing the user's communications accounts\nand establishing a connection to each account via the appropriate Connection Manager when\nrequested.\nThe Channel Dispatcher's role is to listen for incoming channels signalled by each Connection\nManager and dispatch them to clients that indicate their ability to handle that type of channel,\nsuch as text, voice, video, file transfer, tubes. The Channel Dispatcher also provides a service\nso that applications, most importantly applications that are not Telepathy clients, can request\noutgoing channels and have them handled locally by the appropriate client. This allows an\napplication, such as an email application, to request a text chat with a contact, and have your\nIM client show a chat window.\nTelepathy clients handle or observe communications channels. They include both user\ninterfaces like IM and VoIP clients and services such the chat logger. Clients register\nthemselves with the Channel Dispatcher, giving a list of channel types they wish to handle or\nobserve.\nWithin the current implementation of Telepathy, the Account Manager and the Channel Dispatcher\nare both provided by a single process known as Mission Control.\nFigure 20.1: Example Telepathy Components\nThis modular design was based on Doug McIlroy's philosophy, \"Write programs that do one thing\nand do it well,\" and has several important advantages:\nRobustness: a fault in one component won't crash the entire service.\nEase of development: components can be replaced within a running system without affecting\nothers. It's possible to test a development version of one module against another known to\nbe good.\nLanguage independence: components can be written in any language that has a D-Bus\nbinding. If the best implementation of a given communications protocol is in a certain\nlanguage, you are able to write your Connection Manager in that language, and still have it\n", "page": 271, "type": "text", "section": "Page 271"}
{"text": "available to all Telepathy clients. Similarly, if you wish to develop your user interface in a\ncertain language, you have access to all available protocols.\nLicense independence: components can be under different software licenses that would be\nincompatible if everything was running as one process.\nInterface independence: multiple user interfaces can be developed on top of the same\nTelepathy components. This allows native interfaces for desktop environments and hardware\ndevices (e.g., GNOME, KDE, Meego, Sugar).\nSecurity: Components run in separate address spaces and with very limited privileges. For\nexample, a typical Connection Manager only needs access to the network and the D-Bus\nsession bus, making it possible to use something like SELinux to limit what a component can\naccess.\nThe Connection Manager manages a number of Connections, where each Connection represents\na logical connection to a communications service. There is one Connection per configured\naccount. A Connection will contain multiple Channels. Channels are the mechanism through which\ncommunications are carried out. A channel might be an IM conversation, voice or video call, file\ntransfer or some other stateful operation. Connections and channels are discussed in detail in\nSection 20.3.\n20.2. How Telepathy uses D-Bus\nTelepathy components communicate via a D-Bus messaging bus, which is usually the user's\nsession bus. D-Bus provides features common to many IPC systems: each service publishes\nobjects which have a strictly namespaced object path, like\n/org/freedesktop/Telepathy/AccountManager3. Each object implements a number of\ninterfaces. Again strictly namespaced, these have forms like\norg.freedesktop.DBus.Properties and ofdT.Connection. Each interface provides\nmethods, signals and properties that you can call, listen to, or request.\nFigure 20.2: Conceptual Representation of Objects Published by a D-Bus Service\nPublishing D-Bus Objects\nPublishing D-Bus objects is handled entirely by the D-Bus library being used. In effect it is\na mapping from a D-Bus object path to the software object implementing those interfaces.\nThe paths of objects being published by a service are exposed by the optional\norg.freedesktop.DBus.Introspectable interface.\nWhen a service receives an incoming method call with a given destination path (e.g.,\n/ofdT/AccountManager), the D-Bus library is responsible for locating the software\nobject providing that D-Bus object and then making the appropriate method call on that\nobject.\nThe interfaces, methods, signal and properties provided by Telepathy are detailed in an XML-based\nD-Bus IDL that has been expanded to include more information. The specification can be parsed\n", "page": 272, "type": "text", "section": "Page 272"}
{"text": "to generate documentation and language bindings.\nTelepathy services publish a number of objects onto the bus. Mission Control publishes objects for\nthe Account Manager and Channel Dispatcher so that their services can be accessed. Clients\npublish a Client object that can be accessed by the Channel Dispatcher. Finally, Connection\nManagers publish a number of objects: a service object that can be used by the Account Manager\nto request new connections, an object per open connection, and an object per open channel.\nAlthough D-Bus objects do not have a type (only interfaces), Telepathy simulates types several\nways. The object's path tells us whether the object is a connection, channel, client, and so on,\nthough generally you already know this when you request a proxy to it. Each object implements\nthe base interface for that type, e.g., ofdT.Connection or ofdT.Channel. For channels this is\nsort of like an abstract base class. Channel objects then have a concrete class defining their\nchannel type. Again, this is represented by a D-Bus interface. The channel type can be learned by\nreading the ChannelType property on the Channel interface.\nFinally, each object implements a number of optional interfaces (unsurprisingly also represented as\nD-Bus interfaces), which depend on the capabilities of the protocol and the Connection Manager.\nThe interfaces available on a given object are available via the Interfaces property on the object's\nbase class.\nFor Connection objects of type ofdT.Connection, the optional interfaces have names like\nofdT.Connection.Interface.Avatars (if the protocol has a concept of avatars),\nodfT.Connection.Interface.ContactList (if the protocol provides a contact roster\u2014not all\ndo) and odfT.Connection.Interface.Location (if a protocol provides geolocation\ninformation). For Channel objects, of type ofdT.Channel, the concrete classes have interface\nnames of the form ofdT.Channel.Type.Text, odfT.Channel.Type.Call and\nodfT.Channel.Type.FileTransfer. Like Connections, optional interface have names likes\nodfT.Channel.Interface.Messages (if this channel can send and receive text messages) and\nodfT.Channel.Interface.Group (if this channel is to a group containing multiple contacts, e.g.,\na multi-user chat). So, for example, a text channel implements at least the ofdT.Channel,\nofdT.Channel.Type.Text and Channel.Interface.Messages interfaces. If it's a multi-user\nchat, it will also implement odfT.Channel.Interface.Group.\nWhy an Interfaces Property and not D-Bus Introspection?\nYou might wonder why each base class implements an Interfaces property, instead of\nrelying on D-Bus' introspection capabilities to tell us what interfaces are available. The\nanswer is that different channel and connection objects may offer different interfaces to\neach other, depending on the capabilities of the channel or connection, but that most of\nthe implementations of D-Bus introspection assume that all objects of the same object\nclass will have the same interfaces. For example, in telepathy-glib, the D-Bus\ninterfaces listed by D-Bus introspection are retrieved from the object interfaces a class\nimplements, which is statically defined at compile time. We work around this by having D-\nBus introspection provide data for all the interfaces that could exist on an object, and use\nthe Interfaces property to indicate which ones actually do.\nAlthough D-Bus itself provides no sanity checking that connection objects only have connection-\nrelated interfaces and so forth (since D-Bus has no concept of types, only arbitrarily named\ninterfaces), we can use the information contained within the Telepathy specification to provide\nsanity checking within the Telepathy language bindings.\nWhy and How the Specification IDL was Expanded\nThe existing D-Bus specification IDL defines the names, arguments, access restrictions\nand D-Bus type signatures of methods, properties and signals. It provides no support for\ndocumentation, binding hints or named types.\n", "page": 273, "type": "text", "section": "Page 273"}
{"text": "To resolve these limitations, a new XML namespace was added to provide the required\ninformation. This namespace was designed to be generic so that it could be used by other\nD-Bus APIs. New elements were added to include inline documentation, rationales,\nintroduction and deprecation versions and potential exceptions from methods.\nD-Bus type signatures are the low-level type notation of what is serialized over the bus. A\nD-Bus type signature may look like (ii) (which is a structure containing two int32s), or it\nmay be more complex. For example, a{sa(usuu)}, is a map from string to an array of\nstructures containing uint32, string, uint32, uint32 (Figure 20.3). These types, while\ndescriptive of the data format, provide no semantic meaning to the information contained\nin the type.\nIn an effort to provide semantic clarity for programmers and strengthen the typing for\nlanguage bindings, new elements were added to name simple types, structs, maps,\nenums, and flags, providing their type signature, as well as documentation. Elements were\nalso added in order to simulate object inheritance for D-Bus objects.\nFigure 20.3: D-Bus Types (ii) and a{sa(usuu)}\n20.2.1. Handles\nHandles are used in Telepathy to represent identifiers (e.g., contacts and room names). They are\nan unsigned integer value assigned by the connection manager, such that the tuple (connection,\nhandle type, handle) uniquely refers to a given contact or room.\nBecause different communications protocols normalize identifiers in different ways (e.g., case\nsensitivity, resources), handles provide a way for clients to determine if two identifiers are the\nsame. They can request the handle for two different identifiers, and if the handle numbers match,\nthen the identifiers refer to the same contact or room.\nIdentifier normalization rules are different for each protocol, so it is a mistake for clients to\ncompare identifier strings to compare identifiers. For example, escher@tuxedo.cat/bed and\nescher@tuxedo.cat/litterbox are two instances of the same contact (escher@tuxedo.cat)\nin the XMPP protocol, and therefore have the same handle. It is possible for clients to request\nchannels by either identifier or handle, but they should only ever use handles for comparison.\n", "page": 274, "type": "text", "section": "Page 274"}
{"text": "20.2.2. Discovering Telepathy Services\nSome services, such as the Account Manager and the Channel Dispatcher, which always exist,\nhave well known names that are defined in the Telepathy specification. However, the names of\nConnection Managers and clients are not well-known, and must be discovered.\nThere's no service in Telepathy responsible for the registration of running Connection Managers\nand Clients. Instead, interested parties listen on the D-Bus for the announcement of a new\nservice. The D-Bus bus daemon will emit a signal whenever a new named D-Bus service appears\non the bus. The names of Clients and Connection Managers begin with known prefixes, defined by\nthe specification, and new names can be matched against these.\nThe advantage of this design is that it's completely stateless. When a Telepathy component is\nstarting up, it can ask the bus daemon (which has a canonical list, based on its open connections)\nwhat services are currently running. For instance, if the Account Manager crashes, it can look to\nsee what connections are running, and reassociate those with its account objects.\nConnections are Services Too\nAs well as the Connection Managers themselves, the connections are also advertised as\nD-Bus services. This hypothetically allows for the Connection Manager to fork each\nconnection off as a separate process, but to date no Connection Manager like this has\nbeen implemented. More practically, it allows all running connections to be discovered by\nquerying the D-Bus bus daemon for all services beginning with ofdT.Connection.\nThe Channel Dispatcher also uses this method to discover Telepathy clients. These begin with the\nname ofdT.Client, e.g., ofdT.Client.Logger.\n20.2.3. Reducing D-Bus Traffic\nOriginal versions of the Telepathy specification created an excessive amount of D-Bus traffic in the\nform of method calls requesting information desired by lots of consumers on the bus. Later\nversions of the Telepathy have addressed this through a number of optimizations.\nIndividual method calls were replaced by D-Bus properties. The original specification included\nseparate method calls for object properties: GetInterfaces, GetChannelType, etc. Requesting\nall the properties of an object required several method calls, each with its own calling overhead. By\nusing D-Bus properties, everything can be requested at once using the standard GetAll method.\nFurthermore, quite a number of properties on a channel are immutable for the lifetime of the\nchannel. These include things like the channel's type, interfaces, who it's connected to and the\nrequestor. For a file transfer channel, for example, it also includes things like the file size and its\ncontent type.\nA new signal was added to herald the creation of channels (both incoming and in response to\noutgoing requests) that includes a hash table of the immutable properties. This can be passed\ndirectly to the channel proxy constructor (see Section 20.4), which saves interested clients from\nhaving to request this information individually.\nUser avatars are transmitted across the bus as byte arrays. Although Telepathy already used\ntokens to refer to avatars, allowing clients to know when they needed a new avatar and to save\ndownloading unrequired avatars, each client had to individually request the avatar via a\nRequestAvatar method that returned the avatar as its reply. Thus, when the Connection\nManager signalled that a contact had updated its avatar, several individual requests for the avatar\nwould be made, requiring the avatar to be transmitted over the message bus several times.\nThis was resolved by adding a new method which did not return the avatar (it returns nothing).\nInstead, it placed the avatar in a request queue. Retrieving the avatar from the network would\nresult in a signal, AvatarRetrieved, that all interested clients could listen to. This means the\n", "page": 275, "type": "text", "section": "Page 275"}
{"text": "avatar data only needs to be transmitted over the bus once, and will be available to all the\ninterested clients. Once the client's request was in the queue, all further client requests can be\nignored until the emission of the AvatarRetrieved.\nWhenever a large number of contacts need to be loaded (i.e., when loading the contact roster), a\nsignificant amount of information needs to be requested: their aliases, avatars, capabilities, and\ngroup memberships, and possibly their location, address, and telephone numbers. Previously in\nTelepathy this would require one method call per information group (most API calls, such as\nGetAliases already took a list of contacts), resulting in half a dozen or more method calls.\nTo solve this, the Contacts interface was introduced. It allowed information from multiple\ninterfaces to be returned via a single method call. The Telepathy specification was expanded to\ninclude Contact Attributes: namespaced properties returned by the GetContactAttributes\nmethod that shadowed method calls used to retrieve contact information. A client calls\nGetContactAttributes with a list of contacts and interfaces it is interested in, and gets back a\nmap from contacts to a map of contact attributes to values.\nA bit of code will make this clearer. The request looks like this:\nconnection[CONNECTION_INTERFACE_CONTACTS].GetContactAttributes(\n  [ 1, 2, 3 ], # contact handles\n  [ \"ofdT.Connection.Interface.Aliasing\",\n    \"ofdT.Connection.Interface.Avatars\",\n    \"ofdT.Connection.Interface.ContactGroups\",\n    \"ofdT.Connection.Interface.Location\"\n  ],\n  False # don't hold a reference to these contacts\n)\nand the reply might look like this:\n{ 1: { 'ofdT.Connection.Interface.Aliasing/alias': 'Harvey Cat',\n       'ofdT.Connection.Interface.Avatars/token': hex string,\n       'ofdT.Connection.Interface.Location/location': location,\n       'ofdT.Connection.Interface.ContactGroups/groups': [ 'Squid House' ],\n       'ofdT.Connection/contact-id': 'harvey@nom.cat'\n     },\n  2: { 'ofdT.Connection.Interface.Aliasing/alias': 'Escher Cat',\n       'ofdT.Connection.Interface.Avatars/token': hex string,\n       'ofdT.Connection.Interface.Location/location': location,\n       'ofdT.Connection.Interface.ContactGroups/groups': [],\n       'ofdT.Connection/contact-id': 'escher@tuxedo.cat'\n     },\n  3: { 'ofdT.Connection.Interface.Aliasing/alias': 'Cami Cat',\n        \u22ee    \u22ee    \u22ee\n     }\n}\n20.3. Connections, Channels and Clients\n20.3.1. Connections\nA Connection is created by the Connection Manager to establish a connection to a single\nprotocol/account. For example, connecting to the XMPP accounts escher@tuxedo.cat and\ncami@egg.cat would result in two Connections, each represented by a D-Bus object.\nConnections are typically set up by the Account Manager, for the currently enabled accounts.\nThe Connection provides some mandatory functionality for managing and monitoring the\nconnection status and for requesting channels. It can then also provide a number of optional\nfeatures, depending on the features of the protocol. These are provided as optional D-Bus\n", "page": 276, "type": "text", "section": "Page 276"}
{"text": "features, depending on the features of the protocol. These are provided as optional D-Bus\ninterfaces (as discussed in the previous section) and listed by the Connection's Interfaces\nproperty.\nTypically Connections are managed by the Account Manager, created using the properties of the\nrespective accounts. The Account Manager will also synchronize the user's presence for each\naccount to its respective connection and can be asked to provide the connection path for a given\naccount.\n20.3.2. Channels\nChannels are the mechanism through which communications are carried out. A channel is typically\nan IM conversation, voice or video call or file transfer, but channels are also used to provide some\nstateful communication with the server itself, (e.g., to search for chat rooms or contacts). Each\nchannel is represented by a D-Bus object.\nChannels are typically between two or more users, one of whom is yourself. They typically have a\ntarget identifier, which is either another contact, in the case of one-to-one communication; or a\nroom identifier, in the case of multi-user communication (e.g., a chat room). Multi-user channels\nexpose the Group interface, which lets you track the contacts who are currently in the channel.\nChannels belong to a Connection, and are requested from the Connection Manager, usually via the\nChannel Dispatcher; or they are created by the Connection in response to a network event (e.g.,\nincoming chat), and handed to the Channel Dispatcher for dispatching.\nThe type of channel is defined by the channel's ChannelType property. The core features,\nmethods, properties, and signals that are needed for this channel type (e.g., sending and receiving\ntext messages) are defined in the appropriate Channel.Type D-Bus interface, for instance\nChannel.Type.Text. Some channel types may implement optional additional features (e.g.,\nencryption) which appear as additional interfaces listed by the channel's Interfaces property. An\nexample text channel that connects the user to a multi-user chatroom might have the interfaces\nshown in Table 20.1.\nProperty\nPurpose\nodfT.Channel\nFeatures common to all channels\nodfT.Channel.Type.Text\nThe Channel Type, includes features common to text channels\nodfT.Channel.Interface.Messages Rich-text messaging\nodfT.Channel.Interface.Group\nList, track, invite and approve members in this channel\nodfT.Channel.Interface.Room\nRead and set properties such as the chatroom's subject\nTable 20.1: Example Text Channel\nContact List Channels: A Mistake\nIn the first versions of the Telepathy specification, contact lists were considered a type of\nchannel. There were several server-defined contact lists (subscribed users, publish-to\nusers, blocked users), that could be requested from each Connection. The members of\nthe list were then discovered using the Group interface, like for a multi-user chat.\nOriginally this would allow for channel creation to occur only once the contact list had\nbeen retrieved, which takes time on some protocols. A client could request the channel\nwhenever it liked, and it would be delivered once ready, but for users with lots of contacts\nthis meant the request would occasionally time out. Determining the\nsubscription/publish/blocked status of a client required checking three channels.\nContact Groups (e.g., Friends) were also exposed as channels, one channel per group.\nThis proved extremely difficult for client developers to work with. Operations like getting\nthe list of groups a contact was in required a significant amount of code in the client.\nFurther, with the information only available via channels, properties such as a contact's\ngroups or subscription state could not be published via the Contacts interface.\n", "page": 277, "type": "text", "section": "Page 277"}
{"text": "Both channel types have since been replaced by interfaces on the Connection itself which\nexpose contact roster information in ways more useful to client authors, including\nsubscription state of a contact (an enum), groups a contact is in, and contacts in a group.\nA signal indicates when the contact list has been prepared.\n20.3.3. Requesting Channels, Channel Properties and Dispatching\nChannels are requested using a map of properties you wish the desired channel to possess.\nTypically, the channel request will include the channel type, target handle type (contact or room)\nand target. However, a channel request may also include properties such as the filename and\nfilesize for file transfers, whether to initially include audio and video for calls, what existing\nchannels to combine into a conference call, or which contact server to conduct a contact search\non.\nThe properties in the channel request are properties defined by interfaces of the Telepathy spec,\nsuch as the ChannelType property (Table 20.2). They are qualified with the namespace of the\ninterface they come from Properties which can be included in channel requests are marked as\nrequestable in the Telepathy spec.\nProperty\nValue\nofdT.Channel.ChannelType\nofdT.Channel.Type.Text\nofdT.Channel.TargetHandleType Handle_Type_Contact (1)\nofdT.Channel.TargetID\nescher@tuxedo.cat\nTable 20.2: Example Channel Requests\nThe more complicated example in Table 20.3 requests a file transfer channel. Notice how the\nrequested properties are qualified by the interface from which they come. (For brevity, not all\nrequired properties are shown.)\nProperty\nValue\nofdT.Channel.ChannelType\nofdT.Channel.Type.FileTransfer\nofdT.Channel.TargetHandleType\nHandle_Type_Contact (1)\nofdT.Channel.TargetID\nescher@tuxedo.cat\nofdT.Channel.Type.FileTransfer.Filename\nmeow.jpg\nofdT.Channel.Type.FileTransfer.ContentType image/jpeg\nTable 20.3: File Transfer Channel Request\nChannels can either be created or ensured. Ensuring a channel means creating it only if it does not\nalready exist. Asking to create a channel will either result in a completely new and separate channel\nbeing created, or in an error being generated if multiple copies of such a channel cannot exist.\nTypically you wish to ensure text channels and calls (i.e., you only need one conversation open\nwith a person, and in fact many protocols do not support multiple separate conversations with the\nsame contact), and wish to create file transfers and stateful channels.\nNewly created channels (requested or otherwise) are announced by a signal from the Connection.\nThis signal includes a map of the channel's immutable properties. These are the properties which\nare guaranteed not to change throughout the channel's lifetime. Properties which are considered\nimmutable are marked as such in the Telepathy spec, but typically include the channel's type,\ntarget handle type, target, initiator (who created the channel) and interfaces. Properties such as\nthe channel's state are obviously not included.\nOld-School Channel Requesting\n", "page": 278, "type": "text", "section": "Page 278"}
{"text": "Channels were originally requested simply by type, handle type and target handle. This\nwasn't sufficiently flexible because not all channels have a target (e.g., contact search\nchannels), and some channels require additional information included in the initial channel\nrequest (e.g., file transfers, requesting voicemails and channels for sending SMSes).\nIt was also discovered that two different behaviors might be desired when a channel was\nrequested (either to create a guaranteed unique channel, or simply ensure a channel\nexisted), and until this time the Connection had been responsible for deciding which\nbehavior would occur. Hence, the old method was replaced by the newer, more flexible,\nmore explicit ones.\nReturning a channel's immutable properties when you create or ensure the channel makes it much\nfaster to create a proxy object for the channel. This is information we now don't have to request.\nThe map in Table 20.4 shows the immutable properties that might be included when we request a\ntext channel (i.e., using the channel request in Table 20.3). Some properties (including\nTargetHandle and InitiatorHandle) have been excluded for brevity.\nProperty\nValue\nofdT.Channel.ChannelType\nChannel.Type.Text\nofdT.Channel.Interfaces\n{[} Channel.Interface.Messages,\nChannel.Interface.Destroyable,\nChannel.Interface.ChatState {]}\nofdT.Channel.TargetHandleType\nHandle_Type_Contact (1)\nofdT.Channel.TargetID\nescher@tuxedo.cat\nofdT.Channel.InitiatorID\ndanielle.madeley@collabora.co.uk\nofdT.Channel.Requested\nTrue\nofdT.Channel.Interface.Messages.SupportedContentTypes {[} text/html, text/plain {]}\nTable 20.4: Example Immutable Properties Returned by a New Channel\nThe requesting program typically makes a request for a channel to the Channel Dispatcher,\nproviding the account the request is for, the channel request, and optionally the name of a the\ndesired handler (useful if the program wishes to handle the channel itself). Passing the name of an\naccount instead of a connection means that the Channel Dispatcher can ask the Account Manager\nto bring an account online if required.\nOnce the request is complete, the Channel Dispatcher will either pass the channel to the named\nHandler, or locate an appropriate Handler (see below for discussion on Handlers and other\nclients). Making the name of the desired Handler optional makes it possible for programs that have\nno interest in communication channels beyond the initial request to request channels and have\nthem handled by the best program available (e.g., launching a text chat from your email client).\nFigure 20.4: Channel Request and Dispatching\nThe requesting program makes a channel request to the Channel Dispatcher, which in turn\nforwards the request to the appropriate Connection. The Connection emits the NewChannels\nsignal which is picked up by the Channel Dispatcher, which then finds the appropriate client to\nhandle the channel. Incoming, unrequested channels are dispatched in much the same way, with a\n", "page": 279, "type": "text", "section": "Page 279"}
{"text": "Incoming, unrequested channels are dispatched in much the same way, with a\nsignal from the Connection that is picked up by the Channel Dispatcher, but obviously without the\ninitial request from a program.\n20.3.4. Clients\nClients handle or observe incoming and outgoing communications channels. A client is anything\nthat is registered with the Channel Dispatcher. There are three types of clients (though a single\nclient may be two, or all three, types if the developer wishes):\nObservers: Observe channels without interacting with them. Observers tend to be used for\nchat and activity logging (e.g., incoming and outgoing VoIP calls).\nApprovers: Responsible for giving users an opportunity to accept or reject an incoming\nchannel.\nHandlers: Actually interact with the channel. That might be acknowledging and sending text\nmessages, sending or receiving a file, etc. A Handler tends to be associated with a user\ninterface.\nClients offer D-Bus services with up to three interfaces: Client.Observer, Client.Approver,\nand Client.Handler. Each interface provides a method that the Channel Dispatcher can call to\ninform the client about a channel to observe, approve or handle.\nThe Channel Dispatcher dispatches the channel to each group of clients in turn. First, the channel\nis dispatched to all appropriate Observers. Once they have all returned, the channel is dispatched\nto all the appropriate Approvers. Once the first Approver has approved or rejected the channel, all\nother Approvers are informed and the channel is finally dispatched to the Handler. Channel\ndispatching is done in stages because Observers might need time to get set up before the Handler\nbegins altering the channel.\nClients expose a channel filter property which is a list of filters read by the Channel Dispatcher so\nthat it knows what sorts of channels a client is interested in. A filter must include at least the\nchannel type, and target handle type (e.g., contact or room) that the client is interested in, but it\ncan contain more properties. Matching is done against the channel's immutable properties, using\nsimple equality for comparison. The filter in Table 20.5 matches all one-to-one text channels.\nProperty\nValue\nofdT.Channel.ChannelType\nChannel.Type.Text\nofdT.Channel.TargetHandleType Handle_Type_Contact (1)\nTable 20.5: Example Channel Filter\nClients are discoverable via D-Bus because they publish services beginning with the well-known\nname ofdT.Client (for example ofdT.Client.Empathy.Chat). They can also optionally install\na file which the Channel Dispatcher will read specifying the channel filters. This allows the Channel\nDispatcher to start a client if it is not already running. Having clients be discoverable in this way\nmakes the choice of user interface configurable and changeable at any time without having to\nreplace any other part of Telepathy.\nAll or Nothing\nIt is possible to provide a filter indicating you are interested in all channels, but in practice\nthis is only useful as an example of observing channels. Real clients contain code that is\nspecific to channel types.\nAn empty filter indicates a Handler is not interested in any channel types. However it is still\npossible to dispatch a channel to this handler if you do so by name. Temporary Handlers\nwhich are created on demand to handle a specific channel use such a filter.\n", "page": 280, "type": "text", "section": "Page 280"}
{"text": "20.4. The Role of Language Bindings\nAs Telepathy is a D-Bus API, and thus can driven by any programming language that supports D-\nBus. Language bindings are not required for Telepathy, but they can be used to provide a\nconvenient way to use it.\nLanguage bindings can be split into two groups: low-level bindings that include code generated\nfrom the specification, constants, method names, etc.; and high-level bindings, which are hand-\nwritten code that makes it easier for programmers to do things using Telepathy. Examples of\nhigh-level bindings are the GLib and Qt4 bindings. Examples of low-level bindings are the Python\nbindings and the original libtelepathy C bindings, though the GLib and Qt4 bindings include a low-\nlevel binding.\n20.4.1. Asynchronous Programming\nWithin the language bindings, all method calls that make requests over D-Bus are asynchronous:\nthe request is made, and the reply is given in a callback. This is required because D-Bus itself is\nasynchronous.\nLike most network and user interface programming, D-Bus requires the use of an event loop to\ndispatch callbacks for incoming signals and method returns. D-Bus integrates well with the GLib\nmainloop used by the GTK+ and Qt toolkits.\nSome D-Bus language bindings (such as dbus-glib) provide a pseudo-synchronous API, where the\nmain loop is blocked until the method reply is returned. Once upon a time this was exposed via the\ntelepathy-glib API bindings. Unfortunately using pseudo-synchronous API turns out to be fraught\nwith problems, and was eventually removed from telepathy-glib.\nWhy Pseudo-Synchronous D-Bus Calls Don't Work\nThe pseudo-synchronous interface offered by dbus-glib and other D-Bus bindings is\nimplemented using a request-and-block technique. While blocking, only the D-Bus socket\nis polled for new I/O and any D-Bus messages that are not the response to the request\nare queued for later processing.\nThis causes several major and inescapable problems:\nThe caller is blocked while waiting for the request to be answered. It (and its user\ninterface, if any) will be completely unresponsive. If the request requires accessing\nthe network, that takes time; if the callee has locked up, the caller will be\nunresponsive until the call times out. \nThreading is not a solution here because threading is just another way of making\nyour calling asynchronous. Instead you may as well make asynchronous calls where\nthe responses come in via the existing event loop.\nMessages may be reordered. Any messages received before the watched-for reply\nwill be placed on a queue and delivered to the client after the reply. \nThis causes problems in situations where a signal indicating a change of state (i.e.,\nthe object has been destroyed) is now received after the method call on that object\nfails (i.e., with the exception UnknownMethod). In this situation, it is hard to know\nwhat error to display to the user. Whereas if we receive a signal first, we can cancel\npending D-Bus method calls, or ignore their responses.\nTwo processes making pseudo-blocking calls on each other can deadlock, with each\nwaiting for the other to respond to its query. This scenario can occur with processes\nthat are both a D-Bus service and call other D-Bus services (for example, Telepathy\nclients). The Channel Dispatcher calls methods on clients to dispatch channels, but\nclients also call methods on the Channel Dispatcher to request the opening of new\nchannels (or equally they call the Account Manager, which is part of the same\nprocess).\n", "page": 281, "type": "text", "section": "Page 281"}
{"text": "Method calls in the first Telepathy bindings, generated in C, simply used typedef callback functions.\nYour callback function simply had to implement the same type signature.\ntypedef void (*tp_conn_get_self_handle_reply) (\n    DBusGProxy *proxy,\n    guint handle,\n    GError *error,\n    gpointer userdata\n);\nThis idea is simple, and works for C, so was continued into the next generation of bindings.\nIn recent years, people have developed a way to use scripting languages such as Javascript and\nPython, as well as a C#-like language called Vala, that use GLib/GObject-based APIs via a tool called\nGObject-Introspection. Unfortunately, it's extremely difficult to rebind these types of callbacks into\nother languages, so newer bindings are designed to take advantage of the asynchronous callback\nfeatures provided by the languages and GLib.\n20.4.2. Object Readiness\nIn a simple D-Bus API, such as the low-level Telepathy bindings, you can start making method calls\nor receive signals on a D-Bus object simply by creating a proxy object for it. It's as simple as giving\nan object path and interface name and getting started.\nHowever, in Telepathy's high-level API, we want our object proxies to know what interface are\navailable, we want common properties for the object type to be retrieved (e.g., the channel type,\ntarget, initiator), and we want to determine and track the object's state or status (e.g., the\nconnection status).\nThus, the concept of readiness exists for all proxy objects. By making a method call on a proxy\nobject, you are able to asynchronously retrieve the state for that object and be notified when state\nis retrieved and the object is ready for use.\nSince not all clients implement, or are interested in, all the features of a given object, readiness for\nan object type is separated into a number of possible features. Each object implements a core\nfeature, which will prepare crucial information about the object (i.e., its Interfaces property and\nbasic state), plus a number of optional features for additional state, which might include extra\nproperties or state-tracking. Specific examples of additional features you can ready on various\nproxies are contact info, capabilities, geolocation information, chat states (such as \"Escher is\ntyping\u2026\") and user avatars.\nFor example, connection object proxies have:\na core feature which retrieves the interface and connection status;\nfeatures to retrieve the requestable channel classes and support contact info; and\na feature to establish a connection and return ready when connected.\nThe programmer requests that the object is readied, providing a list of features in which they are\ninterested and a callback to call when all of those features are ready. If all the features are already\nready, the callback can be called immediately, else the callback is called once all the information for\nthose features is retrieved.\n20.5. Robustness\nOne of the key advantages of Telepathy is its robustness. The components are modular, so a\ncrash in one component should not bring down the whole system. Here are some of the features\nthat make Telepathy robust:\nThe Account Manager and Channel Dispatcher can recover their state. When Mission Control\n", "page": 282, "type": "text", "section": "Page 282"}
{"text": "(the single process that includes the Account Manager and Channel Dispatcher) starts, it\nlooks at the names of services currently registered on the user's session bus. Any\nConnections it finds that are associated with a known account are reassociated with that\naccount (rather than a new connection being established), and running clients are queried for\nthe list of channels they're handling.\nIf a client disappears while a channel it's handling is open, the Channel Dispatcher will\nrespawn it and reissue the channel. \nIf a client repeatedly crashes the Channel Dispatcher can attempt to launch a different client,\nif available, or else it will close the channel (to prevent the client repeatedly crashing on data it\ncan't handle). \nText messages require acknowledgment before they will disappear from the list of pending\nmessages. A client is only meant to acknowledge a message once it is sure the user has seen\nit (that is, displayed the message in a focused window). This way if the client crashes trying\nto render the message, the channel will still have the previously undisplayed message in the\npending message queue.\nIf a Connection crashes, the Account Manager will respawn it. Obviously the content of any\nstateful channels will be lost, but it will only affect the Connections running in that process\nand no others. Clients can monitor the state of the connections and simply re-request\ninformation like the contact roster and any stateless channels.\n20.6. Extending Telepathy: Sidecars\nAlthough the Telepathy specification tries to cover a wide range of features exported by\ncommunication protocols, some protocols are themselves extensible4. Telepathy's developers\nwanted to make it possible extend your Telepathy connections to make use of such extensions\nwithout having to extend the Telepathy specification itself. This is done through the use of\nsidecars.\nSidecars are typically implemented by plugins in a Connection Manager. Clients call a method\nrequesting a sidecar that implements a given D-Bus interface. For example, someone's\nimplementation of XEP-0016 privacy lists might implement an interface named\ncom.example.PrivacyLists. The method then returns a D-Bus object provided by the plugin,\nwhich should implement that interface (and possibly others). The object exists alongside the main\nConnection object (hence the name sidecar, like on a motorcycle).\nThe History of Sidecars\nIn the early days of Telepathy, the One Laptop Per Child project needed to support\ncustom XMPP extensions (XEPs) to share information between devices. These were\nadded directly to Telepathy-Gabble (the XMPP Connection Manager), and exposed via\nundocumented interfaces on the Connection object. Eventually, with more developers\nwanting support for specific XEPs which have no analogue in other communications\nprotocols, it was agreed that a more generic interface for plugins was needed.\n20.7. A Brief Look Inside a Connection Manager\nMost Connection Managers are written using the C/GLib language binding, and a number of high-\nlevel base classes have been developed to make writing a Connection Manager easier. As\ndiscussed previously, D-Bus objects are published from software objects that implement a\nnumber of software interfaces that map to D-Bus interfaces. Telepathy-GLib provides base objects\nto implement the Connection Manager, Connection and Channel objects. It also provides an\ninterface to implement a Channel Manager. Channel Managers are factories that can be used by\nthe BaseConnection to instantiate and manage channel objects for publishing on the bus.\nThe bindings also provide what are known as mixins. These can be added to a class to provide\nadditional functionality, abstract the specification API and provide backwards compatibility for new\nand deprecated versions of an API through one mechanism. The most commonly used mixin is\n", "page": 283, "type": "text", "section": "Page 283"}
{"text": "one that adds the D-Bus properties interface to an object. There are also mixins to implement the\nofdT.Connection.Interface.Contacts and ofdT.Channel.Interface.Group interfaces\nand mixins making it possible to implement the old and new presence interfaces, and old and new\ntext message interfaces via one set of methods.\nFigure 20.5: Example Connection Manager Architecture\nUsing Mixins to Solve API Mistakes\nOne place where mixins have been used to solve a mistake in the Telepathy specification is\nthe TpPresenceMixin. The original interface exposed by Telepathy\n(odfT.Connection.Interface.Presence) was incredibly complicated, hard to\nimplement for both Connections and Clients, and exposed functionality that was both\nnonexistent in most communications protocols, and very rarely used in others. The\ninterface was replaced by a much simpler interface\n(odfT.Connection.Interface.SimplePresence), which exposed all the functionality\nthat users cared about and had ever actually been implemented in the connection\nmanagers.\nThe presence mixin implements both interfaces on the Connection so that legacy clients\ncontinue to work, but only at the functionality level of the simpler interface.\n20.8. Lessons Learned\nTelepathy is an excellent example of how to build a modular, flexible API on top of D-Bus. It shows\nhow you can develop an extensible, decoupled framework on top of D-Bus. One which requires no\ncentral management daemon and allows components to be restartable, without loss of data in any\nother component. Telepathy also shows how you can use D-Bus efficiently and effectively,\nminimizing the amount of traffic you transmit on the bus.\nTelepathy's development has been iterative, improving its use of D-Bus as time goes on. Mistakes\nwere made, and lessons have been learned. Here are some of the important things we learned in\ndesigning the architecture of Telepathy:\nUse D-Bus properties; don't require dozens of small D-Bus method calls to look up\ninformation. Every method call has a round-trip time. Rather than making lots of individual\ncalls (e.g., GetHandle, GetChannelType, GetInterfaces) use D-Bus properties and return\nall the information via a single call to GetAll.\nProvide as much information as you can when announcing new objects. The first thing clients\nused to do when they learned about a new object was to request all of its properties to learn\nwhether they were even interested in the object. By including the immutable properties of an\nobject in the signal announcing the object, most clients can determine their interest in the\nobject without making any method calls. Furthermore, if they are interested in the object,\nthey do not have to bother requesting any of its immutable properties.\nThe Contacts interface allows requesting information from multiple interfaces at once.\nRather than making numerous GetAll calls to retrieve all the information for a contact, the\nContacts interface lets us request all the information at once, saving a number of D-Bus\n", "page": 284, "type": "text", "section": "Page 284"}
{"text": "round trips.\nDon't use abstractions that don't quite fit. Exposing the contact roster and contact groups\nas channels implementing the Group interface seemed like a good idea because it used\nexisting abstractions rather than requiring additional interfaces. However, it made\nimplementing clients difficult and was ultimately not suitable.\nEnsure your API will meet your future needs. The original channel requesting API was very\nrigid, only permitting very basic channel requests. This did not meet our needs when needing\nto request channels that required more information. This API had to be replaced with one\nthat had significantly more flexibility.\nFootnotes\n1. http://telepathy.freedesktop.org/, or see the developers' manual at\nhttp://telepathy.freedesktop.org/doc/book/\n2. http://telepathy.freedesktop.org/spec/\n3. From here on, /org/freedesktop/Telepathy/ and org.freedesktop.Telepathy will be\nabbreviated to ofdT to save space.\n4. E.g., the Extensible Messaging and Presence Protocol (XMPP).\n", "page": 285, "type": "text", "section": "Page 285"}
{"text": "The Architecture of\nOpen Source Applications\nAmy Brown and Greg Wilson (eds.)\nISBN 978-1-257-63801-7 \nLicense / Buy / Contribute\nChapter 21. Thousand Parsec\nAlan Laudicina and Aaron Mavrinac\nA vast star empire encompasses a hundred worlds, stretching a thousand parsecs across space.\nUnlike some other areas of the galaxy, few warriors live here; this is an intellectual people, with a\nrich cultural and academic tradition. Their magnificent planets, built turn by turn around great\nuniversities of science and technology, are a beacon of light to all in this age of peace and\nprosperity. Starships arrive from the vast reaches of the quadrant and beyond, bearing the\nforemost researchers from far and wide. They come to contribute their skills to the most\nambitious project ever attempted by sentient beings: the development of a decentralized\ncomputer network to connect the entire galaxy, with all its various languages, cultures, and\nsystems of law.\nThousand Parsec is more than a video game: it is a framework, with a complete toolkit for building\nmultiplayer, turn-based space empire strategy games. Its generic game protocol allows diverse\nimplementations of client, server, and AI software, as well as a vast array of possible games.\nThough its size has made planning and execution challenging, forcing contributors to walk a thin\nline between excessively vertical and excessively horizontal development, it also makes it a rather\ninteresting specimen when discussing the architecture of open source applications.\nThe journalist's label for the genre Thousand Parsec games inhabit is \"4X\"\u2014shorthand for \"explore,\nexpand, exploit, and exterminate,\" the modus operandi of the player controlling an empire1.\nTypically in the 4X genre of games, players will scout to reveal the map (explore), create new\nsettlements or extend the influence of existing ones (expand), gather and use resources in areas\nthey control (exploit), and attack and eliminate rival players (exterminate). The emphasis on\neconomic and technological development, micromanagement, and variety of routes to supremacy\nyield a depth and complexity of gameplay unparalleled within the greater strategy genre.\nFrom a player's perspective, three main components are involved in a game of Thousand Parsec.\nFirst, there is the client: this is the application through which the player interacts with the universe.\nThis connects to a server over the network\u2014communicating using the all-important protocol\u2014to\nwhich other players' (or, in some cases, artificial intelligence) clients are also connected. The server\nstores the entire game state, updating clients at the start of each turn. Players can then perform\nvarious actions and communicate them back to the server, which computes the resulting state for\nthe next turn. The nature of the actions a player may perform is dictated by a ruleset: this in\nessence defines the game being played, implemented and enforced on the server side, and\nactualized for the player by any supporting client.\nBecause of the diversity of possible games, and the complexity of the architecture required to\nsupport this diversity, Thousand Parsec is an exciting project both for gamers and for developers.\nWe hope that even the serious coder with little interest in the anatomy of game frameworks might\nfind value in the underlying mechanics of client-server communication, dynamic configuration,\nmetadata handling, and layered implementation, all of which have grown rather organically toward\ngood design over the years in quintessential open source style.\nAt its core, Thousand Parsec is primarily a set of standard specifications for a game protocol and\nother related functionality. This chapter discusses the framework mostly from this abstract\nviewpoint, but in many cases it is much more enlightening to refer to actual implementations. To\nthis end, the authors have chosen the \"flagship\" implementations of each major component for\nconcrete discussion.\nThe case model client is tpclient-pywx, a relatively mature wxPython-based client which at\npresent supports the largest set of features and the latest game protocol version. This is\nsupported by libtpclient-py, a Python client helper library providing caching and other\nfunctionality, and libtpproto-py, a Python library which implements the latest version of the\nThousand Parsec protocol. For the server, tpserver-cpp, the mature C++ implementation\nsupporting the latest features and protocol version, is the specimen. This server sports numerous\nrulesets, among which the Missile and Torpedo Wars milestone ruleset is exemplary for making the\nmost extensive use of features and for being a \"traditional\" 4X space game.\n", "page": 286, "type": "text", "section": "Page 286"}
{"text": "21.1. Anatomy of a Star Empire\nIn order to properly introduce the things that make up a Thousand Parsec universe, it makes\nsense first to give a quick overview of a game. For this, we'll examine the Missile and Torpedo\nWars ruleset, the project's second milestone ruleset, which makes use of most of the major\nfeatures in the current mainline version of the Thousand Parsec protocol. Some terminology will be\nused here which will not yet be familiar; the remainder of this section will elucidate it so that the\npieces all fall into place.\nMissile and Torpedo Wars is an advanced ruleset in that it implements all of the methods available\nin the Thousand Parsec framework. At the time of writing, it is the only ruleset to do so, and it is\nbeing quickly expanded to become a more complete and entertaining game.\nUpon establishing a connection to a Thousand Parsec server, the client probes the server for a list\nof game entities and proceeds to download the entire catalog. This cataloger includes all of the\nobjects, boards, messages, categories, designs, components, properties, players, and resources\nthat make up the state of the game, all of which are covered in detail in this section. While this may\nseem like a lot for the client to digest at the beginning of the game\u2014and also at the end of each\nturn\u2014this information is absolutely vital for the game. Once this information has been\ndownloaded, which generally takes on the order of a few seconds, the client now has everything it\nneeds to plot the information onto its representation of the game universe.\nWhen first connected to the server, a random planet is generated and assigned as the new\nplayer's \"home planet\", and two fleets are automatically created there. Each fleet consists of two\ndefault Scout designs, consisting of a Scout Hull with an Alpha Missile Tube. Since there is no\nExplosive component added, this default fleet is not yet capable of fleet-to-fleet or fleet-to-planet\ncombat; it is, in fact, a sitting duck.\nAt this point, it is important for a player to begin equipping fleets with weaponry. This is achieved\nby creating a weapon design using a Build Weapon order, and then loading the finished product\nonto the target fleet through a Load Armament order. The Build Weapon order converts a planet's\nresources\u2014of which each planet has amounts and proportions assigned by a random distribution\n\u2014into a finished product: an explosive warhead which is planted on the creating planet's surface.\nThe Load Armament order then transfers this completed weapon onto a waiting fleet.\nOnce the easily accessible surface resources of a planet are used up, it is important to obtain\nmore through mining. Resources come in two other states: mineable and inaccessible. Using a\nMine order on a planet, mineable resources may be converted over time into surface resources,\nwhich can then be used for building.\n21.1.1. Objects\nIn a Thousand Parsec universe, every physical thing is an object. In fact, the universe itself is also\nan object. This design allows for a virtually unlimited set of elements in a game, while remaining\nsimple for rulesets which require only a few types of objects. On top of the addition of new object\ntypes, each object can store some of its own specific information that can be sent and used via\nthe Thousand Parsec protocol. Five basic built-in object types are currently provided by default:\nUniverse, Galaxy, Star System, Planet, and Fleet.\nThe Universe is the top-level object in a Thousand Parsec game, and it is always accessible to all\nplayers. While the Universe object does not actually exert much control over the game, it does\nstore one vastly important piece of information: the current turn number. Also known as the\n\"year\" in Thousand Parsec parlance, the turn number, naturally, increments after the completion\nof each turn. It is stored in an unsigned 32-bit integer, allowing for games to run until year\n4,294,967,295. While not impossible in theory, the authors have not, to date, seen a game\nprogress this far.\nA Galaxy is a container for a number of proximate objects\u2014Star Systems, Planets and Fleets\u2014and\nprovides no additional information. A large number of Galaxies may exist in a game, each hosting a\nsubsection of the Universe.\nLike the previous two objects, a Star System is primarily a container for lower-level objects.\nHowever, the Star System object is the first tier of object which is represented graphically by the\nclient. These objects may contain Planets and Fleets (at least temporarily).\nA Planet is a large celestial body which may be inhabited and provide resource mines, production\nfacilities, ground-based armaments, and more. The Planet is the first tier of object which can be\nowned by a player; ownership of a Planet is an accomplishment not to be taken lightly, and not\nowning any planets is a typical condition for rulesets to proclaim a player's defeat. The Planet\n", "page": 287, "type": "text", "section": "Page 287"}
{"text": "object has a relatively large amount of stored data, accounting for the following:\nThe player ID of the Planet's owner (or -1 if not owned by any player).\nA list of the Planet's resources, containing the resource ID (type), and the amount of surface,\nmineable, and inaccessible resources of this type on the Planet.\nThe built-in objects described above provide a good basis for many rulesets following the\ntraditional 4X space game formula. Naturally, in keeping with good software engineering principles,\nobject classes can be extended within rulesets. A ruleset designer thus has the ability to create\nnew object types or store additional information in the existing object types as required by the\nruleset, allowing for virtually unlimited extensibility in terms of the available physical objects in the\ngame.\n21.1.2. Orders\nDefined by each ruleset, orders can be attached to both Fleet and Planet objects. While the core\nserver does not ship with any default order types, these are an essential part of even the most\nbasic game. Depending on the nature of the ruleset, orders may be used to accomplish almost\nany task. In the spirit of the 4X genre, there are a few standard orders which are implemented in\nmost rulesets: these are the Move, Intercept, Build, Colonize, Mine, and Attack orders.\nIn order to fulfill the first imperative (explore) of 4X, one needs to be able to move about the map\nof the universe. This is typically achieved via a Move order appended to a Fleet object. In the\nflexible and extensible spirit of the Thousand Parsec framework, Move orders can be implemented\ndifferently depending on the nature of the ruleset. In Minisec and Missile and Torpedo Wars, a\nMove order typically takes a point in 3D space as a parameter. On the server side, the estimated\ntime of arrival is calculated and the number of required turns is sent back to the client. The Move\norder also acts as a pseudo-Attack order in rulesets where teamwork is not implemented. For\nexample, moving to a point occupied by an enemy fleet in both Minisec and Missile and Torpedo\nWars is almost certain to be followed by a period of intense combat. Some rulesets supporting a\nMove order parameterize it differently (i.e. not using 3D points). For example, the Risk ruleset only\nallows single-turn moves to planets which are directly connected by a \"wormhole\".\nTypically appended to Fleet objects, the Intercept order allows an object to meet another\n(commonly an enemy fleet) within space. This order is similar to Move, but since two objects might\nbe moving in different directions during the execution of a turn, it is impossible to land directly on\nanother fleet simply using spatial coordinates, so a distinct order type is necessary. The Intercept\norder addresses this issue, and can be used to wipe out an enemy fleet in deep space or fend off\nan oncoming attack in a moment of crisis.\nThe Build order helps to fulfill two of the 4X imperatives\u2014expand and exploit. The obvious means\nof expansion throughout the universe is to build many fleets of ships and move them far and\nwide. The Build order is typically appended to Planet objects and is often bound to the amount of\nresources that a planet contains\u2014and how they are exploited. If a player is lucky enough to have\na home planet rich in resources, that player could gain an early advantage in the game through\nbuilding.\nLike the Build order, the Colonize order helps fulfill the expand and exploit imperatives. Almost\nalways appended to Fleet objects, the Colonize order allows the player to take over an unclaimed\nplanet. This helps to expand control over planets throughout the universe.\nThe Mine order embodies the exploit imperative. This order, typically appended to Planet objects\nand other celestial bodies, allows the player to mine for unused resources not immediately\navailable on the surface. Doing so brings these resources to the surface, allowing them to be used\nsubsequently to build and ultimately expand the player's grip on the universe.\nImplemented in some rulesets, the Attack order allows a player to explicitly initiate combat with an\nenemy Fleet or Planet, fulfilling the final 4X imperative (exterminate). In team-based rulesets, the\ninclusion of a distinct Attack order (as opposed to simply using Move and Intercept to implicitly\nattack targets) is important to avoid friendly fire and to coordinate attacks.\nSince the Thousand Parsec framework requires ruleset developers to define their own order\ntypes, it is possible\u2014even encouraged\u2014for them to think outside the box and create custom\norders not found elsewhere. The ability to pack extra data into any object allows developers to do\nvery interesting things with custom order types.\n21.1.3. Resources\nResources are extra pieces of data that are packed into Objects in the game. Extensively used\u2014\nparticularly by Planet objects\u2014resources allow for easy extension of rulesets. As with many of the\n", "page": 288, "type": "text", "section": "Page 288"}
{"text": "design decisions in Thousand Parsec, extensibility was the driving factor in the inclusion of\nresources.\nWhile resources are typically implemented by the ruleset designer, there is one resource that is in\nconsistent use throughout the framework: the Home Planet resource, which is used to identify a\nplayer's home planet.\nAccording to Thousand Parsec best practices, resources are typically used to represent\nsomething that can be converted into some type of object. For example, Minisec implements a\nShip Parts resource, which is assigned in random quantities to each planet object in the universe.\nWhen one of these planets is colonized, you can then convert this Ship Parts resource into actual\nFleets using a Build order.\nMissile and Torpedo Wars makes perhaps the most extensive use of resources of any ruleset to\ndate. It is the first ruleset where the weapons are of a dynamic nature, meaning that they can be\nadded to a ship from a planet and also removed from a ship and added back to a planet. To\naccount for this, the game creates a resource type for each weapon that is created in the game.\nThis allows ships to identify a weapon type by a resource, and move them freely throughout the\nuniverse. Missile and Torpedo Wars also keeps track of factories (the production capability of\nplanets) using a Factories resource tied to each planet.\n21.1.4. Designs\nIn Thousand Parsec, both weapons and ships may be composed of various components. These\ncomponents are combined to form the basis of a Design\u2014a prototype for something which can\nbe built and used within the game. When creating a ruleset, the designer has to make an almost\nimmediate decision: should the ruleset allow dynamic creation of weapon and ship designs, or\nsimply use a predetermined list of designs? On the one hand, a game using pre-packaged designs\nwill be easier to develop and balance, but on the other hand, dynamic creation of designs adds an\nentirely new level of complexity, challenge, and fun to the game.\nUser-created designs allow a game to become far more advanced. Since users must strategically\ndesign their own ships and their armaments, a stratum of variance is added to the game which\ncan help to mitigate otherwise great advantages that might be conferred on a player based on\nluck (e.g., of placement) and other aspects of game strategy. These designs are governed by the\nrules of each component, outlined in the Thousand Parsec Component Language (TPCL, covered\nlater in this chapter), and specific to each ruleset. The upshot is that no additional programming of\nfunctionality is necessary on the part of the developer to implement the design of weapons and\nships; configuring some simple rules for each component available in the ruleset is sufficient.\nWithout careful planning and proper balance, the great advantage of using custom designs can\nbecome its downfall. In the later stages of a game, an inordinate amount of time can be spent\ndesigning new types of weapons and ships to build. The creation of a good user experience on the\nclient side for design manipulation is also a challenge. Since design manipulation can be an integral\npart of one game, while completely irrelevant to another, the integration of a design window into\nclients is a significant obstacle. Thousand Parsec's most complete client, tpclient-pywx, currently\nhouses the launcher for this window in a relatively out-of-the-way place, in a sub-menu of the\nmenu bar (which is rarely used in-game otherwise).\nThe Design functionality is designed to be easily accessible to ruleset developers, while allowing\ngames to expand to virtually unlimited levels of complexity. Many of the existing rulesets allow for\nonly predetermined designs. Missile and Torpedo Wars, however, allows for full weapon and ship\ndesign from a variety of components.\n21.2. The Thousand Parsec Protocol\nOne might say that the Thousand Parsec protocol is the basis upon which everything else in the\nproject is built. It defines the features available to ruleset writers, how servers should work, and\nwhat clients should be able to handle. Most importantly, like an interstellar communications\nstandard, it allows the various software components to understand one another.\nThe server manages the actual state and dynamics of a game according to the instructions\nprovided by the ruleset. Each turn, a player's client receives some of the information about the\nstate of the game: objects and their ownership and current state, orders in progress, resource\nstockpiles, technological progress, messages, and everything else visible to that particular player.\nThe player can then perform certain actions given the current state, such as issuing orders or\ncreating designs, and send these back to the server to be processed into the computation of the\nnext turn. All of this communication is framed in the Thousand Parsec protocol. An interesting and\nquite deliberate effect of this architecture is that AI clients\u2014which are external to the\n", "page": 289, "type": "text", "section": "Page 289"}
{"text": "server/ruleset and are the only means of providing computer players in a game\u2014are bound by the\nsame rules as the clients human players use, and thus cannot \"cheat\" by having unfair access to\ninformation or by being able to bend the rules.\nThe protocol specification describes a series of frames, which are hierarchical in the sense that\neach frame (except the Header frame) has a base frame type to which it adds its own data. There\nare a variety of abstract frame types which are never explicitly used, but simply exist to describe\nbases for concrete frames. Frames may also have a specified direction, with the intent that such\nframes need only be supported for sending by one side (server or client) and receiving by the\nother.\nThe Thousand Parsec protocol is designed to function either standalone over TCP/IP, or tunnelled\nthrough another protocol such as HTTP. It also supports SSL encryption.\n21.2.1. Basics\nThe protocol provides a few generic frames which are ubiquitous in communication between client\nand server. The previously mentioned Header frame simply provides a basis for all other frames\nvia its two direct descendants, the Request and Response frames. The former is the basis for\nframes which initiate communication (in either direction), and the latter for frames which are\nprompted by these. The OK and Fail frames (both Response frames) provide the two values for\nBoolean logic in the exchange. A Sequence frame (also a Response) indicates to the recipient that\nmultiple frames are to follow in response to its request.\nThousand Parsec uses numerical IDs to address things. Accordingly, a vocabulary of frames\nexists to push around data via these IDs. The Get With ID frame is the basic request for things\nwith such an ID; there is also a Get With ID and Slot frame for things which are in a \"slot\" on\na parent thing which has an ID (e.g., an order on an object). Of course, it is often necessary to\nobtain sequences of IDs, such as when initially populating the client's state; this is handled using\nGet ID Sequence type requests and ID Sequence type responses. A common structure for\nrequesting multiple items is a Get ID Sequence request and ID Sequence response, followed\nby a series of Get With ID requests and appropriate responses describing the item requested.\n21.2.2. Players and Games\nBefore a client can begin interacting with a game, some formalities need to be addressed. The\nclient must first issue a Connect frame to the server, to which the server might respond with OK\nor Fail\u2014since the Connect frame includes the client's protocol version, one reason for failure\nmight be a version mismatch. The server can also respond with the Redirect frame, for moves\nor server pools. Next, the client must issue a Login frame, which identifies and possibly\nauthenticates the player; players new to a server can first use the Create Account frame if the\nserver allows it.\nBecause of the vast variability of Thousand Parsec, the client needs some way to ascertain which\nprotocol features are supported by the server; this is accomplished via the Get Features\nrequest and Features response. Some of the features the server might respond with include:\nAvailability of SSL and HTTP tunnelling (on this port or another port).\nSupport for server-side component property calculation.\nOrdering of ID sequences in responses (ascending vs. descending).\nSimilarly, the Get Games request and sequence of Game responses informs the client about the\nnature of the active games on the server. A single Game frame contains the following information\nabout a game:\nThe long (descriptive) name of the game.\nA list of supported protocol versions.\nThe type and version of the server.\nThe name and version of the ruleset.\nA list of possible network connection configurations.\nA few optional items (number of players, number of objects, administrator details, comment,\ncurrent turn number, etc.).\nThe base URL for media used by the game.\nIt is, of course, important for a player to know who he or she is up against (or working with, as\nthe case may be), and there is a set of frames for that. The exchange follows the common item\nsequence pattern with a Get Player IDs request, a List of Player IDs response, and a\nseries of Get Player Data requests and Player Data responses. The Player Data frame\ncontains the player's name and race.\n", "page": 290, "type": "text", "section": "Page 290"}
{"text": "Turns in the game are also controlled via the protocol. When a player has finished performing\nactions, he or she may signal readiness for the next turn via the Finished Turn request; the\nnext turn is computed when all players have done so. Turns also have a time limit imposed by the\nserver, so that slow or unresponsive players cannot hold up a game; the client normally issues a\nGet Time Remaining request, and tracks the turn with a local timer set to the value in the\nserver's Time Remaining response.\nFinally, Thousand Parsec supports messages for a variety of purposes: game broadcasts to all\nplayers, game notifications to a single player, player-to-player communications. These are\norganized into \"board\" containers which manage ordering and visibility; following the item\nsequence pattern, the exchange consists of a Get Board IDs request, a List of Board IDs\nresponse, and a series of Get Board requests and Board responses.\nOnce the client has information on a message board, it can issue Get Message requests to obtain\nmessages on the board by slot (hence, Get Message uses the Get With ID and Slot base\nframe); the server responds with Message frames containing the message subject and body, the\nturn on which the message was generated, and references to any other entities mentioned in the\nmessage. In addition to the normal set of items encountered in Thousand Parsec (players, objects,\nand the like), there are also some special references including message priority, player actions,\nand order status. Naturally, the client can also add messages using the Post Message frame\u2014a\nvehicle for a Messsage frame\u2014and delete them using the Remove Message frame (based on the\nGetMessage frame).\n21.2.3. Objects, Orders, and Resources\nThe bulk of the process of interacting with the universe is accomplished through a series of\nframes comprising the functionality for objects, orders, and resources.\nThe physical state of the universe\u2014or at least that part of it that the player controls or has the\nability to see\u2014must be obtained upon connecting, and every turn thereafter, by the client. The\nclient generally issues a Get Object IDs request (a Get ID Sequence), to which the server\nreplies with a List of Object IDs response. The client can then request details about individual\nobjects using Get Object by ID requests, which are answered with Object frames containing\nsuch details\u2014again subject to visibility by the player\u2014as their type, name, size, position, velocity,\ncontained objects, applicable order types, and current orders. The protocol also provides the Get\nObject IDs by Position request, which allows the client to find all objects within a specified\nsphere of space.\nThe client obtains the set of possible orders following the usual item sequence pattern by issuing a\nGet Order Description IDs request and, for each ID in the List of Order Description\nIDs response, issuing a Get Order Description request and receiving a Order Description\nresponse. The implementation of the orders and order queues themselves has evolved markedly\nover the history of the protocol. Originally, each object had a single order queue. The client would\nissue an Order request (containing the order type, target object, and other information), receive\nan Outcome response detailing the expected result of the order, and, after completion of the\norder, receive a Result frame containing the actual result.\nIn the second version, the Order frame incorporated the contents of the Outcome frame (since,\nbased on the order description, this did not require the server's input), and the Result frame was\nremoved entirely. The latest version of the protocol refactored the order queue out of objects, and\nadded the Get Order Queue IDs, List of Order Queue IDs, Get Order Queue, and Order\nQueue frames, which work similarly to the message and board functionality2. The Get Order and\nRemove Order frames (both GetWithIDSlot requests) allow the client to access and remove\norders on a queue, respectively. The Insert Order frame now acts as a vehicle for the Order\npayload; this was done to allow for another frame, Probe Order, which is used by the client in\nsome cases to obtain information for local use.\nResource descriptions also follow the item sequence pattern: a Get Resource Description\nIDs request, a List of Resource Description IDs response, and a series of Get Resource\nDescription requests and Resource Description responses.\n21.2.4. Design Manipulation\nThe handling of designs in the Thousand Parsec Protocol is broken down into the manipulation of\nfour separate sub-categories: categories, components, properties, and designs.\nCategories differentiate the different design types. Two of the most commonly used design types\nare ships and weapons. Creating a category is simple, as it consists only of a name and\ndescription; the Category frame itself contains only these two strings. Each category is added by\n", "page": 291, "type": "text", "section": "Page 291"}
{"text": "the ruleset to the Design Store using an Add Category request, a vehicle for the Category\nframe. The remainder of the management of categories is handled in the usual item sequence\npattern with the Get Category IDs request and List of Category IDs response.\nComponents consist of the different parts and modules which comprise a design. This can be\nanything from the hull of a ship or missile to the tube that a missile is housed in. Components are\na bit more involved than categories. A Component frame contains the following information:\nThe name and description of the component.\nA list of categories to which the component belongs.\nA Requirements function, in Thousand Parsec Component Language (TPCL).\nA list of properties and their corresponding values.\nOf particular note is the Requirements function associated with the component. Since\ncomponents are the parts that make up a ship, weapon, or other constructed object, it is\nnecessary to ensure that they are valid when adding them to a design. The Requirements\nfunction verifies that each component added to the design conforms to the rules of other\npreviously added components. For example, in Missile and Torpedo Wars, it is impossible to\nhold an Alpha Missile in a ship without an Alpha Missile Tube. This verification occurs on both the\nclient side and the server side, which is why the entire function must appear in a protocol frame,\nand why a concise language (TPCL, covered later in the chapter) was chosen for it.\nAll of a design's properties are communicated via Property frames. Each ruleset exposes a set of\nproperties used within the game. These typically include things like the number of missile tubes of\na certain type allowed on a ship, or the amount of armor included with a certain hull type. Like\nComponent frames, Property frames make use of TPCL. A Property frame contains the\nfollowing information:\nThe (display) name and description of the property.\nA list of categories to which the property belongs.\nThe name (valid TPCL identifier) of the property.\nThe rank of the property.\nCalculate and Requirements functions, in Thousand Parsec Component Language (TPCL).\nThe rank of a property is used to distinguish a hierarchy of dependencies. In TPCL, a function may\nnot depend on any property which has a rank less than or equal to this property. This means that\nif one had an Armor property of rank 1 and an Invisibility property of rank 0, then the Invisibility\nproperty could not directly depend on the Armor property. This ranking was implemented as a\nmethod of curtailing circular dependencies. The Calculate function is used to define how a\nproperty is displayed, differentiating the methods of measurement. Missile and Torpedo Wars\nuses XML to import game properties from a game data file. Figure 21.2 shows an example\nproperty from that game data.\n<prop>\n<CategoryIDName>Ships</CategoryIDName>\n<rank value=\"0\"/>\n<name>Colonise</name>\n<displayName>Can Colonise Planets</displayName>\n<description>Can the ship colonise planets</description>\n<tpclDisplayFunction>\n    (lambda (design bits) (let ((n (apply + bits))) (cons n (if (= n 1) \"Yes\" \"No\")) ) )\n</tpclDisplayFunction>\n<tpclRequirementsFunction>\n    (lambda (design) (cons #t \"\"))\n</tpclRequirementsFunction>\n</prop>\nFigure 21.2: Example Property\nIn this example, we have a property belonging to the Ships category, of rank 0. This property is\ncalled Colonise, and relates to the ability of a ship to colonize planets. A quick look at the TPCL\nCalculate function (listed here as tpclDisplayFunction) reveals that this property outputs\neither \"Yes\" or \"No\" depending on whether the ship in question has said capability. Adding\nproperties in this fashion gives the ruleset designer granular control over metrics of the game and\nthe ability to easily compare them and output them in a player-friendly format.\nThe actual design of ships, weapons, and other game artifacts are created and manipulated using\nthe Design frame and related frames. In all current rulesets, these are used for building ships and\n", "page": 292, "type": "text", "section": "Page 292"}
{"text": "weaponry using the existing pool of components and properties. Since the rules for designs are\nalready handled in TPCL Requirements functions in both properties and components, the\ncreation of a design is a bit simpler. A Design frame contains the following information:\nThe name and description of the design.\nA list of categories to which the design belongs.\nA count of the number of instances of the design.\nThe owner of the design.\nA list of component IDs and their corresponding counts.\nA list of properties and their corresponding display string.\nThe feedback on the design.\nThis frame is a bit different from the others. Most notably, since a design is an owned item in the\ngame, there is a relation to the owner of each design. A design also tracks the number of its\ninstantiations with a counter.\n21.2.5. Server Administration\nA server administration protocol extension is also available, allowing for remote live control of\nsupporting servers. The standard use case is to connect to the server via an administration client\n\u2014perhaps a shell-like command interface or a GUI configuration panel\u2014to change settings or\nperform other maintenance tasks. However, other, more specialized uses are possible, such as\nbehind-the-scenes management for single-player games.\nAs with the game protocol described in the preceding sections, the administration client first\nnegotiates a connection (on a port separate from the normal game port) and authenticates using\nConnect and Login requests. Once connected, the client can receive log messages from and\nissue commands to the server.\nLog messages are pushed to the client via Log Message frames. These contain a severity level\nand text; as appropriate to the context, the client can choose to display all, some, or none of the\nlog messages it receives.\nThe server may also issue a Command Update frame instructing the client to populate or update\nits local command set; supported commands are exposed to the client in the server's response to\na Get Command Description IDs frame. Individual command descriptions must then be\nobtained by issuing a Get Command Description frame for each, to which the server responds\nwith a Command Description frame.\nThis exchange is functionally quite similar to (and, in fact, was originally based on) that of the\norder frames used in the main game protocol. It allows commands to be described to the user\nand vetted locally to some degree, minimizing network usage. The administration protocol was\nconceived at a time when the game protocol was already mature; rather than starting from\nscratch, the developers found existing functionality in the game protocol which did almost what\nwas needed, and added the code to the same protocol libraries.\n21.3. Supporting Functionality\n21.3.1. Server Persistence\nThousand Parsec games, like many in the turn-based strategy genre, have the potential to last for\nquite some time. Besides often running far longer than the circadian rhythms of the players'\nspecies, during this extended period the server process might be prematurely terminated for any\nnumber of reasons. To allow players to pick up a game where they left off, Thousand Parsec\nservers provide persistence by storing the entire state of the universe (or even multiple universes)\nin a database. This functionality is also used in a related way for saving single-player games, which\nwill be covered in more detail later in this section.\nThe flagship server, tpserver-cpp, provides an abstract persistence interface and a modular\nplugin system to allow for various database back ends. At the time of writing, tpserver-cpp\nships with modules for MySQL and SQLite.\nThe abstract Persistence class describes the functionality allowing the server to save, update,\nand retrieve the various elements of a game (as described in the Anatomy of a Star Empire\nsection). The database is updated continuously from various places in the server code where the\ngame state changes, and no matter the point at which the server is terminated or crashes, all\ninformation to that point should be recovered when the server starts again from the saved data.\n21.3.2. Thousand Parsec Component Language\n", "page": 293, "type": "text", "section": "Page 293"}
{"text": "The Thousand Parsec Component Language (TPCL) exists to allow clients to create designs locally\nwithout server interaction\u2014allowing for instant feedback about the properties, makeup, and\nvalidity of the designs. This allows the player to interactively create, for example, new classes of\nstarship, by customizing structure, propulsion, instrumentation, defenses, armaments, and more\naccording to available technology.\nTPCL is a subset of Scheme, with a few minor changes, though close enough to the Scheme R5RS\nstandard that any compatible interpreter can be used. Scheme was originally chosen because of\nits simplicity, a host of precedents for using it as an embedded language, the availability of\ninterpreters implemented in many other languages, and, most importantly to an open source\nproject, vast documentation both on using it and on developing interpreters for it.\nConsider the following example of a Requirements function in TPCL, used by components and\nproperties, which would be included with a ruleset on the server side and communicated to the\nclient over the game protocol:\n(lambda (design)\n  (if (> (designType.MaxSize design) (designType.Size design))\n      (if (= (designType.num-hulls design) 1)\n          (cons #t \"\")\n          (cons #f \"Ship can only have one hull\")\n      )\n      (cons #f \"This many components can't fit into this Hull\")\n  )\n)\nReaders familiar with Scheme will no doubt find this code easy to understand. The game (both\nclient and server) uses it to check other component properties (MaxSize, Size, and Num-Hulls)\nto verify that this component can be added to a design. It first verifies that the Size of the\ncomponent is within the maximum size of the design, then ensures that there are no other hulls in\nthe design (the latter test tips us off that this is the Requirements function from a ship hull).\n21.3.3. BattleXML\nIn war, every battle counts, from the short skirmish in deep space between squadrons of small\nlightly-armed scout craft, to the massive final clash of two flagship fleets in the sky above a capital\nworld. On the Thousand Parsec framework, the details of combat are handled within the ruleset,\nand there is no explicit client-side functionality regarding combat details\u2014typically, the player will\nbe informed of the initiation and results of combat via messages, and the appropriate changes to\nthe objects will take place (e.g., removal of destroyed ships). Though the player's focus will\nnormally be on a higher level, under rulesets with complex combat mechanics, it may prove\nadvantageous (or, at least, entertaining) to examine the battle in more detail.\nThis is where BattleXML comes in. Battle data is split into two major parts: the media definition,\nwhich provides details about the graphics to be used, and the battle definition, which specifies\nwhat actually occurred during a battle. These are intended to be read by a battle viewer, of which\nThousand Parsec currently has two: one in 2D and the other in 3D. Of course, since the nature of\nbattles are entirely a feature of a ruleset, the ruleset code is responsible for actually producing\nBattleXML data.\nThe media definition is tied to the nature of the viewer, and is stored in a directory or an archive\ncontaining the XML data and any graphics or model files it references. The data itself describes\nwhat media should be used for each ship (or other object) type, its animations for actions such as\nfiring and death, and the media and details of its weapons. File locations are assumed to be relative\nto the XML file itself, and cannot reference parent directories.\nThe battle definition is independent of the viewer and media. First, it describes a series of entities\non each side at the start of the battle, with unique identifiers and information such as name,\ndescription, and type. Then, each round of the battle is described: object movement, weapons fire\n(with source and target), damage to objects, death of objects, and a log message. How much\ndetail is used to describe each round of battle is dictated by the ruleset.\n21.3.4. Metaserver\nFinding a public Thousand Parsec server to play on is much like locating a lone stealth scout in\ndeep space\u2014a daunting prospect if one doesn't know where to look. Fortunately, public servers\ncan announce themselves to a metaserver, whose location, as a central hub, should ideally be\nwell-known to players.\n", "page": 294, "type": "text", "section": "Page 294"}
{"text": "The current implementation is metaserver-lite, a PHP script, which lives at some central place\nlike the Thousand Parsec website. Supporting servers send an HTTP request specifying the update\naction and containing the type, location (protocol, host, and port), ruleset, number of players,\nobject count, administrator, and other optional information. Server listings expire after a specified\ntimeout (by default, 10 minutes), so servers are expected to update the metaserver periodically.\nThe script can then, when called with no specified action, be used to embed the list of servers with\ndetails into a web site, presenting clickable URLs (typically with the tp:// scheme name).\nAlternatively, the badge action presents server listings in a compact \"badge\" format.\nClients may issue a request to a metaserver using the get action to obtain a list of available\nservers. In this case, the metaserver returns one or more Game frames for each server in the list\nto the client. In tpclient-pywx, the resulting list is presented through a server browser in the\ninitial connection window.\n21.3.5. Single-Player Mode\nThousand Parsec is designed from the ground up to support networked multiplayer games.\nHowever, there is nothing preventing a player from firing up a local server, connecting a few AI\nclients, and hyperjumping into a custom single-player universe ready to be conquered. The project\ndefines some standard metadata and functionality to support streamlining this process, making\nsetup as easy as running a GUI wizard or double-clicking a scenario file.\nAt the core of this functionality is an XML DTD specifying the format for metadata regarding the\ncapabilities and properties of each component (e.g., server, AI client, ruleset). Component\npackages ship with one or more such XML files, and eventually all of this metadata is aggregated\ninto an associative array divided into two major portions: servers and AI clients. Within a server's\nmetadata will typically be found metadata for one or more rulesets\u2014they are found here because\neven though a ruleset may be implemented for more than one server, some configuration details\nmay differ, so separate metadata is needed in general for each implementation. Each entry for one\nof these components contains the following information:\nDescriptive data, including a short (binary) name, a long (descriptive) name, and a\ndescription.\nThe installed version of the component, and the earliest version whose save data is\ncompatible with the installed version.\nThe command string (if applicable) and any forced parameters passed to it.\nA set of parameters which can be specified by the player.\nForced parameters are not player-configurable and are typically options which allow the\ncomponents to function appropriately for a local, single-player context. The player parameters\nhave their own format indicating such details as the name and description, the data type, default,\nand range of the value, and the format string to append to the main command string.\nWhile specialized cases are possible (e.g., preset game configurations for ruleset-specific clients),\nthe typical process for constructing a single-player game involves selecting a set of compatible\ncomponents. Selection of the client is implicit, as the player will have already launched one in order\nto play a game; a well-designed client follows a user-centric workflow to set up the remainder. The\nnext natural choice to make is the ruleset, so the player is presented with a list\u2014at this point,\nthere is no need to bother with server details. In the event that the chosen ruleset is implemented\nby multiple installed servers (probably a rare condition), the player is prompted to select one;\notherwise, the appropriate server is selected automatically. Next, the player is prompted to\nconfigure options for the ruleset and server, with sane defaults pulled from the metadata. Finally,\nif any compatible AI clients are installed, the player is prompted to configure one or more of them\nto play against.\nWith the game so configured, the client launches the local server with appropriate configuration\nparameters (including the ruleset, its parameters, and any parameters it adds to the server's\nconfiguration), using the command string information from the metadata. Once it has verified that\nthe server is running and accepting connections, perhaps using the administration protocol\nextension discussed previously, it launches each of the specified AI clients similarly, and verifies\nthat they have successfully connected to the game. If all goes well, the client will then connect to\nthe server\u2014just as if it were connecting to an online game\u2014and the player can begin exploring,\ntrading, conquering, and any of a universe of other possibilities.\nAn alternate\u2014and very important\u2014use for the single-player functionality is the saving and loading\nof games, and, more or less equivalently, the loading of ready-to-play scenarios. In this case, the\nsave data (probably, though not necessarily, a single file) stores the single-player game\nconfiguration data alongside the persistence data for the game itself. Provided all appropriate\ncomponents in compatible versions are installed on the player's system, launching a saved game\n", "page": 295, "type": "text", "section": "Page 295"}
{"text": "or scenario is completely automatic. Scenarios in particular thus provide an attractive one-click\nentry into a game. Although Thousand Parsec does not currently have a dedicated scenario editor\nor a client with an edit mode, the concept is to provide some means of crafting the persistence\ndata outside of the normal functioning of the ruleset, and verifying its consistency and\ncompatibility.\nSo far, the description of this functionality has been rather abstract. On a more concrete level, the\nPython client helper library, libtpclient-py, is currently home to the only full realization of\nsingle-player mechanics in the Thousand Parsec project. The library provides the\nSinglePlayerGame class, which upon instantiation automatically aggregates all available single-\nplayer metadata on the system (naturally, there are certain guidelines as to where the XML files\nshould be installed on a given platform). The object can then be queried by the client for various\ninformation on the available components; servers, rulesets, AI clients, and parameters are stored\nas dictionaries (Python's associative arrays). Following the general game building process outlined\nabove, a typical client might perform the following:\n1. Query a list of available rulesets via SinglePlayerGame.rulesets, and configure the object\nwith the chosen ruleset by setting SinglePlayerGame.rname.\n2. Query a list of servers implementing the ruleset via\nSinglePlayerGame.list_servers_with_ruleset, prompt the user to select one if\nnecessary, and configure the object with the chosen (or only) server by setting\nSinglePlayerGame.sname.\n3. Obtain the set of parameters for the server and ruleset via\nSinglePlayerGame.list_rparams and SinglePlayerGame.list_sparams, respectively,\nand prompt the player to configure them.\n4. Find available AI clients supporting the ruleset via\nSinglePlayerGame.list_aiclients_with_ruleset, and prompt the player to configure\none or more of them using the parameters obtained via\nSinglePlayerGame.list_aiparams.\n5. Launch the game by calling SinglePlayerGame.start, which will return a TCP/IP port to\nconnect on if successful.\n6. Eventually, end the game (and kill any launched server and AI client processes) by calling\nSinglePlayerGame.stop.\nThousand Parsec's flagship client, tpclient-pywx, presents a user-friendly wizard which follows\nsuch a procedure, initially prompting instead for a saved game or scenario file to load. The user-\ncentric workflow developed for this wizard is an example of good design arising from the open\nsource development process of the project: the developer initially proposed a very different\nprocess more closely aligned with how things were working under the hood, but community\ndiscussion and some collaborative development produced a result much more usable for the\nplayer.\nFinally, saved games and scenarios are currently implemented in practice in tpserver-cpp, with\nsupporting functionality in libtpclient-py and an interface in tpclient-pywx. This is achieved\nthrough a persistence module using SQLite, a public domain open source RDBMS which requires\nno external process and stores databases in a single file. The server is configured, via a forced\nparameter, to use the SQLite persistence module if it is available, and as usual, the database file\n(living in a temporary location) is constantly updated throughout the game. When the player opts\nto save the game, the database file is copied to the specified location, and a special table is added\nto it containing the single player configuration data. It should be fairly obvious to the reader how\nthis is subsequently loaded.\n21.4. Lessons Learned\nThe creation and growth of the extensive Thousand Parsec framework has allowed the developers\nplenty of opportunity to look back and assess the design decisions that were made along the way.\nThe original core developers (Tim Ansell and Lee Begg) built the original framework from scratch\nand have shared with us some suggestions on starting a similar project.\n21.4.1. What Worked\nA major key to the development of Thousand Parsec was the decision to define and build a subset\nof the framework, followed by the implementation. This iterative and incremental design process\nallowed the framework to grow organically, with new features added seamlessly. This led directly\nto the decision to version the Thousand Parsec protocol, which is credited with a number of major\nsuccesses of the framework. Versioning the protocol allowed the framework to grow over time,\nenabling new methods of gameplay along the way.\nWhen developing such an expansive framework, it is important to have a very short-term\n", "page": 296, "type": "text", "section": "Page 296"}
{"text": "approach for goals and iterations. Short iterations, on the order of weeks for a minor release,\nallowed the project to move forward quickly with immediate returns along the way. Another\nsuccess of the implementation was the client-server model, which allowed for the clients to be\ndeveloped away from any game logic. The separation of game logic from client software was\nimportant to the overall success of Thousand Parsec.\n21.4.2. What Didn't Work\nA major downfall of the Thousand Parsec framework was the decision to use a binary protocol. As\nyou can imagine, debugging a binary protocol is not a fun task and this has lead to many\nprolonged debugging sessions. We would highly recommend that nobody take this path in the\nfuture. The protocol has also grown to have too much flexibility; when creating a protocol, it is\nimportant to implement only the basic features that are required.\nOur iterations have at times grown too large. When managing such a large framework on an open\nsource development schedule, it is important to have a small subset of added features in each\niteration to keep development flowing.\n21.4.3. Conclusion\nLike a construction skiff inspecting the skeletal hull of a massive prototype battleship in an orbital\nconstruction yard, we have passed over the various details of the architecture of Thousand\nParsec. While the general design criteria of flexibility and extensibility have been in the minds of the\ndevelopers from the very beginning, it is evident to us, looking at the history of the framework,\nthat only an open source ecosystem, teeming with fresh ideas and points of view, could have\nproduced the sheer volume of possibilities while remaining functional and cohesive. It is a\nsingularly ambitious project, and as with many of its peers on the open source landscape, much\nremains to be done; it is our hope and expectation that over time, Thousand Parsec will continue\nto evolve and expand its capabilities while new and ever more complex games are developed upon\nit. After all, a journey of a thousand parsecs begins with a single step.\nFootnotes\n1. Some excellent commercial examples of Thousand Parsec's inspiration include VGA Planets\nand Stars!, as well as the Master of Orion, Galactic Civilizations, and Space Empires series. For\nreaders unfamiliar with these titles, the Civilization series is a popular example of the same\ngameplay style, albeit in a different setting. A number of real-time 4X games also exist, such\nas Imperium Galactica and Sins of a Solar Empire.\n2. Actually, it's the other way around: messages and boards were derived from orders in the\nsecond version of the protocol.\n", "page": 297, "type": "text", "section": "Page 297"}
{"text": "The Architecture of\nOpen Source Applications\nAmy Brown and Greg Wilson (eds.)\nISBN 978-1-257-63801-7 \nLicense / Buy / Contribute\nChapter 22. Violet\nCay Horstmann\nIn 2002, I wrote an undergraduate textbook on object-oriented design and patterns [Hor05]. As\nwith so many books, this one was motivated by frustration with the canonical curriculum.\nFrequently, computer science students learn how to design a single class in their first\nprogramming course, and then have no further training in object-oriented design until their senior\nlevel software engineering course. In that course, students rush through a couple of weeks of\nUML and design patterns, which gives no more than an illusion of knowledge. My book supports a\nsemester-long course for students with a background in Java programming and basic data\nstructures (typically from a Java-based CS1/CS2 sequence). The book covers object-oriented\ndesign principles and design patterns in the context of familiar situations. For example, the\nDecorator design pattern is introduced with a Swing JScrollPane, in the hope that this example\nis more memorable than the canonical Java streams example.\nFigure 22.1: A Violet Object Diagram\nI needed a light subset of UML for the book: class diagrams, sequence diagrams, and a variant of\nobject diagrams that shows Java object references (Figure 22.1). I also wanted students to draw\ntheir own diagrams. However, commercial offerings such as Rational Rose were not only\nexpensive but also cumbersome to learn and use [Shu05], and the open source alternatives\navailable at the time were too limited or buggy to be useful1, in which diagrams are specified by\ntextual declarations rather than the more common point-and-click interface.}. In particular,\nsequence diagrams in ArgoUML were seriously broken.\nI decided to try my hand at implementing the simplest editor that is (a) useful to students and (b)\nan example of an extensible framework that students can understand and modify. Thus, Violet\nwas born.\n22.1. Introducing Violet\nViolet is a lightweight UML editor, intended for students, teachers, and authors who need to\nproduce simple UML diagrams quickly. It is very easy to learn and use. It draws class, sequence,\nstate, object and use-case diagrams. (Other diagram types have since been contributed.) It is\nopen-source and cross-platform software. In its core, Violet uses a simple but flexible graph\nframework that takes full advantage of the Java 2D graphics API.\nThe Violet user interface is purposefully simple. You don't have to go through a tedious sequence\nof dialogs to enter attributes and methods. Instead, you just type them into a text field. With a few\nmouse clicks, you can quickly create attractive and useful diagrams.\n", "page": 298, "type": "text", "section": "Page 298"}
{"text": "Violet does not try to be an industrial-strength UML program. Here are some features that Violet\ndoes not have:\nViolet does not generate source code from UML diagrams or UML diagrams from source\ncode.\nViolet does not carry out any semantic checking of models; you can use Violet to draw\ncontradictory diagrams.\nViolet does not generate files that can be imported into other UML tools, nor can it read\nmodel files from other tools.\nViolet does not attempt to lay out diagrams automatically, except for a simple \"snap to grid\"\nfacility.\n(Attempting to address some of these limitations makes good student projects.)\nWhen Violet developed a cult following of designers who wanted something more than a cocktail\nnapkin but less than an industrial-strength UML tool, I published the code on SourceForge under\nthe GNU Public License. Starting in 2005, Alexandre de Pellegrin joined the project by providing an\nEclipse plugin and a prettier user interface. He has since made numerous architectural changes\nand is now the primary maintainer of the project.\nIn this article, I discuss some of the original architectural choices in Violet as well as its evolution. A\npart of the article is focused on graph editing, but other parts\u2014such as the use of JavaBeans\nproperties and persistence, Java WebStart and plugin architecture\u2014should be of general interest.\n22.2. The Graph Framework\nViolet is based on a general graph editing framework that can render and edit nodes and edges of\narbitrary shapes. The Violet UML editor has nodes for classes, objects, activation bars (in\nsequence diagrams), and so on, and edges for the various edge shapes in UML diagrams. Another\ninstance of the graph framework might display entity-relationship diagrams or railroad diagrams.\nFigure 22.2: A Simple Instance of the Editor Framework\nIn order to illustrate the framework, let us consider an editor for very simple graphs, with black\nand white circular nodes and straight edges (Figure 22.2). The SimpleGraph class specifies\nprototype objects for the node and edge types, illustrating the prototype pattern:\npublic class SimpleGraph extends AbstractGraph\n{\n", "page": 299, "type": "text", "section": "Page 299"}
{"text": "  public Node[] getNodePrototypes()\n  {\n    return new Node[]\n    {\n      new CircleNode(Color.BLACK),\n      new CircleNode(Color.WHITE)\n    };\n  }\n  public Edge[] getEdgePrototypes()\n  {\n    return new Edge[]\n    {\n      new LineEdge()\n    };\n  }\n}\nPrototype objects are used to draw the node and edge buttons at the top of Figure 22.2. They are\ncloned whenever the user adds a new node or edge instance to the graph. Node and Edge are\ninterfaces with the following key methods:\nBoth interfaces have a getShape method that returns a Java2D Shape object of the node or\nedge shape.\nThe Edge interface has methods that yield the nodes at the start and end of the edge.\nThe getConnectionPoint method in the Node interface type computes an optimal\nattachment point on the boundary of a node (see Figure 22.3).\nThe getConnectionPoints method of the Edge interface yields the two end points of the\nedge. This method is needed to draw the \"grabbers\" that mark the currently selected edge.\nA node can have children that move together with the parent. A number of methods are\nprovided for enumerating and managing children.\nFigure 22.3: Finding a Connection Point on the Boundary of the Node Shape\nConvenience classes AbstractNode and AbstractEdge implement a number of these methods,\nand classes RectangularNode and SegmentedLineEdge provide complete implementations of\nrectangular nodes with a title string and edges that are made up of line segments.\nIn the case of our simple graph editor, we would need to supply subclasses CircleNode and\nLineEdge that provide a draw method, a contains method, and the getConnectionPoint\nmethod that describes the shape of the node boundary. The code is given below, and Figure 22.4\nshows a class diagram of these classes (drawn, of course, with Violet).\npublic class CircleNode extends AbstractNode\n{\n  public CircleNode(Color aColor)\n  {\n    size = DEFAULT_SIZE;\n    x = 0;\n    y = 0;\n    color = aColor;\n  }\n", "page": 300, "type": "text", "section": "Page 300"}
{"text": "  public void draw(Graphics2D g2)\n  {\n    Ellipse2D circle = new Ellipse2D.Double(x, y, size, size);\n    Color oldColor = g2.getColor();\n    g2.setColor(color);\n    g2.fill(circle);\n    g2.setColor(oldColor);\n    g2.draw(circle);\n  }\n  public boolean contains(Point2D p)\n  {\n    Ellipse2D circle = new Ellipse2D.Double(x, y, size, size);\n    return circle.contains(p);\n  }\n  public Point2D getConnectionPoint(Point2D other)\n  {\n    double centerX = x + size / 2;\n    double centerY = y + size / 2;\n    double dx = other.getX() - centerX;\n    double dy = other.getY() - centerY;\n    double distance = Math.sqrt(dx * dx + dy * dy);\n    if (distance == 0) return other;\n    else return new Point2D.Double(\n      centerX + dx * (size / 2) / distance,\n      centerY + dy * (size / 2) / distance);\n  }\n  private double x, y, size, color;\n  private static final int DEFAULT_SIZE = 20;\n}\npublic class LineEdge extends AbstractEdge\n{\n  public void draw(Graphics2D g2)\n  { g2.draw(getConnectionPoints()); }\n  public boolean contains(Point2D aPoint)\n  {\n    final double MAX_DIST = 2;\n    return getConnectionPoints().ptSegDist(aPoint) < MAX_DIST;\n  }\n}\n", "page": 301, "type": "text", "section": "Page 301"}
{"text": "Figure 22.4: Class Diagram for a Simple Graph\nIn summary, Violet provides a simple framework for producing graph editors. To obtain an editor\ninstance, define node and edge classes and provide methods in a graph class that yield prototype\nnode and edge objects.\nOf course, there are other graph frameworks available, such as JGraph [Ald02] and JUNG2.\nHowever, those frameworks are considerably more complex, and they provide frameworks for\ndrawing graphs, not for applications that draw graphs.\n22.3. Use of JavaBeans Properties\nIn the golden days of client-side Java, the JavaBeans specification was developed in order to\nprovide portable mechanisms for editing GUI components in visual GUI builder environments. The\nvision was that a third-party GUI component could be dropped into any GUI builder, where its\nproperties could be configured in the same way as the standard buttons, text components, and\nso on.\nJava does not have native properties. Instead, JavaBeans properties can be discovered as pairs of\ngetter and setter methods, or specified with companion BeanInfo classes. Moreover, property\neditors can be specified for visually editing property values. The JDK even contains a few basic\nproperty editors, for example for the type java.awt.Color.\nThe Violet framework makes full use of the JavaBeans specification. For example, the CircleNode\nclass can expose a color property simply by providing two methods:\npublic void setColor(Color newValue)\npublic Color getColor()\nNo further work is necessary. The graph editor can now edit node colors of circle nodes\n(Figure 22.5).\n", "page": 302, "type": "text", "section": "Page 302"}
{"text": "Figure 22.5: Editing Circle Colors with the default JavaBeans Color Editor\n22.4. Long-Term Persistence\nJust like any editor program, Violet must save the user's creations in a file and reload them later. I\nhad a look at the XMI specification3 which was designed as a common interchange format for UML\nmodels. I found it cumbersome, confusing, and hard to consume. I don't think I was the only one\n\u2014XMI had a reputation for poor interoperability even with the simplest models [PGL+05].\nI considered simply using Java serialization, but it is difficult to read old versions of a serialized\nobject whose implementation has changed over time. This problem was also anticipated by the\nJavaBeans architects, who developed a standard XML format for long-term persistence4. A Java\nobject\u2014in the case of Violet, the UML diagram\u2014is serialized as a sequence of statements for\nconstructing and modifying it. Here is an example:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<java version=\"1.0\" class=\"java.beans.XMLDecoder\">\n <object class=\"com.horstmann.violet.ClassDiagramGraph\">\n  <void method=\"addNode\">\n   <object id=\"ClassNode0\" class=\"com.horstmann.violet.ClassNode\">\n    <void property=\"name\">\u2026</void>\n   </object>\n   <object class=\"java.awt.geom.Point2D$Double\">\n    <double>200.0</double>\n    <double>60.0</double>\n   </object>\n  </void>\n  <void method=\"addNode\">\n   <object id=\"ClassNode1\" class=\"com.horstmann.violet.ClassNode\">\n    <void property=\"name\">\u2026</void>\n   </object>\n   <object class=\"java.awt.geom.Point2D$Double\">\n    <double>200.0</double>\n    <double>210.0</double>\n   </object>\n  </void>\n  <void method=\"connect\">\n   <object class=\"com.horstmann.violet.ClassRelationshipEdge\">\n", "page": 303, "type": "text", "section": "Page 303"}
{"text": "    <void property=\"endArrowHead\">\n     <object class=\"com.horstmann.violet.ArrowHead\" field=\"TRIANGLE\"/>\n    </void>\n   </object>\n   <object idref=\"ClassNode0\"/>\n   <object idref=\"ClassNode1\"/>\n  </void>\n </object>\n</java>\nWhen the XMLDecoder class reads this file, it executes these statements (package names are\nomitted for simplicity).\nClassDiagramGraph obj1 = new ClassDiagramGraph();\nClassNode ClassNode0 = new ClassNode();\nClassNode0.setName(\u2026);\nobj1.addNode(ClassNode0, new Point2D.Double(200, 60));\nClassNode ClassNode1 = new ClassNode();\nClassNode1.setName(\u2026);\nobj1.addNode(ClassNode1, new Point2D.Double(200, 60));\nClassRelationShipEdge obj2 = new ClassRelationShipEdge();\nobj2.setEndArrowHead(ArrowHead.TRIANGLE);\nobj1.connect(obj2, ClassNode0, ClassNode1);\nAs long as the semantics of the constructors, properties, and methods has not changed, a newer\nversion of the program can read a file that has been produced by an older version.\nProducing such files is quite straightforward. The encoder automatically enumerates the\nproperties of each object and writes setter statements for those property values that differ from\nthe default. Most basic datatypes are handled by the Java platform; however, I had to supply\nspecial handlers for Point2D, Line2D, and Rectangle2D. Most importantly, the encoder must\nknow that a graph can be serialized as a sequence of addNode and connect method calls:\nencoder.setPersistenceDelegate(Graph.class, new DefaultPersistenceDelegate()\n{\n  protected void initialize(Class<?> type, Object oldInstance,\n    Object newInstance, Encoder out)\n  {\n    super.initialize(type, oldInstance, newInstance, out);\n    AbstractGraph g = (AbstractGraph) oldInstance;\n    for (Node n : g.getNodes())\n      out.writeStatement(new Statement(oldInstance, \"addNode\", new Object[]\n      {\n        n,\n        n.getLocation()\n      }));\n    for (Edge e : g.getEdges())\n      out.writeStatement(new Statement(oldInstance, \"connect\", new Object[]\n      {\n        e, e.getStart(), e.getEnd()\n      }));\n   }\n });\nOnce the encoder has been configured, saving a graph is as simple as:\nencoder.writeObject(graph);\nSince the decoder simply executes statements, it requires no configuration. Graphs are simply\nread with:\nGraph graph = (Graph) decoder.readObject();\n", "page": 304, "type": "text", "section": "Page 304"}
{"text": "This approach has worked exceedingly well over numerous versions of Violet, with one exception.\nA recent refactoring changed some package names and thereby broke backwards compatibility.\nOne option would have been to keep the classes in the original packages, even though they no\nlonger matched the new package structure. Instead, the maintainer provided an XML transformer\nfor rewriting the package names when reading a legacy file.\n22.5. Java WebStart\nJava WebStart is a technology for launching an application from a web browser. The deployer\nposts a JNLP file that triggers a helper application in the browser which downloads and runs the\nJava program. The application can be digitally signed, in which case the user must accept the\ncertificate, or it can be unsigned, in which case the program runs in a sandbox that is slightly\nmore permissive than the applet sandbox.\nI do not think that end users can or should be trusted to judge the validity of a digital certificate\nand its security implications. One of the strengths of the Java platform is its security, and I feel it is\nimportant to play to that strength.\nThe Java WebStart sandbox is sufficiently powerful to enable users to carry out useful work,\nincluding loading and saving files and printing. These operations are handled securely and\nconveniently from the user perspective. The user is alerted that the application wants to access\nthe local filesystem and then chooses the file to be read or written. The application merely receives\na stream object, without having an opportunity to peek at the filesystem during the file selection\nprocess.\nIt is annoying that the developer must write custom code to interact with a FileOpenService\nand a FileSaveService when the application is running under WebStart, and it is even more\nannoying that there is no WebStart API call to find out whether the application was launched by\nWebStart.\nSimilarly, saving user preferences must be implemented in two ways: using the Java preferences\nAPI when the application runs normally, or using the WebStart preferences service when the\napplication is under WebStart. Printing, on the other hand, is entirely transparent to the application\nprogrammer.\nViolet provides simple abstraction layers over these services to simplify the lot of the application\nprogrammer. For example, here is how to open a file:\nFileService service = FileService.getInstance(initialDirectory);\n  // detects whether we run under WebStart\nFileService.Open open = fileService.open(defaultDirectory, defaultName,\n  extensionFilter);\nInputStream in = open.getInputStream();\nString title = open.getName();\nThe FileService.Open interface is implemented by two classes: a wrapper over JFileChooser\nor the JNLP FileOpenService.\nNo such convenience is a part of the JNLP API itself, but that API has received little love over its\nlifetime and has been widely ignored. Most projects simply use a self-signed certificate for their\nWebStart application, which gives users no security. This is a shame\u2014open source developers\nshould embrace the JNLP sandbox as a risk-free way to try out a project.\n22.6. Java 2D\nViolet makes intensive use of the Java2D library, one of the lesser known gems in the Java API.\nEvery node and edge has a method getShape that yields a java.awt.Shape, the common\ninterface of all Java2D shapes. This interface is implemented by rectangles, circles, paths, and their\nunions, intersections, and differences. The GeneralPath class is useful for making shapes that\nare composed of arbitrary line and quadratic/cubic curve segments, such as straight and curved\narrows.\n", "page": 305, "type": "text", "section": "Page 305"}
{"text": "To appreciate the flexibility of the Java2D API, consider the following code for drawing a shadow in\nthe AbstractNode.draw method:\nShape shape = getShape();\nif (shape == null) return;\ng2.translate(SHADOW_GAP, SHADOW_GAP);\ng2.setColor(SHADOW_COLOR);\ng2.fill(shape);\ng2.translate(-SHADOW_GAP, -SHADOW_GAP);\ng2.setColor(BACKGROUND_COLOR);\ng2.fill(shape);\nA few lines of code produce a shadow for any shape, even shapes that a developer may add at a\nlater point.\nOf course, Violet saves bitmap images in any format that the javax.imageio package supports;\nthat is, GIF, PNG, JPEG, and so on. When my publisher asked me for vector images, I noted\nanother advantage of the Java 2D library. When you print to a PostScript printer, the Java2D\noperations are translated into PostScript vector drawing operations. If you print to a file, the result\ncan be consumed by a program such as ps2eps and then imported into Adobe Illustrator or\nInkscape. Here is the code, where comp is the Swing component whose paintComponent method\npaints the graph:\nDocFlavor flavor = DocFlavor.SERVICE_FORMATTED.PRINTABLE;\nString mimeType = \"application/postscript\";\nStreamPrintServiceFactory[] factories;\nStreamPrintServiceFactory.lookupStreamPrintServiceFactories(flavor, mimeType);\nFileOutputStream out = new FileOutputStream(fileName);\nPrintService service = factories[0].getPrintService(out);\nSimpleDoc doc = new SimpleDoc(new Printable() {\n  public int print(Graphics g, PageFormat pf, int page) {\n      if (page >= 1) return Printable.NO_SUCH_PAGE;\n      else {\n        double sf1 = pf.getImageableWidth() / (comp.getWidth() + 1);\n        double sf2 = pf.getImageableHeight() / (comp.getHeight() + 1);\n        double s = Math.min(sf1, sf2);\n        Graphics2D g2 = (Graphics2D) g;\n        g2.translate((pf.getWidth() - pf.getImageableWidth()) / 2,\n            (pf.getHeight() - pf.getImageableHeight()) / 2);\n        g2.scale(s, s);\n        comp.paint(g);\n        return Printable.PAGE_EXISTS;\n      }\n  }\n}, flavor, null);\nDocPrintJob job = service.createPrintJob();\nPrintRequestAttributeSet attributes = new HashPrintRequestAttributeSet();\njob.print(doc, attributes);\nAt the beginning, I was concerned that there might be a performance penalty when using general\nshapes, but that has proven not to be the case. Clipping works well enough that only those shape\noperations that are required for updating the current viewport are actually executed .\n22.7. No Swing Application Framework\nMost GUI frameworks have some notion of an application that manages a set of documents that\ndeals with menus, toolbars, status bars, etc. However, this was never a part of the Java API. JSR\n2965 was supposed to supply a basic framework for Swing applications, but it is currently inactive.\nThus, a Swing application author has two choices: reinvent a good number of wheels or base itself\non a third party framework. At the time that Violet was written, the primary choices for an\napplication framework were the Eclipse and NetBeans platform, both of which seemed too\n", "page": 306, "type": "text", "section": "Page 306"}
{"text": "heavyweight at the time. (Nowadays, there are more choices, among them JSR 296 forks such as\nGUTS6.) Thus, Violet had to reinvent mechanisms for handling menus and internal frames.\nIn Violet, you specify menu items in property files, like this:\nfile.save.text=Save\nfile.save.mnemonic=S\nfile.save.accelerator=ctrl S\nfile.save.icon=/icons/16x16/save.png\nA utility method creates the menu item from the prefix (here file.save). The suffixes .text,\n.mnemonic, and so on, are what nowadays would be called \"convention over configuration\". Using\nresource files for describing these settings is obviously far superior to setting up menus with API\ncalls because it allows for easy localization. I reused that mechanism in another open source\nproject, the GridWorld environment for high school computer science education7.\nAn application such as Violet allows users to open multiple \"documents\", each containing a graph.\nWhen Violet was first written, the multiple document interface (MDI) was still commonly used. With\nMDI, the main frame has a menu bar, and each view of a document is displayed in an internal\nframe with a title but no menu bar. Each internal frame is contained in the main frame and can be\nresized or minimized by the user. There are operations for cascading and tiling windows.\nMany developers disliked MDI, and so this style user interface has gone out of fashion. For a while,\na single document interface (SDI), in which an application displays multiple top level frames, was\nconsidered superior, presumably because those frames can be manipulated with the standard\nwindow management tools of the host operating system. When it became clear that having lots of\ntop level windows isn't so great after all, tabbed interfaces started to appear, in which multiple\ndocuments are again contained in a single frame, but now all displayed at full size and selectable\nwith tabs. This does not allow users to compare two documents side by side, but seems to have\nwon out.\nThe original version of Violet used an MDI interface. The Java API has a internal frames feature, but\nI had to add support for tiling and cascading. Alexandre switched to a tabbed interface, which is\nsomewhat better-supported by the Java API. It would be desirable to have an application\nframework where the document display policy was transparent to the developer and perhaps\nselectable by the user.\nAlexandre also added support for sidebars, a status bar, a welcome panel, and a splash screen. All\nthis should ideally be a part of a Swing application framework.\n22.8. Undo/Redo\nImplementing multiple undo/redo seems like a daunting task, but the Swing undo package\n([Top00], Chapter 9) gives good architectural guidance. An UndoManager manages a stack of\nUndoableEdit objects. Each of them has an undo method that undoes the effect of the edit\noperation, and a redo method that undoes the undo (that is, carries out the original edit\noperation). A CompoundEdit is a sequence of UndoableEdit operations that should be undone\nor redone in their entirety. You are encouraged to define small, atomic edit operations (such as\nadding or removing a single edge or node in the case of a graph) that are grouped into compound\nedits as necessary.\nA challenge is to define a small set of atomic operations, each of which can be undone easily. In\nViolet, they are:\nadding or removing a node or edge\nattaching or detaching a node's child\nmoving a node\nchanging a property of a node or edge\nEach of these operations has an obvious undo. For example the undo of adding a node is the\nnode's removal. The undo of moving a node is to move it by the opposite vector.\n", "page": 307, "type": "text", "section": "Page 307"}
{"text": "Figure 22.6: An Undo Operation Must Undo Structural Changes in the Model\nNote that these atomic operations are not the same as the actions in the user interface or the\nmethods of the Graph interface that the user interface actions invoke. For example, consider the\nsequence diagram in Figure 22.6, and suppose the user drags the mouse from the activation bar\nto the lifeline on the right. When the mouse button is released, the method:\npublic boolean addEdgeAtPoints(Edge e, Point2D p1, Point2D p2)\nis invoked. That method adds an edge, but it may also carry out other operations, as specified by\nthe participating Edge and Node subclasses. In this case, an activation bar will be added to the\nlifeline on the right. Undoing the operation needs to remove that activation bar as well. Thus, the\nmodel (in our case, the graph) needs to record the structural changes that need to be undone. It\nis not enough to collect controller operations.\nAs envisioned by the Swing undo package, the graph, node, and edge classes should send\nUndoableEditEvent notifications to an UndoManager whenever a structural edit occurs. Violet\nhas a more general design where the graph itself manages listeners for the following interface:\npublic interface GraphModificationListener\n{\n  void nodeAdded(Graph g, Node n);\n  void nodeRemoved(Graph g, Node n);\n  void nodeMoved(Graph g, Node n, double dx, double dy);\n  void childAttached(Graph g, int index, Node p, Node c);\n  void childDetached(Graph g, int index, Node p, Node c);\n  void edgeAdded(Graph g, Edge e);\n  void edgeRemoved(Graph g, Edge e);\n  void propertyChangedOnNodeOrEdge(Graph g, PropertyChangeEvent event);\n}\nThe framework installs a listener into each graph that is a bridge to the undo manager. For\nsupporting undo, adding generic listener support to the model is overdesigned\u2014the graph\noperations could directly interact with the undo manager. However, I also wanted to support an\nexperimental collaborative editing feature.\nIf you want to support undo/redo in your application, think carefully about the atomic operations\nin your model (and not your user interface). In the model, fire events when a structural change\n", "page": 308, "type": "text", "section": "Page 308"}
{"text": "happens, and allow the Swing undo manager to collect and group these events.\n22.9. Plugin Architecture\nFor a programmer familiar with 2D graphics, it is not difficult to add a new diagram type to Violet.\nFor example, the activity diagrams were contributed by a third party. When I needed to create\nrailroad diagrams and ER diagrams, I found it faster to write Violet extensions instead of fussing\nwith Visio or Dia. (Each diagram type took a day to implement.)\nThese implementations do not require knowledge of the full Violet framework. Only the graph,\nnode, and edge interfaces and convenience implementations are needed. In order to make it\neasier for contributors to decouple themselves from the evolution of the framework, I designed a\nsimple plugin architecture.\nOf course, many programs have a plugin architecture, many quite elaborate. When someone\nsuggested that Violet should support OSGi, I shuddered and instead implemented the simplest\nthing that works.\nContributors simply produce a JAR file with their graph, node, and edge implementations and drop\nit into a plugins directory. When Violet starts, it loads those plugins, using the Java\nServiceLoader class. That class was designed to load services such as JDBC drivers. A\nServiceLoader loads JAR files that promise to provide a class implementing a given interface (in\nour case, the Graph interface.)\nEach JAR file must have a subdirectory META-INF/services containing a file whose name is the\nfully qualified classname of the interface (such as com.horstmann.violet.Graph), and that\ncontains the names of all implementing classes, one per line. The ServiceLoader constructs a\nclass loader for the plugin directory, and loads all plugins:\nServiceLoader<Graph> graphLoader = ServiceLoader.load(Graph.class, classLoader);\nfor (Graph g : graphLoader) // ServiceLoader<Graph> implements Iterable<Graph>\n  registerGraph(g);\nThis is a simple but useful facility of standard Java that you might find valuable for your own\nprojects.\n22.10. Conclusion\nLike so many open source projects, Violet was born of an unmet need\u2014to draw simple UML\ndiagrams with a minimum of fuss. Violet was made possible by the amazing breadth of the Java SE\nplatform, and it draws from a diverse set of technologies that are a part of that platform. In this\narticle, I described how Violet makes use of Java Beans, Long-Term Persistence, Java Web Start,\nJava 2D, Swing Undo/Redo, and the service loader facility. These technologies are not always as\nwell understood as the basics of Java and Swing, but they can greatly simplify the architecture of a\ndesktop application. They allowed me, as the initial sole developer, to produce a successful\napplication in a few months of part-time work. Relying on these standard mechanisms also made it\neasier for others to improve on Violet and to extract pieces of it into their own projects.\nFootnotes\n1. At the time, I was not aware of Diomidis Spinellis' admirable UMLGraph program [Spi03]\n2. http://jung.sourceforge.net\n3. http://www.omg.org/technology/documents/formal/xmi.htm\n4. http://jcp.org/en/jsr/detail?id=57\n5. http://jcp.org/en/jsr/detail?id=296\n6. http://kenai.com/projects/guts\n7. http://horstmann.com/gridworld\n", "page": 309, "type": "text", "section": "Page 309"}
{"text": "The Architecture of\nOpen Source Applications\nAmy Brown and Greg Wilson (eds.)\nISBN 978-1-257-63801-7 \nLicense / Buy / Contribute\nChapter 23. VisTrails\nJuliana Freire, David Koop, Emanuele Santos, Carlos\nScheidegger, Claudio Silva, and Huy T. Vo\nVisTrails1 is an open-source system that supports data exploration and visualization. It includes\nand substantially extends useful features of scientific workflow and visualization systems. Like\nscientific workflow systems such as Kepler and Taverna, VisTrails allows the specification of\ncomputational processes which integrate existing applications, loosely-coupled resources, and\nlibraries according to a set of rules. Like visualization systems such as AVS and ParaView, VisTrails\nmakes advanced scientific and information visualization techniques available to users, allowing\nthem to explore and compare different visual representations of their data. As a result, users can\ncreate complex workflows that encompass important steps of scientific discovery, from data\ngathering and manipulation to complex analyses and visualizations, all integrated in one system.\nA distinguishing feature of VisTrails is its provenance infrastructure [FSC+06]. VisTrails captures\nand maintains a detailed history of the steps followed and data derived in the course of an\nexploratory task. Workflows have traditionally been used to automate repetitive tasks, but in\napplications that are exploratory in nature, such as data analysis and visualization, very little is\nrepeated\u2014change is the norm. As a user generates and evaluates hypotheses about their data, a\nseries of different, but related, workflows are created as they are adjusted iteratively.\nVisTrails was designed to manage these rapidly-evolving workflows: it maintains provenance of\ndata products (e.g., visualizations, plots), of the workflows that derive these products, and their\nexecutions. The system also provides annotation capabilities so users can enrich the\nautomatically-captured provenance.\nBesides enabling reproducible results, VisTrails leverages provenance information through a series\nof operations and intuitive user interfaces that help users to collaboratively analyze data. Notably,\nthe system supports reflective reasoning by storing temporary results, allowing users to examine\nthe actions that led to a result and to follow chains of reasoning backward and forward. Users can\nnavigate workflow versions in an intuitive way, undo changes without losing results, visually\ncompare multiple workflows and show their results side-by-side in a visualization spreadsheet.\nVisTrails addresses important usability issues that have hampered a wider adoption of workflow\nand visualization systems. To cater to a broader set of users, including many who do not have\nprogramming expertise, it provides a series of operations and user interfaces that simplify\nworkflow design and use [FSC+06], including the ability to create and refine workflows by analogy,\nto query workflows by example, and to suggest workflow completions as users interactively\nconstruct their workflows using a recommendation system [SVK+07]. We have also developed a\nnew framework that allows the creation of custom applications that can be more easily deployed\nto (non-expert) end users.\nThe extensibility of VisTrails comes from an infrastructure that makes it simple for users to\nintegrate tools and libraries, as well as to quickly prototype new functions. This has been\ninstrumental in enabling the use of the system in a wide range of application areas, including\nenvironmental sciences, psychiatry, astronomy, cosmology, high-energy physics, quantum\nphysics, and molecular modeling.\nTo keep the system open-source and free for all, we have built VisTrails using only free, open-\nsource packages. VisTrails is written in Python and uses Qt as its GUI toolkit (through PyQt\nPython bindings). Because of the broad range of users and applications, we have designed the\nsystem from the ground up with portability in mind. VisTrails runs on Windows, Mac and Linux.\n", "page": 310, "type": "text", "section": "Page 310"}
{"text": "Figure 23.1: Components of the VisTrails User Interface\n23.1. System Overview\nData exploration is an inherently creative process that requires users to locate relevant data, to\nintegrate and visualize this data, to collaborate with peers while exploring different solutions, and\nto disseminate results. Given the size of data and complexity of analyses that are common in\nscientific exploration, tools are needed that better support creativity.\nThere are two basic requirements for these tools that go hand in hand. First, it is important to be\nable to specify the exploration processes using formal descriptions, which ideally, are executable.\nSecond, to reproduce the results of these processes as well as reason about the different steps\nfollowed to solve a problem, these tools must have the ability to systematically capture\nprovenance. VisTrails was designed with these requirements in mind.\n23.1.1. Workflows and Workflow-Based Systems\nWorkflow systems support the creation of pipelines (workflows) that combine multiple tools. As\nsuch, they enable the automation of repetitive tasks and result reproducibility. Workflows are\nrapidly replacing primitive shell scripts in a wide range of tasks, as evidenced by a number of\nworkflow-based applications, both commercial (e.g., Apple's Mac OS X Automator and Yahoo!\nPipes) and academic (e.g., NiPype, Kepler, and Taverna).\nWorkflows have a number of advantages compared to scripts and programs written in high-level\nlanguages. They provide a simple programming model whereby a sequence of tasks is composed\nby connecting the outputs of one task to the inputs of another. Figure 23.1 shows a workflow\nwhich reads a CSV file that contains weather observations and creates a scatter plot of the values.\nThis simpler programming model allows workflow systems to provide intuitive visual programming\ninterfaces, which make them more suitable for users who do not have substantial programming\nexpertise. Workflows also have an explicit structure: they can be viewed as graphs, where nodes\nrepresent processes (or modules) along with their parameters and edges capture the flow of data\nbetween the processes. In the example of Figure 23.1, the module CSVReader takes as a\nparameter a filename (/weather/temp_precip.dat), reads the file, and feeds its contents into\nthe modules GetTemperature and GetPrecipitation, which in turn send the temperature and\nprecipitation values to a matplotlib function that generates a scatter plot.\nMost workflow systems are designed for a specific application area. For example, Taverna targets\nbioinformatics workflows, and NiPype allows the creation of neuroimaging workflows. While\nVisTrails supports much of the functionality provided by other workflow systems, it was designed\nto support general exploratory tasks in a broad range of areas, integrating multiple tools, libraries,\nand services.\n23.1.2. Data and Workflow Provenance\nThe importance of keeping provenance information for results (and data products) is well\nrecognized in the scientific community. The provenance (also referred to as the audit trail, lineage,\nand pedigree) of a data product contains information about the process and data used to derive\n", "page": 311, "type": "text", "section": "Page 311"}
{"text": "the data product. Provenance provides important documentation that is key to preserving the\ndata, to determining the data's quality and authorship, and to reproducing as well as validating the\nresults [FKSS08].\nAn important component of provenance is information about causality, i.e., a description of a\nprocess (sequence of steps) which, together with input data and parameters, caused the creation\nof a data product. Thus, the structure of provenance mirrors the structure of the workflow (or\nset of workflows) used to derive a given result set.\nIn fact, a catalyst for the widespread use of workflow systems in science has been that they can\nbe easily used to automatically capture provenance. While early workflow systems have been\nextended to capture provenance, VisTrails was designed to support provenance.\nFigure 23.2: Provenance of Exploration Enhanced by Annotations\n23.1.3. User Interface and Basic Functionality\nThe different user interface components of the system are illustrated in Figure 23.1 and\nFigure 23.2. Users create and edit workflows using the Workflow Editor.\nTo build the workflow graphs, users can drag modules from the Module Registry and drop them\ninto the Workflow Editor canvas. VisTrails provides a series of built-in modules, and users can also\nadd their own (see Section 23.3 for details). When a module is selected, VisTrails displays its\nparameters (in the Parameter Edits area) where the user can set and modify their values.\nAs a workflow specification is refined, the system captures the changes and presents them to the\nuser in the Version Tree View described below. Users may interact with the workflows and their\nresults in the VisTrails Spreadsheet. Each cell in the spreadsheet represents a view that\ncorresponds to a workflow instance. In Figure 23.1, the results of the workflow shown in the\nWorkflow Editor are displayed on the top-left cell of the spreadsheet. Users can directly modify\nthe parameters of a workflow as well as synchronize parameters across different cells in the\nspreadsheet.\nThe Version Tree View helps users to navigate through the different workflow versions. As shown\nin Figure 23.2, by clicking on a node in the version tree, users can view a workflow, its associated\nresult (Visualization Preview), and metadata. Some of the metadata is automatically captured, e.g.,\nthe id of the user who created a particular workflow and the creation date, but users may also\nprovide additional metadata, including a tag to identify the workflow and a written description.\n", "page": 312, "type": "text", "section": "Page 312"}
{"text": "Figure 23.3: VisTrails Architecture\n23.2. Project History\nInitial versions of versions of VisTrails were written in Java and C++ [BCC+05]. The C++ version\nwas distributed to a few early adopters, whose feedback was instrumental in shaping our\nrequirements for the system.\nHaving observed a trend in the increase of the number of Python-based libraries and tools in\nmultiple scientific communities, we opted to use Python as the basis for VisTrails. Python is quickly\nbecoming a universal modern glue language for scientific software. Many libraries written in\ndifferent languages such as Fortran, C, and C++ use Python bindings as a way to provide\nscripting capabilities. Since VisTrails aims to facilitate the orchestration of many different software\nlibraries in workflows, a pure Python implementation makes this much easier. In particular, Python\nhas dynamic code loading features similar to the ones seen in LISP environments, while having a\nmuch bigger developer community, and an extremely rich standard library. Late in 2005, we\nstarted the development of the current system using Python/PyQt/Qt. This choice has greatly\nsimplified extensions to the system, in particular, the addition of new modules and packages.\nA beta version of the VisTrails system was first released in January 2007. Since then, the system\nhas been downloaded over twenty-five thousand times.\n23.3. Inside VisTrails\nThe internal components that support the user-interface functionality described above are\ndepicted in the high-level architecture of VisTrails, shown in Figure 23.3. Workflow execution is\ncontrolled by the Execution Engine, which keeps track of invoked operations and their respective\nparameters and captures the provenance of workflow execution (Execution Provenance). As part\nof the execution, VisTrails also allows the caching of intermediate results both in memory and on\ndisk. As we discuss in Section 23.3, only new combinations of modules and parameters are re-\nrun, and these are executed by invoking the appropriate functions from the underlying libraries\n(e.g., matplotlib). Workflow results, connected to their provenance, can then be included in\nelectronic documents (Section 23.4).\nInformation about changes to workflows is captured in a Version Tree, which can be persisted\n", "page": 313, "type": "text", "section": "Page 313"}
{"text": "using different storage back ends, including an XML file store in a local directory and a relational\ndatabase. VisTrails also provides a query engine that allows users to explore the provenance\ninformation.\nWe note that, although VisTrails was designed as an interactive tool, it can also be used in server\nmode. Once workflows are created, they can be executed by a VisTrails server. This feature is\nuseful in a number of scenarios, including the creation of Web-based interfaces that allows users\nto interact with workflows and the ability to run workflows in high-performance computing\nenvironments.\n23.3.1. The Version Tree: Change-Based Provenance\nFigure 23.4: Change-Based Provenance Model\nA new concept we introduced with VisTrails is the notion of provenance of workflow evolution\n[FSC+06]. In contrast to previous workflow and workflow-based visualization systems, which\nmaintain provenance only for derived data products, VisTrails treats the workflows as first-class\ndata items and also captures their provenance. The availability of workflow-evolution provenance\nsupports reflective reasoning. Users can explore multiple chains of reasoning without losing any\nresults, and because the system stores intermediate results, users can reason about and make\ninferences from this information. It also enables a series of operations which simplify exploratory\nprocesses. For example, users can easily navigate through the space of workflows created for a\ngiven task, visually compare the workflows and their results (see Figure 23.4), and explore (large)\nparameter spaces. In addition, users can query the provenance information and learn by example.\nThe workflow evolution is captured using the change-based provenance model. As illustrated in\nFigure 23.4, VisTrails stores the operations or changes that are applied to workflows (e.g., the\naddition of a module, the modification of a parameter, etc.), akin to a database transaction log.\nThis information is modeled as a tree, where each node corresponds to a workflow version, and\nan edge between a parent and a child node represents the change applied to the parent to obtain\nthe child. We use the terms version tree and vistrail (short for visual trail) interchangeably to refer\nto this tree. Note that the change-based model uniformly captures both changes to parameter\nvalues and to workflow definitions. This sequence of changes is sufficient to determine the\nprovenance of data products and it also captures information about how a workflow evolves over\ntime. The model is both simple and compact\u2014it uses substantially less space than the alternative\nof storing multiple versions of a workflow.\nThere are a number of benefits that come from the use of this model. Figure 23.4 shows the\nvisual difference functionality that VisTrails provides for comparing two workflows. Although the\nworkflows are represented as graphs, using the change-based model, comparing two workflows\nbecomes very simple: it suffices to navigate the version tree and identify the series of actions\nrequired to transform one workflow into the other.\nAnother important benefit of the change-based provenance model is that the underlying version\ntree can serve as a mechanism to support collaboration. Because designing workflows is a\nnotoriously difficult task, it often requires multiple users to collaborate. Not only does the version\n", "page": 314, "type": "text", "section": "Page 314"}
{"text": "tree provide an intuitive way to visualize the contribution of different users (e.g., by coloring nodes\naccording to the user who created the corresponding workflow), but the monotonicity of the\nmodel allows for simple algorithms for synchronizing changes performed by multiple users.\nProvenance information can be easily captured while a workflow is being executed. Once the\nexecution completes, it is also important to maintain strong links between a data product and its\nprovenance, i.e., the workflow, parameters and input files used to derive the data product. When\ndata files or provenance are moved or modified, it can be difficult to find the data associated with\nthe provenance or to find the provenance associated with the data. VisTrails provides a persistent\nstorage mechanism that manages input, intermediate, and output data files, strengthening the\nlinks between provenance and data. This mechanism provides better support for reproducibility\nbecause it ensures the data referenced in provenance information can be readily (and correctly)\nlocated. Another important benefit of such management is that it allows caching of intermediate\ndata which can then be shared with other users.\n23.3.2. Workflow Execution and Caching\nThe execution engine in VisTrails was designed to allow the integration of new and existing tools\nand libraries. We tried to accommodate different styles commonly used for wrapping third-party\nscientific visualization and computation software. In particular, VisTrails can be integrated with\napplication libraries that exist either as pre-compiled binaries that are executed on a shell and use\nfiles as input/outputs, or as C++/Java/Python class libraries that pass internal objects as\ninput/output.\nVisTrails adopts a dataflow execution model, where each module performs a computation and the\ndata produced by a module flows through the connections that exist between modules. Modules\nare executed in a bottom-up fashion; each input is generated on-demand by recursively executing\nupstream modules (we say module A is upstream of B when there is a sequence of connections\nthat goes from A to B). The intermediate data is temporarily stored either in memory (as a Python\nobject) or on disk (wrapped by a Python object that contains information on accessing the data).\nTo allow users to add their own functionality to VisTrails, we built an extensible package system\n(see Section 23.3). Packages allow users to include their own or third-party modules in VisTrails\nworkflows. A package developer must identify a set of computational modules and for each,\nidentify the input and output ports as well as define the computation. For existing libraries, a\ncompute method needs to specify the translation from input ports to parameters for the existing\nfunction and the mapping from result values to output ports.\nIn exploratory tasks, similar workflows, which share common sub-structures, are often executed\nin close succession. To improve the efficiency of workflow execution, VisTrails caches intermediate\nresults to minimize recomputation. Because we reuse previous execution results, we implicitly\nassume that cacheable modules are functional: given the same inputs, modules will produce the\nsame outputs. This requirement imposes definite behavior restrictions on classes, but we believe\nthey are reasonable.\nThere are, however, obvious situations where this behavior is unattainable. For example, a module\nthat uploads a file to a remote server or saves a file to disk has a significant side effect while its\noutput is relatively unimportant. Other modules might use randomization, and their non-\ndeterminism might be desirable; such modules can be flagged as non-cacheable. However, some\nmodules that are not naturally functional can be converted; a function that writes data to two files\nmight be wrapped to output the contents of the files.\n23.3.3. Data Serialization and Storage\nOne of the key components of any system supporting provenance is the serialization and storage\nof data. VisTrails originally stored data in XML via simple fromXML and toXML methods embedded\nin its internal objects (e.g., the version tree, each module). To support the evolution of the schema\nof these objects, these functions encoded any translation between schema versions as well. As\nthe project progressed, our user base grew, and we decided to support different serializations,\nincluding relational stores. In addition, as schema objects evolved, we needed to maintain better\ninfrastructure for common data management concerns like versioning schemas, translating\nbetween versions, and supporting entity relationships. To do so, we added a new database (db)\nlayer.\nThe db layer is composed of three core components: the domain objects, the service logic, and\nthe persistence methods. The domain and persistence components are versioned so that each\nschema version has its own set of classes. This way, we maintain code to read each version of the\nschema. There are also classes that define translations for objects from one schema version to\nthose of another. The service classes provide methods to interface with data and deal with\ndetection and translation of schema versions.\n", "page": 315, "type": "text", "section": "Page 315"}
{"text": "Because writing much of this code is tedious and repetitive, we use templates and a meta-schema\nto define both the object layout (and any in-memory indices) and the serialization code. The meta-\nschema is written in XML, and is extensible in that serializations other than the default XML and\nrelational mappings VisTrails defines can be added. This is similar to object-relational mappings and\nframeworks like Hibernate2 and SQLObject3, but adds some special routines to automate tasks\nlike re-mapping identifiers and translating objects from one schema version to the next. In\naddition, we can also use the same meta-schema to generate serialization code for many\nlanguages. After originally writing meta-Python, where the domain and persistence code was\ngenerated by running Python code with variables obtained from the meta-schema, we have\nrecently migrated to Mako templates4.\nAutomatic translation is key for users that need to migrate their data to newer versions of the\nsystem. Our design adds hooks to make this translation slightly less painful for developers.\nBecause we maintain a copy of code for each version, the translation code just needs to map one\nversion to another. At the root level, we define a map to identify how any version can be\ntransformed to any other. For distant versions, this usually involves a chain through multiple\nintermediate versions. Initially, this was a forward-only map, meaning new versions could not be\ntranslated to old versions, but reverse mappings have been added for more-recent schema\nmappings.\nEach object has an update_version method that takes a different version of an object and\nreturns the current version. By default, it does a recursive translation where each object is\nupgraded by mapping fields of the old object to those in a new version. This mapping defaults to\ncopying each field to one with the same name, but it is possible to define a method to \"override\"\nthe default behavior for any field. An override is a method that takes the old object and returns a\nnew version. Because most changes to the schema only affect a small number of fields, the\ndefault mappings cover most cases, but the overrides provide a flexible means for defining local\nchanges.\n23.3.4. Extensibility Through Packages and Python\nThe first prototype of VisTrails had a fixed set of modules. It was an ideal environment to develop\nbasic ideas about the VisTrails version tree and the caching of multiple execution runs, but it\nseverely limited long-term utility.\nWe see VisTrails as infrastructure for computational science, and that means, literally, that the\nsystem should provide scaffolding for other tools and processes to be developed. An essential\nrequirement of this scenario is extensibility. A typical way to achieve this involves defining a target\nlanguage and writing an appropriate interpreter. This is appealing because of the intimate control it\noffers over execution. This appeal is amplified in light of our caching requirements. However,\nimplementing a full-fledged programming language is a large endeavor that has never been our\nprimary goal. More importantly, forcing users who are just trying to use VisTrails to learn an\nentirely new language was out of the question.\nWe wanted a system which made it easy for a user to add custom functionality. At the same time,\nwe needed the system to be powerful enough to express fairly complicated pieces of software. As\nan example, VisTrails supports the VTK visualization library5. VTK contains about 1000 classes,\nwhich change depending on compilation, configuration, and operating system. Since it seems\ncounterproductive and ultimately hopeless to write different code paths for all these cases, we\ndecided it was necessary to dynamically determine the set of VisTrails modules provided by any\ngiven package, and VTK naturally became our model target for a complex package.\nComputational science was one of the areas we originally targeted, and at the time we designed\nthe system, Python was becoming popular as \"glue code\" among these scientists. By specifying\nthe behavior of user-defined VisTrails modules using Python itself, we would all but eliminate a\nlarge barrier for adoption. As it turns out, Python offers a nice infrastructure for dynamically-\ndefined classes and reflection. Almost every definition in Python has an equivalent form as a first-\nclass expression. The two important reflection features of Python for our package system are:\nPython classes can be defined dynamically via function calls to the type callable. The return\nvalue is a representation of a class that can be used in exactly the same way that a typically-\ndefined Python class can.\nPython modules can be imported via function calls to __import__, and the resulting value\nbehaves in the same way as the identifier in a standard import statement. The path from\nwhich these modules come from can also be specified at runtime.\nUsing Python as our target has a few disadvantages, of course. First of all, this dynamic nature of\nPython means that while we would like to ensure some things like type safety of VisTrails\npackages, this is in general not possible. More importantly, some of the requirements for VisTrails\nmodules, notably the ones regarding referential transparency (more on that later) cannot be\n", "page": 316, "type": "text", "section": "Page 316"}
{"text": "enforced in Python. Still, we believe that it is worthwhile to restrict the allowed constructs in\nPython via cultural mechanisms, and with this caveat, Python is an extremely attractive language\nfor software extensibility.\n23.3.5. VisTrails Packages and Bundles\nA VisTrails package encapsulates a set of modules. Its most common representation in disk is the\nsame representation as a Python package (in a possibly unfortunate naming clash). A Python\npackage consists of a set of Python files which define Python values such as functions and\nclasses. A VisTrails package is a Python package that respects a particular interface. It has files\nthat define specific functions and variables. In its simplest form, a VisTrails package should be a\ndirectory containing two files: __init__.py and init.py.\nThe first file __init__.py is a requirement of Python packages, and should only contain a few\ndefinitions which should be constant. Although there is no way to guarantee that this is the case,\nVisTrails packages failing to obey this are considered buggy. The values defined in the file include a\nglobally unique identifier for the package which is used to distinguish modules when workflows are\nserialized, and package versions (package versions become important when handling workflow\nand package upgrades, see Section 23.4). This file can also include functions called\npackage_dependencies and package_requirements. Since we allow VisTrails modules to\nsubclass from other VisTrails modules beside the root Module class, it is conceivable for one\nVisTrails package to extend the behavior of another, and so one package needs to be initialized\nbefore another. These inter-package dependencies are specified by package_dependencies. The\npackage_requirements function, on the other hand, specifies system-level library requirements\nwhich VisTrails, in some cases, can try to automatically satisfy, through its bundle abstraction.\nA bundle is a system-level package that VisTrails manages via system-specific tools such as\nRedHat's RPM or Ubuntu's APT. When these properties are satisfied, VisTrails can determine the\npackage properties by directly importing the Python module and accessing the appropriate\nvariables.\nThe second file, init.py, contains the entry points for all the actual VisTrails module definitions.\nThe most important feature of this file is the definition of two functions, initialize and\nfinalize. The initialize function is called when a package is enabled, after all the dependent\npackages have themselves been enabled. It performs setup tasks for all of the modules in a\npackage. The finalize function, on the other hand, is usually used to release runtime resources\n(for example, temporary files created by the package can be cleaned up).\nEach VisTrails module is represented in a package by one Python class. To register this class in\nVisTrails, a package developer calls the add_module function once for each VisTrails module.\nThese VisTrails modules can be arbitrary Python classes, but they must respect a few\nrequirements. The first of these is that each must be a subclass of a basic Python class defined by\nVisTrails called, perhaps boringly, Module. VisTrails modules can use multiple inheritance, but only\none of the classes should be a VisTrails module\u2014no diamond hierarchies in the VisTrails module\ntree are allowed. Multiple inheritance becomes useful in particular to define class mix-ins: simple\nbehaviors encoded by parent classes which can be composed together to create more\ncomplicated behaviors.\nThe set of available ports determine the interface of a VisTrails module, and so impact not only the\ndisplay of these modules but also their connectivity to other modules. These ports, then, must be\nexplicitly described to the VisTrails infrastructure. This can be done either by making appropriate\ncalls to add_input_port and add_output_port during the call to initialize, or by specifying\nthe per-class lists _input_ports and _output_ports for each VisTrails module.\nEach module specifies the computation to be performed by overriding the compute method. Data\nis passed between modules through ports, and accessed through the get_input_from_port\nand set_result methods. In traditional dataflow environments, execution order is specified on-\ndemand by the data requests. In our case, the execution order is specified by the topological\nsorting of the workflow modules. Since the caching algorithm requires an acyclic graph, we\nschedule the execution in reverse topological sorted order, so the calls to these functions do not\ntrigger executions of upstream modules. We made this decision deliberately: it makes it simpler to\nconsider the behavior of each module separately from all the others, which makes our caching\nstrategy simpler and more robust.\nAs a general guideline, VisTrails modules should refrain from using functions with side-effects\nduring the evaluation of the compute method. As discussed in Section 23.3, this requirement\nmakes caching of partial workflow runs possible: if a module respects this property, then its\nbehavior is a function of the outputs of upstream modules. Every acyclic subgraph then only\nneeds to be computed once, and the results can be reused.\n23.3.6. Passing Data as Modules\n", "page": 317, "type": "text", "section": "Page 317"}
{"text": "One peculiar feature of VisTrails modules and their communication is that the data that is passed\nbetween VisTrails modules are themselves VisTrails modules. In VisTrails, there is a single\nhierarchy for module and data classes. For example, a module can provide itself as an output of a\ncomputation (and, in fact, every module provides a default \"self\" output port). The main\ndisadvantage is the loss of conceptual separation between computation and data that is\nsometimes seen in dataflow-based architectures. There are, however, two big advantages. The\nfirst is that this closely mimics the object type systems of Java and C++, and the choice was not\naccidental: it was very important for us to support automatic wrapping of large class libraries such\nas VTK. These libraries allow objects to produce other objects as computational results, making a\nwrapping that distinguishes between computation and data more complicated.\nThe second advantage this decision brings is that defining constant values and user-settable\nparameters in workflows becomes easier and more uniformly integrated with the rest of the\nsystem. Consider, for example, a workflow that loads a file from a location on the Web specified\nby a constant. This is currently specified by a GUI in which the URL can be specified as a\nparameter (see the Parameter Edits area in Figure 23.1). A natural modification of this workflow is\nto use it to fetch a URL that is computed somewhere upstream. We would like the rest of the\nworkflow to change as little as possible. By assuming modules can output themselves, we can\nsimply connect a string with the right value to the port corresponding to the parameter. Since the\noutput of a constant evaluates to itself, the behavior is exactly the same as if the value had\nactually been specified as a constant.\nFigure 23.5: Prototyping New Functionality with the PythonSource Module\nThere are other considerations involved in designing constants. Each constant type has a different\nideal GUI interface for specifying values. For example, in VisTrails, a file constant module provides\na file chooser dialog; a Boolean value is specified by a checkbox; a color value has a color picker\nnative to each operating system. To achieve this generality, a developer must subclass a custom\nconstant from the Constant base class and provide overrides which define an appropriate GUI\nwidget and a string representation (so that arbitrary constants can be serialized to disk).\nWe note that, for simple prototyping tasks, VisTrails provides a built-in PythonSource module. A\nPythonSource module can be used to directly insert scripts into a workflow. The configuration\nwindow for PythonSource (see Figure 23.5) allows multiple input and output ports to be specified\nalong with the Python code that is to be executed.\n23.4. Components and Features\nAs discussed above, VisTrails provides a set of functionalities and user interfaces that simplify the\ncreation and execution of exploratory computational tasks. Below, we describe some of these. We\nalso briefly discuss how VisTrails is being used as the basis for an infrastructure that supports the\ncreation of provenance-rich publications. For a more comprehensive description of VisTrails and\nits features, see VisTrails' online documentation6.\n", "page": 318, "type": "text", "section": "Page 318"}
{"text": "Figure 23.6: The Visual Spreadsheet\n23.4.1. Visual Spreadsheet\nVisTrails allows users to explore and compare results from multiple workflows using the Visual\nSpreadsheet (see Figure 23.6). The spreadsheet is a VisTrails package with its own interface\ncomposed of sheets and cells. Each sheet contains a set of cells and has a customizable layout. A\ncell contains the visual representation of a result produced by a workflow, and can be customized\nto display diverse types of data.\nTo display a cell on the spreadsheet, a workflow must contain a module that is derived from the\nbase SpreadsheetCell module. Each SpreadsheetCell module corresponds to a cell in the\nspreadsheet, so one workflow can generate multiple cells. The compute method of the\nSpreadsheetCell module handles the communication between the Execution Engine\n(Figure 23.3) and the spreadsheet. During execution, the spreadsheet creates a cell according to\nits type on-demand by taking advantage of Python's dynamic class instantiation. Thus, custom\nvisual representations can be achieved by creating a subclass of SpreadsheetCell and having\nits compute method send a custom cell type to the spreadsheet. For example, the workflow in\nFigure 23.1, MplFigureCell is a SpreadsheetCell module designed to display images created\nby matplotlib.\nSince the spreadsheet uses PyQt as its GUI back end, custom cell widgets must be subclassed\nfrom PyQt's QWidget. They must also define the updateContents method, which is invoked by\nthe spreadsheet to update the widget when new data arrives. Each cell widget may optionally\ndefine a custom toolbar by implementing the toolbar method; it will be displayed in the\nspreadsheet toolbar area when the cell is selected.\nFigure 23.6 shows the spreadsheet when a VTK cell is selected, in this case, the toolbar provides\nspecific widgets to export PDF images, save camera positions back to the workflow, and create\nanimations. The spreadsheet package defines a customizable QCellWidget, which provides\ncommon features such as history replay (animation) and multi-touch events forwarding. This can\nbe used in place of QWidget for faster development of new cell types.\nEven though the spreadsheet only accepts PyQt widgets as cell types, it is possible to integrate\nwidgets written with other GUI toolkits. To do so, the widget must export its elements to the\nnative platform, and PyQt can then be used to grab it. We use this approach for the VTKCell\nwidget because the actual widget is written in C++. At run-time, the VTKCell grabs the window\nid, a Win32, X11, or Cocoa/Carbon handle depending on the system, and maps it to the\nspreadsheet canvas.\nLike cells, sheets may also be customized. By default, each sheet lives in a tabbed view and has a\ntabular layout. However, any sheet can be undocked from the spreadsheet window, allowing\nmultiple sheets to be visible at once. It is also possible to create a different sheet layout by\nsubclassing the StandardWidgetSheet, also a PyQt widget. The StandardWidgetSheet\nmanages cell layouts as well as interactions with the spreadsheet in editing mode. In editing mode,\nusers can manipulate the cell layout and perform advanced actions on the cells, rather than\ninteracting with cell contents. Such actions include applying analogies (see Section 23.4) and\ncreating new workflow versions from parameter explorations.\n", "page": 319, "type": "text", "section": "Page 319"}
{"text": "23.4.2. Visual Differences and Analogies\nAs we designed VisTrails, we wanted to enable the use of provenance information in addition to its\ncapture. First, we wanted users to see the exact differences between versions, but we then\nrealized that a more helpful feature was being able to apply these differences to other workflows.\nBoth of these tasks are possible because VisTrails tracks the evolution of workflows.\nBecause the version tree captures all of the changes and we can invert each action, we can find a\ncomplete sequence of actions that transform one version to another. Note that some changes will\ncancel each other out, making it possible to compress this sequence. For example, the addition of\na module that was later deleted need not be examined when computing the difference. Finally, we\nhave some heuristics to further simplify the sequence: when the same module occurs in both\nworkflows but was added through separate actions, we we cancel the adds and deletes.\nFrom the set of changes, we can create a visual representation that shows similar and different\nmodules, connections, and parameters. This is illustrated in Figure 23.4. Modules and connections\nthat appear in both workflows are colored gray, and those appearing in only one are colored\naccording to the workflow they appear in. Matching modules with different parameters are shaded\na lighter gray and a user can inspect the parameter differences for a specific module in a table\nthat shows the values in each workflow.\nThe analogy operation allows users to take these differences and apply them to other workflows.\nIf a user has made a set of changes to an existing workflow (e.g., changing the resolution and file\nformat of an output image), he can apply the same changes to other workflows via an analogy. To\ndo so, the user selects a source and a target workflow, which delimits the set of desired changes,\nas well as the workflow they wish to apply the analogy to. VisTrails computes the difference\nbetween the first two workflows as a template, and then determines how to remap this difference\nin order to apply it to the third workflow. Because it is possible to apply differences to workflows\nthat do not exactly match the starting workflow, we need a soft matching that allows\ncorrespondences between similar modules. With this matching, we can remap the difference so\nthe sequence of changes can be applied to the selected workflow [SVK+07]. The method is not\nfoolproof and may generate new workflows that are not exactly what was desired. In such cases,\na user may try to fix any introduced mistakes, or go back to the previous version and apply the\nchanges manually.\nTo compute the soft matching used in analogies, we want to balance local matches (identical or\nvery similar modules) with the overall workflow structure. Note that the computation of even the\nidentical matching is inefficient due to the hardness of subgraph isomorphism, so we need to\nemploy a heuristic. In short, if two somewhat-similar modules in the two workflows share similar\nneighbors, we might conclude that these two modules function similarly and should be matched\nas well. More formally, we construct a product graph where each node is a possible pairing of\nmodules in the original workflows and an edge denotes shared connections. Then, we run steps\ndiffusing the scores at each node across the edges to neighboring nodes. This is a Markov\nprocess similar to Google's PageRank, and will eventually converge leaving a set of scores that\nnow includes some global information. From these scores, we can determine the best matching,\nusing a threshold to leave very dissimilar modules unpaired.\n23.4.3. Querying Provenance\nThe provenance captured by VisTrails includes a set of workflows, each with its own structure,\nmetadata, and execution logs. It is important that users can access and explore these data.\nVisTrails provides both text-based and visual (WYSIWYG) query interfaces. For information like\ntags, annotations, and dates, a user can use keyword search with optional markup. For example,\nlook for all workflows with the keyword plot that were created by user:~dakoop. However,\nqueries for specific subgraphs of a workflow are more easily represented through a visual, query-\nby-example interface, where users can either build the query from scratch or copy and modify an\nexisting piece of a pipeline.\nIn designing this query-by-example interface, we kept most of the code from the existing\nWorkflow Editor, with a few changes to parameter construction. For parameters, it is often useful\nto search for ranges or keywords rather than exact values. Thus, we added modifiers to the\nparameter value fields; when a user adds or edits a parameter value, they may choose to select\none of these modifiers which default to exact matches. In addition to visual query construction,\nquery results are shown visually. Matching versions are highlighted in the version tree, and any\nselected workflow is displayed with the matching portion highlighted. The user can exit query\nresults mode by initiating another query or clicking a reset button.\n23.4.4. Persistent Data\nVisTrails saves the provenance of how results were derived and the specification of each step.\n", "page": 320, "type": "text", "section": "Page 320"}
{"text": "However, reproducing a workflow run can be difficult if the data needed by the workflow is no\nlonger available. In addition, for long-running workflows, it may be useful to store intermediate\ndata as a persistent cache across sessions in order to avoid recomputation.\nMany workflow systems store filesystem paths to data as provenance, but this approach is\nproblematic. A user might rename a file, move the workflow to another system without copying\nthe data, or change the data contents. In any of these cases, storing the path as provenance is\nnot sufficient. Hashing the data and storing the hash as provenance helps to determine whether\nthe data might have changed, but does not help one locate the data if it exists. To solve this\nproblem, we created the Persistence Package, a VisTrails package that uses version control\ninfrastructure to store data that can be referenced from provenance. Currently we use Git to\nmanage the data, although other systems could easily be employed.\nWe use universally unique identifiers (UUIDs) to identify data, and commit hashes from git to\nreference versions. If the data changes from one execution to another, a new version is checked\nin to the repository. Thus, the (uuid, version) tuple is a compound identifier to retrieve the\ndata in any state. In addition, we store the hash of the data as well as the signature of the\nupstream portion of the workflow that generated it (if it is not an input). This allows one to link\ndata that might be identified differently as well as reuse data when the same computation is run\nagain.\nThe main concern when designing this package was the way users were able to select and retrieve\ntheir data. Also, we wished to keep all data in the same repository, regardless of whether it is used\nas input, output, or intermediate data (an output of one workflow might be used as the input of\nanother). There are two main modes a user might employ to identify data: choosing to create a\nnew reference or using an existing one. Note that after the first execution, a new reference will\nbecome an existing one as it has been persisted during execution; a user may later choose to\ncreate another reference if they wish but this is a rare case. Because a user often wishes to\nalways use the latest version of data, a reference identified without a specific version will default to\nthe latest version.\nRecall that before executing a module, we recursively update all of its inputs. A persistent data\nmodule will not update its inputs if the upstream computations have already been run. To\ndetermine this, we check the signature of the upstream subworkflow against the persistent\nrepository and retrieve the precomputed data if the signature exists. In addition, we record the\ndata identifiers and versions as provenance so that a specific execution can be reproduced.\n23.4.5. Upgrades\nWith provenance at the core of VisTrails, the ability to upgrade old workflows so they will run with\nnew versions of packages is a key concern. Because packages can be created by third-parties, we\nneed both the infrastructure for upgrading workflows as well as the hooks for package\ndevelopers to specify the upgrade paths. The core action involved in workflow upgrades is the\nreplacement of one module with a new version. Note that this action is complicated because we\nmust replace all of the connections and parameters from the old module. In addition, upgrades\nmay need to reconfigure, reassign, or rename these parameters or connections for a module,\ne.g., when the module interface changes.\nEach package (together with its associated modules) is tagged by a version, and if that version\nchanges, we assume that the modules in that package may have changed. Note that some, or\neven most, may not have changed, but without doing our own code analysis, we cannot check\nthis. We, however, attempt to automatically upgrade any module whose interface has not\nchanged. To do this, we try replacing the module with the new version and throw an exception if it\ndoes not work. When developers have changed the interface of a module or renamed a module,\nwe allow them to specify these changes explicitly. To make this more manageable, we have\ncreated a remap_module method that allows developers to define only the places where the\ndefault upgrade behavior needs to be modified. For example, a developer that renamed an input\nport `file' to `value' can specify that specific remapping so when the new module is created, any\nconnections to `file' in the old module will now connect to `value'. Here is an example of an\nupgrade path for a built-in VisTrails module:\ndef handle_module_upgrade_request(controller, module_id, pipeline):\n   module_remap = {'GetItemsFromDirectory':\n                       [(None, '1.6', 'Directory',\n                         {'dst_port_remap':\n                              {'dir': 'value'},\n                          'src_port_remap':\n                              {'itemlist': 'itemList'},\n                          })],\n                   }\n  return UpgradeWorkflowHandler.remap_module(controller, module_id, pipeline,\n", "page": 321, "type": "text", "section": "Page 321"}
{"text": "                                             module_remap)\nThis piece of code upgrades workflows that use the old GetItemsFromDirectory (any version\nup to 1.6) module to use the Directory module instead. It maps the dir port from the old\nmodule to value and the itemlist port to itemList.\nAny upgrade creates a new version in the version tree so that executions before and after\nupgrades can be differentiated and compared. It is possible that the upgrades change the\nexecution of the workflow (e.g., if a bug is fixed by a package developer), and we need to track\nthis as provenance information. Note that in older vistrails, it may be necessary to upgrade every\nversion in the tree. In order to reduce clutter, we only upgrade versions that a user has navigated\nto. In addition, we provide a preference that allows a user to delay the persistence of any upgrade\nuntil the workflow is modified or executed; if a user just views that version, there is no need to\npersist the upgrade.\n23.4.6. Sharing and Publishing Provenance-Rich Results\nWhile reproducibility is the cornerstone of the scientific method, current publications that describe\ncomputational experiments often fail to provide enough information to enable the results to be\nrepeated or generalized. Recently, there has been a renewed interest in the publication of\nreproducible results. A major roadblock to the more widespread adoption of this practice is the\nfact that it is hard to create a bundle that includes all of the components (e.g., data, code,\nparameter settings) needed to reproduce a result as well as verify that result.\nBy capturing detailed provenance, and through many of the features described above, VisTrails\nsimplifies this process for computational experiments that are carried out within the system.\nHowever, mechanisms are needed to both link documents to and share the provenance\ninformation.\nWe have developed VisTrails packages that enable results present in papers to be linked to their\nprovenance, like a deep caption. Using the LaTeX package we developed, users can include figures\nthat link to VisTrails workflows. The following LaTeX code will generate a figure that contains a\nworkflow result:\n\\begin{figure}[t]\n{\n\\vistrail[wfid=119,buildalways=false]{width=0.9\\linewidth}\n}\n\\caption{Visualizing a binary star system simulation. This is an image\n  that was generated by embedding a workflow directly in the text.}\n\\label{fig:astrophysics}\n\\end{figure}\nWhen the document is compiled using pdflatex, the \\vistrail command will invoke a Python\nscript with the parameters received, which sends an XML-RPC message to a VisTrails server to\nexecute the workflow with id 119. This same Python script downloads the results of the\nworkflow from the server and includes them in the resulting PDF document by generating\nhyperlinked LaTeX \\includegraphics commands using the specified layout options\n(width=0.9\\linewidth).\nIt is also possible to include VisTrails results into Web pages, wikis, Word documents and\nPowerPoint presentations. The linking between Microsoft PowerPoint and VisTrails was done\nthrough the Component Object Model (COM) and Object Linking and Embedding (OLE) interface.\nIn order for an object to interact with PowerPoint, at least the IOleObject, IDataObject and\nIPersistStorage interface of COM must be implemented. As we use the QAxAggregated class\nof Qt, which is an abstraction for implementing COM interfaces, to build our OLE object, both\nIDataObject and IPersistStorage are automatically handled by Qt. Thus, we only need to\nimplement the IOleObject interface. The most important call in this interface is DoVerb. It lets\nVisTrails react to certain actions from PowerPoint, such as object activation. In our\nimplementation, when the VisTrails object is activated, we load the VisTrails application and allow\nusers to open, interact with and select a pipeline that they want to insert. After they close\nVisTrails, the pipeline result will be shown in PowerPoint. Pipeline information is also stored with the\nOLE object.\nTo enable users to freely share their results together with the associated provenance, we have\ncreated crowdLabs.7 crowdLabs is a social Web site that integrates a set of usable tools and a\nscalable infrastructure to provide an environment for scientists to collaboratively analyze and\nvisualize data. crowdLabs is tightly integrated with VisTrails. If a user wants to share any results\nderived in VisTrails, she can connect to the crowdLabs server directly from VisTrails to upload the\ninformation. Once the information is uploaded, users can interact with and execute the workflows\nthrough a Web browser\u2014these workflows are executed by a VisTrails server that powers\n", "page": 322, "type": "text", "section": "Page 322"}
{"text": "crowdLabs. For more details on how VisTrails is used to created reproducible publications, see\nhttp://www.vistrails.org.\n23.5. Lessons Learned\nLuckily, back in 2004 when we started thinking about building a data exploration and visualization\nsystem that supported provenance, we never envisioned how challenging it would be, or how long\nit would take to get to the point we are at now. If we had, we probably would never have started.\nEarly on, one strategy that worked well was quickly prototyping new features and showing them\nto a select set of users. The initial feedback and the encouragement we received from these users\nwas instrumental in driving the project forward. It would have been impossible to design VisTrails\nwithout user feedback. If there is one aspect of the project that we would like to highlight is that\nmost features in the system were designed as direct response to user feedback. However, it is\nworthy to note that many times what a user asks for is not the best solution for his/her need\u2014\nbeing responsive to users does not necessarily mean doing exactly what they ask for. Time and\nagain, we have had to design and re-design features to make sure they would be useful and\nproperly integrated in the system.\nGiven our user-centric approach, one might expect that every feature we have developed would\nbe heavily used. Unfortunately this has not been the case. Sometimes the reason for this is that\nthe feature is highly \"unusual\", since it is not found in other tools. For instance, analogies and even\nthe version tree are not concepts that most users are familiar with, and it takes a while for them\nto get comfortable with them. Another important issue is documentation, or lack thereof. As with\nmany other open source projects, we have been much better at developing new features than at\ndocumenting the existing ones. This lag in documentation leads not only to the underutilization of\nuseful features, but also to many questions on our mailing lists.\nOne of the challenges of using a system like VisTrails is that it is very general. Despite our best\nefforts to improve usability, VisTrails is a complex tool and requires a steep learning curve for\nsome users. We believe that over time, with improved documentation, further refinements to the\nsystem, and more application- and domain-specific examples, the adoption bar for any given field\nwill get lower. Also, as the concept of provenance becomes more widespread, it will be easier for\nusers to understand the philosophy that we have adopted in developing VisTrails.\n23.5.1. Acknowledgments\nWe would like to thank all the talented developers that contributed to VisTrails: Erik Anderson,\nLouis Bavoil, Clifton Brooks, Jason Callahan, Steve Callahan, Lorena Carlo, Lauro Lins, Tommy\nEllkvist, Phillip Mates, Daniel Rees, and Nathan Smith. Special thanks to Antonio Baptista who was\ninstrumental in helping us develop the vision for the project; and Matthias Troyer, whose\ncollaboration has helped us to improve the system, and in particular has provided much of the\nimpetus for the development and release of the provenance-rich publication functionality. The\nresearch and development of the VisTrails system has been funded by the National Science\nFoundation under grants IIS 1050422, IIS-0905385, IIS 0844572, ATM-0835821, IIS-0844546, IIS-\n0746500, CNS-0751152, IIS-0713637, OCE-0424602, IIS-0534628, CNS-0514485, IIS-0513692,\nCNS-0524096, CCF-0401498, OISE-0405402, CCF-0528201, CNS-0551724, the Department of\nEnergy SciDAC (VACET and SDM centers), and IBM Faculty Awards.\nFootnotes\n1. http://www.vistrails.org\n2. http://www.hibernate.org\n3. http://www.sqlobject.org\n4. http://www.makotemplates.org\n5. http://www.vtk.org\n6. http://www.vistrails.org/usersguide\n7. http://www.crowdlabs.org\n", "page": 323, "type": "text", "section": "Page 323"}
{"text": "The Architecture of\nOpen Source Applications\nAmy Brown and Greg Wilson (eds.)\nISBN 978-1-257-63801-7 \nLicense / Buy / Contribute\nChapter 24. VTK\nBerk Geveci and Will Schroeder\nThe Visualization Toolkit (VTK) is a widely used software system for data processing and\nvisualization. It is used in scientific computing, medical image analysis, computational geometry,\nrendering, image processing and informatics. In this chapter we provide a brief overview of VTK,\nincluding some of the basic design patterns that make it a successful system.\nTo really understand a software system it is essential to not only understand what problem it\nsolves, but also the particular culture in which it emerged. In the case of VTK, the software was\nostensibly developed as a 3D visualization system for scientific data. But the cultural context in\nwhich it emerged adds a significant back story to the endeavor, and helps explains why the\nsoftware was designed and deployed as it was.\nAt the time VTK was conceived and written, its initial authors (Will Schroeder, Ken Martin, Bill\nLorensen) were researchers at GE Corporate R&D. We were heavily invested in a precursor\nsystem known as LYMB which was a Smalltalk-like environment implemented in the C programming\nlanguage. While this was a great system for its time, as researchers we were consistently\nfrustrated by two major barriers when trying to promote our work: 1) IP issues and 2) non-\nstandard, proprietary software. IP issues were a problem because trying to distribute the\nsoftware outside of GE was nearly impossible once the corporate lawyers became involved.\nSecond, even if we were deploying the software inside of GE, many of our customers balked at\nlearning a proprietary, non-standard system since the effort to master it did not transition with an\nemployee once she left the company, and it did not have the widespread support of a standard\ntool set. Thus in the end the primary motivation for VTK was to develop an open standard, or\ncollaboration platform through which we could easily transition technology to our customers. Thus\nchoosing an open source license for VTK was probably the most important design decision that\nwe made.\nThe final choice of a non-reciprocal, permissive license (i.e., BSD not GPL) in hindsight was an\nexemplary decision made by the authors because it ultimately enabled the service and consulting\nbased business that became Kitware. At the time we made the decision we were mostly interested\nin reduced barriers to collaborating with academics, research labs, and commercial entities. We\nhave since discovered that reciprocal licenses are avoided by many organizations because of the\npotential havoc they can wreak. In fact we would argue that reciprocal licenses do much to slow\nthe acceptance of open source software, but that is an argument for another time. The point here\nis: one of the major design decisions to make relative to any software system is the choice of\ncopyright license. It's important to review the goals of the project and then address IP issues\nappropriately.\n24.1. What Is VTK?\nVTK was initially conceived as a scientific data visualization system. Many people outside of the field\nnaively consider visualization a particular type of geometric rendering: examining virtual objects\nand interacting with them. While this is indeed part of visualization, in general data visualization\nincludes the whole process of transforming data into sensory input, typically images, but also\nincludes tactile, auditory, and other forms. The data forms not only consist of geometric and\ntopological constructs, including such abstractions as meshes or complex spatial decompositions,\nbut attributes to the core structure such as scalars (e.g., temperature or pressure), vectors (e.g.,\nvelocity), tensors (e.g., stress and strain) plus rendering attributes such as surface normals and\ntexture coordinate.\nNote that data representing spatial-temporal information is generally considered part of scientific\nvisualization. However there are more abstract data forms such as marketing demographics, web\npages, documents and other information that can only be represented through abstract (i.e., non-\nspatial temporal) relationships such as unstructured documents, tables, graphs, and trees. These\n", "page": 324, "type": "text", "section": "Page 324"}
{"text": "abstract data are typically addressed by methods from information visualization. With the help of\nthe community, VTK is now capable of both scientific and information visualization.\nAs a visualization system, the role of VTK is to take data in these forms and ultimately transform\nthem into forms comprehensible by the human sensory apparatus. Thus one of the core\nrequirements of VTK is its ability to create data flow pipelines that are capable of ingesting,\nprocessing, representing and ultimately rendering data. Hence the toolkit is necessarily architected\nas a flexible system and its design reflects this on many levels. For example, we purposely\ndesigned VTK as a toolkit with many interchangeable components that can be combined to\nprocess a wide variety of data.\n24.2. Architectural Features\nBefore getting too far into the specific architectural features of VTK, there are high-level concepts\nthat have significant impact on developing and using the system. One of these is VTK's hybrid\nwrapper facility. This facility automatically generates language bindings to Python, Java, and Tcl\nfrom VTK's C++ implementation (additional languages could be and have been added). Most high-\npowered developers will work in C++. User and application developers may use C++ but often the\ninterpreted languages mentioned above are preferred. This hybrid compiled/interpreted\nenvironment combines the best of both worlds: high performance compute-intensive algorithms\nand flexibility when prototyping or developing applications. In fact this approach to multi-language\ncomputing has found favor with many in the scientific computing community and they often use\nVTK as a template for developing their own software.\nIn terms of software process, VTK has adopted CMake to control the build; CDash/CTest for\ntesting; and CPack for cross-platform deployment. Indeed VTK can be compiled on almost any\ncomputer including supercomputers which are often notoriously primitive development\nenvironments. In addition, web pages, wiki, mailing lists (user and developer), documentation\ngeneration facilities (i.e., Doxygen) and a bug tracker (Mantis) round out the development tools.\n24.2.1. Core Features\nAs VTK is an object-oriented system, the access of class and instance data members is carefully\ncontrolled in VTK. In general, all data members are either protected or private. Access to them is\nthrough Set and Get methods, with special variations for Boolean data, modal data, strings and\nvectors. Many of these methods are actually created by inserting macros into the class header\nfiles. So for example:\nvtkSetMacro(Tolerance,double);\nvtkGetMacro(Tolerance,double);\nbecome on expansion:\nvirtual void SetTolerance(double);\nvirtual double GetTolerance();\nThere are many reasons for using these macros beyond simply code clarity. In VTK there are\nimportant data members controlling debugging, updating an object's modified time (MTime), and\nproperly managing reference counting. These macros correctly manipulate these data and their\nuse is highly recommended. For example, a particularly pernicious bug in VTK occurs when the\nobject's MTime is not managed properly. In this case code may not execute when it should, or\nmay execute too often.\nOne of the strengths of VTK is its relatively simplistic means of representing and managing data.\nTypically various data arrays of particular types (e.g., vtkFloatArray) are used to represent\ncontiguous pieces of information. For example, a list of three XYZ points would be represented\nwith a vtkFloatArray of nine entries (x,y,z, x,y,z, etc.) There is the notion of a tuple in these\narrays, so a 3D point is a 3-tuple, whereas a symmetric 3\u00d73 tensor matrix is represented by a 6-\ntuple (where symmetry space savings are possible). This design was adopted purposely because\nin scientific computing it is common to interface with systems manipulating arrays (e.g., Fortran)\nand it is much more efficient to allocate and deallocate memory in large contiguous chunks.\nFurther, communication, serializing and performing IO is generally much more efficient with\ncontiguous data. These core data arrays (of various types) represent much of the data in VTK and\nhave a variety of convenience methods for inserting and accessing information, including methods\nfor fast access, and methods that automatically allocate memory as needed when adding more\ndata. Data arrays are subclasses of the vtkDataArray abstract class meaning that generic virtual\n", "page": 325, "type": "text", "section": "Page 325"}
{"text": "methods can be used to simplify coding. However, for higher performance static, templated\nfunctions are used which switch based on type, with subsequent, direct access into the\ncontiguous data arrays.\nIn general C++ templates are not visible in the public class API; although templates are used\nwidely for performance reasons. This goes for STL as well: we typically employ the PIMPL1 design\npattern to hide the complexities of a template implementation from the user or application\ndeveloper. This has served us particularly well when it comes to wrapping the code into\ninterpreted code as described previously. Avoiding the complexity of the templates in the public\nAPI means that the VTK implementation, from the application developer point of view, is mostly\nfree of the complexities of data type selection. Of course under the hood the code execution is\ndriven by the data type which is typically determined at run time when the data is accessed.\nSome users wonder why VTK uses reference counting for memory management versus a more\nuser-friendly approach such as garbage collection. The basic answer is that VTK needs complete\ncontrol over when data is deleted, because the data sizes can be huge. For example, a volume of\nbyte data 1000\u00d71000\u00d71000 in size is a gigabyte in size. It is not a good idea to leave such data\nlying around while the garbage collector decides whether or not it is time to release it. In VTK most\nclasses (subclasses of vtkObject) have the built-in capability for reference counting. Every object\ncontains a reference count that it initialized to one when the object is instantiated. Every time a use\nof the object is registered, the reference count is increased by one. Similarly, when a use of the\nobject is unregistered (or equivalently the object is deleted) the reference count is reduced by one.\nEventually the object's reference count is reduced to zero, at which point it self destructs. A\ntypical example looks like the following:\nvtkCamera *camera = vtkCamera::New();   //reference count is 1\ncamera->Register(this);                 //reference count is 2\ncamera->Unregister(this);               //reference count is 1\nrenderer->SetActiveCamera(camera);      //reference count is 2\nrenderer->Delete();                     //ref count is 1 when renderer is deleted\ncamera->Delete();                       //camera self destructs\nThere is another important reason why reference counting is important to VTK\u2014it provides the\nability to efficiently copy data. For example, imagine a data object D1 that consists of a number of\ndata arrays: points, polygons, colors, scalars and texture coordinates. Now imagine processing\nthis data to generate a new data object D2 which is the same as the first plus the addition of\nvector data (located on the points). One wasteful approach is to completely (deep) copy D1 to\ncreate D2, and then add the new vector data array to D2. Alternatively, we create an empty D2\nand then pass the arrays from D1 to D2 (shallow copy), using reference counting to keep track of\ndata ownership, finally adding the new vector array to D2. The latter approach avoids copying data\nwhich, as we have argued previously, is essential to a good visualization system. As we will see\nlater in this chapter, the data processing pipeline performs this type of operation routinely, i.e.,\ncopying data from the input of an algorithm to the output, hence reference counting is essential to\nVTK.\nOf course there are some notorious problems with reference counting. Occasionally reference\ncycles can exist, with objects in the cycle referring to each other in a mutually supportive\nconfiguration. In this case, intelligent intervention is required, or in the case of VTK, the special\nfacility implemented in vtkGarbageCollector is used to manage objects which are involved in\ncycles. When such a class is identified (this is anticipated during development), the class registers\nitself with the garbage collector and overloads its own Register and UnRegister methods. Then\na subsequent object deletion (or unregister) method performs a topological analysis on the local\nreference counting network, searching for detached islands of mutually referencing objects. These\nare then deleted by the garbage collector.\nMost instantiation in VTK is performed through an object factory implemented as a static class\nmember. The typical syntax appears as follows:\nvtkLight *a = vtkLight::New();\nWhat is important to recognize here is what is actually instantiated may not be a vtkLight, it\ncould be a subclass of vtkLight (e.g., vtkOpenGLLight). There are a variety of motivations for\nthe object factory, the most important being application portability and device independence. For\nexample, in the above we are creating a light in a rendered scene. In a particular application on a\nparticular platform, vtkLight::New may result in an OpenGL light, however on different\nplatforms there is potential for other rendering libraries or methods for creating a light in the\n", "page": 326, "type": "text", "section": "Page 326"}
{"text": "graphics system. Exactly what derived class to instantiate is a function of run-time system\ninformation. In the early days of VTK there were a myriad of options including gl, PHIGS, Starbase,\nXGL, and OpenGL. While most of these have now vanished, new approaches have appeared\nincluding DirectX and GPU-based approaches. Over time, an application written with VTK has not\nhad to change as developers have derived new device specific subclasses to vtkLight and other\nrendering classes to support evolving technology. Another important use of the object factory is\nto enable the run-time replacement of performance-enhanced variations. For example, a\nvtkImageFFT may be replaced with a class that accesses special-purpose hardware or a\nnumerics library.\n24.2.2. Representing Data\nOne of the strengths of VTK is its ability to represent complex forms of data. These data forms\nrange from simple tables to complex structures such as finite element meshes. All of these data\nforms are subclasses of vtkDataObject as shown in Figure 24.1 (note this is a partial inheritance\ndiagram of the many data object classes).\nFigure 24.1: Data Object Classes\nOne of the most important characteristics of vtkDataObject is that it can be processed in a\nvisualization pipeline (next subsection). Of the many classes shown, there are just a handful that\nare typically used in most real world applications. vtkDataSet and derived classes are used for\nscientific visualization (Figure 24.2). For example, vtkPolyData is used to represent polygonal\nmeshes; vtkUnstructuredGrid to represent meshes, and vtkImageData represents 2D and\n3D pixel and voxel data.\nFigure 24.2: Data Set Classes\n24.2.3. Pipeline Architecture\nVTK consists of several major subsystems. Probably the subsystem most associated with\nvisualization packages is the data flow/pipeline architecture. In concept, the pipeline architecture\nconsists of three basic classes of objects: objects to represent data (the vtkDataObjects\n", "page": 327, "type": "text", "section": "Page 327"}
{"text": "discussed above), objects to process, transform, filter or map data objects from one form into\nanother (vtkAlgorithm); and objects to execute a pipeline (vtkExecutive) which controls a\nconnected graph of interleaved data and process objects (i.e., the pipeline). Figure 24.3 depicts a\ntypical pipeline.\nFigure 24.3: Typical Pipeline\nWhile conceptually simple, actually implementing the pipeline architecture is challenging. One\nreason is that the representation of data can be complex. For example, some datasets consist of\nhierarchies or grouping of data, so executing across the data requires non-trivial iteration or\nrecursion. To compound matters, parallel processing (whether using shared-memory or scalable,\ndistributed approaches) require partitioning data into pieces, where pieces may be required to\noverlap in order to consistently computer boundary information such as derivatives.\nThe algorithm objects also introduce their own special complexity. Some algorithms may take\nmultiple inputs and/or produce multiple outputs of different types. Some can operate locally on\ndata (i.e., compute the center of a cell) while others require global information, for example to\ncompute a histogram. In all cases, the algorithms treat their inputs as immutable, algorithms only\nread their input in order to produce their output. This is because data may be available as input to\nmultiple algorithms, and it is not a good idea for one algorithm to trample on the input of another.\nFinally the executive can be complicated depending on the particulars of the execution strategy. In\nsome cases we may wish to cache intermediate results between filters. This minimizes the amount\nof recomputation that must be performed if something in the pipeline changes. On the other\nhand, visualization data sets can be huge, in which case we may wish to release data when it is no\nlonger needed for computation. Finally, there are complex execution strategies, such as multi-\nresolution processing of data, which require the pipeline to operate in iterative fashion.\nTo demonstrate some of these concepts and further explain the pipeline architecture, consider\nthe following C++ example:\nvtkPExodusIIReader *reader = vtkPExodusIIReader::New();\nreader->SetFileName(\"exampleFile.exo\");\nvtkContourFilter *cont = vtkContourFilter::New();\ncont->SetInputConnection(reader->GetOutputPort());\ncont->SetNumberOfContours(1);\ncont->SetValue(0, 200);\nvtkQuadricDecimation *deci = vtkQuadricDecimation::New();\ndeci->SetInputConnection(cont->GetOutputPort());\ndeci->SetTargetReduction( 0.75 );\nvtkXMLPolyDataWriter *writer = vtkXMLPolyDataWriter::New();\nwriter->SetInputConnection(deci->GetOuputPort());\nwriter->SetFileName(\"outputFile.vtp\");\nwriter->Write();\nIn this example, a reader object reads a large unstructured grid (or mesh) data file. The next filter\ngenerates an isosurface from the mesh. The vtkQuadricDecimation filter reduces the size of\nthe isosurface, which is a polygonal dataset, by decimating it (i.e., reducing the number of\ntriangles representing the isocontour). Finally after decimation the new, reduced data file is written\nback to disk. The actual pipeline execution occurs when the Write method is invoked by the writer\n(i.e., upon demand for the data).\nAs this example demonstrates, VTK's pipeline execution mechanism is demand driven. When a\nsink such as a writer or a mapper (a data rendering object) needs data, it asks its input. If the\ninput filter already has the appropriate data, it simply returns the execution control to the sink.\n", "page": 328, "type": "text", "section": "Page 328"}
{"text": "However, if the input does not have the appropriate data, it needs to compute it. Consequently, it\nmust first ask its input for data. This process will continue upstream along the pipeline until a filter\nor source that has \"appropriate data\" or the beginning of the pipeline is reached, at which point\nthe filters will execute in correct order and the data will flow to the point in the pipeline at which it\nwas requested.\nHere we should expand on what \"appropriate data\" means. By default, after a VTK source or filter\nexecutes, its output is cached by the pipeline in order to avoid unnecessary executions in the\nfuture. This is done to minimize computation and/or I/O at the cost of memory, and is configurable\nbehavior. The pipeline caches not only the data objects but also the metadata about the conditions\nunder which these data objects were generated. This metadata includes a time stamp (i.e.,\nComputeTime) that captures when the data object was computed. So in the simplest case, the\n\"appropriate data\" is one that was computed after all of the pipeline objects upstream from it were\nmodified. It is easier to demonstrate this behavior by considering the following examples. Let's add\nthe following to the end of the previous VTK program:\nvtkXMLPolyDataWriter *writer2 = vtkXMLPolyDataWriter::New();\nwriter2->SetInputConnection(deci->GetOuputPort());\nwriter2->SetFileName(\"outputFile2.vtp\");\nwriter2->Write();\nAs explained previously, the first writer->Write call causes the execution of the entire pipeline.\nWhen writer2->Write() is called, the pipeline will realize that the cached output of the\ndecimation filter is up to date when it compares the time stamp of the cache with the modification\ntime of the decimation filter, the contour filter and the reader. Therefore, the data request does\nnot have to propagate past writer2. Now, let's consider the following change.\ncont->SetValue(0, 400);\nvtkXMLPolyDataWriter *writer2 = vtkXMLPolyDataWriter::New();\nwriter2->SetInputConnection(deci->GetOuputPort());\nwriter2->SetFileName(\"outputFile2.vtp\");\nwriter2->Write();\nNow the pipeline executive will realize that the contour filter was modified after the outputs of the\ncontour and decimation filters were last executed. Thus, the cache for these two filters are stale\nand they have to be re-executed. However, since the reader was not modified prior to the contour\nfilter its cache is valid and hence the reader does not have to re-execute.\nThe scenario described here is the simplest example of a demand-driven pipeline. VTK's pipeline is\nmuch more sophisticated. When a filter or a sink requires data, it can provide additional\ninformation to request specific data subsets. For example, a filter can perform out-of-core\nanalysis by streaming pieces of data. Let's change our previous example to demonstrate.\nvtkXMLPolyDataWriter *writer = vtkXMLPolyDataWriter::New();\nwriter->SetInputConnection(deci->GetOuputPort());\nwriter->SetNumberOfPieces(2);\nwriter->SetWritePiece(0);\nwriter->SetFileName(\"outputFile0.vtp\");\nwriter->Write();\nwriter->SetWritePiece(1);\nwriter->SetFileName(\"outputFile1.vtp\");\nwriter->Write();\nHere the writer asks the upstream pipeline to load and process data in two pieces each of which\nare streamed independently. You may have noticed that the simple execution logic described\npreviously will not work here. By this logic when the Write function is called for the second time,\nthe pipeline should not re-execute because nothing upstream changed. Thus to address this more\ncomplex case, the executives have additional logic to handle piece requests such as this. VTK's\npipeline execution actually consists of multiple passes. The computation of the data objects is\nactually the last pass. The pass before then is a request pass. This is where sinks and filters can\ntell upstream what they want from the forthcoming computation. In the example above, the writer\nwill notify its input that it wants piece 0 of 2. This request will actually propagate all the way to the\nreader. When the pipeline executes, the reader will then know that it needs to read a subset of the\ndata. Furthermore, information about which piece the cached data corresponds to is stored in the\n", "page": 329, "type": "text", "section": "Page 329"}
{"text": "metadata for the object. The next time a filter asks for data from its input, this metadata will be\ncompared with the current request. Thus in this example the pipeline will re-execute in order to\nprocess a different piece request.\nThere are several more types of request that a filter can make. These include requests for a\nparticular time step, a particular structured extent or the number of ghost layers (i.e., boundary\nlayers for computing neighborhood information). Furthermore, during the request pass, each filter\nis allowed to modify requests from downstream. For example, a filter that is not able to stream\n(e.g., the streamline filter) can ignore the piece request and ask for the whole data.\n24.2.4. Rendering Subsystem\nAt first glance VTK has a simple object-oriented rendering model with classes corresponding to the\ncomponents that make up a 3D scene. For example, vtkActors are objects that are rendered by\na vtkRenderer in conjunction with a vtkCamera, with possibly multiple vtkRenderers existing in\na vtkRenderWindow. The scene is illuminated by one or more vtkLights. The position of each\nvtkActor is controlled by a vtkTransform, and the appearance of an actor is specified through\na vtkProperty. Finally, the geometric representation of an actor is defined by a vtkMapper.\nMappers play an important role in VTK, they serve to terminate the data processing pipeline, as\nwell as interface to the rendering system. Consider this example where we decimate data and\nwrite the result to a file, and then visualize and interact with the result by using a mapper:\nvtkOBJReader *reader = vtkOBJReader::New();\nreader->SetFileName(\"exampleFile.obj\");\nvtkTriangleFilter *tri = vtkTriangleFilter::New();\ntri->SetInputConnection(reader->GetOutputPort());\nvtkQuadricDecimation *deci = vtkQuadricDecimation::New();\ndeci->SetInputConnection(tri->GetOutputPort());\ndeci->SetTargetReduction( 0.75 );\nvtkPolyDataMapper *mapper = vtkPolyDataMapper::New();\nmapper->SetInputConnection(deci->GetOutputPort());\nvtkActor *actor = vtkActor::New();\nactor->SetMapper(mapper);\nvtkRenderer *renderer = vtkRenderer::New();\nrenderer->AddActor(actor);\nvtkRenderWindow *renWin = vtkRenderWindow::New();\nrenWin->AddRenderer(renderer);\nvtkRenderWindowInteractor *interactor = vtkRenderWindowInteractor::New();\ninteractor->SetRenderWindow(renWin);\nrenWin->Render();\nHere a single actor, renderer and render window are created with the addition of a mapper that\nconnects the pipeline to the rendering system. Also note the addition of a\nvtkRenderWindowInteractor, instances of which capture mouse and keyboard events and\ntranslate them into camera manipulations or other actions. This translation process is defined via a\nvtkInteractorStyle (more on this below). By default many instances and data values are set\nbehind the scenes. For example, an identity transform is constructed, as well as a single default\n(head) light and property.\nOver time this object model has become more sophisticated. Much of the complexity has come\nfrom developing derived classes that specialize on an aspect of the rendering process. vtkActors\nare now specializations of vtkProp (like a prop found on stage), and there are a whole slew of\nthese props for rendering 2D overlay graphics and text, specialized 3D objects, and even for\nsupporting advanced rendering techniques such as volume rendering or GPU implementations\n(see Figure 24.4).\nSimilarly, as the data model supported by VTK has grown, so have the various mappers that\ninterface the data to the rendering system. Another area of significant extension is the\n", "page": 330, "type": "text", "section": "Page 330"}
{"text": "transformation hierarchy. What was originally a simple linear 4\u00d74 transformation matrix, has\nbecome a powerful hierarchy that supports non-linear transformations including thin-plate spline\ntransformation. For example, the original vtkPolyDataMapper had device-specific subclasses\n(e.g., vtkOpenGLPolyDataMapper). In recent years it has been replaced with a sophisticated\ngraphics pipeline referred to as the \"painter\" pipeline illustrated in Figure 24.4.\nFigure 24.4: Display Classes\nThe painter design supports a variety of techniques for rendering data that can be combined to\nprovide special rendering effects. This capability greatly surpasses the simple\nvtkPolyDataMapper that was initially implemented in 1994.\nAnother important aspect of a visualization system is the selection subsystem. In VTK there is a\nhierarchy of \"pickers\", roughly categorized into objects that select vtkProps based on hardware-\nbased methods versus software methods (e.g., ray-casting); as well as objects that provide\ndifferent levels of information after a pick operations. For example, some pickers provide only a\nlocation in XYZ world space without indicating which vtkProp they have selected; others provide\nnot only the selected vtkProp but a particular point or cell that make up the mesh defining the\nprop geometry.\n24.2.5. Events and Interaction\nInteracting with data is an essential part of visualization. In VTK this occurs in a variety of ways. At\nits simplest level, users can observe events and respond appropriately through commands (the\ncommand/observer design pattern). All subclasses of vtkObject maintain a list of observers\nwhich register themselves with the object. During registration, the observers indicate which\nparticular event(s) they are interested in, with the addition of an associated command that is\ninvoked if and when the event occurs. To see how this works, consider the following example in\nwhich a filter (here a polygon decimation filter) has an observer which watches for the three\nevents StartEvent, ProgressEvent, and EndEvent. These events are invoked when the filter\nbegins to execute, periodically during execution, and then on completion of execution. In the\nfollowing the vtkCommand class has an Execute method that prints out the appropriate\ninformation relative to the time it take to execute the algorithm:\nclass vtkProgressCommand : public vtkCommand\n{\n  public:\n    static vtkProgressCommand *New() { return new vtkProgressCommand; }\n    virtual void Execute(vtkObject *caller, unsigned long, void *callData)\n    {\n      double progress = *(static_cast<double*>(callData));\n      std::cout << \"Progress at \" << progress<< std::endl;\n    }\n};\nvtkCommand* pobserver = vtkProgressCommand::New();\n", "page": 331, "type": "text", "section": "Page 331"}
{"text": "vtkDecimatePro *deci = vtkDecimatePro::New();\ndeci->SetInputConnection( byu->GetOutputPort() );\ndeci->SetTargetReduction( 0.75 );\ndeci->AddObserver( vtkCommand::ProgressEvent, pobserver );\nWhile this is a primitive form of interaction, it is a foundational element to many applications that\nuse VTK. For example, the simple code above can be easily converted to display and manage a\nGUI progress bar. This Command/Observer subsystem is also central to the 3D widgets in VTK,\nwhich are sophisticated interaction objects for querying, manipulating and editing data and are\ndescribed below.\nReferring to the example above, it is important to note that events in VTK are predefined, but\nthere is a back door for user-defined events. The class vtkCommand defines the set of\nenumerated events (e.g., vtkCommand::ProgressEvent in the above example) as well as a user\nevent. The UserEvent, which is simply an integral value, is typically used as a starting offset value\ninto a set of application user-defined events. So for example vtkCommand::UserEvent+100 may\nrefer to a specific event outside the set of VTK defined events.\nFrom the user's perspective, a VTK widget appears as an actor in a scene except that the user\ncan interact with it by manipulating handles or other geometric features (the handle manipulation\nand geometric feature manipulation is based on the picking functionality described earlier.) The\ninteraction with this widget is fairly intuitive: a user grabs the spherical handles and moves them,\nor grabs the line and moves it. Behind the scenes, however, events are emitted (e.g.,\nInteractionEvent) and a properly programmed application can observe these events, and then\ntake the appropriate action. For example they often trigger on the\nvtkCommand::InteractionEvent as follows:\nvtkLW2Callback *myCallback = vtkLW2Callback::New();\n  myCallback->PolyData = seeds;    // streamlines seed points, updated on interaction\n  myCallback->Actor = streamline;  // streamline actor, made visible on interaction\nvtkLineWidget2 *lineWidget = vtkLineWidget2::New();\n  lineWidget->SetInteractor(iren);\n  lineWidget->SetRepresentation(rep);\n  lineWidget->AddObserver(vtkCommand::InteractionEvent,myCallback);\nVTK widgets are actually constructed using two objects: a subclass of vtkInteractorObserver\nand a subclass of vtkProp. The vtkInteractorObserver simply observes user interaction in\nthe render window (i.e., mouse and keyboard events) and processes them. The subclasses of\nvtkProp (i.e., actors) are simply manipulated by the vtkInteractorObserver. Typically such\nmanipulation consists of modifying the vtkProp's geometry including highlighting handles,\nchanging cursor appearance, and/or transforming data. Of course, the particulars of the widgets\nrequire that subclasses are written to control the nuances of widget behavior, and there are more\nthan 50 different widgets currently in the system.\n24.2.6. Summary of Libraries\nVTK is a large software toolkit. Currently the system consists of approximately 1.5 million lines of\ncode (including comments but not including automatically generated wrapper software), and\napproximately 1000 C++ classes. To manage the complexity of the system and reduce build and\nlink times the system has been partitioned into dozens of subdirectories. Table 24.1 lists these\nsubdirectories, with a brief summary describing what capabilities the library provides.\nCommon\ncore VTK classes\nFiltering\nclasses used to manage pipeline dataflow\nRendering\nrendering, picking, image viewing, and interaction\nVolumeRendering\nvolume rendering techniques\nGraphics\n3D geometry processing\nGenericFiltering none-linear 3D geometry processing\nImaging\nimaging pipeline\nHybrid\nclasses requiring both graphics and imaging functionality\nWidgets\nsophisticated interaction\nIO\nVTK input and output\nInfovis\ninformation visualization\n", "page": 332, "type": "text", "section": "Page 332"}
{"text": "Parallel\nparallel processing (controllers and communicators)\nWrapping\nsupport for Tcl, Python, and Java wrapping\nExamples\nextensive, well-documented examples\nTable 24.1: VTK Subdirectories\n24.3. Looking Back/Looking Forward\nVTK has been an enormously successful system. While the first line of code was written in 1993,\nat the time of this writing VTK is still growing strong and if anything the pace of development is\nincreasing.2 In this section we talk about some lessons learned and future challenges.\n24.3.1. Managing Growth\nOne of the most surprising aspects to the VTK adventure has been the project's longevity. The\npace of development is due to several major reasons:\nNew algorithms and capabilities continue to be added. For example, the informatics\nsubsystem (Titan, primarily developed by Sandia National Labs and Kitware) is a recent\nsignificant addition. Additional charting and rendering classes are also being added, as well as\ncapabilities for new scientific dataset types. Another important addition were the 3D\ninteraction widgets. Finally, the on-going evolution of GPU-based rendering and data\nprocessing is driving new capabilities in VTK.\nThe growing exposure and use of VTK is a self-perpetuating process that adds even more\nusers and developers to the community. For example, ParaView is the most popular scientific\nvisualization application built on VTK and is highly regarded in the high-performance\ncomputing community. 3D Slicer is a major biomedical computing platform that is largely built\non VTK and received millions of dollars per year in funding.\nVTK's development process continues to evolve. In recent years the software process tools\nCMake, CDash, CTest, and CPack have been integrated into the VTK build environment. More\nrecently, the VTK code repository has moved to Git and a more sophisticated work flow.\nThese improvements ensure that VTK remains on the leading edge of software development\nin the scientific computing community.\nWhile growth is exciting, validates the creation of the software system, and bodes well for the\nfuture of VTK, it can be extremely difficult to manage well. As a result, the near term future of VTK\nfocuses more on managing the growth of the community as well as the software. Several steps\nhave been taken in this regard.\nFirst, formalized management structures are being created. An Architecture Review Board has\nbeen created to guide the development of the community and technology, focusing on high-level,\nstrategic issues. The VTK community is also establishing a recognized team of Topic Leads to\nguide the technical development of particular VTK subsystems.\nNext, there are plans to modularize the toolkit further, partially in response to workflow capabilities\nintroduced by git, but also to recognize that users and developers typically want to work with\nsmall subsystems of the toolkit, and do not want to build and link against the entire package.\nFurther, to support the growing community, it's important that contributions of new functionality\nand subsystems are supported, even if they are not necessarily part of the core of the toolkit. By\ncreating a loose, modularized collection of modules it is possible to accommodate the large\nnumber of contributions on the periphery while maintaining core stability.\n24.3.2. Technology Additions\nBesides the software process, there are many technological innovations in the development\npipeline.\nCo-processing is a capability where the visualization engine is integrated into the simulation\ncode, and periodically generates data extracts for visualization. This technology greatly\nreduces the need to output large amounts of complete solution data.\nThe data processing pipeline in VTK is still too complex. Methods are under way to simplify\nand refactor this subsystem.\nThe ability to directly interact with data is increasingly popular with users. While VTK has a\nlarge suite of widgets, many more interaction techniques are emerging including touch-\n", "page": 333, "type": "text", "section": "Page 333"}
{"text": "screen-based and 3D methods. Interaction will continue its development at a rapid pace.\nComputational chemistry is increasing in importance to materials designers and engineers.\nThe ability to visualize and interact with chemistry data is being added to VTK.\nThe rendering system in VTK has been criticized for being too complex, making it difficult to\nderive new classes or support new rendering technology. In addition, VTK does not directly\nsupport the notion of a scene graph, again something that many users have requested.\nFinally new forms of data are constantly emerging. For example, in the medical field\nhierarchical volumetric datasets of varying resolution (e.g., confocal microscopy with local\nmagnification).\n24.3.3. Open Science\nFinally Kitware and more generally the VTK community are committed to Open Science.\nPragmatically this is a way of saying we will promulgate open data, open publication, and open\nsource\u2014the features necessary to ensure that we are creating reproducible scientific systems.\nWhile VTK has long been distributed as an open source and open data system, the documentation\nprocess has been lacking. While there are decent books [Kit10,SML06] there have been a variety\nof ad hoc ways to collect technical publications including new source code contributions. We are\nimproving the situation by developing new publishing mechanisms like the VTK Journal3 that enable\nof articles consisting of documentation, source code, data, and valid test images. The journal also\nenables automated reviews of the code (using VTK's quality software testing process) as well as\nhuman reviews of the submission.\n24.3.4. Lessons Learned\nWhile VTK has been successful there are many things we didn't do right:\nDesign Modularity: We did a good job choosing the modularity of our classes. For example,\nwe didn't do something as silly as creating an object per pixel, rather we created the higher-\nlevel vtkImageClass that under the hood treats data arrays of pixel data. However in some\ncases we made our classes too high level and too complex, in many instances we've had to\nrefactor them into smaller pieces, and are continuing this process. One prime example is the\ndata processing pipeline. Initially, the pipeline was implemented implicitly through interaction\nof the data and algorithm objects. We eventually realized that we had to create an explicit\npipeline executive object to coordinate the interaction between data and algorithms, and to\nimplement different data processing strategies.\nMissed Key Concepts: Once of our biggest regrets is not making widespread use of C++\niterators. In many cases the traversal of data in VTK is akin to the scientific programming\nlanguage Fortran. The additional flexibility of iterators would have been a significant benefit to\nthe system. For example, it is very advantageous to process a local region of data, or only\ndata satisfying some iteration criterion.\nDesign Issues: Of course there is a long list of design decisions that are not optimal. We have\nstruggled with the data execution pipeline, having gone through multiple generations each\ntime making the design better. The rendering system too is complex and hard to derive from.\nAnother challenge resulted from the initial conception of VTK: we saw it as a read-only\nvisualization system for viewing data. However, current customers often want it to be\ncapable of editing data, which requires significantly different data structures.\nOne of the great things about an open source system like VTK is that many of these mistakes can\nand will be rectified over time. We have an active, capable development community that is\nimproving the system every day and we expect this to continue into the foreseeable future.\nFootnotes\n1. http://en.wikipedia.org/wiki/Opaque_pointer.\n2. See the latest VTK code analysis at http://www.ohloh.net/p/vtk/analyses/latest.\n3. http://www.midasjournal.org/?journal=35\n", "page": 334, "type": "text", "section": "Page 334"}
{"text": "The Architecture of\nOpen Source Applications\nAmy Brown and Greg Wilson (eds.)\nISBN 978-1-257-63801-7 \nLicense / Buy / Contribute\nChapter 25. Battle For Wesnoth\nRichard Shimooka and David White\nProgramming tends to be considered a straightforward problem solving activity; a developer has a\nrequirement and codes a solution. Beauty is often judged on the technical implementation's\nelegance or effectiveness; this book is replete with excellent examples. Yet beyond its immediate\ncomputing functions, code can have a profound effect on people's lives. It can inspire people to\nparticipate and create new content. Unfortunately, serious barriers exist that prevent individuals\nfrom participating in a project.\nMost programming languages require significant technical expertise to utilize, which is out of reach\nfor many. In addition, enhancing the accessibility of code is technically difficult and is not\nnecessary for many programs. It rarely translates into neat coding scripts or clever programming\nsolutions. Achieving accessibility requires considerable forethought in project and program design,\nwhich often runs counter-intuitive to normal programming standards. Moreover most projects rely\nupon an established staff of skilled professionals that are expected to operate at a reasonably\nhigh level. They do not require additional programming resources. Thus, code accessibility\nbecomes an afterthought, if considered at all.\nOur project, the Battle for Wesnoth, attempted to address this issue from its origins. The program\nis a turn-based fantasy strategy game, produced in an open source model based on a GPL2\nlicense. It has been a moderate success, with over four million downloads at the time of this\nwriting. While this is an impressive metric, we believe the real beauty of our project is the\ndevelopment model that allowed a band of volunteers from widely different skill levels to interact in\na productive way.\nEnhancing accessibility was not a vague objective set by developers, it was viewed as essential for\nthe project's survival. Wesnoth's open source approach meant that the project could not\nimmediately expect large numbers of highly skilled developers. Making the project accessible to a\nwide a number of contributors, with varying skill levels, would ensure its long-term viability.\nOur developers attempted to lay the foundations for broadening accessibility right from its earliest\niteration. This would have undeniable consequences for all aspect of the programming\narchitecture. Major decisions were made largely with this objective in mind. This chapter will\nprovide an in-depth examination of our program with a focus on the efforts to increase\naccessibility.\nThe first part of this chapter offers a general overview of the project's programming, covering its\nlanguage, dependencies and architecture. The second part will focus on Wesnoth's unique data\nstorage language, known as Wesnoth Markup Language (WML). It will explain the specific\nfunctions of WML, with a particular emphasis on its effects on in-game units. The next section\ncovers multiplayer implementation and external programs. The chapter will end with some\nconcluding observations on our structure and the challenges of broadening participation.\n25.1. Project Overview\nWesnoth's core engine is written in C++, totalling around 200,000 lines at the time of this\npublication. This represents the core game engine, approximately half of the code base without\nany content. The program also allows in game content to be defined in a unique data language\nknown as Wesnoth Markup Language (WML). The game ships with another 250,000 lines of WML\n", "page": 335, "type": "text", "section": "Page 335"}
{"text": "code. The proportion has shifted over the project's existence. As the program matured, game\ncontent that was hardcoded in C++ has increasingly been rewritten so that WML can be used to\ndefine its operation. Figure 25.1 gives a rough picture of the program's architecture; green areas\nare maintained by Wesnoth developers, while white areas are external dependencies.\nFigure 25.1: Program Architecture\nOverall, the project attempts to minimize dependencies in most cases so as to maximize the\nportability of the application. This has the added benefit of reducing the program's complexity, and\ndecreases the need for developers to learn the nuances of a large number of third party APIs. At\nthe same time, the prudent use of some dependencies can actually achieve the same effect. For\nexample, Wesnoth uses the Simple Directmedia Layer (SDL) for video, I/O and event handling. It\nwas chosen because it is easy to use and provides a common I/O interface across many\nplatforms. This allows it to be portable to a wide array of platforms, rather than the alternative of\ncoding to specific APIs on different platforms. This comes at a price however; it is harder to take\nadvantage of some platform specific features. SDL also has an accompanying family of libraries\nthat are used by Wesnoth for various purposes:\nSDL_Mixer for audio and sound\nSDL_Image for loading PNG and other image formats\nSDL_Net for network I/O\nAdditionally, Wesnoth uses several other libraries:\nBoost for a variety of advanced C++ features\nPango with Cairo for internationalized fonts\nzlib for compression\nPython and Lua for scripting support\nGNU gettext for internationalization\nThroughout Wesnoth's engine, the use of WML objects\u2014that is, string dictionaries with child\nnodes\u2014is fairly ubiquitous. Many objects can be constructed from a WML node, and also serialize\nthemselves to a WML node. Some parts of the engine keep data in this WML dictionary based\nformat, interpreting it directly rather than parsing it into a C++ data structure.\nWesnoth utilizes several important subsystems, most of which are as self-contained as possible.\nThis segmented structure has advantages for accessibility. An interested party can easily work a\ncode in a specific area and introduce changes without damaging the rest of the program. The\nmajor subdivisions include:\nA WML parser with preprocessor\nBasic I/O modules that abstract underlying libraries and system calls\u2014a video module, a\nsound module, a network module\nA GUI module containing widget implementations for buttons, lists, menus, etc.\nA display module for rendering the game board, units, animations, and so forth\nAn AI module\nA pathfinding module that includes many utility functions for dealing with a hexagonal gaming\nboard\nA map generation module for generating different kinds of random maps\nThere are also different modules for controlling different parts of the game flow:\nThe titlescreen module, for controlling display of the title screen.\n", "page": 336, "type": "text", "section": "Page 336"}
{"text": "The titlescreen module, for controlling display of the \nThe storyline module, for showing cut-scene sequences.\nThe lobby module, for displaying and allowing setup of games on the multiplayer server.\nThe \"play game\" module that controls the main gameplay.\nThe \"play game\" module and the main display module are the largest within Wesnoth. Their\npurpose is the least well defined, as their function is ever-changing and thus difficult to have a\nclear specification for. Consequently, the modules has often been in danger of suffering from the\nBlob anti-pattern over the program's history\u2014i.e., becoming huge dominant segments without\nwell-defined behaviors. The code in the display and play game modules are regularly reviewed to\nsee if any of it can be separated into a module of its own.\nThere are also ancillary features that are part of the overall project, but are separate from the\nmain program. This includes a multiplayer server that facilitates multiplayer network games, as well\nas a content server that allows users to upload their content to a common server and share it\nwith others. Both are written in C++.\n25.2. Wesnoth Markup Language\nAs an extensible game engine, Wesnoth uses a simple data language to store and load all game\ndata. Although XML was considered initially, we decided that we wanted something a little more\nfriendly to non-technical users, and a little more relaxed with regard to use of visual data. We\ntherefore developed our own data language, called Wesnoth Markup Language (WML). It was\ndesigned with the least technical of users in mind: the hope was that even users who find Python\nor HTML intimidating would be able to make sense of a WML file. All Wesnoth game data is stored\nin WML, including unit definitions, campaigns, scenarios, GUI definitions, and other game logic\nconfiguration.\nWML shares the same basic attributes as XML: elements and attributes, though it doesn't support\ntext within elements. WML attributes are represented simply as a dictionary mapping strings to\nstrings, with the program logic responsible for interpretation of attributes. A simple example of\nWML is a trimmed definition for the Elvish Fighter unit within the game:\n[unit_type]\n    id=Elvish Fighter\n    name= _ \"Elvish Fighter\"\n    race=elf\n    image=\"units/elves-wood/fighter.png\"\n    profile=\"portraits/elves/fighter.png\"\n    hitpoints=33\n    movement_type=woodland\n    movement=5\n    experience=40\n    level=1\n    alignment=neutral\n    advances_to=Elvish Captain,Elvish Hero\n    cost=14\n    usage=fighter\n    {LESS_NIMBLE_ELF}\n    [attack]\n        name=sword\n        description=_\"sword\"\n        icon=attacks/sword-elven.png\n        type=blade\n        range=melee\n        damage=5\n        number=4\n    [/attack]\n[/unit_type]\n", "page": 337, "type": "text", "section": "Page 337"}
{"text": "Since internationalization is important in Wesnoth, WML does have direct support for it: attribute\nvalues which have an underscore prefix are translatable. Any translatable string is converted using\nGNU gettext to the translated version of the string when the WML is parsed.\nRather than have many different WML documents, Wesnoth opts for the approach of all main\ngame data being presented to the game engine in just a single document. This allows for a single\nglobal variable to hold the document, and when the game is loaded all unit definitions, for instance,\nare loaded by looking for elements with the name unit_type within a units element.\nThough all data is stored in a single conceptual WML document, it would be unwieldy to have it all\nin a single file. Wesnoth therefore supports a preprocessor that is run over all WML before\nparsing. This preprocessor allows one file to include the contents of another file, or an entire\ndirectory. For instance:\n{gui/default/window/}\nwill include all the .cfg files within gui/default/window/.\nSince WML can become very verbose, the preprocessor also allows macros to be defined to\ncondense things. For instance, the {LESS_NIMBLE_ELF} invocation in the definition of the Elvish\nFighter is a call to a macro that makes certain elf units less nimble under certain conditions, such\nas when they are stationed in a forest:\n#define LESS_NIMBLE_ELF\n    [defense]\n        forest=40\n    [/defense]\n#enddef\nThis design has the advantage of making the engine agnostic to how the WML document is\nbroken up into files. It is the responsibility of WML authors to decide how to structure and divide\nall game data into different files and directories.\nWhen the game engine loads the WML document, it also defines some preprocessor symbols\naccording to various game settings. For instance, a Wesnoth campaign can define different\ndifficulty settings, with each difficulty setting resulting in a different preprocessor symbol being\ndefined. As an example, a common way to vary difficulty is by varying the amount of resources\ngiven to an opponent (represented by gold). To facilitate this, there is a WML macro defined like\nthis:\n#define GOLD EASY_AMOUNT NORMAL_AMOUNT HARD_AMOUNT\n  #ifdef EASY\n    gold={EASY_AMOUNT}\n  #endif\n  #ifdef NORMAL\n    gold={NORMAL_AMOUNT}\n  #endif\n  #ifdef HARD\n    gold={HARD_AMOUNT}\n  #endif\n#enddef\nThis macro can be invoked using, for instance, {GOLD 50 100 200} within the definition of an\nopponent to define how much gold the opponent has based on the difficulty level.\nSince the WML is processed conditionally, if any of the symbols provided to the WML document\nchange during execution of the Wesnoth engine, the entire WML document must be re-loaded\nand processed. For instance, when the user starts the game, the WML document is loaded and\navailable campaigns among other things are loaded. But then, if the user chooses to start a\ncampaign and chooses a certain difficulty level\u2014easy for instance\u2014then the entire document will\nhave to be re-loaded with EASY defined.\n", "page": 338, "type": "text", "section": "Page 338"}
{"text": "This design is convenient in that a single document contains all game data, and that symbols can\nallow easy configuration of the WML document. However, as a successful project, more and more\ncontent is available for Wesnoth, including much downloadable content\u2014all of which ends up\ninserted into the core document tree\u2014which means the WML document is many megabytes in\nsize. This has become a performance issue for Wesnoth: Loading the document may take up to a\nminute on some computers, causing delays in-game any time the document needs to be reloaded.\nAdditionally, it uses a substantial amount of memory. Some measures are used to counter this:\nwhen a campaign is loaded, it has a symbol unique to that campaign defined in the preprocessor.\nThis means that any content specific to that campaign can be #ifdefed to only be used when\nthat campaign is needed.\nAdditionally, Wesnoth uses a caching system to cache the fully preprocessed version of the WML\ndocument for a given set of key definitions. Naturally this caching system must inspect the\ntimestamp of all WML files so that if any have changed, the cached document is regenerated.\n25.3. Units in Wesnoth\nThe protagonists of Wesnoth are its units. An Elvish Fighter and an Elvish Shaman might battle\nagainst a Troll Warrior and an Orcish Grunt. All units share the same basic behavior, but many\nhave special abilities that alter the normal flow of gameplay. For example, a troll regenerates some\nof its health every turn, an Elvish shaman slows its opponents with an entangling root, and a\nWose is invisible in a forest.\nWhat is the best way to represent this in an engine? It is tempting to make a base unit class in\nC++, with different types of units derived from it. For instance, a wose_unit class could derive\nfrom unit, and unit could have a virtual function, bool is_invisible() const, which\nreturns false, which the wose_unit overrides, returning true if the unit happens to be in forest.\nSuch an approach would work reasonably well for a game with a limited set of rules. Unfortunately\nWesnoth is quite a large game and such an approach is not easily extendable. If a person wanted\nto add a new type of unit under this approach, it would require the addition of a new C++ class to\nthe game. Additionally, it does not allow different characteristics to be combined well: what if you\nhad a unit that regenerated, could slow enemies with a net, and was invisible in a forest? You\nwould have to write an entirely new class that duplicates code in the other classes.\nWesnoth's unit system doesn't use inheritance at all to accomplish this task. Instead, it uses a\nunit class to represent instances of units, and a unit_type class, which represents the\nimmutable characteristics that all units of a certain type share. The unit class has a reference to\nthe type of object that it is. All the possible unit_type objects are stored in a globally held\ndictionary that is loaded when the main WML document is loaded.\nA unit type has a list of all the abilities that that unit has. For instance, a Troll has the \"regeneration\"\nability that makes it heal life every turn. A Saurian Skirmisher has the \"skirmisher\" ability that allows\nit to move through enemy lines. Recognition of these abilities is built into the engine\u2014for instance,\nthe pathfinding algorithms will check if a unit has the \"skirmisher\" flag set to see if it can move\nfreely past enemy lines. This approach allows an individual to add new units, which have any\ncombination of abilities made by the engine, by only editing WML. Of course, it doesn't allow\nadding completely new abilities and unit behavior without modifying the engine.\nAdditionally, each unit in Wesnoth may have any number of ways to attack. For instance, an Elvish\nArcher has a long-range bow attack and also a short-range sword attack. Each deals different\ndamage amounts and characteristics. To represent an attack, there is an attack_type class,\nwith every unit_type instance having a list of possible attack_types.\nTo give each unit more character, Wesnoth has a feature known as traits. Upon recruitment,\nmost units are assigned two traits at random from a predefined list. For instance, a strong unit\ndoes more damage with its melee attacks, while an intelligent unit needs less experience before it\n\"levels up.\" Also, it is possible for units to acquire equipment during the game that make them\nmore powerful. For instance, there might be a sword a unit can pick up that makes their attacks\n", "page": 339, "type": "text", "section": "Page 339"}
{"text": "do more damage. To implement traits and equipment Wesnoth allows modifications on units,\nwhich are WML-defined alterations to a unit's statistics. The modification can even be applied to\ncertain types of attacks. For instance, the strong trait gives strong units more damage when\nattacking in melee, but not when using a ranged strike.\nAllowing completely configurable unit behavior with WML would be an admirable goal, so it is\ninstructional to consider why Wesnoth has never achieved such a goal. WML would need to be\nmuch more flexible than it is if it were to allow arbitrary unit behavior. Rather than being a data-\noriented language, WML would have to be extended into a full-fledged programming language and\nthat would be intimidating for many aspiring contributors.\nAdditionally, the Wesnoth AI, which is developed in C++, recognizes the abilities present in the\ngame. It takes into account regeneration, invisibility, and so forth, and attempts to maneuver its\nunits to take best advantage of these different abilities. Even if a unit ability could be created using\nWML, it would be difficult to make the AI sophisticated enough to recognize this ability to take\nadvantage of it. Implementing an ability but not having it accounted for by the AI would not be a\nvery satisfying implementation. Similarly, implementing an ability in WML and then having to modify\nthe AI in C++ to account for the ability would be awkward. Thus, having units definable in WML,\nbut having abilities hard-wired into the engine is considered a reasonable compromise that works\nbest for Wesnoth's specific requirements.\n25.4. Wesnoth's Multiplayer Implementation\nThe Wesnoth multiplayer implementation uses a simple-as-possible approach to implementing\nmultiplayer in Wesnoth. It attempts to mitigate the possibility of malicious attacks on the server,\nbut doesn't make a serious attempt to prevent cheating. Any movement that is made in a\nWesnoth game\u2014moving of a unit, attacking an enemy, recruiting a unit, and so forth\u2014can be\nsaved as a WML node. For instance, a command to move a unit might be saved into WML like this:\n[move]\n    x=\"11,11,10,9,8,7\"\n    y=\"6,7,7,8,8,9\"\n[/move]\nThis shows the path that a unit follows as a result of a player's commands. The game then has a\nfacility to execute any such WML command given to it. This is very useful because it means that a\ncomplete replay can be saved, by storing the initial state of the game and then all subsequent\ncommands. Being able to replay games is useful both for players to observe each other playing,\nas well as to help in certain kinds of bug reports.\nWe decided that the community would try to focus on friendly, casual games for the network\nmultiplayer implementation of Wesnoth. Rather than fight a technical battle against anti-social\ncrackers trying to compromise cheat prevention systems, the project would simply not try hard to\nprevent cheating. An analysis of other multiplayer games indicated that competitive ranking\nsystems were a key source of anti-social behavior. Deliberately preventing such functions on the\nserver greatly reduced the motivation for individuals to cheat. Moreover the moderators try to\nencourage a positive gaming community where individuals develop personal rapport with other\nplayers and play with them. This placed a greater emphasis on relationships rather than\ncompetition. The outcome of these efforts has been deemed successful, as thus far efforts to\nmaliciously hack the game have been largely isolated.\nWesnoth's multiplayer implementation consists of a typical client-server infrastructure. A server,\nknown as wesnothd, accepts connections from the Wesnoth client, and sends the client a\nsummary of available games. Wesnoth will display a 'lobby' to the player who can choose to join a\ngame or create a new game for others to join. Once players are in a game and the game starts,\neach instance of Wesnoth will generate WML commands describing the actions the player makes.\nThese commands are sent to the server, and then the server relays them on to all the other\nclients in the game. The server will thus act as a very thin, simple relay. The replay system is used\non the other clients to execute the WML commands. Since Wesnoth is a turn-based game, TCP/IP\nis used for all network communication.\n", "page": 340, "type": "text", "section": "Page 340"}
{"text": "This system also allows observers to easily watch a game. An observer can join a game in-\nprogress, in which case the server will send the WML representing the initial state of the game,\nfollowed by a history of all commands that have been carried out since the start of the game. This\nallows new observers to get up to speed on the state of the game. They can see a history of the\ngame, although it does take time for the observer to get to the game's current position\u2014the\nhistory of commands can be fast forwarded but it still takes time. The alternative would be to have\none of the clients generate a snapshot of the game's current state as WML and send it to the new\nobserver; however this approach would burden clients with overhead based on observers, and\ncould facilitate denial-of-service attacks by having many observers join a game.\nOf course, since Wesnoth clients do not share any kind of game state with each other, only\nsending commands, it is important that they agree on the rules of the game. The server is\nsegmented by version, with only players using the same version of the game able to interact.\nPlayers are immediately alerted if their client's game becomes out of sync with others. This also is\na useful system to prevent cheating. Although it is rather easy for a player to cheat by modifying\ntheir client, any difference between versions will immediately be identified to players where it can\nbe dealt with.\n25.5. Conclusion\nWe believe that the beauty of the Battle for Wesnoth as a program is how it made coding\naccessible to a wide variety of individuals. To achieve this aim, the project often made\ncompromises that do not look elegant whatsoever in the code. It should be noted that many of\nthe project's more talented programmers frown upon WML for its inefficient syntax. Yet this\ncompromise enabled one of the project's greatest successes. Today Wesnoth can boast of\nhundreds of user-made campaigns and eras, created mostly by users with little or no\nprogramming experience. Furthermore it has inspired a number of people to take up\nprogramming as a profession, using the project as a learning tool. Those are tangible\naccomplishments that few programs can equal.\nOne of the key lessons a reader should take away from Wesnoth's efforts is to consider the\nchallenges faced by lesser skilled programmers. It requires an awareness of what blocks\ncontributors from actually performing coding and developing their skills. For example an individual\nmight want to contribute to the program but does not possess any programming skills. Dedicated\ntechnological editors like emacs or vim possess a significant learning curve that might prove\ndaunting for such an individual. Thus WML was designed to allow a simple text editor to open up\nits files, giving anybody the tools to contribute.\nHowever, increasing a code base's accessibility is not a simple objective to achieve. There are no\nhard and fast rules for increasing code's accessibility. Rather it requires a balance between\ndifferent considerations, which can have negative consequences that the community must be\naware of. This is apparent in how the program dealt with dependencies. In some cases,\ndependencies can actually increase barriers to participation, while in others they can allow people\nto contribute more easily. Every issue must be considered on a case-by-case basis.\nWe should also be careful not to overstate some of Wesnoth's successes. The project enjoyed\nsome advantages that are not easily replicated by other programs. Making code accessible to a\nwider public is partly a result of the program's setting. As an open source program, Wesnoth had\nseveral advantages in this regard. Legally the GNU license allows someone to open up an existing\nfile, understand how it works and makes changes. Individuals are encouraged to experiment, learn\nand share within this culture, which might not be appropriate for other programs. Nevertheless\nwe hope that there are certain elements that might prove useful for all developers and help them\nin their effort to find beauty in coding.\n", "page": 341, "type": "text", "section": "Page 341"}
{"text": "The Architecture of\nOpen Source Applications\nAmy Brown and Greg Wilson (eds.)\nISBN 978-1-257-63801-7 \nLicense / Buy / Contribute\nBibliography\n[AF94] Rick Adams and Donnalyn Frey: !%@:: A Directory of Electronic Mail Addressing &\nNetworks. O'Reilly Media, Sebastopol, CA, fourth edition, 1994.\n[Ald02] Gaudenz Alder: The JGraph Swing Component. PhD thesis, ETH Zurich, 2002.\n[BCC+05] Louis Bavoil, Steve Callahan, Patricia Crossno, Juliana Freire, Carlos E. Scheidegger,\nCl\u00e1udio T. Silva, and Huy T. Vo: \"VisTrails: Enabling Interactive Multiple-View Visualizations\". Proc.\nIEEE Visualization, pages 135\u2013142, 2005.\n[Bro10] Frederick P. Brooks, Jr.: The Design of Design: Essays from a Computer Scientist. Pearson\nEducation, 2010.\n[CDG+06] Fay Chang, Jeffrey Dean, Sanjary Ghemawat, Wilson C. Hsieh, Deborah A. Wallach, Mike\nBurrows, Tushar Chandra, Andrew Fikes, and Robert E. Gruber: \"BigTable: a Distributed Storage\nSystem for Structured Data\". Proc. 7th USENIX Symposium on Operating Systems Design and\nImplementation (OSDI'06). USENIX Association, 2006.\n[CIRT00] P. H. Carns, W. B. Ligon III, R. B. Ross, and R. Thakur: \"PVFS: A Parallel File System for\nLinux Clusters\". Proc. 4th Annual Linux Showcase and Conference, pages 317\u2013327, 2000.\n[Com79] Douglas Comer: \"Ubiquitous B-Tree\". ACM Computing Surveys, 11:121\u2013137, June 1979.\n[CRS+08] Brian F. Cooper, Raghu Ramakrishnan, Utkarsh Srivastava, Adam Silberstein, Philip\nBohannon, Hans Arno Jacobsen, Nick Puz, Daniel Weaver, and Ramana Yerneni: \"PNUTS: Yahoo!'s\nHosted Data Serving Platform\". PVLDB, 1(2):1277\u20131288, 2008.\n[DG04] Jeffrey Dean and Sanjay Ghemawat: \"MapReduce: Simplified Data Processing on Large\nClusters\". Proc. Sixth Symposium on Operating System Design and Implementation, 2004.\n[DHJ+07] Giuseppe DeCandia, Deniz Hastorun, Madan Jampani, Gunavardhan Kakulapati, Avinash\nLakshman, Alex Pilchin, Swaminathan Sivasubramanian, Peter Vosshall, and Werner Vogels:\n\"Dynamo: Amazon's Highly Available Key-Value Store\". SOSP'07: Proc. Twenty-First ACM SIGOPS\nSymposium on Operating Systems Principles, pages 205\u2013220, 2007.\n[FKSS08] Juliana Freire, David Koop, Emanuele Santos, and Cl\u00e1udio T. Silva: \"Provenance for\nComputational Tasks: A Survey\". Computing in Science and Engineering, 10(3):11\u201321, 2008.\n[FSC+06] Juliana Freire, Cl\u00e1udio T. Silva, Steve Callahan, Emanuele Santos, Carlos E. Scheidegger,\nand Huy T. Vo: \"Managing Rapidly-Evolving Scientific Workflows\". International Provenance and\nAnnotation Workshop (IPAW), LNCS 4145, pages 10\u201318. Springer Verlag, 2006.\n[GGL03] Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung: \"The Google File System\". Proc.\nACM Symposium on Operating Systems Principles, pages 29\u201343, 2003.\n[GL02] Seth Gilbert and Nancy Lynch: \"Brewer's Conjecture and the Feasibility of Consistent\nAvailable Partition-Tolerant Web Services\". ACM SIGACT News, page 2002, 2002.\n[GR09] Adam Goucher and Tim Riley (editors): Beautiful Testing. O'Reilly, 2009.\n[GLPT76] Jim Gray, Raymond Lorie, Gianfranco Putzolu, and Irving Traiger: \"Granularity of Locks\nand Degrees of Consistency in a Shared Data Base\". Proc. 1st International Conference on Very\n", "page": 342, "type": "text", "section": "Page 342"}
{"text": "Large Data Bases, pages 365\u2013394, 1976.\n[Gra81] Jim Gray: \"The Transaction Concept: Virtues and Limitations\". Proc. Seventh International\nConference on Very Large Data Bases, pages 144\u2013154, 1981.\n[Hor05] Cay Horstmann: Object-Oriented Design and Patterns. Wiley, 2 edition, 2005.\n[HR83] Theo Haerder and Andreas Reuter: \"Principles of Transaction-Oriented Database\nRecovery\". ACM Computing Surveys, 15, December 1983.\n[Kit10] Kitware: VTK User's Guide. Kitware, Inc., 11th edition, 2010.\n[Knu74] Donald E. Knuth: \"Structured Programming with Go To Statements\". ACM Computing\nSurveys, 6(4), 1974.\n[LA04] Chris Lattner and Vikram Adve: \"LLVM: A Compilation Framework for Lifelong Program\nAnalysis & Transformation\". Proc. 2004 International Symposium on Code Generation and\nOptimization (CGO'04), Mar 2004.\n[LCWB+11] H. Andr\u00e9es Lagar-Cavilla, Joseph A. Whitney, Roy Bryant, Philip Patchin, Michael\nBrudno, Eyal de Lara, Stephen M. Rumble, M. Satyanarayanan, and Adin Scannell: \"SnowFlock:\nVirtual Machine Cloning as a First-Class Cloud Primitive\". ACM Transactions on Computer Systems,\n19(1), 2011.\n[Mac06] Matt Mackall: \"Towards a Better SCM: Revlog and Mercurial\". 2006 Ottawa Linux\nSymposium, 2006.\n[MQ09] Marshall Kirk McKusick and Sean Quinlan: \"GFS: Evolution on Fast-Forward\". ACM Queue,\n7(7), 2009.\n[PGL+05] Anna Persson, Henrik Gustavsson, Brian Lings, Bj\u00f6rn Lundell, Anders Mattson, and Ulf\n\u00c4rlig: \"OSS Tools in a Heterogeneous Environment for Embedded Systems Modelling: an Analysis\nof Adoptions of XMI\". SIGSOFT Software Engineering Notes, 30(4), 2005.\n[PPT+93] Rob Pike, Dave Presotto, Ken Thompson, Howard Trickey, and Phil Winterbottom: \"The\nUse of Name Spaces in Plan 9\". Operating Systems Review, 27(2):72\u201376, 1993.\n[Rad94] Sanjay Radia: \"Naming Policies in the Spring System\". Proc. 1st IEEE Workshop on\nServices in Distributed and Networked Environments, pages 164\u2013171, 1994.\n[RP93] Sanjay Radia and Jan Pachl: \"The Per-Process View of Naming and Remote Execution\". IEEE\nParallel and Distributed Technology, 1(3):71\u201380, 1993.\n[Shu05] Rose Shumba: \"Usability of Rational Rose and Visio in a Software Engineering Course\".\nSIGCSE Bulletin, 37(2), 2005.\n[Shv10] Konstantin V. Shvachko: \"HDFS Scalability: The Limits to Growth\". ;login:, 35(2), 2010.\n[SML06] Will Schroeder, Ken Martin, and Bill Lorensen: The Visualization Toolkit: An Object-Oriented\nApproach to 3D Graphics. Kitware, Inc., 4 edition, 2006.\n[SO92] Margo Seltzer and Michael Olson: \"LIBTP: Portable, Modular Transactions for Unix\". Proc\n1992 Winter USENIX Conference, pages 9\u201326, January 1992.\n[Spi03] Diomidis Spinellis: \"On the Declarative Specification of Models\". IEEE Software, 20(2), 2003.\n[SVK+07] Carlos E. Scheidegger, Huy T. Vo, David Koop, Juliana Freire, and Cl\u00e1udio T. Silva:\n\"Querying and Creating Visualizations by Analogy\". IEEE Transactions on Visualization and\nComputer Graphics, 13(6):1560\u20131567, 2007.\n[SY91] Margo Seltzer and Ozan Yigit: \"A New Hashing Package for Unix\". Proc. 1991 Winter\nUSENIX Conference, pages 173\u2013184, January 1991.\n", "page": 343, "type": "text", "section": "Page 343"}
{"text": "[Tan06] Audrey Tang: \"\u2013O fun: Optimizing for Fun\". http://www.slideshare.net/autang/ofun-\noptimizing-for-fun, 2006.\n[Top00] Kim Topley: Core Swing: Advanced Programming. Prentice-Hall, 2000.\n", "page": 344, "type": "text", "section": "Page 344"}
{"text": "You may also enjoy\u2026\nMaking Software: What Really Works,\nand Why We Believe It\nAndy Oram and Greg Wilson (eds.)\nO'Reilly Media, 2010, 978-0596808327\nMany claims are made about how certain tools, technologies, and practices improve software\ndevelopment. But which are true, and which are merely wishful thinking? In Making Software,\nleading researchers and practitioners present chapter-length summaries of key empirical findings\nin software engineering, and answer questions like:\nAre some programmers really ten times more productive than others?\nDoes writing tests first help you develop better code faster?\nCan code metrics predict the number of bugs in a piece of software?\nDoes using design patterns actually make software better?\nWhat effect does personality have on pair programming?\nWhat matters more: how far apart people are geographically, or how far apart they are in the\norg chart?\nAs with The Architecture of Open Source Applications, royalties from Making Software will be\ndonated to Amnesty International.\n", "page": 345, "type": "text", "section": "Page 345"}
